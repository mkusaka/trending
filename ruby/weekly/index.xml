<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Ruby Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-11T01:46:11Z</updated>
  <subtitle>Weekly Trending of Ruby in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rails-api/active_model_serializers</title>
    <updated>2022-12-11T01:46:11Z</updated>
    <id>tag:github.com,2022-12-11:/rails-api/active_model_serializers</id>
    <link href="https://github.com/rails-api/active_model_serializers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ActiveModel::Serializer implementation and Rails hooks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ActiveModelSerializers&lt;/h1&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;ActiveModelSerializers is undergoing some renovations. See &lt;a href=&#34;https://raw.githubusercontent.com/rails-api/active_model_serializers/master/#status-of-ams&#34;&gt;Development Status&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Help&lt;/h2&gt; &#xA;&lt;p&gt;If you find a bug, please report an &lt;a href=&#34;https://github.com/rails-api/active_model_serializers/issues/new&#34;&gt;Issue&lt;/a&gt; and see our &lt;a href=&#34;https://raw.githubusercontent.com/rails-api/active_model_serializers/master/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you have a question, please &lt;a href=&#34;http://stackoverflow.com/questions/tagged/active-model-serializers&#34;&gt;post to Stack Overflow&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to chat, we have a &lt;a href=&#34;http://amserializers.herokuapp.com&#34;&gt;community slack&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re reading this at &lt;a href=&#34;https://github.com/rails-api/active_model_serializers&#34;&gt;https://github.com/rails-api/active_model_serializers&lt;/a&gt; you are reading documentation for our &lt;code&gt;master&lt;/code&gt;, which is not yet released.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/rails-api/active_model_serializers/tree/0-10-stable&#34;&gt;0.10 (0-10-stable) Documentation &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;http://www.rubydoc.info/gems/active_model_serializers/0.10.6&#34;&gt; &lt;img src=&#34;http://img.shields.io/badge/yard-docs-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/rails-api/active_model_serializers/tree/v0.10.6/docs&#34;&gt; Guides &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/rails-api/active_model_serializers/tree/0-9-stable&#34;&gt;0.9 (0-9-stable) Documentation &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;http://www.rubydoc.info/github/rails-api/active_model_serializers/0-9-stable&#34;&gt; &lt;img src=&#34;http://img.shields.io/badge/yard-docs-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/rails-api/active_model_serializers/tree/0-8-stable&#34;&gt;0.8 (0-8-stable) Documentation &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;http://www.rubydoc.info/github/rails-api/active_model_serializers/0-8-stable&#34;&gt; &lt;img src=&#34;http://img.shields.io/badge/yard-docs-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Status of AMS&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;em&gt;Status&lt;/em&gt;:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ùóÔ∏è All existing PRs against master will need to be closed and re-opened against 0-10-stable, if so desired&lt;/li&gt; &#xA; &lt;li&gt;‚ùóÔ∏è Master, for the moment, won&#39;t have any released version of AMS on it.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;üëÄ&lt;/span&gt; See below for &lt;a href=&#34;https://raw.githubusercontent.com/rails-api/active_model_serializers/master/#alternatives&#34;&gt;alternatives&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;em&gt;Changes to 0.10.x maintenance&lt;/em&gt;:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The 0.10.x version has become a huge maintenance version. We had hoped to get it in shape for a 1.0 release, but it is clear that isn&#39;t going to happen. Almost none of the maintainers from 0.8, 0.9, or earlier 0.10 are still working on AMS. We&#39;ll continue to maintain 0.10.x on the 0-10-stable branch, but maintainers won&#39;t otherwise be actively developing on it. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We may choose to make a 0.11.x ( 0-11-stable) release based on 0-10-stable that just removes the deprecations.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;em&gt;What&#39;s happening to AMS&lt;/em&gt;:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There&#39;s been a lot of churn around AMS since it began back in &lt;a href=&#34;https://raw.githubusercontent.com/rails-api/active_model_serializers/master/CHANGELOG-prehistory.md&#34;&gt;Rails 3.2&lt;/a&gt; and a lot of new libraries are around and the JSON:API spec has reached 1.0.&lt;/li&gt; &#xA; &lt;li&gt;If there is to be a 1.0 release of AMS, it will need to address the general needs of serialization in much the way ActiveJob can be used with different workers.&lt;/li&gt; &#xA; &lt;li&gt;The next major release &lt;em&gt;is&lt;/em&gt; in development. We&#39;re starting simple and avoiding, at least at the outset, all the complications in AMS version, especially all the implicit behavior from guessing the serializer, to the association&#39;s serializer, to the serialization type, etc.&lt;/li&gt; &#xA; &lt;li&gt;The basic idea is that models to serializers are a one to many relationship. Everything will need to be explicit. If you want to serialize a User with a UserSerializer, you&#39;ll need to call it directly. The serializer will essentially be for defining a basic JSON:API resource object: id, type, attributes, and relationships. The serializer will have an as_json method and can be told which fields (attributes/relationships) to serialize to JSON and will likely &lt;em&gt;not&lt;/em&gt; know serialize any more than the relations id and type. Serializing anything more about the relations would require code that called a serializer. (This is still somewhat in discussion).&lt;/li&gt; &#xA; &lt;li&gt;If this works out, the idea is to get something into Rails that existing libraries can use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/rails-api/active_model_serializers/pull/2121&#34;&gt;PR 2121&lt;/a&gt; where these changes were introduced for more information and any discussion.&lt;/p&gt; &#xA;&lt;h2&gt;Alternatives&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://jsonapi-rb.org/&#34;&gt;jsonapi-rb&lt;/a&gt; is a &lt;a href=&#34;https://gist.github.com/NullVoxPopuli/748e89ddc1732b42fdf42435d773734a&#34;&gt;highly performant&lt;/a&gt; and modular JSON:API-only implementation. There&#39;s a vibrant community around it that has produced projects such as &lt;a href=&#34;https://jsonapi-suite.github.io/jsonapi_suite/&#34;&gt;JSON:API Suite&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fast-jsonapi/fast_jsonapi&#34;&gt;fast_jsonapi&lt;/a&gt; is a lightning fast JSON:API serializer for Ruby Objects.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cerebris/jsonapi-resources&#34;&gt;jsonapi-resources&lt;/a&gt; is a popular resource-focused framework for implementing JSON:API servers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/procore/blueprinter&#34;&gt;blueprinter&lt;/a&gt; is a fast, declarative, and API spec agnostic serializer that uses composable views to reduce duplication. From your friends at Procore.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For benchmarks against alternatives, see &lt;a href=&#34;https://github.com/rails-api/active_model_serializers/tree/benchmarks&#34;&gt;https://github.com/rails-api/active_model_serializers/tree/benchmarks&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Semantic Versioning&lt;/h2&gt; &#xA;&lt;p&gt;This project adheres to &lt;a href=&#34;http://semver.org/&#34;&gt;semver&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/rails-api/active_model_serializers/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>zendesk/ruby-kafka</title>
    <updated>2022-12-11T01:46:11Z</updated>
    <id>tag:github.com,2022-12-11:/zendesk/ruby-kafka</id>
    <link href="https://github.com/zendesk/ruby-kafka" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Ruby client library for Apache Kafka&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deprecation notice&lt;/h1&gt; &#xA;&lt;p&gt;This library is &lt;strong&gt;no longer&lt;/strong&gt; actively developed and has been superseded by &lt;a href=&#34;https://github.com/edenhill/librdkafka&#34;&gt;&lt;code&gt;librdkafka&lt;/code&gt;&lt;/a&gt; via &lt;a href=&#34;https://github.com/appsignal/rdkafka-ruby/&#34;&gt;&lt;code&gt;rdkafka-ruby&lt;/code&gt;&lt;/a&gt; bindings. While this library may still receive security patches and bug fixes, it is no longer recommended for production usage.&lt;/p&gt; &#xA;&lt;p&gt;There needs to be a concerted effort to keep up with Kafka features. There is no point in trying to keep up with Kafka development when other languages use a well-established C binding and get more official support.&lt;/p&gt; &#xA;&lt;h1&gt;ruby-kafka&lt;/h1&gt; &#xA;&lt;p&gt;A Ruby client library for &lt;a href=&#34;http://kafka.apache.org/&#34;&gt;Apache Kafka&lt;/a&gt;, a distributed log and message bus. The focus of this library will be operational simplicity, with good logging and metrics that can make debugging issues easier.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#compatibility&#34;&gt;Compatibility&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#setting-up-the-kafka-client&#34;&gt;Setting up the Kafka Client&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#producing-messages-to-kafka&#34;&gt;Producing Messages to Kafka&lt;/a&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#efficiently-producing-messages&#34;&gt;Efficiently Producing Messages&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#asynchronously-producing-messages&#34;&gt;Asynchronously Producing Messages&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#serialization&#34;&gt;Serialization&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#partitioning&#34;&gt;Partitioning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#buffering-and-error-handling&#34;&gt;Buffering and Error Handling&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#message-durability&#34;&gt;Message Durability&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#message-delivery-guarantees&#34;&gt;Message Delivery Guarantees&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#compression&#34;&gt;Compression&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#producing-messages-from-a-rails-application&#34;&gt;Producing Messages from a Rails Application&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#consuming-messages-from-kafka&#34;&gt;Consuming Messages from Kafka&lt;/a&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#consumer-groups&#34;&gt;Consumer Groups&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#consumer-checkpointing&#34;&gt;Consumer Checkpointing&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#topic-subscriptions&#34;&gt;Topic Subscriptions&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#shutting-down-a-consumer&#34;&gt;Shutting Down a Consumer&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#consuming-messages-in-batches&#34;&gt;Consuming Messages in Batches&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#balancing-throughput-and-latency&#34;&gt;Balancing Throughput and Latency&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#customizing-partition-assignment-strategy&#34;&gt;Customizing Partition Assignment Strategy&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#thread-safety&#34;&gt;Thread Safety&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#logging&#34;&gt;Logging&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#instrumentation&#34;&gt;Instrumentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#monitoring&#34;&gt;Monitoring&lt;/a&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#what-to-monitor&#34;&gt;What to Monitor&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#reporting-metrics-to-statsd&#34;&gt;Reporting Metrics to Statsd&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#reporting-metrics-to-datadog&#34;&gt;Reporting Metrics to Datadog&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#understanding-timeouts&#34;&gt;Understanding Timeouts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#security&#34;&gt;Security&lt;/a&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#encryption-and-authentication-using-ssl&#34;&gt;Encryption and Authentication using SSL&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#authentication-using-sasl&#34;&gt;Authentication using SASL&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#topic-management&#34;&gt;Topic management&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#design&#34;&gt;Design&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#producer-design&#34;&gt;Producer Design&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#asynchronous-producer-design&#34;&gt;Asynchronous Producer Design&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#consumer-design&#34;&gt;Consumer Design&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#support-and-discussion&#34;&gt;Support and Discussion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#higher-level-libraries&#34;&gt;Higher level libraries&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#message-processing-frameworks&#34;&gt;Message processing frameworks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#message-publishing-libraries&#34;&gt;Message publishing libraries&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Add this line to your application&#39;s Gemfile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;gem &#39;ruby-kafka&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ bundle&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or install it yourself as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ gem install ruby-kafka&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Producer API&lt;/th&gt; &#xA;   &lt;th&gt;Consumer API&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 0.8&lt;/th&gt; &#xA;   &lt;td&gt;Full support in v0.4.x&lt;/td&gt; &#xA;   &lt;td&gt;Unsupported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 0.9&lt;/th&gt; &#xA;   &lt;td&gt;Full support in v0.4.x&lt;/td&gt; &#xA;   &lt;td&gt;Full support in v0.4.x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 0.10&lt;/th&gt; &#xA;   &lt;td&gt;Full support in v0.5.x&lt;/td&gt; &#xA;   &lt;td&gt;Full support in v0.5.x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 0.11&lt;/th&gt; &#xA;   &lt;td&gt;Full support in v0.7.x&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 1.0&lt;/th&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 2.0&lt;/th&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 2.1&lt;/th&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 2.2&lt;/th&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 2.3&lt;/th&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 2.4&lt;/th&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 2.5&lt;/th&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 2.6&lt;/th&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Kafka 2.7&lt;/th&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;   &lt;td&gt;Limited support&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;This library is targeting Kafka 0.9 with the v0.4.x series and Kafka 0.10 with the v0.5.x series. There&#39;s limited support for Kafka 0.8, and things should work with Kafka 0.11, although there may be performance issues due to changes in the protocol.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 0.8:&lt;/strong&gt; Full support for the Producer API in ruby-kafka v0.4.x, but no support for consumer groups. Simple message fetching works.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 0.9:&lt;/strong&gt; Full support for the Producer and Consumer API in ruby-kafka v0.4.x.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 0.10:&lt;/strong&gt; Full support for the Producer and Consumer API in ruby-kafka v0.5.x. Note that you &lt;em&gt;must&lt;/em&gt; run version 0.10.1 or higher of Kafka due to limitations in 0.10.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 0.11:&lt;/strong&gt; Full support for Producer API, limited support for Consumer API in ruby-kafka v0.7.x. New features in 0.11.x includes new Record Batch format, idempotent and transactional production. The missing feature is dirty reading of Consumer API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 1.0:&lt;/strong&gt; Everything that works with Kafka 0.11 should still work, but so far no features specific to Kafka 1.0 have been added.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 2.0:&lt;/strong&gt; Everything that works with Kafka 1.0 should still work, but so far no features specific to Kafka 2.0 have been added.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 2.1:&lt;/strong&gt; Everything that works with Kafka 2.0 should still work, but so far no features specific to Kafka 2.1 have been added.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 2.2:&lt;/strong&gt; Everything that works with Kafka 2.1 should still work, but so far no features specific to Kafka 2.2 have been added.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 2.3:&lt;/strong&gt; Everything that works with Kafka 2.2 should still work, but so far no features specific to Kafka 2.3 have been added.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 2.4:&lt;/strong&gt; Everything that works with Kafka 2.3 should still work, but so far no features specific to Kafka 2.4 have been added.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 2.5:&lt;/strong&gt; Everything that works with Kafka 2.4 should still work, but so far no features specific to Kafka 2.5 have been added.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 2.6:&lt;/strong&gt; Everything that works with Kafka 2.5 should still work, but so far no features specific to Kafka 2.6 have been added.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kafka 2.7:&lt;/strong&gt; Everything that works with Kafka 2.6 should still work, but so far no features specific to Kafka 2.7 have been added.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This library requires Ruby 2.1 or higher.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka&#34;&gt;documentation site&lt;/a&gt; for detailed documentation on the latest release. Note that the documentation on GitHub may not match the version of the library you&#39;re using ‚Äì there are still being made many changes to the API.&lt;/p&gt; &#xA;&lt;h3&gt;Setting up the Kafka Client&lt;/h3&gt; &#xA;&lt;p&gt;A client must be initialized with at least one Kafka broker, from which the entire Kafka cluster will be discovered. Each client keeps a separate pool of broker connections. Don&#39;t use the same client from more than one thread.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#34;kafka&#34;&#xA;&#xA;# The first argument is a list of &#34;seed brokers&#34; that will be queried for the full&#xA;# cluster topology. At least one of these *must* be available. `client_id` is&#xA;# used to identify this client in logs and metrics. It&#39;s optional but recommended.&#xA;kafka = Kafka.new([&#34;kafka1:9092&#34;, &#34;kafka2:9092&#34;], client_id: &#34;my-application&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use a hostname with seed brokers&#39; IP addresses:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new(&#34;seed-brokers:9092&#34;, client_id: &#34;my-application&#34;, resolve_seed_brokers: true)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Producing Messages to Kafka&lt;/h3&gt; &#xA;&lt;p&gt;The simplest way to write a message to a Kafka topic is to call &lt;code&gt;#deliver_message&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new(...)&#xA;kafka.deliver_message(&#34;Hello, World!&#34;, topic: &#34;greetings&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will write the message to a random partition in the &lt;code&gt;greetings&lt;/code&gt; topic. If you want to write to a &lt;em&gt;specific&lt;/em&gt; partition, pass the &lt;code&gt;partition&lt;/code&gt; parameter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# Will write to partition 42.&#xA;kafka.deliver_message(&#34;Hello, World!&#34;, topic: &#34;greetings&#34;, partition: 42)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you don&#39;t know exactly how many partitions are in the topic, or if you&#39;d rather have some level of indirection, you can pass in &lt;code&gt;partition_key&lt;/code&gt; instead. Two messages with the same partition key will always be assigned to the same partition. This is useful if you want to make sure all messages with a given attribute are always written to the same partition, e.g. all purchase events for a given customer id.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# Partition keys assign a partition deterministically.&#xA;kafka.deliver_message(&#34;Hello, World!&#34;, topic: &#34;greetings&#34;, partition_key: &#34;hello&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Kafka also supports &lt;em&gt;message keys&lt;/em&gt;. When passed, a message key can be used instead of a partition key. The message key is written alongside the message value and can be read by consumers. Message keys in Kafka can be used for interesting things such as &lt;a href=&#34;http://kafka.apache.org/documentation.html#compaction&#34;&gt;Log Compaction&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#partitioning&#34;&gt;Partitioning&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# Set a message key; the key will be used for partitioning since no explicit&#xA;# `partition_key` is set.&#xA;kafka.deliver_message(&#34;Hello, World!&#34;, key: &#34;hello&#34;, topic: &#34;greetings&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Efficiently Producing Messages&lt;/h4&gt; &#xA;&lt;p&gt;While &lt;code&gt;#deliver_message&lt;/code&gt; works fine for infrequent writes, there are a number of downsides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Kafka is optimized for transmitting messages in &lt;em&gt;batches&lt;/em&gt; rather than individually, so there&#39;s a significant overhead and performance penalty in using the single-message API.&lt;/li&gt; &#xA; &lt;li&gt;The message delivery can fail in a number of different ways, but this simplistic API does not provide automatic retries.&lt;/li&gt; &#xA; &lt;li&gt;The message is not buffered, so if there is an error, it is lost.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The Producer API solves all these problems and more:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# Instantiate a new producer.&#xA;producer = kafka.producer&#xA;&#xA;# Add a message to the producer buffer.&#xA;producer.produce(&#34;hello1&#34;, topic: &#34;test-messages&#34;)&#xA;&#xA;# Deliver the messages to Kafka.&#xA;producer.deliver_messages&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;#produce&lt;/code&gt; will buffer the message in the producer but will &lt;em&gt;not&lt;/em&gt; actually send it to the Kafka cluster. Buffered messages are only delivered to the Kafka cluster once &lt;code&gt;#deliver_messages&lt;/code&gt; is called. Since messages may be destined for different partitions, this could involve writing to more than one Kafka broker. Note that a failure to send all buffered messages after the configured number of retries will result in &lt;code&gt;Kafka::DeliveryFailed&lt;/code&gt; being raised. This can be rescued and ignored; the messages will be kept in the buffer until the next attempt.&lt;/p&gt; &#xA;&lt;p&gt;Read the docs for &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/Producer&#34;&gt;Kafka::Producer&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h4&gt;Asynchronously Producing Messages&lt;/h4&gt; &#xA;&lt;p&gt;A normal producer will block while &lt;code&gt;#deliver_messages&lt;/code&gt; is sending messages to Kafka, possibly for tens of seconds or even minutes at a time, depending on your timeout and retry settings. Furthermore, you have to call &lt;code&gt;#deliver_messages&lt;/code&gt; manually, with a frequency that balances batch size with message delay.&lt;/p&gt; &#xA;&lt;p&gt;In order to avoid blocking during message deliveries you can use the &lt;em&gt;asynchronous producer&lt;/em&gt; API. It is mostly similar to the synchronous API, with calls to &lt;code&gt;#produce&lt;/code&gt; and &lt;code&gt;#deliver_messages&lt;/code&gt;. The main difference is that rather than blocking, these calls will return immediately. The actual work will be done in a background thread, with the messages and operations being sent from the caller over a thread safe queue.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# `#async_producer` will create a new asynchronous producer.&#xA;producer = kafka.async_producer&#xA;&#xA;# The `#produce` API works as normal.&#xA;producer.produce(&#34;hello&#34;, topic: &#34;greetings&#34;)&#xA;&#xA;# `#deliver_messages` will return immediately.&#xA;producer.deliver_messages&#xA;&#xA;# Make sure to call `#shutdown` on the producer in order to avoid leaking&#xA;# resources. `#shutdown` will wait for any pending messages to be delivered&#xA;# before returning.&#xA;producer.shutdown&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the delivery policy will be the same as for a synchronous producer: only when &lt;code&gt;#deliver_messages&lt;/code&gt; is called will the messages be delivered. However, the asynchronous producer offers two complementary policies for &lt;em&gt;automatic delivery&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Trigger a delivery once the producer&#39;s message buffer reaches a specified &lt;em&gt;threshold&lt;/em&gt;. This can be used to improve efficiency by increasing the batch size when sending messages to the Kafka cluster.&lt;/li&gt; &#xA; &lt;li&gt;Trigger a delivery at a &lt;em&gt;fixed time interval&lt;/em&gt;. This puts an upper bound on message delays.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;These policies can be used alone or in combination.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# `async_producer` will create a new asynchronous producer.&#xA;producer = kafka.async_producer(&#xA;  # Trigger a delivery once 100 messages have been buffered.&#xA;  delivery_threshold: 100,&#xA;&#xA;  # Trigger a delivery every 30 seconds.&#xA;  delivery_interval: 30,&#xA;)&#xA;&#xA;producer.produce(&#34;hello&#34;, topic: &#34;greetings&#34;)&#xA;&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When calling &lt;code&gt;#shutdown&lt;/code&gt;, the producer will attempt to deliver the messages and the method call will block until that has happened. Note that there&#39;s no &lt;em&gt;guarantee&lt;/em&gt; that the messages will be delivered.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; if the calling thread produces messages faster than the producer can write them to Kafka, you&#39;ll eventually run into problems. The internal queue used for sending messages from the calling thread to the background worker has a size limit; once this limit is reached, a call to &lt;code&gt;#produce&lt;/code&gt; will raise &lt;code&gt;Kafka::BufferOverflow&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Serialization&lt;/h4&gt; &#xA;&lt;p&gt;This library is agnostic to which serialization format you prefer. Both the value and key of a message is treated as a binary string of data. This makes it easier to use whatever serialization format you want, since you don&#39;t have to do anything special to make it work with ruby-kafka. Here&#39;s an example of encoding data with JSON:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#34;json&#34;&#xA;&#xA;# ...&#xA;&#xA;event = {&#xA;  &#34;name&#34; =&amp;gt; &#34;pageview&#34;,&#xA;  &#34;url&#34; =&amp;gt; &#34;https://example.com/posts/123&#34;,&#xA;  # ...&#xA;}&#xA;&#xA;data = JSON.dump(event)&#xA;&#xA;producer.produce(data, topic: &#34;events&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There&#39;s also an example of &lt;a href=&#34;https://github.com/zendesk/ruby-kafka/wiki/Encoding-messages-with-Avro&#34;&gt;encoding messages with Apache Avro&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Partitioning&lt;/h4&gt; &#xA;&lt;p&gt;Kafka topics are partitioned, with messages being assigned to a partition by the client. This allows a great deal of flexibility for the users. This section describes several strategies for partitioning and how they impact performance, data locality, etc.&lt;/p&gt; &#xA;&lt;h5&gt;Load Balanced Partitioning&lt;/h5&gt; &#xA;&lt;p&gt;When optimizing for efficiency, we either distribute messages as evenly as possible to all partitions, or make sure each producer always writes to a single partition. The former ensures an even load for downstream consumers; the latter ensures the highest producer performance, since message batching is done per partition.&lt;/p&gt; &#xA;&lt;p&gt;If no explicit partition is specified, the producer will look to the partition key or the message key for a value that can be used to deterministically assign the message to a partition. If there is a big number of different keys, the resulting distribution will be pretty even. If no keys are passed, the producer will randomly assign a partition. Random partitioning can be achieved even if you use message keys by passing a random partition key, e.g. &lt;code&gt;partition_key: rand(100)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you wish to have the producer write all messages to a single partition, simply generate a random value and re-use that as the partition key:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;partition_key = rand(100)&#xA;&#xA;producer.produce(msg1, topic: &#34;messages&#34;, partition_key: partition_key)&#xA;producer.produce(msg2, topic: &#34;messages&#34;, partition_key: partition_key)&#xA;&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also base the partition key on some property of the producer, for example the host name.&lt;/p&gt; &#xA;&lt;h5&gt;Semantic Partitioning&lt;/h5&gt; &#xA;&lt;p&gt;By assigning messages to a partition based on some property of the message, e.g. making sure all events tracked in a user session are assigned to the same partition, downstream consumers can make simplifying assumptions about data locality. In this example, a consumer can keep process local state pertaining to a user session knowing that all events for the session will be read from a single partition. This is also called &lt;em&gt;semantic partitioning&lt;/em&gt;, since the partition assignment is part of the application behavior.&lt;/p&gt; &#xA;&lt;p&gt;Typically it&#39;s sufficient to simply pass a partition key in order to guarantee that a set of messages will be assigned to the same partition, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# All messages with the same `session_id` will be assigned to the same partition.&#xA;producer.produce(event, topic: &#34;user-events&#34;, partition_key: session_id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, sometimes it&#39;s necessary to select a specific partition. When doing this, make sure that you don&#39;t pick a partition number outside the range of partitions for the topic:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;partitions = kafka.partitions_for(&#34;events&#34;)&#xA;&#xA;# Make sure that we don&#39;t exceed the partition count!&#xA;partition = some_number % partitions&#xA;&#xA;producer.produce(event, topic: &#34;events&#34;, partition: partition)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Compatibility with Other Clients&lt;/h5&gt; &#xA;&lt;p&gt;There&#39;s no standardized way to assign messages to partitions across different Kafka client implementations. If you have a heterogeneous set of clients producing messages to the same topics it may be important to ensure a consistent partitioning scheme. This library doesn&#39;t try to implement all schemes, so you&#39;ll have to figure out which scheme the other client is using and replicate it. An example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;partitions = kafka.partitions_for(&#34;events&#34;)&#xA;&#xA;# Insert your custom partitioning scheme here:&#xA;partition = PartitioningScheme.assign(partitions, event)&#xA;&#xA;producer.produce(event, topic: &#34;events&#34;, partition: partition)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Another option is to configure a custom client partitioner that implements &lt;code&gt;call(partition_count, message)&lt;/code&gt; and uses the same schema as the other client. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class CustomPartitioner&#xA;  def call(partition_count, message)&#xA;    ...&#xA;  end&#xA;end&#xA;  &#xA;partitioner = CustomPartitioner.new&#xA;Kafka.new(partitioner: partitioner, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, simply create a Proc handling the partitioning logic instead of having to add a new class. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;partitioner = -&amp;gt; (partition_count, message) { ... }&#xA;Kafka.new(partitioner: partitioner, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Supported partitioning schemes&lt;/h5&gt; &#xA;&lt;p&gt;In order for semantic partitioning to work a &lt;code&gt;partition_key&lt;/code&gt; must map to the same partition number every time. The general approach, and the one used by this library, is to hash the key and mod it by the number of partitions. There are many different algorithms that can be used to calculate a hash. By default &lt;code&gt;crc32&lt;/code&gt; is used. &lt;code&gt;murmur2&lt;/code&gt; is also supported for compatibility with Java based Kafka producers.&lt;/p&gt; &#xA;&lt;p&gt;To use &lt;code&gt;murmur2&lt;/code&gt; hashing pass it as an argument to &lt;code&gt;Partitioner&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Kafka.new(partitioner: Kafka::Partitioner.new(hash_function: :murmur2))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Buffering and Error Handling&lt;/h4&gt; &#xA;&lt;p&gt;The producer is designed for resilience in the face of temporary network errors, Kafka broker failovers, and other issues that prevent the client from writing messages to the destination topics. It does this by employing local, in-memory buffers. Only when messages are acknowledged by a Kafka broker will they be removed from the buffer.&lt;/p&gt; &#xA;&lt;p&gt;Typically, you&#39;d configure the producer to retry failed attempts at sending messages, but sometimes all retries are exhausted. In that case, &lt;code&gt;Kafka::DeliveryFailed&lt;/code&gt; is raised from &lt;code&gt;Kafka::Producer#deliver_messages&lt;/code&gt;. If you wish to have your application be resilient to this happening (e.g. if you&#39;re logging to Kafka from a web application) you can rescue this exception. The failed messages are still retained in the buffer, so a subsequent call to &lt;code&gt;#deliver_messages&lt;/code&gt; will still attempt to send them.&lt;/p&gt; &#xA;&lt;p&gt;Note that there&#39;s a maximum buffer size; by default, it&#39;s set to 1,000 messages and 10MB. It&#39;s possible to configure both these numbers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;producer = kafka.producer(&#xA;  max_buffer_size: 5_000,           # Allow at most 5K messages to be buffered.&#xA;  max_buffer_bytesize: 100_000_000, # Allow at most 100MB to be buffered.&#xA;  ...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A final note on buffers: local buffers give resilience against broker and network failures, and allow higher throughput due to message batching, but they also trade off consistency guarantees for higher availability and resilience. If your local process dies while messages are buffered, those messages will be lost. If you require high levels of consistency, you should call &lt;code&gt;#deliver_messages&lt;/code&gt; immediately after &lt;code&gt;#produce&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Message Durability&lt;/h4&gt; &#xA;&lt;p&gt;Once the client has delivered a set of messages to a Kafka broker the broker will forward them to its replicas, thus ensuring that a single broker failure will not result in message loss. However, the client can choose &lt;em&gt;when the leader acknowledges the write&lt;/em&gt;. At one extreme, the client can choose fire-and-forget delivery, not even bothering to check whether the messages have been acknowledged. At the other end, the client can ask the broker to wait until &lt;em&gt;all&lt;/em&gt; its replicas have acknowledged the write before returning. This is the safest option, and the default. It&#39;s also possible to have the broker return as soon as it has written the messages to its own log but before the replicas have done so. This leaves a window of time where a failure of the leader will result in the messages being lost, although this should not be a common occurrence.&lt;/p&gt; &#xA;&lt;p&gt;Write latency and throughput are negatively impacted by having more replicas acknowledge a write, so if you require low-latency, high throughput writes you may want to accept lower durability.&lt;/p&gt; &#xA;&lt;p&gt;This behavior is controlled by the &lt;code&gt;required_acks&lt;/code&gt; option to &lt;code&gt;#producer&lt;/code&gt; and &lt;code&gt;#async_producer&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# This is the default: all replicas must acknowledge.&#xA;producer = kafka.producer(required_acks: :all)&#xA;&#xA;# This is fire-and-forget: messages can easily be lost.&#xA;producer = kafka.producer(required_acks: 0)&#xA;&#xA;# This only waits for the leader to acknowledge.&#xA;producer = kafka.producer(required_acks: 1)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Unless you absolutely need lower latency it&#39;s highly recommended to use the default setting (&lt;code&gt;:all&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Message Delivery Guarantees&lt;/h4&gt; &#xA;&lt;p&gt;There are basically two different and incompatible guarantees that can be made in a message delivery system such as Kafka:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;em&gt;at-most-once&lt;/em&gt; delivery guarantees that a message is at most delivered to the recipient &lt;em&gt;once&lt;/em&gt;. This is useful only if delivering the message twice carries some risk and should be avoided. Implicit is the fact that there&#39;s no guarantee that the message will be delivered at all.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;at-least-once&lt;/em&gt; delivery guarantees that a message is delivered, but it may be delivered more than once. If the final recipient de-duplicates messages, e.g. by checking a unique message id, then it&#39;s even possible to implement &lt;em&gt;exactly-once&lt;/em&gt; delivery.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Of these two options, ruby-kafka implements the second one: when in doubt about whether a message has been delivered, a producer will try to deliver it again.&lt;/p&gt; &#xA;&lt;p&gt;The guarantee is made only for the synchronous producer and boils down to this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;producer = kafka.producer&#xA;&#xA;producer.produce(&#34;hello&#34;, topic: &#34;greetings&#34;)&#xA;&#xA;# If this line fails with Kafka::DeliveryFailed we *may* have succeeded in delivering&#xA;# the message to Kafka but won&#39;t know for sure.&#xA;producer.deliver_messages&#xA;&#xA;# If we get to this line we can be sure that the message has been delivered to Kafka!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That is, once &lt;code&gt;#deliver_messages&lt;/code&gt; returns we can be sure that Kafka has received the message. Note that there are some big caveats here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Depending on how your cluster and topic is configured the message could still be lost by Kafka.&lt;/li&gt; &#xA; &lt;li&gt;If you configure the producer to not require acknowledgements from the Kafka brokers by setting &lt;code&gt;required_acks&lt;/code&gt; to zero there is no guarantee that the message will ever make it to a Kafka broker.&lt;/li&gt; &#xA; &lt;li&gt;If you use the asynchronous producer there&#39;s no guarantee that messages will have been delivered after &lt;code&gt;#deliver_messages&lt;/code&gt; returns. A way of blocking until a message has been delivered with the asynchronous producer may be implemented in the future.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It&#39;s possible to improve your chances of success when calling &lt;code&gt;#deliver_messages&lt;/code&gt;, at the price of a longer max latency:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;producer = kafka.producer(&#xA;  # The number of retries when attempting to deliver messages. The default is&#xA;  # 2, so 3 attempts in total, but you can configure a higher or lower number:&#xA;  max_retries: 5,&#xA;&#xA;  # The number of seconds to wait between retries. In order to handle longer&#xA;  # periods of Kafka being unavailable, increase this number. The default is&#xA;  # 1 second.&#xA;  retry_backoff: 5,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that these values affect the max latency of the operation; see &lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#understanding-timeouts&#34;&gt;Understanding Timeouts&lt;/a&gt; for an explanation of the various timeouts and latencies.&lt;/p&gt; &#xA;&lt;p&gt;If you use the asynchronous producer you typically don&#39;t have to worry too much about this, as retries will be done in the background.&lt;/p&gt; &#xA;&lt;h4&gt;Compression&lt;/h4&gt; &#xA;&lt;p&gt;Depending on what kind of data you produce, enabling compression may yield improved bandwidth and space usage. Compression in Kafka is done on entire messages sets rather than on individual messages. This improves the compression rate and generally means that compressions works better the larger your buffers get, since the message sets will be larger by the time they&#39;re compressed.&lt;/p&gt; &#xA;&lt;p&gt;Since many workloads have variations in throughput and distribution across partitions, it&#39;s possible to configure a threshold for when to enable compression by setting &lt;code&gt;compression_threshold&lt;/code&gt;. Only if the defined number of messages are buffered for a partition will the messages be compressed.&lt;/p&gt; &#xA;&lt;p&gt;Compression is enabled by passing the &lt;code&gt;compression_codec&lt;/code&gt; parameter to &lt;code&gt;#producer&lt;/code&gt; with the name of one of the algorithms allowed by Kafka:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;:snappy&lt;/code&gt; for &lt;a href=&#34;http://google.github.io/snappy/&#34;&gt;Snappy&lt;/a&gt; compression.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;:gzip&lt;/code&gt; for &lt;a href=&#34;https://en.wikipedia.org/wiki/Gzip&#34;&gt;gzip&lt;/a&gt; compression.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;:lz4&lt;/code&gt; for &lt;a href=&#34;https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)&#34;&gt;LZ4&lt;/a&gt; compression.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;:zstd&lt;/code&gt; for &lt;a href=&#34;https://facebook.github.io/zstd/&#34;&gt;zstd&lt;/a&gt; compression.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, all message sets will be compressed if you specify a compression codec. To increase the compression threshold, set &lt;code&gt;compression_threshold&lt;/code&gt; to an integer value higher than one.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;producer = kafka.producer(&#xA;  compression_codec: :snappy,&#xA;  compression_threshold: 10,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Producing Messages from a Rails Application&lt;/h4&gt; &#xA;&lt;p&gt;A typical use case for Kafka is tracking events that occur in web applications. Oftentimes it&#39;s advisable to avoid having a hard dependency on Kafka being available, allowing your application to survive a Kafka outage. By using an asynchronous producer, you can avoid doing IO within the individual request/response cycles, instead pushing that to the producer&#39;s internal background thread.&lt;/p&gt; &#xA;&lt;p&gt;In this example, a producer is configured in a Rails initializer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# config/initializers/kafka_producer.rb&#xA;require &#34;kafka&#34;&#xA;&#xA;# Configure the Kafka client with the broker hosts and the Rails&#xA;# logger.&#xA;$kafka = Kafka.new([&#34;kafka1:9092&#34;, &#34;kafka2:9092&#34;], logger: Rails.logger)&#xA;&#xA;# Set up an asynchronous producer that delivers its buffered messages&#xA;# every ten seconds:&#xA;$kafka_producer = $kafka.async_producer(&#xA;  delivery_interval: 10,&#xA;)&#xA;&#xA;# Make sure to shut down the producer when exiting.&#xA;at_exit { $kafka_producer.shutdown }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In your controllers, simply call the producer directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# app/controllers/orders_controller.rb&#xA;class OrdersController&#xA;  def create&#xA;    @order = Order.create!(params[:order])&#xA;&#xA;    event = {&#xA;      order_id: @order.id,&#xA;      amount: @order.amount,&#xA;      timestamp: Time.now,&#xA;    }&#xA;&#xA;    $kafka_producer.produce(event.to_json, topic: &#34;order_events&#34;)&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Consuming Messages from Kafka&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you&#39;re just looking to get started with Kafka consumers, you might be interested in visiting the &lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#higher-level-libraries&#34;&gt;Higher level libraries&lt;/a&gt; section that lists ruby-kafka based frameworks. Read on, if you&#39;re interested in either rolling your own executable consumers or if you want to learn more about how consumers work in Kafka.&lt;/p&gt; &#xA;&lt;p&gt;Consuming messages from a Kafka topic with ruby-kafka is simple:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#34;kafka&#34;&#xA;&#xA;kafka = Kafka.new([&#34;kafka1:9092&#34;, &#34;kafka2:9092&#34;])&#xA;&#xA;kafka.each_message(topic: &#34;greetings&#34;) do |message|&#xA;  puts message.offset, message.key, message.value&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;While this is great for extremely simple use cases, there are a number of downsides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can only fetch from a single topic at a time.&lt;/li&gt; &#xA; &lt;li&gt;If you want to have multiple processes consume from the same topic, there&#39;s no way of coordinating which processes should fetch from which partitions.&lt;/li&gt; &#xA; &lt;li&gt;If the process dies, there&#39;s no way to have another process resume fetching from the point in the partition that the original process had reached.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Consumer Groups&lt;/h4&gt; &#xA;&lt;p&gt;The Consumer API solves all of the above issues, and more. It uses the Consumer Groups feature released in Kafka 0.9 to allow multiple consumer processes to coordinate access to a topic, assigning each partition to a single consumer. When a consumer fails, the partitions that were assigned to it are re-assigned to other members of the group.&lt;/p&gt; &#xA;&lt;p&gt;Using the API is simple:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#34;kafka&#34;&#xA;&#xA;kafka = Kafka.new([&#34;kafka1:9092&#34;, &#34;kafka2:9092&#34;])&#xA;&#xA;# Consumers with the same group id will form a Consumer Group together.&#xA;consumer = kafka.consumer(group_id: &#34;my-consumer&#34;)&#xA;&#xA;# It&#39;s possible to subscribe to multiple topics by calling `subscribe`&#xA;# repeatedly.&#xA;consumer.subscribe(&#34;greetings&#34;)&#xA;&#xA;# Stop the consumer when the SIGTERM signal is sent to the process.&#xA;# It&#39;s better to shut down gracefully than to kill the process.&#xA;trap(&#34;TERM&#34;) { consumer.stop }&#xA;&#xA;# This will loop indefinitely, yielding each message in turn.&#xA;consumer.each_message do |message|&#xA;  puts message.topic, message.partition&#xA;  puts message.offset, message.key, message.value&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Each consumer process will be assigned one or more partitions from each topic that the group subscribes to. In order to handle more messages, simply start more processes.&lt;/p&gt; &#xA;&lt;h4&gt;Consumer Checkpointing&lt;/h4&gt; &#xA;&lt;p&gt;In order to be able to resume processing after a consumer crashes, each consumer will periodically &lt;em&gt;checkpoint&lt;/em&gt; its position within each partition it reads from. Since each partition has a monotonically increasing sequence of message offsets, this works by &lt;em&gt;committing&lt;/em&gt; the offset of the last message that was processed in a given partition. Kafka handles these commits and allows another consumer in a group to resume from the last commit when a member crashes or becomes unresponsive.&lt;/p&gt; &#xA;&lt;p&gt;By default, offsets are committed every 10 seconds. You can increase the frequency, known as the &lt;em&gt;offset commit interval&lt;/em&gt;, to limit the duration of double-processing scenarios, at the cost of a lower throughput due to the added coordination. If you want to improve throughput, and double-processing is of less concern to you, then you can decrease the frequency. Set the commit interval to zero in order to disable the timer-based commit trigger entirely.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the time based trigger it&#39;s possible to trigger checkpointing in response to &lt;em&gt;n&lt;/em&gt; messages having been processed, known as the &lt;em&gt;offset commit threshold&lt;/em&gt;. This puts a bound on the number of messages that can be double-processed before the problem is detected. Setting this to 1 will cause an offset commit to take place every time a message has been processed. By default this trigger is disabled (set to zero).&lt;/p&gt; &#xA;&lt;p&gt;It is possible to trigger an immediate offset commit by calling &lt;code&gt;Consumer#commit_offsets&lt;/code&gt;. This blocks the caller until the Kafka cluster has acknowledged the commit.&lt;/p&gt; &#xA;&lt;p&gt;Stale offsets are periodically purged by the broker. The broker setting &lt;code&gt;offsets.retention.minutes&lt;/code&gt; controls the retention window for committed offsets, and defaults to 1 day. The length of the retention window, known as &lt;em&gt;offset retention time&lt;/em&gt;, can be changed for the consumer.&lt;/p&gt; &#xA;&lt;p&gt;Previously committed offsets are re-committed, to reset the retention window, at the first commit and periodically at an interval of half the &lt;em&gt;offset retention time&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;consumer = kafka.consumer(&#xA;  group_id: &#34;some-group&#34;,&#xA;&#xA;  # Increase offset commit frequency to once every 5 seconds.&#xA;  offset_commit_interval: 5,&#xA;&#xA;  # Commit offsets when 100 messages have been processed.&#xA;  offset_commit_threshold: 100,&#xA;&#xA;  # Increase the length of time that committed offsets are kept.&#xA;  offset_retention_time: 7 * 60 * 60&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For some use cases it may be necessary to control when messages are marked as processed. Note that since only the consumer position within each partition can be saved, marking a message as processed implies that all messages in the partition with a lower offset should also be considered as having been processed.&lt;/p&gt; &#xA;&lt;p&gt;The method &lt;code&gt;Consumer#mark_message_as_processed&lt;/code&gt; marks a message (and all those that precede it in a partition) as having been processed. This is an advanced API that you should only use if you know what you&#39;re doing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# Manually controlling checkpointing:&#xA;&#xA;# Typically you want to use this API in order to buffer messages until some&#xA;# special &#34;commit&#34; message is received, e.g. in order to group together&#xA;# transactions consisting of several items.&#xA;buffer = []&#xA;&#xA;# Messages will not be marked as processed automatically. If you shut down the&#xA;# consumer without calling `#mark_message_as_processed` first, the consumer will&#xA;# not resume where you left off!&#xA;consumer.each_message(automatically_mark_as_processed: false) do |message|&#xA;  # Our messages are JSON with a `type` field and other stuff.&#xA;  event = JSON.parse(message.value)&#xA;&#xA;  case event.fetch(&#34;type&#34;)&#xA;  when &#34;add_to_cart&#34;&#xA;    buffer &amp;lt;&amp;lt; event&#xA;  when &#34;complete_purchase&#34;&#xA;    # We&#39;ve received all the messages we need, time to save the transaction.&#xA;    save_transaction(buffer)&#xA;&#xA;    # Now we can set the checkpoint by marking the last message as processed.&#xA;    consumer.mark_message_as_processed(message)&#xA;&#xA;    # We can optionally trigger an immediate, blocking offset commit in order&#xA;    # to minimize the risk of crashing before the automatic triggers have&#xA;    # kicked in.&#xA;    consumer.commit_offsets&#xA;&#xA;    # Make the buffer ready for the next transaction.&#xA;    buffer.clear&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Topic Subscriptions&lt;/h4&gt; &#xA;&lt;p&gt;For each topic subscription it&#39;s possible to decide whether to consume messages starting at the beginning of the topic or to just consume new messages that are produced to the topic. This policy is configured by setting the &lt;code&gt;start_from_beginning&lt;/code&gt; argument when calling &lt;code&gt;#subscribe&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# Consume messages from the very beginning of the topic. This is the default.&#xA;consumer.subscribe(&#34;users&#34;, start_from_beginning: true)&#xA;&#xA;# Only consume new messages.&#xA;consumer.subscribe(&#34;notifications&#34;, start_from_beginning: false)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the consumer group has checkpointed its progress in the topic&#39;s partitions, the consumers will always start from the checkpointed offsets, regardless of &lt;code&gt;start_from_beginning&lt;/code&gt;. As such, this setting only applies when the consumer initially starts consuming from a topic.&lt;/p&gt; &#xA;&lt;h4&gt;Shutting Down a Consumer&lt;/h4&gt; &#xA;&lt;p&gt;In order to shut down a running consumer process cleanly, call &lt;code&gt;#stop&lt;/code&gt; on it. A common pattern is to trap a process signal and initiate the shutdown from there:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;consumer = kafka.consumer(...)&#xA;&#xA;# The consumer can be stopped from the command line by executing&#xA;# `kill -s TERM &amp;lt;process-id&amp;gt;`.&#xA;trap(&#34;TERM&#34;) { consumer.stop }&#xA;&#xA;consumer.each_message do |message|&#xA;  ...&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Consuming Messages in Batches&lt;/h4&gt; &#xA;&lt;p&gt;Sometimes it is easier to deal with messages in batches rather than individually. A &lt;em&gt;batch&lt;/em&gt; is a sequence of one or more Kafka messages that all belong to the same topic and partition. One common reason to want to use batches is when some external system has a batch or transactional API.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# A mock search index that we&#39;ll be keeping up to date with new Kafka messages.&#xA;index = SearchIndex.new&#xA;&#xA;consumer.subscribe(&#34;posts&#34;)&#xA;&#xA;consumer.each_batch do |batch|&#xA;  puts &#34;Received batch: #{batch.topic}/#{batch.partition}&#34;&#xA;&#xA;  transaction = index.transaction&#xA;&#xA;  batch.messages.each do |message|&#xA;    # Let&#39;s assume that adding a document is idempotent.&#xA;    transaction.add(id: message.key, body: message.value)&#xA;  end&#xA;&#xA;  # Once this method returns, the messages have been successfully written to the&#xA;  # search index. The consumer will only checkpoint a batch *after* the block&#xA;  # has completed without an exception.&#xA;  transaction.commit!&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;One important thing to note is that the client commits the offset of the batch&#39;s messages only after the &lt;em&gt;entire&lt;/em&gt; batch has been processed.&lt;/p&gt; &#xA;&lt;h4&gt;Balancing Throughput and Latency&lt;/h4&gt; &#xA;&lt;p&gt;There are two performance properties that can at times be at odds: &lt;em&gt;throughput&lt;/em&gt; and &lt;em&gt;latency&lt;/em&gt;. Throughput is the number of messages that can be processed in a given timespan; latency is the time it takes from a message is written to a topic until it has been processed.&lt;/p&gt; &#xA;&lt;p&gt;In order to optimize for throughput, you want to make sure to fetch as many messages as possible every time you do a round trip to the Kafka cluster. This minimizes network overhead and allows processing data in big chunks.&lt;/p&gt; &#xA;&lt;p&gt;In order to optimize for low latency, you want to process a message as soon as possible, even if that means fetching a smaller batch of messages.&lt;/p&gt; &#xA;&lt;p&gt;There are three values that can be tuned in order to balance these two concerns.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;min_bytes&lt;/code&gt; is the minimum number of bytes to return from a single message fetch. By setting this to a high value you can increase the processing throughput. The default value is one byte.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_wait_time&lt;/code&gt; is the maximum number of seconds to wait before returning data from a single message fetch. By setting this high you also increase the processing throughput ‚Äì and by setting it low you set a bound on latency. This configuration overrides &lt;code&gt;min_bytes&lt;/code&gt;, so you&#39;ll &lt;em&gt;always&lt;/em&gt; get data back within the time specified. The default value is one second. If you want to have at most five seconds of latency, set &lt;code&gt;max_wait_time&lt;/code&gt; to 5. You should make sure &lt;code&gt;max_wait_time&lt;/code&gt; * num brokers + &lt;code&gt;heartbeat_interval&lt;/code&gt; is less than &lt;code&gt;session_timeout&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_bytes_per_partition&lt;/code&gt; is the maximum amount of data a broker will return for a single partition when fetching new messages. The default is 1MB, but increasing this number may lead to better throughtput since you&#39;ll need to fetch less frequently. Setting it to a lower value is not recommended unless you have so many partitions that it&#39;s causing network and latency issues to transfer a fetch response from a broker to a client. Setting the number too high may result in instability, so be careful.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The first two settings can be passed to either &lt;code&gt;#each_message&lt;/code&gt; or &lt;code&gt;#each_batch&lt;/code&gt;, e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# Waits for data for up to 5 seconds on each broker, preferring to fetch at least 5KB at a time.&#xA;# This can wait up to num brokers * 5 seconds.&#xA;consumer.each_message(min_bytes: 1024 * 5, max_wait_time: 5) do |message|&#xA;  # ...&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The last setting is configured when subscribing to a topic, and can vary between topics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# Fetches up to 5MB per partition at a time for better throughput.&#xA;consumer.subscribe(&#34;greetings&#34;, max_bytes_per_partition: 5 * 1024 * 1024)&#xA;&#xA;consumer.each_message do |message|&#xA;  # ...&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Customizing Partition Assignment Strategy&lt;/h4&gt; &#xA;&lt;p&gt;In some cases, you might want to assign more partitions to some consumers. For example, in applications inserting some records to a database, the consumers running on hosts nearby the database can process more messages than the consumers running on other hosts. You can use a custom assignment strategy by passing an object that implements &lt;code&gt;#call&lt;/code&gt; as the argument &lt;code&gt;assignment_strategy&lt;/code&gt; like below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class CustomAssignmentStrategy&#xA;  def initialize(user_data)&#xA;    @user_data = user_data&#xA;  end&#xA;&#xA;  # Assign the topic partitions to the group members.&#xA;  #&#xA;  # @param cluster [Kafka::Cluster]&#xA;  # @param members [Hash&amp;lt;String, Kafka::Protocol::JoinGroupResponse::Metadata&amp;gt;] a hash&#xA;  #   mapping member ids to metadata&#xA;  # @param partitions [Array&amp;lt;Kafka::ConsumerGroup::Assignor::Partition&amp;gt;] a list of&#xA;  #   partitions the consumer group processes&#xA;  # @return [Hash&amp;lt;String, Array&amp;lt;Kafka::ConsumerGroup::Assignor::Partition&amp;gt;] a hash&#xA;  #   mapping member ids to partitions.&#xA;  def call(cluster:, members:, partitions:)&#xA;    ...&#xA;  end&#xA;end&#xA;&#xA;strategy = CustomAssignmentStrategy.new(&#34;some-host-information&#34;)&#xA;consumer = kafka.consumer(group_id: &#34;some-group&#34;, assignment_strategy: strategy)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;members&lt;/code&gt; is a hash mapping member IDs to metadata, and partitions is a list of partitions the consumer group processes. The method &lt;code&gt;call&lt;/code&gt; must return a hash mapping member IDs to partitions. For example, the following strategy assigns partitions randomly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class RandomAssignmentStrategy&#xA;  def call(cluster:, members:, partitions:)&#xA;    member_ids = members.keys&#xA;    partitions.each_with_object(Hash.new {|h, k| h[k] = [] }) do |partition, partitions_per_member|&#xA;      partitions_per_member[member_ids[rand(member_ids.count)]] &amp;lt;&amp;lt; partition&#xA;    end&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the strategy needs user data, you should define the method &lt;code&gt;user_data&lt;/code&gt; that returns user data on each consumer. For example, the following strategy uses the consumers&#39; IP addresses as user data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class NetworkTopologyAssignmentStrategy&#xA;  def user_data&#xA;    Socket.ip_address_list.find(&amp;amp;:ipv4_private?).ip_address&#xA;  end&#xA;&#xA;  def call(cluster:, members:, partitions:)&#xA;    # Display the pair of the member ID and IP address&#xA;    members.each do |id, metadata|&#xA;      puts &#34;#{id}: #{metadata.user_data}&#34;&#xA;    end&#xA;&#xA;    # Assign partitions considering the network topology&#xA;    ...&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the strategy uses the class name as the default protocol name. You can change it by defining the method &lt;code&gt;protocol_name&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class NetworkTopologyAssignmentStrategy&#xA;  def protocol_name&#xA;    &#34;networktopology&#34;&#xA;  end&#xA;&#xA;  def user_data&#xA;    Socket.ip_address_list.find(&amp;amp;:ipv4_private?).ip_address&#xA;  end&#xA;&#xA;  def call(cluster:, members:, partitions:)&#xA;    ...&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As the method &lt;code&gt;call&lt;/code&gt; might receive different user data from what it expects, you should avoid using the same protocol name as another strategy that uses different user data.&lt;/p&gt; &#xA;&lt;h3&gt;Thread Safety&lt;/h3&gt; &#xA;&lt;p&gt;You typically don&#39;t want to share a Kafka client object between threads, since the network communication is not synchronized. Furthermore, you should avoid using threads in a consumer unless you&#39;re very careful about waiting for all work to complete before returning from the &lt;code&gt;#each_message&lt;/code&gt; or &lt;code&gt;#each_batch&lt;/code&gt; block. This is because &lt;em&gt;checkpointing&lt;/em&gt; assumes that returning from the block means that the messages that have been yielded have been successfully processed.&lt;/p&gt; &#xA;&lt;p&gt;You should also avoid sharing a synchronous producer between threads, as the internal buffers are not thread safe. However, the &lt;em&gt;asynchronous&lt;/em&gt; producer should be safe to use in a multi-threaded environment. This is because producers, when instantiated, get their own copy of any non-thread-safe data such as network sockets. Furthermore, the asynchronous producer has been designed in such a way to only a single background thread operates on this data while any foreground thread with a reference to the producer object can only send messages to that background thread over a safe queue. Therefore it is safe to share an async producer object between many threads.&lt;/p&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s a very good idea to configure the Kafka client with a logger. All important operations and errors are logged. When instantiating your client, simply pass in a valid logger:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;logger = Logger.new(&#34;log/kafka.log&#34;)&#xA;kafka = Kafka.new(logger: logger, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, nothing is logged.&lt;/p&gt; &#xA;&lt;h3&gt;Instrumentation&lt;/h3&gt; &#xA;&lt;p&gt;Most operations are instrumented using &lt;a href=&#34;http://api.rubyonrails.org/classes/ActiveSupport/Notifications.html&#34;&gt;Active Support Notifications&lt;/a&gt;. In order to subscribe to notifications, make sure to require the notifications library:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#34;active_support/notifications&#34;&#xA;require &#34;kafka&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The notifications are namespaced based on their origin, with separate namespaces for the producer and the consumer.&lt;/p&gt; &#xA;&lt;p&gt;In order to receive notifications you can either subscribe to individual notification names or use regular expressions to subscribe to entire namespaces. This example will subscribe to &lt;em&gt;all&lt;/em&gt; notifications sent by ruby-kafka:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;ActiveSupport::Notifications.subscribe(/.*\.kafka$/) do |*args|&#xA;  event = ActiveSupport::Notifications::Event.new(*args)&#xA;  puts &#34;Received notification `#{event.name}` with payload: #{event.payload.inspect}&#34;&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All notification events have the &lt;code&gt;client_id&lt;/code&gt; key in the payload, referring to the Kafka client id.&lt;/p&gt; &#xA;&lt;h4&gt;Producer Notifications&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;produce_message.producer.kafka&lt;/code&gt; is sent whenever a message is produced to a buffer. It includes the following payload:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;value&lt;/code&gt; is the message value.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;key&lt;/code&gt; is the message key.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;topic&lt;/code&gt; is the topic that the message was produced to.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;buffer_size&lt;/code&gt; is the size of the producer buffer after adding the message.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;max_buffer_size&lt;/code&gt; is the maximum size of the producer buffer.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;deliver_messages.producer.kafka&lt;/code&gt; is sent whenever a producer attempts to deliver its buffered messages to the Kafka brokers. It includes the following payload:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;attempts&lt;/code&gt; is the number of times delivery was attempted.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;message_count&lt;/code&gt; is the number of messages for which delivery was attempted.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;delivered_message_count&lt;/code&gt; is the number of messages that were acknowledged by the brokers - if this number is smaller than &lt;code&gt;message_count&lt;/code&gt; not all messages were successfully delivered.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Consumer Notifications&lt;/h4&gt; &#xA;&lt;p&gt;All notifications have &lt;code&gt;group_id&lt;/code&gt; in the payload, referring to the Kafka consumer group id.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;process_message.consumer.kafka&lt;/code&gt; is sent whenever a message is processed by a consumer. It includes the following payload:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;value&lt;/code&gt; is the message value.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;key&lt;/code&gt; is the message key.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;topic&lt;/code&gt; is the topic that the message was consumed from.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;partition&lt;/code&gt; is the topic partition that the message was consumed from.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;offset&lt;/code&gt; is the message&#39;s offset within the topic partition.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;offset_lag&lt;/code&gt; is the number of messages within the topic partition that have not yet been consumed.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;start_process_message.consumer.kafka&lt;/code&gt; is sent before &lt;code&gt;process_message.consumer.kafka&lt;/code&gt;, and contains the same payload. It is delivered &lt;em&gt;before&lt;/em&gt; the message is processed, rather than &lt;em&gt;after&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;process_batch.consumer.kafka&lt;/code&gt; is sent whenever a message batch is processed by a consumer. It includes the following payload:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;message_count&lt;/code&gt; is the number of messages in the batch.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;topic&lt;/code&gt; is the topic that the message batch was consumed from.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;partition&lt;/code&gt; is the topic partition that the message batch was consumed from.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;highwater_mark_offset&lt;/code&gt; is the message batch&#39;s highest offset within the topic partition.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;offset_lag&lt;/code&gt; is the number of messages within the topic partition that have not yet been consumed.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;start_process_batch.consumer.kafka&lt;/code&gt; is sent before &lt;code&gt;process_batch.consumer.kafka&lt;/code&gt;, and contains the same payload. It is delivered &lt;em&gt;before&lt;/em&gt; the batch is processed, rather than &lt;em&gt;after&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;join_group.consumer.kafka&lt;/code&gt; is sent whenever a consumer joins a consumer group. It includes the following payload:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;group_id&lt;/code&gt; is the consumer group id.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;sync_group.consumer.kafka&lt;/code&gt; is sent whenever a consumer is assigned topic partitions within a consumer group. It includes the following payload:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;group_id&lt;/code&gt; is the consumer group id.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;leave_group.consumer.kafka&lt;/code&gt; is sent whenever a consumer leaves a consumer group. It includes the following payload:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;group_id&lt;/code&gt; is the consumer group id.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;seek.consumer.kafka&lt;/code&gt; is sent when a consumer first seeks to an offset. It includes the following payload:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;group_id&lt;/code&gt; is the consumer group id.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;topic&lt;/code&gt; is the topic we are seeking in.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;partition&lt;/code&gt; is the partition we are seeking in.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;offset&lt;/code&gt; is the offset we have seeked to.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;heartbeat.consumer.kafka&lt;/code&gt; is sent when a consumer group completes a heartbeat. It includes the following payload:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;group_id&lt;/code&gt; is the consumer group id.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;topic_partitions&lt;/code&gt; is a hash of { topic_name =&amp;gt; array of assigned partition IDs }&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Connection Notifications&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;request.connection.kafka&lt;/code&gt; is sent whenever a network request is sent to a Kafka broker. It includes the following payload: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;api&lt;/code&gt; is the name of the API that was called, e.g. &lt;code&gt;produce&lt;/code&gt; or &lt;code&gt;fetch&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;request_size&lt;/code&gt; is the number of bytes in the request.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;response_size&lt;/code&gt; is the number of bytes in the response.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Monitoring&lt;/h3&gt; &#xA;&lt;p&gt;It is highly recommended that you monitor your Kafka client applications in production. Typical problems you&#39;ll see are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;high network error rates, which may impact performance and time-to-delivery;&lt;/li&gt; &#xA; &lt;li&gt;producer buffer growth, which may indicate that producers are unable to deliver messages at the rate they&#39;re being produced;&lt;/li&gt; &#xA; &lt;li&gt;consumer processing errors, indicating exceptions are being raised in the processing code;&lt;/li&gt; &#xA; &lt;li&gt;frequent consumer rebalances, which may indicate unstable network conditions or consumer configurations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can quite easily build monitoring on top of the provided &lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/#instrumentation&#34;&gt;instrumentation hooks&lt;/a&gt;. In order to further help with monitoring, a prebuilt &lt;a href=&#34;https://github.com/etsy/statsd&#34;&gt;Statsd&lt;/a&gt; and &lt;a href=&#34;https://www.datadoghq.com/&#34;&gt;Datadog&lt;/a&gt; reporter is included with ruby-kafka.&lt;/p&gt; &#xA;&lt;h4&gt;What to Monitor&lt;/h4&gt; &#xA;&lt;p&gt;We recommend monitoring the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Low-level Kafka API calls: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The rate of API call errors to the total number of calls by both API and broker.&lt;/li&gt; &#xA;   &lt;li&gt;The API call throughput by both API and broker.&lt;/li&gt; &#xA;   &lt;li&gt;The API call latency by both API and broker.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Producer-level metrics: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Delivery throughput by topic.&lt;/li&gt; &#xA;   &lt;li&gt;The latency of deliveries.&lt;/li&gt; &#xA;   &lt;li&gt;The producer buffer fill ratios.&lt;/li&gt; &#xA;   &lt;li&gt;The async producer queue sizes.&lt;/li&gt; &#xA;   &lt;li&gt;Message delivery delays.&lt;/li&gt; &#xA;   &lt;li&gt;Failed delivery attempts.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Consumer-level metrics: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Message processing throughput by topic.&lt;/li&gt; &#xA;   &lt;li&gt;Processing latency by topic.&lt;/li&gt; &#xA;   &lt;li&gt;Processing errors by topic.&lt;/li&gt; &#xA;   &lt;li&gt;Consumer lag (how many messages are yet to be processed) by topic/partition.&lt;/li&gt; &#xA;   &lt;li&gt;Group join/sync/leave by client host.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Reporting Metrics to Statsd&lt;/h4&gt; &#xA;&lt;p&gt;The Statsd reporter is automatically enabled when the &lt;code&gt;kafka/statsd&lt;/code&gt; library is required. You can optionally change the configuration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#34;kafka/statsd&#34;&#xA;&#xA;# Default is &#34;ruby_kafka&#34;.&#xA;Kafka::Statsd.namespace = &#34;custom-namespace&#34;&#xA;&#xA;# Default is &#34;127.0.0.1&#34;.&#xA;Kafka::Statsd.host = &#34;statsd.something.com&#34;&#xA;&#xA;# Default is 8125.&#xA;Kafka::Statsd.port = 1234&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Reporting Metrics to Datadog&lt;/h4&gt; &#xA;&lt;p&gt;The Datadog reporter is automatically enabled when the &lt;code&gt;kafka/datadog&lt;/code&gt; library is required. You can optionally change the configuration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# This enables the reporter:&#xA;require &#34;kafka/datadog&#34;&#xA;&#xA;# Default is &#34;ruby_kafka&#34;.&#xA;Kafka::Datadog.namespace = &#34;custom-namespace&#34;&#xA;&#xA;# Default is &#34;127.0.0.1&#34;.&#xA;Kafka::Datadog.host = &#34;statsd.something.com&#34;&#xA;&#xA;# Default is 8125.&#xA;Kafka::Datadog.port = 1234&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Understanding Timeouts&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s important to understand how timeouts work if you have a latency sensitive application. This library allows configuring timeouts on different levels:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Network timeouts&lt;/strong&gt; apply to network connections to individual Kafka brokers. There are two config keys here, each passed to &lt;code&gt;Kafka.new&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;connect_timeout&lt;/code&gt; sets the number of seconds to wait while connecting to a broker for the first time. When ruby-kafka initializes, it needs to connect to at least one host in &lt;code&gt;seed_brokers&lt;/code&gt; in order to discover the Kafka cluster. Each host is tried until there&#39;s one that works. Usually that means the first one, but if your entire cluster is down, or there&#39;s a network partition, you could wait up to &lt;code&gt;n * connect_timeout&lt;/code&gt; seconds, where &lt;code&gt;n&lt;/code&gt; is the number of seed brokers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;socket_timeout&lt;/code&gt; sets the number of seconds to wait when reading from or writing to a socket connection to a broker. After this timeout expires the connection will be killed. Note that some Kafka operations are by definition long-running, such as waiting for new messages to arrive in a partition, so don&#39;t set this value too low. When configuring timeouts relating to specific Kafka operations, make sure to make them shorter than this one.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Producer timeouts&lt;/strong&gt; can be configured when calling &lt;code&gt;#producer&lt;/code&gt; on a client instance:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;ack_timeout&lt;/code&gt; is a timeout executed by a broker when the client is sending messages to it. It defines the number of seconds the broker should wait for replicas to acknowledge the write before responding to the client with an error. As such, it relates to the &lt;code&gt;required_acks&lt;/code&gt; setting. It should be set lower than &lt;code&gt;socket_timeout&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;retry_backoff&lt;/code&gt; configures the number of seconds to wait after a failed attempt to send messages to a Kafka broker before retrying. The &lt;code&gt;max_retries&lt;/code&gt; setting defines the maximum number of retries to attempt, and so the total duration could be up to &lt;code&gt;max_retries * retry_backoff&lt;/code&gt; seconds. The timeout can be arbitrarily long, and shouldn&#39;t be too short: if a broker goes down its partitions will be handed off to another broker, and that can take tens of seconds.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When sending many messages, it&#39;s likely that the client needs to send some messages to each broker in the cluster. Given &lt;code&gt;n&lt;/code&gt; brokers in the cluster, the total wait time when calling &lt;code&gt;Kafka::Producer#deliver_messages&lt;/code&gt; can be up to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;n * (connect_timeout + socket_timeout + retry_backoff) * max_retries&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure your application can survive being blocked for so long.&lt;/p&gt; &#xA;&lt;h3&gt;Security&lt;/h3&gt; &#xA;&lt;h4&gt;Encryption and Authentication using SSL&lt;/h4&gt; &#xA;&lt;p&gt;By default, communication between Kafka clients and brokers is unencrypted and unauthenticated. Kafka 0.9 added optional support for &lt;a href=&#34;http://kafka.apache.org/documentation.html#security_ssl&#34;&gt;encryption and client authentication and authorization&lt;/a&gt;. There are two layers of security made possible by this:&lt;/p&gt; &#xA;&lt;h5&gt;Encryption of Communication&lt;/h5&gt; &#xA;&lt;p&gt;By enabling SSL encryption you can have some confidence that messages can be sent to Kafka over an untrusted network without being intercepted.&lt;/p&gt; &#xA;&lt;p&gt;In this case you just need to pass a valid CA certificate as a string when configuring your &lt;code&gt;Kafka&lt;/code&gt; client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new([&#34;kafka1:9092&#34;], ssl_ca_cert: File.read(&#39;my_ca_cert.pem&#39;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Without passing the CA certificate to the client it would be impossible to protect against &lt;a href=&#34;https://en.wikipedia.org/wiki/Man-in-the-middle_attack&#34;&gt;man-in-the-middle attacks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Using your system&#39;s CA cert store&lt;/h5&gt; &#xA;&lt;p&gt;If you want to use the CA certs from your system&#39;s default certificate store, you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new([&#34;kafka1:9092&#34;], ssl_ca_certs_from_system: true)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This configures the store to look up CA certificates from the system default certificate store on an as needed basis. The location of the store can usually be determined by: &lt;code&gt;OpenSSL::X509::DEFAULT_CERT_FILE&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h5&gt;Client Authentication&lt;/h5&gt; &#xA;&lt;p&gt;In order to authenticate the client to the cluster, you need to pass in a certificate and key created for the client and trusted by the brokers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: You can disable hostname validation by passing &lt;code&gt;ssl_verify_hostname: false&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new(&#xA;  [&#34;kafka1:9092&#34;],&#xA;  ssl_ca_cert: File.read(&#39;my_ca_cert.pem&#39;),&#xA;  ssl_client_cert: File.read(&#39;my_client_cert.pem&#39;),&#xA;  ssl_client_cert_key: File.read(&#39;my_client_cert_key.pem&#39;),&#xA;  ssl_client_cert_key_password: &#39;my_client_cert_key_password&#39;,&#xA;  ssl_verify_hostname: false,&#xA;  # ...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once client authentication is set up, it is possible to configure the Kafka cluster to &lt;a href=&#34;http://kafka.apache.org/documentation.html#security_authz&#34;&gt;authorize client requests&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Using JKS Certificates&lt;/h5&gt; &#xA;&lt;p&gt;Typically, Kafka certificates come in the JKS format, which isn&#39;t supported by ruby-kafka. There&#39;s &lt;a href=&#34;https://github.com/zendesk/ruby-kafka/wiki/Creating-X509-certificates-from-JKS-format&#34;&gt;a wiki page&lt;/a&gt; that describes how to generate valid X509 certificates from JKS certificates.&lt;/p&gt; &#xA;&lt;h4&gt;Authentication using SASL&lt;/h4&gt; &#xA;&lt;p&gt;Kafka has support for using SASL to authenticate clients. Currently GSSAPI, SCRAM and PLAIN mechanisms are supported by ruby-kafka.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; With SASL for authentication, it is highly recommended to use SSL encryption. The default behavior of ruby-kafka enforces you to use SSL and you need to configure SSL encryption by passing &lt;code&gt;ssl_ca_cert&lt;/code&gt; or enabling &lt;code&gt;ssl_ca_certs_from_system&lt;/code&gt;. However, this strict SSL mode check can be disabled by setting &lt;code&gt;sasl_over_ssl&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt; while initializing the client.&lt;/p&gt; &#xA;&lt;h5&gt;GSSAPI&lt;/h5&gt; &#xA;&lt;p&gt;In order to authenticate using GSSAPI, set your principal and optionally your keytab when initializing the Kafka client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new(&#xA;  [&#34;kafka1:9092&#34;],&#xA;  sasl_gssapi_principal: &#39;kafka/kafka.example.com@EXAMPLE.COM&#39;,&#xA;  sasl_gssapi_keytab: &#39;/etc/keytabs/kafka.keytab&#39;,&#xA;  # ...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;AWS MSK (IAM)&lt;/h5&gt; &#xA;&lt;p&gt;In order to authenticate using IAM w/ an AWS MSK cluster, set your access key, secret key, and region when initializing the Kafka client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;k = Kafka.new(&#xA;  [&#34;kafka1:9092&#34;],&#xA;  sasl_aws_msk_iam_access_key_id: &#39;iam_access_key&#39;,&#xA;  sasl_aws_msk_iam_secret_key_id: &#39;iam_secret_key&#39;,&#xA;  sasl_aws_msk_iam_aws_region: &#39;us-west-2&#39;,&#xA;  ssl_ca_certs_from_system: true,&#xA;  # ...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;PLAIN&lt;/h5&gt; &#xA;&lt;p&gt;In order to authenticate using PLAIN, you must set your username and password when initializing the Kafka client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new(&#xA;  [&#34;kafka1:9092&#34;],&#xA;  ssl_ca_cert: File.read(&#39;/etc/openssl/cert.pem&#39;),&#xA;  sasl_plain_username: &#39;username&#39;,&#xA;  sasl_plain_password: &#39;password&#39;&#xA;  # ...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;SCRAM&lt;/h5&gt; &#xA;&lt;p&gt;Since 0.11 kafka supports &lt;a href=&#34;https://kafka.apache.org/documentation.html#security_sasl_scram&#34;&gt;SCRAM&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new(&#xA;  [&#34;kafka1:9092&#34;],&#xA;  sasl_scram_username: &#39;username&#39;,&#xA;  sasl_scram_password: &#39;password&#39;,&#xA;  sasl_scram_mechanism: &#39;sha256&#39;,&#xA;  # ...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;OAUTHBEARER&lt;/h5&gt; &#xA;&lt;p&gt;This mechanism is supported in kafka &amp;gt;= 2.0.0 as of &lt;a href=&#34;https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=75968876&#34;&gt;KIP-255&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;In order to authenticate using OAUTHBEARER, you must set the client with an instance of a class that implements a &lt;code&gt;token&lt;/code&gt; method (the interface is described in &lt;a href=&#34;https://raw.githubusercontent.com/zendesk/ruby-kafka/master/lib/kafka/sasl/oauth.rb&#34;&gt;Kafka::Sasl::OAuth&lt;/a&gt;) which returns an ID/Access token.&lt;/p&gt; &#xA;&lt;p&gt;Optionally, the client may implement an &lt;code&gt;extensions&lt;/code&gt; method that returns a map of key-value pairs. These can be sent with the SASL/OAUTHBEARER initial client response. This is only supported in kafka &amp;gt;= 2.1.0.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class TokenProvider&#xA;  def token&#xA;    &#34;some_id_token&#34;&#xA;  end&#xA;end&#xA;# ...&#xA;client = Kafka.new(&#xA;  [&#34;kafka1:9092&#34;],&#xA;  sasl_oauth_token_provider: TokenProvider.new&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Topic management&lt;/h3&gt; &#xA;&lt;p&gt;In addition to producing and consuming messages, ruby-kafka supports managing Kafka topics and their configurations. See &lt;a href=&#34;https://kafka.apache.org/documentation/#topicconfigs&#34;&gt;the Kafka documentation&lt;/a&gt; for a full list of topic configuration keys.&lt;/p&gt; &#xA;&lt;h4&gt;List all topics&lt;/h4&gt; &#xA;&lt;p&gt;Return an array of topic names.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new([&#34;kafka:9092&#34;])&#xA;kafka.topics&#xA;# =&amp;gt; [&#34;topic1&#34;, &#34;topic2&#34;, &#34;topic3&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Create a topic&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new([&#34;kafka:9092&#34;])&#xA;kafka.create_topic(&#34;topic&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, the new topic has 1 partition, replication factor 1 and default configs from the brokers. Those configurations are customizable:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new([&#34;kafka:9092&#34;])&#xA;kafka.create_topic(&#34;topic&#34;,&#xA;  num_partitions: 3,&#xA;  replication_factor: 2,&#xA;  config: {&#xA;    &#34;max.message.bytes&#34; =&amp;gt; 100000&#xA;  }&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Create more partitions for a topic&lt;/h4&gt; &#xA;&lt;p&gt;After a topic is created, you can increase the number of partitions for the topic. The new number of partitions must be greater than the current one.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new([&#34;kafka:9092&#34;])&#xA;kafka.create_partitions_for(&#34;topic&#34;, num_partitions: 10)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Fetch configuration for a topic (alpha feature)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new([&#34;kafka:9092&#34;])&#xA;kafka.describe_topic(&#34;topic&#34;, [&#34;max.message.bytes&#34;, &#34;retention.ms&#34;])&#xA;# =&amp;gt; {&#34;max.message.bytes&#34;=&amp;gt;&#34;100000&#34;, &#34;retention.ms&#34;=&amp;gt;&#34;604800000&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Alter a topic configuration (alpha feature)&lt;/h4&gt; &#xA;&lt;p&gt;Update the topic configurations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This feature is for advanced usage. Only use this if you know what you&#39;re doing.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new([&#34;kafka:9092&#34;])&#xA;kafka.alter_topic(&#34;topic&#34;, &#34;max.message.bytes&#34; =&amp;gt; 100000, &#34;retention.ms&#34; =&amp;gt; 604800000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Delete a topic&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;kafka = Kafka.new([&#34;kafka:9092&#34;])&#xA;kafka.delete_topic(&#34;topic&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After a topic is marked as deleted, Kafka only hides it from clients. It would take a while before a topic is completely deleted.&lt;/p&gt; &#xA;&lt;h2&gt;Design&lt;/h2&gt; &#xA;&lt;p&gt;The library has been designed as a layered system, with each layer having a clear responsibility:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;strong&gt;network layer&lt;/strong&gt; handles low-level connection tasks, such as keeping open connections to each Kafka broker, reconnecting when there&#39;s an error, etc. See &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/Connection&#34;&gt;&lt;code&gt;Kafka::Connection&lt;/code&gt;&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;strong&gt;protocol layer&lt;/strong&gt; is responsible for encoding and decoding the Kafka protocol&#39;s various structures. See &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/Protocol&#34;&gt;&lt;code&gt;Kafka::Protocol&lt;/code&gt;&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;strong&gt;operational layer&lt;/strong&gt; provides high-level operations, such as fetching messages from a topic, that may involve more than one API request to the Kafka cluster. Some complex operations are made available through &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/Cluster&#34;&gt;&lt;code&gt;Kafka::Cluster&lt;/code&gt;&lt;/a&gt;, which represents an entire cluster, while simpler ones are only available through &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/Broker&#34;&gt;&lt;code&gt;Kafka::Broker&lt;/code&gt;&lt;/a&gt;, which represents a single Kafka broker. In general, &lt;code&gt;Kafka::Cluster&lt;/code&gt; is the high-level API, with more polish.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;strong&gt;API layer&lt;/strong&gt; provides APIs to users of the libraries. The Consumer API is implemented in &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/Consumer&#34;&gt;&lt;code&gt;Kafka::Consumer&lt;/code&gt;&lt;/a&gt; while the Producer API is implemented in &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/Producer&#34;&gt;&lt;code&gt;Kafka::Producer&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/AsyncProducer&#34;&gt;&lt;code&gt;Kafka::AsyncProducer&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;strong&gt;configuration layer&lt;/strong&gt; provides a way to set up and configure the client, as well as easy entrypoints to the various APIs. &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/Client&#34;&gt;&lt;code&gt;Kafka::Client&lt;/code&gt;&lt;/a&gt; implements the public APIs. For convenience, the method &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka.new&#34;&gt;&lt;code&gt;Kafka.new&lt;/code&gt;&lt;/a&gt; can instantiate the class for you.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that only the API and configuration layers have any backwards compatibility guarantees ‚Äì the other layers are considered internal and may change without warning. Don&#39;t use them directly.&lt;/p&gt; &#xA;&lt;h3&gt;Producer Design&lt;/h3&gt; &#xA;&lt;p&gt;The producer is designed with resilience and operational ease of use in mind, sometimes at the cost of raw performance. For instance, the operation is heavily instrumented, allowing operators to monitor the producer at a very granular level.&lt;/p&gt; &#xA;&lt;p&gt;The producer has two main internal data structures: a list of &lt;em&gt;pending messages&lt;/em&gt; and a &lt;em&gt;message buffer&lt;/em&gt;. When the user calls &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka%2FProducer%3Aproduce&#34;&gt;&lt;code&gt;Kafka::Producer#produce&lt;/code&gt;&lt;/a&gt;, a message is appended to the pending message list, but no network communication takes place. This means that the call site does not have to handle the broad range of errors that can happen at the network or protocol level. Instead, those errors will only happen once &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka%2FProducer%3Adeliver_messages&#34;&gt;&lt;code&gt;Kafka::Producer#deliver_messages&lt;/code&gt;&lt;/a&gt; is called. This method will go through the pending messages one by one, making sure they&#39;re assigned a partition. This may fail for some messages, as it could require knowing the current configuration for the message&#39;s topic, necessitating API calls to Kafka. Messages that cannot be assigned a partition are kept in the list, while the others are written into the message buffer. The producer then figures out which topic partitions are led by which Kafka brokers so that messages can be sent to the right place ‚Äì in Kafka, it is the responsibility of the client to do this routing. A separate &lt;em&gt;produce&lt;/em&gt; API request will be sent to each broker; the response will be inspected; and messages that were acknowledged by the broker will be removed from the message buffer. Any messages that were &lt;em&gt;not&lt;/em&gt; acknowledged will be kept in the buffer.&lt;/p&gt; &#xA;&lt;p&gt;If there are any messages left in either the pending message list &lt;em&gt;or&lt;/em&gt; the message buffer after this operation, &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/DeliveryFailed&#34;&gt;&lt;code&gt;Kafka::DeliveryFailed&lt;/code&gt;&lt;/a&gt; will be raised. This exception must be rescued and handled by the user, possibly by calling &lt;code&gt;#deliver_messages&lt;/code&gt; at a later time.&lt;/p&gt; &#xA;&lt;h3&gt;Asynchronous Producer Design&lt;/h3&gt; &#xA;&lt;p&gt;The synchronous producer allows the user fine-grained control over when network activity and the possible errors arising from that will take place, but it requires the user to handle the errors nonetheless. The async producer provides a more hands-off approach that trades off control for ease of use and resilience.&lt;/p&gt; &#xA;&lt;p&gt;Instead of writing directly into the pending message list, &lt;a href=&#34;http://www.rubydoc.info/gems/ruby-kafka/Kafka/AsyncProducer&#34;&gt;&lt;code&gt;Kafka::AsyncProducer&lt;/code&gt;&lt;/a&gt; writes the message to an internal thread-safe queue, returning immediately. A background thread reads messages off the queue and passes them to a synchronous producer.&lt;/p&gt; &#xA;&lt;p&gt;Rather than triggering message deliveries directly, users of the async producer will typically set up &lt;em&gt;automatic triggers&lt;/em&gt;, such as a timer.&lt;/p&gt; &#xA;&lt;h3&gt;Consumer Design&lt;/h3&gt; &#xA;&lt;p&gt;The Consumer API is designed for flexibility and stability. The first is accomplished by not dictating any high-level object model, instead opting for a simple loop-based approach. The second is accomplished by handling group membership, heartbeats, and checkpointing automatically. Messages are marked as processed as soon as they&#39;ve been successfully yielded to the user-supplied processing block, minimizing the cost of processing errors.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;After checking out the repo, run &lt;code&gt;bin/setup&lt;/code&gt; to install dependencies. Then, run &lt;code&gt;rake spec&lt;/code&gt; to run the tests. You can also run &lt;code&gt;bin/console&lt;/code&gt; for an interactive prompt that will allow you to experiment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the specs require a working &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; instance, but should work out of the box if you have Docker installed. Please create an issue if that&#39;s not the case.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to contribute to ruby-kafka, please &lt;a href=&#34;https://ruby-kafka-slack.herokuapp.com/&#34;&gt;join our Slack team&lt;/a&gt; and ask how best to do it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/zendesk/ruby-kafka/tree/master&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/zendesk/ruby-kafka.svg?style=shield&#34; alt=&#34;Circle CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Support and Discussion&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;ve discovered a bug, please file a &lt;a href=&#34;https://github.com/zendesk/ruby-kafka/issues/new&#34;&gt;Github issue&lt;/a&gt;, and make sure to include all the relevant information, including the version of ruby-kafka and Kafka that you&#39;re using.&lt;/p&gt; &#xA;&lt;p&gt;If you have other questions, or would like to discuss best practises, how to contribute to the project, or any other ruby-kafka related topic, &lt;a href=&#34;https://ruby-kafka-slack.herokuapp.com/&#34;&gt;join our Slack team&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Version 0.4 will be the last minor release with support for the Kafka 0.9 protocol. It is recommended that you pin your dependency on ruby-kafka to &lt;code&gt;~&amp;gt; 0.4.0&lt;/code&gt; in order to receive bugfixes and security updates. New features will only target version 0.5 and up, which will be incompatible with the Kafka 0.9 protocol.&lt;/p&gt; &#xA;&lt;h3&gt;v0.4&lt;/h3&gt; &#xA;&lt;p&gt;Last stable release with support for the Kafka 0.9 protocol. Bug and security fixes will be released in patch updates.&lt;/p&gt; &#xA;&lt;h3&gt;v0.5&lt;/h3&gt; &#xA;&lt;p&gt;Latest stable release, with native support for the Kafka 0.10 protocol and eventually newer protocol versions. Kafka 0.9 is no longer supported by this release series.&lt;/p&gt; &#xA;&lt;h2&gt;Higher level libraries&lt;/h2&gt; &#xA;&lt;p&gt;Currently, there are three actively developed frameworks based on ruby-kafka, that provide higher level API that can be used to work with Kafka messages and two libraries for publishing messages.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Many of the frameworks and libraries below use &lt;code&gt;ruby-kafka&lt;/code&gt; only in older versions. See the deprecation notice at the top of this readme for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Message processing frameworks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/zendesk/racecar/tree/v1-stable&#34;&gt;Racecar 1.3&lt;/a&gt; - A simple framework that integrates with Ruby on Rails to provide a seamless way to write, test, configure, and run Kafka consumers. It comes with sensible defaults and conventions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/karafka/karafka/tree/1.4&#34;&gt;Karafka 1.4&lt;/a&gt; - Framework used to simplify Apache Kafka based Ruby and Rails applications development. Karafka provides higher abstraction layers, including Capistrano, Docker and Heroku support.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/klarna/phobos&#34;&gt;Phobos&lt;/a&gt; - Micro framework and library for applications dealing with Apache Kafka. It wraps common behaviors needed by consumers and producers in an easy and convenient API.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Message publishing libraries&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/zendesk/delivery_boy&#34;&gt;DeliveryBoy&lt;/a&gt; ‚Äì A library that integrates with Ruby on Rails, making it easy to publish Kafka messages from any Rails application.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/karafka/waterdrop/tree/1.4&#34;&gt;WaterDrop 1.4&lt;/a&gt; ‚Äì A library for Ruby and Ruby on Rails applications, to easy publish Kafka messages in both sync and async way.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Why Create A New Library?&lt;/h2&gt; &#xA;&lt;p&gt;There are a few existing Kafka clients in Ruby:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bpot/poseidon&#34;&gt;Poseidon&lt;/a&gt; seems to work for Kafka 0.8, but the project is unmaintained and has known issues.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reiseburo/hermann&#34;&gt;Hermann&lt;/a&gt; wraps the C library &lt;a href=&#34;https://github.com/edenhill/librdkafka&#34;&gt;librdkafka&lt;/a&gt; and seems to be very efficient, but its API and mode of operation is too intrusive for our needs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/joekiller/jruby-kafka&#34;&gt;jruby-kafka&lt;/a&gt; is a great option if you&#39;re running on JRuby.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We needed a robust client that could be used from our existing Ruby apps, allowed our Ops to monitor operation, and provided flexible error handling. There didn&#39;t exist such a client, hence this project.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Bug reports and pull requests are welcome on GitHub at &lt;a href=&#34;https://github.com/zendesk/ruby-kafka&#34;&gt;https://github.com/zendesk/ruby-kafka&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Copyright and license&lt;/h2&gt; &#xA;&lt;p&gt;Copyright 2015 Zendesk&lt;/p&gt; &#xA;&lt;p&gt;Licensed under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License.&lt;/p&gt; &#xA;&lt;p&gt;You may obtain a copy of the License at &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>carrierwaveuploader/carrierwave</title>
    <updated>2022-12-11T01:46:11Z</updated>
    <id>tag:github.com,2022-12-11:/carrierwaveuploader/carrierwave</id>
    <link href="https://github.com/carrierwaveuploader/carrierwave" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Classier solution for file uploads for Rails, Sinatra and other Ruby web frameworks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CarrierWave&lt;/h1&gt; &#xA;&lt;p&gt;This gem provides a simple and extremely flexible way to upload files from Ruby applications. It works well with Rack based web applications, such as Ruby on Rails.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave/actions&#34;&gt;&lt;img src=&#34;https://github.com/carrierwaveuploader/carrierwave/workflows/Test/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codeclimate.com/github/carrierwaveuploader/carrierwave&#34;&gt;&lt;img src=&#34;https://codeclimate.com/github/carrierwaveuploader/carrierwave.svg?sanitize=true&#34; alt=&#34;Code Climate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dependabot.com/compatibility-score.html?dependency-name=carrierwave&amp;amp;package-manager=bundler&amp;amp;version-scheme=semver&#34;&gt;&lt;img src=&#34;https://api.dependabot.com/badges/compatibility_score?dependency-name=carrierwave&amp;amp;package-manager=bundler&amp;amp;version-scheme=semver&#34; alt=&#34;SemVer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Information&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RDoc documentation &lt;a href=&#34;http://rubydoc.info/gems/carrierwave/frames&#34;&gt;available on RubyDoc.info&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Source code &lt;a href=&#34;http://github.com/carrierwaveuploader/carrierwave&#34;&gt;available on GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;More information, known limitations, and how-tos &lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave/wiki&#34;&gt;available on the wiki&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Help&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please ask the community on &lt;a href=&#34;https://stackoverflow.com/questions/tagged/carrierwave&#34;&gt;Stack Overflow&lt;/a&gt; for help if you have any questions. Please do not post usage questions on the issue tracker.&lt;/li&gt; &#xA; &lt;li&gt;Please report bugs on the &lt;a href=&#34;http://github.com/carrierwaveuploader/carrierwave/issues&#34;&gt;issue tracker&lt;/a&gt; but read the &#34;getting help&#34; section in the wiki first.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install the latest release:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ gem install carrierwave&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In Rails, add it to your Gemfile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;gem &#39;carrierwave&#39;, &#39;&amp;gt;= 3.0.0.beta&#39;, &#39;&amp;lt; 4.0&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, restart the server to apply the changes.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Start off by generating an uploader:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rails generate uploader Avatar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;this should give you a file in:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;app/uploaders/avatar_uploader.rb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check out this file for some hints on how you can customize your uploader. It should look something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class AvatarUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  storage :file&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use your uploader class to store and retrieve files like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;uploader = AvatarUploader.new&#xA;&#xA;uploader.store!(my_file)&#xA;&#xA;uploader.retrieve_from_store!(&#39;my_file.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CarrierWave gives you a &lt;code&gt;store&lt;/code&gt; for permanent storage, and a &lt;code&gt;cache&lt;/code&gt; for temporary storage. You can use different stores, including filesystem and cloud storage.&lt;/p&gt; &#xA;&lt;p&gt;Most of the time you are going to want to use CarrierWave together with an ORM. It is quite simple to mount uploaders on columns in your model, so you can simply assign files and get going:&lt;/p&gt; &#xA;&lt;h3&gt;ActiveRecord&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you are loading CarrierWave after loading your ORM, otherwise you&#39;ll need to require the relevant extension manually, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;carrierwave/orm/activerecord&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add a string column to the model you want to mount the uploader by creating a migration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rails g migration add_avatar_to_users avatar:string&#xA;rails db:migrate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open your model file and mount the uploader:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class User &amp;lt; ApplicationRecord&#xA;  mount_uploader :avatar, AvatarUploader&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can cache files by assigning them to the attribute, they will automatically be stored when the record is saved.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;u = User.new&#xA;u.avatar = params[:file] # Assign a file like this, or&#xA;&#xA;# like this&#xA;File.open(&#39;somewhere&#39;) do |f|&#xA;  u.avatar = f&#xA;end&#xA;&#xA;u.save!&#xA;u.avatar.url # =&amp;gt; &#39;/url/to/file.png&#39;&#xA;u.avatar.current_path # =&amp;gt; &#39;path/to/file.png&#39;&#xA;u.avatar_identifier # =&amp;gt; &#39;file.png&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;code&gt;u.avatar&lt;/code&gt; will never return nil, even if there is no photo associated to it. To check if a photo was saved to the model, use &lt;code&gt;u.avatar.file.nil?&lt;/code&gt; instead.&lt;/p&gt; &#xA;&lt;h3&gt;DataMapper, Mongoid, Sequel&lt;/h3&gt; &#xA;&lt;p&gt;Other ORM support has been extracted into separate gems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave-datamapper&#34;&gt;carrierwave-datamapper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave-mongoid&#34;&gt;carrierwave-mongoid&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave-sequel&#34;&gt;carrierwave-sequel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;There are more extensions listed in &lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave/wiki&#34;&gt;the wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Multiple file uploads&lt;/h2&gt; &#xA;&lt;p&gt;CarrierWave also has convenient support for multiple file upload fields.&lt;/p&gt; &#xA;&lt;h3&gt;ActiveRecord&lt;/h3&gt; &#xA;&lt;p&gt;Add a column which can store an array. This could be an array column or a JSON column for example. Your choice depends on what your database supports. For example, create a migration like this:&lt;/p&gt; &#xA;&lt;h4&gt;For databases with ActiveRecord json data type support (e.g. PostgreSQL, MySQL)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;rails g migration add_avatars_to_users avatars:json&#xA;rails db:migrate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For database without ActiveRecord json data type support (e.g. SQLite)&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;rails g migration add_avatars_to_users avatars:string&#xA;rails db:migrate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: JSON datatype doesn&#39;t exists in SQLite adapter, that&#39;s why you can use a string datatype which will be serialized in model.&lt;/p&gt; &#xA;&lt;p&gt;Open your model file and mount the uploader:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class User &amp;lt; ApplicationRecord&#xA;  mount_uploaders :avatars, AvatarUploader&#xA;  serialize :avatars, JSON # If you use SQLite, add this line.&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure that you mount the uploader with write (mount_uploaders) with &lt;code&gt;s&lt;/code&gt; not (mount_uploader) in order to avoid errors when uploading multiple files&lt;/p&gt; &#xA;&lt;p&gt;Make sure your file input fields are set up as multiple file fields. For example in Rails you&#39;ll want to do something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-erb&#34;&gt;&amp;lt;%= form.file_field :avatars, multiple: true %&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also, make sure your upload controller permits the multiple file upload attribute, &lt;em&gt;pointing to an empty array in a hash&lt;/em&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;params.require(:user).permit(:email, :first_name, :last_name, {avatars: []})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can select multiple files in the upload dialog (e.g. SHIFT+SELECT), and they will automatically be stored when the record is saved.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;u = User.new(params[:user])&#xA;u.save!&#xA;u.avatars[0].url # =&amp;gt; &#39;/url/to/file.png&#39;&#xA;u.avatars[0].current_path # =&amp;gt; &#39;path/to/file.png&#39;&#xA;u.avatars[0].identifier # =&amp;gt; &#39;file.png&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to preserve existing files on uploading new one, you can go like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-erb&#34;&gt;&amp;lt;% user.avatars.each do |avatar| %&amp;gt;&#xA;  &amp;lt;%= hidden_field :user, :avatars, multiple: true, value: avatar.identifier %&amp;gt;&#xA;&amp;lt;% end %&amp;gt;&#xA;&amp;lt;%= form.file_field :avatars, multiple: true %&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sorting avatars is supported as well by reordering &lt;code&gt;hidden_field&lt;/code&gt;, an example using jQuery UI Sortable is available &lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave/wiki/How-to%3A-Add%2C-remove-and-reorder-images-using-multiple-file-upload&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Changing the storage directory&lt;/h2&gt; &#xA;&lt;p&gt;In order to change where uploaded files are put, just override the &lt;code&gt;store_dir&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  def store_dir&#xA;    &#39;public/my/upload/directory&#39;&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This works for the file storage as well as Amazon S3 and Rackspace Cloud Files. Define &lt;code&gt;store_dir&lt;/code&gt; as &lt;code&gt;nil&lt;/code&gt; if you&#39;d like to store files at the root level.&lt;/p&gt; &#xA;&lt;p&gt;If you store files outside the project root folder, you may want to define &lt;code&gt;cache_dir&lt;/code&gt; in the same way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  def cache_dir&#xA;    &#39;/tmp/projectname-cache&#39;&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Securing uploads&lt;/h2&gt; &#xA;&lt;p&gt;Certain files might be dangerous if uploaded to the wrong location, such as PHP files or other script files. CarrierWave allows you to specify an allowlist of allowed extensions or content types.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re mounting the uploader, uploading a file with the wrong extension will make the record invalid instead. Otherwise, an error is raised.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  def extension_allowlist&#xA;    %w(jpg jpeg gif png)&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The same thing could be done using content types. Let&#39;s say we need an uploader that accepts only images. This can be done like this&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  def content_type_allowlist&#xA;    /image\//&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use a denylist to reject content types. Let&#39;s say we need an uploader that reject JSON files. This can be done like this&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class NoJsonUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  def content_type_denylist&#xA;    [&#39;application/text&#39;, &#39;application/json&#39;]&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CVE-2016-3714 (ImageTragick)&lt;/h3&gt; &#xA;&lt;p&gt;This version of CarrierWave has the ability to mitigate CVE-2016-3714. However, you &lt;strong&gt;MUST&lt;/strong&gt; set a content_type_allowlist in your uploaders for this protection to be effective, and you &lt;strong&gt;MUST&lt;/strong&gt; either disable ImageMagick&#39;s default SVG delegate or use the RSVG delegate for SVG processing.&lt;/p&gt; &#xA;&lt;p&gt;A valid allowlist that will restrict your uploader to images only, and mitigate the CVE is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  def content_type_allowlist&#xA;    [/image\//]&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: A &lt;code&gt;content_type_allowlist&lt;/code&gt; is the only form of allowlist or denylist supported by CarrierWave that can effectively mitigate against CVE-2016-3714. Use of &lt;code&gt;extension_allowlist&lt;/code&gt; will not inspect the file headers, and thus still leaves your application open to the vulnerability.&lt;/p&gt; &#xA;&lt;h3&gt;Filenames and unicode chars&lt;/h3&gt; &#xA;&lt;p&gt;Another security issue you should care for is the file names (see &lt;a href=&#34;http://guides.rubyonrails.org/security.html#file-uploads&#34;&gt;Ruby On Rails Security Guide&lt;/a&gt;). By default, CarrierWave provides only English letters, arabic numerals and some symbols as allowlisted characters in the file name. If you want to support local scripts (Cyrillic letters, letters with diacritics and so on), you have to override &lt;code&gt;sanitize_regexp&lt;/code&gt; method. It should return regular expression which would match all &lt;em&gt;non&lt;/em&gt;-allowed symbols.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;CarrierWave::SanitizedFile.sanitize_regexp = /[^[:word:]\.\-\+]/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also make sure that allowing non-latin characters won&#39;t cause a compatibility issue with a third-party plugins or client-side software.&lt;/p&gt; &#xA;&lt;h2&gt;Setting the content type&lt;/h2&gt; &#xA;&lt;p&gt;As of v0.11.0, the &lt;code&gt;mime-types&lt;/code&gt; gem is a runtime dependency and the content type is set automatically. You no longer need to do this manually.&lt;/p&gt; &#xA;&lt;h2&gt;Adding versions&lt;/h2&gt; &#xA;&lt;p&gt;Often you&#39;ll want to add different versions of the same file. The classic example is image thumbnails. There is built in support for this*:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; You must have Imagemagick installed to do image resizing.&lt;/p&gt; &#xA;&lt;p&gt;Some documentation refers to RMagick instead of MiniMagick but MiniMagick is recommended.&lt;/p&gt; &#xA;&lt;p&gt;To install Imagemagick on OSX with homebrew type the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ brew install imagemagick&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  include CarrierWave::MiniMagick&#xA;&#xA;  process resize_to_fit: [800, 800]&#xA;&#xA;  version :thumb do&#xA;    process resize_to_fill: [200,200]&#xA;  end&#xA;&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When this uploader is used, an uploaded image would be scaled to be no larger than 800 by 800 pixels. The original aspect ratio will be kept.&lt;/p&gt; &#xA;&lt;p&gt;A version called &lt;code&gt;:thumb&lt;/code&gt; is then created, which is scaled to exactly 200 by 200 pixels. The thumbnail uses &lt;code&gt;resize_to_fill&lt;/code&gt; which makes sure that the width and height specified are filled, only cropping if the aspect ratio requires it.&lt;/p&gt; &#xA;&lt;p&gt;The above uploader could be used like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;uploader = AvatarUploader.new&#xA;uploader.store!(my_file)                              # size: 1024x768&#xA;&#xA;uploader.url # =&amp;gt; &#39;/url/to/my_file.png&#39;               # size: 800x800&#xA;uploader.thumb.url # =&amp;gt; &#39;/url/to/thumb_my_file.png&#39;   # size: 200x200&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;One important thing to remember is that process is called &lt;em&gt;before&lt;/em&gt; versions are created. This can cut down on processing cost.&lt;/p&gt; &#xA;&lt;h3&gt;Processing Methods: mini_magick&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;convert&lt;/code&gt; - Changes the image encoding format to the given format, eg. jpg&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;resize_to_limit&lt;/code&gt; - Resize the image to fit within the specified dimensions while retaining the original aspect ratio. Will only resize the image if it is larger than the specified dimensions. The resulting image may be shorter or narrower than specified in the smaller dimension but will not be larger than the specified values.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;resize_to_fit&lt;/code&gt; - Resize the image to fit within the specified dimensions while retaining the original aspect ratio. The image may be shorter or narrower than specified in the smaller dimension but will not be larger than the specified values.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;resize_to_fill&lt;/code&gt; - Resize the image to fit within the specified dimensions while retaining the aspect ratio of the original image. If necessary, crop the image in the larger dimension. Optionally, a &#34;gravity&#34; may be specified, for example &#34;Center&#34;, or &#34;NorthEast&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;resize_and_pad&lt;/code&gt; - Resize the image to fit within the specified dimensions while retaining the original aspect ratio. If necessary, will pad the remaining area with the given color, which defaults to transparent (for gif and png, white for jpeg). Optionally, a &#34;gravity&#34; may be specified, as above.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;code&gt;carrierwave/processing/mini_magick.rb&lt;/code&gt; for details.&lt;/p&gt; &#xA;&lt;h3&gt;conditional process&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use conditional process, you can only use &lt;code&gt;if&lt;/code&gt; statement.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;carrierwave/uploader/processing.rb&lt;/code&gt; for details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  process :scale =&amp;gt; [200, 200], :if =&amp;gt; :image?&#xA;  &#xA;  def image?(carrier_wave_sanitized_file)&#xA;    true&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Nested versions&lt;/h3&gt; &#xA;&lt;p&gt;It is possible to nest versions within versions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;&#xA;  version :animal do&#xA;    version :human&#xA;    version :monkey&#xA;    version :llama&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Conditional versions&lt;/h3&gt; &#xA;&lt;p&gt;Occasionally you want to restrict the creation of versions on certain properties within the model or based on the picture itself.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;&#xA;  version :human, if: :is_human?&#xA;  version :monkey, if: :is_monkey?&#xA;  version :banner, if: :is_landscape?&#xA;&#xA;private&#xA;&#xA;  def is_human? picture&#xA;    model.can_program?(:ruby)&#xA;  end&#xA;&#xA;  def is_monkey? picture&#xA;    model.favorite_food == &#39;banana&#39;&#xA;  end&#xA;&#xA;  def is_landscape? picture&#xA;    image = MiniMagick::Image.new(picture.path)&#xA;    image[:width] &amp;gt; image[:height]&#xA;  end&#xA;&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;model&lt;/code&gt; variable points to the instance object the uploader is attached to.&lt;/p&gt; &#xA;&lt;h3&gt;Create versions from existing versions&lt;/h3&gt; &#xA;&lt;p&gt;For performance reasons, it is often useful to create versions from existing ones instead of using the original file. If your uploader generates several versions where the next is smaller than the last, it will take less time to generate from a smaller, already processed image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;&#xA;  version :thumb do&#xA;    process resize_to_fill: [280, 280]&#xA;  end&#xA;&#xA;  version :small_thumb, from_version: :thumb do&#xA;    process resize_to_fill: [20, 20]&#xA;  end&#xA;&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The option &lt;code&gt;:from_version&lt;/code&gt; uses the file cached in the &lt;code&gt;:thumb&lt;/code&gt; version instead of the original version, potentially resulting in faster processing.&lt;/p&gt; &#xA;&lt;h2&gt;Making uploads work across form redisplays&lt;/h2&gt; &#xA;&lt;p&gt;Often you&#39;ll notice that uploaded files disappear when a validation fails. CarrierWave has a feature that makes it easy to remember the uploaded file even in that case. Suppose your &lt;code&gt;user&lt;/code&gt; model has an uploader mounted on &lt;code&gt;avatar&lt;/code&gt; file, just add a hidden field called &lt;code&gt;avatar_cache&lt;/code&gt; (don&#39;t forget to add it to the attr_accessible list as necessary). In Rails, this would look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-erb&#34;&gt;&amp;lt;%= form_for @user, html: { multipart: true } do |f| %&amp;gt;&#xA;  &amp;lt;p&amp;gt;&#xA;    &amp;lt;label&amp;gt;My Avatar&amp;lt;/label&amp;gt;&#xA;    &amp;lt;%= f.file_field :avatar %&amp;gt;&#xA;    &amp;lt;%= f.hidden_field :avatar_cache %&amp;gt;&#xA;  &amp;lt;/p&amp;gt;&#xA;&amp;lt;% end %&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It might be a good idea to show the user that a file has been uploaded, in the case of images, a small thumbnail would be a good indicator:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-erb&#34;&gt;&amp;lt;%= form_for @user, html: { multipart: true } do |f| %&amp;gt;&#xA;  &amp;lt;p&amp;gt;&#xA;    &amp;lt;label&amp;gt;My Avatar&amp;lt;/label&amp;gt;&#xA;    &amp;lt;%= image_tag(@user.avatar_url) if @user.avatar? %&amp;gt;&#xA;    &amp;lt;%= f.file_field :avatar %&amp;gt;&#xA;    &amp;lt;%= f.hidden_field :avatar_cache %&amp;gt;&#xA;  &amp;lt;/p&amp;gt;&#xA;&amp;lt;% end %&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Removing uploaded files&lt;/h2&gt; &#xA;&lt;p&gt;If you want to remove a previously uploaded file on a mounted uploader, you can easily add a checkbox to the form which will remove the file when checked.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-erb&#34;&gt;&amp;lt;%= form_for @user, html: { multipart: true } do |f| %&amp;gt;&#xA;  &amp;lt;p&amp;gt;&#xA;    &amp;lt;label&amp;gt;My Avatar&amp;lt;/label&amp;gt;&#xA;    &amp;lt;%= image_tag(@user.avatar_url) if @user.avatar? %&amp;gt;&#xA;    &amp;lt;%= f.file_field :avatar %&amp;gt;&#xA;  &amp;lt;/p&amp;gt;&#xA;&#xA;  &amp;lt;p&amp;gt;&#xA;    &amp;lt;label&amp;gt;&#xA;      &amp;lt;%= f.check_box :remove_avatar %&amp;gt;&#xA;      Remove avatar&#xA;    &amp;lt;/label&amp;gt;&#xA;  &amp;lt;/p&amp;gt;&#xA;&amp;lt;% end %&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to remove the file manually, you can call &lt;code&gt;remove_avatar!&lt;/code&gt;, then save the object.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-erb&#34;&gt;@user.remove_avatar!&#xA;@user.save&#xA;#=&amp;gt; true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Uploading files from a remote location&lt;/h2&gt; &#xA;&lt;p&gt;Your users may find it convenient to upload a file from a location on the Internet via a URL. CarrierWave makes this simple, just add the appropriate attribute to your form and you&#39;re good to go:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-erb&#34;&gt;&amp;lt;%= form_for @user, html: { multipart: true } do |f| %&amp;gt;&#xA;  &amp;lt;p&amp;gt;&#xA;    &amp;lt;label&amp;gt;My Avatar URL:&amp;lt;/label&amp;gt;&#xA;    &amp;lt;%= image_tag(@user.avatar_url) if @user.avatar? %&amp;gt;&#xA;    &amp;lt;%= f.text_field :remote_avatar_url %&amp;gt;&#xA;  &amp;lt;/p&amp;gt;&#xA;&amp;lt;% end %&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re using ActiveRecord, CarrierWave will indicate invalid URLs and download failures automatically with attribute validation errors. If you aren&#39;t, or you disable CarrierWave&#39;s &lt;code&gt;validate_download&lt;/code&gt; option, you&#39;ll need to handle those errors yourself.&lt;/p&gt; &#xA;&lt;h3&gt;Retry option for download from remote location&lt;/h3&gt; &#xA;&lt;p&gt;If you want to retry the download from the Remote URL, enable the download_retry_count option, an error occurs during download, it will try to execute the specified number of times every 5 second. This option is effective when the remote destination is unstable.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rb&#34;&gt;CarrierWave.configure do |config|&#xA;  config.download_retry_count = 3 # Default 0&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Providing a default URL&lt;/h2&gt; &#xA;&lt;p&gt;In many cases, especially when working with images, it might be a good idea to provide a default url, a fallback in case no file has been uploaded. You can do this easily by overriding the &lt;code&gt;default_url&lt;/code&gt; method in your uploader:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  def default_url(*args)&#xA;    &#34;/images/fallback/&#34; + [version_name, &#34;default.png&#34;].compact.join(&#39;_&#39;)&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if you are using the Rails asset pipeline:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  def default_url(*args)&#xA;    ActionController::Base.helpers.asset_path(&#34;fallback/&#34; + [version_name, &#34;default.png&#34;].compact.join(&#39;_&#39;))&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Recreating versions&lt;/h2&gt; &#xA;&lt;p&gt;You might come to a situation where you want to retroactively change a version or add a new one. You can use the &lt;code&gt;recreate_versions!&lt;/code&gt; method to recreate the versions from the base file. This uses a naive approach which will re-upload and process the specified version or all versions, if none is passed as an argument.&lt;/p&gt; &#xA;&lt;p&gt;When you are generating random unique filenames you have to call &lt;code&gt;save!&lt;/code&gt; on the model after using &lt;code&gt;recreate_versions!&lt;/code&gt;. This is necessary because &lt;code&gt;recreate_versions!&lt;/code&gt; doesn&#39;t save the new filename to the database. Calling &lt;code&gt;save!&lt;/code&gt; yourself will prevent that the database and file system are running out of sync.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;instance = MyUploader.new&#xA;instance.recreate_versions!(:thumb, :large)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or on a mounted uploader:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;User.find_each do |user|&#xA;  user.avatar.recreate_versions!&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: &lt;code&gt;recreate_versions!&lt;/code&gt; will throw an exception on records without an image. To avoid this, scope the records to those with images or check if an image exists within the block. If you&#39;re using ActiveRecord, recreating versions for a user avatar might look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;User.find_each do |user|&#xA;  user.avatar.recreate_versions! if user.avatar?&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configuring CarrierWave&lt;/h2&gt; &#xA;&lt;p&gt;CarrierWave has a broad range of configuration options, which you can configure, both globally and on a per-uploader basis:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;CarrierWave.configure do |config|&#xA;  config.permissions = 0666&#xA;  config.directory_permissions = 0777&#xA;  config.storage = :file&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or alternatively:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class AvatarUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  permissions 0777&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re using Rails, create an initializer for this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;config/initializers/carrierwave.rb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want CarrierWave to fail noisily in development, you can change these configs in your environment file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;CarrierWave.configure do |config|&#xA;  config.ignore_integrity_errors = false&#xA;  config.ignore_processing_errors = false&#xA;  config.ignore_download_errors = false&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing with CarrierWave&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s a good idea to test your uploaders in isolation. In order to speed up your tests, it&#39;s recommended to switch off processing in your tests, and to use the file storage. In Rails you could do that by adding an initializer with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;if Rails.env.test? or Rails.env.cucumber?&#xA;  CarrierWave.configure do |config|&#xA;    config.storage = :file&#xA;    config.enable_processing = false&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remember, if you have already set &lt;code&gt;storage :something&lt;/code&gt; in your uploader, the &lt;code&gt;storage&lt;/code&gt; setting from this initializer will be ignored.&lt;/p&gt; &#xA;&lt;p&gt;If you need to test your processing, you should test it in isolation, and enable processing only for those tests that need it.&lt;/p&gt; &#xA;&lt;p&gt;CarrierWave comes with some RSpec matchers which you may find useful:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#39;carrierwave/test/matchers&#39;&#xA;&#xA;describe MyUploader do&#xA;  include CarrierWave::Test::Matchers&#xA;&#xA;  let(:user) { double(&#39;user&#39;) }&#xA;  let(:uploader) { MyUploader.new(user, :avatar) }&#xA;&#xA;  before do&#xA;    MyUploader.enable_processing = true&#xA;    File.open(path_to_file) { |f| uploader.store!(f) }&#xA;  end&#xA;&#xA;  after do&#xA;    MyUploader.enable_processing = false&#xA;    uploader.remove!&#xA;  end&#xA;&#xA;  context &#39;the thumb version&#39; do&#xA;    it &#34;scales down a landscape image to be exactly 64 by 64 pixels&#34; do&#xA;      expect(uploader.thumb).to have_dimensions(64, 64)&#xA;    end&#xA;  end&#xA;&#xA;  context &#39;the small version&#39; do&#xA;    it &#34;scales down a landscape image to fit within 200 by 200 pixels&#34; do&#xA;      expect(uploader.small).to be_no_larger_than(200, 200)&#xA;    end&#xA;  end&#xA;&#xA;  it &#34;makes the image readable only to the owner and not executable&#34; do&#xA;    expect(uploader).to have_permissions(0600)&#xA;  end&#xA;&#xA;  it &#34;has the correct format&#34; do&#xA;    expect(uploader).to be_format(&#39;png&#39;)&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re looking for minitest asserts, checkout &lt;a href=&#34;https://github.com/hcfairbanks/carrierwave_asserts&#34;&gt;carrierwave_asserts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Setting the enable_processing flag on an uploader will prevent any of the versions from processing as well. Processing can be enabled for a single version by setting the processing flag on the version like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;@uploader.thumb.enable_processing = true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fog&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use fog you must add in your CarrierWave initializer the following lines&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;config.fog_credentials = { ... } # Provider specific credentials&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using Amazon S3&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://github.com/fog/fog-aws&#34;&gt;Fog AWS&lt;/a&gt; is used to support Amazon S3. Ensure you have it in your Gemfile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;gem &#34;fog-aws&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll need to provide your fog_credentials and a fog_directory (also known as a bucket) in an initializer. For the sake of performance it is assumed that the directory already exists, so please create it if it needs to be. You can also pass in additional options, as documented fully in lib/carrierwave/storage/fog.rb. Here&#39;s a full example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;CarrierWave.configure do |config|&#xA;  config.fog_credentials = {&#xA;    provider:              &#39;AWS&#39;,                        # required&#xA;    aws_access_key_id:     &#39;xxx&#39;,                        # required unless using use_iam_profile&#xA;    aws_secret_access_key: &#39;yyy&#39;,                        # required unless using use_iam_profile&#xA;    use_iam_profile:       true,                         # optional, defaults to false&#xA;    region:                &#39;eu-west-1&#39;,                  # optional, defaults to &#39;us-east-1&#39;&#xA;    host:                  &#39;s3.example.com&#39;,             # optional, defaults to nil&#xA;    endpoint:              &#39;https://s3.example.com:8080&#39; # optional, defaults to nil&#xA;  }&#xA;  config.fog_directory  = &#39;name_of_bucket&#39;                                      # required&#xA;  config.fog_public     = false                                                 # optional, defaults to true&#xA;  config.fog_attributes = { cache_control: &#34;public, max-age=#{365.days.to_i}&#34; } # optional, defaults to {}&#xA;  # For an application which utilizes multiple servers but does not need caches persisted across requests,&#xA;  # uncomment the line :file instead of the default :storage.  Otherwise, it will use AWS as the temp cache store.&#xA;  # config.cache_storage = :file&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In your uploader, set the storage to :fog&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class AvatarUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  storage :fog&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! You can still use the &lt;code&gt;CarrierWave::Uploader#url&lt;/code&gt; method to return the url to the file on Amazon S3.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: for Carrierwave to work properly it needs credentials with the following permissions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;s3:ListBucket&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;s3:PutObject&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;s3:GetObject&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;s3:DeleteObject&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;s3:PutObjectAcl&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Using Rackspace Cloud Files&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://github.com/fog/fog&#34;&gt;Fog&lt;/a&gt; is used to support Rackspace Cloud Files. Ensure you have it in your Gemfile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;gem &#34;fog&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll need to configure a directory (also known as a container), username and API key in the initializer. For the sake of performance it is assumed that the directory already exists, so please create it if need be.&lt;/p&gt; &#xA;&lt;p&gt;Using a US-based account:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;CarrierWave.configure do |config|&#xA;  config.fog_credentials = {&#xA;    provider:           &#39;Rackspace&#39;,&#xA;    rackspace_username: &#39;xxxxxx&#39;,&#xA;    rackspace_api_key:  &#39;yyyyyy&#39;,&#xA;    rackspace_region:   :ord                      # optional, defaults to :dfw&#xA;  }&#xA;  config.fog_directory = &#39;name_of_directory&#39;&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using a UK-based account:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;CarrierWave.configure do |config|&#xA;  config.fog_credentials = {&#xA;    provider:           &#39;Rackspace&#39;,&#xA;    rackspace_username: &#39;xxxxxx&#39;,&#xA;    rackspace_api_key:  &#39;yyyyyy&#39;,&#xA;    rackspace_auth_url: Fog::Rackspace::UK_AUTH_ENDPOINT,&#xA;    rackspace_region:   :lon&#xA;  }&#xA;  config.fog_directory = &#39;name_of_directory&#39;&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally include your CDN host name in the configuration. This is &lt;em&gt;highly&lt;/em&gt; recommended, as without it every request requires a lookup of this information.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;config.asset_host = &#34;http://c000000.cdn.rackspacecloud.com&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In your uploader, set the storage to :fog&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class AvatarUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  storage :fog&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! You can still use the &lt;code&gt;CarrierWave::Uploader#url&lt;/code&gt; method to return the url to the file on Rackspace Cloud Files.&lt;/p&gt; &#xA;&lt;h2&gt;Using Google Cloud Storage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://github.com/fog/fog-google&#34;&gt;Fog&lt;/a&gt; is used to support Google Cloud Storage. Ensure you have it in your Gemfile:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;gem &#34;fog-google&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll need to configure a directory (also known as a bucket) and the credentials in the initializer. For the sake of performance it is assumed that the directory already exists, so please create it if need be.&lt;/p&gt; &#xA;&lt;p&gt;Please read the &lt;a href=&#34;https://github.com/fog/fog-google/raw/master/README.md&#34;&gt;fog-google README&lt;/a&gt; on how to get credentials.&lt;/p&gt; &#xA;&lt;p&gt;For Google Storage JSON API (recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;CarrierWave.configure do |config|&#xA;    config.fog_provider = &#39;fog/google&#39;&#xA;    config.fog_credentials = {&#xA;        provider:               &#39;Google&#39;,&#xA;        google_project:         &#39;my-project&#39;,&#xA;        google_json_key_string: &#39;xxxxxx&#39;&#xA;        # or use google_json_key_location if using an actual file&#xA;    }&#xA;    config.fog_directory = &#39;google_cloud_storage_bucket_name&#39;&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For Google Storage XML API:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;CarrierWave.configure do |config|&#xA;    config.fog_provider = &#39;fog/google&#39;&#xA;    config.fog_credentials = {&#xA;        provider:                         &#39;Google&#39;,&#xA;        google_storage_access_key_id:     &#39;xxxxxx&#39;,&#xA;        google_storage_secret_access_key: &#39;yyyyyy&#39;&#xA;    }&#xA;    config.fog_directory = &#39;google_cloud_storage_bucket_name&#39;&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In your uploader, set the storage to :fog&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class AvatarUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  storage :fog&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it! You can still use the &lt;code&gt;CarrierWave::Uploader#url&lt;/code&gt; method to return the url to the file on Google.&lt;/p&gt; &#xA;&lt;h2&gt;Optimized Loading of Fog&lt;/h2&gt; &#xA;&lt;p&gt;Since Carrierwave doesn&#39;t know which parts of Fog you intend to use, it will just load the entire library (unless you use e.g. [&lt;code&gt;fog-aws&lt;/code&gt;, &lt;code&gt;fog-google&lt;/code&gt;] instead of fog proper). If you prefer to load fewer classes into your application, you need to load those parts of Fog yourself &lt;em&gt;before&lt;/em&gt; loading CarrierWave in your Gemfile. Ex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;gem &#34;fog&#34;, &#34;~&amp;gt; 1.27&#34;, require: &#34;fog/rackspace/storage&#34;&#xA;gem &#34;carrierwave&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A couple of notes about versions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This functionality was introduced in Fog v1.20.&lt;/li&gt; &#xA; &lt;li&gt;This functionality is slated for CarrierWave v1.0.0.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re not relying on Gemfile entries alone and are requiring &#34;carrierwave&#34; anywhere, ensure you require &#34;fog/rackspace/storage&#34; before it. Ex:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#34;fog/rackspace/storage&#34;&#xA;require &#34;carrierwave&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Beware that this specific require is only needed when working with a fog provider that was not extracted to its own gem yet. A list of the extracted providers can be found in the page of the &lt;code&gt;fog&lt;/code&gt; organizations &lt;a href=&#34;https://github.com/fog&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When in doubt, inspect &lt;code&gt;Fog.constants&lt;/code&gt; to see what has been loaded.&lt;/p&gt; &#xA;&lt;h2&gt;Dynamic Asset Host&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;asset_host&lt;/code&gt; config property can be assigned a proc (or anything that responds to &lt;code&gt;call&lt;/code&gt;) for generating the host dynamically. The proc-compliant object gets an instance of the current &lt;code&gt;CarrierWave::Storage::Fog::File&lt;/code&gt; or &lt;code&gt;CarrierWave::SanitizedFile&lt;/code&gt; as its only argument.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;CarrierWave.configure do |config|&#xA;  config.asset_host = proc do |file|&#xA;    identifier = # some logic&#xA;    &#34;http://#{identifier}.cdn.rackspacecloud.com&#34;&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using RMagick&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re uploading images, you&#39;ll probably want to manipulate them in some way, you might want to create thumbnail images for example. CarrierWave comes with a small library to make manipulating images with RMagick easier, you&#39;ll need to include it in your Uploader:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class AvatarUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  include CarrierWave::RMagick&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The RMagick module gives you a few methods, like &lt;code&gt;CarrierWave::RMagick#resize_to_fill&lt;/code&gt; which manipulate the image file in some way. You can set a &lt;code&gt;process&lt;/code&gt; callback, which will call that method any time a file is uploaded. There is a demonstration of convert here. Convert will only work if the file has the same file extension, thus the use of the filename method.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class AvatarUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  include CarrierWave::RMagick&#xA;&#xA;  process resize_to_fill: [200, 200]&#xA;  process convert: &#39;png&#39;&#xA;&#xA;  def filename&#xA;    super.chomp(File.extname(super)) + &#39;.png&#39; if original_filename.present?&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Check out the manipulate! method, which makes it easy for you to write your own manipulation methods.&lt;/p&gt; &#xA;&lt;h2&gt;Using MiniMagick&lt;/h2&gt; &#xA;&lt;p&gt;MiniMagick is similar to RMagick but performs all the operations using the &#39;convert&#39; CLI which is part of the standard ImageMagick kit. This allows you to have the power of ImageMagick without having to worry about installing all the RMagick libraries.&lt;/p&gt; &#xA;&lt;p&gt;See the MiniMagick site for more details:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/minimagick/minimagick&#34;&gt;https://github.com/minimagick/minimagick&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And the ImageMagick command line options for more for what&#39;s on offer:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.imagemagick.org/script/command-line-options.php&#34;&gt;http://www.imagemagick.org/script/command-line-options.php&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Currently, the MiniMagick carrierwave processor provides exactly the same methods as for the RMagick processor.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class AvatarUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  include CarrierWave::MiniMagick&#xA;&#xA;  process resize_to_fill: [200, 200]&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Migrating from Paperclip&lt;/h2&gt; &#xA;&lt;p&gt;If you are using Paperclip, you can use the provided compatibility module:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class AvatarUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  include CarrierWave::Compatibility::Paperclip&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the documentation for &lt;code&gt;CarrierWave::Compatibility::Paperclip&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Be sure to use mount_on to specify the correct column:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;mount_uploader :avatar, AvatarUploader, mount_on: :avatar_file_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;I18n&lt;/h2&gt; &#xA;&lt;p&gt;The Active Record validations use the Rails &lt;code&gt;i18n&lt;/code&gt; framework. Add these keys to your translations file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;errors:&#xA;  messages:&#xA;    carrierwave_processing_error: failed to be processed&#xA;    carrierwave_integrity_error: is not of an allowed file type&#xA;    carrierwave_download_error: could not be downloaded&#xA;    extension_allowlist_error: &#34;You are not allowed to upload %{extension} files, allowed types: %{allowed_types}&#34;&#xA;    extension_denylist_error: &#34;You are not allowed to upload %{extension} files, prohibited types: %{prohibited_types}&#34;&#xA;    content_type_allowlist_error: &#34;You are not allowed to upload %{content_type} files, allowed types: %{allowed_types}&#34;&#xA;    content_type_denylist_error: &#34;You are not allowed to upload %{content_type} files&#34;&#xA;    processing_error: &#34;Failed to manipulate, maybe it is not an image?&#34;&#xA;    min_size_error: &#34;File size should be greater than %{min_size}&#34;&#xA;    max_size_error: &#34;File size should be less than %{max_size}&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave-i18n&#34;&gt;&lt;code&gt;carrierwave-i18n&lt;/code&gt;&lt;/a&gt; library adds support for additional locales.&lt;/p&gt; &#xA;&lt;h2&gt;Large files&lt;/h2&gt; &#xA;&lt;p&gt;By default, CarrierWave copies an uploaded file twice, first copying the file into the cache, then copying the file into the store. For large files, this can be prohibitively time consuming.&lt;/p&gt; &#xA;&lt;p&gt;You may change this behavior by overriding either or both of the &lt;code&gt;move_to_cache&lt;/code&gt; and &lt;code&gt;move_to_store&lt;/code&gt; methods:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; CarrierWave::Uploader::Base&#xA;  def move_to_cache&#xA;    true&#xA;  end&#xA;&#xA;  def move_to_store&#xA;    true&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When the &lt;code&gt;move_to_cache&lt;/code&gt; and/or &lt;code&gt;move_to_store&lt;/code&gt; methods return true, files will be moved (instead of copied) to the cache and store respectively.&lt;/p&gt; &#xA;&lt;p&gt;This has only been tested with the local filesystem store.&lt;/p&gt; &#xA;&lt;h2&gt;Skipping ActiveRecord callbacks&lt;/h2&gt; &#xA;&lt;p&gt;By default, mounting an uploader into an ActiveRecord model will add a few callbacks. For example, this code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class User&#xA;  mount_uploader :avatar, AvatarUploader&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Will add these callbacks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;before_save :write_avatar_identifier&#xA;after_save :store_previous_changes_for_avatar&#xA;after_commit :remove_avatar!, on: :destroy&#xA;after_commit :mark_remove_avatar_false, on: :update&#xA;after_commit :remove_previously_stored_avatar, on: :update&#xA;after_commit :store_avatar!, on: [:create, :update]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to skip any of these callbacks (eg. you want to keep the existing avatar, even after uploading a new one), you can use ActiveRecord‚Äôs &lt;code&gt;skip_callback&lt;/code&gt; method.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class User&#xA;  mount_uploader :avatar, AvatarUploader&#xA;  skip_callback :commit, :after, :remove_previously_stored_avatar&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Uploader Callbacks&lt;/h2&gt; &#xA;&lt;p&gt;In addition to the ActiveRecord callbacks described above, uploaders also have callbacks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;class MyUploader &amp;lt; ::CarrierWave::Uploader::Base&#xA;  before :remove, :log_removal&#xA;  private&#xA;  def log_removal&#xA;    ::Rails.logger.info(format(&#39;Deleting file on S3: %s&#39;, @file))&#xA;  end&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Uploader callbacks can be &lt;code&gt;before&lt;/code&gt; or &lt;code&gt;after&lt;/code&gt; the following events:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cache&#xA;process&#xA;remove&#xA;retrieve_from_cache&#xA;store&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing to CarrierWave&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/carrierwaveuploader/carrierwave/raw/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The MIT License (MIT)&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2008-2015 Jonas Nicklas&lt;/p&gt; &#xA;&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &#34;Software&#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt; &#xA;&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;</summary>
  </entry>
</feed>