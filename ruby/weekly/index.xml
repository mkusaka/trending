<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Ruby Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-14T01:58:28Z</updated>
  <subtitle>Weekly Trending of Ruby in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>patterns-ai-core/langchainrb</title>
    <updated>2024-04-14T01:58:28Z</updated>
    <id>tag:github.com,2024-04-14:/patterns-ai-core/langchainrb</id>
    <link href="https://github.com/patterns-ai-core/langchainrb" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build LLM-backed Ruby applications&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;üíéüîó Langchain.rb&lt;/h2&gt; &#xA;&lt;p&gt;‚ö° Building LLM-powered applications in Ruby ‚ö°&lt;/p&gt; &#xA;&lt;p&gt;For deep Rails integration see: &lt;a href=&#34;https://github.com/andreibondarev/langchainrb_rails&#34;&gt;langchainrb_rails&lt;/a&gt; gem.&lt;/p&gt; &#xA;&lt;p&gt;Available for paid consulting engagements! &lt;a href=&#34;mailto:andrei@sourcelabs.io&#34;&gt;Email me&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/andreibondarev/langchainrb/actions/workflows/ci.yml/badge.svg?branch=main&#34; alt=&#34;Tests status&#34;&gt; &lt;a href=&#34;https://badge.fury.io/rb/langchainrb&#34;&gt;&lt;img src=&#34;https://badge.fury.io/rb/langchainrb.svg?sanitize=true&#34; alt=&#34;Gem Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://rubydoc.info/gems/langchainrb&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/yard-docs-blue.svg?sanitize=true&#34; alt=&#34;Docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/andreibondarev/langchainrb/raw/main/LICENSE.txt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/WDARp7J2n8&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/WDARp7J2n8?compact=true&amp;amp;style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/rushing_andrei&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&amp;amp;label=Follow%20%40rushing_andrei&#34; alt=&#34;X&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Use Cases&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Retrieval Augmented Generation (RAG) and vector search&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#assistants&#34;&gt;Assistants&lt;/a&gt; (chat bots)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#large-language-models-llms&#34;&gt;Large Language Models (LLMs)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#prompt-management&#34;&gt;Prompt Management&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#output-parsers&#34;&gt;Output Parsers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#building-retrieval-augment-generation-rag-system&#34;&gt;Building RAG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#assistants&#34;&gt;Assistants&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#evaluations-evals&#34;&gt;Evaluations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#logging&#34;&gt;Logging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#problems&#34;&gt;Problems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#development&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/patterns-ai-core/langchainrb/main/#discord&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install the gem and add to the application&#39;s Gemfile by executing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bundle add langchainrb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If bundler is not being used to manage dependencies, install the gem by executing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gem install langchainrb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additional gems may be required. They&#39;re not included by default so you can include only what you need.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;require &#34;langchain&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Large Language Models (LLMs)&lt;/h2&gt; &#xA;&lt;p&gt;Langchain.rb wraps supported LLMs in a unified interface allowing you to easily swap out and test out different models.&lt;/p&gt; &#xA;&lt;h4&gt;Supported LLMs and features:&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;LLM providers&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;embed()&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;complete()&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;chat()&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;summarize()&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openai.com/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;OpenAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Including Azure OpenAI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai21.com/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;AI21&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://anthropic.com/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Anthropic&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/bedrock?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;AWS Bedrock&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Provides AWS, Cohere, AI21, Antropic and Stability AI models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cohere.com/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Cohere&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.google/discover/palm2?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;GooglePalm&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cloud.google.com/vertex-ai?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Google Vertex AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mistral.ai/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Mistral AI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ollama.ai/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Ollama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://replicate.com/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Replicate&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Using standalone LLMs:&lt;/h4&gt; &#xA;&lt;h4&gt;OpenAI&lt;/h4&gt; &#xA;&lt;p&gt;Add &lt;code&gt;gem &#34;ruby-openai&#34;, &#34;~&amp;gt; 6.3.0&#34;&lt;/code&gt; to your Gemfile.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;llm = Langchain::LLM::OpenAI.new(api_key: ENV[&#34;OPENAI_API_KEY&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can pass additional parameters to the constructor, it will be passed to the OpenAI client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;llm = Langchain::LLM::OpenAI.new(api_key: ENV[&#34;OPENAI_API_KEY&#34;], llm_options: { ... })&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generate vector embeddings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;llm.embed(text: &#34;foo bar&#34;).embedding&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generate a chat completion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;llm.chat(messages: [{role: &#34;user&#34;, content: &#34;What is the meaning of life?&#34;}]).completion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Summarize the text:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;llm.summarize(text: &#34;...&#34;).completion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use any other LLM by invoking the same interface:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;llm = Langchain::LLM::GooglePalm.new(api_key: ENV[&#34;GOOGLE_PALM_API_KEY&#34;], default_options: { ... })&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prompt Management&lt;/h3&gt; &#xA;&lt;h4&gt;Prompt Templates&lt;/h4&gt; &#xA;&lt;p&gt;Create a prompt with input variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;prompt = Langchain::Prompt::PromptTemplate.new(template: &#34;Tell me a {adjective} joke about {content}.&#34;, input_variables: [&#34;adjective&#34;, &#34;content&#34;])&#xA;prompt.format(adjective: &#34;funny&#34;, content: &#34;chickens&#34;) # &#34;Tell me a funny joke about chickens.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Creating a PromptTemplate using just a prompt and no input_variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;prompt = Langchain::Prompt::PromptTemplate.from_template(&#34;Tell me a funny joke about chickens.&#34;)&#xA;prompt.input_variables # []&#xA;prompt.format # &#34;Tell me a funny joke about chickens.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Save prompt template to JSON file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;prompt.save(file_path: &#34;spec/fixtures/prompt/prompt_template.json&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Loading a new prompt template using a JSON file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;prompt = Langchain::Prompt.load_from_path(file_path: &#34;spec/fixtures/prompt/prompt_template.json&#34;)&#xA;prompt.input_variables # [&#34;adjective&#34;, &#34;content&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Few Shot Prompt Templates&lt;/h4&gt; &#xA;&lt;p&gt;Create a prompt with a few shot examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;prompt = Langchain::Prompt::FewShotPromptTemplate.new(&#xA;  prefix: &#34;Write antonyms for the following words.&#34;,&#xA;  suffix: &#34;Input: {adjective}\nOutput:&#34;,&#xA;  example_prompt: Langchain::Prompt::PromptTemplate.new(&#xA;    input_variables: [&#34;input&#34;, &#34;output&#34;],&#xA;    template: &#34;Input: {input}\nOutput: {output}&#34;&#xA;  ),&#xA;  examples: [&#xA;    { &#34;input&#34;: &#34;happy&#34;, &#34;output&#34;: &#34;sad&#34; },&#xA;    { &#34;input&#34;: &#34;tall&#34;, &#34;output&#34;: &#34;short&#34; }&#xA;  ],&#xA;   input_variables: [&#34;adjective&#34;]&#xA;)&#xA;&#xA;prompt.format(adjective: &#34;good&#34;)&#xA;&#xA;# Write antonyms for the following words.&#xA;#&#xA;# Input: happy&#xA;# Output: sad&#xA;#&#xA;# Input: tall&#xA;# Output: short&#xA;#&#xA;# Input: good&#xA;# Output:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Save prompt template to JSON file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;prompt.save(file_path: &#34;spec/fixtures/prompt/few_shot_prompt_template.json&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Loading a new prompt template using a JSON file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;prompt = Langchain::Prompt.load_from_path(file_path: &#34;spec/fixtures/prompt/few_shot_prompt_template.json&#34;)&#xA;prompt.prefix # &#34;Write antonyms for the following words.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Loading a new prompt template using a YAML file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;prompt = Langchain::Prompt.load_from_path(file_path: &#34;spec/fixtures/prompt/prompt_template.yaml&#34;)&#xA;prompt.input_variables #=&amp;gt; [&#34;adjective&#34;, &#34;content&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Output Parsers&lt;/h3&gt; &#xA;&lt;p&gt;Parse LLM text responses into structured output, such as JSON.&lt;/p&gt; &#xA;&lt;h4&gt;Structured Output Parser&lt;/h4&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;StructuredOutputParser&lt;/code&gt; to generate a prompt that instructs the LLM to provide a JSON response adhering to a specific JSON schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;json_schema = {&#xA;  type: &#34;object&#34;,&#xA;  properties: {&#xA;    name: {&#xA;      type: &#34;string&#34;,&#xA;      description: &#34;Persons name&#34;&#xA;    },&#xA;    age: {&#xA;      type: &#34;number&#34;,&#xA;      description: &#34;Persons age&#34;&#xA;    },&#xA;    interests: {&#xA;      type: &#34;array&#34;,&#xA;      items: {&#xA;        type: &#34;object&#34;,&#xA;        properties: {&#xA;          interest: {&#xA;            type: &#34;string&#34;,&#xA;            description: &#34;A topic of interest&#34;&#xA;          },&#xA;          levelOfInterest: {&#xA;            type: &#34;number&#34;,&#xA;            description: &#34;A value between 0 and 100 of how interested the person is in this interest&#34;&#xA;          }&#xA;        },&#xA;        required: [&#34;interest&#34;, &#34;levelOfInterest&#34;],&#xA;        additionalProperties: false&#xA;      },&#xA;      minItems: 1,&#xA;      maxItems: 3,&#xA;      description: &#34;A list of the person&#39;s interests&#34;&#xA;    }&#xA;  },&#xA;  required: [&#34;name&#34;, &#34;age&#34;, &#34;interests&#34;],&#xA;  additionalProperties: false&#xA;}&#xA;parser = Langchain::OutputParsers::StructuredOutputParser.from_json_schema(json_schema)&#xA;prompt = Langchain::Prompt::PromptTemplate.new(template: &#34;Generate details of a fictional character.\n{format_instructions}\nCharacter description: {description}&#34;, input_variables: [&#34;description&#34;, &#34;format_instructions&#34;])&#xA;prompt_text = prompt.format(description: &#34;Korean chemistry student&#34;, format_instructions: parser.get_format_instructions)&#xA;# Generate details of a fictional character.&#xA;# You must format your output as a JSON value that adheres to a given &#34;JSON Schema&#34; instance.&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then parse the llm response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;llm = Langchain::LLM::OpenAI.new(api_key: ENV[&#34;OPENAI_API_KEY&#34;])&#xA;llm_response = llm.chat(messages: [{role: &#34;user&#34;, content: prompt_text}]).completion&#xA;parser.parse(llm_response)&#xA;# {&#xA;#   &#34;name&#34; =&amp;gt; &#34;Kim Ji-hyun&#34;,&#xA;#   &#34;age&#34; =&amp;gt; 22,&#xA;#   &#34;interests&#34; =&amp;gt; [&#xA;#     {&#xA;#       &#34;interest&#34; =&amp;gt; &#34;Organic Chemistry&#34;,&#xA;#       &#34;levelOfInterest&#34; =&amp;gt; 85&#xA;#     },&#xA;#     ...&#xA;#   ]&#xA;# }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the parser fails to parse the LLM response, you can use the &lt;code&gt;OutputFixingParser&lt;/code&gt;. It sends an error message, prior output, and the original prompt text to the LLM, asking for a &#34;fixed&#34; response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;begin&#xA;  parser.parse(llm_response)&#xA;rescue Langchain::OutputParsers::OutputParserException =&amp;gt; e&#xA;  fix_parser = Langchain::OutputParsers::OutputFixingParser.from_llm(&#xA;    llm: llm,&#xA;    parser: parser&#xA;  )&#xA;  fix_parser.parse(llm_response)&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, if you don&#39;t need to handle the &lt;code&gt;OutputParserException&lt;/code&gt;, you can simplify the code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# we already have the `OutputFixingParser`:&#xA;# parser = Langchain::OutputParsers::StructuredOutputParser.from_json_schema(json_schema)&#xA;fix_parser = Langchain::OutputParsers::OutputFixingParser.from_llm(&#xA;  llm: llm,&#xA;  parser: parser&#xA;)&#xA;fix_parser.parse(llm_response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/andreibondarev/langchainrb/tree/main/examples/create_and_manage_prompt_templates_using_structured_output_parser.rb&#34;&gt;here&lt;/a&gt; for a concrete example&lt;/p&gt; &#xA;&lt;h2&gt;Building Retrieval Augment Generation (RAG) system&lt;/h2&gt; &#xA;&lt;p&gt;RAG is a methodology that assists LLMs generate accurate and up-to-date information. A typical RAG workflow follows the 3 steps below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Relevant knowledge (or data) is retrieved from the knowledge base (typically a vector search DB)&lt;/li&gt; &#xA; &lt;li&gt;A prompt, containing retrieved knowledge above, is constructed.&lt;/li&gt; &#xA; &lt;li&gt;LLM receives the prompt above to generate a text completion. Most common use-case for a RAG system is powering Q&amp;amp;A systems where users pose natural language questions and receive answers in natural language.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Vector search databases&lt;/h3&gt; &#xA;&lt;p&gt;Langchain.rb provides a convenient unified interface on top of supported vectorsearch databases that make it easy to configure your index, add data, query and retrieve from it.&lt;/p&gt; &#xA;&lt;h4&gt;Supported vector search databases and features:&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Database&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Open-source&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Cloud offering&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://trychroma.com/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Chroma&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://epsilla.com/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Epsilla&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nmslib/hnswlib/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Hnswlib&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://milvus.io/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Milvus&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ Zilliz Cloud&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.pinecone.io/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Pinecone&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/pgvector/pgvector/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Pgvector&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://qdrant.tech/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Qdrant&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://weaviate.io/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Weaviate&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.elastic.co/?utm_source=langchainrb&amp;amp;utm_medium=github&#34;&gt;Elasticsearch&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Using Vector Search Databases üîç&lt;/h3&gt; &#xA;&lt;p&gt;Pick the vector search database you&#39;ll be using, add the gem dependency and instantiate the client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;gem &#34;weaviate-ruby&#34;, &#34;~&amp;gt; 0.8.9&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Choose and instantiate the LLM provider you&#39;ll be using to generate embeddings&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;llm = Langchain::LLM::OpenAI.new(api_key: ENV[&#34;OPENAI_API_KEY&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;client = Langchain::Vectorsearch::Weaviate.new(&#xA;    url: ENV[&#34;WEAVIATE_URL&#34;],&#xA;    api_key: ENV[&#34;WEAVIATE_API_KEY&#34;],&#xA;    index_name: &#34;Documents&#34;,&#xA;    llm: llm&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can instantiate any other supported vector search database:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;client = Langchain::Vectorsearch::Chroma.new(...)   # `gem &#34;chroma-db&#34;, &#34;~&amp;gt; 0.6.0&#34;`&#xA;client = Langchain::Vectorsearch::Epsilla.new(...)  # `gem &#34;epsilla-ruby&#34;, &#34;~&amp;gt; 0.0.3&#34;`&#xA;client = Langchain::Vectorsearch::Hnswlib.new(...)  # `gem &#34;hnswlib&#34;, &#34;~&amp;gt; 0.8.1&#34;`&#xA;client = Langchain::Vectorsearch::Milvus.new(...)   # `gem &#34;milvus&#34;, &#34;~&amp;gt; 0.9.2&#34;`&#xA;client = Langchain::Vectorsearch::Pinecone.new(...) # `gem &#34;pinecone&#34;, &#34;~&amp;gt; 0.1.6&#34;`&#xA;client = Langchain::Vectorsearch::Pgvector.new(...) # `gem &#34;pgvector&#34;, &#34;~&amp;gt; 0.2&#34;`&#xA;client = Langchain::Vectorsearch::Qdrant.new(...)   # `gem &#34;qdrant-ruby&#34;, &#34;~&amp;gt; 0.9.3&#34;`&#xA;client = Langchain::Vectorsearch::Elasticsearch.new(...)   # `gem &#34;elasticsearch&#34;, &#34;~&amp;gt; 8.2.0&#34;`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create the default schema:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;client.create_default_schema&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add plain text data to your vector search database:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;client.add_texts(&#xA;  texts: [&#xA;    &#34;Begin by preheating your oven to 375¬∞F (190¬∞C). Prepare four boneless, skinless chicken breasts by cutting a pocket into the side of each breast, being careful not to cut all the way through. Season the chicken with salt and pepper to taste. In a large skillet, melt 2 tablespoons of unsalted butter over medium heat. Add 1 small diced onion and 2 minced garlic cloves, and cook until softened, about 3-4 minutes. Add 8 ounces of fresh spinach and cook until wilted, about 3 minutes. Remove the skillet from heat and let the mixture cool slightly.&#34;,&#xA;      &#34;In a bowl, combine the spinach mixture with 4 ounces of softened cream cheese, 1/4 cup of grated Parmesan cheese, 1/4 cup of shredded mozzarella cheese, and 1/4 teaspoon of red pepper flakes. Mix until well combined. Stuff each chicken breast pocket with an equal amount of the spinach mixture. Seal the pocket with a toothpick if necessary. In the same skillet, heat 1 tablespoon of olive oil over medium-high heat. Add the stuffed chicken breasts and sear on each side for 3-4 minutes, or until golden brown.&#34;&#xA;  ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use the file parsers to load, parse and index data into your database:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;my_pdf = Langchain.root.join(&#34;path/to/my.pdf&#34;)&#xA;my_text = Langchain.root.join(&#34;path/to/my.txt&#34;)&#xA;my_docx = Langchain.root.join(&#34;path/to/my.docx&#34;)&#xA;&#xA;client.add_data(paths: [my_pdf, my_text, my_docx])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Supported file formats: docx, html, pdf, text, json, jsonl, csv, xlsx, eml.&lt;/p&gt; &#xA;&lt;p&gt;Retrieve similar documents based on the query string passed in:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;client.similarity_search(&#xA;  query:,&#xA;  k:       # number of results to be retrieved&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve similar documents based on the query string passed in via the &lt;a href=&#34;https://arxiv.org/abs/2212.10496&#34;&gt;HyDE technique&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;client.similarity_search_with_hyde()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve similar documents based on the embedding passed in:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;client.similarity_search_by_vector(&#xA;  embedding:,&#xA;  k:       # number of results to be retrieved&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;RAG-based querying&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;client.ask(question: &#34;...&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Assistants&lt;/h2&gt; &#xA;&lt;p&gt;Assistants are Agent-like objects that leverage helpful instructions, LLMs, tools and knowledge to respond to user queries. Assistants can be configured with an LLM of your choice (currently only OpenAI), any vector search database and easily extended with additional tools.&lt;/p&gt; &#xA;&lt;h3&gt;Available Tools üõ†Ô∏è&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ENV Requirements&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Gem Requirements&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;calculator&#34;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Useful for getting the result of a math expression&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;gem &#34;eqn&#34;, &#34;~&amp;gt; 1.6.5&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;database&#34;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Useful for querying a SQL database&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;gem &#34;sequel&#34;, &#34;~&amp;gt; 5.68.0&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;file_system&#34;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Interacts with the file system&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;ruby_code_interpreter&#34;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Interprets Ruby expressions&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;gem &#34;safe_ruby&#34;, &#34;~&amp;gt; 1.0.4&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;google_search&#34;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A wrapper around Google Search&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ENV[&#34;SERPAPI_API_KEY&#34;]&lt;/code&gt; (&lt;a href=&#34;https://serpapi.com/manage-api-key&#34;&gt;https://serpapi.com/manage-api-key&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;gem &#34;google_search_results&#34;, &#34;~&amp;gt; 2.0.0&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;weather&#34;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Calls Open Weather API to retrieve the current weather&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;ENV[&#34;OPEN_WEATHER_API_KEY&#34;]&lt;/code&gt; (&lt;a href=&#34;https://home.openweathermap.org/api_keys&#34;&gt;https://home.openweathermap.org/api_keys&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;gem &#34;open-weather-ruby-client&#34;, &#34;~&amp;gt; 0.3.0&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&#34;wikipedia&#34;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Calls Wikipedia API to retrieve the summary&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;gem &#34;wikipedia-client&#34;, &#34;~&amp;gt; 1.17.0&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Demos&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/83aa4fd8dccb492aad4ca95da40ed0b2&#34;&gt;Building an AI Assistant that operates a simulated E-commerce Store&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.loom.com/share/e883a4a49b8746c1b0acf9d58cf6da36&#34;&gt;New Langchain.rb Assistants interface&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Creating an Assistant&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Instantiate an LLM of your choice&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;llm = Langchain::LLM::OpenAI.new(api_key: ENV[&#34;OPENAI_API_KEY&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Instantiate a Thread. Threads keep track of the messages in the Assistant conversation.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;thread = Langchain::Thread.new&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can pass old message from previously using the Assistant:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;thread.messages = messages&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Messages contain the conversation history and the whole message history is sent to the LLM every time. A Message belongs to 1 of the 4 roles:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Message(role: &#34;system&#34;)&lt;/code&gt; message usually contains the instructions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Message(role: &#34;user&#34;)&lt;/code&gt; messages come from the user.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Message(role: &#34;assistant&#34;)&lt;/code&gt; messages are produced by the LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Message(role: &#34;tool&#34;)&lt;/code&gt; messages are sent in response to tool calls with tool outputs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Instantiate an Assistant&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;assistant = Langchain::Assistant.new(&#xA;  llm: llm,&#xA;  thread: thread,&#xA;  instructions: &#34;You are a Meteorologist Assistant that is able to pull the weather for any location&#34;,&#xA;  tools: [&#xA;    Langchain::Tool::GoogleSearch.new(api_key: ENV[&#34;SERPAPI_API_KEY&#34;])&#xA;  ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using an Assistant&lt;/h3&gt; &#xA;&lt;p&gt;You can now add your message to an Assistant.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;assistant.add_message content: &#34;What&#39;s the weather in New York City?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the Assistant to generate a response.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;assistant.run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If a Tool is invoked you can manually submit an output.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;assistant.submit_tool_output tool_call_id: &#34;...&#34;, output: &#34;It&#39;s 70 degrees and sunny in New York City&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or run the assistant with &lt;code&gt;auto_tool_execution: tool&lt;/code&gt; to call Tools automatically.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;assistant.add_message content: &#34;How about San Diego, CA?&#34;&#xA;assistant.run(auto_tool_execution: true)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also combine the two by calling:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;assistant.add_message_and_run content: &#34;What about Sacramento, CA?&#34;, auto_tool_execution: true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Accessing Thread messages&lt;/h3&gt; &#xA;&lt;p&gt;You can access the messages in a Thread by calling &lt;code&gt;assistant.thread.messages&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;assistant.thread.messages&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Assistant checks the context window limits before every request to the LLM and remove oldest thread messages one by one if the context window is exceeded.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations (Evals)&lt;/h2&gt; &#xA;&lt;p&gt;The Evaluations module is a collection of tools that can be used to evaluate and track the performance of the output products by LLM and your RAG (Retrieval Augmented Generation) pipelines.&lt;/p&gt; &#xA;&lt;h3&gt;RAGAS&lt;/h3&gt; &#xA;&lt;p&gt;Ragas helps you evaluate your Retrieval Augmented Generation (RAG) pipelines. The implementation is based on this &lt;a href=&#34;https://arxiv.org/abs/2309.15217&#34;&gt;paper&lt;/a&gt; and the original Python &lt;a href=&#34;https://github.com/explodinggradients/ragas&#34;&gt;repo&lt;/a&gt;. Ragas tracks the following 3 metrics and assigns the 0.0 - 1.0 scores:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Faithfulness - the answer is grounded in the given context.&lt;/li&gt; &#xA; &lt;li&gt;Context Relevance - the retrieved context is focused, containing little to no irrelevant information.&lt;/li&gt; &#xA; &lt;li&gt;Answer Relevance - the generated answer addresses the actual question that was provided.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;# We recommend using Langchain::LLM::OpenAI as your llm for Ragas&#xA;ragas = Langchain::Evals::Ragas::Main.new(llm: llm)&#xA;&#xA;# The answer that the LLM generated&#xA;# The question (or the original prompt) that was asked&#xA;# The context that was retrieved (usually from a vectorsearch database)&#xA;ragas.score(answer: &#34;&#34;, question: &#34;&#34;, context: &#34;&#34;)&#xA;# =&amp;gt;&#xA;# {&#xA;#   ragas_score: 0.6601257446503674,&#xA;#   answer_relevance_score: 0.9573145866787608,&#xA;#   context_relevance_score: 0.6666666666666666,&#xA;#   faithfulness_score: 0.5&#xA;# }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Additional examples available: &lt;a href=&#34;https://github.com/andreibondarev/langchainrb/tree/main/examples&#34;&gt;/examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Logging&lt;/h2&gt; &#xA;&lt;p&gt;Langchain.rb uses standard logging mechanisms and defaults to &lt;code&gt;:warn&lt;/code&gt; level. Most messages are at info level, but we will add debug or warn statements as needed. To show all log messages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ruby&#34;&gt;Langchain.logger.level = :debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Problems&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re having issues installing &lt;code&gt;unicode&lt;/code&gt; gem required by &lt;code&gt;pragmatic_segmenter&lt;/code&gt;, try running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gem install unicode -- --with-cflags=&#34;-Wno-incompatible-function-pointer-types&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/andreibondarev/langchainrb.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cp .env.example .env&lt;/code&gt;, then fill out the environment variables in &lt;code&gt;.env&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bundle exec rake&lt;/code&gt; to ensure that the tests pass and to run standardrb&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bin/console&lt;/code&gt; to load the gem in a REPL session. Feel free to add your own instances of LLMs, Tools, Agents, etc. and experiment with them.&lt;/li&gt; &#xA; &lt;li&gt;Optionally, install lefthook git hooks for pre-commit to auto lint: &lt;code&gt;gem install lefthook &amp;amp;&amp;amp; lefthook install -f&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Discord&lt;/h2&gt; &#xA;&lt;p&gt;Join us in the &lt;a href=&#34;https://discord.gg/WDARp7J2n8&#34;&gt;Langchain.rb&lt;/a&gt; Discord server.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#andreibondarev/langchainrb&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=andreibondarev/langchainrb&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Bug reports and pull requests are welcome on GitHub at &lt;a href=&#34;https://github.com/andreibondarev/langchainrb&#34;&gt;https://github.com/andreibondarev/langchainrb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The gem is available as open source under the terms of the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>