<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Ruby Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-25T01:40:39Z</updated>
  <subtitle>Daily Trending of Ruby in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fe1ixxu/ALMA</title>
    <updated>2024-01-25T01:40:39Z</updated>
    <id>tag:github.com,2024-01-25:/fe1ixxu/ALMA</id>
    <link href="https://github.com/fe1ixxu/ALMA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is repository for ALMA translation models.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;ALMA&#34; src=&#34;https://raw.githubusercontent.com/fe1ixxu/ALMA/master/alma_logo.png&#34; width=&#34;500&#34; height=&#34;203&#34;&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;ALMA: Advanced Language Model-based translator&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/fe1ixxu/ALMA/master/LICENSE&#34; alt=&#34;MIT License&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-FAD689.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.11674&#34; alt=&#34;ALMA paper&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ALMA-Paper-D9AB42&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.08417&#34; alt=&#34;ALMA-R paper&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ALMA--R-Paper-F6C555&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;https://notes.aimodels.fyi/alma-a-new-training-method-that-boosts-translation-performance-for-large-language-models/&#34;&gt;&lt;img alt=&#34;Summary Link&#34; src=&#34;https://img.shields.io/badge/summary-link-F6C555&#34; /&gt;&lt;/a&gt; --&gt; &lt;a href=&#34;https://www.clsp.jhu.edu/&#34; alt=&#34;jhu&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Johns_Hopkins_University-BEC23F&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/&#34; alt=&#34;MSlogo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Microsoft-B1B479?logo=microsoft&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/fe1ixxu&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/haoranxu?style=social&amp;amp;logo=twitter&#34; alt=&#34;follow on Twitter&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ALMA&lt;/strong&gt; (&lt;strong&gt;A&lt;/strong&gt;dvanced &lt;strong&gt;L&lt;/strong&gt;anguage &lt;strong&gt;M&lt;/strong&gt;odel-based Tr&lt;strong&gt;A&lt;/strong&gt;nslator) is a many-to-many LLM-based translation model, which adopts a new translation model paradigm: it begins with fine-tuning on monolingual data and is further optimized using high-quality parallel data. This two-step fine-tuning process ensures strong translation performance.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.08417v2.pdf&#34;&gt;ALMA-R&lt;/a&gt; (NEW!)&lt;/strong&gt; builds upon ALMA models, with further LoRA fine-tuning with our proposed &lt;strong&gt;Contrastive Preference Optimization (CPO)&lt;/strong&gt; as opposed to the Supervised Fine-tuning used in ALMA. CPO fine-tuning requires our &lt;a href=&#34;https://huggingface.co/datasets/haoranxu/ALMA-R-Preference&#34;&gt;triplet preference data&lt;/a&gt; for preference learning. ALMA-R now can matches or even exceeds GPT-4 or WMT winners!&lt;/h3&gt; &#xA;&lt;p&gt;The original ALMA repository can be found &lt;a href=&#34;https://github.com/fe1ixxu/ALMA/tree/a3cc7877752779346312bb07798172eadc83d692&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;News üåü&lt;/h1&gt; &#xA;&lt;p&gt;‚≠ê Jan.16 2024 &lt;strong&gt;ALMA-R&lt;/strong&gt; is Released! Please check more details with our new paper: &lt;a href=&#34;https://arxiv.org/abs/2401.08417&#34;&gt;Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚≠ê Jan.16 2024 The ALMA paper: &lt;a href=&#34;https://arxiv.org/abs/2309.11674&#34;&gt;A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models&lt;/a&gt; has been accepted at ICLR 2024! Check out more details &lt;a href=&#34;https://openreview.net/forum?id=farT6XXntP&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;Contents üìÑ&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fe1ixxu/ALMA/master/#download-alma-r-models-and-dataset-&#34;&gt;Download ALMA(-R) Models and Dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fe1ixxu/ALMA/master/#environment-setup-&#34;&gt;Environment Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fe1ixxu/ALMA/master/#evaluation-&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fe1ixxu/ALMA/master/#training-&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fe1ixxu/ALMA/master/#data-information-&#34;&gt;Data Information&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/fe1ixxu/ALMA/master/#faqs-&#34;&gt;FAQs&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;span&gt;‚≠ê&lt;/span&gt; Supports &lt;span&gt;‚≠ê&lt;/span&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AMD and Nvidia Cards&lt;/li&gt; &#xA; &lt;li&gt;Data Parallel Evaluation&lt;/li&gt; &#xA; &lt;li&gt;Also support LLaMA-1, LLaMA-2, OPT, Faclon, BLOOM, MPT&lt;/li&gt; &#xA; &lt;li&gt;LoRA Fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;Monolingual data fine-tuning, parallel data fine-tuning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/fe1ixxu/ALMA/master/almar.png&#34; width=&#34;700&#34; height=&#34;560&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Download ALMA(-R) Models and Dataset üöÄ&lt;/h1&gt; &#xA;&lt;p&gt;We release six translation models presented in the paper:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ALMA-7B&lt;/li&gt; &#xA; &lt;li&gt;ALMA-7B-LoRA&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ALMA-7B-R (NEW!)&lt;/strong&gt;: Further LoRA fine-tuning upon ALMA-7B-LoRA with contrastive preference optimization.&lt;/li&gt; &#xA; &lt;li&gt;ALMA-13B&lt;/li&gt; &#xA; &lt;li&gt;ALMA-13B-LoRA&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ALMA-13B-R (NEW!)&lt;/strong&gt;: Further LoRA fine-tuning upon ALMA-13B-LoRA with contrastive preference optimization (BEST MODEL!).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;We have also provided the WMT&#39;22 and WMT&#39;23 translation outputs from ALMA-13B-LoRA and ALMA-13B-R in the &lt;code&gt;outputs&lt;/code&gt; directory. These outputs also includes our outputs of baselines and can be directly accessed and used for subsequent evaluations.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Model checkpoints are released at huggingface:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Base Model Link&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;LoRA Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALMA-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/haoranxu/ALMA-7B&#34;&gt;haoranxu/ALMA-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALMA-7B-LoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/haoranxu/ALMA-7B-Pretrain&#34;&gt;haoranxu/ALMA-7B-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/haoranxu/ALMA-7B-Pretrain-LoRA&#34;&gt;haoranxu/ALMA-7B-Pretrain-LoRA&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;ALMA-7B-R (NEW!)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/haoranxu/ALMA-7B-R&#34;&gt;haoranxu/ALMA-7B-R (LoRA merged)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/haoranxu/ALMA-13B&#34;&gt;haoranxu/ALMA-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ALMA-13B-LoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/haoranxu/ALMA-13B-Pretrain&#34;&gt;haoranxu/ALMA-13B-Pretrain&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/haoranxu/ALMA-13B-Pretrain-LoRA&#34;&gt;haoranxu/ALMA-13B-Pretrain-LoRA&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;ALMA-13B-R (NEW!)&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/haoranxu/ALMA-13B-R&#34;&gt;haoranxu/ALMA-13B-R (LoRA merged)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note that &lt;code&gt;ALMA-7B-Pretrain&lt;/code&gt; and &lt;code&gt;ALMA-13B-Pretrain&lt;/code&gt; are NOT translation models. They only experience stage 1 monolingual fine-tuning (20B tokens for the 7B model and 12B tokens for the 13B model), and should be utilized in conjunction with their LoRA models.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Datasets used by ALMA and ALMA-R are also released at huggingface now (NEW!)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Datasets&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Train / Validation&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Test&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Human-Written Parallel Data (ALMA)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/haoranxu/ALMA-Human-Parallel&#34;&gt;train and validation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/haoranxu/WMT22-Test&#34;&gt;WMT&#39;22&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Triplet Preference Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/haoranxu/ALMA-R-Preference&#34;&gt;train&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/haoranxu/WMT22-Test&#34;&gt;WMT&#39;22&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/datasets/haoranxu/WMT23-Test&#34;&gt;WMT&#39;23&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;A quick start to use our best system (ALMA-13B-R) for translation. An example of translating &#34;ÊàëÁà±Êú∫Âô®ÁøªËØë„ÄÇ&#34; into English:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;from transformers import AutoModelForCausalLM&#xA;from transformers import AutoTokenizer&#xA;&#xA;# Load base model and LoRA weights&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;haoranxu/ALMA-13B-R&#34;, torch_dtype=torch.float16, device_map=&#34;auto&#34;)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;haoranxu/ALMA-13B-R&#34;, padding_side=&#39;left&#39;)&#xA;&#xA;# Add the source sentence into the prompt template&#xA;prompt=&#34;Translate this from Chinese to English:\nChinese: ÊàëÁà±Êú∫Âô®ÁøªËØë„ÄÇ\nEnglish:&#34;&#xA;input_ids = tokenizer(prompt, return_tensors=&#34;pt&#34;, padding=True, max_length=40, truncation=True).input_ids.cuda()&#xA;&#xA;# Translation&#xA;with torch.no_grad():&#xA;    generated_ids = model.generate(input_ids=input_ids, num_beams=5, max_new_tokens=20, do_sample=True, temperature=0.6, top_p=0.9)&#xA;outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)&#xA;print(outputs)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The general translation prompt is:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Translate this from &amp;lt;source language name&amp;gt; into &amp;lt;target language name&amp;gt;:&#xA;&amp;lt;source language name&amp;gt;: &amp;lt;source language sentence&amp;gt;&#xA;&amp;lt;target language name&amp;gt;:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Environment Setup üîß&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n alma-r python=3.11&#xA;conda activate alma-r&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use &lt;strong&gt;Nvidia GPUs&lt;/strong&gt;, install torch with cuda 11.8&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you use &lt;strong&gt;AMD GPUs&lt;/strong&gt;, install torch with ROCm 5.6&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then install other dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash install_alma.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Evaluation üíª&lt;/h1&gt; &#xA;&lt;h3&gt;Evaluation on ALMA-13B-R&lt;/h3&gt; &#xA;&lt;p&gt;This is a quick start to evaluate our ALMA-13B-R model. To produce translation outputs for WMT&#39;22 in both en‚Üícs and cs‚Üíen directions (If you want to evaluate WMT&#39;23 instead, simply pass &lt;code&gt;--override_test_data_path haoranxu/WMT23-Test&lt;/code&gt;. Please look at &lt;code&gt;evals/alma_13b_r_wmt23.sh&lt;/code&gt; as an example), run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch --config_file configs/deepspeed_eval_config_bf16.yaml \&#xA;    run_llmmt.py \&#xA;    --model_name_or_path haoranxu/ALMA-13B-R \&#xA;    --do_predict \&#xA;    --low_cpu_mem_usage \&#xA;    --language_pairs en-cs,cs-en \&#xA;    --mmt_data_path ./human_written_data/ \&#xA;    --per_device_eval_batch_size 1 \&#xA;    --output_dir ./your_output_dir/ \&#xA;    --predict_with_generate \&#xA;    --max_new_tokens 256 \&#xA;    --max_source_length 256 \&#xA;    --bf16 \&#xA;    --seed 42 \&#xA;    --num_beams 5 \&#xA;    --overwrite_cache \&#xA;    --overwrite_output_dir&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The generated outputs will be saved in the &lt;code&gt;your_output_dir&lt;/code&gt;. The translation file for the &lt;code&gt;en‚Üícs&lt;/code&gt; direction is named &lt;code&gt;test-en-cs&lt;/code&gt;, and the file for the cs‚Üíen direction is &lt;code&gt;test-cs-en&lt;/code&gt;. We have prepared a bash file for the user to easily run the evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash evals/alma_13b_r.sh ${your_output_dir} ${test_pairs}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The variable &lt;code&gt;${test_pairs}&lt;/code&gt; denotes the translation directions you wish to evaluate. It supports testing multiple directions at once. For example, you can use &lt;code&gt;de-en,en-de,en-cs,cs-en&lt;/code&gt;. Once the bash script completes its execution, both the BLEU scores and COMET results will be automatically displayed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note that this will perform data-parallel evaluation supported by deepspeed: that is, placing a single full copy of your model onto each available GPU and splitting batches across GPUs to evaluate on K GPUs K times faster than on one&lt;/strong&gt;. For those with limited GPU memory, we offer an alternative method. The user can pass &lt;code&gt;--multi_gpu_one_model&lt;/code&gt; to run the process by distributing a single model across multiple GPUs. Please see evaluation examples in &lt;code&gt;evals/alma_13b_r.sh&lt;/code&gt; or &lt;code&gt;evals/*no_parallel&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;h3&gt;Few-Shot In-Context Learning&lt;/h3&gt; &#xA;&lt;p&gt;To append examples in the prompt, you need to pass the &lt;code&gt;--few_shot_eval_path&lt;/code&gt; flag and specify the location of their shot files. As a demonstration, you can execute the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash evals/llama-2-13b-5-shot.sh ${OUTPUT_DIR} ${test_pairs}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Training üî•&lt;/h1&gt; &#xA;&lt;p&gt;Here we show how to&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;(NEW!) contrastive Preference Optmization Upon ALMA Models (ALMA‚ÜíALMA-R).&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;fine-tune LLaMA-2-7B on monolingual OSCAR data (stage 1)&lt;/li&gt; &#xA; &lt;li&gt;fine-tune human-written parallel data fine-tuning once stage 1 is completed, including full-weight and LoRA fine-tuning (stage 2)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;strong&gt;CPO Fine-Tuning&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To run the CPO fine-tuning with our triplet preference data, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash runs/cpo_ft.sh ${your_output_dir}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;OSCAR Monolingual Fine-Tuning&lt;/h3&gt; &#xA;&lt;p&gt;To execute the OSCAR monolingual fine-tuning, use the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash runs/mono_ft.sh ${your_output_dir}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Parallel Data Fine-Tuning (Full-Weight)&lt;/h3&gt; &#xA;&lt;p&gt;Once the monolingual data fine-tuning is complete, proceed to the parallel data fine-tuning using the full-weight approach. Execute the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash runs/parallel_ft.sh ${your_output_dir} $training_pairs$&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where &lt;code&gt;training_pairs&lt;/code&gt; is the translation directions you considered. The default is all 10 directions: &lt;code&gt;de-en,cs-en,is-en,zh-en,ru-en,en-de,en-cs,en-is,en-zh,en-ru&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Parallel Data Fine-Tuning (LoRA)&lt;/h3&gt; &#xA;&lt;p&gt;In Stage 2, there&#39;s also an option to employ LoRA for fine-tuning on the parallel data. To do so, execute the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash runs/parallel_ft_lora.sh ${your_output_dir} $training_pairs$&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Data Information üíæ&lt;/h1&gt; &#xA;&lt;p&gt;Human-written training dataset, along with the WMT&#39;22 test dataset, can be found in the &lt;code&gt;human_written_data&lt;/code&gt; directory. Within this directory, there are five subfolders, each representing one of the five language pairs. Each of these subfolders contains the training, development, and test sets for its respective language pair.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;-deen&#xA;    -train.de-en.json&#xA;    -valid.de-en.json&#xA;    -test.de-en.json&#xA;    -test.en-de.json&#xA;    ....&#xA;-csen&#xA;-isen&#xA;-ruen&#xA;.....&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The data format in json files must be:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &#34;translation&#34;:&#xA;  {&#xA;      &#34;src(de)&#34;: &#34;source sentence&#34;,&#xA;      &#34;tgt(en)&#34;: &#34;target sentence&#34;,&#xA;}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Within this directory, there are two additional subfolders specifically designed for few-shot in-context learning:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Filtered-5-shot&lt;/code&gt;: This contains the &#34;filtered shots&#34; as referenced in the paper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;HW-5-shot&lt;/code&gt;: This contains the &#34;randomly extracted human-written data&#34; mentioned in the paper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;FAQs ‚ùì&lt;/h1&gt; &#xA;&lt;h3&gt;What language directions do ALMA and ALMA-R support?&lt;/h3&gt; &#xA;&lt;p&gt;Currently, ALMA supports 10 directions: English‚ÜîGerman, Englishs‚ÜîCzech, Englishs‚ÜîIcelandic, Englishs‚ÜîChinese, Englishs‚ÜîRussian. However, it may surprise us in other directions :)&lt;/p&gt; &#xA;&lt;h3&gt;When should I stop fine-tuning at stage 1?&lt;/h3&gt; &#xA;&lt;p&gt;Our 7B and 13B models are trained on 20B and 12B tokens, respectively. However, as indicated in the paper, fine-tuning 1B tokens should boost the performance substantially. The steps required to fine-tune 1 billion tokens also vary based on your batch size. In our case, the batch size is calculated as follows: 16 GPUs * 4 (batch size per GPU) * 4 (gradient accumulation steps) = 256. With a sequence length of 512, we need approximately 8,000 steps to train on 1 billion tokens, calculated as 10^9 / (256*512) ‚âà8000 steps. However, you may choose to fine-tune more steps to get better performance.&lt;/p&gt; &#xA;&lt;h3&gt;How to decide the interleave probability at stage 1?&lt;/h3&gt; &#xA;&lt;p&gt;Please find the reasons for interleave probability selection for stage 1 in Appendix D.1 in the &lt;a href=&#34;https://arxiv.org/pdf/2309.11674.pdf&#34;&gt;paper&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;Reference&lt;/h1&gt; &#xA;&lt;p&gt;Please find more details for ALMA models in our &lt;a href=&#34;https://arxiv.org/abs/2309.11674&#34;&gt;paper&lt;/a&gt; or the &lt;a href=&#34;https://notes.aimodels.fyi/alma-a-new-training-method-that-boosts-translation-performance-for-large-language-models/&#34;&gt;summary&lt;/a&gt; of the paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{xu2023paradigm,&#xA;      title={A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models}, &#xA;      author={Haoran Xu and Young Jin Kim and Amr Sharaf and Hany Hassan Awadalla},&#xA;      year={2023},&#xA;      eprint={2309.11674},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please also find more detailed information for the ALMA-R model with Contrastive Preference Optimization in the &lt;a href=&#34;https://arxiv.org/pdf/2401.08417v2.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{xu2024contrastive,&#xA;      title={Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation}, &#xA;      author={Haoran Xu and Amr Sharaf and Yunmo Chen and Weiting Tan and Lingfeng Shen and Benjamin Van Durme and Kenton Murray and Young Jin Kim},&#xA;      year={2024},&#xA;      eprint={2401.08417},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>vidarh/rubywm</title>
    <updated>2024-01-25T01:40:39Z</updated>
    <id>tag:github.com,2024-01-25:/vidarh/rubywm</id>
    <link href="https://github.com/vidarh/rubywm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An X11 window manager in pure Ruby&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;A Ruby X11 Window Manager&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;This is experimental. It will eat your cat and burn down your house, format your hard drive and post all your secrets to Facebook.&lt;/p&gt; &#xA;&lt;p&gt;Also it &lt;em&gt;will&lt;/em&gt; likely crash on you.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re not comfortable figuring out how to recover from an X session where your window manager is gone and lots of your windows appears to have disappeared ... somewhere, and you might not be able to get focus to a terminal window without switching to the text console, this is not yet for you.&lt;/p&gt; &#xA;&lt;h2&gt;So why should I run this?&lt;/h2&gt; &#xA;&lt;p&gt;You almost certainly shouldn&#39;t.&lt;/p&gt; &#xA;&lt;h2&gt;But what is it then, at least?&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s a minimalist (currently &amp;lt;1K lines) pure Ruby (including the X11 driver) X11 window manager. It is focused on tiling, but allows you to choose to assign a tiling layout to specific desktops or leave them floating. Currently &lt;em&gt;whether or not you use tiling or floating layout&lt;/em&gt; there is &lt;em&gt;no window decoration&lt;/em&gt; and windows are not draggable or resizable by pulling on borders (but you can do that with Windows key + left/right mouse button)&lt;/p&gt; &#xA;&lt;p&gt;Like bspwm, which was an inspiration, the wm supports &lt;em&gt;no&lt;/em&gt; keyboard handling - all keyboard handling is deferred to separate tools like sxhkd. Unlike bspwm this WM has no dedicated IPC mechanism. Instead, so far, all communication happens via X11 ClientMessage events, which means any tool, like xdotool etc. that can produce those events can control the WM.&lt;/p&gt; &#xA;&lt;p&gt;It currently does &lt;em&gt;not&lt;/em&gt; do anything to facilitate working on multiple monitors, as in my current setup I&#39;m only using a single monitor for my Linux machine.&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;Note that most of what you see here is not the wm. The wm decoration is minimalist: a 1 pixel rectangular frame. Nothing else. But people want to see screenshots anyway, so here:&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://github.com/vidarh/rubywm/raw/8eed458c1b9f9d25372df3932ab1237149bb90c0/screenshots/2024-01-11_19-45.png?raw=true&#34;&gt;&lt;img style=&#34;display: inline; width: 48%&#34; src=&#34;https://github.com/vidarh/rubywm/raw/8eed458c1b9f9d25372df3932ab1237149bb90c0/screenshots/2024-01-11_19-45.png?raw=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/vidarh/rubywm/raw/master/screenshots/2024-01-11_20-04.png?raw=true&#34;&gt;&lt;img style=&#34;display: inline; width: 48%&#34; src=&#34;https://github.com/vidarh/rubywm/raw/master/screenshots/2024-01-11_20-04.png?raw=true&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Why did you write this?&lt;/h2&gt; &#xA;&lt;p&gt;It started with mild frustration that bspwm handled my desire for one of my virtual desktops to have floating windows by default poorly. It&#39;s possible, but didn&#39;t work great for me. It also frustrated me that my file manager was visible on all the virtual desktops instead of just the floating one. I also happened to know an X11 WM can be &lt;em&gt;really&lt;/em&gt; minimal to start off with.&lt;/p&gt; &#xA;&lt;p&gt;So I ditched bspwm, and translated TinyWM - a really minimal C wm - to Ruby, made that my main wm, and gradually started adding the features I needed, drawing a lot of inspiration from the code of KatriaWM to figure out how to make my experience gradually less painful.&lt;/p&gt; &#xA;&lt;p&gt;This has been my only WM since that day, and I now feel that &lt;em&gt;I&lt;/em&gt; have rough parity in term of the features &lt;em&gt;I&lt;/em&gt; use with bspwm. That does not mean it will have parity for you - it lacks lots of things. It also does not mean there aren&#39;t plenty of bugs, because there are.&lt;/p&gt; &#xA;&lt;h2&gt;Will you add...?&lt;/h2&gt; &#xA;&lt;p&gt;Maybe. As long as it can either 1) be done with little code, and/or 2) be done by you, and/or 3) it can easily be kept as a separate gem.&lt;/p&gt; &#xA;&lt;p&gt;Talk to me. But please respect I&#39;m primarily releasing this &#34;as is&#34;, and I&#39;m not committing to supporting this - I &lt;em&gt;do not care&lt;/em&gt; if you decide it doesn&#39;t work for you and is horrible. I&#39;ll think it&#39;s great if you get some utility out of this code, though. But my goal is not a big user base. Or &lt;em&gt;a&lt;/em&gt; user base.&lt;/p&gt; &#xA;&lt;p&gt;My goal is a functional, minimalist WM that works &lt;em&gt;for me&lt;/em&gt;. And so, I&#39;ll help if it&#39;s not compromising my own goal. To the extent our goals are not compatible, I&#39;m happy to e.g. split out generic/reusable functionality so people can fork this and we can still benefit from sharing the bits where we do agree how things should be.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-requisites:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;sxhkd or similar is needed to handle input, as this WM does &lt;em&gt;not&lt;/em&gt; listen to keybindings other than grabbing windows+ left/right mouse button for move and resize.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A recent version of Ruby. I currently use 3.2.2&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to run&lt;/h2&gt; &#xA;&lt;p&gt;This is a subset of my .xinitrc.&lt;/p&gt; &#xA;&lt;p&gt;WARNING: You probably want to try this in a vm or something first and see if it works for you:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;  (sxhkd 2&amp;gt;&amp;amp;1 | logger -t sxhkd) &amp;amp;&#xA;  (cd ~/Desktop/Projects/wm ; ruby rubywm.rb 2&amp;gt;&amp;amp;1 | logger -t rubywm) &amp;amp;&#xA;  &#xA;  while true do&#xA;    wait&#xA;    sleep 5&#xA;  done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For most &#34;normal&#34; window managers, people tend to start the window manager last and let it end the X session when it quits, but since this is in development, I&#39;m not going to do that because most stuff on my desktop can survive my WM crashing and being restarted just fine, as it should be, but will obviously get killed if the X session dies.&lt;/p&gt; &#xA;&lt;h2&gt;Using with sxhkd&lt;/h2&gt; &#xA;&lt;p&gt;This is my related integration with sxhkd from my sxhkd config, but any app that supports sending XClientMessage events can work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    # Full screen&#xA;    super + f&#xA;    &#x9;/home/vidarh/bin/xclimsg -mpw focused _NET_WM_STATE 2 _NET_WM_STATE_FULLSCREEN 0 2&#xA;    &#xA;    # Shift Focus&#xA;    super + {Left,Down,Up,Right}&#xA;    &#x9;/home/vidarh/bin/xclimsg -mpw focused _RWM_FOCUS {Left,Down,Up,Right}&#xA;    &#xA;    # Shift direction&#xA;    super + shift + d&#xA;    &#x9;/home/vidarh/bin/xclimsg -mpw focused _RWM_SHIFT_DIRECTION 0&#xA;    &#xA;    # Swap node layout&#xA;    super + shift + l&#xA;    &#x9;/home/vidarh/bin/xclimsg -mpw focused _RWM_SWAP_NODES 0&#xA;    &#xA;    # Move&#xA;    super + shift + {Left,Down,Up,Right}&#xA;    &#x9;/home/vidarh/bin/xclimsg -mpw focused _RWM_MOVE {Left,Down,Up,Right}&#xA;    &#xA;    super + F1&#xA;    &#x9;/home/vidarh/bin/xclimsg -mpw focused _NET_RESTACK_WINDOW 2 0 0&#xA;    &#xA;    super + F2&#xA;    &#x9;/home/vidarh/bin/xclimsg -mpw focused _NET_RESTACK_WINDOW 2 0 1&#xA;    &#xA;    # Focus desktop&#xA;    super + {1-9,0}&#xA;    &#x9;/home/vidarh/bin/xclimsg -mp _NET_CURRENT_DESKTOP {0-8,9}&#xA;    &#xA;    # Move to desktop&#xA;    super + shift + {1-9,0}&#xA;    &#x9;/home/vidarh/bin/xclimsg -mpw focused _NET_WM_DESKTOP {0-8,9}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;_RWM&lt;/code&gt; events are custom for this WM. The others works on other EWMH compatible wms.&lt;/p&gt; &#xA;&lt;p&gt;xclimsg is from &lt;a href=&#34;https://github.com/phillbush/xclimsg&#34;&gt;https://github.com/phillbush/xclimsg&lt;/a&gt; I intend to &#34;build in&#34; the same client code in rubywm to avoid that external dependency. Alternatively you can e.g. use xdotool or similar&lt;/p&gt;</summary>
  </entry>
</feed>