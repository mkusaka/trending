<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Jupyter Notebook Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-05-29T02:13:50Z</updated>
  <subtitle>Weekly Trending of Jupyter Notebook in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jeffheaton/t81_558_deep_learning</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/jeffheaton/t81_558_deep_learning</id>
    <link href="https://github.com/jeffheaton/t81_558_deep_learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Washington University (in St. Louis) Course T81-558: Applications of Deep Neural Networks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;T81 558:Applications of Deep Neural Networks&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.wustl.edu&#34;&gt;Washington University in St. Louis&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instructor: &lt;a href=&#34;https://sites.wustl.edu/jeffheaton/&#34;&gt;Jeff Heaton&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The content of this course changes as technology evolves&lt;/strong&gt;, to keep up to date with changes &lt;a href=&#34;https://github.com/jeffheaton&#34;&gt;follow me on GitHub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1. Fall 2022, Monday, 2:30 PM, Location: TBD&lt;/li&gt; &#xA; &lt;li&gt;Section 2. Fall 2022, Online&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Course Description&lt;/h1&gt; &#xA;&lt;p&gt;Deep learning is a group of exciting new technologies for neural networks. Through a combination of advanced training techniques and neural network architectural components, it is now possible to create neural networks that can handle tabular data, images, text, and audio as both input and output. Deep learning allows a neural network to learn hierarchies of information in a way that is like the function of the human brain. This course will introduce the student to classic neural network structures, Convolution Neural Networks (CNN), Long Short-Term Memory (LSTM), Gated Recurrent Neural Networks (GRU), General Adversarial Networks (GAN) and reinforcement learning. Application of these architectures to computer vision, time series, security, natural language processing (NLP), and data generation will be covered. High Performance Computing (HPC) aspects will demonstrate how deep learning can be leveraged both on graphical processing units (GPUs), as well as grids. Focus is primarily upon the application of deep learning to problems, with some introduction to mathematical foundations. Students will use the Python programming language to implement deep learning using Google TensorFlow and Keras. It is not necessary to know Python prior to this course; however, familiarity of at least one programming language is assumed. This course will be delivered in a hybrid format that includes both classroom and online instruction.&lt;/p&gt; &#xA;&lt;h1&gt;Textbook&lt;/h1&gt; &#xA;&lt;p&gt;The complete text for this course is here on GitHub. This same material is also available in &lt;a href=&#34;https://www.heatonresearch.com/book/applications-deep-neural-networks-keras.html&#34;&gt;book format&lt;/a&gt;. The course textbook is “Applications of Deep Neural networks with Keras“, ISBN 9798416344269.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to cite the material from this course/book, please use the following BibTex citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{heaton2020applications,&#xA;    title={Applications of Deep Neural Networks},&#xA;    author={Jeff Heaton},&#xA;    year={2020},&#xA;    eprint={2009.05673},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.LG}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Objectives&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Explain how neural networks (deep and otherwise) compare to other machine learning models.&lt;/li&gt; &#xA; &lt;li&gt;Determine when a deep neural network would be a good choice for a particular problem.&lt;/li&gt; &#xA; &lt;li&gt;Demonstrate your understanding of the material through a final project uploaded to GitHub.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Syllabus&lt;/h1&gt; &#xA;&lt;p&gt;This syllabus presents the expected class schedule, due dates, and reading assignments. &lt;a href=&#34;https://data.heatonresearch.com/wustl/jheaton-t81-558-spring-2022-syllabus.pdf&#34;&gt;Download current syllabus.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Module&lt;/th&gt; &#xA;   &lt;th&gt;Content&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_01_1_overview.ipynb&#34;&gt;Module 1&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 08/29/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 1: Python Preliminaries&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 1.1: Course Overview&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.2: Introduction to Python&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.3: Python Lists, Dictionaries, Sets &amp;amp; JSON&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.4: File Handling&lt;/li&gt;&#xA;     &lt;li&gt;Part 1.5: Functions, Lambdas, and Map/ReducePython Preliminaries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (first meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_02_1_python_pandas.ipynb&#34;&gt;Module 2&lt;/a&gt;&lt;br&gt;Week of 09/12/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 2: Python for Machine Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt; Part 2.1: Introduction to Pandas for Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.2: Encoding Categorical Values in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.3: Grouping, Sorting, and Shuffling&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.4: Using Apply and Map in Pandas&lt;/li&gt;&#xA;     &lt;li&gt;Part 2.5: Feature Engineering in Padas&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class1.ipynb&#34;&gt;Module 1 Program&lt;/a&gt; due: 09/13/2022&lt;/li&gt;&#xA;     &lt;li&gt; Icebreaker due: 09/13/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_03_1_neural_net.ipynb&#34;&gt;Module 3&lt;/a&gt;&lt;br&gt;Week of 09/19/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 3: TensorFlow and Keras for Neural Networks&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 3.1: Deep Learning and Neural Network Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.2: Introduction to Tensorflow &amp;amp; Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.3: Saving and Loading a Keras Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.4: Early Stopping in Keras to Prevent Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 3.5: Extracting Keras Weights and Manual Neural Network Calculation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class2.ipynb&#34;&gt;Module 2: Program&lt;/a&gt; due: 09/20/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_04_1_feature_encode.ipynb&#34;&gt;Module 4&lt;/a&gt;&lt;br&gt;Week of 09/26/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 4: Training for Tabular Data&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 4.1: Encoding a Feature Vector for Keras Deep Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.2: Keras Multiclass Classification for Deep Neural Networks with ROC and AUC&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.3: Keras Regression for Deep Neural Networks with RMSE&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.4: Backpropagation, Nesterov Momentum, and ADAM Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 4.5: Neural Network RMSE and Log Loss Error Calculation from Scratch&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class3.ipynb&#34;&gt;Module 3 Program&lt;/a&gt; due: 09/27/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_05_1_reg_ridge_lasso.ipynb&#34;&gt;Module 5&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/03/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 5: Regularization and Dropout&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 5.1: Introduction to Regularization: Ridge and Lasso&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.2: Using K-Fold Cross Validation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.3: Using L1 and L2 Regularization with Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.4: Drop Out for Keras to Decrease Overfitting&lt;/li&gt;&#xA;     &lt;li&gt;Part 5.5: Bootstrapping and Benchmarking Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class4.ipynb&#34;&gt;Module 4 Program&lt;/a&gt; due: 10/04/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (second meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_06_1_python_images.ipynb&#34;&gt;Module 6&lt;/a&gt;&lt;br&gt;Week of 10/17/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 6: CNN for Vision&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;      Part 6.1: Image Processing in Python&#xA;     &lt;li&gt;Part 6.2: Using Convolutional Networks with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.3: Using Pretrained Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.4: Looking at Keras Generators and Image Augmentation&lt;/li&gt;&#xA;     &lt;li&gt;Part 6.5: Recognizing Multiple Images with YOLOv5&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class5.ipynb&#34;&gt;Module 5 Program&lt;/a&gt; due: 10/18/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_07_1_gan_intro.ipynb&#34;&gt;Module 7&lt;/a&gt;&lt;br&gt;Week of 10/24/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 7: Generative Adversarial Networks (GANs)&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 7.1: Introduction to GANS for Image and Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.2: Train StyleGAN3 with your Own Images&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.3: Exploring the StyleGAN Latent Vector&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.4: GANS to Enhance Old Photographs Deoldify&lt;/li&gt;&#xA;     &lt;li&gt;Part 7.5: GANs for Tabular Synthetic Data Generation&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class6.ipynb&#34;&gt;Module 6 Assignment&lt;/a&gt; due: 10/25/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_08_1_kaggle_intro.ipynb&#34;&gt;Module 8&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 10/31/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 8: Kaggle&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 8.1: Introduction to Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.2: Building Ensembles with Scikit-Learn and Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.3: How Should you Architect Your Keras Neural Network: Hyperparameters&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.4: Bayesian Hyperparameter Optimization for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 8.5: Current Semester&#39;s Kaggle&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class7.ipynb&#34;&gt;Module 7 Assignment&lt;/a&gt; due: 11/01/2022&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (third meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_09_1_keras_transfer.ipynb&#34;&gt;Module 9&lt;/a&gt;&lt;br&gt;Week of 11/07/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 9: Transfer Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 9.1: Introduction to Keras Transfer Learning&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.2: Keras Transfer Learning for Computer Vision&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.3: Transfer Learning for NLP with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.4: Transfer Learning for Facial Feature Recognition&lt;/li&gt;&#xA;     &lt;li&gt;Part 9.5: Transfer Learning for Style Transfer&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class8.ipynb&#34;&gt;Module 8 Assignment&lt;/a&gt; due: 11/08/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_10_1_timeseries.ipynb&#34;&gt;Module 10&lt;/a&gt;&lt;br&gt;Week of 11/14/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 10: Time Series in Keras&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 10.1: Time Series Data Encoding for Deep Learning, Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.2: Programming LSTM with Keras and&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.3: Text Generation with Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.4: Introduction to Transformers&lt;/li&gt;&#xA;     &lt;li&gt;Part 10.5: Transformers for Timeseries&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class9.ipynb&#34;&gt;Module 9 Assignment&lt;/a&gt; due: 11/15/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_11_01_huggingface.ipynb&#34;&gt;Module 11&lt;/a&gt;&lt;br&gt;Week of 11/21/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 11: Natural Language Processing&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 11.1: Hugging Face Introduction&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.2: Hugging Face Tokenizers&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.3: Hugging Face Data Sets&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.4: Training a Model in Hugging Face&lt;/li&gt;&#xA;     &lt;li&gt;Part 11.5: What are Embedding Layers in Keras&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class10.ipynb&#34;&gt;Module 10 Assignment&lt;/a&gt; due: 11/22/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_12_01_ai_gym.ipynb&#34;&gt;Module 12&lt;/a&gt;&lt;br&gt;Week of 11/28/2022&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 12: Reinforcement Learning&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Kaggle Assignment due: 11/29/2022 (approx 4-6PM, due to Kaggle GMT timezone)&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.1: Introduction to the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.2: Introduction to Q-Learning for Keras&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.3: Keras Q-Learning in the OpenAI Gym&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.4: Atari Games with Keras Neural Networks&lt;/li&gt;&#xA;     &lt;li&gt;Part 12.5: Application of Reinforcement Learning&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class11.ipynb&#34;&gt;Module 11 Assignment&lt;/a&gt; due: 11/29/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/t81_558_class_13_01_flask.ipynb&#34;&gt;Module 13&lt;/a&gt;&lt;br&gt;&lt;strong&gt;Meet on 12/05/2022&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Module 13: Deployment and Monitoring&lt;/strong&gt;&#xA;    &lt;ul&gt;&#xA;     &lt;li&gt;Part 13.1: Flask and Deep Learning Web Services &lt;/li&gt;&#xA;     &lt;li&gt;Part 13.2: Interrupting and Continuing Training&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.3: Using a Keras Deep Neural Network with a Web Application&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.4: When to Retrain Your Neural Network&lt;/li&gt;&#xA;     &lt;li&gt;Part 13.5: Tensor Processing Units (TPUs)&lt;/li&gt;&#xA;     &lt;li&gt;&lt;strong&gt;We will meet on campus this week! (fourth meeting)&lt;/strong&gt;&lt;/li&gt;&#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/jeffheaton/t81_558_deep_learning/raw/master/assignments/assignment_yourname_class12.ipynb&#34;&gt;Module 12 Assignment&lt;/a&gt; due: 12/06/2022&lt;/li&gt;&#xA;    &lt;/ul&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Datasets&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://data.heatonresearch.com/data/t81-558/index.html&#34;&gt;Datasets can be downloaded here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Pierian-Data/Complete-Python-3-Bootcamp</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/Pierian-Data/Complete-Python-3-Bootcamp</id>
    <link href="https://github.com/Pierian-Data/Complete-Python-3-Bootcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Python-3-Bootcamp&lt;/h1&gt; &#xA;&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt; &#xA;&lt;p&gt;Copyright(©) by Pierian Data Inc.&lt;/p&gt; &#xA;&lt;p&gt;Get it now for 95% off with the link: &lt;a href=&#34;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&#34;&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ageron/handson-ml2</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/ageron/handson-ml2</id>
    <link href="https://github.com/ageron/handson-ml2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in Python using Scikit-Learn, Keras and TensorFlow 2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Machine Learning Notebooks&lt;/h1&gt; &#xA;&lt;p&gt;This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in the second edition of my O&#39;Reilly book &lt;a href=&#34;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&#34;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;img src=&#34;https://images-na.ssl-images-amazon.com/images/I/51aqYc1QyrL._SX379_BO1,204,203,200_.jpg&#34; title=&#34;book&#34; width=&#34;150&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you are looking for the first edition notebooks, check out &lt;a href=&#34;https://github.com/ageron/handson-ml&#34;&gt;ageron/handson-ml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Want to play with these notebooks online without having to install anything?&lt;/h3&gt; &#xA;&lt;p&gt;Use any of the following services (I recommended Colab or Kaggle, since they offer free GPUs and TPUs).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WARNING&lt;/strong&gt;: &lt;em&gt;Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/ageron/handson-ml2/blob/master/&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/kaggle/&#34;&gt;&lt;img src=&#34;https://kaggle.com/static/images/open-in-kaggle.svg?sanitize=true&#34; alt=&#34;Open in Kaggle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/ageron/handson-ml2/HEAD?filepath=%2Findex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Launch binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://homl.info/deepnote/&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote-small.svg?sanitize=true&#34; alt=&#34;Launch in Deepnote&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Just want to quickly look at some notebooks, without executing any code?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/ageron/handson-ml2/blob/master/index.ipynb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg?sanitize=true&#34; alt=&#34;Render nbviewer&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ageron/handson-ml2/raw/master/index.ipynb&#34;&gt;github.com&#39;s notebook viewer&lt;/a&gt; also works but it&#39;s not ideal: it&#39;s slower, the math equations are not always displayed correctly, and large notebooks often fail to open.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Want to run this project using a Docker image?&lt;/h3&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://github.com/ageron/handson-ml2/tree/master/docker&#34;&gt;Docker instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Want to install this project on your own machine?&lt;/h3&gt; &#xA;&lt;p&gt;Start by installing &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda&lt;/a&gt; (or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;), &lt;a href=&#34;https://git-scm.com/downloads&#34;&gt;git&lt;/a&gt;, and if you have a TensorFlow-compatible GPU, install the &lt;a href=&#34;https://www.nvidia.com/Download/index.aspx&#34;&gt;GPU driver&lt;/a&gt;, as well as the appropriate version of CUDA and cuDNN (see TensorFlow&#39;s documentation for more details).&lt;/p&gt; &#xA;&lt;p&gt;Next, clone this project by opening a terminal and typing the following commands (do not type the first &lt;code&gt;$&lt;/code&gt; signs on each line, they just indicate that these are terminal commands):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/ageron/handson-ml2.git&#xA;$ cd handson-ml2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda env create -f environment.yml&#xA;$ conda activate tf2&#xA;$ python -m ipykernel install --user --name=python3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, start Jupyter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you need further instructions, read the &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;detailed installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Which Python version should I use?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;I recommend Python 3.8. If you follow the installation instructions above, that&#39;s the version you will get. Most code will work with other versions of Python 3, but some libraries do not support Python 3.9 or 3.10 yet, which is why I recommend Python 3.8.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an error when I call &lt;code&gt;load_housing_data()&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Make sure you call &lt;code&gt;fetch_housing_data()&lt;/code&gt; &lt;em&gt;before&lt;/em&gt; you call &lt;code&gt;load_housing_data()&lt;/code&gt;. If you&#39;re getting an HTTP error, make sure you&#39;re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;m getting an SSL error on MacOSX&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You probably need to install the SSL certificates (see this &lt;a href=&#34;https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error&#34;&gt;StackOverflow question&lt;/a&gt;). If you downloaded Python from the official website, then run &lt;code&gt;/Applications/Python\ 3.8/Install\ Certificates.command&lt;/code&gt; in a terminal (change &lt;code&gt;3.8&lt;/code&gt; to whatever version you installed). If you installed Python using MacPorts, run &lt;code&gt;sudo port install curl-ca-bundle&lt;/code&gt; in a terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I&#39;ve installed this project locally. How do I update it to the latest version?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How do I update my Python libraries to the latest versions, when using Anaconda?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/ageron/handson-ml2/master/INSTALL.md&#34;&gt;INSTALL.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;I would like to thank everyone &lt;a href=&#34;https://github.com/ageron/handson-ml2/graphs/contributors&#34;&gt;who contributed to this project&lt;/a&gt;, either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the &lt;code&gt;docker&lt;/code&gt; directory, and to github user SuperYorio who helped on some exercise solutions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bmild/nerf</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/bmild/nerf</id>
    <link href="https://github.com/bmild/nerf" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code release for NeRF (Neural Radiance Fields)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeRF: Neural Radiance Fields&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;http://tancik.com/nerf&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/JuH79E8rdKc&#34;&gt;Video&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2003.08934&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;Data&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/bmild/nerf/blob/master/tiny_nerf.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open Tiny-NeRF in Colab&#34;&gt;&lt;/a&gt;&lt;br&gt; Tensorflow implementation of optimizing a neural representation for a single scene and rendering new views.&lt;br&gt;&lt;br&gt; &lt;a href=&#34;http://tancik.com/nerf&#34;&gt;NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://people.eecs.berkeley.edu/~bmild/&#34;&gt;Ben Mildenhall&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://people.eecs.berkeley.edu/~pratul/&#34;&gt;Pratul P. Srinivasan&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://tancik.com/&#34;&gt;Matthew Tancik&lt;/a&gt;*&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://jonbarron.info/&#34;&gt;Jonathan T. Barron&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;http://cseweb.ucsd.edu/~ravir/&#34;&gt;Ravi Ramamoorthi&lt;/a&gt;&lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://www2.eecs.berkeley.edu/Faculty/Homepages/yirenng.html&#34;&gt;Ren Ng&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;UC Berkeley, &lt;sup&gt;2&lt;/sup&gt;Google Research, &lt;sup&gt;3&lt;/sup&gt;UC San Diego&lt;br&gt; *denotes equal contribution&lt;br&gt; in ECCV 2020 (Oral Presentation, Best Paper Honorable Mention)&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/bmild/nerf/master/imgs/pipeline.jpg&#34;&gt; &#xA;&lt;h2&gt;TL;DR quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To setup a conda environment, download example training data, begin the training process, and launch Tensorboard:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;conda activate nerf&#xA;bash download_example_data.sh&#xA;python run_nerf.py --config config_fern.txt&#xA;tensorboard --logdir=logs/summaries --port=6006&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything works without errors, you can now go to &lt;code&gt;localhost:6006&lt;/code&gt; in your browser and watch the &#34;Fern&#34; scene train.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Python 3 dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensorflow 1.15&lt;/li&gt; &#xA; &lt;li&gt;matplotlib&lt;/li&gt; &#xA; &lt;li&gt;numpy&lt;/li&gt; &#xA; &lt;li&gt;imageio&lt;/li&gt; &#xA; &lt;li&gt;configargparse&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The LLFF data loader requires ImageMagick.&lt;/p&gt; &#xA;&lt;p&gt;We provide a conda environment setup file including all of the above dependencies. Create the conda environment &lt;code&gt;nerf&lt;/code&gt; by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env create -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will also need the &lt;a href=&#34;http://github.com/fyusion/llff&#34;&gt;LLFF code&lt;/a&gt; (and COLMAP) set up to compute poses if you want to run on your own real data.&lt;/p&gt; &#xA;&lt;h2&gt;What is a NeRF?&lt;/h2&gt; &#xA;&lt;p&gt;A neural radiance field is a simple fully connected network (weights are ~5MB) trained to reproduce input views of a single scene using a rendering loss. The network directly maps from spatial location and viewing direction (5D input) to color and opacity (4D output), acting as the &#34;volume&#34; so we can use volume rendering to differentiably render new views.&lt;/p&gt; &#xA;&lt;p&gt;Optimizing a NeRF takes between a few hours and a day or two (depending on resolution) and only requires a single GPU. Rendering an image from an optimized NeRF takes somewhere between less than a second and ~30 seconds, again depending on resolution.&lt;/p&gt; &#xA;&lt;h2&gt;Running code&lt;/h2&gt; &#xA;&lt;p&gt;Here we show how to run our code on two example scenes. You can download the rest of the synthetic and real data used in the paper &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Optimizing a NeRF&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download_example_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to get the our synthetic Lego dataset and the LLFF Fern dataset.&lt;/p&gt; &#xA;&lt;p&gt;To optimize a low-res Fern NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config config_fern.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After 200k iterations (about 15 hours), you should get a video like this at &lt;code&gt;logs/fern_test/fern_test_spiral_200000_rgb.mp4&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://people.eecs.berkeley.edu/~bmild/nerf/fern_200k_256w.gif&#34; alt=&#34;ferngif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To optimize a low-res Lego NeRF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_nerf.py --config config_lego.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After 200k iterations, you should get a video like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://people.eecs.berkeley.edu/~bmild/nerf/lego_200k_256w.gif&#34; alt=&#34;legogif&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Rendering a NeRF&lt;/h3&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download_example_weights.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to get a pretrained high-res NeRF for the Fern dataset. Now you can use &lt;a href=&#34;https://github.com/bmild/nerf/raw/master/render_demo.ipynb&#34;&gt;&lt;code&gt;render_demo.ipynb&lt;/code&gt;&lt;/a&gt; to render new views.&lt;/p&gt; &#xA;&lt;h3&gt;Replicating the paper results&lt;/h3&gt; &#xA;&lt;p&gt;The example config files run at lower resolutions than the quantitative/qualitative results in the paper and video. To replicate the results from the paper, start with the config files in &lt;a href=&#34;https://github.com/bmild/nerf/tree/master/paper_configs&#34;&gt;&lt;code&gt;paper_configs/&lt;/code&gt;&lt;/a&gt;. Our synthetic Blender data and LLFF scenes are hosted &lt;a href=&#34;https://drive.google.com/drive/folders/128yBriW1IG_3NJ5Rp7APSTZsJqdJdfc1&#34;&gt;here&lt;/a&gt; and the DeepVoxels data is hosted by Vincent Sitzmann &lt;a href=&#34;https://drive.google.com/open?id=1lUvJWB6oFtT8EQ_NzBrXnmi25BufxRfl&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Extracting geometry from a NeRF&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://github.com/bmild/nerf/raw/master/extract_mesh.ipynb&#34;&gt;&lt;code&gt;extract_mesh.ipynb&lt;/code&gt;&lt;/a&gt; for an example of running marching cubes to extract a triangle mesh from a trained NeRF network. You&#39;ll need the install the &lt;a href=&#34;https://github.com/pmneila/PyMCubes&#34;&gt;PyMCubes&lt;/a&gt; package for marching cubes plus the &lt;a href=&#34;https://github.com/mikedh/trimesh&#34;&gt;trimesh&lt;/a&gt; and &lt;a href=&#34;https://github.com/mmatl/pyrender&#34;&gt;pyrender&lt;/a&gt; packages if you want to render the mesh inside the notebook:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install trimesh pyrender PyMCubes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Generating poses for your own scenes&lt;/h2&gt; &#xA;&lt;h3&gt;Don&#39;t have poses?&lt;/h3&gt; &#xA;&lt;p&gt;We recommend using the &lt;code&gt;imgs2poses.py&lt;/code&gt; script from the &lt;a href=&#34;https://github.com/fyusion/llff&#34;&gt;LLFF code&lt;/a&gt;. Then you can pass the base scene directory into our code using &lt;code&gt;--datadir &amp;lt;myscene&amp;gt;&lt;/code&gt; along with &lt;code&gt;-dataset_type llff&lt;/code&gt;. You can take a look at the &lt;code&gt;config_fern.txt&lt;/code&gt; config file for example settings to use for a forward facing scene. For a spherically captured 360 scene, we recomment adding the &lt;code&gt;--no_ndc --spherify --lindisp&lt;/code&gt; flags.&lt;/p&gt; &#xA;&lt;h3&gt;Already have poses!&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;run_nerf.py&lt;/code&gt; and all other code, we use the same pose coordinate system as in OpenGL: the local camera coordinate system of an image is defined in a way that the X axis points to the right, the Y axis upwards, and the Z axis backwards as seen from the image.&lt;/p&gt; &#xA;&lt;p&gt;Poses are stored as 3x4 numpy arrays that represent camera-to-world transformation matrices. The other data you will need is simple pinhole camera intrinsics (&lt;code&gt;hwf = [height, width, focal length]&lt;/code&gt;) and near/far scene bounds. Take a look at &lt;a href=&#34;https://github.com/bmild/nerf/raw/master/run_nerf.py#L406&#34;&gt;our data loading code&lt;/a&gt; to see more.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{mildenhall2020nerf,&#xA;  title={NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},&#xA;  author={Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},&#xA;  year={2020},&#xA;  booktitle={ECCV},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>rlabbe/Kalman-and-Bayesian-Filters-in-Python</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/rlabbe/Kalman-and-Bayesian-Filters-in-Python</id>
    <link href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kalman Filter book using Jupyter Notebook. Focuses on building intuition and experience, not formal proofs. Includes Kalman filters,extended Kalman filters, unscented Kalman filters, particle filters, and more. All exercises include solutions.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python&#34;&gt;Kalman and Bayesian Filters in Python&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Introductory text for Kalman and Bayesian filters. All code is written in Python, and the book itself is written using Jupyter Notebook so that you can run and modify the code in your browser. What better way to learn?&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&#34;Kalman and Bayesian Filters in Python&#34; looks amazing! ... your book is just what I needed&lt;/strong&gt; - Allen Downey, Professor and O&#39;Reilly author.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Thanks for all your work on publishing your introductory text on Kalman Filtering, as well as the Python Kalman Filtering libraries. We’ve been using it internally to teach some key state estimation concepts to folks and it’s been a huge help.&lt;/strong&gt; - Sam Rodkey, SpaceX&lt;/p&gt; &#xA;&lt;p&gt;Start reading online now by clicking the binder or Azure badge below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master&#34;&gt;&lt;img src=&#34;http://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master/animations/05_dog_track.gif&#34; alt=&#34;alt tag&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What are Kalman and Bayesian Filters?&lt;/h2&gt; &#xA;&lt;p&gt;Sensors are noisy. The world is full of data and events that we want to measure and track, but we cannot rely on sensors to give us perfect information. The GPS in my car reports altitude. Each time I pass the same point in the road it reports a slightly different altitude. My kitchen scale gives me different readings if I weigh the same object twice.&lt;/p&gt; &#xA;&lt;p&gt;In simple cases the solution is obvious. If my scale gives slightly different readings I can just take a few readings and average them. Or I can replace it with a more accurate scale. But what do we do when the sensor is very noisy, or the environment makes data collection difficult? We may be trying to track the movement of a low flying aircraft. We may want to create an autopilot for a drone, or ensure that our farm tractor seeded the entire field. I work on computer vision, and I need to track moving objects in images, and the computer vision algorithms create very noisy and unreliable results.&lt;/p&gt; &#xA;&lt;p&gt;This book teaches you how to solve these sorts of filtering problems. I use many different algorithms, but they are all based on Bayesian probability. In simple terms Bayesian probability determines what is likely to be true based on past information.&lt;/p&gt; &#xA;&lt;p&gt;If I asked you the heading of my car at this moment you would have no idea. You&#39;d prefer a number between 1° and 360° degrees, and have a 1 in 360 chance of being right. Now suppose I told you that 2 seconds ago its heading was 243°. In 2 seconds my car could not turn very far, so you could make a far more accurate prediction. You are using past information to more accurately infer information about the present or future.&lt;/p&gt; &#xA;&lt;p&gt;The world is also noisy. That prediction helps you make a better estimate, but it also subject to noise. I may have just braked for a dog or swerved around a pothole. Strong winds and ice on the road are external influences on the path of my car. In control literature we call this noise though you may not think of it that way.&lt;/p&gt; &#xA;&lt;p&gt;There is more to Bayesian probability, but you have the main idea. Knowledge is uncertain, and we alter our beliefs based on the strength of the evidence. Kalman and Bayesian filters blend our noisy and limited knowledge of how a system behaves with the noisy and limited sensor readings to produce the best possible estimate of the state of the system. Our principle is to never discard information.&lt;/p&gt; &#xA;&lt;p&gt;Say we are tracking an object and a sensor reports that it suddenly changed direction. Did it really turn, or is the data noisy? It depends. If this is a jet fighter we&#39;d be very inclined to believe the report of a sudden maneuver. If it is a freight train on a straight track we would discount it. We&#39;d further modify our belief depending on how accurate the sensor is. Our beliefs depend on the past and on our knowledge of the system we are tracking and on the characteristics of the sensors.&lt;/p&gt; &#xA;&lt;p&gt;The Kalman filter was invented by Rudolf Emil Kálmán to solve this sort of problem in a mathematically optimal way. Its first use was on the Apollo missions to the moon, and since then it has been used in an enormous variety of domains. There are Kalman filters in aircraft, on submarines, and on cruise missiles. Wall street uses them to track the market. They are used in robots, in IoT (Internet of Things) sensors, and in laboratory instruments. Chemical plants use them to control and monitor reactions. They are used to perform medical imaging and to remove noise from cardiac signals. If it involves a sensor and/or time-series data, a Kalman filter or a close relative to the Kalman filter is usually involved.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;The motivation for this book came out of my desire for a gentle introduction to Kalman filtering. I&#39;m a software engineer that spent almost two decades in the avionics field, and so I have always been &#39;bumping elbows&#39; with the Kalman filter, but never implemented one myself. As I moved into solving tracking problems with computer vision the need became urgent. There are classic textbooks in the field, such as Grewal and Andrew&#39;s excellent &lt;em&gt;Kalman Filtering&lt;/em&gt;. But sitting down and trying to read many of these books is a dismal experience if you do not have the required background. Typically the first few chapters fly through several years of undergraduate math, blithely referring you to textbooks on topics such as Itō calculus, and present an entire semester&#39;s worth of statistics in a few brief paragraphs. They are good texts for an upper undergraduate course, and an invaluable reference to researchers and professionals, but the going is truly difficult for the more casual reader. Symbology is introduced without explanation, different texts use different terms and variables for the same concept, and the books are almost devoid of examples or worked problems. I often found myself able to parse the words and comprehend the mathematics of a definition, but had no idea as to what real world phenomena they describe. &#34;But what does that &lt;em&gt;mean?&lt;/em&gt;&#34; was my repeated thought.&lt;/p&gt; &#xA;&lt;p&gt;However, as I began to finally understand the Kalman filter I realized the underlying concepts are quite straightforward. A few simple probability rules, some intuition about how we integrate disparate knowledge to explain events in our everyday life and the core concepts of the Kalman filter are accessible. Kalman filters have a reputation for difficulty, but shorn of much of the formal terminology the beauty of the subject and of their math became clear to me, and I fell in love with the topic.&lt;/p&gt; &#xA;&lt;p&gt;As I began to understand the math and theory more difficulties present themselves. A book or paper&#39;s author makes some statement of fact and presents a graph as proof. Unfortunately, why the statement is true is not clear to me, nor is the method for making that plot obvious. Or maybe I wonder &#34;is this true if R=0?&#34; Or the author provides pseudocode at such a high level that the implementation is not obvious. Some books offer Matlab code, but I do not have a license to that expensive package. Finally, many books end each chapter with many useful exercises. Exercises which you need to understand if you want to implement Kalman filters for yourself, but exercises with no answers. If you are using the book in a classroom, perhaps this is okay, but it is terrible for the independent reader. I loathe that an author withholds information from me, presumably to avoid &#39;cheating&#39; by the student in the classroom.&lt;/p&gt; &#xA;&lt;p&gt;From my point of view none of this is necessary. Certainly if you are designing a Kalman filter for an aircraft or missile you must thoroughly master all of the mathematics and topics in a typical Kalman filter textbook. I just want to track an image on a screen, or write some code for an Arduino project. I want to know how the plots in the book are made, and chose different parameters than the author chose. I want to run simulations. I want to inject more noise in the signal and see how a filter performs. There are thousands of opportunities for using Kalman filters in everyday code, and yet this fairly straightforward topic is the provenance of rocket scientists and academics.&lt;/p&gt; &#xA;&lt;p&gt;I wrote this book to address all of those needs. This is not the book for you if you program navigation computers for Boeing or design radars for Raytheon. Go get an advanced degree at Georgia Tech, UW, or the like, because you&#39;ll need it. This book is for the hobbyist, the curious, and the working engineer that needs to filter or smooth data.&lt;/p&gt; &#xA;&lt;p&gt;This book is interactive. While you can read it online as static content, I urge you to use it as intended. It is written using Jupyter Notebook, which allows me to combine text, math, Python, and Python output in one place. Every plot, every piece of data in this book is generated from Python that is available to you right inside the notebook. Want to double the value of a parameter? Click on the Python cell, change the parameter&#39;s value, and click &#39;Run&#39;. A new plot or printed output will appear in the book.&lt;/p&gt; &#xA;&lt;p&gt;This book has exercises, but it also has the answers. I trust you. If you just need an answer, go ahead and read the answer. If you want to internalize this knowledge, try to implement the exercise before you read the answer.&lt;/p&gt; &#xA;&lt;p&gt;This book has supporting libraries for computing statistics, plotting various things related to filters, and for the various filters that we cover. This does require a strong caveat; most of the code is written for didactic purposes. It is rare that I chose the most efficient solution (which often obscures the intent of the code), and in the first parts of the book I did not concern myself with numerical stability. This is important to understand - Kalman filters in aircraft are carefully designed and implemented to be numerically stable; the naive implementation is not stable in many cases. If you are serious about Kalman filters this book will not be the last book you need. My intention is to introduce you to the concepts and mathematics, and to get you to the point where the textbooks are approachable.&lt;/p&gt; &#xA;&lt;p&gt;Finally, this book is free. The cost for the books required to learn Kalman filtering is somewhat prohibitive even for a Silicon Valley engineer like myself; I cannot believe they are within the reach of someone in a depressed economy, or a financially struggling student. I have gained so much from free software like Python, and free books like those from Allen B. Downey &lt;a href=&#34;http://www.greenteapress.com/&#34;&gt;here&lt;/a&gt;. It&#39;s time to repay that. So, the book is free, it is hosted on free servers, and it uses only free and open software such as IPython and MathJax to create the book.&lt;/p&gt; &#xA;&lt;h2&gt;Reading Online&lt;/h2&gt; &#xA;&lt;p&gt;The book is written as a collection of Jupyter Notebooks, an interactive, browser based system that allows you to combine text, Python, and math into your browser. There are multiple ways to read these online, listed below.&lt;/p&gt; &#xA;&lt;h3&gt;binder&lt;/h3&gt; &#xA;&lt;p&gt;binder serves interactive notebooks online, so you can run the code and change the code within your browser without downloading the book or installing Jupyter.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://beta.mybinder.org/v2/gh/rlabbe/Kalman-and-Bayesian-Filters-in-Python/master&#34;&gt;&lt;img src=&#34;http://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;nbviewer&lt;/h3&gt; &#xA;&lt;p&gt;The website &lt;a href=&#34;http://nbviewer.org&#34;&gt;http://nbviewer.org&lt;/a&gt; provides a Jupyter Notebook server that renders notebooks stored at github (or elsewhere). The rendering is done in real time when you load the book. You may use &lt;a href=&#34;http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/table_of_contents.ipynb&#34;&gt;&lt;em&gt;this nbviewer link&lt;/em&gt;&lt;/a&gt; to access my book via nbviewer. If you read my book today, and then I make a change tomorrow, when you go back tomorrow you will see that change. Notebooks are rendered statically - you can read them, but not modify or run the code.&lt;/p&gt; &#xA;&lt;p&gt;nbviewer seems to lag the checked in version by a few days, so you might not be reading the most recent content.&lt;/p&gt; &#xA;&lt;h3&gt;GitHub&lt;/h3&gt; &#xA;&lt;p&gt;GitHub is able to render the notebooks directly. The quickest way to view a notebook is to just click on them above. However, it renders the math incorrectly, and I cannot recommend using it if you are doing more than just dipping into the book.&lt;/p&gt; &#xA;&lt;h2&gt;PDF Version&lt;/h2&gt; &#xA;&lt;p&gt;A PDF version of the book is available [here]&lt;a href=&#34;https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view?usp=sharing&amp;amp;resourcekey=0-41olC9ht9xE3wQe2zHZ45A&#34;&gt;https://drive.google.com/file/d/0By_SW19c1BfhSVFzNHc0SjduNzg/view?usp=sharing&amp;amp;resourcekey=0-41olC9ht9xE3wQe2zHZ45A&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;The PDF will usually lag behind what is in github as I don&#39;t update it for every minor check in.&lt;/p&gt; &#xA;&lt;h2&gt;Downloading and Running the Book&lt;/h2&gt; &#xA;&lt;p&gt;However, this book is intended to be interactive and I recommend using it in that form. It&#39;s a little more effort to set up, but worth it. If you install IPython and some supporting libraries on your computer and then clone this book you will be able to run all of the code in the book yourself. You can perform experiments, see how filters react to different data, see how different filters react to the same data, and so on. I find this sort of immediate feedback both vital and invigorating. You do not have to wonder &#34;what happens if&#34;. Try it and see!&lt;/p&gt; &#xA;&lt;p&gt;The book and supporting software can be downloaded from GitHub by running this command on the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --depth=1 https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python.git&#xA;pip install filterpy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instructions for installation of the IPython ecosystem can be found in the Installation appendix, found &lt;a href=&#34;http://nbviewer.ipython.org/github/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/Appendix-A-Installation.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once the software is installed you can navigate to the installation directory and run Jupyter notebook with the command line instruction&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will open a browser window showing the contents of the base directory. The book is organized into chapters, each contained within one IPython Notebook (these notebook files have a .ipynb file extension). For example, to read Chapter 2, click on the file &lt;em&gt;02-Discrete-Bayes.ipynb&lt;/em&gt;. Sometimes there are supporting notebooks for doing things like generating animations that are displayed in the chapter. These are not intended to be read by the end user, but of course if you are curious as to how an animation is made go ahead and take a look. You can find these notebooks in the folder named &lt;em&gt;Supporting_Notebooks&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This is admittedly a somewhat cumbersome interface to a book; I am following in the footsteps of several other projects that are somewhat repurposing Jupyter Notebook to generate entire books. I feel the slight annoyances have a huge payoff - instead of having to download a separate code base and run it in an IDE while you try to read a book, all of the code and text is in one place. If you want to alter the code, you may do so and immediately see the effects of your change. If you find a bug, you can make a fix, and push it back to my repository so that everyone in the world benefits. And, of course, you will never encounter a problem I face all the time with traditional books - the book and the code are out of sync with each other, and you are left scratching your head as to which source to trust.&lt;/p&gt; &#xA;&lt;h2&gt;Companion Software&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://pypi.python.org/pypi/filterpy&#34;&gt;&lt;img src=&#34;http://img.shields.io/pypi/v/filterpy.svg?sanitize=true&#34; alt=&#34;Latest Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I wrote an open source Bayesian filtering Python library called &lt;strong&gt;FilterPy&lt;/strong&gt;. I have made the project available on PyPi, the Python Package Index. To install from PyPi, at the command line issue the command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install filterpy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you do not have pip, you may follow the instructions here: &lt;a href=&#34;https://pip.pypa.io/en/latest/installing.html&#34;&gt;https://pip.pypa.io/en/latest/installing.html&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All of the filters used in this book as well as others not in this book are implemented in my Python library FilterPy, available &lt;a href=&#34;https://github.com/rlabbe/filterpy&#34;&gt;here&lt;/a&gt;. You do not need to download or install this to read the book, but you will likely want to use this library to write your own filters. It includes Kalman filters, Fading Memory filters, H infinity filters, Extended and Unscented filters, least square filters, and many more. It also includes helper routines that simplify the designing the matrices used by some of the filters, and other code such as Kalman based smoothers.&lt;/p&gt; &#xA;&lt;p&gt;FilterPy is hosted on github at (&lt;a href=&#34;https://github.com/rlabbe/filterpy&#34;&gt;https://github.com/rlabbe/filterpy&lt;/a&gt;). If you want the bleeding edge release you will want to grab a copy from github, and follow your Python installation&#39;s instructions for adding it to the Python search path. This might expose you to some instability since you might not get a tested release, but as a benefit you will also get all of the test scripts used to test the library. You can examine these scripts to see many examples of writing and running filters while not in the Jupyter Notebook environment.&lt;/p&gt; &#xA;&lt;h2&gt;Alternative Way of Running the Book in Conda environment&lt;/h2&gt; &#xA;&lt;p&gt;If you have conda or miniconda installed, you can create an environment by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda env update -f environment.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate kf_bf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda deactivate kf_bf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to activate and deactivate the environment.&lt;/p&gt; &#xA;&lt;h2&gt;Issues or Questions&lt;/h2&gt; &#xA;&lt;p&gt;If you have comments, you can write an issue at GitHub so that everyone can read it along with my response. Please don&#39;t view it as a way to report bugs only. Alternatively I&#39;ve created a gitter room for more informal discussion. &lt;a href=&#34;https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/rlabbe/Kalman-and-Bayesian-Filters-in-Python&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;https://i.creativecommons.org/l/by/4.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;span xmlns:dct=&#34;http://purl.org/dc/terms/&#34; property=&#34;dct:title&#34;&gt;Kalman and Bayesian Filters in Python&lt;/span&gt; by &lt;a xmlns:cc=&#34;http://creativecommons.org/ns#&#34; href=&#34;https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python&#34; property=&#34;cc:attributionName&#34; rel=&#34;cc:attributionURL&#34;&gt;Roger R. Labbe&lt;/a&gt; is licensed under a &lt;a rel=&#34;license&#34; href=&#34;http://creativecommons.org/licenses/by/4.0/&#34;&gt;Creative Commons Attribution 4.0 International License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All software in this book, software that supports this book (such as in the the code directory) or used in the generation of the book (in the pdf directory) that is contained in this repository is licensed under the following MIT license:&lt;/p&gt; &#xA;&lt;p&gt;The MIT License (MIT)&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2015 Roger R. Labbe Jr&lt;/p&gt; &#xA;&lt;p&gt;Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &#34;Software&#34;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:&lt;/p&gt; &#xA;&lt;p&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/p&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#34;AS IS&#34;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.TION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;rlabbejr at gmail.com&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NielsRogge/Transformers-Tutorials</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/NielsRogge/Transformers-Tutorials</id>
    <link href="https://github.com/NielsRogge/Transformers-Tutorials" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository contains demos I made with the Transformers library by HuggingFace.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Transformers-Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;Hi there!&lt;/p&gt; &#xA;&lt;p&gt;This repository contains demos I made with the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers library&lt;/a&gt; by 🤗 HuggingFace. Currently, all of them are implemented in PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;NOTE: if you are not familiar with HuggingFace and/or Transformers, I highly recommend to check out our &lt;a href=&#34;https://huggingface.co/course/chapter1&#34;&gt;free course&lt;/a&gt;, which introduces you to several Transformer architectures (such as BERT, GPT-2, T5, BART, etc.), as well as an overview of the HuggingFace libraries, including &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;Tokenizers&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;Datasets&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;Accelerate&lt;/a&gt; and the &lt;a href=&#34;https://huggingface.co/&#34;&gt;hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Currently, it contains the following demos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;BERT (&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;BertForTokenClassification&lt;/code&gt; on a named entity recognition (NER) dataset. &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;BertForSequenceClassification&lt;/code&gt; for multi-label text classification. &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;BEiT (&lt;a href=&#34;https://raw.githubusercontent.com/NielsRogge/Transformers-Tutorials/master/%5Bhttps://arxiv.org/abs/2103.06874%5D(https://arxiv.org/abs/2106.08254)&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;understanding &lt;code&gt;BeitForMaskedImageModeling&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BEiT/Understanding_BeitForMaskedImageModeling.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;CANINE (&lt;a href=&#34;https://arxiv.org/abs/2103.06874&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;CanineForSequenceClassification&lt;/code&gt; on IMDb &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/CANINE/Fine_tune_CANINE_on_IMDb_(movie_review_binary_classification).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ConvNeXT (&lt;a href=&#34;https://arxiv.org/abs/2201.03545&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning (and performing inference with) &lt;code&gt;ConvNextForImageClassification&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ConvNeXT/Fine_tune_ConvNeXT_for_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DPT (&lt;a href=&#34;https://arxiv.org/abs/2103.13413&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with DPT for monocular depth estimation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DPT/DPT_inference_notebook_(depth_estimation).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with DPT for semantic segmentation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DPT/DPT_inference_notebook_(semantic_segmentation).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DETR (&lt;a href=&#34;https://arxiv.org/abs/2005.12872&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;DetrForObjectDetection&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;DetrForObjectDetection&lt;/code&gt; on a custom object detection dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evaluating &lt;code&gt;DetrForObjectDetection&lt;/code&gt; on the COCO detection 2017 validation set &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Evaluating_DETR_on_COCO_validation_2017.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;DetrForSegmentation&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_panoptic_segmentation_minimal_example_(with_DetrFeatureExtractor).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;DetrForSegmentation&lt;/code&gt; on COCO panoptic 2017 &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForSegmentation_on_custom_dataset_end_to_end_approach.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;DiT (&lt;a href=&#34;https://arxiv.org/abs/2203.02378&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with DiT for document image classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DiT/Inference_with_DiT_(Document_Image_Transformer)_for_document_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GLPN (&lt;a href=&#34;https://arxiv.org/abs/2201.07436&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;GLPNForDepthEstimation&lt;/code&gt; to illustrate monocular depth estimation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GLPN/GLPN_inference_(depth_estimation).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GPT-J-6B (&lt;a href=&#34;https://github.com/kingoflolz/mesh-transformer-jax&#34;&gt;repository&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;GPTJForCausalLM&lt;/code&gt; to illustrate few-shot learning and code generation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/GPT-J-6B/Inference_with_GPT_J_6B.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ImageGPT (&lt;a href=&#34;https://openai.com/blog/image-gpt/&#34;&gt;blog post&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(un)conditional image generation with &lt;code&gt;ImageGPTForCausalLM&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ImageGPT/(Un)conditional_image_generation_with_ImageGPT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;linear probing with ImageGPT &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ImageGPT/Linear_probing_with_ImageGPT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LUKE (&lt;a href=&#34;https://arxiv.org/abs/2010.01057&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LukeForEntityPairClassification&lt;/code&gt; on a custom relation extraction dataset using PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LUKE/Supervised_relation_extraction_with_LukeForEntityPairClassification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LayoutLM (&lt;a href=&#34;https://arxiv.org/abs/1912.13318&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMForTokenClassification&lt;/code&gt; on the &lt;a href=&#34;https://guillaumejaume.github.io/FUNSD/&#34;&gt;FUNSD&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMForSequenceClassification&lt;/code&gt; on the &lt;a href=&#34;https://www.cs.cmu.edu/~aharley/rvl-cdip/&#34;&gt;RVL-CDIP&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;adding image embeddings to LayoutLM during fine-tuning on the &lt;a href=&#34;https://guillaumejaume.github.io/FUNSD/&#34;&gt;FUNSD&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Add_image_embeddings_to_LayoutLM.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv2 (&lt;a href=&#34;https://arxiv.org/abs/2012.14740&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForSequenceClassification&lt;/code&gt; on RVL-CDIP &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/RVL-CDIP/Fine_tuning_LayoutLMv2ForSequenceClassification_on_RVL_CDIP.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on FUNSD &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on FUNSD using the 🤗 Trainer &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on FUNSD &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;true inference with &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; (when no labels are available) + Gradio demo &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForTokenClassification&lt;/code&gt; on CORD &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/CORD/Fine_tuning_LayoutLMv2ForTokenClassification_on_CORD.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv2ForQuestionAnswering&lt;/code&gt; on DOCVQA &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv3 (&lt;a href=&#34;https://arxiv.org/abs/2204.08387&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;LayoutLMv3ForTokenClassification&lt;/code&gt; on the &lt;a href=&#34;https://guillaumejaume.github.io/FUNSD/&#34;&gt;FUNSD&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MaskFormer (&lt;a href=&#34;https://arxiv.org/abs/2107.06278&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;MaskFormer&lt;/code&gt; (both semantic and panoptic segmentation): &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/MaskFormer/maskformer_minimal_example(with_MaskFormerFeatureExtractor).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;MaskFormer&lt;/code&gt; on a custom dataset for semantic segmentation &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/MaskFormer/Fine_tune_MaskFormer_on_custom_dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Perceiver IO (&lt;a href=&#34;https://arxiv.org/abs/2107.14795&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;showcasing masked language modeling and image classification with the Perceiver &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_masked_language_modeling_and_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning the Perceiver for image classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Fine_tune_the_Perceiver_for_image_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning the Perceiver for text classification &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Fine_tune_Perceiver_for_text_classification.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;predicting optical flow between a pair of images with &lt;code&gt;PerceiverForOpticalFlow&lt;/code&gt;&lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_Optical_Flow.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;auto-encoding a video (images, audio, labels) with &lt;code&gt;PerceiverForMultimodalAutoencoding&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_Multimodal_Autoencoding.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;SegFormer (&lt;a href=&#34;https://arxiv.org/abs/2105.15203&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;SegformerForSemanticSegmentation&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Segformer_inference_notebook.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;SegformerForSemanticSegmentation&lt;/code&gt; on custom data using native PyTorch &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;T5 (&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;T5ForConditionalGeneration&lt;/code&gt; on a Dutch summarization dataset on TPU using HuggingFace Accelerate &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/tree/master/T5&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;T5ForConditionalGeneration&lt;/code&gt; (CodeT5) for Ruby code summarization using PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/T5/Fine_tune_CodeT5_for_generating_docstrings_from_Ruby_code.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;TAPAS (&lt;a href=&#34;https://arxiv.org/abs/2004.02349&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;TapasForQuestionAnswering&lt;/code&gt; on the Microsoft &lt;a href=&#34;https://www.microsoft.com/en-us/download/details.aspx?id=54253&#34;&gt;Sequential Question Answering (SQA)&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evaluating &lt;code&gt;TapasForSequenceClassification&lt;/code&gt; on the &lt;a href=&#34;https://tabfact.github.io/&#34;&gt;Table Fact Checking (TabFact)&lt;/a&gt; dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;TrOCR (&lt;a href=&#34;https://arxiv.org/abs/2109.10282&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;TrOCR&lt;/code&gt; to illustrate optical character recognition with Transformers, as well as making a Gradio demo &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Inference_with_TrOCR_%2B_Gradio_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;TrOCR&lt;/code&gt; on the IAM dataset using the Seq2SeqTrainer &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;TrOCR&lt;/code&gt; on the IAM dataset using native PyTorch &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evaluating &lt;code&gt;TrOCR&lt;/code&gt; on the IAM test set &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Evaluating_TrOCR_base_handwritten_on_the_IAM_test_set.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ViLT (&lt;a href=&#34;https://arxiv.org/abs/2102.03334&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;ViLT&lt;/code&gt; for visual question answering (VQA) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Fine_tuning_ViLT_for_VQA.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViLT&lt;/code&gt; to illustrate visual question answering (VQA) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Inference_with_ViLT_(visual_question_answering).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;masked language modeling (MLM) with a pre-trained &lt;code&gt;ViLT&lt;/code&gt; model &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Masked_language_modeling_with_ViLT.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViLT&lt;/code&gt; for image-text retrieval &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/Using_ViLT_for_image_text_retrieval.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViLT&lt;/code&gt; to illustrate natural language for visual reasoning (NLVR) &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViLT/ViLT_for_natural_language_visual_reasoning.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ViTMAE (&lt;a href=&#34;https://arxiv.org/abs/2111.06377&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;reconstructing pixel values with &lt;code&gt;ViTMAEForPreTraining&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/ViTMAE/ViT_MAE_visualization_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Vision Transformer (&lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;performing inference with &lt;code&gt;ViTForImageClassification&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Quick_demo_of_HuggingFace_version_of_Vision_Transformer_inference.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;ViTForImageClassification&lt;/code&gt; on &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34;&gt;CIFAR-10&lt;/a&gt; using PyTorch Lightning &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;ViTForImageClassification&lt;/code&gt; on &lt;a href=&#34;https://www.cs.toronto.edu/~kriz/cifar.html&#34;&gt;CIFAR-10&lt;/a&gt; using the 🤗 Trainer &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;YOLOS (&lt;a href=&#34;https://arxiv.org/abs/2106.00666&#34;&gt;paper&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;fine-tuning &lt;code&gt;YolosForObjectDetection&lt;/code&gt; on a custom dataset &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/YOLOS/Fine_tuning_YOLOS_for_object_detection_on_custom_dataset_(balloon).ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;inference with &lt;code&gt;YolosForObjectDetection&lt;/code&gt; &lt;a href=&#34;https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/YOLOS/YOLOS_minimal_inference_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;... more to come! 🤗&lt;/p&gt; &#xA;&lt;p&gt;If you have any questions regarding these demos, feel free to open an issue on this repository.&lt;/p&gt; &#xA;&lt;p&gt;Btw, I was also the main contributor to add the following algorithms to the library:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TAbular PArSing (TAPAS) by Google AI&lt;/li&gt; &#xA; &lt;li&gt;Vision Transformer (ViT) by Google AI&lt;/li&gt; &#xA; &lt;li&gt;DINO by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;Data-efficient Image Transformers (DeiT) by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;LUKE by Studio Ousia&lt;/li&gt; &#xA; &lt;li&gt;DEtection TRansformers (DETR) by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;CANINE by Google AI&lt;/li&gt; &#xA; &lt;li&gt;BEiT by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv2 (and LayoutXLM) by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;TrOCR by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;SegFormer by NVIDIA&lt;/li&gt; &#xA; &lt;li&gt;ImageGPT by OpenAI&lt;/li&gt; &#xA; &lt;li&gt;Perceiver by Deepmind&lt;/li&gt; &#xA; &lt;li&gt;MAE by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;ViLT by NAVER AI Lab&lt;/li&gt; &#xA; &lt;li&gt;ConvNeXT by Facebook AI&lt;/li&gt; &#xA; &lt;li&gt;DiT By Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;GLPN by KAIST&lt;/li&gt; &#xA; &lt;li&gt;DPT by Intel Labs&lt;/li&gt; &#xA; &lt;li&gt;TAPEX by Microsoft Research&lt;/li&gt; &#xA; &lt;li&gt;LayoutLMv3 by Microsoft Research&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All of them were an incredible learning experience. I can recommend anyone to contribute an AI algorithm to the library!&lt;/p&gt; &#xA;&lt;h2&gt;Data preprocessing&lt;/h2&gt; &#xA;&lt;p&gt;Regarding preparing your data for a PyTorch model, there are a few options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a native PyTorch dataset + dataloader. This is the standard way to prepare data for a PyTorch model, namely by subclassing &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;, and then a creating corresponding &lt;code&gt;DataLoader&lt;/code&gt; (which is a Python generator that allows to loop over the items of a dataset). When subclassing the &lt;code&gt;Dataset&lt;/code&gt; class, one needs to implement 3 methods: &lt;code&gt;__init__&lt;/code&gt;, &lt;code&gt;__len__&lt;/code&gt; (which returns the number of examples of the dataset) and &lt;code&gt;__getitem__&lt;/code&gt; (which returns an example of the dataset, given an integer index). Here&#39;s an example of creating a basic text classification dataset (assuming one has a CSV that contains 2 columns, namely &#34;text&#34; and &#34;label&#34;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import Dataset&#xA;&#xA;class CustomTrainDataset(Dataset):&#xA;    def __init__(self, df, tokenizer):&#xA;        self.df = df&#xA;        self.tokenizer = tokenizer&#xA;&#xA;    def __len__(self):&#xA;        return len(self.df)&#xA;&#xA;    def __getitem__(self, idx):&#xA;        # get item&#xA;        item = df.iloc[idx]&#xA;        text = item[&#39;text&#39;]&#xA;        label = item[&#39;label&#39;]&#xA;        # encode text&#xA;        encoding = self.tokenizer(text, padding=&#34;max_length&#34;, max_length=128, truncation=True, return_tensors=&#34;pt&#34;)&#xA;        # remove batch dimension which the tokenizer automatically adds&#xA;        encoding = {k:v.squeeze() for k,v in encoding.items()}&#xA;        # add label&#xA;        encoding[&#34;label&#34;] = torch.tensor(label)&#xA;        &#xA;        return encoding&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instantiating the dataset then happens as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import BertTokenizer&#xA;import pandas as pd&#xA;&#xA;tokenizer = BertTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)&#xA;df = pd.read_csv(&#34;path_to_your_csv&#34;)&#xA;&#xA;train_dataset = CustomTrainDataset(df=df tokenizer=tokenizer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Accessing the first example of the dataset can then be done as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;encoding = train_dataset[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In practice, one creates a corresponding &lt;code&gt;DataLoader&lt;/code&gt;, that allows to get batches from the dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import DataLoader&#xA;&#xA;train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I often check whether the data is created correctly by fetching the first batch from the data loader, and then printing out the shapes of the tensors, decoding the input_ids back to text, etc.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;batch = next(iter(train_dataloader))&#xA;for k,v in batch.items():&#xA;    print(k, v.shape)&#xA;# decode the input_ids of the first example of the batch&#xA;print(tokenizer.decode(batch[&#39;input_ids&#39;][0].tolist())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/datasets/&#34;&gt;HuggingFace Datasets&lt;/a&gt;. Datasets is a library by HuggingFace that allows to easily load and process data in a very fast and memory-efficient way. It is backed by &lt;a href=&#34;https://arrow.apache.org/&#34;&gt;Apache Arrow&lt;/a&gt;, and has cool features such as memory-mapping, which allow you to only load data into RAM when it is required. It only has deep interoperability with the &lt;a href=&#34;https://huggingface.co/datasets&#34;&gt;HuggingFace hub&lt;/a&gt;, allowing to easily load well-known datasets as well as share your own with the community.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Loading a custom dataset as a Dataset object can be done as follows (you can install datasets using &lt;code&gt;pip install datasets&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from datasets import load_dataset&#xA;&#xA;dataset = load_dataset(&#39;csv&#39;, data_files={&#39;train&#39;: [&#39;my_train_file_1.csv&#39;, &#39;my_train_file_2.csv&#39;] &#39;test&#39;: &#39;my_test_file.csv&#39;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here I&#39;m loading local CSV files, but there are other formats supported (including JSON, Parquet, txt) as well as loading data from a local Pandas dataframe or dictionary for instance. You can check out the &lt;a href=&#34;https://huggingface.co/docs/datasets/loading.html#local-and-remote-files&#34;&gt;docs&lt;/a&gt; for all details.&lt;/p&gt; &#xA;&lt;h2&gt;Training frameworks&lt;/h2&gt; &#xA;&lt;p&gt;Regarding fine-tuning Transformer models (or more generally, PyTorch models), there are a few options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;using native PyTorch. This is the most basic way to train a model, and requires the user to manually write the training loop. The advantage is that this is very easy to debug. The disadvantage is that one needs to implement training him/herself, such as setting the model in the appropriate mode (&lt;code&gt;model.train()&lt;/code&gt;/&lt;code&gt;model.eval()&lt;/code&gt;), handle device placement (&lt;code&gt;model.to(device)&lt;/code&gt;), etc. A typical training loop in PyTorch looks as follows (inspired by &lt;a href=&#34;&#34;&gt;this great PyTorch intro tutorial&lt;/a&gt;):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;model = ...&#xA;&#xA;# I almost always use a learning rate of 5e-5 when fine-tuning Transformer based models&#xA;optimizer = torch.optim.Adam(model.parameters(), lr=5-e5)&#xA;&#xA;# put model on GPU, if available&#xA;device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;model.to(device)&#xA;&#xA;for epoch in range(epochs):&#xA;    model.train()&#xA;    train_loss = 0.0&#xA;    for batch in train_dataloader:&#xA;        # put batch on device&#xA;        batch = {k:v.to(device) for k,v in batch.items()}&#xA;        &#xA;        # forward pass&#xA;        outputs = model(**batch)&#xA;        loss = outputs.loss&#xA;        &#xA;        train_loss += loss.item()&#xA;        &#xA;        loss.backward()&#xA;        optimizer.step()&#xA;        optimizer.zero_grad()&#xA;&#xA;    print(&#34;Loss after epoch {epoch}:&#34;, train_loss/len(train_dataloader))&#xA;    &#xA;    model.eval()&#xA;    val_loss = 0.0&#xA;    with torch.no_grad():&#xA;        for batch in eval_dataloader:&#xA;            # put batch on device&#xA;            batch = {k:v.to(device) for k,v in batch.items()}&#xA;            &#xA;            # forward pass&#xA;            outputs = model(**batch)&#xA;            loss = outputs.logits&#xA;            &#xA;            val_loss += loss.item()&#xA;                  &#xA;    print(&#34;Validation loss after epoch {epoch}:&#34;, val_loss/len(eval_dataloader))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pytorchlightning.ai/&#34;&gt;PyTorch Lightning (PL)&lt;/a&gt;. PyTorch Lightning is a framework that automates the training loop written above, by abstracting it away in a Trainer object. Users don&#39;t need to write the training loop themselves anymore, instead they can just do &lt;code&gt;trainer = Trainer()&lt;/code&gt; and then &lt;code&gt;trainer.fit(model)&lt;/code&gt;. The advantage is that you can start training models very quickly (hence the name lightning), as all training-related code is handled by the &lt;code&gt;Trainer&lt;/code&gt; object. The disadvantage is that it may be more difficult to debug your model, as the training and evaluation is now abstracted away.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/transformers/main_classes/trainer.html&#34;&gt;HuggingFace Trainer&lt;/a&gt;. The HuggingFace Trainer API can be seen as a framework similar to PyTorch Lightning in the sense that it also abstracts the training away using a Trainer object. However, contrary to PyTorch Lightning, it is not meant not be a general framework. Rather, it is made especially for fine-tuning Transformer-based models available in the HuggingFace Transformers library. The Trainer also has an extension called &lt;code&gt;Seq2SeqTrainer&lt;/code&gt; for encoder-decoder models, such as BART, T5 and the &lt;code&gt;EncoderDecoderModel&lt;/code&gt; classes. Note that all &lt;a href=&#34;https://github.com/huggingface/transformers/tree/master/examples/pytorch&#34;&gt;PyTorch example scripts&lt;/a&gt; of the Transformers library make use of the Trainer.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;HuggingFace Accelerate&lt;/a&gt;: Accelerate is a new project, that is made for people who still want to write their own training loop (as shown above), but would like to make it work automatically irregardless of the hardware (i.e. multiple GPUs, TPU pods, mixed precision, etc.).&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example</id>
    <link href="https://github.com/ine-rmotr-curriculum/FreeCodeCamp-Pandas-Real-Life-Example" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;rmotr.com&lt;/h3&gt; &#xA;&lt;h1&gt;Data Science with Python Course&lt;/h1&gt; &#xA;&lt;p&gt;This material is created for our &lt;a href=&#34;https://rmotr.com/data-science-python-course&#34;&gt;Data Science with Python Course&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bnsreenu/python_for_microscopists</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/bnsreenu/python_for_microscopists</id>
    <link href="https://github.com/bnsreenu/python_for_microscopists" rel="alternate"></link>
    <summary type="html">&lt;p&gt;https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w?sub_confirmation=1&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;b&gt; Author: Dr. Sreenivas Bhattiprolu &lt;/b&gt;&lt;br&gt; &lt;b&gt;Twitter: @digitalsreeni &lt;/b&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Python for Microscopists and other image processing enthusiasts&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w?sub_confirmation=1&#34;&gt;https://www.youtube.com/channel/UC34rW-HtPJulxr5wp2Xa04w?sub_confirmation=1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The YouTube channel associated with this code walks you through the entire process of learning to code in Python; all the way from basics to advanced machine learning and deep learning. The primary emphasis will be on image processing and other relevant functionality.&lt;/p&gt; &#xA;&lt;p&gt;Why did I create this channel? To help you (students and researchers) gain a new skill and succeed in your respective fields.&lt;/p&gt; &#xA;&lt;p&gt;You may think coding is hard and that it&#39;s not your cup of tea, but Python made it easy to code even advanced algorithms. In addition, coding will make you self sufficient, it will teach you how to think, it improves your collaborative skills and it can take your career to new heights. Therefore, if you want to stay ahead of your peers and relevant in your field, overcome your fears and start coding!&lt;/p&gt; &#xA;&lt;p&gt;Also, checkout WWW.APEER.COM if you want free image processing in the cloud! Free for non-profits / academics / personal use.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alembics/disco-diffusion</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/alembics/disco-diffusion</id>
    <link href="https://github.com/alembics/disco-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Disco Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open in Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A frankensteinian amalgamation of notebooks, models and techniques for the generation of AI Art and Animations.&lt;/p&gt; &#xA;&lt;p&gt;[to be updated with further info soon]&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project uses a special conversion tool to convert the python files into notebooks for easier development.&lt;/p&gt; &#xA;&lt;p&gt;What this means is you do not have to touch the notebook directly to make changes to it&lt;/p&gt; &#xA;&lt;p&gt;the tool being used is called &lt;a href=&#34;https://github.com/MSFTserver/colab-convert&#34;&gt;Colab-Convert&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;install using &lt;code&gt;pip install colab-convert&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .py to .ipynb &lt;code&gt;colab-convert /path/to/file.py /path/to/file.ipynb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;convert .ipynb to .py &lt;code&gt;colab-convert /path/to/file.ipynb /path/to/file.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;h4&gt;v1 Oct 29th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial QoL improvements added, including user friendly UI, settings+prompt saving and improved google drive folder organization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v1.1 Nov 13th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now includes sizing options, intermediate saves and fixed image prompts and perlin inits. unexposed batch option since it doesn&#39;t work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v2 Update: Nov 22nd 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial addition of Katherine Crowson&#39;s Secondary Model Method (&lt;a href=&#34;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&#34;&gt;https://colab.research.google.com/drive/1mpkrhOjoyzPeSWy2r7T8EYRaU7amYOOi#scrollTo=X5gODNAMEUCR&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fix for incorrectly named settings files&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v3 Update: Dec 24th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Dango&#39;s advanced cutout method&lt;/li&gt; &#xA; &lt;li&gt;Added SLIP models, thanks to NeuralDivergent&lt;/li&gt; &#xA; &lt;li&gt;Fixed issue with NaNs resulting in black images, with massive help and testing from @Softology&lt;/li&gt; &#xA; &lt;li&gt;Perlin now changes properly within batches (not sure where this perlin_regen code came from originally, but thank you)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4 Update: Jan 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Implemented Diffusion Zooming&lt;/li&gt; &#xA; &lt;li&gt;Added Chigozie keyframing&lt;/li&gt; &#xA; &lt;li&gt;Made a bunch of edits to processes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v4.1 Update: Jan 14th 2021 - Somnai&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added video input mode&lt;/li&gt; &#xA; &lt;li&gt;Added license that somehow went missing&lt;/li&gt; &#xA; &lt;li&gt;Added improved prompt keyframing, fixed image_prompts and multiple prompts&lt;/li&gt; &#xA; &lt;li&gt;Improved UI&lt;/li&gt; &#xA; &lt;li&gt;Significant under the hood cleanup and improvement&lt;/li&gt; &#xA; &lt;li&gt;Refined defaults for each mode&lt;/li&gt; &#xA; &lt;li&gt;Removed SLIP models for the time being due to import conflicts&lt;/li&gt; &#xA; &lt;li&gt;Added latent-diffusion SuperRes for sharpening&lt;/li&gt; &#xA; &lt;li&gt;Added resume run mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5 Update: Feb 20th 2022 - gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added 3D animation mode. Uses weighted combination of AdaBins and MiDaS depth estimation models. Uses pytorch3d for 3D transforms on Colab and/or Linux.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Mar 30th 2022 - zippy / Chris Allen and gandamu / Adam Letts&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Integrated Turbo+Smooth features from Disco Diffusion Turbo -- just the implementation, without its defaults.&lt;/li&gt; &#xA; &lt;li&gt;Implemented resume of turbo animations in such a way that it&#39;s now possible to resume from different batch folders and batch numbers.&lt;/li&gt; &#xA; &lt;li&gt;3D rotation parameter units are now degrees (rather than radians)&lt;/li&gt; &#xA; &lt;li&gt;Corrected name collision in sampling_mode (now diffusion_sampling_mode for plms/ddim, and sampling_mode for 3D transform sampling)&lt;/li&gt; &#xA; &lt;li&gt;Added video_init_seed_continuity option to make init video animations more continuous&lt;/li&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.1 Update: Apr 4th 2022 - MSFTserver aka HostsServer&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed pytorch3d from needing to be compiled with a lite version specifically made for Disco Diffusion&lt;/li&gt; &#xA; &lt;li&gt;Remove Super Resolution&lt;/li&gt; &#xA; &lt;li&gt;Remove Slip Models&lt;/li&gt; &#xA; &lt;li&gt;Update for crossplatform support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;v5.2 Update: Apr 10th 2022 - nin_artificial / Tom Mason&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VR Mode&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notebook Provenance&lt;/h2&gt; &#xA;&lt;p&gt;Original notebook by Katherine Crowson (&lt;a href=&#34;https://github.com/crowsonkb&#34;&gt;https://github.com/crowsonkb&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/RiversHaveWings&#34;&gt;https://twitter.com/RiversHaveWings&lt;/a&gt;). It uses either OpenAI&#39;s 256x256 unconditional ImageNet or Katherine Crowson&#39;s fine-tuned 512x512 diffusion model (&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;https://github.com/openai/guided-diffusion&lt;/a&gt;), together with CLIP (&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;https://github.com/openai/CLIP&lt;/a&gt;) to connect text prompts with images.&lt;/p&gt; &#xA;&lt;p&gt;Modified by Daniel Russell (&lt;a href=&#34;https://github.com/russelldc&#34;&gt;https://github.com/russelldc&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/danielrussruss&#34;&gt;https://twitter.com/danielrussruss&lt;/a&gt;) to include (hopefully) optimal params for quick generations in 15-100 timesteps rather than 1000, as well as more robust augmentations.&lt;/p&gt; &#xA;&lt;p&gt;Further improvements from Dango233 and nsheppard helped improve the quality of diffusion in general, and especially so for shorter runs like this notebook aims to achieve.&lt;/p&gt; &#xA;&lt;p&gt;Vark added code to load in multiple Clip models at once, which all prompts are evaluated against, which may greatly improve accuracy.&lt;/p&gt; &#xA;&lt;p&gt;The latest zoom, pan, rotation, and keyframes features were taken from Chigozie Nri&#39;s VQGAN Zoom Notebook (&lt;a href=&#34;https://github.com/chigozienri&#34;&gt;https://github.com/chigozienri&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/chigozienri&#34;&gt;https://twitter.com/chigozienri&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Advanced DangoCutn Cutout method is also from Dango223.&lt;/p&gt; &#xA;&lt;p&gt;--&lt;/p&gt; &#xA;&lt;p&gt;Somnai (&lt;a href=&#34;https://twitter.com/Somnai_dreams&#34;&gt;https://twitter.com/Somnai_dreams&lt;/a&gt;) added 2D Diffusion animation techniques, QoL improvements and various implementations of tech and techniques, mostly listed in the changelog below.&lt;/p&gt; &#xA;&lt;p&gt;3D animation implementation added by Adam Letts (&lt;a href=&#34;https://twitter.com/gandamu_ml&#34;&gt;https://twitter.com/gandamu_ml&lt;/a&gt;) in collaboration with Somnai.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google-research/google-research</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/google-research/google-research</id>
    <link href="https://github.com/google-research/google-research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Google Research&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Research&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains code released by &lt;a href=&#34;https://research.google&#34;&gt;Google Research&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All datasets in this repository are released under the CC BY 4.0 International license, which can be found here: &lt;a href=&#34;https://creativecommons.org/licenses/by/4.0/legalcode&#34;&gt;https://creativecommons.org/licenses/by/4.0/legalcode&lt;/a&gt;. All source files in this repository are released under the Apache 2.0 license, the text of which can be found in the LICENSE file.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Because the repo is large, we recommend you download only the subdirectory of interest:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;SUBDIR=foo&#xA;svn export https://github.com/google-research/google-research/trunk/$SUBDIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;d like to submit a pull request, you&#39;ll need to clone the repository; we recommend making a shallow clone (without history).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:google-research/google-research.git --depth=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Disclaimer: This is not an official Google product.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CoreyMSchafer/code_snippets</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/CoreyMSchafer/code_snippets</id>
    <link href="https://github.com/CoreyMSchafer/code_snippets" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;code_snippets&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>mrdbourke/tensorflow-deep-learning</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/mrdbourke/tensorflow-deep-learning</id>
    <link href="https://github.com/mrdbourke/tensorflow-deep-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;All course materials for the Zero to Mastery Deep Learning with TensorFlow course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zero to Mastery Deep Learning with TensorFlow&lt;/h1&gt; &#xA;&lt;p&gt;All of the course materials for the &lt;a href=&#34;https://dbourke.link/ZTMTFcourse&#34;&gt;Zero to Mastery Deep Learning with TensorFlow course&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This course will teach you foundations of deep learning and TensorFlow as well as prepare you to pass the TensorFlow Developer Certification exam (optional).&lt;/p&gt; &#xA;&lt;h2&gt;Important links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🎥 Watch the &lt;a href=&#34;https://dbourke.link/tfpart1part2&#34;&gt;first 14-hours of the course on YouTube&lt;/a&gt; (notebooks 00, 01, 02)&lt;/li&gt; &#xA; &lt;li&gt;📖 Read the &lt;a href=&#34;https://dev.mrdbourke.com/tensorflow-deep-learning/&#34;&gt;beautiful online book version of the course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;💻 &lt;a href=&#34;https://dbourke.link/ZTMTFcourse&#34;&gt;Sign up&lt;/a&gt; to the full course on the Zero to Mastery Academy (videos for notebooks 03-10)&lt;/li&gt; &#xA; &lt;li&gt;🤔 Got questions about the course? Check out the &lt;a href=&#34;https://youtu.be/rqAqcFcfeK8&#34;&gt;livestream Q&amp;amp;A for the course launch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents of this page&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#fixes-and-updates&#34;&gt;Fixes and updates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#course-materials&#34;&gt;Course materials&lt;/a&gt; (everything you&#39;ll need for completing the course)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#course-structure&#34;&gt;Course structure&lt;/a&gt; (how this course is taught)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#should-you-do-this-course&#34;&gt;Should you do this course?&lt;/a&gt; (decide by answering a couple simple questions)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#prerequisites&#34;&gt;Prerequisites&lt;/a&gt; (what skills you&#39;ll need to do this course)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-exercises---extra-curriculum&#34;&gt;Exercises &amp;amp; Extra-curriculum&lt;/a&gt; (challenges to practice what you&#39;ve learned and resources to learn more)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#ask-questions&#34;&gt;Ask a question&lt;/a&gt; (like to know more? go here)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#log&#34;&gt;Log&lt;/a&gt; (updates, changes and progress)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Fixes and updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;02 Dec 2021 - Added fix for TensorFlow 2.7.0+ for notebook 02, &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/278&#34;&gt;see discussion for more&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;11 Nov 2021 - Added fix for TensorFlow 2.7.0+ for notebook 01, &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/256&#34;&gt;see discussion for more&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;14 Aug 2021 - Added a &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/166&#34;&gt;discussion with TensorFlow 2.6 updates and EfficientNetV2 notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Course materials&lt;/h2&gt; &#xA;&lt;p&gt;This table is the ground truth for course materials. All the links you need for everything will be here.&lt;/p&gt; &#xA;&lt;p&gt;Key:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Number:&lt;/strong&gt; The number of the target notebook (this may not match the video section of the course but it ties together all of the materials in the table)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Notebook:&lt;/strong&gt; The notebook for a particular module with lots of code and text annotations (notebooks from the videos are based on these)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data/model:&lt;/strong&gt; Links to datasets/pre-trained models for the assosciated notebook&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Exercises &amp;amp; Extra-curriculum:&lt;/strong&gt; Each module comes with a set of exercises and extra-curriculum to help practice your skills and learn more, I suggest going through these &lt;strong&gt;before&lt;/strong&gt; you move onto the next module&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Slides:&lt;/strong&gt; Although we focus on writing TensorFlow code, we sometimes use pretty slides to describe different concepts, you&#39;ll find them here&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; You can get all of the notebook code created during the videos in the &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/tree/main/video_notebooks&#34;&gt;&lt;code&gt;video_notebooks&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Number&lt;/th&gt; &#xA;   &lt;th&gt;Notebook&lt;/th&gt; &#xA;   &lt;th&gt;Data/Model&lt;/th&gt; &#xA;   &lt;th&gt;Exercises &amp;amp; Extra-curriculum&lt;/th&gt; &#xA;   &lt;th&gt;Slides&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;00&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/00_tensorflow_fundamentals.ipynb&#34;&gt;TensorFlow Fundamentals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-00-tensorflow-fundamentals-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/00_introduction_to_tensorflow_and_deep_learning.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;01&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/01_neural_network_regression_in_tensorflow.ipynb&#34;&gt;TensorFlow Regression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-01-neural-network-regression-with-tensorflow-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/01_neural_network_regression_with_tensorflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;02&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/02_neural_network_classification_in_tensorflow.ipynb&#34;&gt;TensorFlow Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-02-neural-network-classification-with-tensorflow-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/02_neural_network_classification_with_tensorflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;03&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/03_convolutional_neural_networks_in_tensorflow.ipynb&#34;&gt;TensorFlow Computer Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip&#34;&gt;&lt;code&gt;pizza_steak&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip&#34;&gt;&lt;code&gt;10_food_classes_all_data&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-03-computer-vision--convolutional-neural-networks-in-tensorflow-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/03_convolution_neural_networks_and_computer_vision_with_tensorflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb&#34;&gt;Transfer Learning Part 1: Feature extraction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip&#34;&gt;&lt;code&gt;10_food_classes_10_percent&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-04-transfer-learning-in-tensorflow-part-1-feature-extraction-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/04_transfer_learning_with_tensorflow_part_1_feature_extraction.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb&#34;&gt;Transfer Learning Part 2: Fine-tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip&#34;&gt;&lt;code&gt;10_food_classes_10_percent&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip&#34;&gt;&lt;code&gt;10_food_classes_1_percent&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip&#34;&gt;&lt;code&gt;10_food_classes_all_data&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-05-transfer-learning-in-tensorflow-part-2-fine-tuning-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/05_transfer_learning_with_tensorflow_part_2_fine_tuning.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;06&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb&#34;&gt;Transfer Learning Part 3: Scaling up&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip&#34;&gt;&lt;code&gt;101_food_classes_10_percent&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip&#34;&gt;&lt;code&gt;custom_food_images&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/06_101_food_class_10_percent_saved_big_dog_model.zip&#34;&gt;&lt;code&gt;fine_tuned_efficientnet_model&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-06-transfer-learning-in-tensorflow-part-3-scaling-up-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/06_transfer_learning_with_tensorflow_part_3_scaling_up.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;07&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/07_food_vision_milestone_project_1.ipynb&#34;&gt;Milestone Project 1: Food Vision 🍔👁&lt;/a&gt;, &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/extras/TEMPLATE_07_food_vision_milestone_project_1.ipynb&#34;&gt;Template (your challenge)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision.zip&#34;&gt;&lt;code&gt;feature_extraction_mixed_precision_efficientnet_model&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_fine_tuned_101_classes_mixed_precision.zip&#34;&gt;&lt;code&gt;fine_tuned_mixed_precision_efficientnet_model&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-07-milestone-project-1--food-vision-big-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/07_milestone_project_1_food_vision.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;08&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/08_introduction_to_nlp_in_tensorflow.ipynb&#34;&gt;TensorFlow NLP Fundamentals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip&#34;&gt;&lt;code&gt;diaster_or_no_diaster_tweets&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/08_model_6_USE_feature_extractor.zip&#34;&gt;&lt;code&gt;USE_feature_extractor_model&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-08-introduction-to-nlp-natural-language-processing-in-tensorflow-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/08_natural_language_processing_in_tensorflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;09&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/09_SkimLit_nlp_milestone_project_2.ipynb&#34;&gt;Milestone Project 2: SkimLit 📄🔥&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Franck-Dernoncourt/pubmed-rct.git&#34;&gt;&lt;code&gt;pubmed_RCT_200k_dataset&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://storage.googleapis.com/ztm_tf_course/skimlit/skimlit_tribrid_model.zip&#34;&gt;&lt;code&gt;skimlit_tribrid_model&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#-09-milestone-project-2-skimlit--exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/09_milestone_project_2_skimlit.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/10_time_series_forecasting_in_tensorflow.ipynb&#34;&gt;TensorFlow Time Series Fundamentals &amp;amp; Milestone Project 3: BitPredict 💰📈 (videos coming soon)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/BTC_USD_2013-10-01_2021-05-18-CoinDesk.csv&#34;&gt;&lt;code&gt;bitcoin_price_data_USD_2013-10-01_2021-05-18.csv&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/README.md#-10-time-series-fundamentals-and-milestone-project-3-bitpredict--exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/10_time_series_fundamentals_and_milestone_project_3_bitpredict.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/11_passing_the_tensorflow_developer_certification_exam.md&#34;&gt;Preparing to Pass the TensorFlow Developer Certification Exam&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/README.md#-11-passing-the-tensorflow-developer-certification-exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/11_passing_the_tensorflow_developer_certification_exam.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Course structure&lt;/h2&gt; &#xA;&lt;p&gt;This course is code first. The goal is to get you writing deep learning code as soon as possible.&lt;/p&gt; &#xA;&lt;p&gt;It is taught with the following mantra:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Code -&amp;gt; Concept -&amp;gt; Code -&amp;gt; Concept -&amp;gt; Code -&amp;gt; Concept&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This means we write code first then step through the concepts behind it.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;ve got 6-months experience writing Python code and a willingness to learn (most important), you&#39;ll be able to do the course.&lt;/p&gt; &#xA;&lt;h2&gt;Should you do this course?&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Do you have 1+ years experience with deep learning and writing TensorFlow code?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If yes, no you shouldn&#39;t, use your skills to build something.&lt;/p&gt; &#xA;&lt;p&gt;If no, move onto the next question.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Have you done at least one beginner machine learning course and would like to learn about deep learning/pass the TensorFlow Developer Certification?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If yes, this course is for you.&lt;/p&gt; &#xA;&lt;p&gt;If no, go and do a beginner machine learning course and if you decide you want to learn TensorFlow, this page will still be here.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;What do I need to know to go through this course?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;6+ months writing Python code.&lt;/strong&gt; Can you write a Python function which accepts and uses parameters? That’s good enough. If you don’t know what that means, spend another month or two writing Python code and then come back here.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;At least one beginner machine learning course.&lt;/strong&gt; Are you familiar with the idea of training, validation and test sets? Do you know what supervised learning is? Have you used pandas, NumPy or Matplotlib before? If no to any of these, I’d going through at least one machine learning course which teaches these first and then coming back.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comfortable using Google Colab/Jupyter Notebooks.&lt;/strong&gt; This course uses Google Colab throughout. If you have never used Google Colab before, it works very similar to Jupyter Notebooks with a few extra features. If you’re not familiar with Google Colab notebooks, I’d suggest going through the Introduction to Google Colab notebook.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plug:&lt;/strong&gt; The &lt;a href=&#34;https://dbourke.link/ZTMMLcourse&#34;&gt;Zero to Mastery beginner-friendly machine learning course&lt;/a&gt; (I also teach this) teaches all of the above (and this course is designed as a follow on).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛠 Exercises &amp;amp; 📖 Extra-curriculum&lt;/h2&gt; &#xA;&lt;p&gt;To prevent the course from being 100+ hours (deep learning is a broad field), various external resources for different sections are recommended to puruse under your own discrestion.&lt;/p&gt; &#xA;&lt;p&gt;You can find solutions to the exercises in &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/tree/main/extras/solutions&#34;&gt;&lt;code&gt;extras/solutions/&lt;/code&gt;&lt;/a&gt;, there&#39;s a notebook per set of exercises (one for 00, 01, 02... etc). Thank you to &lt;a href=&#34;https://github.com/ashikshafi08&#34;&gt;Ashik Shafi&lt;/a&gt; for all of the efforts creating these.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 00. TensorFlow Fundamentals Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a vector, scalar, matrix and tensor with values of your choosing using &lt;code&gt;tf.constant()&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Find the shape, rank and size of the tensors you created in 1.&lt;/li&gt; &#xA; &lt;li&gt;Create two tensors containing random values between 0 and 1 with shape &lt;code&gt;[5, 300]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Multiply the two tensors you created in 3 using matrix multiplication.&lt;/li&gt; &#xA; &lt;li&gt;Multiply the two tensors you created in 3 using dot product.&lt;/li&gt; &#xA; &lt;li&gt;Create a tensor with random values between 0 and 1 with shape &lt;code&gt;[224, 224, 3]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Find the min and max values of the tensor you created in 6 along the first axis.&lt;/li&gt; &#xA; &lt;li&gt;Created a tensor with random values of shape &lt;code&gt;[1, 224, 224, 3]&lt;/code&gt; then squeeze it to change the shape to &lt;code&gt;[224, 224, 3]&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Create a tensor with shape &lt;code&gt;[10]&lt;/code&gt; using your own choice of values, then find the index which has the maximum value.&lt;/li&gt; &#xA; &lt;li&gt;One-hot encode the tensor you created in 9.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;📖 00. TensorFlow Fundamentals Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read through the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/&#34;&gt;list of TensorFlow Python APIs&lt;/a&gt;, pick one we haven&#39;t gone through in this notebook, reverse engineer it (write out the documentation code for yourself) and figure out what it does.&lt;/li&gt; &#xA; &lt;li&gt;Try to create a series of tensor functions to calculate your most recent grocery bill (it&#39;s okay if you don&#39;t use the names of the items, just the price in numerical form). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;How would you calculate your grocery bill for the month and for the year using tensors?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Go through the &lt;a href=&#34;https://www.tensorflow.org/tutorials/quickstart/beginner&#34;&gt;TensorFlow 2.x quick start for beginners&lt;/a&gt; tutorial (be sure to type out all of the code yourself, even if you don&#39;t understand it). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Are there any functions we used in here that match what&#39;s used in there? Which are the same? Which haven&#39;t you seen before?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Watch the video &lt;a href=&#34;https://www.youtube.com/watch?v=f5liqUk0ZTw&#34;&gt;&#34;What&#39;s a tensor?&#34;&lt;/a&gt; - a great visual introduction to many of the concepts we&#39;ve covered in this notebook.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 01. Neural network regression with TensorFlow Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create your own regression dataset (or make the one we created in &#34;Create data to view and fit&#34; bigger) and build fit a model to it.&lt;/li&gt; &#xA; &lt;li&gt;Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?&lt;/li&gt; &#xA; &lt;li&gt;Try and improve the results we got on the insurance dataset, some things you might want to try include:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Building a larger model (how does one with 4 dense layers go?).&lt;/li&gt; &#xA; &lt;li&gt;Increasing the number of units in each layer.&lt;/li&gt; &#xA; &lt;li&gt;Lookup the documentation of &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam&#34;&gt;Adam&lt;/a&gt; and find out what the first parameter is, what happens if you increase it by 10x?&lt;/li&gt; &#xA; &lt;li&gt;What happens if you train for longer (say 300 epochs instead of 200)?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Import the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data&#34;&gt;Boston pricing dataset&lt;/a&gt; from TensorFlow &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/datasets&#34;&gt;&lt;code&gt;tf.keras.datasets&lt;/code&gt;&lt;/a&gt; and model it.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;📖 01. Neural network regression with TensorFlow Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/njKP3FqW3Sk&#34;&gt;MIT introduction deep learning lecture 1&lt;/a&gt; - gives a great overview of what&#39;s happening behind all of the code we&#39;re running.&lt;/li&gt; &#xA; &lt;li&gt;Reading: 1-hour of &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/chap1.html&#34;&gt;Chapter 1 of Neural Networks and Deep Learning&lt;/a&gt; by Michael Nielson - a great in-depth and hands-on example of the intuition behind neural networks.&lt;/li&gt; &#xA; &lt;li&gt;To practice your regression modelling with TensorFlow, I&#39;d also encourage you to look through &lt;a href=&#34;https://lionbridge.ai/datasets/&#34;&gt;Lion Bridge&#39;s collection of datasets&lt;/a&gt; or &lt;a href=&#34;https://www.kaggle.com/data&#34;&gt;Kaggle&#39;s datasets&lt;/a&gt;, find a regression dataset which sparks your interest and try to model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 02. Neural network classification with TensorFlow Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Play with neural networks in the &lt;a href=&#34;https://playground.tensorflow.org/&#34;&gt;TensorFlow Playground&lt;/a&gt; for 10-minutes. Especially try different values of the learning, what happens when you decrease it? What happens when you increase it?&lt;/li&gt; &#xA; &lt;li&gt;Replicate the model pictured in the &lt;a href=&#34;https://playground.tensorflow.org/#activation=relu&amp;amp;batchSize=10&amp;amp;dataset=circle&amp;amp;regDataset=reg-plane&amp;amp;learningRate=0.001&amp;amp;regularizationRate=0&amp;amp;noise=0&amp;amp;networkShape=6,6,6,6,6&amp;amp;seed=0.51287&amp;amp;showTestData=false&amp;amp;discretize=false&amp;amp;percTrainData=50&amp;amp;x=true&amp;amp;y=true&amp;amp;xTimesY=false&amp;amp;xSquared=false&amp;amp;ySquared=false&amp;amp;cosX=false&amp;amp;sinX=false&amp;amp;cosY=false&amp;amp;sinY=false&amp;amp;collectStats=false&amp;amp;problem=classification&amp;amp;initZero=false&amp;amp;hideText=false&amp;amp;regularization_hide=true&amp;amp;discretize_hide=true&amp;amp;regularizationRate_hide=true&amp;amp;percTrainData_hide=true&amp;amp;dataset_hide=true&amp;amp;problem_hide=true&amp;amp;noise_hide=true&amp;amp;batchSize_hide=true&#34;&gt;TensorFlow Playground diagram&lt;/a&gt; below using TensorFlow code. Compile it using the Adam optimizer, binary crossentropy loss and accuracy metric. Once it&#39;s compiled check a summary of the model. &lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/02-tensorflow-playground-replication-exercise.png&#34; alt=&#34;tensorflow playground example neural network&#34;&gt; &lt;em&gt;Try this network out for yourself on the &lt;a href=&#34;https://playground.tensorflow.org/#activation=relu&amp;amp;batchSize=10&amp;amp;dataset=circle&amp;amp;regDataset=reg-plane&amp;amp;learningRate=0.001&amp;amp;regularizationRate=0&amp;amp;noise=0&amp;amp;networkShape=6,6,6,6,6&amp;amp;seed=0.51287&amp;amp;showTestData=false&amp;amp;discretize=false&amp;amp;percTrainData=50&amp;amp;x=true&amp;amp;y=true&amp;amp;xTimesY=false&amp;amp;xSquared=false&amp;amp;ySquared=false&amp;amp;cosX=false&amp;amp;sinX=false&amp;amp;cosY=false&amp;amp;sinY=false&amp;amp;collectStats=false&amp;amp;problem=classification&amp;amp;initZero=false&amp;amp;hideText=false&amp;amp;regularization_hide=true&amp;amp;discretize_hide=true&amp;amp;regularizationRate_hide=true&amp;amp;percTrainData_hide=true&amp;amp;dataset_hide=true&amp;amp;problem_hide=true&amp;amp;noise_hide=true&amp;amp;batchSize_hide=true&#34;&gt;TensorFlow Playground website&lt;/a&gt;. Hint: there are 5 hidden layers but the output layer isn&#39;t pictured, you&#39;ll have to decide what the output layer should be based on the input data.&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a classification dataset using Scikit-Learn&#39;s &lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html&#34;&gt;&lt;code&gt;make_moons()&lt;/code&gt;&lt;/a&gt; function, visualize it and then build a model to fit it at over 85% accuracy.&lt;/li&gt; &#xA; &lt;li&gt;Train a model to get 88%+ accuracy on the fashion MNIST test set. Plot a confusion matrix to see the results after.&lt;/li&gt; &#xA; &lt;li&gt;Recreate &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax&#34;&gt;TensorFlow&#39;s&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Softmax_function&#34;&gt;softmax activation function&lt;/a&gt; in your own code. Make sure it can accept a tensor and return that tensor after having the softmax function applied to it.&lt;/li&gt; &#xA; &lt;li&gt;Create a function (or write code) to visualize multiple image predictions for the fashion MNIST at the same time. Plot at least three different images and their prediciton labels at the same time. Hint: see the &lt;a href=&#34;https://www.tensorflow.org/tutorials/keras/classification&#34;&gt;classifcation tutorial in the TensorFlow documentation&lt;/a&gt; for ideas.&lt;/li&gt; &#xA; &lt;li&gt;Make a function to show an image of a certain class of the fashion MNIST dataset and make a prediction on it. For example, plot 3 images of the &lt;code&gt;T-shirt&lt;/code&gt; class with their predictions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;📖 02. Neural network classification with TensorFlow Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Watch 3Blue1Brown&#39;s neural networks video 2: &lt;a href=&#34;https://www.youtube.com/watch?v=IHZwWFHWa-w&#34;&gt;&lt;em&gt;Gradient descent, how neural networks learn&lt;/em&gt;&lt;/a&gt;. After you&#39;re done, write 100 words about what you&#39;ve learned. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you haven&#39;t already, watch video 1: &lt;a href=&#34;https://youtu.be/aircAruvnKk&#34;&gt;&lt;em&gt;But what is a Neural Network?&lt;/em&gt;&lt;/a&gt;. Note the activation function they talk about at the end.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Watch &lt;a href=&#34;https://youtu.be/njKP3FqW3Sk&#34;&gt;MIT&#39;s introduction to deep learning lecture 1&lt;/a&gt; (if you haven&#39;t already) to get an idea of the concepts behind using linear and non-linear functions.&lt;/li&gt; &#xA; &lt;li&gt;Spend 1-hour reading &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/index.html&#34;&gt;Michael Nielsen&#39;s Neural Networks and Deep Learning book&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html&#34;&gt;ML-Glossary documentation on activation functions&lt;/a&gt;. Which one is your favourite? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;After you&#39;ve read the ML-Glossary, see which activation functions are available in TensorFlow by searching &#34;tensorflow activation functions&#34;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 03. Computer vision &amp;amp; convolutional neural networks in TensorFlow Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Spend 20-minutes reading and interacting with the &lt;a href=&#34;https://poloclub.github.io/cnn-explainer/&#34;&gt;CNN explainer website&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What are the key terms? e.g. explain convolution in your own words, pooling in your own words&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Play around with the &#34;understanding hyperparameters&#34; section in the &lt;a href=&#34;https://poloclub.github.io/cnn-explainer/&#34;&gt;CNN explainer&lt;/a&gt; website for 10-minutes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is the kernel size?&lt;/li&gt; &#xA; &lt;li&gt;What is the stride?&lt;/li&gt; &#xA; &lt;li&gt;How could you adjust each of these in TensorFlow code?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Take 10 photos of two different things and build your own CNN image classifier using the techniques we&#39;ve built here.&lt;/li&gt; &#xA; &lt;li&gt;Find an ideal learning rate for a simple convolutional neural network model on your the 10 class dataset.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;📖 03. Computer vision &amp;amp; convolutional neural networks in TensorFlow Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Watch:&lt;/strong&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=iaSUYvmCekI&amp;amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;amp;index=3&#34;&gt;MIT&#39;s Introduction to Deep Computer Vision&lt;/a&gt; lecture. This will give you a great intuition behind convolutional neural networks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Watch:&lt;/strong&gt; Deep dive on &lt;a href=&#34;https://youtu.be/-_4Zi8fCZO4&#34;&gt;mini-batch gradient descent&lt;/a&gt; by deeplearning.ai. If you&#39;re still curious about why we use &lt;strong&gt;batches&lt;/strong&gt; to train models, this technical overview covers many of the reasons why.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Read:&lt;/strong&gt; &lt;a href=&#34;https://cs231n.github.io/convolutional-networks/&#34;&gt;CS231n Convolutional Neural Networks for Visual Recognition&lt;/a&gt; class notes. This will give a very deep understanding of what&#39;s going on behind the scenes of the convolutional neural network architectures we&#39;re writing.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Read:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/pdf/1603.07285.pdf&#34;&gt;&#34;A guide to convolution arithmetic for deep learning&#34;&lt;/a&gt;. This paper goes through all of the mathematics running behind the scenes of our convolutional layers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code practice:&lt;/strong&gt; &lt;a href=&#34;https://www.tensorflow.org/tutorials/images/data_augmentation&#34;&gt;TensorFlow Data Augmentation Tutorial&lt;/a&gt;. For a more in-depth introduction on data augmentation with TensorFlow, spend an hour or two reading through this tutorial.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 04. Transfer Learning in TensorFlow Part 1: Feature Extraction Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Build and fit a model using the same data we have here but with the MobileNetV2 architecture feature extraction (&lt;a href=&#34;https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4&#34;&gt;&lt;code&gt;mobilenet_v2_100_224/feature_vector&lt;/code&gt;&lt;/a&gt;) from TensorFlow Hub, how does it perform compared to our other models?&lt;/li&gt; &#xA; &lt;li&gt;Name 3 different image classification models on TensorFlow Hub that we haven&#39;t used.&lt;/li&gt; &#xA; &lt;li&gt;Build a model to classify images of two different things you&#39;ve taken photos of.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can use any feature extraction layer from TensorFlow Hub you like for this.&lt;/li&gt; &#xA; &lt;li&gt;You should aim to have at least 10 images of each class, for example to build a fridge versus oven classifier, you&#39;ll want 10 images of fridges and 10 images of ovens.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;What is the current best performing model on ImageNet?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hint: you might want to check &lt;a href=&#34;https://www.sotabench.com&#34;&gt;sotabench.com&lt;/a&gt; for this.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;📖 04. Transfer Learning in TensorFlow Part 1: Feature Extraction Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read through the &lt;a href=&#34;https://www.tensorflow.org/tutorials/images/transfer_learning&#34;&gt;TensorFlow Transfer Learning Guide&lt;/a&gt; and define the main two types of transfer learning in your own words.&lt;/li&gt; &#xA; &lt;li&gt;Go through the &lt;a href=&#34;https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub&#34;&gt;Transfer Learning with TensorFlow Hub tutorial&lt;/a&gt; on the TensorFlow website and rewrite all of the code yourself into a new Google Colab notebook making comments about what each step does along the way.&lt;/li&gt; &#xA; &lt;li&gt;We haven&#39;t covered fine-tuning with TensorFlow Hub in this notebook, but if you&#39;d like to know more, go through the &lt;a href=&#34;https://www.tensorflow.org/hub/tf2_saved_model#fine-tuning&#34;&gt;fine-tuning a TensorFlow Hub model tutorial&lt;/a&gt; on the TensorFlow homepage.How to fine-tune a tensorflow hub model:&lt;/li&gt; &#xA; &lt;li&gt;Look into &lt;a href=&#34;https://www.wandb.com/experiment-tracking&#34;&gt;experiment tracking with Weights &amp;amp; Biases&lt;/a&gt;, how could you integrate it with our existing TensorBoard logs?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 05. Transfer Learning in TensorFlow Part 2: Fine-tuning Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use feature-extraction to train a transfer learning model on 10% of the Food Vision data for 10 epochs using &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB0&#34;&gt;&lt;code&gt;tf.keras.applications.EfficientNetB0&lt;/code&gt;&lt;/a&gt; as the base model. Use the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint&#34;&gt;&lt;code&gt;ModelCheckpoint&lt;/code&gt;&lt;/a&gt; callback to save the weights to file.&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune the last 20 layers of the base model you trained in 2 for another 10 epochs. How did it go?&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune the last 30 layers of the base model you trained in 2 for another 10 epochs. How did it go?&lt;/li&gt; &#xA; &lt;li&gt;Write a function to visualize an image from any dataset (train or test file) and any class (e.g. &#34;steak&#34;, &#34;pizza&#34;... etc), visualize it and make a prediction on it using a trained model.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;📖 05. Transfer Learning in TensorFlow Part 2: Fine-tuning Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://www.tensorflow.org/tutorials/images/data_augmentation&#34;&gt;documentation on data augmentation&lt;/a&gt; in TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://arxiv.org/abs/1801.06146&#34;&gt;ULMFit paper&lt;/a&gt; (technical) for an introduction to the concept of freezing and unfreezing different layers.&lt;/li&gt; &#xA; &lt;li&gt;Read up on learning rate scheduling (there&#39;s a &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler&#34;&gt;TensorFlow callback&lt;/a&gt; for this), how could this influence our model training? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you&#39;re training for longer, you probably want to reduce the learning rate as you go... the closer you get to the bottom of the hill, the smaller steps you want to take. Imagine it like finding a coin at the bottom of your couch. In the beginning your arm movements are going to be large and the closer you get, the smaller your movements become.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 06. Transfer Learning in TensorFlow Part 3: Scaling-up Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Take 3 of your own photos of food and use the trained model to make predictions on them, share your predictions with the other students in Discord and show off your Food Vision model 🍔👁.&lt;/li&gt; &#xA; &lt;li&gt;Train a feature-extraction transfer learning model for 10 epochs on the same data and compare its performance versus a model which used feature extraction for 5 epochs and fine-tuning for 5 epochs (like we&#39;ve used in this notebook). Which method is better?&lt;/li&gt; &#xA; &lt;li&gt;Recreate the first model (the feature extraction model) with &lt;a href=&#34;https://www.tensorflow.org/guide/mixed_precision&#34;&gt;&lt;code&gt;mixed_precision&lt;/code&gt;&lt;/a&gt; turned on.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Does it make the model train faster?&lt;/li&gt; &#xA; &lt;li&gt;Does it effect the accuracy or performance of our model?&lt;/li&gt; &#xA; &lt;li&gt;What&#39;s the advatanges of using &lt;code&gt;mixed_precision&lt;/code&gt; training?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;📖 06. Transfer Learning in TensorFlow Part 3: Scaling-up Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Spend 15-minutes reading up on the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping&#34;&gt;EarlyStopping callback&lt;/a&gt;. What does it do? How could we use it in our model training?&lt;/li&gt; &#xA; &lt;li&gt;Spend an hour reading about &lt;a href=&#34;https://www.streamlit.io/&#34;&gt;Streamlit&lt;/a&gt;. What does it do? How might you integrate some of the things we&#39;ve done in this notebook in a Streamlit app?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 07. Milestone Project 1: 🍔👁 Food Vision Big™ Exercises&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The chief exercise for Milestone Project 1 is to finish the &#34;TODO&#34; sections in the &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/extras/TEMPLATE_07_food_vision_milestone_project_1.ipynb&#34;&gt;Milestone Project 1 Template notebook&lt;/a&gt;. After doing so, move onto the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use the same evaluation techniques on the large-scale Food Vision model as you did in the previous notebook (&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb&#34;&gt;Transfer Learning Part 3: Scaling up&lt;/a&gt;). More specifically, it would be good to see:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A confusion matrix between all of the model&#39;s predictions and true labels.&lt;/li&gt; &#xA; &lt;li&gt;A graph showing the f1-scores of each class.&lt;/li&gt; &#xA; &lt;li&gt;A visualization of the model making predictions on various images and comparing the predictions to the ground truth. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For example, plot a sample image from the test dataset and have the title of the plot show the prediction, the prediction probability and the ground truth label.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Note:&lt;/strong&gt; To compare predicted labels to test labels, it might be a good idea when loading the test data to set &lt;code&gt;shuffle=False&lt;/code&gt; (so the ordering of test data is preserved alongside the order of predicted labels).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Take 3 of your own photos of food and use the Food Vision model to make predictions on them. How does it go? Share your images/predictions with the other students.&lt;/li&gt; &#xA; &lt;li&gt;Retrain the model (feature extraction and fine-tuning) we trained in this notebook, except this time use &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/applications/EfficientNetB4&#34;&gt;&lt;code&gt;EfficientNetB4&lt;/code&gt;&lt;/a&gt; as the base model instead of &lt;code&gt;EfficientNetB0&lt;/code&gt;. Do you notice an improvement in performance? Does it take longer to train? Are there any tradeoffs to consider?&lt;/li&gt; &#xA; &lt;li&gt;Name one important benefit of mixed precision training, how does this benefit take place?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;📖 07. Milestone Project 1: 🍔👁 Food Vision Big™ Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read up on learning rate scheduling and the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler&#34;&gt;learning rate scheduler callback&lt;/a&gt;. What is it? And how might it be helpful to this project?&lt;/li&gt; &#xA; &lt;li&gt;Read up on TensorFlow data loaders (&lt;a href=&#34;https://www.tensorflow.org/guide/data_performance&#34;&gt;improving TensorFlow data loading performance&lt;/a&gt;). Is there anything we&#39;ve missed? What methods you keep in mind whenever loading data in TensorFlow? Hint: check the summary at the bottom of the page for a gret round up of ideas.&lt;/li&gt; &#xA; &lt;li&gt;Read up on the documentation for &lt;a href=&#34;https://www.tensorflow.org/guide/mixed_precision&#34;&gt;TensorFlow mixed precision training&lt;/a&gt;. What are the important things to keep in mind when using mixed precision training?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 08. Introduction to NLP (Natural Language Processing) in TensorFlow Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Rebuild, compile and train &lt;code&gt;model_1&lt;/code&gt;, &lt;code&gt;model_2&lt;/code&gt; and &lt;code&gt;model_5&lt;/code&gt; using the &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/Sequential&#34;&gt;Keras Sequential API&lt;/a&gt; instead of the Functional API.&lt;/li&gt; &#xA; &lt;li&gt;Retrain the baseline model with 10% of the training data. How does perform compared to the Universal Sentence Encoder model with 10% of the training data?&lt;/li&gt; &#xA; &lt;li&gt;Try fine-tuning the TF Hub Universal Sentence Encoder model by setting &lt;code&gt;training=True&lt;/code&gt; when instantiating it as a Keras layer.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;# We can use this encoding layer in place of our text_vectorizer and embedding layer&#xA;sentence_encoder_layer = hub.KerasLayer(&#34;https://tfhub.dev/google/universal-sentence-encoder/4&#34;,&#xA;                                        input_shape=[],&#xA;                                        dtype=tf.string,&#xA;                                        trainable=True) # turn training on to fine-tune the TensorFlow Hub model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Retrain the best model you&#39;ve got so far on the whole training set (no validation split). Then use this trained model to make predictions on the test dataset and format the predictions into the same format as the &lt;code&gt;sample_submission.csv&lt;/code&gt; file from Kaggle (see the Files tab in Colab for what the &lt;code&gt;sample_submission.csv&lt;/code&gt; file looks like). Once you&#39;ve done this, &lt;a href=&#34;https://www.kaggle.com/c/nlp-getting-started/data&#34;&gt;make a submission to the Kaggle competition&lt;/a&gt;, how did your model perform?&lt;/li&gt; &#xA; &lt;li&gt;Combine the ensemble predictions using the majority vote (mode), how does this perform compare to averaging the prediction probabilities of each model?&lt;/li&gt; &#xA; &lt;li&gt;Make a confusion matrix with the best performing model&#39;s predictions on the validation set and the validation ground truth labels.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;📖 08. Introduction to NLP (Natural Language Processing) in TensorFlow Extra-curriculum&lt;/h3&gt; &#xA;&lt;p&gt;To practice what you&#39;ve learned, a good idea would be to spend an hour on 3 of the following (3-hours total, you could through them all if you want) and then write a blog post about what you&#39;ve learned.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For an overview of the different problems within NLP and how to solve them read through: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32&#34;&gt;A Simple Introduction to Natural Language Processing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e&#34;&gt;How to solve 90% of NLP problems: a step-by-step guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Go through &lt;a href=&#34;https://youtu.be/SEnXr6v2ifU&#34;&gt;MIT&#39;s Recurrent Neural Networks lecture&lt;/a&gt;. This will be one of the greatest additions to what&#39;s happening behind the RNN model&#39;s you&#39;ve been building.&lt;/li&gt; &#xA; &lt;li&gt;Read through the &lt;a href=&#34;https://www.tensorflow.org/tutorials/text/word_embeddings&#34;&gt;word embeddings page on the TensorFlow website&lt;/a&gt;. Embeddings are such a large part of NLP. We&#39;ve covered them throughout this notebook but extra practice would be well worth it. A good exercise would be to write out all the code in the guide in a new notebook.&lt;/li&gt; &#xA; &lt;li&gt;For more on RNN&#39;s in TensorFlow, read and reproduce &lt;a href=&#34;https://www.tensorflow.org/guide/keras/rnn&#34;&gt;the TensorFlow RNN guide&lt;/a&gt;. We&#39;ve covered many of the concepts in this guide, but it&#39;s worth writing the code again for yourself.&lt;/li&gt; &#xA; &lt;li&gt;Text data doesn&#39;t always come in a nice package like the data we&#39;ve downloaded. So if you&#39;re after more on preparing different text sources for being with your TensorFlow deep learning models, it&#39;s worth checking out the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/load_data/text&#34;&gt;TensorFlow text loading tutorial&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://realpython.com/read-write-files-python/&#34;&gt;Reading text files with Python&lt;/a&gt; by Real Python.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;This notebook has focused on writing NLP code. For a mathematically rich overview of how NLP with Deep Learning happens, read &lt;a href=&#34;https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf&#34;&gt;Standford&#39;s Natural Language Processing with Deep Learning lecture notes Part 1&lt;/a&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For an even deeper dive, you could even do the whole &lt;a href=&#34;http://web.stanford.edu/class/cs224n/&#34;&gt;CS224n&lt;/a&gt; (Natural Language Processing with Deep Learning) course.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Great blog posts to read: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Andrei Karpathy&#39;s &lt;a href=&#34;https://karpathy.github.io/2015/05/21/rnn-effectiveness/&#34;&gt;The Unreasonable Effectiveness of RNNs&lt;/a&gt; dives into generating Shakespeare text with RNNs.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794&#34;&gt;Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT&lt;/a&gt; by Mauro Di Pietro. An overview of different techniques for turning text into numbers and then classifying it.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://machinelearningmastery.com/what-are-word-embeddings/&#34;&gt;What are word embeddings?&lt;/a&gt; by Machine Learning Mastery.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Other topics worth looking into: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&#34;&gt;Attention mechanisms&lt;/a&gt;. These are a foundational component of the transformer architecture and also often add improvments to deep NLP models.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34;&gt;Transformer architectures&lt;/a&gt;. This model architecture has recently taken the NLP world by storm, achieving state of the art on many benchmarks. However, it does take a little more processing to get off the ground, the &lt;a href=&#34;https://huggingface.co/models/&#34;&gt;HuggingFace Models (formerly HuggingFace Transformers) library&lt;/a&gt; is probably your best quick start.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 09. Milestone Project 2: SkimLit 📄🔥 Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Train &lt;code&gt;model_5&lt;/code&gt; on all of the data in the training dataset for as many epochs until it stops improving. Since this might take a while, you might want to use:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint&#34;&gt;&lt;code&gt;tf.keras.callbacks.ModelCheckpoint&lt;/code&gt;&lt;/a&gt; to save the model&#39;s best weights only.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping&#34;&gt;&lt;code&gt;tf.keras.callbacks.EarlyStopping&lt;/code&gt;&lt;/a&gt; to stop the model from training once the validation loss has stopped improving for ~3 epochs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Checkout the &lt;a href=&#34;https://keras.io/examples/nlp/pretrained_word_embeddings/&#34;&gt;Keras guide on using pretrained GloVe embeddings&lt;/a&gt;. Can you get this working with one of our models?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hint: You&#39;ll want to incorporate it with a custom token &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding&#34;&gt;Embedding&lt;/a&gt; layer.&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s up to you whether or not you fine-tune the GloVe embeddings or leave them frozen.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Try replacing the TensorFlow Hub Universal Sentence Encoder pretrained embedding for the &lt;a href=&#34;https://tfhub.dev/google/experts/bert/pubmed/2&#34;&gt;TensorFlow Hub BERT PubMed expert&lt;/a&gt; (a language model pretrained on PubMed texts) pretrained embedding. Does this effect results?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Note: Using the BERT PubMed expert pretrained embedding requires an extra preprocessing step for sequences (as detailed in the &lt;a href=&#34;https://tfhub.dev/google/experts/bert/pubmed/2&#34;&gt;TensorFlow Hub guide&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Does the BERT model beat the results mentioned in this paper? &lt;a href=&#34;https://arxiv.org/pdf/1710.06071.pdf&#34;&gt;https://arxiv.org/pdf/1710.06071.pdf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;What happens if you were to merge our &lt;code&gt;line_number&lt;/code&gt; and &lt;code&gt;total_lines&lt;/code&gt; features for each sequence? For example, created a &lt;code&gt;X_of_Y&lt;/code&gt; feature instead? Does this effect model performance?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Another example: &lt;code&gt;line_number=1&lt;/code&gt; and &lt;code&gt;total_lines=11&lt;/code&gt; turns into &lt;code&gt;line_of_X=1_of_11&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Write a function (or series of functions) to take a sample abstract string, preprocess it (in the same way our model has been trained), make a prediction on each sequence in the abstract and return the abstract in the format:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;PREDICTED_LABEL&lt;/code&gt;: &lt;code&gt;SEQUENCE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PREDICTED_LABEL&lt;/code&gt;: &lt;code&gt;SEQUENCE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PREDICTED_LABEL&lt;/code&gt;: &lt;code&gt;SEQUENCE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;PREDICTED_LABEL&lt;/code&gt;: &lt;code&gt;SEQUENCE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;... &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can find your own unstrcutured RCT abstract from PubMed or try this one from: &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/22244707/&#34;&gt;&lt;em&gt;Baclofen promotes alcohol abstinence in alcohol dependent cirrhotic patients with hepatitis C virus (HCV) infection&lt;/em&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;📖 09. Milestone Project 2: SkimLit 📄🔥 Extra-curriculum&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For more on working with text/spaCy, see &lt;a href=&#34;https://course.spacy.io/en/&#34;&gt;spaCy&#39;s advanced NLP course&lt;/a&gt;. If you&#39;re going to be working on production-level NLP problems, you&#39;ll probably end up using spaCy.&lt;/li&gt; &#xA; &lt;li&gt;For another look at how to approach a text classification problem like the one we&#39;ve just gone through, I&#39;d suggest going through &lt;a href=&#34;https://developers.google.com/machine-learning/guides/text-classification&#34;&gt;Google&#39;s Machine Learning Course for text classification&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Since our dataset has imbalanced classes (as with many real-world datasets), so it might be worth looking into the &lt;a href=&#34;https://www.tensorflow.org/tutorials/structured_data/imbalanced_data&#34;&gt;TensorFlow guide for different methods to training a model with imbalanced classes&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 10. Time series fundamentals and Milestone Project 3: BitPredict 💰📈 Exercises&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Does scaling the data help for univariate/multivariate data? (e.g. getting all of the values between 0 &amp;amp; 1)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try doing this for a univariate model (e.g. &lt;code&gt;model_1&lt;/code&gt;) and a multivariate model (e.g. &lt;code&gt;model_6&lt;/code&gt;) and see if it effects model training or evaluation results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Get the most up to date data on Bitcoin, train a model &amp;amp; see how it goes (our data goes up to May 18 2021).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can download the Bitcoin historical data for free from &lt;a href=&#34;https://www.coindesk.com/price/bitcoin&#34;&gt;coindesk.com/price/bitcoin&lt;/a&gt; and clicking &#34;Export Data&#34; -&amp;gt; &#34;CSV&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;For most of our models we used &lt;code&gt;WINDOW_SIZE=7&lt;/code&gt;, but is there a better window size?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Setup a series of experiments to find whether or not there&#39;s a better window size.&lt;/li&gt; &#xA; &lt;li&gt;For example, you might train 10 different models with &lt;code&gt;HORIZON=1&lt;/code&gt; but with window sizes ranging from 2-12.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Create a windowed dataset just like the ones we used for &lt;code&gt;model_1&lt;/code&gt; using &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/timeseries_dataset_from_array&#34;&gt;&lt;code&gt;tf.keras.preprocessing.timeseries_dataset_from_array()&lt;/code&gt;&lt;/a&gt; and retrain &lt;code&gt;model_1&lt;/code&gt; using the recreated dataset.&lt;/li&gt; &#xA; &lt;li&gt;For our multivariate modelling experiment, we added the Bitcoin block reward size as an extra feature to make our time series multivariate.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Are there any other features you think you could add?&lt;/li&gt; &#xA; &lt;li&gt;If so, try it out, how do these affect the model?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Make prediction intervals for future forecasts. To do so, one way would be to train an ensemble model on all of the data, make future forecasts with it and calculate the prediction intervals of the ensemble just like we did for &lt;code&gt;model_8&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;For future predictions, try to make a prediction, retrain a model on the predictions, make a prediction, retrain a model, make a prediction, retrain a model, make a prediction (retrain a model each time a new prediction is made). Plot the results, how do they look compared to the future predictions where a model wasn&#39;t retrained for every forecast (&lt;code&gt;model_9&lt;/code&gt;)?&lt;/li&gt; &#xA; &lt;li&gt;Throughout this notebook, we&#39;ve only tried algorithms we&#39;ve handcrafted ourselves. But it&#39;s worth seeing how a purpose built forecasting algorithm goes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try out one of the extra algorithms listed in the modelling experiments part such as: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/Kats&#34;&gt;Facebook&#39;s Kats library&lt;/a&gt; - there are many models in here, remember the machine learning practioner&#39;s motto: experiment, experiment, experiment.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/linkedin/greykite&#34;&gt;LinkedIn&#39;s Greykite library&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;📖 10. Time series fundamentals and Milestone Project 3: BitPredict 💰📈 Extra-curriculum&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve only really scratched the surface with time series forecasting and time series modelling in general. But the good news is, you&#39;ve got plenty of hands-on coding experience with it already.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to dig deeper in to the world of time series, I&#39;d recommend the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://otexts.com/fpp3/&#34;&gt;Forecasting: Principles and Practice&lt;/a&gt; is an outstanding online textbook which discusses at length many of the most important concepts in time series forecasting. I&#39;d especially recommend reading at least Chapter 1 in full. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;I&#39;d definitely recommend at least checking out chapter 1 as well as the chapter on forecasting accuracy measures.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;🎥 &lt;a href=&#34;https://youtu.be/wqQKFu41FIw&#34;&gt;Introduction to machine learning and time series&lt;/a&gt; by Markus Loning goes through different time series problems and how to approach them. It focuses on using the &lt;code&gt;sktime&lt;/code&gt; library (Scikit-Learn for time series), though the principles are applicable elsewhere.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/why-you-should-care-about-the-nate-silver-vs-nassim-taleb-twitter-war-a581dce1f5fc&#34;&gt;&lt;em&gt;Why you should care about the Nate Silver vs. Nassim Taleb Twitter war&lt;/em&gt;&lt;/a&gt; by Isaac Faber is an outstanding discussion insight into the role of uncertainty in the example of election prediction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.tensorflow.org/tutorials/structured_data/time_series&#34;&gt;TensorFlow time series tutorial&lt;/a&gt; - A tutorial on using TensorFlow to forecast weather time series data with TensorFlow.&lt;/li&gt; &#xA; &lt;li&gt;📕 &lt;a href=&#34;https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable&#34;&gt;&lt;em&gt;The Black Swan&lt;/em&gt;&lt;/a&gt; by Nassim Nicholas Taleb - Nassim Taleb was a pit trader (a trader who trades on their own behalf) for 25 years, this book compiles many of the lessons he learned from first-hand experience. It changed my whole perspective on our ability to predict.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/3-facts-about-time-series-forecasting-that-surprise-experienced-machine-learning-practitioners-69c18ee89387&#34;&gt;&lt;em&gt;3 facts about time series forecasting that surprise experienced machine learning practitioners&lt;/em&gt;&lt;/a&gt; by Skander Hannachi, Ph.D - time series data is different to other kinds of data, if you&#39;ve worked on other kinds of machine learning problems before, getting into time series might require some adjustments, Hannachi outlines 3 of the most common.&lt;/li&gt; &#xA; &lt;li&gt;🎥 World-class lectures by Jordan Kern, watching these will take you from 0 to 1 with time series problems: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://youtu.be/Prpu_U5tKkE&#34;&gt;Time Series Analysis&lt;/a&gt; - how to analyse time series data.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=s3XH7fTHMb4&#34;&gt;Time Series Modelling&lt;/a&gt; - different techniques for modelling time series data (many of which aren&#39;t deep learning).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;🛠 11. Passing the TensorFlow Developer Certification Exercises&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Preparing your brain&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Read through the &lt;a href=&#34;https://www.tensorflow.org/extras/cert/TF_Certificate_Candidate_Handbook.pdf&#34;&gt;TensorFlow Developer Certificate Candidate Handbook&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Go through the Skills checklist section of the TensorFlow Developer Certification Candidate Handbook and create a notebook which covers all of the skills required, write code for each of these (this notebook can be used as a point of reference during the exam).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-map-the-skills-checklist-to-a-notebook.png&#34; alt=&#34;mapping the TensorFlow Developer handbook to code in a notebook&#34;&gt; &lt;em&gt;Example of mapping the Skills checklist section of the TensorFlow Developer Certification Candidate handbook to a notebook.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prearing your computer&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go through the &lt;a href=&#34;https://www.jetbrains.com/pycharm/learning-center/&#34;&gt;PyCharm quick start&lt;/a&gt; tutorials to make sure you&#39;re familiar with PyCharm (the exam uses PyCharm, you can download the free version).&lt;/li&gt; &#xA; &lt;li&gt;Read through and follow the suggested steps in the &lt;a href=&#34;https://www.tensorflow.org/extras/cert/Setting_Up_TF_Developer_Certificate_Exam.pdf&#34;&gt;setting up for the TensorFlow Developer Certificate Exam guide&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;After going through (2), go into PyCharm and make sure you can train a model in TensorFlow. The model and dataset in the example &lt;code&gt;image_classification_test.py&lt;/code&gt; &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/extras/image_classification_test.py&#34;&gt;script on GitHub&lt;/a&gt; should be enough. If you can train and save the model in under 5-10 minutes, your computer will be powerful enough to train the models in the exam. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure you&#39;ve got experience running models locally in PyCharm before taking the exam. Google Colab (what we used through the course) is a little different to PyCharm.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/11-getting-example-script-to-run-in-pycharm.png&#34; alt=&#34;before taking the TensorFlow Developer certification exam, make sure you can run TensorFlow code in PyCharm on your local machine&#34;&gt; &lt;em&gt;Before taking the exam make sure you can run TensorFlow code on your local machine in PyCharm. If the &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/extras/image_classification_test.py&#34;&gt;example &lt;code&gt;image_class_test.py&lt;/code&gt; script&lt;/a&gt; can run completely in under 5-10 minutes on your local machine, your local machine can handle the exam (if not, you can use Google Colab to train, save and download models to submit for the exam).&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;📖 11. Passing the TensorFlow Developer Certification Extra-curriculum&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d like some extra materials to go through to further your skills with TensorFlow and deep learning in general or to prepare more for the exam, I&#39;d highly recommend the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📄 &lt;strong&gt;Read:&lt;/strong&gt; &lt;a href=&#34;https://www.mrdbourke.com/how-i-got-tensorflow-developer-certified/&#34;&gt;How I got TensorFlow Developer Certified (and how you can too)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🎥 &lt;strong&gt;Watch:&lt;/strong&gt; &lt;a href=&#34;https://youtu.be/ya5NwvKafDk&#34;&gt;How I passed the TensorFlow Developer Certification exam (and how you can too)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go through the &lt;a href=&#34;https://dbourke.link/tfinpractice&#34;&gt;TensorFlow in Practice Specialization on Coursera&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read through the second half of &lt;a href=&#34;https://amzn.to/3aYexF2&#34;&gt;Hands-On Machine Learning with Scikit-Learn, Keras &amp;amp; TensorFlow 2nd Edition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What this course is missing&lt;/h2&gt; &#xA;&lt;p&gt;Deep learning is a broad topic. So this course doesn&#39;t cover it all.&lt;/p&gt; &#xA;&lt;p&gt;Here are some of the main topics you might want to look into next:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Transformers (the neural network architecture taking the NLP world by storm)&lt;/li&gt; &#xA; &lt;li&gt;Multi-modal models (models which use more than one data source such as text &amp;amp; images)&lt;/li&gt; &#xA; &lt;li&gt;Reinforcement learning&lt;/li&gt; &#xA; &lt;li&gt;Unsupervised learning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Extensions (possible places to go after the course)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34;&gt;Neural Networks and Deep Learning Book&lt;/a&gt; by Michael Nielsen - If the Zero to Mastery TensorFlow for Deep Learning course is top down, this book is bottom up. A fantastic resource to sandwich your knowledge.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai&#34;&gt;Deeplearning.AI specializations&lt;/a&gt; - This course focuses on code-first, the deeplearning.ai specializations will teach you what&#39;s going on behind the code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/&#34;&gt;Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow Book&lt;/a&gt; (especially the 2nd half) - Many of the materials in this course were inspired by and guided by the pages of this beautiful text book.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fullstackdeeplearning.com&#34;&gt;Full Stack Deep Learning&lt;/a&gt; - Learn how to turn your models into machine learning-powered applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://madewithml.com/#mlops&#34;&gt;Made with ML MLOps materials&lt;/a&gt; - Similar to Full Stack Deep Learning but comprised into many small lessons around all the pieces of the puzzle (data collection, labelling, deployment and more) required to build a full-stack machine learning-powered application.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.fast.ai&#34;&gt;fast.ai Curriculum&lt;/a&gt; - One of the best (and free) AI/deep learning courses online. Enough said.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mrdbourke.com/how-can-a-beginner-data-scientist-like-me-gain-experience/&#34;&gt;&#34;How does a beginner data scientist like me gain experience?&#34;&lt;/a&gt; by Daniel Bourke - Read this on how to get experience for a job after studying online/at unveristy (start the job before you have it).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Ask questions&lt;/h2&gt; &#xA;&lt;p&gt;Contact &lt;a href=&#34;mailto:daniel@mrdbourke.com&#34;&gt;Daniel Bourke&lt;/a&gt; or &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions&#34;&gt;add a discussion&lt;/a&gt; (preferred).&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;As of: 14 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;Course completed! 🕺&lt;/p&gt; &#xA;&lt;p&gt;Any further updates/changes will be added below.&lt;/p&gt; &#xA;&lt;h2&gt;Log&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;02 Dec 2021 - add fix for TensorFlow 2.7 to notebook 02&lt;/li&gt; &#xA; &lt;li&gt;11 Nov 2021 - add fix for TensorFlow 2.7 to notebook 01&lt;/li&gt; &#xA; &lt;li&gt;14 Aug 2021 - added a discussion with TensorFlow 2.6 updates and EfficientNetV2 notes: &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/166&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/discussions/166&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;16 Jul 2021 - added 35 videos to ZTM Academy + Udemy versions of the course for time series and how to pass TensorFlow Developer Certificaiton&lt;/li&gt; &#xA; &lt;li&gt;10 Jul 2021 - added 29 edited time series videos to ZTM Academy + Udemy versions of the course, more to come soon&lt;/li&gt; &#xA; &lt;li&gt;07 Jul 2021 - recorded 5 videos for passing TensorFlow Developer Certification exam section - ALL VIDEOS FOR COURSE DONE!!! time to edit/upload! 🎉&lt;/li&gt; &#xA; &lt;li&gt;06 Jul 2021 - added guide to TensorFlow Certification Exam: &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/11_passing_the_tensorflow_developer_certification_exam.md&#34;&gt;https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/11_passing_the_tensorflow_developer_certification_exam.md&lt;/a&gt; - going to record videos for it tomorrow&lt;/li&gt; &#xA; &lt;li&gt;05 Jul 2021 - making materials for TF certification exam (what/why/how)&lt;/li&gt; &#xA; &lt;li&gt;02 Jul 2021 - FINISHED RECORDING VIDEOS FOR TIME SERIES SECTION!!!!! time to upload&lt;/li&gt; &#xA; &lt;li&gt;30 Jun 2021 - recorded 12 videos for time series section, total heading past 60 (the biggest section yet), nearly done!!!&lt;/li&gt; &#xA; &lt;li&gt;29 Jun 2021 - recorded 10 videos for time series section, total heading towards 60&lt;/li&gt; &#xA; &lt;li&gt;28 Jun 2021 - recorded 10 videos for time series section, the line below says 40 videos total, actually more like 50&lt;/li&gt; &#xA; &lt;li&gt;26 Jun 2021 - recorded 4 videos for time series section, looks like it&#39;ll be about 40 videos total&lt;/li&gt; &#xA; &lt;li&gt;25 Jun 2021 - recorded 8 videos for time series section + fixed a bunch of typos in time series notebook&lt;/li&gt; &#xA; &lt;li&gt;24 Jun 2021 - recorded 14 videos for time series section, more to come tomorrow&lt;/li&gt; &#xA; &lt;li&gt;23 Jun 2021 - finished adding images to time series notebook, now to start video recording&lt;/li&gt; &#xA; &lt;li&gt;22 Jun 2021 - added a bunch of images to the time series notebook/started making slides&lt;/li&gt; &#xA; &lt;li&gt;21 Jun 2021 - code for time series notebook is done, now creating slides/images to prepare for recording&lt;/li&gt; &#xA; &lt;li&gt;19 Jun 2021 - turned curriculum into an online book, you can read it here: &lt;a href=&#34;https://dev.mrdbourke.com/tensorflow-deep-learning/&#34;&gt;https://dev.mrdbourke.com/tensorflow-deep-learning/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;18 Jun 2021 - add exercises/extra-curriculum/outline to time series notebook&lt;/li&gt; &#xA; &lt;li&gt;17 Jun 2021 - add annotations for turkey problem and model comparison in time series notebook, next is outline/images&lt;/li&gt; &#xA; &lt;li&gt;16 Jun 2021 - add annotations for uncertainty and future predictions in time series notebook, next is turkey problem&lt;/li&gt; &#xA; &lt;li&gt;14 Jun 2021 - add annotations for ensembling, begin on prediction intervals&lt;/li&gt; &#xA; &lt;li&gt;10 Jun 2021 - finished annotations for N-BEATS algorithm, now onto ensembling/prediction intervals&lt;/li&gt; &#xA; &lt;li&gt;9 Jun 2021 - add annotations for N-BEATS algorithm implementation for time series notebook&lt;/li&gt; &#xA; &lt;li&gt;8 Jun 2021 - add annotations to time series notebook, all will be finished by end of week (failed)&lt;/li&gt; &#xA; &lt;li&gt;4 Jun 2021 - more annotation updates to time series notebook, brick by brick!&lt;/li&gt; &#xA; &lt;li&gt;3 Jun 2021 - added a bunch of annotations/explanations to time series notebook, momentum building, plenty more to come!&lt;/li&gt; &#xA; &lt;li&gt;2 Jun 2021 - started adding annotations explaining the code + resources to learn more, will continue for next few days&lt;/li&gt; &#xA; &lt;li&gt;1 Jun 2021 - added turkey problem to time series notebook, cleaned up a bunch of code, draft code is ready, now to write annotations/explanations&lt;/li&gt; &#xA; &lt;li&gt;28 May 2021 - added future forecasts, added ensemble model, added prediction intervals to time series notebook&lt;/li&gt; &#xA; &lt;li&gt;25 May 2021 - added multivariate time series to time series notebook, fix LSTM model, next we add TensorFlow windowing/experimenting with window sizes&lt;/li&gt; &#xA; &lt;li&gt;24 May 2021 - fixed broken preprocessing function in time series notebook, LSTM model is broken, more material to come&lt;/li&gt; &#xA; &lt;li&gt;20 May 2021 - more time series material creation&lt;/li&gt; &#xA; &lt;li&gt;19 May 2021 - more time series material creation, streaming much of it live on Twitch - &lt;a href=&#34;https://twitch.tv/mrdbourke&#34;&gt;https://twitch.tv/mrdbourke&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;18 May 2021 - added time series forecasting notebook outline (&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/10_time_series_forecasting_in_tensorflow.ipynb&#34;&gt;notebook 10&lt;/a&gt;), going to really start ramping up the materials here&lt;/li&gt; &#xA; &lt;li&gt;12 May 2021 - all videos for 09 have now been released on Udemy &amp;amp; ZTM!!! enjoy build SkimLit 📄🔥&lt;/li&gt; &#xA; &lt;li&gt;11 May 2021 - 40+ section 08 &amp;amp; 09 videos released on Udemy &amp;amp; ZTM!!!&lt;/li&gt; &#xA; &lt;li&gt;10 May 2021 - time series materials research + preparation&lt;/li&gt; &#xA; &lt;li&gt;08 May 2021 - time series materials research + preparation&lt;/li&gt; &#xA; &lt;li&gt;05 May 2021 - ~20+ videos edited for 08, ~10+ videos edited for 09, time series materials in 1st draft mode&lt;/li&gt; &#xA; &lt;li&gt;04 May 2021 - fixed the remaining videos for 08 (audio missing), now onto making time series materials!&lt;/li&gt; &#xA; &lt;li&gt;03 May 2021 - rerecorded 10 videos for 08 fixing the sound isse, these are going straight to editing and should be uploaded by end of week&lt;/li&gt; &#xA; &lt;li&gt;02 May 2021 - found an issue with videos 09-20 of section 08 (no audio), going to rerecord them&lt;/li&gt; &#xA; &lt;li&gt;29 Apr 2021 - 🚀🚀🚀 launched on Udemy!!! 🚀🚀🚀&lt;/li&gt; &#xA; &lt;li&gt;22 Apr 2021 - finished recording videos for 09! added slides and video notebook 09&lt;/li&gt; &#xA; &lt;li&gt;21 Apr 2021 - recorded 14 videos for 09! biggggg day of recording! getting closer to finishing 09&lt;/li&gt; &#xA; &lt;li&gt;20 Apr 2021 - recorded 10 videos for 09&lt;/li&gt; &#xA; &lt;li&gt;19 Apr 2021 - recorded 9 videos for 09&lt;/li&gt; &#xA; &lt;li&gt;16 Apr 2021 - slides done for 09, ready to start recording!&lt;/li&gt; &#xA; &lt;li&gt;15 Apr 2021 - added slides, extra-curriculum, exercises and video notebook for 08, started making slides for 09, will finish tomorrow&lt;/li&gt; &#xA; &lt;li&gt;14 Apr 2021 - recorded 12 videos for notebook 08, finished the section! time to make slides for 09 and get into it&lt;/li&gt; &#xA; &lt;li&gt;10 Apr 2021 - recorded 4 videos for notebook 08&lt;/li&gt; &#xA; &lt;li&gt;9 Apr 2021 - recorded 6 videos for notebook 08&lt;/li&gt; &#xA; &lt;li&gt;8 Apr 2021 - recorded 10 videos for notebook 08! more coming tomorrow! home stretch baby!!!&lt;/li&gt; &#xA; &lt;li&gt;7 Apr 2021 - added a whole bunch of images to notebook 08, getting ready for recording tomorrow!&lt;/li&gt; &#xA; &lt;li&gt;1 Apr 2021 - added &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/09_SkimLit_nlp_milestone_project_2.ipynb&#34;&gt;notebook 09: SkimLit&lt;/a&gt;, almost finished, a little cleaning and we&#39;ll be ready for slide making!&lt;/li&gt; &#xA; &lt;li&gt;31 Mar 2021 - added notebook 08, going to finish tomorrow, then onto 09!&lt;/li&gt; &#xA; &lt;li&gt;24 Mar 2021 - Recorded 8 videos for 07, finished! onto materials (slides/notebooks) for 08, 09&lt;/li&gt; &#xA; &lt;li&gt;23 Mar 2021 - Recorded 6 videos for 07 (finally), going to finish tomorrow&lt;/li&gt; &#xA; &lt;li&gt;22 Mar 2021 - Polished notebook 07 ready for recording, made slides for 07, added template for 07 (for a student to go through and practice), ready to record!&lt;/li&gt; &#xA; &lt;li&gt;17 Mar 2021 - 99% finished notebook 07, added links to first 14 hours of the course on YouTube (&lt;a href=&#34;https://youtu.be/tpCFfeUEGs8&#34;&gt;10 hours in part 1&lt;/a&gt;, &lt;a href=&#34;https://youtu.be/ZUKz4125WNI&#34;&gt;4 hours in part 2&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;11 Mar 2021 - added even more text annotations to notebook 07, finishing tomorrow, then slides&lt;/li&gt; &#xA; &lt;li&gt;10 Mar 2021 - Typed a whole bunch of explanations into notebook 07, continuing tomorrow&lt;/li&gt; &#xA; &lt;li&gt;09 Mar 2021 - fixed plenty of code in notebook 07, should run end to end very cleanly (though loading times are still a thing)&lt;/li&gt; &#xA; &lt;li&gt;05 Mar 2021 - added draft notebook 07 (heaps of data loading and model training improvements in this one!), gonna fix up over next few days&lt;/li&gt; &#xA; &lt;li&gt;01 Mar 2021 - Added slides for 06 (&lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning/raw/main/slides/06_transfer_learning_with_tensorflow_part_3_scaling_up.pdf&#34;&gt;see them here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;26 Feb 2021 - 🚀 LAUNCHED!!!!! also finished recording videos for 06, onto 07, 08, 09 for next release&lt;/li&gt; &#xA; &lt;li&gt;24 Feb 2021 - recorded 9 videos for section 06, launch inbound!!!&lt;/li&gt; &#xA; &lt;li&gt;23 Feb 2021 - rearranged GitHub in preparation for launch 🚀&lt;/li&gt; &#xA; &lt;li&gt;18 Feb 2021 - recorded 8 videos for 05 and... it&#39;s done! onto polishing the GitHub&lt;/li&gt; &#xA; &lt;li&gt;17 Feb 2021 - recorded 10 videos for 05! going to finish tomorrow 🚀&lt;/li&gt; &#xA; &lt;li&gt;16 Feb 2021 - polished slides for 05 and started recording videos, got 7 videos done for 05&lt;/li&gt; &#xA; &lt;li&gt;15 Feb 2021 - finished videos for 04, now preparing to record for 05!&lt;/li&gt; &#xA; &lt;li&gt;12 Feb 2021 - recored 7 videos for section 04... wanted 10 but we&#39;ll take 7 (🤔 this seems to have happened before)&lt;/li&gt; &#xA; &lt;li&gt;11 Feb 2021 - NO PROGRESS - gave a Machine Learning deployment tutorial for &lt;a href=&#34;https://stanford-cs329s.github.io/syllabus.html&#34;&gt;Stanford&#39;s CS329s&lt;/a&gt; (using the model code from this course!!!) - &lt;a href=&#34;https://github.com/mrdbourke/cs329s-ml-deployment-tutorial&#34;&gt;see the full tutorial materials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;08 Feb 2021 - recorded 10 videos for section 03... and section 03 is done! 🚀 onto section 04&lt;/li&gt; &#xA; &lt;li&gt;30 Jan 2021 - 07 Feb 2021: NO PROGRESS (working on a ML deployment lecture for &lt;a href=&#34;https://stanford-cs329s.github.io/syllabus.html&#34;&gt;Stanford&#39;s CS329s&lt;/a&gt;... more on this later)&lt;/li&gt; &#xA; &lt;li&gt;29 Jan 2021 - recorded 9 videos for section 03... closer to 10 than yesterday but still not there&lt;/li&gt; &#xA; &lt;li&gt;28 Jan 2021 - recorded 7 videos for section 03... wanted 10 but we&#39;ll take 7&lt;/li&gt; &#xA; &lt;li&gt;27 Jan 2021 - recorded 10 videos for section 03&lt;/li&gt; &#xA; &lt;li&gt;26 Jan 2021 - polished GitHub README (what you&#39;re looking at) with a &lt;a href=&#34;https://github.com/mrdbourke/tensorflow-deep-learning#course-materials&#34;&gt;nice table&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;23 Jan 2021 - finished slides of 06&lt;/li&gt; &#xA; &lt;li&gt;22 Jan 2021 - finished review of notebook 06 &amp;amp; started slides of 06&lt;/li&gt; &#xA; &lt;li&gt;21 Jan 2021 - finished slides for 05 &amp;amp; started review of 06&lt;/li&gt; &#xA; &lt;li&gt;20 Jan 2021 - finished notebook 05 &amp;amp; 95% slides for 05&lt;/li&gt; &#xA; &lt;li&gt;19 Jan 2021 - found a storage idea for data during course (use Google Storage in same region as Colab Notebooks, cheapest/fastest)&lt;/li&gt; &#xA; &lt;li&gt;18 Jan 2021 - reviewed notebook 05 &amp;amp; slides for 05&lt;/li&gt; &#xA; &lt;li&gt;17 Jan 2021 - finished notebook 04 &amp;amp; slides for 04&lt;/li&gt; &#xA; &lt;li&gt;16 Jan 2021 - review notebook 04 &amp;amp; made slides for transfer learning&lt;/li&gt; &#xA; &lt;li&gt;13 Jan 2021 - review notebook 03 again &amp;amp; finished slides for 03, BIGGGGG updates to the README, notebook 03 99% done, just need to figure out optimum way to transfer data (e.g. when a student downloads it, where&#39;s best to store it in the meantime? Dropbox? S3? &lt;del&gt;GS&lt;/del&gt; (too expensive)&lt;/li&gt; &#xA; &lt;li&gt;11 Jan 2021 - reviewed notebook 03, 95% ready for recording, onto slides for 03&lt;/li&gt; &#xA; &lt;li&gt;9 Jan 2021 - I&#39;m back baby! Finished all videos for 02, now onto slides/materials for 03, 04, 05 (then I&#39;ll get back in the lab)&lt;/li&gt; &#xA; &lt;li&gt;19 Dec 2020 - ON HOLD (family holiday until Jan 02 2021)&lt;/li&gt; &#xA; &lt;li&gt;18 Dec 2020 - recorded 75% of videos for 02&lt;/li&gt; &#xA; &lt;li&gt;17 Dec 2020 - recorded 50% of videos for 02&lt;/li&gt; &#xA; &lt;li&gt;16 Dec 2020 - recorded 100% of videos for 01&lt;/li&gt; &#xA; &lt;li&gt;15 Dec 2020 - recorded 90% of videos for 01&lt;/li&gt; &#xA; &lt;li&gt;09 Dec 2020 - finished recording videos for 00&lt;/li&gt; &#xA; &lt;li&gt;08 Dec 2020 - recorded 90% of videos for 00&lt;/li&gt; &#xA; &lt;li&gt;05 Dec 2020 - trialled recording studio for ~6 videos with notebook 00 material&lt;/li&gt; &#xA; &lt;li&gt;04 Dec 2020 - setup &lt;a href=&#34;https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/misc-studio-setup.jpeg&#34;&gt;recording studio in closet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;03 Dec 2020 - finished notebook 02, finished slides for 02, time to setup recording studio&lt;/li&gt; &#xA; &lt;li&gt;02 Dec 2020 - notebook 02 95% done, slides for 02 90% done&lt;/li&gt; &#xA; &lt;li&gt;01 Dec 2020 - added notebook 02 (90% polished), start preparing slides for 02&lt;/li&gt; &#xA; &lt;li&gt;27 Nov 2020 - polished notebook 01, made slides for notebook 01&lt;/li&gt; &#xA; &lt;li&gt;26 Nov 2020 - polished notebook 00, made slides for notebook 00&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>DataTalksClub/mlops-zoomcamp</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/DataTalksClub/mlops-zoomcamp</id>
    <link href="https://github.com/DataTalksClub/mlops-zoomcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Free MLOps course from DataTalks.Club&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLOps Zoomcamp&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=3T5kUA3eWWc&amp;amp;list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/images/banner.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Our MLOps Zoomcamp course&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sign up here: &lt;a href=&#34;https://airtable.com/shrCb8y6eTbPKwSTL&#34;&gt;https://airtable.com/shrCb8y6eTbPKwSTL&lt;/a&gt; (it&#39;s not automated, you will not receive an email immediately after filling in the form)&lt;/li&gt; &#xA; &lt;li&gt;Register in &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;DataTalks.Club&#39;s Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join the &lt;a href=&#34;https://app.slack.com/client/T01ATQK62F8/C02R98X7DS9&#34;&gt;&lt;code&gt;#course-mlops-zoomcamp&lt;/code&gt;&lt;/a&gt; channel&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ctt.ac/fH67W&#34;&gt;Tweet about the course!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Subscribe to the &lt;a href=&#34;https://calendar.google.com/calendar/?cid=M3Jzbmg0ZDA2aHVsY2M1ZjcyNDJtODNyMTRAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ&#34;&gt;public Google calendar&lt;/a&gt; (subscription works from desktop only)&lt;/li&gt; &#xA; &lt;li&gt;Start watching course videos! &lt;a href=&#34;https://www.youtube.com/playlist?list=PL3MmuxUbc_hIUISrluw_A7wDSmfOhErJK&#34;&gt;Course playlist&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/12TlBfhIiKtyBv8RnsoJR6F72bkPDGEvPOItJIxaEzE0/edit&#34;&gt;Technical FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vRhinTR4Gpxcud-xX0cPBVqboO8RE5gFY7W2dfgfhzECuPFOaCoo9TVWUTxxrSmzvbZY0D-N1vai8RN/pubhtml&#34;&gt;Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;h3&gt;Objective&lt;/h3&gt; &#xA;&lt;p&gt;Teach practical aspects of productionizing ML services — from collecting requirements to model deployment and monitoring.&lt;/p&gt; &#xA;&lt;h3&gt;Target audience&lt;/h3&gt; &#xA;&lt;p&gt;Data scientists and ML engineers. Also software engineers and data engineers interested in learning about putting ML in production.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-requisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python&lt;/li&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA; &lt;li&gt;Being comfortable with command line&lt;/li&gt; &#xA; &lt;li&gt;Prior exposure to machine learning (at work or from other courses, e.g. from &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp&#34;&gt;ML Zoomcamp&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Prior programming experience (at least 1+ year)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Timeline&lt;/h3&gt; &#xA;&lt;p&gt;Course start: 16 of May&lt;/p&gt; &#xA;&lt;h2&gt;Syllabus&lt;/h2&gt; &#xA;&lt;p&gt;This is a draft and will change.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/01-intro&#34;&gt;Module 1: Introduction&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;What is MLOps&lt;/li&gt; &#xA; &lt;li&gt;MLOps maturity model&lt;/li&gt; &#xA; &lt;li&gt;Running example: NY Taxi trips dataset&lt;/li&gt; &#xA; &lt;li&gt;Why do we need MLOps&lt;/li&gt; &#xA; &lt;li&gt;Course overview&lt;/li&gt; &#xA; &lt;li&gt;Environment preparation&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/01-intro&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/02-experiment-tracking&#34;&gt;Module 2: Experiment tracking and model management&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Experiment tracking intro&lt;/li&gt; &#xA; &lt;li&gt;Getting started with MLflow&lt;/li&gt; &#xA; &lt;li&gt;Experiment tracking with MLflow&lt;/li&gt; &#xA; &lt;li&gt;Saving and loading models with MLflow&lt;/li&gt; &#xA; &lt;li&gt;Model registry&lt;/li&gt; &#xA; &lt;li&gt;MLflow in practice&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/02-experiment-tracking&#34;&gt;More details&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Module 3: Orchestration and ML Pipelines&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ML Pipelines: introduction&lt;/li&gt; &#xA; &lt;li&gt;Prefect&lt;/li&gt; &#xA; &lt;li&gt;Turning a notebook into a pipeline&lt;/li&gt; &#xA; &lt;li&gt;Kubeflow Pipelines&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Module 4: Model Deployment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Batch vs online&lt;/li&gt; &#xA; &lt;li&gt;For online: web services vs streaming&lt;/li&gt; &#xA; &lt;li&gt;Serving models in Batch mode&lt;/li&gt; &#xA; &lt;li&gt;Web services&lt;/li&gt; &#xA; &lt;li&gt;Streaming (Kinesis/SQS + AWS Lambda)&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Module 5: Model Monitoring&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ML monitoring vs software monitoring&lt;/li&gt; &#xA; &lt;li&gt;Data quality monitoring&lt;/li&gt; &#xA; &lt;li&gt;Data drift / concept drift&lt;/li&gt; &#xA; &lt;li&gt;Batch vs real-time monitoring&lt;/li&gt; &#xA; &lt;li&gt;Tools: Evidently, Prometheus and Grafana&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Module 6: Best Practices&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Devops&lt;/li&gt; &#xA; &lt;li&gt;Virtual environments and Docker&lt;/li&gt; &#xA; &lt;li&gt;Python: logging, linting&lt;/li&gt; &#xA; &lt;li&gt;Testing: unit, integration, regression&lt;/li&gt; &#xA; &lt;li&gt;CI/CD (github actions)&lt;/li&gt; &#xA; &lt;li&gt;Infrastructure as code (terraform, cloudformation)&lt;/li&gt; &#xA; &lt;li&gt;Cookiecutter&lt;/li&gt; &#xA; &lt;li&gt;Makefiles&lt;/li&gt; &#xA; &lt;li&gt;Homework&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Module 7: Processes&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CRISP-DM, CRISP-ML&lt;/li&gt; &#xA; &lt;li&gt;ML Canvas&lt;/li&gt; &#xA; &lt;li&gt;Data Landscape canvas&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://miro.com/miroverse/mlops-stack-canvas/&#34;&gt;MLOps Stack Canvas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Documentation practices in ML projects (Model Cards Toolkit)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Project&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;End-to-end project with all the things above&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running example&lt;/h2&gt; &#xA;&lt;p&gt;To make it easier to connect different modules together, we’d like to use the same running example throughout the course.&lt;/p&gt; &#xA;&lt;p&gt;Possible candidates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page&#34;&gt;https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page&lt;/a&gt; - predict the ride duration or if the driver is going to be tipped or not&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Instructors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Larysa Visengeriyeva&lt;/li&gt; &#xA; &lt;li&gt;Cristian Martinez&lt;/li&gt; &#xA; &lt;li&gt;Kevin Kho&lt;/li&gt; &#xA; &lt;li&gt;Theofilos Papapanagiotou&lt;/li&gt; &#xA; &lt;li&gt;Alexey Grigorev&lt;/li&gt; &#xA; &lt;li&gt;Emeli Dral&lt;/li&gt; &#xA; &lt;li&gt;Sejal Vaidya&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other courses from DataTalks.Club:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp&#34;&gt;Machine Learning Zoomcamp - free 4-month course about ML Engineering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DataTalksClub/data-engineering-zoomcamp/&#34;&gt;Data Engineering Zoomcamp - free 9-week course about Data Engineering&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;I want to start preparing for the course. What can I do?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you haven&#39;t used Flask or Docker&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/05-deployment&#34;&gt;Module 5&lt;/a&gt; form ML Zoomcamp&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_1_basics_n_setup/2_docker_sql&#34;&gt;section about Docker&lt;/a&gt; from Data Engineering Zoomcamp could also be useful&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have no previous experience with ML&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/01-intro&#34;&gt;Module 1&lt;/a&gt; from ML Zoomcamp for an overview&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/03-classification&#34;&gt;Module 3&lt;/a&gt; will also be helpful if you want to learn Scikit-Learn (we&#39;ll use it in this course)&lt;/li&gt; &#xA; &lt;li&gt;We&#39;ll also use XGBoost. You don&#39;t have to know it well, but if you want to learn more about it, refer to &lt;a href=&#34;https://github.com/alexeygrigorev/mlbookcamp-code/tree/master/course-zoomcamp/06-trees&#34;&gt;module 6&lt;/a&gt; of ML Zoomcamp&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;I registered but haven&#39;t received an invite link. Is it normal?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes, we haven&#39;t automated it. You&#39;ll get a mail from us eventually, don&#39;t worry.&lt;/p&gt; &#xA;&lt;p&gt;If you want to make sure you don&#39;t miss anything:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Register in &lt;a href=&#34;https://datatalks.club/slack.html&#34;&gt;our Slack&lt;/a&gt; and join the &lt;code&gt;#course-mlops-zoomcamp&lt;/code&gt; channel&lt;/li&gt; &#xA; &lt;li&gt;Subscribe to &lt;a href=&#34;https://youtube.com/c/datatalksclub&#34;&gt;our YouTube channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is it going to be live?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;No and yes. There will be two parts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lectures: Pre-recorded, you can watch them when it&#39;s convenient for you.&lt;/li&gt; &#xA; &lt;li&gt;Office hours: Live on Mondays (17:00 CET), but recorded, so you can watch later.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supporters and partners&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to the course sponsors for making it possible to create this course&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.prefect.io/&#34;&gt; &lt;img height=&#34;100&#34; src=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/images/prefect.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Thanks to our friends for spreading the word about the course&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://dphi.tech/&#34;&gt; &lt;img height=&#34;75&#34; src=&#34;https://datatalks.club/images/partners/dphi.png&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.confetti.ai/&#34;&gt; &lt;img height=&#34;75&#34; src=&#34;https://datatalks.club/images/partners/confetti.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://mlopsworld.com/&#34;&gt; &lt;img height=&#34;75&#34; src=&#34;https://raw.githubusercontent.com/DataTalksClub/mlops-zoomcamp/main/images/mlops-world.png&#34;&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>amanchadha/coursera-deep-learning-specialization</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/amanchadha/coursera-deep-learning-specialization</id>
    <link href="https://github.com/amanchadha/coursera-deep-learning-specialization" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notes, programming assignments and quizzes from all courses within the Coursera Deep Learning specialization offered by deeplearning.ai: (i) Neural Networks and Deep Learning; (ii) Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization; (iii) Structuring Machine Learning Projects; (iv) Convolutional Neural Network…&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Deep Learning Specialization on Coursera (offered by deeplearning.ai)&lt;/h1&gt; &#xA;&lt;p&gt;Programming assignments and quizzes from all courses in the Coursera &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Deep Learning specialization&lt;/a&gt; offered by &lt;code&gt;deeplearning.ai&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Instructor: &lt;a href=&#34;http://www.andrewng.org/&#34;&gt;Andrew Ng&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;h3&gt;For detailed interview-ready notes on all courses in the Coursera Deep Learning specialization, refer &lt;a href=&#34;https://aman.ai/&#34;&gt;www.aman.ai&lt;/a&gt;.&lt;/h3&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;setup.sh&lt;/code&gt; to (i) download a pre-trained VGG-19 dataset and (ii) extract the zip&#39;d pre-trained models and datasets that are needed for all the assignments.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This repo contains my work for this specialization. The code base, quiz questions and diagrams are taken from the &lt;a href=&#34;https://www.coursera.org/specializations/deep-learning&#34;&gt;Deep Learning Specialization on Coursera&lt;/a&gt;, unless specified otherwise.&lt;/p&gt; &#xA;&lt;h2&gt;2021 Version&lt;/h2&gt; &#xA;&lt;p&gt;This specialization was updated in April 2021 to include developments in deep learning and programming frameworks, with the biggest change being shifting from TensorFlow 1 to TensorFlow 2. This repo has been updated accordingly as well.&lt;/p&gt; &#xA;&lt;h2&gt;Programming Assignments&lt;/h2&gt; &#xA;&lt;h3&gt;Course 1: Neural Networks and Deep Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Python%20Basics%20with%20Numpy/Python_Basics_With_Numpy_v3a.ipynb&#34;&gt;Week 2 - PA 1 - Python Basics with Numpy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Logistic%20Regression%20as%20a%20Neural%20Network/Logistic_Regression_with_a_Neural_Network_mindset_v6a.ipynb&#34;&gt;Week 2 - PA 2 - Logistic Regression with a Neural Network mindset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Planar%20data%20classification%20with%20one%20hidden%20layer/Planar_data_classification_with_onehidden_layer_v6c.ipynb&#34;&gt;Week 3 - PA 3 - Planar data classification with one hidden layer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Building%20your%20Deep%20Neural%20Network%20-%20Step%20by%20Step/Building_your_Deep_Neural_Network_Step_by_Step_v8a.ipynb&#34;&gt;Week 4 - PA 4 - Building your Deep Neural Network: Step by Step&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/Deep%20Neural%20Network%20-%20Application%20v8.ipynb&#34;&gt;Week 4 - PA 5 - Deep Neural Network for Image Classification: Application&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Initialization/Initialization.ipynb&#34;&gt;Week 1 - PA 1 - Initialization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Regularization/Regularization_v2a.ipynb&#34;&gt;Week 1 - PA 2 - Regularization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Gradient%20Checking/Gradient%20Checking%20v1.ipynb&#34;&gt;Week 1 - PA 3 - Gradient Checking&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Optimization_methods_v1b.ipynb&#34;&gt;Week 2 - PA 4 - Optimization Methods&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Tensorflow_introduction.ipynb&#34;&gt;Week 3 - PA 5 - TensorFlow Tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 3: Structuring Machine Learning Projects&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are no programming assignments for this course. But this course comes with very interesting case study quizzes (below).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 4: Convolutional Neural Networks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Step_by_Step_v1.ipynb&#34;&gt;Week 1 - PA 1 - Convolutional Model: step by step&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Convolution_model_Application.ipynb&#34;&gt;Week 1 - PA 2 - Convolutional Neural Networks: Application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/KerasTutorial/Keras%20-%20Tutorial%20-%20Happy%20House%20v2.ipynb&#34;&gt;Week 2 - PA 1 - Keras - Tutorial - Happy House&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/ResNets/Residual_Networks.ipynb&#34;&gt;Week 2 - PA 2 - Residual Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Transfer%20Learning%20with%20MobileNet/Transfer_learning_with_MobileNet_v1.ipynb&#34;&gt;Week 2 - PA 2 - Transfer Learning with MobileNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Car%20detection%20for%20Autonomous%20Driving/Autonomous_driving_application_Car_detection.ipynb&#34;&gt;Week 3 - PA 1 - Car detection with YOLO for Autonomous Driving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Image%20Segmentation%20Unet/Image_segmentation_Unet_v2.ipynb&#34;&gt;Week 3 - PA 2 - Image Segmentation Unet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Neural%20Style%20Transfer/Art_Generation_with_Neural_Style_Transfer.ipynb&#34;&gt;Week 4 - PA 1 - Art Generation with Neural Style Transfer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Face%20Recognition/Face_Recognition.ipynb&#34;&gt;Week 4 - PA 2 - Face Recognition&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 5: Sequence Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step/Building_a_Recurrent_Neural_Network_Step_by_Step.ipynb&#34;&gt;Week 1 - PA 1 - Building a Recurrent Neural Network - Step by Step&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Dinosaur%20Island%20--%20Character-level%20language%20model/Dinosaurus_Island_Character_level_language_model.ipynb&#34;&gt;Week 1 - PA 2 - Dinosaur Land -- Character-level Language Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Jazz%20improvisation%20with%20LSTM/Improvise_a_Jazz_Solo_with_an_LSTM_Network_v4_Solution.ipynb&#34;&gt;Week 1 - PA 3 - Jazz improvisation with LSTM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Word%20Vector%20Representation/Operations_on_word_vectors_v2a.ipynb&#34;&gt;Week 2 - PA 1 - Word Vector Representation and Debiasing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Emojify/Emoji_v3a.ipynb&#34;&gt;Week 2 - PA 2 - Emojify!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Machine%20Translation/Neural_machine_translation_with_attention_v4a.ipynb&#34;&gt;Week 3 - PA 1 - Neural Machine Translation with Attention&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Trigger%20word%20detection/Trigger_word_detection_v2a.ipynb&#34;&gt;Week 3 - PA 2 - Trigger Word Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%204/Transformer%20Subclass/C5_W4_A1_Transformer_Subclass_v1.ipynb&#34;&gt;Week 4 - PA 1 - Transformer Network&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Named%20Entity%20Recognition/Transformer_application_Named_Entity_Recognition.ipynb&#34;&gt;Week 3 - PA 2 - Transformer Network Application: Named-Entity Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Question%20Answering/QA_transformer.ipynb&#34;&gt;Week 3 - PA 2 - Transformer Network Application: Question Answering&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quiz Solutions&lt;/h2&gt; &#xA;&lt;h3&gt;Course 1: Neural Networks and Deep Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Introduction to deep learning: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%201/Week%201%20Quiz%20-%20Introduction%20to%20deep%20learning.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Neural Network Basics: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%202/Week%202%20Quiz%20-%20Neural%20Network%20Basics.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Shallow Neural Networks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%203/Week%203%20Quiz%20-%20Shallow%20Neural%20Networks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 4 Quiz - Key concepts on Deep Neural Networks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C1%20-%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Week%204%20Quiz%20-%20Key%20concepts%20on%20Deep%20Neural%20Networks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 2: Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Practical aspects of deep learning: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%201/Week%201%20Quiz%20-%20Practical%20aspects%20of%20deep%20learning.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Optimization algorithms: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%202/Week%202%20Quiz%20-%20Optimization%20algorithms.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Hyperparameter tuning, Batch Normalization, Programming Frameworks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C2%20-%20Improving%20Deep%20Neural%20Networks%20Hyperparameter%20tuning%2C%20Regularization%20and%20Optimization/Week%203/Week%203%20Quiz%20-%20Hyperparameter%20tuning%2C%20Batch%20Normalization%2C%20Programming%20Frameworks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 3: Structuring Machine Learning Projects&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Bird recognition in the city of Peacetopia (case study): &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%201%20Quiz%20-%20Bird%20recognition%20in%20the%20city%20of%20Peacetopia%20(case%20study).pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Autonomous driving (case study): &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C3%20-%20Structuring%20Machine%20Learning%20Projects/Week%202%20Quiz%20-%20Autonomous%20driving%20(case%20study).pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 4: Convolutional Neural Networks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - The basics of ConvNets: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%201/Week%201%20Quiz%20-%20The%20basics%20of%20ConvNets.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Deep convolutional models: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%202/Week%202%20Quiz%20-%20Deep%20convolutional%20models.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Detection algorithms: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%203/Week%203%20Quiz%20-%20Detection%20algorithms.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 4 Quiz - Special applications: Face recognition &amp;amp; Neural style transfer: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C4%20-%20Convolutional%20Neural%20Networks/Week%204/Week%204%20Quiz%20-%20Special%20applications%20Face%20Recognition%20and%20Neural%20Style%20Transfer.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Course 5: Sequence Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Week 1 Quiz - Recurrent Neural Networks: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%201/Week%201%20Quiz%20-%20Recurrent%20Neural%20Networks.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 2 Quiz - Natural Language Processing &amp;amp; Word Embeddings: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%202/Week%202%20Quiz%20-%20Natural%20Language%20Processing%20%26%20Word%20Embeddings.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Week 3 Quiz - Sequence models &amp;amp; Attention mechanism: &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.md&#34;&gt;Text&lt;/a&gt; | &lt;a href=&#34;https://nbviewer.jupyter.org/github/amanchadha/coursera-deep-learning-specialization/blob/master/C5%20-%20Sequence%20Models/Week%203/Week%203%20Quiz%20-%20Sequence%20models%20%26%20Attention%20mechanisms.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;I recognize the time people spend on building intuition, understanding new concepts and debugging assignments. The solutions uploaded here are &lt;strong&gt;only for reference&lt;/strong&gt;. They are meant to unblock you if you get stuck somewhere. Please do not copy any part of the code as-is (the programming assignments are fairly easy if you read the instructions carefully). Similarly, try out the quizzes yourself before you refer to the quiz solutions. This course is the most straight-forward deep learning course I have ever taken, with fabulous course content and structure. It&#39;s a treasure by the deeplearning.ai team.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>saic-mdal/lama</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/saic-mdal/lama</id>
    <link href="https://github.com/saic-mdal/lama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🦙 LaMa Image Inpainting, Resolution-robust Large Mask Inpainting with Fourier Convolutions, WACV 2022&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🦙 LaMa: Resolution-robust Large Mask Inpainting with Fourier Convolutions&lt;/h1&gt; &#xA;&lt;p&gt;Official implementation by Samsung Research&lt;/p&gt; &#xA;&lt;p&gt;by Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, Victor Lempitsky.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; &#34;font-size:30px;&#34;&gt; 🔥🔥🔥 &lt;br&gt; &lt;b&gt; LaMa generalizes surprisingly well to much higher resolutions (~2k❗️) than it saw during training (256x256), and achieves the excellent performance even in challenging scenarios, e.g. completion of periodic structures.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://saic-mdal.github.io/lama-project/&#34;&gt;Project page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2109.07161&#34;&gt;arXiv&lt;/a&gt;] [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf&#34;&gt;Supplementary&lt;/a&gt;] [&lt;a href=&#34;https://senya-ashukha.github.io/projects/lama_21/paper.txt&#34;&gt;BibTeX&lt;/a&gt;] [&lt;a href=&#34;https://www.casualganpapers.com/large-masks-fourier-convolutions-inpainting/LaMa-explained.html&#34;&gt;Casual GAN Papers Summary&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://colab.research.google.com/github/saic-mdal/lama/blob/master//colab/LaMa_inpainting.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;br&gt; Try out in Google Colab &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/ezgif-4-0db51df695a8.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/senya-ashukha/senya-ashukha.github.io/master/projects/lama_21/gif_for_lightning_v1_white.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Non-official 3rd party apps:&lt;/h1&gt; &#xA;&lt;p&gt;(Feel free to share your app/implementation/demo by creating an issue)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt; - a simple interactive object removal tool by &lt;a href=&#34;https://twitter.com/cyrildiagne&#34;&gt;@cyrildiagne&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;lama-cleaner&lt;/a&gt; by &lt;a href=&#34;https://github.com/Sanster/lama-cleaner&#34;&gt;@Sanster&lt;/a&gt; is a self-host version of &lt;a href=&#34;https://cleanup.pictures/&#34;&gt;https://cleanup.pictures&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Integrated to &lt;a href=&#34;https://huggingface.co/spaces&#34;&gt;Huggingface Spaces&lt;/a&gt; with &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;. See demo: &lt;a href=&#34;https://huggingface.co/spaces/akhaliq/lama&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; by &lt;a href=&#34;https://github.com/AK391&#34;&gt;@AK391&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Telegram bot &lt;a href=&#34;https://t.me/MagicEraserBot&#34;&gt;@MagicEraserBot&lt;/a&gt; by &lt;a href=&#34;https://github.com/Moldoteck&#34;&gt;@Moldoteck&lt;/a&gt;, &lt;a href=&#34;https://github.com/Moldoteck/MagicEraser&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/andy971022/auto-lama&#34;&gt;Auto-LaMa&lt;/a&gt; = DE:TR object detection + LaMa inpainting by &lt;a href=&#34;https://github.com/andy971022&#34;&gt;@andy971022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhaoyun0071/LAMA-Magic-Eraser-Local&#34;&gt;LAMA-Magic-Eraser-Local&lt;/a&gt; = a standalone inpainting application built with PyQt5 by &lt;a href=&#34;https://github.com/zhaoyun0071&#34;&gt;@zhaoyun0071&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.hama.app/&#34;&gt;Hama&lt;/a&gt; - object removal with a smart brush which simplifies mask drawing.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Environment setup&lt;/h1&gt; &#xA;&lt;p&gt;Clone the repo: &lt;code&gt;git clone https://github.com/saic-mdal/lama.git&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are three options of an environment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Python virtualenv:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;virtualenv inpenv --python=/usr/bin/python3&#xA;source inpenv/bin/activate&#xA;pip install torch==1.8.0 torchvision==0.9.0&#xA;&#xA;cd lama&#xA;pip install -r requirements.txt &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Conda&lt;/p&gt; &lt;pre&gt;&lt;code&gt;% Install conda for Linux, for other OS download miniconda at https://docs.conda.io/en/latest/miniconda.html&#xA;wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh&#xA;bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda&#xA;$HOME/miniconda/bin/conda init bash&#xA;&#xA;cd lama&#xA;conda env create -f conda_env.yml&#xA;conda activate lama&#xA;conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch -y&#xA;pip install pytorch-lightning==1.2.9&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Docker: No actions are needed 🎉.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Inference &lt;a name=&#34;prediction&#34;&gt;&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Download pre-trained models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install tool for yandex disk link extraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install wldhx.yadisk-direct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The best model (Places2, Places Challenge):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/ouP6l8VJ0HpMZg) -o big-lama.zip&#xA;unzip big-lama.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All models (Places &amp;amp; CelebA-HQ):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/EgqaSnLohjuzAg) -o lama-models.zip&#xA;unzip lama-models.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Prepare images and masks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download test images:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -L $(yadisk-direct https://disk.yandex.ru/d/xKQJZeVRk5vLlQ) -o LaMa_test_images.zip&#xA;unzip LaMa_test_images.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;OR prepare your data:&lt;/summary&gt; 1) Create masks named as `[images_name]_maskXXX[image_suffix]`, put images and masks in the same folder. &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;You can use the &lt;a href=&#34;https://github.com/saic-mdal/lama/raw/main/bin/gen_mask_dataset.py&#34;&gt;script&lt;/a&gt; for random masks generation.&lt;/li&gt; &#xA;  &lt;li&gt;Check the format of the files: &lt;pre&gt;&lt;code&gt;image1_mask001.png&#xA;image1.png&#xA;image2_mask001.png&#xA;image2.png&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;ol start=&#34;2&#34;&gt; &#xA;  &lt;li&gt;Specify &lt;code&gt;image_suffix&lt;/code&gt;, e.g. &lt;code&gt;.png&lt;/code&gt; or &lt;code&gt;.jpg&lt;/code&gt; or &lt;code&gt;_input.jpg&lt;/code&gt; in &lt;code&gt;configs/prediction/default.yaml&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Predict&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/predict.py model.path=$(pwd)/big-lama indir=$(pwd)/LaMa_test_images outdir=$(pwd)/output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker&lt;/p&gt; &#xA;&lt;p&gt;The following command will pull the docker image from Docker Hub and execute the prediction script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/2_predict.sh $(pwd)/big-lama $(pwd)/LaMa_test_images $(pwd)/output device=cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker cuda: TODO&lt;/p&gt; &#xA;&lt;h1&gt;Train and Eval&lt;/h1&gt; &#xA;&lt;p&gt;⚠️ Warning: The training is not fully tested yet, e.g., did not re-training after refactoring ⚠️&lt;/p&gt; &#xA;&lt;p&gt;Make sure you run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then download models for &lt;em&gt;perceptual loss&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p ade20k/ade20k-resnet50dilated-ppm_deepsup/&#xA;wget -P ade20k/ade20k-resnet50dilated-ppm_deepsup/ http://sceneparsing.csail.mit.edu/model/pytorch/ade20k-resnet50dilated-ppm_deepsup/encoder_epoch_20.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Places&lt;/h2&gt; &#xA;&lt;p&gt;⚠️ NB: FID/SSIM/LPIPS metric values for Places that we see in LaMa paper are computed on 30000 images that we produce in evaluation section below. For more details on evaluation data check [&lt;a href=&#34;https://ashukha.com/projects/lama_21/lama_supmat_2021.pdf#subsection.3.1&#34;&gt;Section 3. Dataset splits in Supplementary&lt;/a&gt;] ⚠️&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Download data from http://places2.csail.mit.edu/download.html&#xA;# Places365-Standard: Train(105GB)/Test(19GB)/Val(2.1GB) from High-resolution images section&#xA;wget http://data.csail.mit.edu/places/places365/train_large_places365standard.tar&#xA;wget http://data.csail.mit.edu/places/places365/val_large.tar&#xA;wget http://data.csail.mit.edu/places/places365/test_large.tar&#xA;&#xA;# Unpack train/test/val data and create .yaml config for it&#xA;bash fetch_data/places_standard_train_prepare.sh&#xA;bash fetch_data/places_standard_test_val_prepare.sh&#xA;&#xA;# Sample images for test and viz at the end of epoch&#xA;bash fetch_data/places_standard_test_val_sample.sh&#xA;bash fetch_data/places_standard_test_val_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=places_standard&#xA;&#xA;# To evaluate trained model and report metrics as in our paper&#xA;# we need to sample previously unseen 30k images and generate masks for them&#xA;bash fetch_data/places_standard_evaluation_prepare_data.sh&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and 512 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;outdir=$(pwd)/inference/random_thick_512 model.checkpoint=last.ckpt&#xA;&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/places_standard_dataset/evaluation/random_thick_512/ \&#xA;$(pwd)/inference/random_thick_512 \&#xA;$(pwd)/inference/random_thick_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;CelebA&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# Download CelebA-HQ dataset&#xA;# Download data256x256.zip from https://drive.google.com/drive/folders/11Vz0fqHS2rXDb5pprgTjpD7S2BAJhi1P&#xA;&#xA;# unzip &amp;amp; split into train/test/visualization &amp;amp; create config for it&#xA;bash fetch_data/celebahq_dataset_prepare.sh&#xA;&#xA;# generate masks for test and visual_test at the end of epoch&#xA;bash fetch_data/celebahq_gen_masks.sh&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier-celeba data.batch_size=10&#xA;&#xA;# Infer model on thick/thin/medium masks in 256 and run evaluation &#xA;# like this:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier-celeba_/ \&#xA;indir=$(pwd)/celeba-hq-dataset/visual_test_256/random_thick_256/ \&#xA;outdir=$(pwd)/inference/celeba_random_thick_256 model.checkpoint=last.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Places Challenge&lt;/h2&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# This script downloads multiple .tar files in parallel and unpacks them&#xA;# Places365-Challenge: Train(476GB) from High-resolution images (to train Big-Lama) &#xA;bash places_challenge_train_download.sh&#xA;&#xA;TODO: prepare&#xA;TODO: train &#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Docker: TODO&lt;/p&gt; &#xA;&lt;h2&gt;Create your data&lt;/h2&gt; &#xA;&lt;p&gt;Please check bash scripts for data preparation and mask generation from CelebaHQ section, if you stuck at one of the following steps.&lt;/p&gt; &#xA;&lt;p&gt;On the host machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Make shure you are in lama folder&#xA;cd lama&#xA;export TORCH_HOME=$(pwd) &amp;amp;&amp;amp; export PYTHONPATH=$(pwd)&#xA;&#xA;# You need to prepare following image folders:&#xA;$ ls my_dataset&#xA;train&#xA;val_source # 2000 or more images&#xA;visual_test_source # 100 or more images&#xA;eval_source # 2000 or more images&#xA;&#xA;# LaMa generates random masks for the train data on the flight,&#xA;# but needs fixed masks for test and visual_test for consistency of evaluation.&#xA;&#xA;# Suppose, we want to evaluate and pick best models &#xA;# on 512x512 val dataset  with thick/thin/medium masks &#xA;# And your images have .jpg extention:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \ # thick, thin, medium&#xA;my_dataset/val_source/ \&#xA;my_dataset/val/random_&amp;lt;size&amp;gt;_512.yaml \# thick, thin, medium&#xA;--ext jpg&#xA;&#xA;# So the mask generator will: &#xA;# 1. resize and crop val images and save them as .png&#xA;# 2. generate masks&#xA;&#xA;ls my_dataset/val/random_medium_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Generate thick, thin, medium masks for visual_test folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/visual_test_source/ \&#xA;my_dataset/visual_test/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;ls my_dataset/visual_test/random_thick_512/&#xA;image1_crop000_mask000.png&#xA;image1_crop000.png&#xA;image2_crop000_mask000.png&#xA;image2_crop000.png&#xA;...&#xA;&#xA;# Same process for eval_source image folder:&#xA;&#xA;python3 bin/gen_mask_dataset.py \&#xA;$(pwd)/configs/data_gen/random_&amp;lt;size&amp;gt;_512.yaml \  #thick, thin, medium&#xA;my_dataset/eval_source/ \&#xA;my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \ #thick, thin, medium&#xA;--ext jpg&#xA;&#xA;&#xA;&#xA;# Generate location config file which locate these folders:&#xA;&#xA;touch my_dataset.yaml&#xA;echo &#34;data_root_dir: $(pwd)/my_dataset/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;out_root_dir: $(pwd)/experiments/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;echo &#34;tb_dir: $(pwd)/tb_logs/&#34; &amp;gt;&amp;gt; my_dataset.yaml&#xA;mv my_dataset.yaml ${PWD}/configs/training/location/&#xA;&#xA;&#xA;# Check data config for consistency with my_dataset folder structure:&#xA;$ cat ${PWD}/configs/training/data/abl-04-256-mh-dist&#xA;...&#xA;train:&#xA;  indir: ${location.data_root_dir}/train&#xA;  ...&#xA;val:&#xA;  indir: ${location.data_root_dir}/val&#xA;  img_suffix: .png&#xA;visual_test:&#xA;  indir: ${location.data_root_dir}/visual_test&#xA;  img_suffix: .png&#xA;&#xA;&#xA;# Run training&#xA;python3 bin/train.py -cn lama-fourier location=my_dataset data.batch_size=10&#xA;&#xA;# Evaluation: LaMa training procedure picks best few models according to &#xA;# scores on my_dataset/val/ &#xA;&#xA;# To evaluate one of your best models (i.e. at epoch=32) &#xA;# on previously unseen my_dataset/eval do the following &#xA;# for thin, thick and medium:&#xA;&#xA;# infer:&#xA;python3 bin/predict.py \&#xA;model.path=$(pwd)/experiments/&amp;lt;user&amp;gt;_&amp;lt;date:time&amp;gt;_lama-fourier_/ \&#xA;indir=$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;outdir=$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;model.checkpoint=epoch32.ckpt&#xA;&#xA;# metrics calculation:&#xA;python3 bin/evaluate_predicts.py \&#xA;$(pwd)/configs/eval2_gpu.yaml \&#xA;$(pwd)/my_dataset/eval/random_&amp;lt;size&amp;gt;_512/ \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512 \&#xA;$(pwd)/inference/my_dataset/random_&amp;lt;size&amp;gt;_512_metrics.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;OR&lt;/strong&gt; in the docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;TODO: train&#xA;TODO: eval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Hints&lt;/h1&gt; &#xA;&lt;h3&gt;Generate different kinds of masks&lt;/h3&gt; &#xA;&lt;p&gt;The following command will execute a script that generates random masks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash docker/1_generate_masks_from_raw_images.sh \&#xA;    configs/data_gen/random_medium_512.yaml \&#xA;    /directory_with_input_images \&#xA;    /directory_where_to_store_images_and_masks \&#xA;    --ext png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The test data generation command stores images in the format, which is suitable for &lt;a href=&#34;https://raw.githubusercontent.com/saic-mdal/lama/main/#prediction&#34;&gt;prediction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The table below describes which configs we used to generate different test sets from the paper. Note that we &lt;em&gt;do not fix a random seed&lt;/em&gt;, so the results will be slightly different each time.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Places 512x512&lt;/th&gt; &#xA;   &lt;th&gt;CelebA 256x256&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Narrow&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thin_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_medium_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Wide&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_512.yaml&lt;/td&gt; &#xA;   &lt;td&gt;random_thick_256.yaml&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to change the config path (argument #1) to any other config in &lt;code&gt;configs/data_gen&lt;/code&gt; or adjust config files themselves.&lt;/p&gt; &#xA;&lt;h3&gt;Override parameters in configs&lt;/h3&gt; &#xA;&lt;p&gt;Also you can override parameters in config like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 bin/train.py -cn &amp;lt;config&amp;gt; data.batch_size=10 run_title=my-title&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where .yaml file extension is omitted&lt;/p&gt; &#xA;&lt;h3&gt;Models options&lt;/h3&gt; &#xA;&lt;p&gt;Config names for models from paper (substitude into the training command):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;* big-lama&#xA;* big-lama-regular&#xA;* lama-fourier&#xA;* lama-regular&#xA;* lama_small_train_masks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which are seated in configs/training/folder&lt;/p&gt; &#xA;&lt;h3&gt;Links&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All the data (models, test images, etc.) &lt;a href=&#34;https://disk.yandex.ru/d/AmdeG-bIjmvSug&#34;&gt;https://disk.yandex.ru/d/AmdeG-bIjmvSug&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Test images from the paper &lt;a href=&#34;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&#34;&gt;https://disk.yandex.ru/d/xKQJZeVRk5vLlQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The pre-trained models &lt;a href=&#34;https://disk.yandex.ru/d/EgqaSnLohjuzAg&#34;&gt;https://disk.yandex.ru/d/EgqaSnLohjuzAg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The models for perceptual loss &lt;a href=&#34;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&#34;&gt;https://disk.yandex.ru/d/ncVmQlmT_kTemQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Our training logs are available at &lt;a href=&#34;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&#34;&gt;https://disk.yandex.ru/d/9Bt1wNSDS4jDkQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Training time &amp;amp; resources&lt;/h3&gt; &#xA;&lt;p&gt;TODO&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Segmentation code and models if form &lt;a href=&#34;https://github.com/CSAILVision/semantic-segmentation-pytorch&#34;&gt;CSAILVision&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;LPIPS metric is from &lt;a href=&#34;https://github.com/richzhang/PerceptualSimilarity&#34;&gt;richzhang&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SSIM is from &lt;a href=&#34;https://github.com/Po-Hsun-Su/pytorch-ssim&#34;&gt;Po-Hsun-Su&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FID is from &lt;a href=&#34;https://github.com/mseitzer/pytorch-fid&#34;&gt;mseitzer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this code helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{suvorov2021resolution,&#xA;  title={Resolution-robust Large Mask Inpainting with Fourier Convolutions},&#xA;  author={Suvorov, Roman and Logacheva, Elizaveta and Mashikhin, Anton and Remizova, Anastasia and Ashukha, Arsenii and Silvestrov, Aleksei and Kong, Naejin and Goka, Harshith and Park, Kiwoong and Lempitsky, Victor},&#xA;  journal={arXiv preprint arXiv:2109.07161},&#xA;  year={2021}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div style=&#34;text-align:center&#34; align=&#34;center&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;img loading=&#34;lazy&#34; height=&#34;50px&#34; src=&#34;https://raw.githubusercontent.com/saic-mdal/lama-project/main/docs/img/samsung_ai.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p style=&#34;font-weight:normal; font-size: 16pt;text-align:center&#34; align=&#34;center&#34;&gt;Copyright © 2021&lt;/p&gt; &#xA;&lt;br&gt;</summary>
  </entry>
  <entry>
    <title>jakevdp/PythonDataScienceHandbook</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/jakevdp/PythonDataScienceHandbook</id>
    <link href="https://github.com/jakevdp/PythonDataScienceHandbook" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python Data Science Handbook: full text in Jupyter Notebooks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python Data Science Handbook&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the entire &lt;a href=&#34;http://shop.oreilly.com/product/0636920034919.do&#34;&gt;Python Data Science Handbook&lt;/a&gt;, in the form of (free!) Jupyter notebooks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks/figures/PDSH-cover.png&#34; alt=&#34;cover image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to Use this Book&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Read the book in its entirety online at &lt;a href=&#34;https://jakevdp.github.io/PythonDataScienceHandbook/&#34;&gt;https://jakevdp.github.io/PythonDataScienceHandbook/&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the code using the Jupyter notebooks available in this repository&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/notebooks&#34;&gt;notebooks&lt;/a&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch executable versions of these notebooks using &lt;a href=&#34;http://colab.research.google.com&#34;&gt;Google Colab&lt;/a&gt;: &lt;a href=&#34;https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch a live notebook server with these notebooks using &lt;a href=&#34;https://beta.mybinder.org/&#34;&gt;binder&lt;/a&gt;: &lt;a href=&#34;https://mybinder.org/v2/gh/jakevdp/PythonDataScienceHandbook/master?filepath=notebooks%2FIndex.ipynb&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Buy the printed book through &lt;a href=&#34;http://shop.oreilly.com/product/0636920034919.do&#34;&gt;O&#39;Reilly Media&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;The book was written and tested with Python 3.5, though other Python versions (including Python 2.7) should work in nearly all cases.&lt;/p&gt; &#xA;&lt;p&gt;The book introduces the core libraries essential for working with data in Python: particularly &lt;a href=&#34;http://ipython.org&#34;&gt;IPython&lt;/a&gt;, &lt;a href=&#34;http://numpy.org&#34;&gt;NumPy&lt;/a&gt;, &lt;a href=&#34;http://pandas.pydata.org&#34;&gt;Pandas&lt;/a&gt;, &lt;a href=&#34;http://matplotlib.org&#34;&gt;Matplotlib&lt;/a&gt;, &lt;a href=&#34;http://scikit-learn.org&#34;&gt;Scikit-Learn&lt;/a&gt;, and related packages. Familiarity with Python as a language is assumed; if you need a quick introduction to the language itself, see the free companion project, &lt;a href=&#34;https://github.com/jakevdp/WhirlwindTourOfPython&#34;&gt;A Whirlwind Tour of Python&lt;/a&gt;: it&#39;s a fast-paced introduction to the Python language aimed at researchers and scientists.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb&#34;&gt;Index.ipynb&lt;/a&gt; for an index of the notebooks available to accompany the text.&lt;/p&gt; &#xA;&lt;h2&gt;Software&lt;/h2&gt; &#xA;&lt;p&gt;The code in the book was tested with Python 3.5, though most (but not all) will also work correctly with Python 2.7 and other older Python versions.&lt;/p&gt; &#xA;&lt;p&gt;The packages I used to run the code in the book are listed in &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt; (Note that some of these exact version numbers may not be available on your platform: you may have to tweak them for your own use). To install the requirements using &lt;a href=&#34;http://conda.pydata.org&#34;&gt;conda&lt;/a&gt;, run the following at the command-line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda install --file requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To create a stand-alone environment named &lt;code&gt;PDSH&lt;/code&gt; with Python 3.5 and all the required package versions, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ conda create -n PDSH python=3.5 --file requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can read more about using conda environments in the &lt;a href=&#34;http://conda.pydata.org/docs/using/envs.html&#34;&gt;Managing Environments&lt;/a&gt; section of the conda documentation.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-CODE&#34;&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Text&lt;/h3&gt; &#xA;&lt;p&gt;The text content of the book is released under the &lt;a href=&#34;https://raw.githubusercontent.com/jakevdp/PythonDataScienceHandbook/master/LICENSE-TEXT&#34;&gt;CC-BY-NC-ND license&lt;/a&gt;. Read more at &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode&#34;&gt;Creative Commons&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/notebooks</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/huggingface/notebooks</id>
    <link href="https://github.com/huggingface/notebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Notebooks using the Hugging Face libraries 🤗&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;notebooks&lt;/h1&gt; &#xA;&lt;p&gt;Notebooks using the Hugging Face libraries 🤗&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>onnx/models</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/onnx/models</id>
    <link href="https://github.com/onnx/models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of pre-trained, state-of-the-art models in the ONNX format&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ONNX Model Zoo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://onnx.ai&#34;&gt;Open Neural Network Exchange (ONNX)&lt;/a&gt; is an open standard format for representing machine learning models. ONNX is supported by a community of partners who have implemented it in many frameworks and tools.&lt;/p&gt; &#xA;&lt;p&gt;The ONNX Model Zoo is a collection of pre-trained, state-of-the-art models in the ONNX format contributed by community members like you. Accompanying each model are &lt;a href=&#34;http://jupyter.org&#34;&gt;Jupyter notebooks&lt;/a&gt; for model training and running inference with the trained model. The notebooks are written in Python and include links to the training dataset as well as references to the original paper that describes the model architecture.&lt;/p&gt; &#xA;&lt;p&gt;We have standardized on &lt;a href=&#34;https://git-lfs.github.com/&#34;&gt;Git LFS (Large File Storage)&lt;/a&gt; to store ONNX model files. To download an ONNX model, navigate to the appropriate Github page and click the &lt;code&gt;Download&lt;/code&gt; button on the top right.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;h4&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#usage-&#34;&gt;Usage&lt;/a&gt; section below for more details on the file formats in the ONNX Model Zoo (.onnx, .pb, .npz), downloading multiple ONNX models through &lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#gitlfs-&#34;&gt;Git LFS command line&lt;/a&gt;, and starter Python code for validating your ONNX model using test data.&lt;/h4&gt; &#xA;&lt;h4&gt;INT8 models are generated by &lt;a href=&#34;https://github.com/intel/neural-compressor&#34;&gt;Intel® Neural Compressor&lt;/a&gt;. &lt;a href=&#34;https://github.com/intel/neural-compressor&#34;&gt;Intel® Neural Compressor&lt;/a&gt; is an open-source Python library which supports automatic accuracy-driven tuning strategies to help user quickly find out the best quantized model. It implements dynamic and static quantization for ONNX models and can represent quantized ONNX models with operator oriented as well as tensor oriented (QDQ) ways. Users can use web-based UI service or python code to do quantization. Read the &lt;a href=&#34;https://github.com/intel/neural-compressor/raw/master/README.md&#34;&gt;Introduction&lt;/a&gt; for more details.&lt;/h4&gt; &#xA;&lt;h4&gt;Vision&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#image_classification&#34;&gt;Image Classification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#object_detection&#34;&gt;Object Detection &amp;amp; Image Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#body_analysis&#34;&gt;Body, Face &amp;amp; Gesture Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#image_manipulation&#34;&gt;Image Manipulation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Language&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#machine_comprehension&#34;&gt;Machine Comprehension&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#machine_translation&#34;&gt;Machine Translation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#language_modelling&#34;&gt;Language Modelling&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Other&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#visual_qna&#34;&gt;Visual Question Answering &amp;amp; Dialog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#speech&#34;&gt;Speech &amp;amp; Audio Processing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/#others&#34;&gt;Other interesting models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Image Classification &lt;a name=&#34;image_classification&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This collection of models take images as input, then classifies the major objects in the images into 1000 object categories such as keyboard, mouse, pencil, and many animals.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Huggingface Spaces&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/mobilenet&#34;&gt;MobileNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.04381&#34;&gt;Sandler et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Light-weight deep neural network best suited for mobile and embedded vision applications. &lt;br&gt;Top-5 error from paper - ~10%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/resnet&#34;&gt;ResNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34;&gt;He et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A CNN model (up to 152 layers). Uses shortcut connections to achieve higher accuracy when classifying images. &lt;br&gt; Top-5 error from paper - ~3.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/ResNet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/squeezenet&#34;&gt;SqueezeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.07360&#34;&gt;Iandola et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A light-weight CNN model providing AlexNet level accuracy with 50x fewer parameters. &lt;br&gt;Top-5 error from paper - ~20%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/SqueezeNet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/vgg&#34;&gt;VGG&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1409.1556&#34;&gt;Simonyan et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deep CNN model(up to 19 layers). Similar to AlexNet but uses multiple smaller kernel-sized filters that provides more accuracy when classifying images. &lt;br&gt;Top-5 error from paper - ~8%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/VGG&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/alexnet&#34;&gt;AlexNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&#34;&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A Deep CNN model (up to 8 layers) where the input is an image and the output is a vector of 1000 numbers. &lt;br&gt; Top-5 error from paper - ~15%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/AlexNet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/inception_and_googlenet/googlenet&#34;&gt;GoogleNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1409.4842.pdf&#34;&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deep CNN model(up to 22 layers). Comparatively smaller and faster than VGG and more accurate in detailing than AlexNet. &lt;br&gt; Top-5 error from paper - ~6.7%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/GoogleNet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/caffenet&#34;&gt;CaffeNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ucb-icsi-vision-group.github.io/caffe-paper/caffe.pdf&#34;&gt;Krizhevsky et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deep CNN variation of AlexNet for Image Classification in Caffe where the max pooling precedes the local response normalization (LRN) so that the LRN takes less compute and memory.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/CaffeNet&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/rcnn_ilsvrc13&#34;&gt;RCNN_ILSVRC13&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1311.2524&#34;&gt;Girshick et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Pure Caffe implementation of R-CNN for image classification. This model uses localization of regions to classify and extract features from images.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/densenet-121&#34;&gt;DenseNet-121&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1608.06993&#34;&gt;Huang et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Model that has every layer connected to every other layer and passes on its own feature providing strong gradient flow and more diversified features.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/DenseNet-121&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/inception_and_googlenet/inception_v1&#34;&gt;Inception_V1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1409.4842&#34;&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This model is same as GoogLeNet, implemented through Caffe2 that has improved utilization of the computing resources inside the network and helps with the vanishing gradient problem. &lt;br&gt; Top-5 error from paper - ~6.7%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/Inception_v1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/inception_and_googlenet/inception_v2&#34;&gt;Inception_V2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.00567&#34;&gt;Szegedy et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deep CNN model for Image Classification as an adaptation to Inception v1 with batch normalization. This model has reduced computational cost and improved image resolution compared to Inception v1. &lt;br&gt; Top-5 error from paper ~4.82%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/shufflenet&#34;&gt;ShuffleNet_V1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1707.01083&#34;&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Extremely computation efficient CNN model that is designed specifically for mobile devices. This model greatly reduces the computational cost and provides a ~13x speedup over AlexNet on ARM-based mobile devices. Compared to MobileNet, ShuffleNet achieves superior performance by a significant margin due to it&#39;s efficient structure. &lt;br&gt; Top-1 error from paper - ~32.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/shufflenet&#34;&gt;ShuffleNet_V2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1807.11164&#34;&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Extremely computation efficient CNN model that is designed specifically for mobile devices. This network architecture design considers direct metric such as speed, instead of indirect metric like FLOP. &lt;br&gt; Top-1 error from paper - ~30.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/zfnet-512&#34;&gt;ZFNet-512&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1311.2901&#34;&gt;Zeiler et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deep CNN model (up to 8 layers) that increased the number of features that the network is capable of detecting that helps to pick image features at a finer level of resolution. &lt;br&gt; Top-5 error from paper - ~14.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/ZFNet-512&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/efficientnet-lite4&#34;&gt;EfficientNet-Lite4&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1905.11946&#34;&gt;Tan et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CNN model with an order of magnitude of few computations and parameters, while still acheiving state-of-the-art accuracy and better efficiency than previous ConvNets. &lt;br&gt; Top-5 error from paper - ~2.9%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/EfficientNet-Lite4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Domain-based Image Classification &lt;a name=&#34;domain_based_image&#34;&gt;&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This subset of models classify images for specific domains and datasets.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/classification/mnist&#34;&gt;MNIST-Handwritten Digit Recognition&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Microsoft/CNTK/raw/master/Tutorials/CNTK_103D_MNIST_ConvolutionalNeuralNetwork.ipynb&#34;&gt;Convolutional Neural Network with MNIST&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deep CNN model for handwritten digit identification&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Object Detection &amp;amp; Image Segmentation &lt;a name=&#34;object_detection&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Object detection models detect the presence of multiple objects in an image and segment out areas of the image where the objects are detected. Semantic segmentation models partition an input image by labeling each pixel into a set of pre-defined categories.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/tiny-yolov2&#34;&gt;Tiny YOLOv2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.08242.pdf&#34;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A real-time CNN for object detection that detects 20 different classes. A smaller version of the more complex full YOLOv2 network.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/ssd&#34;&gt;SSD&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1512.02325&#34;&gt;Liu et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Single Stage Detector: real-time CNN for object detection that detects 80 different classes.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/ssd-mobilenetv1&#34;&gt;SSD-MobileNetV1&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34;&gt;Howard et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A variant of MobileNet that uses the Single Shot Detector (SSD) model framework. The model detects 80 different object classes and locates up to 10 objects in an image.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/faster-rcnn&#34;&gt;Faster-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34;&gt;Ren et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Increases efficiency from R-CNN by connecting a RPN with a CNN to create a single, unified network for object detection that detects 80 different classes.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/faster-rcnn&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/mask-rcnn&#34;&gt;Mask-RCNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.06870&#34;&gt;He et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A real-time neural network for object instance segmentation that detects 80 different classes. Extends Faster R-CNN as each of the 300 elected ROIs go through 3 parallel branches of the network: label prediction, bounding box prediction and mask prediction.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/mask-rcnn&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/retinanet&#34;&gt;RetinaNet&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34;&gt;Lin et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A real-time dense detector network for object detection that addresses class imbalance through Focal Loss. RetinaNet is able to match the speed of previous one-stage detectors and defines the state-of-the-art in two-stage detectors (surpassing R-CNN).&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/yolov2-coco&#34;&gt;YOLO v2-coco&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1612.08242&#34;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A CNN model for real-time object detection system that can detect over 9000 object categories. It uses a single network evaluation, enabling it to be more than 1000x faster than R-CNN and 100x faster than Faster R-CNN. This model is trained with COCO dataset and contains 80 classes.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/yolov3&#34;&gt;YOLO v3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1804.02767.pdf&#34;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A deep CNN model for real-time object detection that detects 80 different classes. A little bigger than YOLOv2 but still very fast. As accurate as SSD but 3 times faster.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/tiny-yolov3&#34;&gt;Tiny YOLOv3&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1804.02767.pdf&#34;&gt;Redmon et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A smaller version of YOLOv3 model.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/yolov4&#34;&gt;YOLOv4&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.10934&#34;&gt;Bochkovskiy et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Optimizes the speed and accuracy of object detection. Two times faster than EfficientDet. It improves YOLOv3&#39;s AP and FPS by 10% and 12%, respectively, with mAP50 of 52.32 on the COCO 2017 dataset and FPS of 41.7 on a Tesla V100.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/yolov4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/duc&#34;&gt;DUC&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1702.08502&#34;&gt;Wang et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deep CNN based pixel-wise semantic segmentation model with &amp;gt;80% &lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/models/semantic_segmentation/DUC/README.md/#metric&#34;&gt;mIOU&lt;/a&gt; (mean Intersection Over Union). Trained on cityscapes dataset, which can be effectively implemented in self driving vehicle systems.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/DUC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/object_detection_segmentation/fcn&#34;&gt;FCN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&#34;&gt;Long et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deep CNN based segmentation model trained end-to-end, pixel-to-pixel that produces efficient inference and learning. Built off of AlexNet, VGG net, GoogLeNet classification methods. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/FCN&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Body, Face &amp;amp; Gesture Analysis &lt;a name=&#34;body_analysis&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Face detection models identify and/or recognize human faces and emotions in given images. Body and Gesture Analysis models identify gender and age in given image.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/body_analysis/arcface&#34;&gt;ArcFace&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.07698&#34;&gt;Deng et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A CNN based model for face recognition which learns discriminative features of faces and produces embeddings for input face images.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/ArcFace&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/body_analysis/ultraface&#34;&gt;UltraFace&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB&#34;&gt;Ultra-lightweight face detection model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This model is a lightweight facedetection model designed for edge computing devices.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/ultraface&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/body_analysis/emotion_ferplus&#34;&gt;Emotion FerPlus&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1608.01041&#34;&gt;Barsoum et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deep CNN for emotion recognition trained on images of faces.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/body_analysis/age_gender&#34;&gt;Age and Gender Classification using Convolutional Neural Networks&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://data.vision.ee.ethz.ch/cvl/publications/papers/proceedings/eth_biwi_01229.pdf&#34;&gt;Rothe et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This model accurately classifies gender and age even the amount of learning data is limited.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Image Manipulation &lt;a name=&#34;image_manipulation&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Image manipulation models use neural networks to transform input images to modified output images. Some popular models in this category involve style transfer or enhancing images by increasing resolution.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unpaired Image to Image Translation using Cycle consistent Adversarial Network&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1703.10593&#34;&gt;Zhu et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The model uses learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/super_resolution/sub_pixel_cnn_2016&#34;&gt;Super Resolution with sub-pixel CNN&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.05158&#34;&gt;Shi et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A deep CNN that uses sub-pixel convolution layers to upscale the input image.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/sub_pixel_cnn_2016&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/vision/style_transfer/fast_neural_style&#34;&gt;Fast Neural Style Transfer&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1603.08155&#34;&gt;Johnson et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This method uses a loss network pretrained for image classification to define perceptual loss functions that measure perceptual differences in content and style between images. The loss network remains fixed during the training process.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Speech &amp;amp; Audio Processing &lt;a name=&#34;speech&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This class of models uses audio data to train models that can identify voice, generate music, or even read text out loud.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Speech recognition with deep recurrent neural networks&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cs.toronto.edu/~fritz/absps/RNN13.pdf&#34;&gt;Graves et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A RNN model for sequential data for speech recognition. Labels problems where the input-output alignment is unknown&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deep voice: Real time neural text to speech&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1702.07825&#34;&gt;Arik et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A DNN model that performs end-to-end neural speech synthesis. Requires fewer parameters and it is faster than other systems. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sound Generative models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.03499&#34;&gt;WaveNet: A Generative Model for Raw Audio &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A CNN model that generates raw audio waveforms. Has predictive distribution for each audio sample. Generates realistic music fragments. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Machine Comprehension &lt;a name=&#34;machine_comprehension&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This subset of natural language processing models that answer questions about a given context paragraph.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Hugging Face Spaces&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/text/machine_comprehension/bidirectional_attention_flow&#34;&gt;Bidirectional Attention Flow&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1611.01603&#34;&gt;Seo et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model that answers a query about a given context paragraph.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/BiDAF&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/text/machine_comprehension/bert-squad&#34;&gt;BERT-Squad&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1810.04805.pdf&#34;&gt;Devlin et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This model answers questions based on the context of the given input paragraph.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/BERT-Squad&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/text/machine_comprehension/roberta&#34;&gt;RoBERTa&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1907.11692.pdf&#34;&gt;Liu et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A large transformer-based model that predicts sentiment based on given input text.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/RoBERTa&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/text/machine_comprehension/gpt-2&#34;&gt;GPT-2&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;Radford et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A large transformer-based language model that given a sequence of words within some text, predicts the next word.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/GPT-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/text/machine_comprehension/t5&#34;&gt;T5&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.10683&#34;&gt;Raffel et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A large transformer-based language model trained on multiple tasks at once to achieve better semantic understanding of the prompt, capable of sentiment-analysis, question-answering, similarity-detection, translation, summarization, etc.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/onnx/T5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Machine Translation &lt;a name=&#34;machine_translation&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This class of natural language processing models learns how to translate input text to another language.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Neural Machine Translation by jointly learning to align and translate&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Bahdanau et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Aims to build a single neural network that can be jointly tuned to maximize the translation performance. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google&#39;s Neural Machine Translation System&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1609.08144&#34;&gt;Wu et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This model helps to improve issues faced by the Neural Machine Translation (NMT) systems like parallelism that helps accelerate the final translation speed.&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Language Modelling &lt;a name=&#34;language_modelling&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This subset of natural language processing models learns representations of language from large corpuses of text.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deep Neural Network Language Models&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pdfs.semanticscholar.org/a177/45f1d7045636577bcd5d513620df5860e9e5.pdf&#34;&gt;Arisoy et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A DNN acoustic model. Used in many natural language technologies. Represents a probability distribution over all possible word strings in a language. &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Visual Question Answering &amp;amp; Dialog &lt;a name=&#34;visual_qna&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This subset of natural language processing models uses input images to answer questions about those images.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VQA: Visual Question Answering&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1505.00468v6.pdf&#34;&gt;Agrawal et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model that takes an image and a free-form, open-ended natural language question about the image and outputs a natural-language answer. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Yin and Yang: Balancing and Answering Binary Visual Questions&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1511.05099.pdf&#34;&gt;Zhang et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Addresses VQA by converting the question to a tuple that concisely summarizes the visual concept to be detected in the image. Next, if the concept can be found in the image, it provides a “yes” or “no” answer. Its performance matches the traditional VQA approach on unbalanced dataset, and outperforms it on the balanced dataset. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Making the V in VQA Matter&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1612.00837.pdf&#34;&gt;Goyal et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Balances the VQA dataset by collecting complementary images such that every question is associated with a pair of similar images that result in two different answers to the question, providing a unique interpretable model that provides a counter-example based explanation. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Visual Dialog&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1611.08669&#34;&gt;Das et al.&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;An AI agent that holds a meaningful dialog with humans in natural, conversational language about visual content. Curates a large-scale Visual Dialog dataset (VisDial). &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Other interesting models &lt;a name=&#34;others&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;There are many interesting deep learning models that do not fit into the categories described above. The ONNX team would like to highly encourage users and researchers to &lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt; their models to the growing model zoo.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Class&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Text to Image&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1605.05396&#34;&gt;Generative Adversarial Text to image Synthesis &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Effectively bridges the advances in text and image modeling, translating visual concepts from characters to pixels. Generates plausible images of birds and flowers from detailed text descriptions. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Time Series Forecasting&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.07015.pdf&#34;&gt;Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The model extracts short-term local dependency patterns among variables and to discover long-term patterns for time series trends. It helps to predict solar plant energy output, electricity consumption, and traffic jam situations. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Recommender systems&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf&#34;&gt;DropoutNet: Addressing Cold Start in Recommender Systems&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A collaborative filtering method that makes predictions about an individual’s preference based on preference information from other users.&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Collaborative filtering&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1708.05031.pdf&#34;&gt;Neural Collaborative Filtering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A DNN model based on the interaction between user and item features using matrix factorization. &lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Autoencoders&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1506.01057&#34;&gt;A Hierarchical Neural Autoencoder for Paragraphs and Documents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;An LSTM (long-short term memory) auto-encoder to preserve and reconstruct multi-sentence paragraphs.&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Usage &lt;a name=&#34;usage-&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Every ONNX backend should support running the models out of the box. After downloading and extracting the tarball of each model, you will find:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A protobuf file &lt;code&gt;model.onnx&lt;/code&gt; that represents the serialized ONNX model.&lt;/li&gt; &#xA; &lt;li&gt;Test data (in the form of serialized protobuf TensorProto files or serialized NumPy archives).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Usage - Test data starter code&lt;/h3&gt; &#xA;&lt;p&gt;The test data files can be used to validate ONNX models from the Model Zoo. We have provided the following interface examples for you to get started. Please replace &lt;code&gt;onnx_backend&lt;/code&gt; in your code with the appropriate framework of your choice that provides ONNX inferencing support, and likewise replace &lt;code&gt;backend.run_model&lt;/code&gt; with the framework&#39;s model evaluation logic.&lt;/p&gt; &#xA;&lt;p&gt;There are two different formats for the test data files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Serialized protobuf TensorProtos (.pb), stored in folders with the naming convention &lt;code&gt;test_data_set_*&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import onnx&#xA;import os&#xA;import glob&#xA;import onnx_backend as backend&#xA;&#xA;from onnx import numpy_helper&#xA;&#xA;model = onnx.load(&#39;model.onnx&#39;)&#xA;test_data_dir = &#39;test_data_set_0&#39;&#xA;&#xA;# Load inputs&#xA;inputs = []&#xA;inputs_num = len(glob.glob(os.path.join(test_data_dir, &#39;input_*.pb&#39;)))&#xA;for i in range(inputs_num):&#xA;    input_file = os.path.join(test_data_dir, &#39;input_{}.pb&#39;.format(i))&#xA;    tensor = onnx.TensorProto()&#xA;    with open(input_file, &#39;rb&#39;) as f:&#xA;        tensor.ParseFromString(f.read())&#xA;    inputs.append(numpy_helper.to_array(tensor))&#xA;&#xA;# Load reference outputs&#xA;ref_outputs = []&#xA;ref_outputs_num = len(glob.glob(os.path.join(test_data_dir, &#39;output_*.pb&#39;)))&#xA;for i in range(ref_outputs_num):&#xA;    output_file = os.path.join(test_data_dir, &#39;output_{}.pb&#39;.format(i))&#xA;    tensor = onnx.TensorProto()&#xA;    with open(output_file, &#39;rb&#39;) as f:&#xA;        tensor.ParseFromString(f.read())&#xA;    ref_outputs.append(numpy_helper.to_array(tensor))&#xA;&#xA;# Run the model on the backend&#xA;outputs = list(backend.run_model(model, inputs))&#xA;&#xA;# Compare the results with reference outputs.&#xA;for ref_o, o in zip(ref_outputs, outputs):&#xA;    np.testing.assert_almost_equal(ref_o, o)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Serialized Numpy archives, stored in files with the naming convention &lt;code&gt;test_data_*.npz&lt;/code&gt;. Each file contains one set of test inputs and outputs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import onnx&#xA;import onnx_backend as backend&#xA;&#xA;# Load the model and sample inputs and outputs&#xA;model = onnx.load(model_pb_path)&#xA;sample = np.load(npz_path, encoding=&#39;bytes&#39;)&#xA;inputs = list(sample[&#39;inputs&#39;])&#xA;outputs = list(sample[&#39;outputs&#39;])&#xA;&#xA;# Run the model with an onnx backend and verify the results&#xA;np.testing.assert_almost_equal(outputs, backend.run_model(model, inputs))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage - Model quantization&lt;/h3&gt; &#xA;&lt;p&gt;You can get quantized ONNX models by using &lt;a href=&#34;https://github.com/intel/neural-compressor&#34;&gt;Intel® Neural Compressor&lt;/a&gt;. It provides web-based UI service to make quantization easier and supports code-based usage for more abundant quantization settings. Refer to &lt;a href=&#34;https://github.com/intel/neural-compressor/raw/master/docs/bench.md&#34;&gt;bench document&lt;/a&gt; for how to use web-based UI service and &lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/resource/docs/INC_code.md&#34;&gt;example document&lt;/a&gt; for a simple code-based demo. &lt;img src=&#34;https://raw.githubusercontent.com/onnx/models/main/resource/images/INC_GUI.gif&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Usage - Git LFS &lt;a name=&#34;gitlfs-&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;On default, cloning this repository will not download any ONNX models. Install Git LFS with &lt;code&gt;pip install git-lfs&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To download a specific model: &lt;code&gt;git lfs pull --include=&#34;[path to model].onnx&#34; --exclude=&#34;&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To download all models: &lt;code&gt;git lfs pull --include=&#34;*&#34; --exclude=&#34;&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Usage - Model visualization&lt;/h3&gt; &#xA;&lt;p&gt;You can see visualizations of each model&#39;s network architecture by using &lt;a href=&#34;https://github.com/lutzroeder/Netron&#34;&gt;Netron&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Do you want to contribute a model? To get started, pick any model presented above with the &lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/contribute.md&#34;&gt;contribute&lt;/a&gt; link under the Description column. The links point to a page containing guidelines for making a contribution.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/onnx/models/main/LICENSE&#34;&gt;Apache License v2.0&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/Data-Science-For-Beginners</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/microsoft/Data-Science-For-Beginners</id>
    <link href="https://github.com/microsoft/Data-Science-For-Beginners" rel="alternate"></link>
    <summary type="html">&lt;p&gt;10 Weeks, 20 Lessons, Data Science for All!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Data Science for Beginners - A Curriculum&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/graphs/contributors/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/issues/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/pulls/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/microsoft/Data-Science-For-Beginners.svg?sanitize=true&#34; alt=&#34;GitHub pull-requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/watchers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/watchers/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Watch&#34; alt=&#34;GitHub watchers&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Fork&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/microsoft/Data-Science-For-Beginners/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/Data-Science-For-Beginners.svg?style=social&amp;amp;label=Star&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Azure Cloud Advocates at Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. Each lesson includes pre-lesson and post-lesson quizzes, written instructions to complete the lesson, a solution, and an assignment. Our project-based pedagogy allows you to learn while building, a proven way for new skills to &#39;stick&#39;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hearty thanks to our authors:&lt;/strong&gt; &lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine Greenaway&lt;/a&gt;, &lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry Soshnikov&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya Narasimhan&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen McGee&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen Looper&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud Levy&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany Souterre&lt;/a&gt;, &lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher Harrison&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;🙏 Special thanks 🙏 to our &lt;a href=&#34;https://studentambassadors.microsoft.com/&#34;&gt;Microsoft Student Ambassador&lt;/a&gt; authors, reviewers and content contributors,&lt;/strong&gt; notably Aaryan Arora, &lt;a href=&#34;https://github.com/AdityaGarg00&#34;&gt;Aditya Garg&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/alondra-sanchez-molina/&#34;&gt;Alondra Sanchez&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/ankitasingh007&#34;&gt;Ankita Singh&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/anupam--mishra/&#34;&gt;Anupam Mishra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/arpitadas01/&#34;&gt;Arpita Das&lt;/a&gt;, ChhailBihari Dubey, &lt;a href=&#34;https://www.linkedin.com/in/dibrinsofor&#34;&gt;Dibri Nsofor&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/dishita-bhasin-7065281bb&#34;&gt;Dishita Bhasin&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/majd-s/&#34;&gt;Majd Safi&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/max-blum-6036a1186/&#34;&gt;Max Blum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/miguelmque/&#34;&gt;Miguel Correa&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/iftu119&#34;&gt;Mohamma Iftekher (Iftu) Ebne Jalal&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/nawrin-tabassum&#34;&gt;Nawrin Tabassum&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/raymond-wp/&#34;&gt;Raymond Wangsa Putra&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/rty2423&#34;&gt;Rohit Yadav&lt;/a&gt;, Samridhi Sharma, &lt;a href=&#34;https://www.linkedin.com/mwlite/in/sanya-sinha-13aab1200&#34;&gt;Sanya Sinha&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/sheena-narua-n/&#34;&gt;Sheena Narula&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/tauqeerahmad5201/&#34;&gt;Tauqeer Ahmad&lt;/a&gt;, Yogendrasingh Pawar , &lt;a href=&#34;https://www.linkedin.com/in/vidushi-gupta07/&#34;&gt;Vidushi Gupta&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/jasleen-sondhi/&#34;&gt;Jasleen Sondhi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Title.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Teachers&lt;/strong&gt;: we have &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/for-teachers.md&#34;&gt;included some suggestions&lt;/a&gt; on how to use this curriculum. We&#39;d love your feedback &lt;a href=&#34;https://github.com/microsoft/Data-Science-For-Beginners/discussions&#34;&gt;in our discussion forum&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://aka.ms/student-page&#34;&gt;Students&lt;/a&gt;&lt;/strong&gt;: to use this curriculum on your own, fork the entire repo and complete the exercises on your own, starting with a pre-lecture quiz. Then read the lecture and complete the rest of the activities. Try to create the projects by comprehending the lessons rather than copying the solution code; however, that code is available in the /solutions folders in each project-oriented lesson. Another idea would be to form a study group with friends and go through the content together. For further study, we recommend &lt;a href=&#34;https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/qprpajyoy3x0g7?WT.mc_id=academic-40229-cxa&#34;&gt;Microsoft Learn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Meet the Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/8mzavjQSMM4&#34; title=&#34;Promo video&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/ds-for-beginners.gif&#34; alt=&#34;Promo video&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gif by&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/mohitjaisal&#34;&gt;Mohit Jaisal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🎥 Click the image above for a video about the project the folks who created it!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Pedagogy&lt;/h2&gt; &#xA;&lt;p&gt;We have chosen two pedagogical tenets while building this curriculum: ensuring that it is project-based and that it includes frequent quizzes. By the end of this series, students will have learned basic principles of data science, including ethical concepts, data preparation, different ways of working with data, data visualization, data analysis, real-world use cases of data science, and more.&lt;/p&gt; &#xA;&lt;p&gt;In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 10 week cycle.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Find our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translation&lt;/a&gt; guidelines. We welcome your constructive feedback!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Each lesson includes:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Optional sketchnote&lt;/li&gt; &#xA; &lt;li&gt;Optional supplemental video&lt;/li&gt; &#xA; &lt;li&gt;Pre-lesson warmup quiz&lt;/li&gt; &#xA; &lt;li&gt;Written lesson&lt;/li&gt; &#xA; &lt;li&gt;For project-based lessons, step-by-step guides on how to build the project&lt;/li&gt; &#xA; &lt;li&gt;Knowledge checks&lt;/li&gt; &#xA; &lt;li&gt;A challenge&lt;/li&gt; &#xA; &lt;li&gt;Supplemental reading&lt;/li&gt; &#xA; &lt;li&gt;Assignment&lt;/li&gt; &#xA; &lt;li&gt;Post-lesson quiz&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;A note about quizzes&lt;/strong&gt;: All quizzes are contained &lt;a href=&#34;https://red-water-0103e7a0f.azurestaticapps.net/&#34;&gt;in this app&lt;/a&gt;, for 40 total quizzes of three questions each. They are linked from within the lessons, but the quiz app can be run locally; follow the instruction in the &lt;code&gt;quiz-app&lt;/code&gt; folder. They are gradually being localized.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Lessons&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/sketchnotes/00-Roadmap.png&#34; alt=&#34; Sketchnote by (@sketchthedocs) &#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science For Beginners: Roadmap - &lt;em&gt;Sketchnote by &lt;a href=&#34;https://twitter.com/nitya&#34;&gt;@nitya&lt;/a&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Number&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Topic&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Lesson Grouping&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Learning Objectives&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Linked Lesson&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Author&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data Science&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn the basic concepts behind data science and how it’s related to artificial intelligence, machine learning, and big data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/01-defining-data-science/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/beZ7Mb_oz9I&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science Ethics&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Ethics Concepts, Challenges &amp;amp; Frameworks.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/02-ethics/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Defining Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;How data is classified and its common sources.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/03-defining-data/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to Statistics &amp;amp; Probability&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/README.md&#34;&gt;Introduction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The mathematical techniques of probability and statistics to understand data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/1-Introduction/04-stats-and-probability/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/Z5Zy85g4Yjw&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Relational Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to relational data and the basics of exploring and analyzing relational data with the Structured Query Language, also known as SQL (pronounced “see-quell”).&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/geektrainer&#34;&gt;Christopher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with NoSQL Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to non-relational data, its various types and the basics of exploring and analyzing document databases.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/06-non-relational/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Working with Python&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Basics of using Python for data exploration with libraries such as Pandas. Foundational understanding of Python programming is recommended.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/07-python/README.md&#34;&gt;lesson&lt;/a&gt; &lt;a href=&#34;https://youtu.be/dZjWOGbsN4Y&#34;&gt;video&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://soshnikov.com&#34;&gt;Dmitry&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Preparation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/README.md&#34;&gt;Working With Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Topics on data techniques for cleaning and transforming the data to handle challenges of missing, inaccurate, or incomplete data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/08-data-preparation/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Quantities&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Learn how to use Matplotlib to visualize bird data 🦆&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/09-visualization-quantities/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Distributions of Data&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing observations and trends within an interval.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/10-visualization-distributions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Proportions&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing discrete and grouped percentages.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/11-visualization-proportions/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing Relationships&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Visualizing connections and correlations between sets of data and their variables.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/12-visualization-relationships/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Meaningful Visualizations&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/README.md&#34;&gt;Data Visualization&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Techniques and guidance for making your visualizations valuable for effective problem solving and insights.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/3-Data-Visualization/13-meaningful-visualizations/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/jenlooper&#34;&gt;Jen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the Data Science lifecycle&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Introduction to the data science lifecycle and its first step of acquiring and extracting data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/14-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Analyzing&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on techniques to analyze data.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/15-analyzing/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/paladique&#34;&gt;Jasmine&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Communication&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/README.md&#34;&gt;Lifecycle&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This phase of the data science lifecycle focuses on presenting the insights from the data in a way that makes it easier for decision makers to understand.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/4-Data-Science-Lifecycle/16-communication/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/JalenMcG&#34;&gt;Jalen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;This series of lessons introduces data science in the cloud and its benefits.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/17-Introduction/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Training models using Low Code tools.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/18-Low-Code/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Cloud&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/README.md&#34;&gt;Cloud Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Deploying models with Azure Machine Learning Studio.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/5-Data-Science-In-Cloud/19-Azure/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/TiffanySouterre&#34;&gt;Tiffany&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/maudstweets&#34;&gt;Maud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data Science in the Wild&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/README.md&#34;&gt;In the Wild&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Data science driven projects in the real world.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/6-Data-Science-In-Wild/20-Real-World-Examples/README.md&#34;&gt;lesson&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://twitter.com/nitya&#34;&gt;Nitya&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Offline access&lt;/h2&gt; &#xA;&lt;p&gt;You can run this documentation offline by using &lt;a href=&#34;https://docsify.js.org/#/&#34;&gt;Docsify&lt;/a&gt;. Fork this repo, &lt;a href=&#34;https://docsify.js.org/#/quickstart&#34;&gt;install Docsify&lt;/a&gt; on your local machine, then in the root folder of this repo, type &lt;code&gt;docsify serve&lt;/code&gt;. The website will be served on port 3000 on your localhost: &lt;code&gt;localhost:3000&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note, notebooks will not be rendered via Docsify, so when you need to run a notebook, do that separately in VS Code running a Python kernel.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;PDF&lt;/h2&gt; &#xA;&lt;p&gt;A PDF of all of the lessons can be found &lt;a href=&#34;https://microsoft.github.io/Data-Science-For-Beginners/pdf/readme.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Help Wanted!&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to translate all or part of the curriculum, please follow our &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/Data-Science-For-Beginners/main/TRANSLATIONS.md&#34;&gt;Translations&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;Other Curricula&lt;/h2&gt; &#xA;&lt;p&gt;Our team produces other curricula! Check out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ml-beginners&#34;&gt;Machine Learning for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/iot-beginners&#34;&gt;IoT for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/webdev-beginners&#34;&gt;Web Dev for Beginners&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aka.ms/ai-beginners&#34;&gt;AI for Beginners&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mszell/geospatialdatascience</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/mszell/geospatialdatascience</id>
    <link href="https://github.com/mszell/geospatialdatascience" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course materials for: Geospatial Data Science&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Course materials for: Geospatial Data Science&lt;/h1&gt; &#xA;&lt;p&gt;These course materials cover the lectures for the course held for the first time in spring 2022 at IT University of Copenhagen. Public course page: &lt;a href=&#34;https://learnit.itu.dk/local/coursebase/view.php?ciid=940&#34;&gt;https://learnit.itu.dk/local/coursebase/view.php?ciid=940&lt;/a&gt;&lt;br&gt; Materials were slightly improved and reordered after the course.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: Basics in data science (including statistics, Python and pandas)&lt;br&gt; &lt;strong&gt;Ideal level/program&lt;/strong&gt;: 1st year Master in Data Science&lt;/p&gt; &#xA;&lt;h2&gt;Topics&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mszell/geospatialdatascience/main/docs/images/topics.png&#34; alt=&#34;alt text&#34; title=&#34;Topics&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;· 1. Geometric objects · 2. Geospatial data in Python · 3. Choropleth mapping · 4. Spatial weights · 5. Spatial autocorrelation · 6. Spatial clustering · 7. Point pattern analysis · 8. OpenStreetMap and OSMnx · 9. Spatial networks · 10. Bicycle networks · 11. Individual mobility · 12. Mobility patterns · 13. Aggregate mobility and urban scaling · 14. Sustainable mobility and geospatial epidemiology ·&lt;/p&gt; &#xA;&lt;h2&gt;Exercise materials and tutorials&lt;/h2&gt; &#xA;&lt;p&gt;See: &lt;a href=&#34;https://github.com/anerv/GDS2022_exercises&#34;&gt;https://github.com/anerv/GDS2022_exercises&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Schedule&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mszell/geospatialdatascience/main/docs/images/courseschedule.png&#34; alt=&#34;alt text&#34; title=&#34;Course Schedule&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sources&lt;/h2&gt; &#xA;&lt;p&gt;The course materials were adapted/inspired from a number of sources, &lt;em&gt;standing on the shoulders of giants&lt;/em&gt;, ordered by appearance in the course:&lt;/p&gt; &#xA;&lt;h3&gt;Main sources&lt;/h3&gt; &#xA;&lt;p&gt;Percentages are approximative.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[7%] Tenkanen, Heikinheimo, Aagesen: &lt;a href=&#34;https://autogis-site.readthedocs.io/en/latest/&#34;&gt;Automating GIS-processes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[40%] Arribas-Bel: &lt;a href=&#34;https://darribas.org/gds_course/content/home.html&#34;&gt;Geographic Data Science&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[7%] Boeing: &lt;a href=&#34;https://github.com/gboeing/osmnx-examples/tree/main/notebooks&#34;&gt;OSMnx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[1%] Tenkanen: &lt;a href=&#34;https://pyrosm.readthedocs.io/en/latest/index.html&#34;&gt;pyrosm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2%] Gaboardo, Rey, Lumnitz: &lt;a href=&#34;https://pysal.org/spaghetti/&#34;&gt;spaghetti&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2%] Pappalardo: &lt;a href=&#34;https://github.com/scikit-mobility/tutorials&#34;&gt;scikit-mobility&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other major sources and further materials&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rey, Arribas-Bel, Wolf: &lt;a href=&#34;https://geographicdata.science/book/intro.html&#34;&gt;Geographic Data Science with Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prapas: &lt;a href=&#34;https://www.learndatasci.com/tutorials/geospatial-data-python-geopandas-shapely/&#34;&gt;Analyze Geospatial Data in Python: GeoPandas and Shapely&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gimond: &lt;a href=&#34;https://mgimond.github.io/Spatial/index.html&#34;&gt;Intro to GIS and Spatial Analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tan, Steinbach, Kumar: Introduction to Data Mining&lt;/li&gt; &#xA; &lt;li&gt;Timaite, Lovelace: &lt;a href=&#34;https://udsleeds.github.io/openinfra/articles/openinfra.html&#34;&gt;Getting started with open data on transport infrastructure&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Rodrigue: &lt;a href=&#34;https://transportgeography.org/&#34;&gt;The Geography of Transport Systems&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Barthelemy: &lt;a href=&#34;https://link.springer.com/book/10.1007/978-3-030-94106-2&#34;&gt;Spatial Networks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Barbosa et al: &lt;a href=&#34;https://doi.org/10.1016/j.physrep.2018.01.001&#34;&gt;Human mobility: Models and applications&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Mobility papers: &lt;a href=&#34;https://www.nature.com/articles/nature04292&#34;&gt;Brockmann et al&lt;/a&gt;, &lt;a href=&#34;https://www.nature.com/articles/nature06958&#34;&gt;Gonzalez et al&lt;/a&gt;, &lt;a href=&#34;https://www.nature.com/articles/srep00457&#34;&gt;Szell et al&lt;/a&gt;, &lt;a href=&#34;https://www.science.org/doi/abs/10.1126/science.1177170&#34;&gt;Song et al&lt;/a&gt;, &lt;a href=&#34;https://www.nature.com/articles/ncomms9166&#34;&gt;Pappalardo et al&lt;/a&gt;, &lt;a href=&#34;https://www.nature.com/articles/nphys1760&#34;&gt;Song et al&lt;/a&gt;, &lt;a href=&#34;https://www.nature.com/articles/srep01376&#34;&gt;De Montjoye et al&lt;/a&gt;, &lt;a href=&#34;https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2013.0246&#34;&gt;Schneider et al&lt;/a&gt;, &lt;a href=&#34;https://www.pnas.org/content/113/36/9977.short&#34;&gt;Sekara et al&lt;/a&gt;, &lt;a href=&#34;https://www.nature.com/articles/nature10856&#34;&gt;Simini et al&lt;/a&gt;, &lt;a href=&#34;https://www.science.org/doi/10.1126/science.1245200&#34;&gt;Brockmann &amp;amp; Helbing&lt;/a&gt;, &lt;a href=&#34;https://www.nature.com/articles/s41598-022-10783-y&#34;&gt;Szell et al&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Kapp: &lt;a href=&#34;https://alexandrakapp.blog/2022/03/14/privacy-preserving-techniques-and-how-they-apply-to-mobility-data/&#34;&gt;Privacy-preserving techniques and how they apply to mobility data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Batty: &lt;a href=&#34;https://mitpress.mit.edu/books/new-science-cities&#34;&gt;The New Science of Cities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Barthelemy: &lt;a href=&#34;https://www.cambridge.org/core/books/structure-and-dynamics-of-cities/50359353B081D0A38928961FE16FB2FD&#34;&gt;The Structure and Dynamics of Cities&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tenkanen: &lt;a href=&#34;https://sustainability-gis.readthedocs.io/en/latest/index.html&#34;&gt;Spatial data science for sustainable development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;OECD: &lt;a href=&#34;https://www.oecd.org/environment/transport-strategies-for-net-zero-systems-by-design-0a20f779-en.htm&#34;&gt;Transport Strategies for Net-Zero Systems by Design&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/GDSL-UL/Teaching_Links&#34;&gt;The GDSL Big List of Teaching Links&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More sources are referenced within the slides and notebooks.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;All materials were used for educational, non-commercial reasons only. Feel free to use as you wish for the same purpose, at your own risk. For other re-use questions please consult the license of the respective source. Our main sources use the &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;CC BY-SA 4.0 license&lt;/a&gt; so we use it too.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Lectures: &lt;a href=&#34;http://michael.szell.net/&#34;&gt;Michael Szell&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/anerv/GDS2022_exercises&#34;&gt;Exercises and tutorials&lt;/a&gt;: Ane Rahbek Vierø &amp;amp; Anastassia Vybornova&lt;/p&gt; &#xA;&lt;p&gt;Thanks to all our main sources for being so helpful and open with your materials! Special thanks to Adéla Sobotkova for helpful discussions and materials concerning syllabus, exam form, and project description, and to Vedran Sekara for slide materials.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mrdbourke/zero-to-mastery-ml</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/mrdbourke/zero-to-mastery-ml</id>
    <link href="https://github.com/mrdbourke/zero-to-mastery-ml" rel="alternate"></link>
    <summary type="html">&lt;p&gt;All course materials for the Zero to Mastery Machine Learning and Data Science course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zero to Mastery Machine Learning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/mrdbourke/zero-to-mastery-ml/master&#34;&gt;&lt;img src=&#34;https://mybinder.org/badge_logo.svg?sanitize=true&#34; alt=&#34;Binder&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.deepnote.com/launch?template=data-science&amp;amp;url=https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/section-2-data-science-and-ml-tools/introduction-to-pandas.ipynb&#34;&gt;&lt;img src=&#34;https://deepnote.com/buttons/launch-in-deepnote.svg?sanitize=true&#34; alt=&#34;Deepnote&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/mrdbourke/zero-to-mastery-ml/blob/master&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome! This repository contains all of the code, notebooks, images and other materials related to the &lt;a href=&#34;https://dbourke.link/mlcourse&#34;&gt;Zero to Mastery Machine Learning Course on Udemy&lt;/a&gt; and &lt;a href=&#34;https://dbourke.link/ZTMmlcourse&#34;&gt;zerotomastery.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to see anything in particular, please send me an email: &lt;a href=&#34;mailto:daniel@mrdbourke.com&#34;&gt;daniel@mrdbourke.com&lt;/a&gt; or leave an issue.&lt;/p&gt; &#xA;&lt;h2&gt;What this course focuses on&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a framework for working through problems (&lt;a href=&#34;https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/section-1-getting-ready-for-machine-learning/a-6-step-framework-for-approaching-machine-learning-projects.md&#34;&gt;6 step machine learning modelling framework&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Find tools to fit the framework&lt;/li&gt; &#xA; &lt;li&gt;Targeted practice = use tools and framework steps to work on end-to-end machine learning modelling projects&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How this course is structured&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Section 1 - Getting your mind and computer ready for machine learning (concepts, computer setup)&lt;/li&gt; &#xA; &lt;li&gt;Section 2 - Tools for machine learning and data science (pandas, NumPy, Matplotlib, Scikit-Learn)&lt;/li&gt; &#xA; &lt;li&gt;Section 3 - End-to-end structured data projects (classification and regression)&lt;/li&gt; &#xA; &lt;li&gt;Section 4 - Neural networks, deep learning and transfer learning with TensorFlow 2.0&lt;/li&gt; &#xA; &lt;li&gt;Section 5 - Communicating and sharing your work&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Student notes&lt;/h2&gt; &#xA;&lt;p&gt;Some students have taken and shared extensive notes on this course, see them below.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to submit yours, leave a pull request.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Chester&#39;s notes - &lt;a href=&#34;https://github.com/chesterheng/machinelearning-datascience&#34;&gt;https://github.com/chesterheng/machinelearning-datascience&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sophia&#39;s notes - &lt;a href=&#34;https://www.rockyourcode.com/tags/udemy-complete-machine-learning-and-data-science-zero-to-mastery/&#34;&gt;https://www.rockyourcode.com/tags/udemy-complete-machine-learning-and-data-science-zero-to-mastery/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>wesm/pydata-book</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/wesm/pydata-book</id>
    <link href="https://github.com/wesm/pydata-book" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Materials and IPython notebooks for &#34;Python for Data Analysis&#34; by Wes McKinney, published by O&#39;Reilly Media&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Python for Data Analysis, 2nd Edition&lt;/h1&gt; &#xA;&lt;p&gt;Materials and IPython notebooks for &#34;Python for Data Analysis&#34; by Wes McKinney, published by O&#39;Reilly Media&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://amzn.to/2vvBijB&#34;&gt;Buy the book on Amazon&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://notebooks.azure.com/import/gh/wesm/pydata-book&#34;&gt;&lt;img src=&#34;https://notebooks.azure.com/launch.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Follow Wes on Twitter: &lt;a href=&#34;https://twitter.com/wesmckinn&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/wesmckinn.svg?style=social&amp;amp;label=Follow&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;1st Edition Readers&lt;/h1&gt; &#xA;&lt;p&gt;If you are reading the &lt;a href=&#34;http://amzn.to/2vvBijB&#34;&gt;1st Edition&lt;/a&gt; (published in 2012), please find the reorganized book materials on the &lt;a href=&#34;https://github.com/wesm/pydata-book/tree/1st-edition&#34;&gt;&lt;code&gt;1st-edition&lt;/code&gt; branch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Translations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/BrambleXu/pydata-notebook&#34;&gt;Chinese&lt;/a&gt; by Xu Liang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mbiesiad/pydata-book/tree/pl_PL&#34;&gt;Polish&lt;/a&gt; by Michal Biesiada&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;IPython Notebooks:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch02.ipynb&#34;&gt;Chapter 2: Python Language Basics, IPython, and Jupyter Notebooks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch03.ipynb&#34;&gt;Chapter 3: Built-in Data Structures, Functions, and Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch04.ipynb&#34;&gt;Chapter 4: NumPy Basics: Arrays and Vectorized Computation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch05.ipynb&#34;&gt;Chapter 5: Getting Started with pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch06.ipynb&#34;&gt;Chapter 6: Data Loading, Storage, and File Formats&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch07.ipynb&#34;&gt;Chapter 7: Data Cleaning and Preparation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch08.ipynb&#34;&gt;Chapter 8: Data Wrangling: Join, Combine, and Reshape&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch09.ipynb&#34;&gt;Chapter 9: Plotting and Visualization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch10.ipynb&#34;&gt;Chapter 10: Data Aggregation and Group Operations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch11.ipynb&#34;&gt;Chapter 11: Time Series&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch12.ipynb&#34;&gt;Chapter 12: Advanced pandas&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch13.ipynb&#34;&gt;Chapter 13: Introduction to Modeling Libraries in Python&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/ch14.ipynb&#34;&gt;Chapter 14: Data Analysis Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://nbviewer.ipython.org/github/pydata/pydata-book/blob/2nd-edition/appa.ipynb&#34;&gt;Appendix A: Advanced NumPy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;p&gt;The code in this repository, including all code samples in the notebooks listed above, is released under the &lt;a href=&#34;https://raw.githubusercontent.com/wesm/pydata-book/2nd-edition/LICENSE-CODE&#34;&gt;MIT license&lt;/a&gt;. Read more at the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;Open Source Initiative&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Azure/MachineLearningNotebooks</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/Azure/MachineLearningNotebooks</id>
    <link href="https://github.com/Azure/MachineLearningNotebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python notebooks with ML and deep learning examples with Azure Machine Learning Python SDK | Microsoft&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Azure Machine Learning Python SDK notebooks&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;a community-driven repository of examples using mlflow for tracking can be found at &lt;a href=&#34;https://github.com/Azure/azureml-examples&#34;&gt;https://github.com/Azure/azureml-examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Welcome to the Azure Machine Learning Python SDK notebooks repository!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;These notebooks are recommended for use in an Azure Machine Learning &lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning/concept-compute-instance&#34;&gt;Compute Instance&lt;/a&gt;, where you can run them without any additional set up.&lt;/p&gt; &#xA;&lt;p&gt;However, the notebooks can be run in any development environment with the correct &lt;code&gt;azureml&lt;/code&gt; packages installed.&lt;/p&gt; &#xA;&lt;p&gt;Install the &lt;code&gt;azureml.core&lt;/code&gt; Python package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install additional packages as needed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install azureml-mlflow&#xA;pip install azureml-dataset-runtime&#xA;pip install azureml-automl-runtime&#xA;pip install azureml-pipeline&#xA;pip install azureml-pipeline-steps&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend starting with one of the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/tutorials/compute-instance-quickstarts&#34;&gt;quickstarts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This repository is a push-only mirror. Pull requests are ignored.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/Azure/MachineLearningNotebooks/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.microsoft.com/azure/machine-learning&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>codebasics/py</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/codebasics/py</id>
    <link href="https://github.com/codebasics/py" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repository to store sample python programs for python learning&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;py&lt;/h1&gt; &#xA;&lt;p&gt;Repository to store sample Python programs.&lt;/p&gt; &#xA;&lt;p&gt;This repository is meant for beginners to assist them in their learning of Python. The repository covers a wide range of algorithms and other programs, and would prove immensely helpful for everybody interested in Python programming.&lt;/p&gt; &#xA;&lt;p&gt;If this is your first time coding in Python, I would love to suggest you begin from the &lt;a href=&#34;https://github.com/codebasics/py/tree/master/Basics&#34;&gt;Basics&lt;/a&gt;. They are simple to understand and hopefully will prove fun to you.&lt;/p&gt; &#xA;&lt;p&gt;You can also pay a visit to my very own &lt;a href=&#34;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&#34;&gt;Youtube channel&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Contributions to the repository are welcome.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ&#34;&gt;&lt;img src=&#34;https://yt3.ggpht.com/ytc/AAUvwnihwx4a5idwBTE5JFpXHb-ykyh-i1gXtFiGJYV1=s176-c-k-c0x00ffffff-no-rj&#34; alt=&#34;CodeBasics&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Happy coding!&lt;/h4&gt;</summary>
  </entry>
  <entry>
    <title>geohot/tinyvoice</title>
    <updated>2022-05-29T02:13:50Z</updated>
    <id>tag:github.com,2022-05-29:/geohot/tinyvoice</id>
    <link href="https://github.com/geohot/tinyvoice" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Letting computers listen to you and really care&lt;/p&gt;&lt;hr&gt;&lt;p&gt;A fun reimplementation of Speech to Text and Text to Speech&lt;/p&gt; &#xA;&lt;p&gt;Trained on LJ Speech&lt;/p&gt;</summary>
  </entry>
</feed>