<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-21T03:12:16Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>coreweave/cuda-ssh-server</title>
    <updated>2024-04-21T03:12:16Z</updated>
    <id>tag:github.com,2024-04-21:/coreweave/cuda-ssh-server</id>
    <link href="https://github.com/coreweave/cuda-ssh-server" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple Docker image with CUDA and an OpenSSH server&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>rh-aiservices-bu/llm-on-openshift</title>
    <updated>2024-04-21T03:12:16Z</updated>
    <id>tag:github.com,2024-04-21:/rh-aiservices-bu/llm-on-openshift</id>
    <link href="https://github.com/rh-aiservices-bu/llm-on-openshift" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Resources, demos, recipes,... to work with LLMs on OpenShift with OpenShift AI or Open Data Hub.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLM on OpenShift&lt;/h1&gt; &#xA;&lt;p&gt;In this repo you will find resources, demos, recipes... to work with LLMs on OpenShift with &lt;a href=&#34;https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-ai&#34;&gt;OpenShift AI&lt;/a&gt; or &lt;a href=&#34;https://opendatahub.io/&#34;&gt;Open Data Hub&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Content&lt;/h2&gt; &#xA;&lt;h3&gt;Inference Servers&lt;/h3&gt; &#xA;&lt;p&gt;The following &lt;strong&gt;Inference Servers&lt;/strong&gt; for LLMs can be deployed standalone on OpenShift:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/llm-servers/vllm/README.md&#34;&gt;vLLM&lt;/a&gt;: how to deploy &lt;a href=&#34;https://docs.vllm.ai/en/latest/index.html&#34;&gt;vLLM&lt;/a&gt;, the &#34;Easy, fast, and cheap LLM serving for everyone&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/llm-servers/hf_tgi/README.md&#34;&gt;Hugging Face TGI&lt;/a&gt;: how to deploy the &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;Text Generation Inference&lt;/a&gt; server from Hugging Face.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/opendatahub-io/caikit-tgis-serving&#34;&gt;Caikit-TGIS-Serving&lt;/a&gt; (external): how to deploy the Caikit-TGIS-Serving stack, from OpenDataHub.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/llm-servers/ollama/README.md&#34;&gt;Ollama&lt;/a&gt;: how to deploy &lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt; using CPU only for inference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Serving Runtimes deployment&lt;/h3&gt; &#xA;&lt;p&gt;The following &lt;strong&gt;Runtimes&lt;/strong&gt; can be imported in the Single-Model Serving stack of Open Data Hub or OpenShift AI.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/serving-runtimes/vllm_runtime/README.md&#34;&gt;vLLM Serving Runtime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/serving-runtimes/hf_tgi_runtime/README.md&#34;&gt;Hugging Face Text Generation Inference&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Vector Databases&lt;/h3&gt; &#xA;&lt;p&gt;The following &lt;strong&gt;Databases&lt;/strong&gt; can be used as a Vector Store for Retrieval Augmented Generation (RAG) applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/vector-databases/milvus/README.md&#34;&gt;Milvus&lt;/a&gt;: Full recipe to deploy the Milvus vector store, in standalone or cluster mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/vector-databases/pgvector/README.md&#34;&gt;PostgreSQL+pgvector&lt;/a&gt;: Full recipe to create an instance of PostgreSQL with the pgvector extension, making it usable as a vector store.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/vector-databases/redis/README.md&#34;&gt;Redis&lt;/a&gt;: Full recipe to deploy Redis, create a Cluster and a suitable Database for a Vector Store.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Inference and application examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/examples/notebooks/caikit-basic-query/README.md&#34;&gt;Caikit&lt;/a&gt;: Basic example demonstrating how to work with Caikit+TGIS for LLM serving.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/examples/notebooks/langchain/README.md&#34;&gt;Langchain examples&lt;/a&gt;: Various notebooks demonstrating how to work with &lt;a href=&#34;https://www.langchain.com/&#34;&gt;Langchain&lt;/a&gt;. Examples are provided for different types of LLM servers (standalone or using the Single-Model Serving stack of Open Data Hub or OpenShift AI) and different vector databases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/examples/langflow/README.md&#34;&gt;Langflow examples&lt;/a&gt;: Various examples demonstrating how to work with Langflow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rh-aiservices-bu/llm-on-openshift/main/examples/ui/README.md&#34;&gt;UI examples&lt;/a&gt;: Various examples on how to create and deploy a UI to interact with your LLM.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>