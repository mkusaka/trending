<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-29T01:29:45Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/Google-Cloud-Containers</title>
    <updated>2024-08-29T01:29:45Z</updated>
    <id>tag:github.com,2024-08-29:/huggingface/Google-Cloud-Containers</id>
    <link href="https://github.com/huggingface/Google-Cloud-Containers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Including Hugging Face Deep learning Containers for Google Cloud&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸ¤— Hugging Face Deep Learning Containers for Google Cloud&lt;/h1&gt; &#xA;&lt;img alt=&#34;Hugging Face x Google Cloud&#34; src=&#34;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/Google-Cloud-Containers/thumbnail.png&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face&#34;&gt;Hugging Face Deep Learning Containers for Google Cloud&lt;/a&gt; are a set of Docker images for training and deploying Transformers, Sentence Transformers, and Diffusers models on Google Cloud Vertex AI and Google Kubernetes Engine (GKE).&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/huggingface/Google-Cloud-Containers/tree/main&#34;&gt;Google-Cloud-Containers&lt;/a&gt; repository contains the container files for building Hugging Face-specific Deep Learning Containers (DLCs), examples on how to train and deploy models on Google Cloud. The containers are publicly maintained, updated and released periodically by Hugging Face and the Google Cloud Team and available for all Google Cloud Customers within the &lt;a href=&#34;https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face&#34;&gt;Google Cloud&#39;s Artifact Registry&lt;/a&gt;. For each supported combination of use-case (training, inference), accelerator type (CPU, GPU, TPU), and framework (PyTorch, TGI, TEI) containers are created. Those include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Training &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/pytorch/training/README.md&#34;&gt;PyTorch&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;GPU&lt;/li&gt; &#xA;     &lt;li&gt;TPU (soon)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Inference &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/pytorch/inference/README.md&#34;&gt;PyTorch&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;CPU&lt;/li&gt; &#xA;     &lt;li&gt;GPU&lt;/li&gt; &#xA;     &lt;li&gt;TPU (soon)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/tgi/README.md&#34;&gt;Text Generation Inference&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;GPU&lt;/li&gt; &#xA;     &lt;li&gt;TPU (soon)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/tei/README.md&#34;&gt;Text Embeddings Inference&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;CPU&lt;/li&gt; &#xA;     &lt;li&gt;GPU&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Published Containers&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Container URI&lt;/th&gt; &#xA;   &lt;th&gt;Path&lt;/th&gt; &#xA;   &lt;th&gt;Framework&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Accelerator&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/tgi/gpu/2.2.0/Dockerfile&#34;&gt;text-generation-inference-gpu.2.2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TGI&lt;/td&gt; &#xA;   &lt;td&gt;Inference&lt;/td&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-embeddings-inference-cu122.1-2.ubuntu2204&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/tei/gpu/1.2.0/Dockerfile&#34;&gt;text-embeddings-inference-gpu.1.2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TEI&lt;/td&gt; &#xA;   &lt;td&gt;Inference&lt;/td&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-embeddings-inference-cpu.1-2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/tei/cpu/1.2.0/Dockerfile&#34;&gt;text-embeddings-inference-cpu.1.2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TEI&lt;/td&gt; &#xA;   &lt;td&gt;Inference&lt;/td&gt; &#xA;   &lt;td&gt;CPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-training-cu121.2-3.transformers.4-42.ubuntu2204.py310&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/pytorch/training/gpu/2.3.0/transformers/4.42.3/py310/Dockerfile&#34;&gt;huggingface-pytorch-training-gpu.2.3.0.transformers.4.42.3.py310&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Training&lt;/td&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-inference-cu121.2-2.transformers.4-44.ubuntu2204.py311&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/pytorch/inference/gpu/2.2.2/transformers/4.44.0/py311/Dockerfile&#34;&gt;huggingface-pytorch-inference-gpu.2.2.2.transformers.4.44.0.py311&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Inference&lt;/td&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-pytorch-inference-cpu.2-2.transformers.4-44.ubuntu2204.py311&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/containers/pytorch/inference/cpu/2.2.2/transformers/4.44.0/py311/Dockerfile&#34;&gt;huggingface-pytorch-inference-cpu.2.2.2.transformers.4.44.0.py311&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch&lt;/td&gt; &#xA;   &lt;td&gt;Inference&lt;/td&gt; &#xA;   &lt;td&gt;CPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] The listing above only contains the latest version of each of the Hugging Face DLCs, the full listing of the available published containers in Google Cloud can be found either in the &lt;a href=&#34;https://cloud.google.com/deep-learning-containers/docs/choosing-container#hugging-face&#34;&gt;Deep Learning Containers Documentation&lt;/a&gt;, in the &lt;a href=&#34;https://console.cloud.google.com/artifacts/docker/deeplearning-platform-release/us/gcr.io&#34;&gt;Google Cloud Artifact Registry&lt;/a&gt; or via the &lt;code&gt;gcloud container images list --repository=&#34;us-docker.pkg.dev/deeplearning-platform-release/gcr.io&#34; | grep &#34;huggingface-&#34;&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory contains examples for using the containers on different scenarios, and digging deeper on some of the features of the containers offered within Google Cloud.&lt;/p&gt; &#xA;&lt;h3&gt;Training Examples&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Service&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GKE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/gke/trl-full-fine-tuning&#34;&gt;trl-full-fine-tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Full SFT fine-tuning of Gemma 2B in a multi-GPU instance with TRL on GKE.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GKE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/gke/trl-lora-fine-tuning&#34;&gt;trl-lora-fine-tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LoRA SFT fine-tuning of Mistral 7B v0.3 in a single GPU instance with TRL on GKE.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vertex AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/vertex-ai/notebooks/trl-full-sft-fine-tuning-on-vertex-ai&#34;&gt;trl-full-sft-fine-tuning-on-vertex-ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Full SFT fine-tuning of Mistral 7B v0.3 in a multi-GPU instance with TRL on Vertex AI.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vertex AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/vertex-ai/notebooks/trl-lora-sft-fine-tuning-on-vertex-ai&#34;&gt;trl-lora-sft-fine-tuning-on-vertex-ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LoRA SFT fine-tuning of Mistral 7B v0.3 in a single GPU instance with TRL on Vertex AI.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference Examples&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Service&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GKE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/gke/tgi-deployment&#34;&gt;tgi-deployment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deploying Llama3 8B with Text Generation Inference (TGI) on GKE.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GKE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/gke/tgi-from-gcs-deployment&#34;&gt;tgi-from-gcs-deployment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deploying Qwen2 7B Instruct with Text Generation Inference (TGI) from a GCS Bucket on GKE.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GKE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/gke/tei-deployment&#34;&gt;tei-deployment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deploying Snowflake&#39;s Arctic Embed (M) with Text Embeddings Inference (TEI) on GKE.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GKE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/gke/tei-from-gcs-deployment&#34;&gt;tei-from-gcs-deployment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deploying BGE Base v1.5 (English) with Text Embeddings Inference (TEI) from a GCS Bucket on GKE.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vertex AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/vertex-ai/notebooks/deploy-bert-on-vertex-ai&#34;&gt;deploy-bert-on-vertex-ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deploying a BERT model for a text classification task using &lt;code&gt;huggingface-inference-toolkit&lt;/code&gt; for a Custom Prediction Routine (CPR) on Vertex AI.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vertex AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/vertex-ai/notebooks/deploy-embedding-on-vertex-ai&#34;&gt;deploy-embedding-on-vertex-ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deploying an embedding model with Text Embeddings Inference (TEI) on Vertex AI.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vertex AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/vertex-ai/notebooks/deploy-gemma-on-vertex-ai&#34;&gt;deploy-gemma-on-vertex-ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deploying Gemma 7B Instruct with Text Generation Inference (TGI) on Vertex AI.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vertex AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/Google-Cloud-Containers/main/examples/vertex-ai/notebooks/deploy-gemma-from-gcs-on-vertex-ai&#34;&gt;deploy-gemma-from-gcs-on-vertex-ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Deploying Gemma 7B Instruct with Text Generation Inference (TGI) from a GCS Bucket on Vertex AI.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>