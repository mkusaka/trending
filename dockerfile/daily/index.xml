<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-11T01:34:41Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yewtudotbe/invidious-custom</title>
    <updated>2023-06-11T01:34:41Z</updated>
    <id>tag:github.com,2023-06-11:/yewtudotbe/invidious-custom</id>
    <link href="https://github.com/yewtudotbe/invidious-custom" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Invidious with some git patches (mostly from existing PRs)&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>Lfninja/test</title>
    <updated>2023-06-11T01:34:41Z</updated>
    <id>tag:github.com,2023-06-11:/Lfninja/test</id>
    <link href="https://github.com/Lfninja/test" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;test&lt;/h1&gt;</summary>
  </entry>
  <entry>
    <title>mvillarrealb/docker-spark-cluster</title>
    <updated>2023-06-11T01:34:41Z</updated>
    <id>tag:github.com,2023-06-11:/mvillarrealb/docker-spark-cluster</id>
    <link href="https://github.com/mvillarrealb/docker-spark-cluster" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple spark standalone cluster for your testing environment purposses&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Spark Cluster with Docker &amp;amp; docker-compose(2021 ver.)&lt;/h1&gt; &#xA;&lt;h1&gt;General&lt;/h1&gt; &#xA;&lt;p&gt;A simple spark standalone cluster for your testing environment purposses. A &lt;em&gt;docker-compose up&lt;/em&gt; away from you solution for your spark development environment.&lt;/p&gt; &#xA;&lt;p&gt;The Docker compose will create the following containers:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;container&lt;/th&gt; &#xA;   &lt;th&gt;Exposed ports&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;spark-master&lt;/td&gt; &#xA;   &lt;td&gt;9090 7077&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;spark-worker-1&lt;/td&gt; &#xA;   &lt;td&gt;9091&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;spark-worker-2&lt;/td&gt; &#xA;   &lt;td&gt;9092&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;demo-database&lt;/td&gt; &#xA;   &lt;td&gt;5432&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;The following steps will make you run your spark cluster&#39;s containers.&lt;/p&gt; &#xA;&lt;h2&gt;Pre requisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Docker installed&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Docker compose installed&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Build the image&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker build -t cluster-apache-spark:3.0.2 .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run the docker-compose&lt;/h2&gt; &#xA;&lt;p&gt;The final step to create your test cluster will be to run the compose file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Validate your cluster&lt;/h2&gt; &#xA;&lt;p&gt;Just validate your cluster accesing the spark UI on each worker &amp;amp; master URL.&lt;/p&gt; &#xA;&lt;h3&gt;Spark Master&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://localhost:9090/&#34;&gt;http://localhost:9090/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mvillarrealb/docker-spark-cluster/master/docs/spark-master.png&#34; alt=&#34;alt text&#34; title=&#34;Spark master UI&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Spark Worker 1&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://localhost:9091/&#34;&gt;http://localhost:9091/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mvillarrealb/docker-spark-cluster/master/docs/spark-worker-1.png&#34; alt=&#34;alt text&#34; title=&#34;Spark worker 1 UI&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Spark Worker 2&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://localhost:9092/&#34;&gt;http://localhost:9092/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mvillarrealb/docker-spark-cluster/master/docs/spark-worker-2.png&#34; alt=&#34;alt text&#34; title=&#34;Spark worker 2 UI&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Resource Allocation&lt;/h1&gt; &#xA;&lt;p&gt;This cluster is shipped with three workers and one spark master, each of these has a particular set of resource allocation(basically RAM &amp;amp; cpu cores allocation).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The default CPU cores allocation for each spark worker is 1 core.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The default RAM for each spark-worker is 1024 MB.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The default RAM allocation for spark executors is 256mb.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The default RAM allocation for spark driver is 128mb&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you wish to modify this allocations just edit the env/spark-worker.sh file.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Binded Volumes&lt;/h1&gt; &#xA;&lt;p&gt;To make app running easier I&#39;ve shipped two volume mounts described in the following chart:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Host Mount&lt;/th&gt; &#xA;   &lt;th&gt;Container Mount&lt;/th&gt; &#xA;   &lt;th&gt;Purposse&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;apps&lt;/td&gt; &#xA;   &lt;td&gt;/opt/spark-apps&lt;/td&gt; &#xA;   &lt;td&gt;Used to make available your app&#39;s jars on all workers &amp;amp; master&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;data&lt;/td&gt; &#xA;   &lt;td&gt;/opt/spark-data&lt;/td&gt; &#xA;   &lt;td&gt;Used to make available your app&#39;s data on all workers &amp;amp; master&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;This is basically a dummy DFS created from docker Volumes...(maybe not...)&lt;/p&gt; &#xA;&lt;h1&gt;Run Sample applications&lt;/h1&gt; &#xA;&lt;h2&gt;NY Bus Stops Data [Pyspark]&lt;/h2&gt; &#xA;&lt;p&gt;This programs just loads archived data from &lt;a href=&#34;http://web.mta.info/developers/MTA-Bus-Time-historical-data.html&#34;&gt;MTA Bus Time&lt;/a&gt; and apply basic filters using spark sql, the result are persisted into a postgresql table.&lt;/p&gt; &#xA;&lt;p&gt;The loaded table will contain the following structure:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;latitude&lt;/th&gt; &#xA;   &lt;th&gt;longitude&lt;/th&gt; &#xA;   &lt;th&gt;time_received&lt;/th&gt; &#xA;   &lt;th&gt;vehicle_id&lt;/th&gt; &#xA;   &lt;th&gt;distance_along_trip&lt;/th&gt; &#xA;   &lt;th&gt;inferred_direction_id&lt;/th&gt; &#xA;   &lt;th&gt;inferred_phase&lt;/th&gt; &#xA;   &lt;th&gt;inferred_route_id&lt;/th&gt; &#xA;   &lt;th&gt;inferred_trip_id&lt;/th&gt; &#xA;   &lt;th&gt;next_scheduled_stop_distance&lt;/th&gt; &#xA;   &lt;th&gt;next_scheduled_stop_id&lt;/th&gt; &#xA;   &lt;th&gt;report_hour&lt;/th&gt; &#xA;   &lt;th&gt;report_date&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;40.668602&lt;/td&gt; &#xA;   &lt;td&gt;-73.986697&lt;/td&gt; &#xA;   &lt;td&gt;2014-08-01 04:00:01&lt;/td&gt; &#xA;   &lt;td&gt;469&lt;/td&gt; &#xA;   &lt;td&gt;4135.34710710144&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;IN_PROGRESS&lt;/td&gt; &#xA;   &lt;td&gt;MTA NYCT_B63&lt;/td&gt; &#xA;   &lt;td&gt;MTA NYCT_JG_C4-Weekday-141500_B63_123&lt;/td&gt; &#xA;   &lt;td&gt;2.63183804205619&lt;/td&gt; &#xA;   &lt;td&gt;MTA_305423&lt;/td&gt; &#xA;   &lt;td&gt;2014-08-01 04:00:00&lt;/td&gt; &#xA;   &lt;td&gt;2014-08-01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To submit the app connect to one of the workers or the master and execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/opt/spark/bin/spark-submit --master spark://spark-master:7077 \&#xA;--jars /opt/spark-apps/postgresql-42.2.22.jar \&#xA;--driver-memory 1G \&#xA;--executor-memory 1G \&#xA;/opt/spark-apps/main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mvillarrealb/docker-spark-cluster/master/articles/images/pyspark-demo.png&#34; alt=&#34;alt text&#34; title=&#34;Spark UI with pyspark program running&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;MTA Bus Analytics[Scala]&lt;/h2&gt; &#xA;&lt;p&gt;This program takes the archived data from &lt;a href=&#34;http://web.mta.info/developers/MTA-Bus-Time-historical-data.html&#34;&gt;MTA Bus Time&lt;/a&gt; and make some aggregations on it, the calculated results are persisted on postgresql tables.&lt;/p&gt; &#xA;&lt;p&gt;Each persisted table correspond to a particullar aggregation:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Table&lt;/th&gt; &#xA;   &lt;th&gt;Aggregation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;day_summary&lt;/td&gt; &#xA;   &lt;td&gt;A summary of vehicles reporting, stops visited, average speed and distance traveled(all vehicles)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;speed_excesses&lt;/td&gt; &#xA;   &lt;td&gt;Speed excesses calculated in a 5 minute window&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;average_speed&lt;/td&gt; &#xA;   &lt;td&gt;Average speed by vehicle&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;distance_traveled&lt;/td&gt; &#xA;   &lt;td&gt;Total Distance traveled by vehicle&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To submit the app connect to one of the workers or the master and execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;/opt/spark/bin/spark-submit --deploy-mode cluster \&#xA;--master spark://spark-master:7077 \&#xA;--total-executor-cores 1 \&#xA;--class mta.processing.MTAStatisticsApp \&#xA;--driver-memory 1G \&#xA;--executor-memory 1G \&#xA;--jars /opt/spark-apps/postgresql-42.2.22.jar \&#xA;--conf spark.driver.extraJavaOptions=&#39;-Dconfig-path=/opt/spark-apps/mta.conf&#39; \&#xA;--conf spark.executor.extraJavaOptions=&#39;-Dconfig-path=/opt/spark-apps/mta.conf&#39; \&#xA;/opt/spark-apps/mta-processing.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will notice on the spark-ui a driver program and executor program running(In scala we can use deploy-mode cluster)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mvillarrealb/docker-spark-cluster/master/articles/images/stats-app.png&#34; alt=&#34;alt text&#34; title=&#34;Spark UI with scala program running&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Summary&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;We compiled the necessary docker image to run spark master and worker containers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We created a spark standalone cluster using 2 worker nodes and 1 master node using docker &amp;amp;&amp;amp; docker-compose.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Copied the resources necessary to run demo applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We ran a distributed application at home(just need enough cpu cores and RAM to do so).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Why a standalone cluster?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;This is intended to be used for test purposes, basically a way of running distributed spark apps on your laptop or desktop.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;This will be useful to use CI/CD pipelines for your spark apps(A really difficult and hot topic)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Steps to connect and use a pyspark shell interactively&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the steps to run the docker-compose file. You can scale this down if needed to 1 worker.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker-compose up --scale spark-worker=1&#xA;docker exec -it docker-spark-cluster_spark-worker_1 bash&#xA;apt update&#xA;apt install python3-pip&#xA;pip3 install pyspark&#xA;pyspark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;What&#39;s left to do?&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Right now to run applications in deploy-mode cluster is necessary to specify arbitrary driver port.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The spark submit entry in the start-spark.sh is unimplemented, the submit used in the demos can be triggered from any worker&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>