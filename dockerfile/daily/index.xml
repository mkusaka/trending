<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-14T01:32:10Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tmm1/flyapp-mastodon</title>
    <updated>2022-11-14T01:32:10Z</updated>
    <id>tag:github.com,2022-11-14:/tmm1/flyapp-mastodon</id>
    <link href="https://github.com/tmm1/flyapp-mastodon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;mastodon on fly.io&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Mastodon on fly.io&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mastodon/mastodon&#34;&gt;Mastodon&lt;/a&gt; is a free, open-source social network server based on ActivityPub.&lt;/p&gt; &#xA;&lt;p&gt;The Mastodon server is implemented a rails app, which relies on postgres and redis. It uses sidekiq for background jobs, along with a separate nodejs http streaming server.&lt;/p&gt; &#xA;&lt;p&gt;Docker images: &lt;a href=&#34;https://hub.docker.com/r/tootsuite/mastodon/&#34;&gt;https://hub.docker.com/r/tootsuite/mastodon/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dockerfile: &lt;a href=&#34;https://github.com/mastodon/mastodon/raw/main/Dockerfile&#34;&gt;https://github.com/mastodon/mastodon/blob/main/Dockerfile&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;docker-compose.yml: &lt;a href=&#34;https://github.com/mastodon/mastodon/raw/main/docker-compose.yml&#34;&gt;https://github.com/mastodon/mastodon/blob/main/docker-compose.yml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;h4&gt;App&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ fly apps create --region iad --name mastodon&#xA;$ fly scale memory 512 # rails needs more than 256mb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Secrets&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ SECRET_KEY_BASE=$(docker run --rm -it tootsuite/mastodon:latest bin/rake secret)&#xA;$ OTP_SECRET=$(docker run --rm -it tootsuite/mastodon:latest bin/rake secret)&#xA;$ fly secrets set OTP_SECRET=$OTP_SECRET SECRET_KEY_BASE=$SECRET_KEY_BASE&#xA;$ docker run --rm -e OTP_SECRET=$OTP_SECRET -e SECRET_KEY_BASE=$SECRET_KEY_BASE -it tootsuite/mastodon:latest bin/rake mastodon:webpush:generate_vapid_key | fly secrets import&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Redis server&lt;/h4&gt; &#xA;&lt;p&gt;Redis is used to store the home/list feeds, along with the sidekiq queue information. The feeds can be regenerated using &lt;code&gt;tootctl&lt;/code&gt;, so persistence is &lt;a href=&#34;https://docs.joinmastodon.org/admin/backups/#failure&#34;&gt;not strictly necessary&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ fly apps create --region iad --name mastodon-redis&#xA;$ fly volumes create -c fly.redis.toml --region iad mastodon_redis&#xA;$ fly deploy --config fly.redis.toml --build-target redis-server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Storage (user uploaded photos and videos)&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;fly.toml&lt;/code&gt; uses a &lt;code&gt;[mounts]&lt;/code&gt; section to connect the &lt;code&gt;/opt/mastodon/public/system&lt;/code&gt; folder to a persistent volume.&lt;/p&gt; &#xA;&lt;p&gt;Create that volume below, or remove the &lt;code&gt;[mounts]&lt;/code&gt; section and uncomment &lt;code&gt;[env] &amp;gt; S3_ENABLED&lt;/code&gt; for S3 storage.&lt;/p&gt; &#xA;&lt;h5&gt;Option 1: Local volume&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ fly volumes create --region iad mastodon_uploads&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Option 2: S3, etc&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ fly secrets set AWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=yyy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/mastodon/mastodon/raw/5ba46952af87e42a64962a34f7ec43bc710bdcaf/lib/tasks/mastodon.rake#L137&#34;&gt;lib/tasks/mastodon.rake&lt;/a&gt; for how to change your &lt;code&gt;[env]&lt;/code&gt; section for Wasabi, Minio or Google Cloud Storage.&lt;/p&gt; &#xA;&lt;h4&gt;Postgres database&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ fly pg create --region iad --name mastodon-pg&#xA;$ fly pg attach --postgres-app mastodon-pg&#xA;$ fly deploy -c fly.setup.toml # run `rails db:setup`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploy&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ fly deploy&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>jenkinsci/docker-agent</title>
    <updated>2022-11-14T01:32:10Z</updated>
    <id>tag:github.com,2022-11-14:/jenkinsci/docker-agent</id>
    <link href="https://github.com/jenkinsci/docker-agent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Base Docker image for Jenkins Agents&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Jenkins Agent Docker image&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitter.im/jenkinsci/docker?utm_source=badge&amp;amp;utm_medium=badge&amp;amp;utm_campaign=pr-badge&amp;amp;utm_content=badge&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/jenkinsci/docker.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/jenkinsci/docker&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jenkinsci/docker-agent&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/jenkinsci/docker-agent?label=GitHub%20stars&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/jenkins/agent/&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/jenkins/agent.svg?sanitize=true&#34; alt=&#34;Docker Pulls&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jenkinsci/docker-agent/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/jenkinsci/docker-agent.svg?label=changelog&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a base image for Docker, which includes JDK and the Jenkins agent executable (agent.jar). This executable is an instance of the &lt;a href=&#34;https://github.com/jenkinsci/remoting&#34;&gt;Jenkins Remoting library&lt;/a&gt;. JDK version depends on the image and the platform, see the &lt;em&gt;Configurations&lt;/em&gt; section below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;❗&lt;/span&gt; &lt;strong&gt;Warning!&lt;/strong&gt; This image used to be published as &lt;a href=&#34;https://hub.docker.com/r/jenkinsci/slave/&#34;&gt;jenkinsci/slave&lt;/a&gt; and &lt;a href=&#34;https://hub.docker.com/r/jenkins/slave/&#34;&gt;jenkins/slave&lt;/a&gt;. These images are now deprecated, use &lt;a href=&#34;https://hub.docker.com/r/jenkins/agent/&#34;&gt;jenkins/agent&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Changelog&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/jenkinsci/docker-agent/releases&#34;&gt;GitHub releases&lt;/a&gt; for versions &lt;code&gt;3.35-1&lt;/code&gt; and above. There is no changelog for previous versions, see the commit history.&lt;/p&gt; &#xA;&lt;p&gt;Jenkins remoting changelogs are available &lt;a href=&#34;https://github.com/jenkinsci/remoting/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;This image is used as the basis for the &lt;a href=&#34;https://github.com/jenkinsci/docker-inbound-agent/&#34;&gt;Docker Inbound Agent&lt;/a&gt; image. In that image, the container is launched externally and attaches to Jenkins.&lt;/p&gt; &#xA;&lt;p&gt;This image may instead be used to launch an agent using the &lt;strong&gt;Launch method&lt;/strong&gt; of &lt;strong&gt;Launch agent via execution of command on the master&lt;/strong&gt;. For example on Linux you can try&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker run -i --rm --name agent --init jenkins/agent java -jar /usr/share/jenkins/agent.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;after setting &lt;strong&gt;Remote root directory&lt;/strong&gt; to &lt;code&gt;/home/jenkins/agent&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;or if using Windows Containers&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;docker run -i --rm --name agent --init jenkins/agent:jdk11-windowsservercore-ltsc2019 java -jar C:/ProgramData/Jenkins/agent.jar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;after setting &lt;strong&gt;Remote root directory&lt;/strong&gt; to &lt;code&gt;C:\Users\jenkins\Agent&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Agent Work Directories&lt;/h3&gt; &#xA;&lt;p&gt;Starting from &lt;a href=&#34;https://github.com/jenkinsci/remoting/raw/master/CHANGELOG.md#38&#34;&gt;Remoting 3.8&lt;/a&gt; there is a support of Work directories, which provides logging by default and change the JAR Caching behavior.&lt;/p&gt; &#xA;&lt;p&gt;Call example for Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker run -i --rm --name agent1 --init -v agent1-workdir:/home/jenkins/agent jenkins/agent java -jar /usr/share/jenkins/agent.jar -workDir /home/jenkins/agent&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Call example for Windows Containers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;docker run -i --rm --name agent1 --init -v agent1-workdir:C:/Users/jenkins/Work jenkins/agent:jdk11-windowsservercore-ltsc2019 java -jar C:/ProgramData/Jenkins/agent.jar -workDir C:/Users/jenkins/Work&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Configurations&lt;/h2&gt; &#xA;&lt;p&gt;The image has several supported configurations, which can be accessed via the following tags:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux Images: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;latest&lt;/code&gt; (&lt;code&gt;jdk11&lt;/code&gt;, &lt;code&gt;bullseye-jdk11&lt;/code&gt;, &lt;code&gt;latest-bullseye-jdk11&lt;/code&gt;, &lt;code&gt;latest-jdk11&lt;/code&gt;): Latest version with the newest remoting and Java 11 (based on &lt;code&gt;debian:bullseye-${builddate}&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;alpine&lt;/code&gt; (&lt;code&gt;alpine-jdk11&lt;/code&gt;, &lt;code&gt;latest-alpine&lt;/code&gt;, &lt;code&gt;latest-alpine-jdk11&lt;/code&gt;): Small image based on Alpine Linux (based on &lt;code&gt;alpine:${version}&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;archlinux&lt;/code&gt; (&lt;code&gt;latest-archlinux&lt;/code&gt;, &lt;code&gt;archlinux-jdk11&lt;/code&gt;, &lt;code&gt;latest-archlinux-jdk11&lt;/code&gt;): Image based on Arch Linux with JDK11 (based on &lt;code&gt;archlinux:latest&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;bullseye-jdk17&lt;/code&gt; (&lt;code&gt;jdk17&lt;/code&gt;, &lt;code&gt;latest-bullseye-jdk17&lt;/code&gt;, &lt;code&gt;latest-jdk17&lt;/code&gt;): JDK17 version with the newest remoting (based on &lt;code&gt;debian:bullseye-${builddate}&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;From version 4.11.2, the alpine images are tagged using the alpine OS version as well (i.e. &lt;code&gt;alpine&lt;/code&gt; ==&amp;gt; &lt;code&gt;alpine3.16&lt;/code&gt;, &lt;code&gt;alpine-jdk11&lt;/code&gt; ==&amp;gt; &lt;code&gt;alpine3.16-jdk11&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows Images: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;jdk11-windowsservercore-ltsc2019&lt;/code&gt;: Latest version with the newest remoting and Java 11 (based on &lt;code&gt;eclipse-temurin:11.xxx-jdk-windowsservercore-ltsc2019&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;jdk11-nanoserver-1809&lt;/code&gt;: Latest version with the newest remoting with Windows Nano Server and Java 11 (based on &lt;code&gt;eclipse-temurin:11.xxx-jdk-nanoserver-1809&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;jdk17-windowsservercore-ltsc2019&lt;/code&gt;: Latest version with the newest remoting and Java 17 (based on &lt;code&gt;eclipse-temurin:17.xxx-jdk-windowsservercore-ltsc2019&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;jdk17-nanoserver-1809&lt;/code&gt;: Latest version with the newest remoting with Windows Nano Server and Java 17 (based on &lt;code&gt;eclipse-temurin:17.xxx-jdk-nanoserver-1809&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The file &lt;code&gt;docker-bake.hcl&lt;/code&gt; defines all the configuration for Linux images and their associated tags.&lt;/p&gt; &#xA;&lt;p&gt;There are also versioned tags in DockerHub, and they are recommended for production use. See the full list &lt;a href=&#34;https://hub.docker.com/r/jenkins/agent/tags&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deemoprobe/kubernetes</title>
    <updated>2022-11-14T01:32:10Z</updated>
    <id>tag:github.com,2022-11-14:/deemoprobe/kubernetes</id>
    <link href="https://github.com/deemoprobe/kubernetes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Some notes about k8s.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kubernetes基础之高可用集群搭建-kubeadm方式&lt;/h1&gt; &#xA;&lt;h2&gt;环境说明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;宿主机系统: Windows 10&lt;/li&gt; &#xA; &lt;li&gt;虚拟机版本: VMware® Workstation 16 Pro&lt;/li&gt; &#xA; &lt;li&gt;IOS镜像版本: CentOS Linux release 7.9.2009&lt;/li&gt; &#xA; &lt;li&gt;集群操作用户: root&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;CentOS7虚拟机安装和配置静态IP请参考博客文章：&lt;a href=&#34;http://www.deemoprobe.com/principle/vmware-workstation%e5%ae%89%e8%a3%85centos7%e5%b9%b6%e9%85%8d%e7%bd%ae%e9%9d%99%e6%80%81ip/&#34;&gt;VMWARE WORKSTATION安装CENTOS7并配置静态IP&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;资源分配&lt;/h2&gt; &#xA;&lt;h3&gt;网段划分&lt;/h3&gt; &#xA;&lt;p&gt;Kubernetes集群需要规划三个网段：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;宿主机网段：Kubernetes集群节点的网段&lt;/li&gt; &#xA; &lt;li&gt;Pod网段：集群内Pod的网段，相当于容器的IP&lt;/li&gt; &#xA; &lt;li&gt;Service网段：集群内服务发现使用的网段，service用于集群容器通信&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;生产环境根据申请到的IP资源进行分配即可，原则是三个网段不要有交叉。本文虚拟机练习环境IP地址段分配如下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;宿主机网段：&lt;code&gt;192.168.43.1/24&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pod网段：&lt;code&gt;172.16.0.0/12&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Service：&lt;code&gt;10.96.0.0/12&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;节点分配&lt;/h3&gt; &#xA;&lt;p&gt;本次实验采用3管理节点2工作节点的高可用Kubernetes集群模式:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;k8s-master01/k8s-master02/k8s-master03 集群的Master节点&lt;/li&gt; &#xA; &lt;li&gt;三个master节点同时做etcd集群&lt;/li&gt; &#xA; &lt;li&gt;k8s-node01/k8s-node02 集群的Node节点&lt;/li&gt; &#xA; &lt;li&gt;k8s-master-vip: 192.168.43.182,是做高可用k8s-master01~3的虚拟IP,不占用物理资源&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;主机节点名称&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;IP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CPU核心数&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;内存大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;磁盘大小&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;k8s-master-vip&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;192.168.43.182&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;/&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;k8s-master01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;192.168.43.183&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;k8s-master02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;192.168.43.184&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;k8s-master03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;192.168.43.185&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;k8s-node01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;192.168.43.186&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;k8s-node02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;192.168.43.187&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40G&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;操作步骤&lt;/h2&gt; &#xA;&lt;p&gt;小括号注释说明：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ALL 所有节点都要执行&lt;/li&gt; &#xA; &lt;li&gt;Master 只需要在master节点(k8s-master01/k8s-master02/k8s-master03)执行&lt;/li&gt; &#xA; &lt;li&gt;Node 只需要在node节点(k8s-node01/k8s-node02)执行&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;准备工作(ALL)&lt;/h3&gt; &#xA;&lt;p&gt;添加主机信息、关闭防火墙、关闭swap、关闭SELinux、dnsmasq、NetworkManager&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 以k8s-master01为例&#xA;[root@k8s-master01 ~]# cat &amp;lt;&amp;lt; EOF &amp;gt;&amp;gt; /etc/hosts&#xA;192.168.43.182    k8s-master-vip&#xA;192.168.43.183    k8s-master01&#xA;192.168.43.184    k8s-master02&#xA;192.168.43.185    k8s-master03&#xA;192.168.43.186    k8s-node01&#xA;192.168.43.187    k8s-node02&#xA;EOF&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 直接执行下面命令&#xA;systemctl stop firewalld&#xA;systemctl disable firewalld&#xA;systemctl disable --now dnsmasq&#xA;systemctl disable --now NetworkManager&#xA;&#xA;swapoff -a&#xA;sed -i &#39;/swap/s/^\(.*\)$/#\1/g&#39; /etc/fstab&#xA;&#xA;setenforce 0&#xA;sed -i &#34;s/=enforcing/=disabled/g&#34; /etc/selinux/config&#xA;&#xA;# 值得注意的是/etc/sysconfig/selinux文件是/etc/selinux/config文件的软连接，用sed -i命令修改软连接文件的话会破坏软连接属性，将/etc/sysconfig/selinux变为一个文件，即使该文件被修改了，但源文件/etc/selinux/config配置是没变的，所以推荐直接修改/etc/selinux/config中的配置，要么就直接vim编辑文件（编辑模式不会修改文件属性）修改也可以&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;更新源并升级内核(ALL)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 默认的yum源太慢，更新为阿里源，同时用sed命令删除包含下面不需要的两个URL的行&#xA;curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo&#xA;sed -i -e &#39;/mirrors.cloud.aliyuncs.com/d&#39; -e &#39;/mirrors.aliyuncs.com/d&#39; /etc/yum.repos.d/CentOS-Base.repo&#xA;# 配置阿里云Kubernetes镜像源&#xA;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo&#xA;[kubernetes]&#xA;name=Kubernetes&#xA;baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/&#xA;enabled=1&#xA;gpgcheck=1&#xA;repo_gpgcheck=1&#xA;gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg&#xA;       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg&#xA;EOF&#xA;&#xA;# 安装常用工具包（在需要时安装也可以，通常一起装了比较省事）&#xA;yum install wget jq psmisc vim net-tools telnet yum-utils device-mapper-persistent-data lvk8s-master02 git -y&#xA;&#xA;# 升级内核，4.17以下的内核cgroup存在内存泄漏的BUG，具体分析过程浏览器搜一下“Kubernetes集群为什么要升级内核”会有一大波文章&#xA;cd /root&#xA;wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm&#xA;wget http://193.49.22.109/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm&#xA;# 所有节点安装内核&#xA;cd /root &amp;amp;&amp;amp; yum localinstall -y kernel-ml*&#xA;# 所有节点更改内核启动顺序&#xA;grub2-set-default  0 &amp;amp;&amp;amp; grub2-mkconfig -o /etc/grub2.cfg&#xA;grubby --args=&#34;user_namespace.enable=1&#34; --update-kernel=&#34;$(grubby --default-kernel)&#34;&#xA;# 检查默认内核是不是4.19，并重启节点&#xA;grubby --default-kernel&#xA;reboot&#xA;&#xA;# 检查内核是不是4.19&#xA;uname -a&#xA;# （可选）删除老版本的内核，避免以后被升级取代默认的开机4.19内核&#xA;rpm -qa | grep kernel&#xA;yum remove -y kernel-3*&#xA;&#xA;# 升级系统软件包（如果跳过内核升级加参数 --exclude=kernel*）&#xA;yum update -y&#xA;&#xA;# 安装IPVS内核模块，由于IPVS在资源消耗和性能上均已明显优于iptables，所以推荐开启&#xA;# 具体原因可参考官网介绍 https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/&#xA;yum install ipvsadm ipset sysstat conntrack libseccomp -y&#xA;# 加载模块，最后一条4.18以下内核使用nf_conntrack_ipv4，4.19已改为nf_conntrack&#xA;modprobe -- ip_vs&#xA;modprobe -- ip_vs_rr&#xA;modprobe -- ip_vs_wrr&#xA;modprobe -- ip_vs_sh&#xA;modprobe -- nf_conntrack&#xA;# 编写参数文件&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/modules-load.d/ipvs.conf &#xA;ip_vs&#xA;ip_vs_lc&#xA;ip_vs_wlc&#xA;ip_vs_rr&#xA;ip_vs_wrr&#xA;ip_vs_lblc&#xA;ip_vs_lblcr&#xA;ip_vs_dh&#xA;ip_vs_sh&#xA;ip_vs_fo&#xA;ip_vs_nq&#xA;ip_vs_sed&#xA;ip_vs_ftp&#xA;ip_vs_sh&#xA;nf_conntrack&#xA;ip_tables&#xA;ip_set&#xA;xt_set&#xA;ipt_set&#xA;ipt_rpfilter&#xA;ipt_REJECT&#xA;ipip&#xA;EOF&#xA;# 加载&#xA;systemctl enable --now systemd-modules-load.service&#xA;# 自定义内核参数配置文件&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/sysctl.d/kubernetes.conf&#xA;net.ipv4.ip_forward = 1&#xA;net.bridge.bridge-nf-call-iptables = 1&#xA;net.bridge.bridge-nf-call-ip6tables = 1&#xA;fs.may_detach_mounts = 1&#xA;net.ipv4.conf.all.route_localnet = 1&#xA;vm.overcommit_memory=1&#xA;vm.panic_on_oom=0&#xA;fs.inotify.max_user_watches=89100&#xA;fs.file-max=52706963&#xA;fs.nr_open=52706963&#xA;net.netfilter.nf_conntrack_max=2310720&#xA;net.ipv4.tcp_keepalive_time = 600&#xA;net.ipv4.tcp_keepalive_probes = 3&#xA;net.ipv4.tcp_keepalive_intvl =15&#xA;net.ipv4.tcp_max_tw_buckets = 36000&#xA;net.ipv4.tcp_tw_reuse = 1&#xA;net.ipv4.tcp_max_orphans = 327680&#xA;net.ipv4.tcp_orphan_retries = 3&#xA;net.ipv4.tcp_syncookies = 1&#xA;net.ipv4.tcp_max_syn_backlog = 16384&#xA;net.ipv4.ip_conntrack_max = 65536&#xA;net.ipv4.tcp_max_syn_backlog = 16384&#xA;net.ipv4.tcp_timestamps = 0&#xA;net.core.somaxconn = 16384&#xA;EOF&#xA;# 加载&#xA;sysctl --system&#xA;# 重启查看PIVS模块是否依旧加载&#xA;reboot&#xA;lsmod | grep ip_vs&#xA;&#xA;# 配置ntpdate,同步服务器时间&#xA;rpm -ivh http://mirrors.wlnmp.com/centos/wlnmp-release-centos.noarch.rpm&#xA;yum install ntpdate -y&#xA;# 同步时区和时间&#xA;ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime&#xA;echo &#39;Asia/Shanghai&#39; &amp;gt;/etc/timezone&#xA;ntpdate time2.aliyun.com&#xA;&#xA;# 配置limits&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt;&amp;gt; /etc/security/limits.conf&#xA;* soft nofile 655360&#xA;* hard nofile 131072&#xA;* soft nproc 655350&#xA;* hard nproc 655350&#xA;* soft memlock unlimited&#xA;* hard memlock unlimited&#xA;EOF&#xA;&#xA;# 配置免密登录, k8s-master01到其他节点&#xA;# 先生成认证文件&#xA;ssh-keygen -t rsa&#xA;# 拷贝公钥信息到其他节点，同时认证一次各个节点的root密码，以后就可以免密ssh到其他节点&#xA;for i in k8s-master02 k8s-master03 k8s-node01 k8s-node02;do ssh-copy-id -i .ssh/id_rsa.pub $i;done&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;部署Docker(ALL)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 卸载已存在docker，新机器的话这步可以忽略&#xA;yum remove -y docker \&#xA;              docker-client \&#xA;              docker-client-latest \&#xA;              docker-common \&#xA;              docker-latest \&#xA;              docker-latest-logrotate \&#xA;              docker-logrotate \&#xA;              docker-engine&#xA;&#xA;yum remove -y docker-ce docker-ce-cli containerd.io&#xA;&#xA;# 设置docker仓库&#xA;yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo&#xA;&#xA;# 安装最新版本docker&#xA;yum install docker-ce docker-ce-cli containerd.io -y&#xA;&#xA;# （可选）如果想要安装指定版本docker，先查询一下。安装指定版本，例如20.10.9-3.el7&#xA;yum list docker-ce docker-ce-cli --showduplicates | grep &#34;^doc&#34; | sort -r&#xA;yum install docker-ce-20.10.9-3.el7 docker-ce-cli-20.10.9-3.el7 containerd.io -y&#xA;&#xA;# 加入开机启动并启动&#xA;systemctl enable docker&#xA;systemctl start docker&#xA;&#xA;# 测试运行hello-world镜像并查看docker版本信息&#xA;docker run hello-world&#xA;docker version&#xA;&#xA;# 配置阿里docker镜像加速器，阿里云(登录账号--&amp;gt;点击管理控制台--&amp;gt;搜索容器镜像服务--&amp;gt;镜像工具--&amp;gt;镜像加速器--&amp;gt;复制加速器地址)&#xA;# docker文件驱动改成 systemd&#xA;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/docker/daemon.json&#xA;{&#xA;  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],&#xA;  &#34;registry-mirror&#34;: [&#34;https://ynirk4k5.mirror.aliyuncs.com&#34;]&#xA;}&#xA;EOF&#xA;&#xA;# 重启docker&#xA;systemctl restart docker&#xA;# 如果启动失败,强制加载再启动试试&#xA;systemctl reset-failed docker&#xA;systemctl restart docker&#xA;&#xA;# 查看docker配置信息&#xA;docker info&#xA;docker info | grep Driver&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;安装kubernetes(ALL)&lt;/h3&gt; &#xA;&lt;p&gt;一般kebectl在master节点安装即可,node节点装不装均可&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 查看可以安装的版本号&#xA;yum list kubeadm --showduplicates | sort -r&#xA;# 不指定版本的话默认安装最新版本安装&#xA;yum install -y kubelet kubeadm kubectl&#xA;# 指定版本进行安装，如1.22.4&#xA;yum install -y kubelet-1.22.4 kubeadm-1.22.4 kubectl-1.22.4&#xA;# 配置pause镜像仓库，默认的gcr.io国内无法访问，可以使用阿里仓库&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/sysconfig/kubelet&#xA;KUBELET_EXTRA_ARGS=&#34;--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.5&#34;&#xA;EOF&#xA;# 先加入开机启动&#xA;systemctl enable kubelet&#xA;# 先不启动kubelet,因为会启动失败,提示初始化文件不存在,kubernetes集群初始化完成后会启动&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;高可用组件安装(Master)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 所有master节点安装Keepalived和haproxy&#xA;yum install keepalived haproxy -y&#xA;# 为所有master节点添加haproxy配置，配置都一样，检查最后三行主机名和IP地址对应上就行&#xA;mkdir /etc/haproxy&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/haproxy/haproxy.cfg&#xA;global&#xA;  maxconn  2000&#xA;  ulimit-n  16384&#xA;  log  127.0.0.1 local0 err&#xA;  stats timeout 30s&#xA;&#xA;defaults&#xA;  log global&#xA;  mode  http&#xA;  option  httplog&#xA;  timeout connect 5000&#xA;  timeout client  50000&#xA;  timeout server  50000&#xA;  timeout http-request 15s&#xA;  timeout http-keep-alive 15s&#xA;&#xA;frontend monitor-in&#xA;  bind *:33305&#xA;  mode http&#xA;  option httplog&#xA;  monitor-uri /monitor&#xA;&#xA;frontend k8s-master&#xA;  bind 0.0.0.0:16443&#xA;  bind 127.0.0.1:16443&#xA;  mode tcp&#xA;  option tcplog&#xA;  tcp-request inspect-delay 5s&#xA;  default_backend k8s-master&#xA;&#xA;backend k8s-master&#xA;  mode tcp&#xA;  option tcplog&#xA;  option tcp-check&#xA;  balance roundrobin&#xA;  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100&#xA;  server k8s-master01  192.168.43.183:6443  check&#xA;  server k8s-master02  192.168.43.184:6443  check&#xA;  server k8s-master03  192.168.43.185:6443  check&#xA;EOF&#xA;# keepalived配置不一样，注意区分网卡名、IP地址和虚拟IP地址&#xA;# 检查服务器网卡名&#xA;ip a 或 ifconfig&#xA;# k8s-master01 Keepalived配置&#xA;mkdir /etc/keepalived&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/keepalived/keepalived.conf&#xA;! Configuration File for keepalived&#xA;global_defs {&#xA;    router_id LVS_DEVEL&#xA;script_user root&#xA;    enable_script_security&#xA;}&#xA;vrrp_script chk_apiserver {&#xA;    script &#34;/etc/keepalived/check_apiserver.sh&#34;&#xA;    interval 5&#xA;    weight -5&#xA;    fall 2  &#xA;rise 1&#xA;}&#xA;vrrp_instance VI_1 {&#xA;    state MASTER&#xA;    interface ens33&#xA;    mcast_src_ip 192.168.43.183&#xA;    virtual_router_id 51&#xA;    priority 101&#xA;    advert_int 2&#xA;    authentication {&#xA;        auth_type PASS&#xA;        auth_pass K8SHA_KA_AUTH&#xA;    }&#xA;    virtual_ipaddress {&#xA;        192.168.43.182&#xA;    }&#xA;    track_script {&#xA;       chk_apiserver&#xA;    }&#xA;}&#xA;EOF&#xA;# k8s-master02 Keepalived配置&#xA;mkdir /etc/keepalived&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/keepalived/keepalived.conf&#xA;! Configuration File for keepalived&#xA;global_defs {&#xA;    router_id LVS_DEVEL&#xA;script_user root&#xA;    enable_script_security&#xA;}&#xA;vrrp_script chk_apiserver {&#xA;    script &#34;/etc/keepalived/check_apiserver.sh&#34;&#xA;   interval 5&#xA;    weight -5&#xA;    fall 2  &#xA;rise 1&#xA;}&#xA;vrrp_instance VI_1 {&#xA;    state BACKUP&#xA;    interface ens33&#xA;    mcast_src_ip 192.168.43.184&#xA;    virtual_router_id 51&#xA;    priority 100&#xA;    advert_int 2&#xA;    authentication {&#xA;        auth_type PASS&#xA;        auth_pass K8SHA_KA_AUTH&#xA;    }&#xA;    virtual_ipaddress {&#xA;        192.168.43.182&#xA;    }&#xA;    track_script {&#xA;       chk_apiserver&#xA;    }&#xA;}&#xA;EOF&#xA;# k8s-master03 Keepalived配置&#xA;mkdir /etc/keepalived&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/keepalived/keepalived.conf&#xA;! Configuration File for keepalived&#xA;global_defs {&#xA;    router_id LVS_DEVEL&#xA;script_user root&#xA;    enable_script_security&#xA;}&#xA;vrrp_script chk_apiserver {&#xA;    script &#34;/etc/keepalived/check_apiserver.sh&#34;&#xA; interval 5&#xA;    weight -5&#xA;    fall 2  &#xA;rise 1&#xA;}&#xA;vrrp_instance VI_1 {&#xA;    state BACKUP&#xA;    interface ens33&#xA;    mcast_src_ip 192.168.43.185&#xA;    virtual_router_id 51&#xA;    priority 100&#xA;    advert_int 2&#xA;    authentication {&#xA;        auth_type PASS&#xA;        auth_pass K8SHA_KA_AUTH&#xA;    }&#xA;    virtual_ipaddress {&#xA;        192.168.43.182&#xA;    }&#xA;    track_script {&#xA;       chk_apiserver&#xA;    }&#xA;}&#xA;EOF&#xA;# 所有master节点配置Keepalived健康检查脚本&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; /etc/keepalived/check_apiserver.sh &#xA;#!/bin/bash&#xA;err=0&#xA;for k in $(seq 1 3)&#xA;do&#xA;    check_code=$(pgrep haproxy)&#xA;    if [[ $check_code == &#34;&#34; ]]; then&#xA;        err=$(expr $err + 1)&#xA;        sleep 1&#xA;        continue&#xA;    else&#xA;        err=0&#xA;        break&#xA;    fi&#xA;done&#xA;&#xA;if [[ $err != &#34;0&#34; ]]; then&#xA;    echo &#34;systemctl stop keepalived&#34;&#xA;    /usr/bin/systemctl stop keepalived&#xA;    exit 1&#xA;else&#xA;    exit 0&#xA;fi&#xA;EOF&#xA;# 赋予可执行权限&#xA;chmod +x /etc/keepalived/check_apiserver.sh&#xA;# 启动haproxy和Keepalived并加入开机启动&#xA;systemctl start haproxy&#xA;systemctl start keepalived&#xA;systemctl enable haproxy&#xA;systemctl enable keepalived&#xA;&#xA;# 测试一波&#xA;telnet k8s-master-vip 16443&#xA;ping k8s-master-vip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;部署k8s-master(k8s-master01)&lt;/h3&gt; &#xA;&lt;p&gt;在k8s-master01节点上执行，个别步骤在所有master节点执行，已另行说明，没说明的均是在k8s-master01执行。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 创建初始化文件，注意版本号和IP对应上，&#xA;cat &amp;lt;&amp;lt; EOF &amp;gt; kubeadm-config.yaml&#xA;apiVersion: kubeadm.k8s.io/v1beta2&#xA;bootstrapTokens:&#xA;- groups:&#xA;  - system:bootstrappers:kubeadm:default-node-token&#xA;  token: 7t2weq.bjbawausm0jaxury&#xA;  ttl: 24h0m0s&#xA;  usages:&#xA;  - signing&#xA;  - authentication&#xA;kind: InitConfiguration&#xA;localAPIEndpoint:&#xA;  advertiseAddress: 192.168.43.183&#xA;  bindPort: 6443&#xA;nodeRegistration:&#xA;  criSocket: /var/run/dockershim.sock&#xA;  name: k8s-master01&#xA;  taints:&#xA;  - effect: NoSchedule&#xA;    key: node-role.kubernetes.io/master&#xA;---&#xA;apiServer:&#xA;  certSANs:&#xA;  - 192.168.43.182&#xA;  timeoutForControlPlane: 4m0s&#xA;apiVersion: kubeadm.k8s.io/v1beta2&#xA;certificatesDir: /etc/kubernetes/pki&#xA;clusterName: kubernetes&#xA;controlPlaneEndpoint: 192.168.43.182:16443&#xA;controllerManager: {}&#xA;dns:&#xA;  type: CoreDNS&#xA;etcd:&#xA;  local:&#xA;    dataDir: /var/lib/etcd&#xA;imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers&#xA;kind: ClusterConfiguration&#xA;kubernetesVersion: v1.23.0&#xA;networking:&#xA;  dnsDomain: cluster.local&#xA;  podSubnet: 172.16.0.0/12&#xA;  serviceSubnet: 10.96.0.0/12&#xA;scheduler: {}&#xA;EOF&#xA;&#xA;# 更新初始化文件&#xA;kubeadm config migrate --old-config kubeadm-config.yaml --new-config new.yaml&#xA;# 将new.yaml复制到其他master节点上&#xA;for i in k8s-master02 k8s-master03;do scp new.yaml $i:/root/;done&#xA;&#xA;# 镜像预下载，节省集群初始化的时间（这一步在所有master节点上执行）&#xA;kubeadm config images pull --config /root/new.yaml&#xA;# 执行结果如下&#xA;[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.23.0&#xA;[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.23.0&#xA;[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.23.0&#xA;[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.23.0&#xA;[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.6&#xA;[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.5.1-0&#xA;[config/images] Pulled registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.6&#xA;&#xA;# master01初始化，初始化完成后，加入其他节点即可&#xA;kubeadm init --config /root/new.yaml  --upload-certs&#xA;# 初始化如果失败，可用下面命令清除初始化信息，然后再次尝试初始化&#xA;kubeadm reset -f ; ipvsadm --clear  ; rm -rf ~/.kube&#xA;# 初始化成功类似于下面输出，保存好这些信息&#xA;...&#xA;Your Kubernetes control-plane has initialized successfully!&#xA;&#xA;To start using your cluster, you need to run the following as a regular user:&#xA;&#xA;  mkdir -p $HOME/.kube&#xA;  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&#xA;  sudo chown $(id -u):$(id -g) $HOME/.kube/config&#xA;&#xA;Alternatively, if you are the root user, you can run:&#xA;&#xA;  export KUBECONFIG=/etc/kubernetes/admin.conf&#xA;&#xA;You should now deploy a pod network to the cluster.&#xA;Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:&#xA;  https://kubernetes.io/docs/concepts/cluster-administration/addons/&#xA;&#xA;You can now join any number of the control-plane node running the following command on each as root:&#xA;&#xA;  kubeadm join 192.168.43.182:16443 --token 7t2weq.bjbawausm0jaxury \&#xA;        --discovery-token-ca-cert-hash sha256:5257d44118ab035adc5af89dd7d5a24ca4c31c33e1918b3453ea9aa32597121b \&#xA;        --control-plane --certificate-key 9a2e86718fceba001c96e503e9df47db3a645d4917bf783decaea9c5d0a726ed&#xA;&#xA;Please note that the certificate-key gives access to cluster sensitive data, keep it secret!&#xA;As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use&#xA;&#34;kubeadm init phase upload-certs --upload-certs&#34; to reload certs afterward.&#xA;&#xA;Then you can join any number of worker nodes by running the following on each as root:&#xA;&#xA;kubeadm join 192.168.43.182:16443 --token 7t2weq.bjbawausm0jaxury \&#xA;        --discovery-token-ca-cert-hash sha256:5257d44118ab035adc5af89dd7d5a24ca4c31c33e1918b3453ea9aa32597121b&#xA;&#xA;# 按照提示，如果你是普通用户在操作，执行一下下面几条&#xA;mkdir -p $HOME/.kube&#xA;sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config&#xA;sudo chown $(id -u):$(id -g) $HOME/.kube/config&#xA;# 如果是root用户在操作初始化，执行下面一条即可&#xA;export KUBECONFIG=/etc/kubernetes/admin.conf&#xA;&#xA;# 查看docker镜像,可以看到kube..和etcd等镜像&#xA;docker images&#xA;&#xA;# Token过期后生成新的token（没提示过期下面两步就不用管了）&#xA;kubeadm token create --print-join-command&#xA;# Master需要生成--certificate-key&#xA;kubeadm init phase upload-certs  --upload-certs&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;其他节点加入集群&lt;/h3&gt; &#xA;&lt;p&gt;其他master节点加入k8s-master01&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 根据kubeadm init提示的token&#xA;kubeadm join 192.168.43.182:16443 --token 7t2weq.bjbawausm0jaxury \&#xA;        --discovery-token-ca-cert-hash sha256:5257d44118ab035adc5af89dd7d5a24ca4c31c33e1918b3453ea9aa32597121b \&#xA;        --control-plane --certificate-key 9a2e86718fceba001c96e503e9df47db3a645d4917bf783decaea9c5d0a726ed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;node节点加入k8s-master01&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 根据kubeadm init提示的token&#xA;kubeadm join 192.168.43.182:16443 --token 7t2weq.bjbawausm0jaxury \&#xA;        --discovery-token-ca-cert-hash sha256:5257d44118ab035adc5af89dd7d5a24ca4c31c33e1918b3453ea9aa32597121b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# master01上查看加入后的节点信息，因为还未配置CNI插件，所以node之间通信还未打通&#xA;[root@k8s-master01 ~]# kubectl get no&#xA;NAME           STATUS     ROLES                  AGE     VERSION&#xA;k8s-master01   NotReady   control-plane,master   19m     v1.23.0&#xA;k8s-master02   NotReady   control-plane,master   3m39s   v1.23.0&#xA;k8s-master03   NotReady   control-plane,master   3m35s   v1.23.0&#xA;k8s-node01     NotReady   &amp;lt;none&amp;gt;                 2m21s   v1.23.0&#xA;k8s-node02     NotReady   &amp;lt;none&amp;gt;                 2m21s   v1.23.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;配置calico网络&lt;/h3&gt; &#xA;&lt;p&gt;在k8s-master01节点上执行&lt;/p&gt; &#xA;&lt;p&gt;网络方案也可以选择其他(例如：flannel等)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 先把所需的配置文件从GitHub拉下来&#xA;git clone https://github.com/deemoprobe/k8s-ha-install.git&#xA;# 切换到1.23分支并进入calico文件夹&#xA;cd /root/k8s-ha-install &amp;amp;&amp;amp; git checkout manual-installation-v1.23.x &amp;amp;&amp;amp; cd calico/&#xA;# 替换一下POD网段&#xA;POD_SUBNET=`cat /etc/kubernetes/manifests/kube-controller-manager.yaml | grep cluster-cidr= | awk -F= &#39;{print $NF}&#39;`&#xA;sed -i &#34;s#POD_CIDR#${POD_SUBNET}#g&#34; calico.yaml&#xA;# 应用calico插件&#xA;kubectl apply -f calico.yaml&#xA;# 集群节点均已处于Ready状态&#xA;[root@k8s-master01 calico]# kubectl get no&#xA;NAME           STATUS   ROLES                  AGE   VERSION&#xA;k8s-master01   Ready    control-plane,master   29m   v1.23.0&#xA;k8s-master02   Ready    control-plane,master   13m   v1.23.0&#xA;k8s-master03   Ready    control-plane,master   13m   v1.23.0&#xA;k8s-node01     Ready    &amp;lt;none&amp;gt;                 11m   v1.23.0&#xA;k8s-node02     Ready    &amp;lt;none&amp;gt;                 11m   v1.23.0&#xA;# 查看calico Pod是否都正常&#xA;kubectl get pod -A&#xA;# 如果不正常，可以排查一下，一般是镜像拉取问题，多等待几分钟即可，也可以根据报错简单处理一下&#xA;kubectl describe pod XXX -n kube-system&#xA;# 比如我这里有个pod处于pending状态，查看一下原因&#xA;[root@k8s-master01 calico]# kubectl get pod -A&#xA;NAMESPACE     NAME                                       READY   STATUS    RESTARTS      AGE&#xA;...&#xA;kube-system   calico-typha-8445487f56-hx8w9              1/1     Running   0             11m&#xA;kube-system   calico-typha-8445487f56-mh6tp              0/1     Pending   0             11m&#xA;kube-system   calico-typha-8445487f56-pxthb              1/1     Running   0             11m&#xA;...&#xA;# 可以看到提示说2个node节点无法提供足量的pod端口分配需求，而且提示master节点设置了污点&#xA;[root@k8s-master01 calico]# kubectl describe pod calico-typha-8445487f56-mh6tp -n kube-system&#xA;...&#xA;Events:&#xA;  Type     Reason            Age                    From               Message&#xA;  ----     ------            ----                   ----               -------&#xA;  Warning  FailedScheduling  11m                    default-scheduler  0/5 nodes are available: 2 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn&#39;t tolerate, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn&#39;t tolerate.&#xA;  Warning  FailedScheduling  10m                    default-scheduler  0/5 nodes are available: 1 node(s) didn&#39;t have free ports for the requested pod ports, 1 node(s) had taint {node.kubernetes.io/not-ready: }, that the pod didn&#39;t tolerate, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn&#39;t tolerate.&#xA;  Warning  FailedScheduling  10m                    default-scheduler  0/5 nodes are available: 2 node(s) didn&#39;t have free ports for the requested pod ports, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn&#39;t tolerate.&#xA;  Warning  FailedScheduling  8m24s (x1 over 9m24s)  default-scheduler  0/5 nodes are available: 2 node(s) didn&#39;t have free ports for the requested pod ports, 3 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn&#39;t tolerate.&#39;&#xA;# 确认一下，可以看到三个master节点打上了不可调度pod的污点&#xA;[root@k8s-master01 calico]# for i in k8s-master01 k8s-master02 k8s-master03;do kubectl describe node $i | grep -i taint;done&#xA;Taints:             node-role.kubernetes.io/master:NoSchedule&#xA;Taints:             node-role.kubernetes.io/master:NoSchedule&#xA;Taints:             node-role.kubernetes.io/master:NoSchedule&#xA;# 由于是练习环境，我就把不可调度的污点取消了，如果是生产环境，建议扩容node工作节点来实现足量的端口分配&#xA;[root@k8s-master01 calico]# for i in k8s-master01 k8s-master02 k8s-master03;do kubectl taint node $i node-role.kubernetes.io/master:NoSchedule-;done&#xA;node/k8s-master01 untainted&#xA;node/k8s-master02 untainted&#xA;node/k8s-master03 untainted&#xA;# 污点成功取消&#xA;[root@k8s-master01 calico]# for i in k8s-master01 k8s-master02 k8s-master03;do kubectl describe node $i | grep -i taint;done&#xA;Taints:             &amp;lt;none&amp;gt;&#xA;Taints:             &amp;lt;none&amp;gt;&#xA;Taints:             &amp;lt;none&amp;gt;&#xA;# 再查看刚才处于pending的pod发现已经处于running状态了&#xA;[root@k8s-master01 calico]# kubectl get po -A | grep calico-typha-8445487f56-mh6tp&#xA;kube-system   calico-typha-8445487f56-mh6tp              1/1     Running   0             23m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;部署Metrics&lt;/h2&gt; &#xA;&lt;p&gt;在新版的Kubernetes中系统资源的采集均使用Metrics-server，可以通过Metrics采集节点和Pod的内存、磁盘、CPU和网络的使用率。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 将Master01节点的front-proxy-ca.crt复制到所有Node节点&#xA;[root@k8s-master01 calico]# for i in k8s-node01 k8s-node02;do scp /etc/kubernetes/pki/front-proxy-ca.crt $i:/etc/kubernetes/pki/front-proxy-ca.crt;done&#xA;front-proxy-ca.crt                                                                                                   100% 1115   593.4KB/s   00:00    &#xA;front-proxy-ca.crt                                                                                                   100% 1115     1.4MB/s   00:00  &#xA;# 安装metrics server&#xA;[root@k8s-master01 calico]# cd /root/k8s-ha-install/kubeadm-metrics-server&#xA;[root@k8s-master01 kubeadm-metrics-server]# kubectl apply -f comp.yaml &#xA;serviceaccount/metrics-server created&#xA;clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created&#xA;clusterrole.rbac.authorization.k8s.io/system:metrics-server created&#xA;rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created&#xA;clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created&#xA;clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created&#xA;service/metrics-server created&#xA;deployment.apps/metrics-server created&#xA;apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created&#xA;# 查看运行状态&#xA;[root@k8s-master01 kubeadm-metrics-server]# kubectl get po -A | grep metrics&#xA;kube-system   metrics-server-5cf8885b66-2nnb6            1/1     Running   0             68s&#xA;# 部署后便可以查看指标了&#xA;[root@k8s-master01 kubeadm-metrics-server]# kubectl top node&#xA;NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   &#xA;k8s-master01   177m         8%     1196Mi          64%       &#xA;k8s-master02   153m         7%     1101Mi          58%       &#xA;k8s-master03   163m         8%     1102Mi          58%       &#xA;k8s-node01     88m          4%     848Mi           45%       &#xA;k8s-node02     85m          4%     842Mi           45%       &#xA;[root@k8s-master01 kubeadm-metrics-server]# kubectl top po&#xA;NAME                     CPU(cores)   MEMORY(bytes)   &#xA;nginx-85b98978db-7mn6r   0m           3Mi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;部署Dashboard&lt;/h2&gt; &#xA;&lt;p&gt;Dashboard是一个展示Kubernetes集群资源和Pod日志，甚至可以执行容器命令的web控制台。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 直接部署即可&#xA;[root@k8s-master01 kubeadm-metrics-server]# cd /root/k8s-ha-install/dashboard/&#xA;[root@k8s-master01 dashboard]# ls&#xA;dashboard-user.yaml  dashboard.yaml&#xA;[root@k8s-master01 dashboard]# kubectl apply -f .&#xA;serviceaccount/admin-user created&#xA;clusterrolebinding.rbac.authorization.k8s.io/admin-user created&#xA;namespace/kubernetes-dashboard created&#xA;serviceaccount/kubernetes-dashboard created&#xA;service/kubernetes-dashboard created&#xA;secret/kubernetes-dashboard-certs created&#xA;secret/kubernetes-dashboard-csrf created&#xA;secret/kubernetes-dashboard-key-holder created&#xA;configmap/kubernetes-dashboard-settings created&#xA;role.rbac.authorization.k8s.io/kubernetes-dashboard created&#xA;clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created&#xA;rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created&#xA;clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created&#xA;deployment.apps/kubernetes-dashboard created&#xA;service/dashboard-metrics-scraper created&#xA;deployment.apps/dashboard-metrics-scraper created&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 查看dashboard端口，默认是NodePort模式，访问集群内任意节点的31073端口即可&#xA;[root@k8s-master01 dashboard]# kubectl get svc -owide -A | grep dash&#xA;kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.105.172.8    &amp;lt;none&amp;gt;        8000/TCP                 19m    k8s-app=dashboard-metrics-scraper&#xA;kubernetes-dashboard   kubernetes-dashboard        NodePort    10.99.148.159   &amp;lt;none&amp;gt;        443:31073/TCP            19m    k8s-app=kubernetes-dashboard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;访问dashboard：&lt;a href=&#34;https://%E9%9B%86%E7%BE%A4%E5%86%85%E4%BB%BB%E6%84%8F%E8%8A%82%E7%82%B9IP:31073&#34;&gt;https://集群内任意节点IP:31073&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;发现提示隐私设置错误的问题，如图：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20211213153948.png&#34; alt=&#34;20211213153948&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;在Chrome浏览器启动参数加入&lt;code&gt;--test-type --ignore-certificate-errors&lt;/code&gt;，然后再访问就没有这个安全提示了&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20211213154024.png&#34; alt=&#34;20211213154024&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20211213154133.png&#34; alt=&#34;20211213154133&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 获取登陆令牌（token）&#xA;[root@k8s-master01 dashboard]# kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk &#39;{print $1}&#39;)&#xA;Name:         admin-user-token-mwnfs&#xA;Namespace:    kube-system&#xA;Labels:       &amp;lt;none&amp;gt;&#xA;Annotations:  kubernetes.io/service-account.name: admin-user&#xA;              kubernetes.io/service-account.uid: 29584392-1cbd-4d5c-91af-9dd4703008aa&#xA;&#xA;Type:  kubernetes.io/service-account-token&#xA;&#xA;Data&#xA;====&#xA;ca.crt:     1099 bytes&#xA;namespace:  11 bytes&#xA;token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjRyZlh6Ukxta0FlajlHREF5ei1mdl8tZmR6ekwteV9fVEIwalQtejRwUk0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLW13bmZzIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIyOTU4NDM5Mi0xY2JkLTRkNWMtOTFhZi05ZGQ0NzAzMDA4YWEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.pMBMkLAP2AoymIXJC7H47IPu3avdBWPYSZfvjRME7lEQAnbe-SM-yrTFGPzcsJQC3O9gPDvXgIZ1x1tQUtQhc_333GtDMj_VL9oEZxYiOdd578CnBiFmF0BWVX06pAzONgKbguamMD8XEPAvKt4mnlDUr7WCeQJZf_juXKdl7ZOBtrM5Zae0UQHFG6juKLmFP-XxIgoDVIPhcxeAH1ktOHM9Fk1M831hywL1SL2OLHiN52wGLT4WuYrP2iUbJkNpt2PYitSp3iNuh7rESL4Ur7lmFQkLZa9e5vNMCc1wTwOAWvaW4P5TbxtfI_ng4NK_avquiXJY-67D77G-8WKzWg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20211213154451.png&#34; alt=&#34;20211213154451&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;一些必要的更改&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 更改kube-proxy模式为ipvs&#xA;[root@k8s-master01 dashboard]# kubectl edit cm kube-proxy -n kube-system&#xA;mode: &#34;ipvs&#34;&#xA;# 更新kube-proxy的pod&#xA;[root@k8s-master01 dashboard]# kubectl patch daemonset kube-proxy -p &#34;{\&#34;spec\&#34;:{\&#34;template\&#34;:{\&#34;metadata\&#34;:{\&#34;annotations\&#34;:{\&#34;date\&#34;:\&#34;`date +&#39;%s&#39;`\&#34;}}}}}&#34; -n kube-system&#xA;daemonset.apps/kube-proxy patched&#xA;# 验证&#xA;[root@k8s-master01 dashboard]# curl 127.0.0.1:10249/proxyMode&#xA;ipvs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;测试集群&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 增加node节点的节点role名称&#xA;kubectl label nodes k8s-node1 node-role.kubernetes.io/node=&#xA;# 删除node节点的节点role名称&#xA;kubectl label nodes k8s-node1 node-role.kubernetes.io/node-&#xA;&#xA;# 添加标签后查看集群状态&#xA;[root@k8s-master01 calico]# kubectl get no&#xA;NAME           STATUS   ROLES                  AGE   VERSION&#xA;k8s-master01   Ready    control-plane,master   56m   v1.23.0&#xA;k8s-master02   Ready    control-plane,master   40m   v1.23.0&#xA;k8s-master03   Ready    control-plane,master   39m   v1.23.0&#xA;k8s-node01     Ready    node                   38m   v1.23.0&#xA;k8s-node02     Ready    node                   38m   v1.23.0&#xA;&#xA;# 测试namespace&#xA;kubectl get namespace&#xA;kubectl create namespace test&#xA;kubectl get namespace&#xA;kubectl delete namespace test&#xA;&#xA;# 创建nginx实例并开放端口&#xA;kubectl create deployment nginx --image=nginx&#xA;kubectl expose deployment nginx --port=80 --type=NodePort&#xA;# 查看调度状态和端口号&#xA;[root@k8s-master01 calico]# kubectl get pod,svc -owide&#xA;NAME                         READY   STATUS    RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES&#xA;pod/nginx-85b98978db-7mn6r   1/1     Running   0          2m16s   172.27.14.193   k8s-node02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;&#xA;&#xA;NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE    SELECTOR&#xA;service/kubernetes   ClusterIP   10.96.0.1      &amp;lt;none&amp;gt;        443/TCP        59m    &amp;lt;none&amp;gt;&#xA;service/nginx        NodePort    10.104.33.99   &amp;lt;none&amp;gt;        80:31720/TCP   2m6s   app=nginx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;可见调度到了k8s-node02（node IP地址是192.168.43.184）上，对应的NodePort为31720&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;在浏览器输入&lt;a href=&#34;http://192.168.43.184:31720/&#34;&gt;http://192.168.43.184:31720/&lt;/a&gt; 访问nginx，访问结果如图&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://deemoprobe.oss-cn-shanghai.aliyuncs.com/images/20211213143124.png&#34; alt=&#34;20211213143124&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;至此，基于kubeadm的Kubernetes高可用集群搭建成功。&lt;/p&gt;</summary>
  </entry>
</feed>