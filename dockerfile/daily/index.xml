<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-05T01:34:31Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Atinoda/text-generation-webui-docker</title>
    <updated>2023-06-05T01:34:31Z</updated>
    <id>tag:github.com,2023-06-05:/Atinoda/text-generation-webui-docker</id>
    <link href="https://github.com/Atinoda/text-generation-webui-docker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;This project dockerises the deployment of &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;oobabooga/text-generation-webui&lt;/a&gt; and its variants. It provides a default configuration (corresponding to a vanilla deployment of the application) as well as pre-configured support for other set-ups (e.g., latest &lt;code&gt;llama-cpp-python&lt;/code&gt; with GPU offloading, the more recent &lt;code&gt;triton&lt;/code&gt; and &lt;code&gt;cuda&lt;/code&gt; branches of GPTQ).&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This goal of this project is to be to &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;oobabooga/text-generation-webui&lt;/a&gt;, what &lt;a href=&#34;https://github.com/AbdBarho/stable-diffusion-webui-docker&#34;&gt;AbdBarho/stable-diffusion-webui-docker&lt;/a&gt; is to &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;This project currently supports Linux as the deployment platform. It will also probably work using WSL2.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pre-Requisites&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;docker&lt;/li&gt; &#xA; &lt;li&gt;docker compose&lt;/li&gt; &#xA; &lt;li&gt;CUDA docker runtime&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker Compose&lt;/h2&gt; &#xA;&lt;p&gt;This is the recommended deployment method.&lt;/p&gt; &#xA;&lt;h3&gt;Select variant&lt;/h3&gt; &#xA;&lt;p&gt;Choose the desired variant by setting the image &lt;code&gt;:tag&lt;/code&gt; in &lt;code&gt;docker-compose.yml&lt;/code&gt; to one of the following options:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Variant&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;default&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Implementation of the vanilla deployment from source. Also includes pre-installed &lt;code&gt;AutoGPTQ&lt;/code&gt; library from &lt;code&gt;PanQiWei/AutoGPTQ&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;triton&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Updated GPTQ using the latest &lt;code&gt;triton&lt;/code&gt; branch from &lt;code&gt;qwopqwop200/GPTQ-for-LLaMa&lt;/code&gt;. Suitable for Linux only.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;cuda&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Updated GPTQ using the latest &lt;code&gt;cuda&lt;/code&gt; branch from &lt;code&gt;qwopqwop200/GPTQ-for-LLaMa&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;monkey-patch&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use LoRAs in 4-Bit GPTQ mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;llama-cublas&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CUDA GPU offloading enabled for llama-cpp. Use by setting option &lt;code&gt;n-gpu-layers&lt;/code&gt; &amp;gt; 0.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;See: &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/docs/GPTQ-models-(4-bit-mode).md&#34;&gt;oobabooga/text-generation-webui/blob/main/docs/GPTQ-models-(4-bit-mode).md&lt;/a&gt; and &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/docs/llama.cpp-models.md&#34;&gt;obabooga/text-generation-webui/blob/main/docs/llama.cpp-models.md&lt;/a&gt; for more information on variants.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Deploy&lt;/h3&gt; &#xA;&lt;p&gt;Deploy the service:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker compose up&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Remove&lt;/h3&gt; &#xA;&lt;p&gt;Remove the service:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker compose down -v&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;These configuration instructions describe the relevant details for this docker wrapper. Refer to &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;oobabooga/text-generation-webui&lt;/a&gt; documentation for usage of the application itself.&lt;/p&gt; &#xA;&lt;h3&gt;Ports&lt;/h3&gt; &#xA;&lt;p&gt;Three commonly used ports are exposed:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Port&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Configuration&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;7860&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Web UI port&lt;/td&gt; &#xA;   &lt;td&gt;Pre-configured and enabled in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;5000&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;API port&lt;/td&gt; &#xA;   &lt;td&gt;Enable by adding &lt;code&gt;--api --extensions api&lt;/code&gt; to launch args then uncomment mapping in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;5005&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Streaming port&lt;/td&gt; &#xA;   &lt;td&gt;Enable by adding &lt;code&gt;--api --extensions api&lt;/code&gt; to launch args then uncomment mapping in &lt;code&gt;docker-compose.yml&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;Extensions may use additional ports - check the application documentation for more details.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Volumes&lt;/h3&gt; &#xA;&lt;p&gt;The provided example docker compose maps several volumes from the local &lt;code&gt;config&lt;/code&gt; directory into the container: &lt;code&gt;loras, models, presets, prompts, softprompts, training&lt;/code&gt;. If these folders are empty, they will be initialised when the container is run.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you are getting an error about missing files, try clearing these folders and letting the service re-populate them.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Extra launch arguments&lt;/h3&gt; &#xA;&lt;p&gt;Extra launch arguments can be defined in the environment variable &lt;code&gt;EXTRA_LAUNCH_ARGS&lt;/code&gt; (e.g., &lt;code&gt;&#34;--model MODEL_NAME&#34;&lt;/code&gt;, to load a model at launch). The provided default extra arguments are &lt;code&gt;--verbose&lt;/code&gt; and &lt;code&gt;--listen&lt;/code&gt; (which makes the webui available on your local network) and these are set in the &lt;code&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Launch arguments should be defined as a space-separated list, just like writing them on the command line. These arguments are passed to the &lt;code&gt;server.py&lt;/code&gt; module.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Updates&lt;/h3&gt; &#xA;&lt;p&gt;These projects are moving quickly! To update to the most recent version on Docker hub, pull the latest image:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker compose pull&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then recreate the container:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker compose up&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;When the container is launched, it will print out how many commits behind origin the current build is, so you can decide if you want to update it. Docker hub images will be periodically updated, but if you need bleeding edge versions you must build locally.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Build (optional)&lt;/h3&gt; &#xA;&lt;p&gt;The provided &lt;code&gt;docker-compose.yml.build&lt;/code&gt; shows how to build the image locally. You can use it as a reference to modify the original &lt;code&gt;docker-compose.yml&lt;/code&gt;, or you can rename it and use it as-is. Choose the desired variant to build by setting the build &lt;code&gt;target&lt;/code&gt; and then run:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker compose build&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you choose a different variant later, you must &lt;strong&gt;rebuild&lt;/strong&gt; the image.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Developers / Advanced Users&lt;/h3&gt; &#xA;&lt;p&gt;The Dockerfile can be easily modified to compile and run the application from a local source folder. This is useful if you want to do some development or run a custom version. See the Dockerfile itself for instructions on how to do this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Support is not provided for this deployment pathway. It is assumed that you are competent and willing to do your own debugging! Pro-tip: start by placing a &lt;code&gt;text-generation-webui&lt;/code&gt; repo into the project folder.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Standalone Container&lt;/h2&gt; &#xA;&lt;p&gt;NOT recommended, instructions are included for completeness.&lt;/p&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;p&gt;Run a container (and destroy it upon completion):&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker run --it --rm --gpus all -p 7860:7860 atinoda/text-generation-webui:default&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Build and run (optional)&lt;/h3&gt; &#xA;&lt;p&gt;Build the image for the default target and tag it as &lt;code&gt;local&lt;/code&gt; :&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker build --target default -t text-generation-webui:local .&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run the local image (and destroy it upon completion):&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;docker run --it --rm --gpus all -p 7860:7860 text-generation-webui:local&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributions&lt;/h1&gt; &#xA;&lt;p&gt;Contributions are welcomed - please feel free to submit a PR. More variants (e.g., AMD/ROC-M support) and Windows support can help lower the barrier to entry, make this technology accessible to as many people as possible, and push towards democratising the severe impacts that AI is having on our society.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Also - it&#39;s fun to code and LLMs are cool.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;DISCLAIMER&lt;/h1&gt; &#xA;&lt;p&gt;THE SOFTWARE IS PROVIDED &#39;AS IS&#39;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;</summary>
  </entry>
</feed>