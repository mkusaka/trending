<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-23T01:30:45Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mattcurf/ollama-intel-gpu</title>
    <updated>2024-12-23T01:30:45Z</updated>
    <id>tag:github.com,2024-12-23:/mattcurf/ollama-intel-gpu</id>
    <link href="https://github.com/mattcurf/ollama-intel-gpu" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ollama-intel-gpu&lt;/h1&gt; &#xA;&lt;p&gt;This repo illlustrates the use of Ollama with support for Intel ARC GPU based via SYCL. Run the recently released &lt;a href=&#34;https://llama.meta.com/&#34;&gt;Meta llama3.1&lt;/a&gt; or &lt;a href=&#34;https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential&#34;&gt;Microsoft phi3&lt;/a&gt; models on your local Intel ARC GPU based PC using Linux or Windows WSL2.&lt;/p&gt; &#xA;&lt;h2&gt;Screenshot&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mattcurf/ollama-intel-gpu/main/doc/screenshot.png&#34; alt=&#34;screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Prerequisites&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 24.04 or newer (for Intel ARC GPU kernel driver support. Tested with Ubuntu 24.04), or Windows 11 with WSL2 (graphics driver &lt;a href=&#34;https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html&#34;&gt;101.5445&lt;/a&gt; or newer)&lt;/li&gt; &#xA; &lt;li&gt;Installed Docker and Docker-compose tools (for Linux) or Docker Desktop (for Windows)&lt;/li&gt; &#xA; &lt;li&gt;Intel ARC series GPU (tested with Intel ARC A770 16GB and Intel(R) Core(TM) Ultra 5 125H integrated GPU)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;The following will build the Ollama with Intel ARC GPU support, and compose those with the public docker image based on OpenWEB UI from &lt;a href=&#34;https://github.com/open-webui/open-webui&#34;&gt;https://github.com/open-webui/open-webui&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/mattcurf/ollama-intel-gpu&#xA;$ cd ollama-intel-gpu&#xA;$ docker compose up &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Windows WSL2:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/mattcurf/ollama-intel-gpu&#xA;$ cd ollama-intel-gpu&#xA;$ docker-compose -f docker-compose-wsl2.yml up &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then launch your web browser to &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; to launch the web ui. Create a local OpenWeb UI credential, then click the settings icon in the top right of the screen, then select &#39;Models&#39;, then click &#39;Show&#39;, then download a model like &#39;llama3.1:8b-instruct-q8_0&#39; for Intel ARC A770 16GB VRAM&lt;/p&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dgpu-docs.intel.com/driver/client/overview.html&#34;&gt;https://dgpu-docs.intel.com/driver/client/overview.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html&#34;&gt;https://ipex-llm.readthedocs.io/en/latest/doc/LLM/Quickstart/ollama_quickstart.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>