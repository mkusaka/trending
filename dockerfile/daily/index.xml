<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-28T01:26:55Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nogibjj/rust-candle-demos</title>
    <updated>2023-08-28T01:26:55Z</updated>
    <id>tag:github.com,2023-08-28:/nogibjj/rust-candle-demos</id>
    <link href="https://github.com/nogibjj/rust-candle-demos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Demos using Rust Candle&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Hugging Face Candle Demos&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/nogibjj/rust-candle-demos/assets/58792/1b90a2e5-1343-4088-aab3-ce08134ee384&#34; alt=&#34;4 1-candle-framework-rust&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Installation (GPU works for most)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow steps from &lt;a href=&#34;https://huggingface.github.io/candle/guide/installation.html&#34;&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make verify&lt;/code&gt; to ensure CUDA compiler driver and GPU capability&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Verify CUDA:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run the following commands to verify your setup: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;cargo run --features cuda --example whisper --release&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;cargo run --features cuda --example bert --release&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This repo contains a pre-configured &lt;a href=&#34;https://github.com/nogibjj/rust-candle-demos/tree/main/.devcontainer&#34;&gt;GitHub .devcontainer&lt;/a&gt; that sets up CUDA for you. It utilizes the &lt;a href=&#34;https://docs.github.com/en/enterprise-cloud@latest/codespaces/developing-in-codespaces/getting-started-with-github-codespaces-for-machine-learning#configuring-nvidia-cuda-for-your-codespace&#34;&gt;features shown here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/ALqw6vfottY&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/ALqw6vfottY/0.jpg&#34; alt=&#34;Watch A Demo of Using GitHub Codespaces with Rust Candle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Developing in Rust&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow Guide Here: &lt;a href=&#34;https://huggingface.github.io/candle/guide/installation.html&#34;&gt;https://huggingface.github.io/candle/guide/installation.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/5vFPlv6M9Cs&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/5vFPlv6M9Cs/0.jpg&#34; alt=&#34;Rust Hugging Face Candle Hello World CUDA&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Invoke an LLM for Starcoder&lt;/h3&gt; &#xA;&lt;p&gt;Run starcoder:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Checkout the repo: &lt;code&gt;git clone https://github.com/huggingface/candle.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;cd into candle&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cargo run --features cuda --example bigcode --release -- --prompt &#34;python function that adds two numbers&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/g7WGCU3YSXc&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/g7WGCU3YSXc/0.jpg&#34; alt=&#34;Exploring Hugging Face Starcoder in Rust&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Troubleshooting&lt;/h4&gt; &#xA;&lt;p&gt;Necessary for Starcoder model: &lt;code&gt;echo $HUGGING_FACE_HUB_TOKEN &amp;gt; $HOME/.cache/huggingface/token&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or &lt;code&gt;pip install huggingface_hub huggingface-cli login&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;See this &lt;a href=&#34;https://github.com/huggingface/candle/issues/350&#34;&gt;issue&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Also you must allow &lt;a href=&#34;https://huggingface.co/bigcode/starcoderbase-1b&#34;&gt;Gated model access&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Invoke an LLM for Falcon&lt;/h3&gt; &#xA;&lt;h3&gt;CUDA Falcon&lt;/h3&gt; &#xA;&lt;p&gt;cargo run --features cuda --example falcon --release -- --prompt &#34;What is the best type of Apple to eat&#34;?&lt;/p&gt; &#xA;&lt;p&gt;This is an error&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   Compiling candle-examples v0.1.3 (/workspaces/rust-candle-demos/candle/candle-examples)&#xA;    Finished release [optimized] target(s) in 6.23s&#xA;     Running `target/release/examples/falcon --prompt &#39;What is the best type of Apple to eat?&#39;`&#xA;tokenizer.json [00:00:00] [████████████████████████████████████████████████████████████████] 2.61 MiB/2.61 MiB 31.24 MiB/s (0s)&#xA;..del-00001-of-00002.safetensors [00:00:32] [█████████████████████████████████████████████] 9.27 GiB/9.27 GiB 295.19 MiB/s (0s)&#xA;..del-00002-of-00002.safetensors [00:00:16] [██████████████████████████████████████████████████████████████████████████████████████] 4.18 GiB/4.18 GiB 261.20 MiB/s (0s)retrieved the files in 81.9103885s&#xA;loaded the model in 8.3061299s&#xA;starting the inference loop&#xA;Error: DriverError(CUDA_ERROR_NOT_FOUND, &#34;named symbol not found&#34;) when loading is_u32_bf16&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This works...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;RUST_BACKTRACE=1 &amp;amp;&amp;amp; cargo run --example falcon --release -- --prompt &#34;which 100m sprinter won the 1984 olympics&#34;? --use-f32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This does not...because the box running this only has a &amp;lt;8 cap GPU.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;codespace@codespaces-f226cf:/workspaces/rust-candle-demos/candle$ RUST_BACKTRACE=1 &amp;amp;&amp;amp; cargo run --example falcon --release -- --prompt &#34;which 100m sprinter won the 1984 olympics&#34;? --features cuda&#xA;warning: some crates are on edition 2021 which defaults to `resolver = &#34;2&#34;`, but virtual workspaces default to `resolver = &#34;1&#34;`&#xA;note: to keep the current resolver, specify `workspace.resolver = &#34;1&#34;` in the workspace root&#39;s manifest&#xA;note: to use the edition 2021 resolver, specify `workspace.resolver = &#34;2&#34;` in the workspace root&#39;s manifest&#xA;    Finished release [optimized] target(s) in 0.20s&#xA;     Running `target/release/examples/falcon --prompt &#39;which 100m sprinter won the 1984 olympics?&#39; --features cuda`&#xA;error: unexpected argument &#39;--features&#39; found&#xA;&#xA;Usage: falcon &amp;lt;--cpu|--prompt &amp;lt;PROMPT&amp;gt;|--use-f32|--temperature &amp;lt;TEMPERATURE&amp;gt;|--seed &amp;lt;SEED&amp;gt;|--sample-len &amp;lt;SAMPLE_LEN&amp;gt;|--model-id &amp;lt;MODEL_ID&amp;gt;|--revision &amp;lt;REVISION&amp;gt;&amp;gt;&#xA;&#xA;For more information, try &#39;--help&#39;.&#xA;codespace@codespaces-f226cf:/workspaces/rust-candle-demos/candle$ RUST_BACKTRACE=1 &amp;amp;&amp;amp; cargo run --features cuda --example falco&#xA;n --release -- --prompt &#34;which 100m sprinter won the 1984 olympics&#34;? &#xA;warning: some crates are on edition 2021 which defaults to `resolver = &#34;2&#34;`, but virtual workspaces default to `resolver = &#34;1&#34;`&#xA;note: to keep the current resolver, specify `workspace.resolver = &#34;1&#34;` in the workspace root&#39;s manifest&#xA;note: to use the edition 2021 resolver, specify `workspace.resolver = &#34;2&#34;` in the workspace root&#39;s manifest&#xA;   Compiling crossbeam-deque v0.8.3&#xA;   Compiling cudarc v0.9.14&#xA;   Compiling candle-examples v0.1.3 (/workspaces/rust-candle-demos/candle/candle-examples)&#xA;error: failed to run custom build command for `cudarc v0.9.14`&#xA;&#xA;Caused by:&#xA;  process didn&#39;t exit successfully: `/workspaces/rust-candle-demos/candle/target/release/build/cudarc-4b11c4da84f29f3d/build-script-build` (exit status: 101)&#xA;  --- stdout&#xA;  cargo:rerun-if-changed=build.rs&#xA;  cargo:rerun-if-env-changed=CUDA_ROOT&#xA;  cargo:rerun-if-env-changed=CUDA_PATH&#xA;  cargo:rerun-if-env-changed=CUDA_TOOLKIT_ROOT_DIR&#xA;&#xA;  --- stderr&#xA;  thread &#39;main&#39; panicked at &#39;Unable to find `include/cuda.h` under any of: [&#34;/usr&#34;, &#34;/usr/local/cuda&#34;, &#34;/opt/cuda&#34;, &#34;/usr/lib/cuda&#34;, &#34;C:/Program Files/NVIDIA GPU Computing Toolkit&#34;, &#34;C:/CUDA&#34;]. Set the `CUDA_ROOT` environment variable to `$CUDA_ROOT/include/cuda.h` to override path.&#39;, /usr/local/cargo/registry/src/index.crates.io-6f17d22bba15001f/cudarc-0.9.14/build.rs:21:13&#xA;  note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace&#xA;warning: build failed, waiting for other jobs to finish...&#xA;error: failed to run custom build command for `candle-examples v0.1.3 (/workspaces/rust-candle-demos/candle/candle-examples)`&#xA;&#xA;Caused by:&#xA;  process didn&#39;t exit successfully: `/workspaces/rust-candle-demos/candle/target/release/build/candle-examples-f3567e9c5827622f/build-script-build` (exit status: 1)&#xA;  --- stdout&#xA;  cargo:rerun-if-changed=build.rs&#xA;&#xA;  --- stderr&#xA;  Error: cannot find include/cuda.h&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Verify Starcoder&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;     Running `target/release/examples/bigcode --prompt &#39;build a python marco polo function&#39;`&#xA;tokenizer.json [00:00:00] [████████████████████████████████████████████████████████████████] 1.96 MiB/1.96 MiB 17.99 MiB/s (0s)&#xA;model.safetensors [00:00:13] [████████████████████████████████████████████████████████████] 4.24 GiB/4.24 GiB 321.27 MiB/s (0s)retrieved the files in 20.9290968s&#xA;loaded the model in 3.0363304s&#xA;starting the inference loop&#xA;build a python marco polo function to call the marco polo function.&#xA;&#xA;```python&#xA;def marco_polo(x, y, z):&#xA;    return x + y + z&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;marco_polo(1, 2, 3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;marco_polo(x=1, y=2, z=3)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference on AWS&lt;/h3&gt; &#xA;&lt;p&gt;One way to do inference for Rust Candle is to use the &lt;a href=&#34;https://docs.aws.amazon.com/dlami/latest/devguide/what-is-dlami.html&#34;&gt;AWS Deep Learning AMI&lt;/a&gt;, then remotely talk to it via VSCode + SSH. For Rust, a good choice is the &lt;a href=&#34;https://docs.aws.amazon.com/dlami/latest/devguide/overview-base.html&#34;&gt;Deep Learning Base AMI&lt;/a&gt;. A good price point for performance is the &lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/g5/&#34;&gt;G5 Instance Type&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/dSPQtZaQ-BE&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/dSPQtZaQ-BE/0.jpg&#34; alt=&#34;Inference on AWS via Remote SSH and VS Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Steps to Run on AWS&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Launch an &lt;a href=&#34;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html&#34;&gt;Accelerated Computing instance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Select the Deep Learning Base AMI (Ubuntu)&lt;/li&gt; &#xA; &lt;li&gt;SSH and setup rust via &lt;a href=&#34;https://rustup.rs/&#34;&gt;Rustup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;clone candle: &lt;code&gt;git clone https://github.com/huggingface/candle.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://code.visualstudio.com/docs/remote/ssh&#34;&gt;VS Code SSH-Remote plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open the candle folder in VS Code after setting up SSH&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/nogibjj/rust-candle-demos/assets/58792/6f57943f-7665-48f6-b582-fbc2f7325835&#34; alt=&#34;Screenshot 2023-08-25 at 4 57 10 PM&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Notes to get NVCC installed&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;sudo apt-get install cuda-nvcc-12-2&lt;/li&gt; &#xA; &lt;li&gt;export PATH=$PATH:/usr/local/cuda-12.2/bin&lt;/li&gt; &#xA; &lt;li&gt;export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH&lt;/li&gt; &#xA; &lt;li&gt;sudo apt-get install cuda-toolkit-12-2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;ls /usr/local/cuda/lib64/libnvrtc.so&#xA;ls /usr/local/cuda/lib64/libcurand.so&#xA;ls /usr/local/cuda/lib64/libcublas.so&#xA;ls /usr/local/cuda/lib64/libcublasLt.so&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Potential Development and Deployment Architecture&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/nogibjj/rust-candle-demos/assets/58792/5d524446-8017-42a9-9899-56eb9a4565f1&#34; alt=&#34;4 7-exploring-remote-dev-aws&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;References&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.github.com/en/codespaces/developing-in-codespaces/getting-started-with-github-codespaces-for-machine-learning&#34;&gt;GitHub CodeSpaces CUDA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables&#34;&gt;HUGGING_FACE_HUB_TOKEN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/blogs/compute/hosting-hugging-face-models-on-aws-lambda/&#34;&gt;Serverless Hosting Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>r0binak/MTKPI</title>
    <updated>2023-08-28T01:26:55Z</updated>
    <id>tag:github.com,2023-08-28:/r0binak/MTKPI</id>
    <link href="https://github.com/r0binak/MTKPI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🧰 Multi Tool Kubernetes Pentest Image&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MTKPI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/r0binak/MTKPI/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/r0binak/MTKPI/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/r0binak/MTKPI&#34; alt=&#34;Github Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/r0binak/MTKPI/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/r0binak/mtkpi&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/r0binak/mtkpi?logo=docker&#34; alt=&#34;Docker Pulls MTKPI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/r0binak/MTKPI/main/images/logo.jpg&#34; alt=&#34;Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MTKPI&lt;/strong&gt; – Multi Tool Kubernetes Pentest Image. This docker image contains all the most popular and necessary tools for Kubernetes penetration testing. Everything you need at your fingertips.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Image was generated by &lt;a href=&#34;https://www.sberbank.com/promo/kandinsky/&#34;&gt;Kandinsky 2.2&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;When you&#39;re pentesting a Kubernetes cluster, you&#39;ll certainly use automated tools to perform the checks. But what if your cluster is network-limited and you can&#39;t download the tools you need inside the Pod? Or a read-only container file system? In this case, the only solution is to use a ready-to-use image, inside of which there are all the tools you need. This image includes all possible popular tools for pentesting a Kubernetes cluster, including those with automatic checks.&lt;/p&gt; &#xA;&lt;h2&gt;Threat Matrix for Kubernetes&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/r0binak/MTKPI/main/images/matrix.png&#34; alt=&#34;Logo&#34;&gt; MTKPI covers most of the techniques described in Microsoft Threat Matrix for Kubernetes. This in turn provides a wide range of pentesting possibilities. If necessary, you can add the necessary tools to the image and increase the coverage of the matrix.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s inside?&lt;/h2&gt; &#xA;&lt;h3&gt;Shell via web&lt;/h3&gt; &#xA;&lt;p&gt;Often, when pentesting Kubernetes Cluster, you have a developer Service Account with limited permissions. In other words, you don&#39;t have sufficient permissions to run &lt;code&gt;pods/exec&lt;/code&gt;, which means you just can&#39;t get inside the container. However, it&#39;s more common for developers to have rights to create &lt;code&gt;port-forward&lt;/code&gt;. This is why I used &lt;a href=&#34;https://github.com/tsl0922/ttyd&#34;&gt;ttyd&lt;/a&gt; as the base image ― it is a simple command-line tool for sharing terminals over the web.&lt;/p&gt; &#xA;&lt;h3&gt;Tools&lt;/h3&gt; &#xA;&lt;p&gt;For convenience, I also have made a list of all possible tools that can be useful when pentesting Kubernetes and packed it in an image:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/brompwnie/botb&#34;&gt;botb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cyberark/kubeletctl&#34;&gt;kubeletctl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cyberark/kubesploit&#34;&gt;kubesploit agent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cdk-team/CDK&#34;&gt;CDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/inguardians/peirates&#34;&gt;peirates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/liamg/traitor&#34;&gt;traitor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ctrsploit/ctrsploit&#34;&gt;ctrsploit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/quarkslab/kdigger&#34;&gt;kdigger&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/&#34;&gt;kubectl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sleventyeleven/linuxprivchecker&#34;&gt;linuxprivchecker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stealthcopter/deepce&#34;&gt;deepce&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://helm.sh&#34;&gt;helm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aquasecurity/kube-hunter&#34;&gt;kube-hunter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aquasecurity/kube-bench&#34;&gt;kube-bench&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/carlospolop/DDexec&#34;&gt;DDexec&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Bypass signature engine&lt;/h3&gt; &#xA;&lt;p&gt;Sometimes, runtime security tools are found in Kubernetes clusters that work on a signature-based approach. Security tools like Falco and Tracee are quite easy to bypass, as their behavior is predefined by rules and signatures. There are quite a few ways to do this, one of the simplest being to rename executables. This is the method used in MTKPI.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;kubectl&lt;/code&gt; → &lt;code&gt;k&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python3&lt;/code&gt; → &lt;code&gt;pton3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;curl&lt;/code&gt; → &lt;code&gt;kurl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;wget&lt;/code&gt; → &lt;code&gt;vget&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can read more about the ways to bypass Falco &lt;a href=&#34;https://github.com/blackberry/Falco-bypasses&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;For fast deployment, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/r0binak/MTKPI/main/deploy/mtkpi.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pod:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: mtkpi-pod&#xA;  labels:&#xA;    app: mtkpi&#xA;spec:&#xA;  containers:&#xA;  - name: mtkpi-pod&#xA;    image: r0binak/mtkpi:v1&#xA;    ports:&#xA;    - containerPort: 7681&#xA;    securityContext:&#xA;      readOnlyRootFilesystem: true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Service:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: mtkpi-svc&#xA;  labels:&#xA;    app: mtkpi&#xA;spec:&#xA;  type: ClusterIP&#xA;  ports:&#xA;  - port: 7681&#xA;    protocol: TCP&#xA;  selector:&#xA;    app: mtkpi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To access the container, just run the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;kubectl port-forward mtkpi-pod 7681:7681&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open in your browser:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;localhost:7681&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/r0binak/MTKPI/main/images/in-action.png&#34; alt=&#34;In action&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you liked this, I&#39;d appreciate some PR 🙂&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/madhuakula/hacker-container&#34;&gt;https://github.com/madhuakula/hacker-container&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/antitree/cmd_and_kubectl_demos/tree/master/images/botty&#34;&gt;https://github.com/antitree/cmd_and_kubectl_demos/tree/master/images/botty&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/raesene/alpine-containertools&#34;&gt;https://github.com/raesene/alpine-containertools&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>kiali/kiali-operator</title>
    <updated>2023-08-28T01:26:55Z</updated>
    <id>tag:github.com,2023-08-28:/kiali/kiali-operator</id>
    <link href="https://github.com/kiali/kiali-operator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kiali operator that is used to install, manage, and update Kiali deployments.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kiali Operator&lt;/h1&gt; &#xA;&lt;div class=&#34;paragraph&#34;&gt; &#xA; &lt;p&gt;&lt;span class=&#34;image&#34;&gt;&lt;a class=&#34;image&#34; href=&#34;https://raw.githubusercontent.com/kiali/kiali-operator/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache2-blue.svg?sanitize=true&#34; alt=&#34;Apache 2.0 license&#34;&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div class=&#34;paragraph&#34;&gt; &#xA; &lt;p&gt;This contains the Kiali Operator source. It has a small &lt;a href=&#34;https://raw.githubusercontent.com/kiali/kiali-operator/master/Makefile&#34; class=&#34;bare&#34;&gt;Makefile&lt;/a&gt; whose only job is to build the operator image and push the built image to Quay.io. Thus you can build releases directly from this repo.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div class=&#34;paragraph&#34;&gt; &#xA; &lt;p&gt;This git repo is meant to be used within the &lt;a href=&#34;https://github.com/kiali/kiali&#34;&gt;kiali/kiali&lt;/a&gt; parent repo (as its &#34;operator&#34; subdirectory). That parent repo has additional Makefile targets to do more things with this operator such as run molecule tests and push the operator image to remote clusters for testing.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div class=&#34;paragraph&#34;&gt; &#xA; &lt;p&gt;For more details on how developers can consume and work with this Kiali Operator git repo, see &lt;a href=&#34;https://raw.githubusercontent.com/kiali/kiali-operator/master/DEVELOPING.adoc&#34; class=&#34;bare&#34;&gt;DEVELOPING.adoc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>