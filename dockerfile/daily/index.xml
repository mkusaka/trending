<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-02T01:33:36Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>aorwall/SWE-bench-docker</title>
    <updated>2024-06-02T01:33:36Z</updated>
    <id>tag:github.com,2024-06-02:/aorwall/SWE-bench-docker</id>
    <link href="https://github.com/aorwall/SWE-bench-docker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Docker based solution of the SWE-bench evaluation framework&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This is a Dockerfile based solution of the &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/tree/main/swebench/harness&#34;&gt;SWE-Bench evaluation framework&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The solution is designed so that each &#34;testbed&#34; for testing a version of a repository is built in a separate Docker image. Each test is then run in its own Docker container. This approach ensures more stable test results because the environment is completely isolated and is reset for each test. Since the Docker container can be recreated each time, there&#39;s no need for reinstallation, speeding up the benchmark process.&lt;/p&gt; &#xA;&lt;h2&gt;Validation&lt;/h2&gt; &#xA;&lt;h3&gt;SWE-Bench_Lite&lt;/h3&gt; &#xA;&lt;p&gt;Docker images for testbeds used in the &lt;code&gt;SWE-Bench_Lite&lt;/code&gt; dataset has been built and tested on &lt;em&gt;gold predictions&lt;/em&gt;. 2 benchmark instances are currently failing. See results in the &lt;a href=&#34;https://github.com/aorwall/SWE-bench-docker/raw/main/evaluations/SWE-bench_Lite_golden&#34;&gt;evaluations/SWE-bench_Lite_golden&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;SWE-Bench&lt;/h3&gt; &#xA;&lt;p&gt;Docker images for testbeds used in the &lt;code&gt;SWE-Bench&lt;/code&gt; dataset has been built and tested on the &lt;code&gt;check-harness&lt;/code&gt; predictions &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench/tree/main/docs/20240415_eval_bug&#34;&gt;published by SWE-bench&lt;/a&gt;. 10 benchmark instances are currently failing. See results in the &lt;a href=&#34;https://github.com/aorwall/SWE-bench-docker/raw/main/evaluations/SWE-bench_Lite_golden_harness&#34;&gt;evaluations/SWE-bench_check_harness&lt;/a&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Comparing results from other agents&lt;/h3&gt; &#xA;&lt;p&gt;I have tested running Docker benchmarks on the SWE-Agents GPT-4 benchmark and Auto Code Rover&#39;s first benchmark run.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/princeton-nlp/SWE-agent&#34;&gt;SWE-Agent&lt;/a&gt; GPT-4 predictions yield exactly the same &lt;a href=&#34;https://github.com/aorwall/SWE-bench-docker/raw/main/evaluations/20240402_sweagent_gpt4&#34;&gt;results of 18% (54) resolved issues&lt;/a&gt; as SWE-Agent&#39;s own &lt;a href=&#34;https://github.com/swe-bench/experiments/raw/main/evaluation/lite/20240402_sweagent_gpt4/results/results.json&#34;&gt;results&lt;/a&gt;, which seems to show that the Docker image approach works with the same accuracy.&lt;/p&gt; &#xA;&lt;p&gt;However, the Docker benchmark provides better results for &lt;a href=&#34;https://github.com/nus-apr/auto-code-rover&#34;&gt;AutoCodeRover&lt;/a&gt;. In AutoCodeRover&#39;s own benchmarks, they achieve 16.00% (48), 15.67% (47), and 16.67% (50) resolved issues. In swe-bench-docker, the same predictions result in &lt;a href=&#34;https://github.com/aorwall/SWE-bench-docker/raw/main/evaluations/auto-code-rover-run-1&#34;&gt;18.00% (54)&lt;/a&gt;, &lt;a href=&#34;https://github.com/aorwall/SWE-bench-docker/raw/main/evaluations/auto-code-rover-run-2&#34;&gt;19% (57)&lt;/a&gt; and &lt;a href=&#34;https://github.com/aorwall/SWE-bench-docker/raw/main/evaluations/auto-code-rover-run-3&#34;&gt;19% (57)&lt;/a&gt; resolved issues. This adds up to a pass@3 of 26% (78) compared to 22.33% (67) reported in the &lt;a href=&#34;https://arxiv.org/pdf/2404.05427&#34;&gt;AutoCodeRover paper&lt;/a&gt;. This suggests that other agents&#39; benchmarks may show lower results than they actually achieve because it&#39;s challenging to conduct evaluations with completely accurate results.&lt;/p&gt; &#xA;&lt;h2&gt;Docker images types&lt;/h2&gt; &#xA;&lt;p&gt;There are currently three different Docker images for running benchmarks.&lt;/p&gt; &#xA;&lt;h3&gt;Conda&lt;/h3&gt; &#xA;&lt;p&gt;Testbeds are set up in a Conda environment similar to the original SWE-bench environment.&lt;/p&gt; &#xA;&lt;h3&gt;Pyenv&lt;/h3&gt; &#xA;&lt;p&gt;Since each benchmark is tested in its own container, using Conda may be overkill. Testbeds are set up with only the correct Python version installed via Pyenv. This approach has been shown to result in fewer erroneous benchmark instances in repositories where it has been tested, and the image becomes smaller. Currently, &lt;code&gt;django&lt;/code&gt;, &lt;code&gt;psf/requests&lt;/code&gt; and &lt;code&gt;scikit-learn&lt;/code&gt; use this type of Docker image. Hopefully, more repositories can be run this way.&lt;/p&gt; &#xA;&lt;h3&gt;Instance image&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;code&gt;scikit-learn&lt;/code&gt;, some benchmarks seem to fail because Cython code isn&#39;t compiled. To avoid building the project before each test, an image is built for each benchmark instance.&lt;/p&gt; &#xA;&lt;h2&gt;Run evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;run_evaluation.py&lt;/code&gt; to evaluate a predictions file. A log for each test is written to log_dir in the same format as in the SWE-bench evaluation tools, and the same tooling can then be used to generate a report.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_evaluation.py &#xA;    --predictions_path [Required]  Path to the predictions file &#xA;    --log_dir          [Required]  Path to directory to save evaluation log files &#xA;    --swe_bench_tasks  [Required]  Path to SWE-bench task instances file or dataset &#xA;    --namespace        [Optional]  Namespace of the Docker repository &#xA;    --log_suffix       [Optional]  Suffix to append to log file names&#xA;    --skip_existing    [Optional]  Skip evaluating task instances with logs that already exist&#xA;    --timeout          [Optional]  Timeout for installation + test script execution&#xA;    --num_processes    [Optional]  Number of processes to run in parallel (-1 for unlimited)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pull Docker images&lt;/h3&gt; &#xA;&lt;p&gt;It might be worth pulling all Images before running the script to achieve more consistent timing in the evaluation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/pull_docker_images.sh [Dockerfiles directory] [Namespace]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Build Docker images&lt;/h2&gt; &#xA;&lt;h3&gt;Generate Dockerfiles&lt;/h3&gt; &#xA;&lt;p&gt;Generates Dockerfiles for all test beds in a SWE-Bench benchmark dataset. These can then be used to build Docker images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_dockerfile_generator.py &#xA;    --swe_bench_tasks  [Required]  Path to SWE-bench task instances file or dataset &#xA;    --namespace        [Required]  Namespace of the Docker repository &#xA;    --docker_dir       [Required]  Path to the directory where the Dockerfiles will be saved&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build Docker images&lt;/h3&gt; &#xA;&lt;p&gt;This script builds Docker images from all Dockerfiles.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/build_docker_images.sh [Dockerfiles directory] [Namespace]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Push Docker images&lt;/h3&gt; &#xA;&lt;p&gt;This script builds Docker images from all Dockerfiles.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/push_docker_images.sh [Dockerfiles directory] [Namespace]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;h3&gt;Run single instance&lt;/h3&gt; &#xA;&lt;p&gt;Run a single instance and print logs to stdout.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_single_instance.py &#xA;    --instance_id      [Required]  Instance ID of the task to run&#xA;    --swe_bench_tasks  [Optional]  Path to SWE-bench task instances file or dataset (default is princeton-nlp/SWE-bench_Lite)&#xA;    --namespace        [Optional]  Namespace of the Docker repository&#xA;    --predictions_path [Optional]  Path to the predictions file, if not set the golden patch will be used&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build single Docker image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/build_docker_images.sh [Namespace] [Testbed directory]&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>LinkedInLearning/test_et_TDD-3853126</title>
    <updated>2024-06-02T01:33:36Z</updated>
    <id>tag:github.com,2024-06-02:/LinkedInLearning/test_et_TDD-3853126</id>
    <link href="https://github.com/LinkedInLearning/test_et_TDD-3853126" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Java : Tests et TDD&lt;/h1&gt; &#xA;&lt;p&gt;Ce dossier Repository est lié au cours &lt;code&gt;Java : Tests et TDD&lt;/code&gt;. Le cours entier est disponible sur &lt;a href=&#34;https://www.linkedin.com/learning/java-tests-et-tdd&#34;&gt;LinkedIn Learning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://media.licdn.com/dms/image/D4E0DAQEhvghtnwPWUQ/learning-public-crop_675_1200/0/1714725438492?e=2147483647&amp;amp;v=beta&amp;amp;t=cKbvUMJUCCkgl8Dqo8iOllM5gg3yTFlJSRMt-apvNW4&#34; alt=&#34;Nom final de la formation&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dans cette formation pour développeurs de tous niveaux, Sylvain Labasse vous guide dans les tests et la méthode TDD (Test Driven Development) en Java. Apprenez JUnit, écrivez des tests efficaces, gérez les exceptions et validez les collections. Avec TDD, passez du test rouge au vert, personnalisez les libellés, réduisez la redondance et utilisez les tags. Simulez les dépendances avec Mockito, explorez les tests d&#39;intégration, les tests d&#39;API, les tests fonctionnels avec Selenium WebDriver, la méthode BDD et les tests de charge avec JMeter.&lt;/p&gt; &#xA;&lt;p&gt;La meilleure façon d&#39;apprendre un langage est de l&#39;utiliser dans la pratique. C&#39;est pourquoi ce cours est intégré à GitHub Codespaces, un environnement de développement instantané « dans le nuage » qui offre toutes les fonctionnalités de votre IDE préféré sans nécessiter de configuration sur une machine locale. Avec Codespaces, vous pouvez vous exercer à partir de n&#39;importe quelle machine, à tout moment, tout en utilisant un outil que vous êtes susceptible de rencontrer sur votre lieu de travail. Consultez la vidéo &#34;Utilisation de Codespaces GitHub dans le cadre de ce cours&#34; pour savoir comment démarrer.&lt;/p&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;p&gt;Pour utiliser GitHub Codespaces et faire apparaître l&#39;environnement du cours dans votre navigateur, il vous suffit de cliquer sur :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork à droite du nom du dépôt, pour faire votre fork de ce dépôt,&lt;/li&gt; &#xA; &lt;li&gt;Le bouton &amp;lt;&amp;gt; Code, pour faire sélectionner l&#39;onglet Codespaces,&lt;/li&gt; &#xA; &lt;li&gt;Le bouton Create codespace on main, pour initier et faire apparaître un codespace.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Ce dépôt possède une ou deux branches par leçon (vidéo de cours). Vous pouvez naviguer dans les versions grâce au menu d&#39;accès aux branches accessible en cliquant sur le nom de branche en bas à gauche de VisualStudio Code.&lt;/p&gt; &#xA;&lt;h2&gt;Branches&lt;/h2&gt; &#xA;&lt;p&gt;Les branches sont structurées de manière à correspondre aux vidéos du cours. La convention de nommage est : CHAPITRE#_VIDEO#. Par exemple, la branche nommée 02_03 correspond au second chapitre, et à la troisième vidéo de ce chapitre. Certaines branches ont un état de départ et de fin. La branche 02_03_d correspond au code du début de la vidéo. La branche 02_03_f correspond au code à la fin de la vidéo.&lt;/p&gt; &#xA;&lt;p&gt;En changeant de branche, après avoir fait des changements, il se peut que vous ayez ce message :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;erreur : Vos changements locaux sur les fichiers suivants seront écrasés par le basculement (checkout) : [fichiers]&#xA;Validez (commit) vos changement ou réservez (stash) les avant de changer de branche.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pour résoudre ce problème :&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ajoutez vos changements avec la commande : git add .&lt;/li&gt; &#xA; &lt;li&gt;Validez les avec commande: git commit -m &#34;un message&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Formateur&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sylvain Labasse&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Retrouvez mes autres formations sur &lt;a href=&#34;https://www.linkedin.com/learning/instructors/sylvain-labasse&#34;&gt;LinkedIn Learning&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>