<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-12T01:38:28Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>l3v11/Jackett-heroku</title>
    <updated>2022-06-12T01:38:28Z</updated>
    <id>tag:github.com,2022-06-12:/l3v11/Jackett-heroku</id>
    <link href="https://github.com/l3v11/Jackett-heroku" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Jackett • Heroku edition&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Jackett on Heroku&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Host your very own &lt;a href=&#34;https://github.com/Jackett/Jackett&#34;&gt;Jackett&lt;/a&gt; on Heroku&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Deploying with CLI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone the &lt;a href=&#34;https://github.com/l3v11/Jackett-heroku&#34;&gt;repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://devcenter.heroku.com/articles/heroku-cli&#34;&gt;Heroku CLI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd Jackett-heroku&#xA;heroku login&#xA;heroku apps:create $APP&#xA;heroku stack:set container -a $APP&#xA;heroku git:remote -a $APP&#xA;git push heroku main --force&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deploying with Workflow&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork the &lt;a href=&#34;https://github.com/l3v11/Jackett-heroku&#34;&gt;repo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;On the forked repo, go to &lt;em&gt;Settings&lt;/em&gt; -&amp;gt; &lt;em&gt;Secrets&lt;/em&gt; and click on the &lt;em&gt;New repository secret&lt;/em&gt; button&lt;/li&gt; &#xA; &lt;li&gt;Now enter the vars one by one with value&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;HEROKU_API_KEY&lt;/code&gt;: Get the API Key from Heroku &lt;a href=&#34;https://dashboard.heroku.com/account&#34;&gt;Account Settings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;HEROKU_EMAIL&lt;/code&gt;: Email address of your Heroku Account&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;HEROKU_APP_NAME&lt;/code&gt;: Name of your Heroku App. It must be unique on Heroku.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Then go to the &lt;em&gt;Actions&lt;/em&gt; tab on your repo&lt;/li&gt; &#xA; &lt;li&gt;Select &lt;em&gt;Deploy to Heroku&lt;/em&gt; from the &lt;em&gt;All workflow&lt;/em&gt; list&lt;/li&gt; &#xA; &lt;li&gt;Click on &lt;em&gt;Run workflow&lt;/em&gt; -&amp;gt; &lt;em&gt;Run workflow&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;After that turn on the app dyno&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Updating Jackett&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scheduled to update at 12:00 on every Sunday&lt;/li&gt; &#xA; &lt;li&gt;If you don&#39;t like to update automatically, go to &lt;em&gt;Settings&lt;/em&gt; -&amp;gt; &lt;em&gt;Actions&lt;/em&gt; -&amp;gt; &lt;em&gt;General&lt;/em&gt; on the forked repo and select &lt;em&gt;Disable Actions&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; For &lt;a href=&#34;https://github.com/l3v11/Jackett-heroku#deploying-with-workflow&#34;&gt;Deploying with Workflow&lt;/a&gt; method only&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>Guilherme-Silveira/bigdata-k8s</title>
    <updated>2022-06-12T01:38:28Z</updated>
    <id>tag:github.com,2022-06-12:/Guilherme-Silveira/bigdata-k8s</id>
    <link href="https://github.com/Guilherme-Silveira/bigdata-k8s" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bigdata-k8s&lt;/h1&gt; &#xA;&lt;p&gt;Esse repositório tem como propósito criar um ambiente big data do zero no Kubernetes. As ferramentas utilizadas nesse projeto são:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Minio (Data Lake)&lt;/li&gt; &#xA; &lt;li&gt;Airflow (Orquestrador)&lt;/li&gt; &#xA; &lt;li&gt;Apache Kafka (Streaming)&lt;/li&gt; &#xA; &lt;li&gt;Hive Metastore (Metadados - Tabelas)&lt;/li&gt; &#xA; &lt;li&gt;Apache Spark on K8S (Processamento Batch e Streaming)&lt;/li&gt; &#xA; &lt;li&gt;JupyterHub (Processamento - Integrado com Apache Spark)&lt;/li&gt; &#xA; &lt;li&gt;Delta (Delta Lake integrado com o Apache Spark e Jupyterhub)&lt;/li&gt; &#xA; &lt;li&gt;Trino (Virtualizacão de dados - Camada SQL)&lt;/li&gt; &#xA; &lt;li&gt;Superset (Data Viz)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Arquitetura do projeto: &lt;img src=&#34;https://user-images.githubusercontent.com/40548889/170894059-884e771f-3970-427e-a2a2-47f726d8fe8f.png&#34; alt=&#34;architeture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Todo esse ambiente foi criado em um cluster Kubernetes local na minha máquina pessoal utilizando o K3D, que utiliza o Docker para simular um cluster Kubernetes multi-node rodando em containers. Porém, todos os manifestos e helm charts criados nesse repositório podem ser utilizados em servicos gerenciados de Kubernetes de Cloud Providers (EKS, GKE, AKS), os únicos pré-requisitos seriam os seguintes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/&#34;&gt;Ingress Controller&lt;/a&gt; configurado&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/storage-classes/&#34;&gt;Storage Class&lt;/a&gt; configurado&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Nesse tutorial, todo o ambiente será criado utilizando o K3D para rodar em uma máquina local.&lt;/p&gt; &#xA;&lt;p&gt;Pré-Requisitos:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.docker.com/products/docker-desktop/&#34;&gt;Docker&lt;/a&gt; (No meu caso, o meu PC é um mac, mas você pode baixar a versão correspondente do seu Sistema Operacional)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;OBS: o ambiente pode ser um pouco pesado, então em alguns casos será necessário mudar os valores de memória default dos manifests/helm charts.&lt;/p&gt; &#xA;&lt;p&gt;Após a instalacão do Docker, teremos o necessário para configurar nosso ambiente big data, então bora para o tutorial!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;K3D&lt;/h1&gt; &#xA;&lt;p&gt;Para instalar o K3D, execute um dos comandos abaixo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;curl -s https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Após instalado, já podemos criar um cluster. Para criar um cluster Kubernetes usando o K3D, execute o seguinte comando:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;k3d cluster create --agents 3 -p &#39;80:30000&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ele criará um cluster Kubernetes com 1 master e 3 worker nodes. Lembrando que você pode colocar a qualquer quantidade de worker nodes, mas nesse tutorial, vamos seguir com 3.&lt;/p&gt; &#xA;&lt;p&gt;Quando o K3D é instalado ele automaticamente instala o &lt;code&gt;kubectl&lt;/code&gt; junto, então após o comando de criacão do cluster, você pode confirmar se tudo aconteceu conforme o esperado executando o seguinte comando:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl get nodes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OBS: Esse Bind da porta 80 para a 30000 do cluster Kubernetes criado no Docker será explicado no próximo tópico.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Ingress Controller&lt;/h1&gt; &#xA;&lt;p&gt;O Ingress Controller é um componente que permite o acesso externo a pods que estão executando dentro do Kubernetes. Em outras palavras, o Ingress Controller será nossa porta de acesso ao cluster. Mas espera aí, é possível utilizá-lo em um ambiente rodando em uma máquina local?&lt;/p&gt; &#xA;&lt;p&gt;Sim, é possível! Não é um método muito &#34;elegante&#34;, mas é muito útil para o dia a dia. O mais interessante é que tudo que for aplicado nesse ambiente local, seria praticamente da mesma em um Cloud Provider. A única diferenca é que o Cloud Provider criaria um Load Balancer e Zonas DNS e aqui, nós utilizaremos o redirecionamento de porta do Docker e o arquivos Hosts da máquina local, mas a nível usuário e manisfestos, eles serão exatamente os mesmos.&lt;/p&gt; &#xA;&lt;p&gt;Nesse tutorial, utilizaremos o Nginx como nosso Ingress Controller. Para criá-lo, a partir do diretório raiz do projeto, execute os seguintes comandos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ingress-controller&#xA;kubectl apply -f ingress-controller.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Dentro desse manifest, o Service do Nginx foi configurado como NodePort, tendo como bind a porta 30000 e aqui está a mágica de como tudo isso vai permitir acessos externos ao servicos do cluster.&lt;/p&gt; &#xA;&lt;p&gt;No passo anterior, na criacao do cluster, definimos que toda requisicão feita na porta 80 da máquina local terá seu tráfego redirecionado para a porta 30000 do cluster Kubernetes que está executando dentro do Docker e agora, configuramos que o servico que está rodando nessa porta dentro do cluster Kubernetes será o Nginx, ou seja, o acesso externo terá o seguinte fluxo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/40548889/170897468-e252bd8a-db5a-41d4-8190-cbd1102d9c74.png&#34; alt=&#34;network_flow&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Arquivo Hosts&lt;/h1&gt; &#xA;&lt;p&gt;O arquivo hosts da sua máquina permite que você adicione uma relacão de IP&#39;s e &#34;registros DNS&#34; que sua máquina irá traduzir para estabelecer uma comunicacão. Se você quiser utilizar os valores padrões que foram definidos nesse projeto, execute os seguintes passos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo vim /etc/hosts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Adicione o seguinte conteúdo ao arquivo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;127.0.0.1 minio.silveira.com&#xA;127.0.0.1 console-minio.silveira.com&#xA;127.0.0.1 trino.silveira.com&#xA;127.0.0.1 kafka.silveira.com&#xA;127.0.0.1 kafka-ui.silveira.com&#xA;127.0.0.1 superset.silveira.com&#xA;127.0.0.1 airflow.silveira.com&#xA;127.0.0.1 jupyterhub.silveira.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Lembrando que é possível usar seu próprios &#34;registros DNS&#34; customizados, mas se esse for o caso, lembre-se de mudar os valores necessários nos manifests/helm charts durante o deploy de cada uma das ferramentas da stack.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Namespace&lt;/h1&gt; &#xA;&lt;p&gt;No intuito de organizar, todo esse projeto vai ser criado em uma namespace específica do Kubernetes chamada bigdata. Para criá-la, execute o seguinte comando no diretório raiz do projeto:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash create-namespace.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Minio&lt;/h1&gt; &#xA;&lt;p&gt;O &lt;a href=&#34;https://min.io/&#34;&gt;Minio&lt;/a&gt; é um Object Storage nativo para Kubernetes. O fato curioso é que ele utiliza o protocolo S3 para comunicacão, então é quase que uma solucão de S3 on-premises. Ele será o nosso Data Lake, onde todos os dados serão armazenados. Ele possui dois métodos principais de instalacão:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Helm Chart&lt;/li&gt; &#xA; &lt;li&gt;Operator&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Nesse tutorial, instalaremos o Minio via Helm Chart.&lt;/p&gt; &#xA;&lt;p&gt;Para instalá-lo, a partir do diretório raiz do projeto, execute os seguintes comandos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd minio&#xA;bash install-minio.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Esse script simplesmente faz o download do repositório do Helm do Minio e instala-o utilizando como paramêtro o arquivo values.yaml que está dentro do diretório. Nesse arquivo são definidas todas as propriedades que Minio vai possuir (memória, storage, quantidade de nodes). Os valores default podem não atender exatamente o seu caso de uso, então sinta-se livre para modificar esse arquivo conforme sua necessidade.&lt;/p&gt; &#xA;&lt;p&gt;Para validar que tudo ocorreu de acordo com o previsto, você pode acessar, no seu navegador, a console do Minio na seguinte URL:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://console-minio.silveira.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;O usuário e senha serão os seguintes, respectivamente:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;silveira&lt;/li&gt; &#xA; &lt;li&gt;guilherme@123&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;OBS: O values.yaml será utilizado para todas as ferramentas que forem instaladas via Helm, então sinta-se livre para modificar as configuracões desse arquivo de acordo com seu caso de uso para qualquer ferramenta que o utilizar, por exemplo:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ingress host (DNS)&lt;/li&gt; &#xA; &lt;li&gt;usuario&lt;/li&gt; &#xA; &lt;li&gt;senha&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Hive Metastore&lt;/h1&gt; &#xA;&lt;p&gt;O Hive Metastore não está descrito na arquitetura, mas é um componente importantíssimo para todo esse ambiente Big Data. O Hive Metastore é responsável por armazenar todos os metadados de tabelas que forem criadas via Spark e Trino. Ele necessita de um banco de dados relacional para armazenar esses metadados, então além do Hive Metastore, um deploy do MariaDB também será realizado. Para instalar o Hive Metastore, a partir do diretório raiz do projeto, execute os comandos abaixo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd hive-metastore&#xA;bash create-configmap.sh&#xA;kubectl apply -f maria_pvc.yaml&#xA;kubectl apply -f maria_deployment.yaml&#xA;kubectl apply -f hive-initschema.yaml&#xA;kubectl apply -f metastore.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;OBS: Caso o usuário e senha do Minio tenham sido alterados no passo anterior, será necessário executar os seguintes passos a partir do diretório raiz do projeto:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd hive-metastore/build&#xA;vim core-site.xml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modifique os seguintes paramêtros no arquivo para os valores configurados no Minio:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;property&amp;gt;&#xA;    &amp;lt;name&amp;gt;fs.s3a.awsAccessKeyId&amp;lt;/name&amp;gt;&#xA;    &amp;lt;value&amp;gt;silveira&amp;lt;/value&amp;gt;&#xA;&amp;lt;/property&amp;gt;&#xA;&#xA;&amp;lt;property&amp;gt;&#xA;    &amp;lt;name&amp;gt;fs.s3a.awsSecretAccessKey&amp;lt;/name&amp;gt;&#xA;    &amp;lt;value&amp;gt;guilherme@123&amp;lt;/value&amp;gt;&#xA;&amp;lt;/property&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Salve o arquivo e agora execute os comandos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ..&#xA;bash create-configmap.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Após isso, execute os comandos usando &lt;code&gt;kubectl&lt;/code&gt; descritos no ínicio desse tópico.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Trino&lt;/h1&gt; &#xA;&lt;p&gt;O Trino é uma ferramenta de virtualizacão de dados que usa a linguagem SQL para interagir com diversas fontes de dados. Nesse tutorial, o Trino vai estar configurado com o Hive Metastore e com o Delta para se interagir com os dados armazenados no Minio. Para instalá-lo, a partir do diretório raiz do projeto, execute os seguintes comandos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd trino&#xA;bash install-trino.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Se os valores utilizados de ingress forem os defaults configurados nesse repositório, tente acessar no seu navegador a seguinte URL para validar se o Trino está funcionando:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://trino.silveira.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Caso não sejam os valores default, use a URL customizada que foi definida.&lt;/p&gt; &#xA;&lt;p&gt;Obs: Caso você não esteja utilizando o usuário e senha padrões definidos nesse tutorial para o Minio, você deve modificar os parâmetros de Access Key e Secret Key dentro do arquivo values.yaml para os conectores do Hive Metastore e do Delta Lake.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Jupyterhub&lt;/h1&gt; &#xA;&lt;p&gt;O Jupyterhub é uma ferramenta enterprise que te permite criar notebooks, mas com a possibilidade de segregá-lo em um contexto de usuários. Resumindo, no Kubernetes, ele é capaz de criar pods para cada usuário e cada usuário tem seus próprios notebooks (armazenados em PVC&#39;s separados) Para instalá-lo, a partir do diretório raiz, execute os seguintes comandos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd jupyter&#xA;bash install-jupyterhub.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Se os valores utilizados de ingress forem os defaults configurados nesse repositório, tente acessar no seu navegador a seguinte URL para validar se o Jupyterhub está funcionando:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://jupyterhub.silveira.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Caso não sejam os valores default, use a URL customizada que foi definida.&lt;/p&gt; &#xA;&lt;p&gt;OBS: Todo usuário pode ser criado sem o uso de senha, mas caso seja necessário implantar o controle de acesso, veja como aplicá-lo &lt;a href=&#34;https://zero-to-jupyterhub.readthedocs.io/en/latest/administrator/authentication.html&#34;&gt;aqui&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Spark on K8S&lt;/h1&gt; &#xA;&lt;p&gt;O Spark on K8S funciona a partir de um Operator. Esse Operator foi criado pelo Google e é utilizado para rodar o Spark utilizando o Kubernetes API como master, sendo possível criar pods em tempo de execucão para que um job possa ser executado de forma paralela. Para instalar o Operator, a partir do diretório raiz do projeto, execute os seguintes comandos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd spark&#xA;bash install-spark-operator.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Nesse mesmo diretório tem exemplos de manifest sutilizando o Spark Operator para fazer o deploy de um job Spark no Kubernetes. Mas esses jobs não devem ser executados, o objetivo dele é fornecer um entendimento sobre a estrutura do manifest utilizando o Spark Operator. Para mais detalhes, clique &lt;a href=&#34;https://github.com/GoogleCloudPlatform/spark-on-k8s-operator&#34;&gt;aqui&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Kafka&lt;/h1&gt; &#xA;&lt;p&gt;O Apache Kafka é uma ferramenta de mensageria utilizada para Streaming de dados. Ele é muito mais poderoso que isso, mas para descrever tudo que o Kafka é capaz de fazer, precisaria de tutorial só para ele. Para instalá-lo, a partir do diretório raiz, execute os seguintes comandos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd kafka/strimzi&#xA;bash install-kafka.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Uma UI também é instalada no procedimento para gerenciar o cluster Kafka. Se os valores utilizados de ingress forem os defaults configurados nesse repositório, tente acessar no seu navegador a seguinte URL para validar se o Kafka está funcionando:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://kafka-ui.silveira.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Caso não sejam os valores default, use a URL customizada que foi definida.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Airflow&lt;/h1&gt; &#xA;&lt;p&gt;O Apache Airflow é uma ferramenta de orquestracão de jobs muito usado no contexto de pipelines de ingestão Big Data.&lt;/p&gt; &#xA;&lt;p&gt;Um ponto importante sobre o airflow, é que todas as suas pipelines (chamadas de DAG&#39;s) são armazenadas em um repositório, utilizando a sincronizacão com o Git. Para vincular as DAG&#39;s do Airflow ao seu repositório corporativo/pessoal, executa os seguintes passos:&lt;/p&gt; &#xA;&lt;p&gt;Abra o arquivo values.yaml&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd airflow&#xA;vim values.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modifique os paramêtros &lt;code&gt;dags.gitSync&lt;/code&gt; para os paramêtros desejados:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;enabled: true&#xA;repo: https://github.com/Guilherme-Silveira/airflow-dags.git&#xA;branch: main&#xA;rev: HEAD&#xA;depth: 1&#xA;maxFailures: 0&#xA;subPath: &#34;dags&#34;&#xA;credentialsSecret: git-credentials&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;É importante notar que o paramêtro &lt;code&gt;credentialsSecret: git-credentials&lt;/code&gt; faz referência a uma Secret que deve ser criada com suas credenciais de acesso ao repositório. Para fazer isso, execute os seguintes passos:&lt;/p&gt; &#xA;&lt;p&gt;Crie um arquivo chamado &lt;code&gt;git-secret.yaml&lt;/code&gt; com o seguinte conteúdo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Secret&#xA;metadata:&#xA;  name: git-credentials&#xA;  namespace: bigdata&#xA;data:&#xA;  GIT_SYNC_USERNAME: &amp;lt;base64_encoded_git_username&amp;gt;&#xA;  GIT_SYNC_PASSWORD: &amp;lt;base64_encoded_git_password&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Após isso, execute o seguinte comando para criar a Secret:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl apply -f git-secret.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Para concluir a instalacão do Airflow, execute os seguintes comandos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash install-airflow.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Se os valores utilizados de ingress forem os defaults configurados nesse repositório, tente acessar no seu navegador a seguinte URL para validar se o Airflow está funcionando:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://airflow.silveira.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Caso não sejam os valores default, use a URL customizada que foi definida.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Superset&lt;/h1&gt; &#xA;&lt;p&gt;O Apache Superset é uma ferramenta de visualizacão de dados. Para instalá-la, a partir do diretório raiz, execute os seguintes comandos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd superset&#xA;bash install-superset.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Se os valores utilizados de ingress forem os defaults configurados nesse repositório, tente acessar no seu navegador a seguinte URL para validar se o Superset está funcionando:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://superset.silveira.com&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Caso não sejam os valores default, use a URL customizada que foi definida.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Após todos esses procedimentos, seu ambiente Big Data estará funcionando! Espero que isso possa ser útil para vocês! Qualquer sugestão ou crítica construtiva, só avisar!&lt;/p&gt;</summary>
  </entry>
</feed>