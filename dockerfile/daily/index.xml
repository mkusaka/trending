<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-14T01:26:51Z</updated>
  <subtitle>Daily Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sismics/docker-apache2</title>
    <updated>2023-09-14T01:26:51Z</updated>
    <id>tag:github.com,2023-09-14:/sismics/docker-apache2</id>
    <link href="https://github.com/sismics/docker-apache2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Dockerfile for Apache2 Web Server&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/sismics/docker-backupninja/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/sismics/docker-apache2.svg?style=flat-square&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;Docker image for Apache Web Server&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Make a new Dockerfile that extends from this image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FROM sismics/apache2:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add your documents to the web root:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;RUN rm -fr /var/www/html/*&#xA;ADD www /var/www/html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start a container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -d -h apache2 --name apache2 -p 80:80 --restart=always \&#xA;    sismics/apache2&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>tairov/llama2.mojo</title>
    <updated>2023-09-14T01:26:51Z</updated>
    <id>tag:github.com,2023-09-14:/tairov/llama2.mojo</id>
    <link href="https://github.com/tairov/llama2.mojo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference Llama 2 in one file of pure ðŸ”¥&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;llama2.ðŸ”¥&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tairov/llama2.mojo/master/assets/llama2.mojo-demo.gif&#34; width=&#34;700&#34; alt=&#34;llama2.mojo logo&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;why this port?&lt;/h2&gt; &#xA;&lt;p&gt;This repository serves as a port that provides a Mojo-based implementation of &lt;code&gt;llama2.c&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;With the release of &lt;a href=&#34;https://www.modular.com/blog/mojo-its-finally-here&#34;&gt;Mojo&lt;/a&gt;, I was inspired to take my Python port of &lt;a href=&#34;https://github.com/tairov/llama2.py&#34;&gt;llama2.py&lt;/a&gt; and transition it to Mojo. The result? A version that leverages Mojo&#39;s SIMD &amp;amp; vectorization primitives, boosting the Python performance by nearly 250x. Impressively, the Mojo version now outperforms the original &lt;code&gt;llama2.c&lt;/code&gt; compiled in &lt;code&gt;runfast&lt;/code&gt; mode out of the box by 15-20%. This showcases the potential of hardware-level optimizations through Mojo&#39;s advanced features. I think this also can help us to see how far can we go with the original &lt;code&gt;llama2.c&lt;/code&gt; hardware optimizations.&lt;/p&gt; &#xA;&lt;h2&gt;performance&lt;/h2&gt; &#xA;&lt;p&gt;Since there were some debates was this comparison legit or not I did some research and found that in &lt;code&gt;runfast&lt;/code&gt; mode &lt;code&gt;llama2.c&lt;/code&gt; includes multiple optimizations like aggressive vectorization, which makes comparison fair with Mojo SIMD vectorization.&lt;/p&gt; &#xA;&lt;p&gt;Further researches of both solutions in parallelized mode compilation showed that &lt;code&gt;llama2.c&lt;/code&gt; is faster by ~20% I&#39;m still investigating in this direction since not all the possible optimizations were applied to the Mojo version so far.&lt;/p&gt; &#xA;&lt;h3&gt;comparison&lt;/h3&gt; &#xA;&lt;h4&gt;OS/HW specs&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;OS:         Ubuntu 20.04&#xA;CPU(s):     6&#xA;Model name: Intel(R) Core(TM) i7-8700 CPU @ 3.20GHz&#xA;CPU MHz:    3191.998&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/tairov/llama2.py&#34;&gt;llama2.py&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; (runfast)&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; (OMP/parallelized)&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;llama2.mojo&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;llama2.mojo&lt;/strong&gt; (parallelized)&lt;/th&gt; &#xA;   &lt;th&gt;llama2.mojo (naive matmul)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;stories15M.bin&lt;/td&gt; &#xA;   &lt;td&gt;1.3 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;75.73 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;237 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;450 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;260 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;390 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;67.26 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;stories110M.bin&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;9 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;30 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;64 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;40 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;57 tok/s&lt;/td&gt; &#xA;   &lt;td&gt;9.20 tok/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you have installed and &lt;a href=&#34;https://docs.modular.com/mojo/manual/get-started/index.html&#34;&gt;configured mojo on your environment&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or you can use &lt;a href=&#34;https://playground.modular.com/&#34;&gt;mojo playground&lt;/a&gt; to run this model.&lt;/p&gt; &#xA;&lt;h2&gt;feel the ðŸ”¥ magic&lt;/h2&gt; &#xA;&lt;p&gt;First, navigate to the folder when you keep your projects and clone this repository to this folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/tairov/llama2.mojo.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, open the repository folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd llama2.mojo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, let&#39;s download the model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, just run the Mojo&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mojo llama2.mojo stories15M.bin -s 100 -n 256 -t 0.5 -i &#34;Llama is an animal&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example output&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;num hardware threads:  6&#xA;SIMD vector width:  16&#xA;checkpoint size:  60816028&#xA;Llama is an animal was walking down the street. She stopped and looked up with a big smile on her face. She had a puppy in her arms. She was so excited to have a new friend.&#xA;The puppy ran up to her and said, &#34;Hi! I&#39;m here to be your friend!&#34;&#xA;Mandy smiled and said, &#34;Hi! I&#39;m Mandy. Can I play with you?&#34;&#xA;The puppy barked and wagged his tail. Mandy was so happy! She gave the puppy a big hug and they played with the puppy all afternoon.&#xA;When it was time to go home, Mandy said, &#34;I have to go now. Goodbye!&#34;&#xA;The puppy barked and said, &#34;Goodbye Mandy! See you tomorrow!&#34;&#xA;Mandy waved goodbye and then she went back home. She was so happy to have a new friend.&#xA;&amp;lt;s&amp;gt;&#xA;Once upon a time, there was a little girl named Lily. She loved to play outside and explore the world around her. One day, she went for a walk in the park with her mommy. They saw a big tree with lots of leaves.&#xA;Lily said,&#xA;achieved tok/s:  359.66149506346966&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running via Docker&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t llama2.mojo .&#xA;docker run -it llama2.mojo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With Gradio UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# uncomment the last line in Dockerfile CMD [&#34;python&#34;, &#34;gradio_app.py&#34;]&#xA;docker run -it -p 0.0.0.0:7860:7860 llama2.mojo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>