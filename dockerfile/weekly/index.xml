<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Dockerfile Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-19T01:38:26Z</updated>
  <subtitle>Weekly Trending of Dockerfile in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pathwaycom/llm-app</title>
    <updated>2024-05-19T01:38:26Z</updated>
    <id>tag:github.com,2024-05-19:/pathwaycom/llm-app</id>
    <link href="https://github.com/pathwaycom/llm-app" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM App templates for RAG, knowledge mining, and stream analytics. Ready to run with Docker,‚ö°in sync with your data sources.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://pathway.com/&#34;&gt;&lt;img src=&#34;https://d14l3brkh44201.cloudfront.net/pathway-llm.png&#34; alt=&#34;pathwaycom/llm-app: Build your LLM App in 30 lines of code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h1&gt;LLM App&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/pathwaycom/llm-app/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/pathwaycom/llm-app?style=plastic&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pathwaycom/llm-app/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/pathwaycom/llm-app?style=plastic&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;!-- [![Contributors](https://img.shields.io/github/actions/workflow/status/pathwaycom/llm-app/install_package.yml?style=plastic)](https://github.com/pathwaycom/llm-app/actions/workflows/install_package.yml) ---&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/OS-Linux-green&#34; alt=&#34;Linux&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/OS-macOS-green&#34; alt=&#34;macOS&#34;&gt; &lt;a href=&#34;https://discord.gg/pathway&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1042405378304004156?logo=discord&#34; alt=&#34;chat on Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=pathway_com&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/pathway_com?style=social&amp;amp;logo=twitter&#34; alt=&#34;follow on Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Pathway&#39;s &lt;strong&gt;LLM (Large Language Model) Apps&lt;/strong&gt; allow you to quickly put in production AI applications which use the most up-to-date knowledge available in your data sources. You can directly run a 24/7 service to answer natural language queries about an ever-changing private document knowledge base, or run an LLM-powered data transformation pipeline on a data stream.&lt;/p&gt; &#xA;&lt;p&gt;The Python application examples provided in this repo are ready-to-use. They can be run as Docker containers, and expose an HTTP API to the frontend. To allow quick testing and demos, most app examples also include an optional Streamlit UI which connects to this API. The apps rely on the &lt;a href=&#34;https://github.com/pathwaycom/pathway&#34;&gt;Pathway framework&lt;/a&gt; for data source synchronization, for serving API requests, and for all low-latency data processing. The apps connect to document data sources on S3, Google Drive, Sharepoint, etc. with no infrastructure dependencies (such as a vector database) that would need a separate setup.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quick links&lt;/strong&gt; - üëÄ &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#why-use-pathway-llm-apps&#34;&gt;Why use Pathway LLM Apps?&lt;/a&gt; üöÄ &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#watch-it-in-action&#34;&gt;Watch it in action&lt;/a&gt; üìö &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#how-it-works&#34;&gt;How it works&lt;/a&gt; üåü &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#application-examples&#34;&gt;Application examples&lt;/a&gt; üèÅ &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#get-started&#34;&gt;Get Started&lt;/a&gt; üíº &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#showcases&#34;&gt;Showcases&lt;/a&gt; üõ†Ô∏è &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#troubleshooting&#34;&gt;Troubleshooting&lt;/a&gt; üë• &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#troubleshooting&#34;&gt;Contributing&lt;/a&gt; ‚öôÔ∏è &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#%EF%B8%8F-hosted-version-%EF%B8%8F&#34;&gt;Hosted Version&lt;/a&gt; üí° &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#need-help&#34;&gt;Need help?&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why use Pathway LLM Apps?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simplicity&lt;/strong&gt; - Simplify your AI pipeline by consolidating capabilities into one platform. No need to integrate and maintain separate modules for your Gen AI app: &lt;del&gt;Vector Database (e.g. Pinecone/Weaviate/Qdrant) + Cache (e.g. Redis) + API Framework (e.g. Fast API)&lt;/del&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Real-time data syncing&lt;/strong&gt; - Sync both structured and unstructured data from diverse sources, enabling real-time Retrieval Augmented Generation (RAG).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy alert setup&lt;/strong&gt; - Configure alerts for key business events with simple configurations. Ask a question, and get updated when new info is available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt; - Handle heavy data loads and usage without degradation in performance. Metrics help track usage and scalability. Learn more about the performance of the underlying &lt;a href=&#34;https://github.com/pathwaycom/pathway/&#34;&gt;Pathway data processing framework&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt; - Provide visibility into model behavior via monitoring, tracing errors, anomaly detection, and replay for debugging. Helps with response quality.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Security&lt;/strong&gt; - Designed for Enterprise, with capabilities like Personally Identifiable Information (PII) detection, content moderation, permissions, and version control. Pathway apps can run in your private cloud with local LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unification&lt;/strong&gt; - Cover multiple aspects of your choice with a unified application logic: back-end, embedding, retrieval, LLM tech stack.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Watch it in action&lt;/h2&gt; &#xA;&lt;h3&gt;Effortlessly extract and organize unstructured data from PDFs, docs, and more into SQL tables - in real-time.&lt;/h3&gt; &#xA;&lt;p&gt;Analysis of live documents streams.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/unstructured_to_sql_on_the_fly/unstructured_to_sql_demo.gif&#34; alt=&#34;Effortlessly extract and organize unstructured data from PDFs, docs, and more into SQL tables - in real-time&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Check out: &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/gpt_4o_multimodal_rag/README.md&#34;&gt;&lt;code&gt;gpt_4o_multimodal_rag&lt;/code&gt;&lt;/a&gt; to see the whole pipeline in the works. You may also check out: &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/unstructured_to_sql_on_the_fly/app.py&#34;&gt;&lt;code&gt;unstructured-to-sql&lt;/code&gt;&lt;/a&gt; for a minimal example which works with non-multimodal models as well.)&lt;/p&gt; &#xA;&lt;h3&gt;Automated real-time knowledge mining and alerting.&lt;/h3&gt; &#xA;&lt;p&gt;Monitor streams of changing documents, get real-time alerts when answers change.&lt;/p&gt; &#xA;&lt;p&gt;Using incremental vector search, only the most relevant context is automatically passed into the LLM for analysis, minimizing token use - even when thousands of documents change every minute. This is real-time RAG taken to a new level üòä.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/drive_alert/drive_alert_demo.gif&#34; alt=&#34;Automated real-time knowledge mining and alerting&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For the code, see the &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#examples&#34;&gt;&lt;code&gt;drive_alert&lt;/code&gt;&lt;/a&gt; app example. You can find more details in a &lt;a href=&#34;https://pathway.com/developers/showcases/llm-alert-pathway&#34;&gt;blog post on alerting with LLM-App&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;The default &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/contextful/app.py&#34;&gt;&lt;code&gt;contextful&lt;/code&gt;&lt;/a&gt; app example launches an application that connects to a source folder with documents, stored in &lt;a href=&#34;https://aws.amazon.com/s3/&#34;&gt;AWS S3&lt;/a&gt; or locally on your computer. The app is &lt;strong&gt;always in sync&lt;/strong&gt; with updates to your documents, building in real-time a &#34;vector index&#34; using the Pathway package. It waits for user queries that come as HTTP REST requests, then uses the index to find relevant documents and responds using &lt;a href=&#34;https://openai.com/blog/openai-api&#34;&gt;OpenAI API&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; in natural language. This way, it provides answers that are always best on the freshest and most accurate &lt;strong&gt;real-time data&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This application template can also be combined with streams of fresh data, such as news feeds or status reports, either through REST or a technology like Kafka. It can also be combined with extra static data sources and user-specific contexts, to provide more relevant answers and reduce LLM hallucination.&lt;/p&gt; &#xA;&lt;p&gt;Read more about the implementation details and how to extend this application in &lt;a href=&#34;https://pathway.com/developers/user-guide/llm-xpack/llm-app-pathway/&#34;&gt;our blog article&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Instructional videos&lt;/h3&gt; &#xA;&lt;p&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://www.youtube.com/watch?v=kcrJSk00duw&#34;&gt;Building an LLM Application without a vector database&lt;/a&gt; - by &lt;a href=&#34;https://scholar.google.com/citations?user=Yc94070AAAAJ&#34;&gt;Jan Chorowski&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚ñ∂Ô∏è &lt;a href=&#34;https://www.youtube.com/watch?v=k1XGo7ts4tI&#34;&gt;Let&#39;s build a real-world LLM app in 11 minutes&lt;/a&gt; - by &lt;a href=&#34;https://substack.com/@paulabartabajo&#34;&gt;Pau Labarta Bajo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Advanced Features&lt;/h2&gt; &#xA;&lt;p&gt;LLM Apps built with Pathway can also include the following capabilities:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local Machine Learning models&lt;/strong&gt; - Pathway LLM Apps can run with local LLMs and embedding models, without making API calls outside of the User&#39;s Organization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple live data sources&lt;/strong&gt; - Pathway LLM Apps can &lt;a href=&#34;https://pathway.com/developers/user-guide/connecting-to-data/connectors/&#34;&gt;connect to live data sources&lt;/a&gt; of diverse types (news feeds, APIs, data streams in Kafka, and others),&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Extensible enterprise logic&lt;/strong&gt; - user permissions, user session handling, and a data security layer can all be embedded in your application logic by integrating with your enterprise SSO, AD Domains, LDAP, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Live knowledge graphs&lt;/strong&gt; - the Pathway framework enables concept mining, organizing data and metadata as knowledge graphs, and knowledge-graph-based indexes, kept in sync with live data sources.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about advanced features see: &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/FEATURES-for-organizations.md&#34;&gt;Features for Organizations&lt;/a&gt;, or reach out to the Pathway team.&lt;/p&gt; &#xA;&lt;h2&gt;Application Examples&lt;/h2&gt; &#xA;&lt;p&gt;Pick one that is closest to your needs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Example app (template)&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/demo-question-answering/app.py&#34;&gt;&lt;code&gt;demo-question-answering&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The question-answering pipeline that uses the GPT model of choice to provide answers to the queries about a set of documents. You can also try it on the Pathway &lt;a href=&#34;https://cloud.pathway.com&#34;&gt;Hosted Pipelines website&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/demo-document-indexing/main.py&#34;&gt;&lt;code&gt;demo-document-indexing&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The real-time document indexing pipeline that provides the monitoring of several kinds of data sources and health-check endpoints. It is available on the Pathway &lt;a href=&#34;https://cloud.pathway.com&#34;&gt;Hosted Pipelines website&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/contextless/app.py&#34;&gt;&lt;code&gt;contextless&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This simple example calls OpenAI ChatGPT API but does not use an index when processing queries. It relies solely on the given user query. We recommend it to start your Pathway LLM journey.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/contextful/app.py&#34;&gt;&lt;code&gt;contextful&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This default example of the app will index the jsonlines documents located in the &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/data/pathway-docs&#34;&gt;&lt;code&gt;data/pathway-docs&lt;/code&gt;&lt;/a&gt; directory. These indexed documents are then taken into account when processing queries.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/contextful_s3/app.py&#34;&gt;&lt;code&gt;contextful-s3&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This example operates similarly to the contextful mode. The main difference is that the documents are stored and indexed from an S3 bucket, allowing the handling of a larger volume of documents. This can be more suitable for production environments.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/unstructured/app.py&#34;&gt;&lt;code&gt;unstructured&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Process unstructured documents such as PDF, HTML, DOCX, PPTX, and more. Visit &lt;a href=&#34;https://unstructured-io.github.io/unstructured/&#34;&gt;unstructured-io&lt;/a&gt; for the full list of supported formats.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/local/app.py&#34;&gt;&lt;code&gt;local&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This example runs the application using Huggingface Transformers, which eliminates the need for the data to leave the machine. It provides a convenient way to use state-of-the-art NLP models locally.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/unstructured_to_sql_on_the_fly/app.py&#34;&gt;&lt;code&gt;unstructured-to-sql&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This example extracts the data from unstructured files and stores it into a PostgreSQL table. It also transforms the user query into an SQL query which is then executed on the PostgreSQL table.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/alert/app.py&#34;&gt;&lt;code&gt;alert&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ask questions, get alerted whenever response changes. Pathway is always listening for changes, whenever new relevant information is added to the stream (local files in this example), LLM decides if there is a substantial difference in response and notifies the user with a Slack message.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/drive_alert/app.py&#34;&gt;&lt;code&gt;drive-alert&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/alert/app.py&#34;&gt;&lt;code&gt;alert&lt;/code&gt;&lt;/a&gt; example on steroids. Whenever relevant information on Google Docs is modified or added, get real-time alerts via Slack. See the &lt;a href=&#34;https://pathway.com/developers/showcases/llm-alert-pathway&#34;&gt;&lt;code&gt;tutorial&lt;/code&gt;&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/contextful_geometric/app.py&#34;&gt;&lt;code&gt;contextful-geometric&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/contextful/app.py&#34;&gt;&lt;code&gt;contextful&lt;/code&gt;&lt;/a&gt; example, which optimises use of tokens in queries. It asks the same questions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;with increasing number of documents given as a context in the question, until ChatGPT finds an answer.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure that &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt; 3.10 or above installed on your machine.&lt;/li&gt; &#xA; &lt;li&gt;Download and Install &lt;a href=&#34;https://pip.pypa.io/en/stable/installation/&#34;&gt;Pip&lt;/a&gt; to manage project packages.&lt;/li&gt; &#xA; &lt;li&gt;[Optional if you use OpenAI models]. Create an &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt; account and generate a new API Key: To access the OpenAI API, you will need to create an API Key. You can do this by logging into the &lt;a href=&#34;https://openai.com/product&#34;&gt;OpenAI website&lt;/a&gt; and navigating to the API Key management page.&lt;/li&gt; &#xA; &lt;li&gt;[Important if you use Windows OS]. The examples only support Unix-like systems (such as Linux, macOS, and BSD). If you are a Windows user, we highly recommend leveraging &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/install&#34;&gt;Windows Subsystem for Linux (WSL)&lt;/a&gt; or Dockerize the app to run as a container.&lt;/li&gt; &#xA; &lt;li&gt;[Optional if you use Docker to run samples]. Download and install &lt;a href=&#34;https://www.docker.com/&#34;&gt;docker&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Now, follow the steps to install and &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#examples&#34;&gt;get started with one of the provided examples&lt;/a&gt;. You can pick any example that you find interesting - if not sure, pick &lt;code&gt;contextful&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, you can also take a look at the &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#showcases&#34;&gt;application showcases&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Clone the repository&lt;/h3&gt; &#xA;&lt;p&gt;This is done with the &lt;code&gt;git clone&lt;/code&gt; command followed by the URL of the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/pathwaycom/llm-app.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run the chosen example&lt;/h3&gt; &#xA;&lt;p&gt;Each &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/examples/pipelines/&#34;&gt;example&lt;/a&gt; contains a README.md with instructions on how to run it.&lt;/p&gt; &#xA;&lt;h3&gt;Bonus: Build your own Pathway-powered LLM App&lt;/h3&gt; &#xA;&lt;p&gt;Want to learn more about building your own app? See step-by-step guide &lt;a href=&#34;https://pathway.com/developers/showcases/llm-app-pathway&#34;&gt;Building a llm-app tutorial&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or,&lt;/p&gt; &#xA;&lt;p&gt;Simply add &lt;code&gt;llm-app&lt;/code&gt; to your project&#39;s dependencies and copy one of the &lt;a href=&#34;https://raw.githubusercontent.com/pathwaycom/llm-app/main/#examples&#34;&gt;examples&lt;/a&gt; to get started!&lt;/p&gt; &#xA;&lt;h2&gt;Showcases&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/pathway-labs/chatgpt-api-python-sales&#34;&gt;Python sales&lt;/a&gt; - Find real-time sales with AI-powered Python API using ChatGPT and LLM (Large Language Model) App.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/pathway-labs/dropbox-ai-chat&#34;&gt;Dropbox Data Observability&lt;/a&gt; - See how to get started with chatting with your Dropbox and having data observability.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Please check out our &lt;a href=&#34;https://github.com/pathwaycom/llm-app/discussions/categories/q-a&#34;&gt;Q&amp;amp;A&lt;/a&gt; to get solutions for common installation problems and other issues.&lt;/p&gt; &#xA;&lt;h3&gt;Raise an issue&lt;/h3&gt; &#xA;&lt;p&gt;To provide feedback or report a bug, please &lt;a href=&#34;https://github.com/pathwaycom/pathway/issues&#34;&gt;raise an issue on our issue tracker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Anyone who wishes to contribute to this project, whether documentation, features, bug fixes, code cleanup, testing, or code reviews, is very much encouraged to do so.&lt;/p&gt; &#xA;&lt;p&gt;To join, just raise your hand on the &lt;a href=&#34;https://discord.com/invite/pathway&#34;&gt;Pathway Discord server&lt;/a&gt; (#get-help) or the GitHub &lt;a href=&#34;https://github.com/pathwaycom/llm-app/discussions&#34;&gt;discussion&lt;/a&gt; board.&lt;/p&gt; &#xA;&lt;p&gt;If you are unfamiliar with how to contribute to GitHub projects, here is a &lt;a href=&#34;https://docs.github.com/en/get-started/quickstart/contributing-to-projects&#34;&gt;Get Started Guide&lt;/a&gt;. A full set of contribution guidelines, along with templates, are in progress.&lt;/p&gt; &#xA;&lt;h2&gt;Coming Soon&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Templates for retrieving context via graph walks.&lt;/li&gt; &#xA; &lt;li&gt;Easy setup for model drift monitoring.&lt;/li&gt; &#xA; &lt;li&gt;Templates for model A/B testing.&lt;/li&gt; &#xA; &lt;li&gt;Real-time OpenAI API observability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚òÅÔ∏è Hosted Version ‚òÅÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://cloud.pathway.com/&#34;&gt;cloud.pathway.com&lt;/a&gt; for hosted services. You can quickly set up variants of the &lt;code&gt;unstructured&lt;/code&gt; app, which connect live data sources on Google Drive and Sharepoint to your Gen AI app.&lt;/p&gt; &#xA;&lt;h2&gt;Need help?&lt;/h2&gt; &#xA;&lt;p&gt;Interested in building your own Pathway LLM App with your data source, stack, and custom use cases? Connect with us to get help with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Connecting your own live data sources to your LLM application (e.g. Google or Microsoft Drive documents, Kafka, databases, API&#39;s, ...).&lt;/li&gt; &#xA; &lt;li&gt;Explore how you can get your LLM application up and running in popular cloud platforms such as Azure and AWS.&lt;/li&gt; &#xA; &lt;li&gt;Developing knowledge graph use cases.&lt;/li&gt; &#xA; &lt;li&gt;End-to-end solution implementation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Reach us at &lt;a href=&#34;mailto:contact@pathway.com&#34;&gt;contact@pathway.com&lt;/a&gt; or via &lt;a href=&#34;https://pathway.com/solutions/llm-app&#34;&gt;Pathway&#39;s website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Supported and maintained by&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/pathwaycom/&#34;&gt;&lt;img src=&#34;https://pathway.com/logo-light.svg?sanitize=true&#34; alt=&#34;Pathway&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pathway.com/solutions/llm-app&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/See%20Pathway&#39;s%20offering%20for%20AI%20applications-0000FF&#34; alt=&#34;See Pathway&#39;s offering for AI applications&#34;&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LinkedInLearning/guide-to-java-design-patterns-4512383</title>
    <updated>2024-05-19T01:38:26Z</updated>
    <id>tag:github.com,2024-05-19:/LinkedInLearning/guide-to-java-design-patterns-4512383</id>
    <link href="https://github.com/LinkedInLearning/guide-to-java-design-patterns-4512383" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repo is for the Linkedin Learning course: A Complete Guide to Java Design Patterns: Creational, Behavioral, and Structural&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;A Complete Guide to Java Design Patterns: Creational, Behavioral, and Structural&lt;/h1&gt; &#xA;&lt;p&gt;This is the repository for the LinkedIn Learning course &lt;code&gt;A Complete Guide to Java Design Patterns: Creational, Behavioral, and Structural&lt;/code&gt;. The full course is available from &lt;a href=&#34;https://www.linkedin.com/learning/complete-guide-to-java-design-patterns-creational-behavioral-and-structural&#34;&gt;LinkedIn Learning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://media.licdn.com/dms/image/D4D0DAQEyFMOVuMzMew/learning-public-crop_675_1200/0/1711582600788?e=2147483647&amp;amp;v=beta&amp;amp;t=8fPn5iZ-ghDqbhm2QXeWYKlDNbUaIIHAV7rhwBEa8OU&#34; alt=&#34;lil-thumbnail-url&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In this course, developer Bethan Palmer guides you through using creative, behavioral, and structural design patterns in Java. Find out how design patterns help you write cleaner, more elegant code, and explore a wide range of patterns with real world examples. Practice and reinforce what you learn with hands-on challenges in each chapter.&lt;/p&gt; &#xA;&lt;p&gt;The best way to learn a language is to use it in practice. That‚Äôs why this course is integrated with GitHub Codespaces, an instant cloud developer environment that offers all the functionality of your favorite IDE without the need for any local machine setup. With GitHub Codespaces, you can get hands-on practice from any machine, at any time‚Äîall while using a tool that you‚Äôll likely encounter in the workplace. Check out the ‚ÄúUsing GitHub Codespaces with this course‚Äù video to learn how to get started.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;See the readme file in the main branch for updated instructions and information.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Instructions&lt;/h2&gt; &#xA;&lt;p&gt;This repository has branches for each of the videos in the course. You can use the branch pop up menu in github to switch to a specific branch and take a look at the course at that stage, or you can add &lt;code&gt;/tree/BRANCH_NAME&lt;/code&gt; to the URL to go to the branch you want to access.&lt;/p&gt; &#xA;&lt;h2&gt;Branches&lt;/h2&gt; &#xA;&lt;p&gt;The branches are structured to correspond to the videos in the course. The naming convention is &lt;code&gt;CHAPTER#_MOVIE#&lt;/code&gt;. As an example, the branch named &lt;code&gt;02_03&lt;/code&gt; corresponds to the second chapter and the third video in that chapter. Some branches will have a beginning and an end state. These are marked with the letters &lt;code&gt;b&lt;/code&gt; for &#34;beginning&#34; and &lt;code&gt;e&lt;/code&gt; for &#34;end&#34;. The &lt;code&gt;b&lt;/code&gt; branch contains the code as it is at the beginning of the movie. The &lt;code&gt;e&lt;/code&gt; branch contains the code as it is at the end of the movie. The &lt;code&gt;main&lt;/code&gt; branch holds the final state of the code when in the course.&lt;/p&gt; &#xA;&lt;p&gt;When switching from one exercise files branch to the next after making changes to the files, you may get a message like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;error: Your local changes to the following files would be overwritten by checkout:        [files]&#xA;Please commit your changes or stash them before you switch branches.&#xA;Aborting&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To resolve this issue:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Add changes to git using this command: git add .&#xA;Commit changes using this command: git commit -m &#34;some message&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Instructor&lt;/h3&gt; &#xA;&lt;p&gt;Bethan Palmer&lt;/p&gt; &#xA;&lt;p&gt;Developer&lt;/p&gt; &#xA;&lt;p&gt;Check out my other courses on &lt;a href=&#34;https://www.linkedin.com/learning/instructors/bethan-palmer?u=104&#34;&gt;LinkedIn Learning&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>RedHatOfficial/rhelai-dev-preview</title>
    <updated>2024-05-19T01:38:26Z</updated>
    <id>tag:github.com,2024-05-19:/RedHatOfficial/rhelai-dev-preview</id>
    <link href="https://github.com/RedHatOfficial/rhelai-dev-preview" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Red Hat Enterprise Linux AI -- Developer Preview&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RHEL AI Developer Preview Guide&lt;/h1&gt; &#xA;&lt;p&gt;This guide will help you assemble and test a &lt;a href=&#34;https://access.redhat.com/support/offerings/devpreview&#34;&gt;developer preview&lt;/a&gt; version of the RHEL AI product.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the &lt;strong&gt;Red Hat Enterprise Linux AI Developer Preview!&lt;/strong&gt; This guide is meant to introduce you to RHEL AI Developer Preview capabilities. As with other Developer Previews, expect changes to these workflows, additional automation and simplification, as well as a broadening of capabilities, hardware and software support versions, performance improvements (and other optimizations) prior to GA.&lt;/p&gt; &#xA;&lt;p&gt;RHEL AI is an open-source product that includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/instructlab&#34;&gt;Granite&lt;/a&gt;: an open source, Apache 2 licensed foundation model from IBM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/instructlab&#34;&gt;InstructLab&lt;/a&gt;: a CLI and tuning backend that provides a simple user interface for contributing knowledge and skills to a base model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/containers/bootc&#34;&gt;RHEL Image Mode (bootc)&lt;/a&gt;: RHEL AI is distributed as a ‚Äúbootable container‚Äù image. Provision RHEL AI appliances via kickstart onto bare metal or cloud instances.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;: A high-throughput and memory-efficient inference and serving engine for LLMs, based on PyTorch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deepspeed.ai/&#34;&gt;deepspeed&lt;/a&gt;: A deep learning optimization software suite for both training and inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;: PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] RHEL AI is targeted at server platforms and workstations with discrete GPUs. For laptops, please use upstream &lt;a href=&#34;https://github.com/instructlab&#34;&gt;InstructLab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RedHatOfficial/rhelai-dev-preview/main/images/rhel-ai-developer-preview.png&#34; alt=&#34;RHEL AI Developer Preview&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Validated Hardware for Developer Preview&lt;/h2&gt; &#xA;&lt;p&gt;Here is a list of servers validated by Red Hat engineers to work with the RHEL AI Developer Preview. We anticipate that recent systems certified to run RHEL 9, with recent datacenter GPUs such as those listed below, will work with this Developer Preview.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Minimum Total GPU memory: 320GB (e.g., 4 GPUs @ 80GB memory each)&lt;/li&gt; &#xA; &lt;li&gt;Minimum 200GB of disk space for model storage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Compute Vendor&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;GPU Vendor / Specs&lt;/th&gt; &#xA;   &lt;th&gt;RHEL AI Dev Preview&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dell (4) NVIDIA H100&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://community.ibm.com/community/user/blogs/dan-waugh/2024/05/06/rhel-ai-on-ibm-cloud-for-custom-model-tuning&#34;&gt;IBM&lt;/a&gt; &lt;code&gt;GX3&lt;/code&gt; Instances&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Lenovo (8) AMD MI300x&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AWS p4 and p5 instances (NVIDIA)&lt;/td&gt; &#xA;   &lt;td&gt;Ongoing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel&lt;/td&gt; &#xA;   &lt;td&gt;Ongoing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Training Performance - What to Expect&lt;/h3&gt; &#xA;&lt;p&gt;For the best experience using the RHEL AI developer preview period, we have included a pruned taxonomy tree inside the InstructLab container. This will allow for validating training to complete in a reasonable time frame on a single server.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add your knowledge and skills to this version of the taxonomy. We recommend you add no more than 5 additions to the taxonomy tree to keep the resource requirements reasonable.&lt;/li&gt; &#xA; &lt;li&gt;On systems like the above, training should take ~1 hour.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Formula:&lt;/strong&gt; A single GPU can train ~250 samples per minute. If you have 8 GPUs and 10,000 samples, expect it to take $&lt;code&gt;(10000/250/8*10)&lt;/code&gt;$ minutes, or about 50 minutes for 10 epochs. For smoke testing, feel free to run 1-2 epochs (note we recommend 10 epochs for best results).&lt;/p&gt; &#xA;&lt;h2&gt;Trying it Out&lt;/h2&gt; &#xA;&lt;p&gt;By the end of this exercise, you‚Äôll have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Built a set of Image-Mode and InstructLab containers&lt;/li&gt; &#xA; &lt;li&gt;Booted your system into the RHEL AI Image-Mode container&lt;/li&gt; &#xA; &lt;li&gt;Run through the InstructLab exercises to demonstrate the technology&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;What is bootc?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://containers.github.io/bootc/&#34;&gt;&lt;code&gt;bootc&lt;/code&gt;&lt;/a&gt; is a transactional, in-place operating system that provisions and updates using OCI/Docker container images. &lt;code&gt;bootc&lt;/code&gt; is the key component in a broader mission of bootable containers.&lt;/p&gt; &#xA;&lt;p&gt;The original Docker container model of using &#34;layers&#34; to model applications has been extremely successful. This project aims to apply the same technique for bootable host systems - using standard OCI/Docker containers as a transport and delivery format for base operating system updates.&lt;/p&gt; &#xA;&lt;p&gt;The container image includes a Linux kernel (in e.g. &lt;code&gt;/usr/lib/modules&lt;/code&gt;), which is used to boot. At runtime on a target system, the base userspace is not itself running in a container by default. For example, assuming &lt;code&gt;systemd&lt;/code&gt; is in use, &lt;code&gt;systemd&lt;/code&gt; acts as &lt;code&gt;pid1&lt;/code&gt; as usual - there&#39;s no &#34;outer&#34; process.&lt;/p&gt; &#xA;&lt;p&gt;In the following example, the bootc container is labeled &lt;code&gt;Node Base Image&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RedHatOfficial/rhelai-dev-preview/main/images/bootc-example.png&#34; alt=&#34;bootc Example&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Build Host Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Depending on your build host hardware and internet connection speed, building and uploading container images could take up to 2 hours.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RHEL 9.4&lt;/li&gt; &#xA; &lt;li&gt;Connection to the internet (some images are &amp;gt; 15GB)&lt;/li&gt; &#xA; &lt;li&gt;4 CPU, 16GB RAM, 400GB disk space (tested with AWS EC2 &lt;code&gt;m5.xlarge&lt;/code&gt; using GP3 storage)&lt;/li&gt; &#xA; &lt;li&gt;A place to push container images that you will build ‚Äì e.g., &lt;code&gt;quay.io&lt;/code&gt; or another image registry.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Preparing the Build Host&lt;/h2&gt; &#xA;&lt;p&gt;Register the host (&lt;a href=&#34;https://access.redhat.com/solutions/253273&#34;&gt;How to register and subscribe a RHEL system to the Red Hat Customer Portal using Red Hat Subscription-Manager?&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo subscription-manager register --username &amp;lt;username&amp;gt; --password &amp;lt;password&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install required packages&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo dnf install git make podman buildah lorax -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Clone the RHEL AI Developer Preview git repo&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/RedHatOfficial/rhelai-dev-preview&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Authenticate to the Red Hat registry (&lt;a href=&#34;https://access.redhat.com/RegistryAuthentication&#34;&gt;Red Hat Container Registry Authentication&lt;/a&gt;) using your &lt;code&gt;redhat.com&lt;/code&gt; account.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;podman login registry.redhat.io --username &amp;lt;username&amp;gt; --password &amp;lt;password&amp;gt;&#xA;podman login --get-login registry.redhat.io&#xA;Your_login_here&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Ensure you have an SSH key on the build host. This is used during the driver toolkit image build. (&lt;a href=&#34;https://www.redhat.com/sysadmin/configure-ssh-keygen&#34;&gt;Using &lt;code&gt;ssh-keygen&lt;/code&gt; and sharing for key-based authentication in Linux | Enable Sysadmin&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h3&gt;Creating bootc containers&lt;/h3&gt; &#xA;&lt;p&gt;RHEL AI includes a set of Makefiles to facilitate creating the container images. Depending on your build host hardware and internet connection speed, this could take up to an hour.&lt;/p&gt; &#xA;&lt;p&gt;Build the InstructLab NVIDIA container image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make instruct-nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;&lt;code&gt;vllm&lt;/code&gt;&lt;/a&gt; container image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make vllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Build the &lt;a href=&#34;https://www.deepspeed.ai/&#34;&gt;&lt;code&gt;deepspeed&lt;/code&gt;&lt;/a&gt; container image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make deepspeed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Last, build the RHEL AI NVIDIA &lt;code&gt;bootc&lt;/code&gt; container image. This is the RHEL Image-mode ‚Äúbootable‚Äù container. We embed the 3 images above into this container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make nvidia FROM=registry.redhat.io/rhel9/rhel-bootc:9.4 REGISTRY=&amp;lt;your-registry&amp;gt; REGISTRY_ORG=&amp;lt;your-org-name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The resulting image is tagged &lt;code&gt;${REGISTRY}/${REGISTRY_ORG}/nvidia-bootc:latest&lt;/code&gt;. For more variables and examples, see the &lt;a href=&#34;https://github.com/RedHatOfficial/rhelai-dev-preview/tree/main/training&#34;&gt;training/README&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Push the resulting image to your registry. You will refer to this URL inside a kickstart file in an upcoming step.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;podman push ${REGISTRY}/${REGISTRY_ORG}/nvidia-bootc:latest&#xA;e.g. podman push quay.io/&amp;lt;your-user-name&amp;gt;/nvidia-bootc.latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;At this point you have a RHEL AI bootable container image ready to be installed on a physical or virtual host.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Provisioning your GPU host (kickstart method)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.anaconda.com/free/anaconda/install/index.html&#34;&gt;Anaconda&lt;/a&gt; is the Red Hat Enterprise Linux installer, and it is embedded in all RHEL downloadable ISO images. The main method of automating RHEL installation is via scripts called Kickstart. For more information about Anaconda and Kickstart, &lt;a href=&#34;https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html-single/performing_an_advanced_rhel_9_installation/index#what-are-kickstart-installations_kickstart-installation-basics&#34;&gt;read these documents&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A recent kickstart command called &lt;a href=&#34;https://pykickstart.readthedocs.io/en/latest/kickstart-docs.html#ostreecontainer&#34;&gt;&lt;code&gt;ostreecontainer&lt;/code&gt;&lt;/a&gt; was introduced with RHEL 9.4. We use &lt;code&gt;ostreecontainer&lt;/code&gt; to provision the bootable &lt;code&gt;nvidia-bootc&lt;/code&gt; container you just pushed to your registry over the network.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of a kickstart file. Copy it to a file called &lt;code&gt;rhelai-dev-preview-bootc.ks&lt;/code&gt;, and customize it for your environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;# text&#xA;## customize this for your target system&#xA;# network --bootproto=dhcp --device=link --activate&#xA;&#xA;## Basic partitioning&#xA;## customize this for your target system&#xA;# clearpart --all --initlabel --disklabel=gpt&#xA;# reqpart --add-boot&#xA;# part / --grow --fstype xfs&#xA;&#xA;# ostreecontainer --url quay.io/&amp;lt;your-user-name&amp;gt;/nvidia-bootc:latest&#xA;&#xA;# firewall --disabled&#xA;# services --enabled=sshd&#xA;&#xA;## optionally add a user&#xA;# user --name=cloud-user --groups=wheel --plaintext --password&#xA;# sshkey --username cloud-user &#34;ssh-ed25519 AAAAC3Nza.....&#34;&#xA;&#xA;## if desired, inject an SSH key for root&#xA;# rootpw --iscrypted locked&#xA;# sshkey --username root &#34;ssh-ed25519 AAAAC3Nza...&#34;&#xA;# reboot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Embed your kickstart into the RHEL Boot ISO&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developers.redhat.com/products/rhel/download#rhel-new-product-download-list-61451&#34;&gt;Download the RHEL 9.4&lt;/a&gt; ‚ÄúBoot ISO‚Äù, and use &lt;code&gt;mkksiso&lt;/code&gt; command to embed the kickstart into the RHEL boot ISO.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mkksiso rhelai-dev-preview-bootc.ks rhel-9.4-x86_64-boot.iso rhelai-dev-preview-bootc-ks.iso&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At this point you should have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;nvidia-bootc:latest&lt;/code&gt;: a bootable container image with support for NVIDIA GPUs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rhelai-dev-preview-bootc.ks&lt;/code&gt;: a kickstart file customized to provision RHEL from your container registry to your target system.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;rhelai-dev-preview-bootc-ks.iso&lt;/code&gt;: a bootable RHEL 9.4 ISO with the kickstart embedded.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Boot your target system using the &lt;code&gt;rhelai-dev-preview-bootc-ks.iso&lt;/code&gt; file. anaconda will pull the nvidia-bootc:latest image from your registry and provision RHEL according to your kickstart file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alternative&lt;/strong&gt;: the kickstart file can be served via HTTP. On the installation via kernel command line and an external HTTP server ‚Äì add &lt;code&gt;inst.ks=http(s)://kickstart/url/rhelai-dev-preview-bootc.ks&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Using RHEL AI and InstructLab&lt;/h2&gt; &#xA;&lt;h3&gt;Download Models&lt;/h3&gt; &#xA;&lt;p&gt;Before using the RHEL AI environment, you must download two models, each tailored to a key function in the high-fidelity tuning process. &lt;a href=&#34;https://huggingface.co/instructlab&#34;&gt;Granite&lt;/a&gt; is used as the student model and is responsible for facilitating the training of a new fine-tuned mode. &lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1&#34;&gt;Mixtral&lt;/a&gt; is used as the teacher model and is responsible for aiding the generation phase of the LAB process, where skills and knowledge are used in concert to produce a rich training dataset.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before you can start the download process, you need to create an account on &lt;a href=&#34;https://huggingface.co/&#34;&gt;HuggingFace.co&lt;/a&gt; and manually acknowledge the terms and conditions for Mixtral.&lt;/li&gt; &#xA; &lt;li&gt;Additionally, you will need to create a token on the Hugging Face site so we can download the model from the command line. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Click on your profile in the upper right corner and click &lt;code&gt;Settings&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Click &lt;code&gt;Access Tokens&lt;/code&gt;. Click the &lt;code&gt;New token&lt;/code&gt; button and provide a name. The new token only requires the use of &lt;code&gt;Read&lt;/code&gt; permissions since it&#39;s only being used to fetch models. On this screen, you will be able to generate the token content and save and copy the text to authenticate.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RedHatOfficial/rhelai-dev-preview/main/images/hf-access-tokens.png&#34; alt=&#34;HuggingFace Access Tokens&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Review and accept the terms of the Mixtral model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RedHatOfficial/rhelai-dev-preview/main/images/mixtral-agree.png&#34; alt=&#34;Agree to terms for Mixtral&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Understanding the Differences Between ilab and RHEL AI CLIs&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;ilab&lt;/code&gt; command line interface that is part of the InstructLab project focuses on running lightweight quantized models on personal computing devices like laptops. In contrast, RHEL AI enables the use of high-fidelity training using full-precision models. For familiarity, the command and parameters mirror that of InstructLab‚Äôs &lt;code&gt;ilab&lt;/code&gt; command; however, the backing implementation is very different.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In RHEL AI, the &lt;code&gt;ilab&lt;/code&gt; command is a &lt;strong&gt;wrapper&lt;/strong&gt; that acts as a front-end to a container architecture pre-bundled on the RHEL AI system.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Using the &lt;code&gt;ilab&lt;/code&gt; Command Line Interface&lt;/h3&gt; &#xA;&lt;h3&gt;Create a working directory for your project&lt;/h3&gt; &#xA;&lt;p&gt;The first step is to create a new working directory for your project. Everything will be relative to this working directory. It will contain your models, logs, and training data.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;mkdir my-project&#xA;cd my-project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Initialize your project&lt;/h4&gt; &#xA;&lt;p&gt;The very first &lt;code&gt;ilab&lt;/code&gt; command you will run sets up the base environment, including downloading the taxonomy repo if you choose. This will be needed for later steps, so it is recommended to do so.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ilab init&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Download Granite-7B (~27GB on disk)&lt;/h4&gt; &#xA;&lt;p&gt;Next, download the IBM Granite base model. Important: Do not download the ‚Äúlab‚Äù versions of the model. The granite &lt;strong&gt;base&lt;/strong&gt; model is most effective when performing high-fidelity training.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ilab download --repository ibm/granite-7b-base&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Download Mixtral-8x7B-Instruct (~96GB on disk)&lt;/h4&gt; &#xA;&lt;p&gt;Follow the same process, but additionally define an environment variable using the HF token you created in the above section under Access Tokens.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export HF_TOKEN=&amp;lt;paste token value here&amp;gt;&#xA;ilab download --repository mistralai/Mixtral-8x7B-Instruct-v0.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Directory Structure&lt;/h4&gt; &#xA;&lt;p&gt;Now that you have initialized your project and downloaded your first models, observe the directory structure of your project&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;my-project/&#xA;‚îú‚îÄ models/&#xA;‚îú‚îÄ generated/&#xA;‚îú‚îÄ taxonomy/&#xA;‚îú‚îÄ training/&#xA;‚îú‚îÄ training_output/&#xA;‚îú‚îÄ cache/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Folder&lt;/th&gt; &#xA;   &lt;th&gt;Purpose&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;models&lt;/td&gt; &#xA;   &lt;td&gt;Holds all language models, including the saved output of ones you generate with RHEL AI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;generated&lt;/td&gt; &#xA;   &lt;td&gt;Generated data output from the generation phase, built on modifications to the taxonomy repository&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;taxonomy&lt;/td&gt; &#xA;   &lt;td&gt;Skill or Knowledge data used by the LAB method to generate synthetic data for training&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;training&lt;/td&gt; &#xA;   &lt;td&gt;Converted seed data to facilitate the training process&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;training_output&lt;/td&gt; &#xA;   &lt;td&gt;All transient output of the training process, including logs and in-flight sample checkpoints&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cache&lt;/td&gt; &#xA;   &lt;td&gt;An internal cache used by the model data&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Modifying the Taxonomy&lt;/h3&gt; &#xA;&lt;p&gt;The next step is to contribute new knowledge or skills into the taxonomy repo. See the &lt;a href=&#34;https://github.com/instructlab/taxonomy/raw/main/README.md&#34;&gt;InstructLab documentation&lt;/a&gt; for more information and examples of how to do this. We also have a set of lab exercises here.&lt;/p&gt; &#xA;&lt;h4&gt;Launching the Teacher model&lt;/h4&gt; &#xA;&lt;p&gt;With the additional taxonomy data added, it‚Äôs now possible to generate new synthetic data to eventually train a new model. Although, before generation can begin, a teacher model first needs to be started to assist the generator in constructing new data. In a separate terminal session, run the ‚Äúserve‚Äù command and wait for the VLLM startup to complete. Note this process can take several minutes to complete&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ilab serve&#xA;INFO:     Application startup complete.&#xA;INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Generating new Synthetic Data&lt;/h4&gt; &#xA;&lt;p&gt;Now that VLLM is serving the teacher mode, the generation process can be started using the &lt;code&gt;ilab&lt;/code&gt; generate command. This process will take some time to complete and will continually output the total number of instructions generated as it is updated. This defaults to 5000 instructions, but you can adjust this with the &lt;code&gt;--num-instructions&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ilab generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Q&amp;gt; How do cytokines influence the outcome of certain diseases involving tonsils?&#xA;A&amp;gt; The outcome of infectious, autoimmune, or malignant diseases affecting tonsils may be influenced by the overall balance of production profiles of pro-inflammatory and anti-inflammatory cytokines. Determining cytokine profiles in tonsil studies is essential for understanding the causes and underlying mechanisms of these disorders.&#xA; 35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Examining the Synthetic Data Set&lt;/h4&gt; &#xA;&lt;p&gt;In addition to the current data printed to the screen during generation, a full output is recorded in the generated folder. Before training it is recommended to review this output to verify it meets expectations. If it is not satisfactory, try modifying or creating new examples in the taxonomy and rerunning.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;less generated/generated_Mixtral*.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Stopping VLLM&lt;/h4&gt; &#xA;&lt;p&gt;Once the generated data is satisfactory, the training process can begin. Although first close the VLLM instance in the terminal session that was started for generation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;CTRL+C&#xA;INFO:     Application shutdown complete.&#xA;INFO:     Finished server process [1]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You may receive a Python KeyboardInterrupt exception and stack trace. This can be safely ignored.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Starting Training&lt;/h3&gt; &#xA;&lt;p&gt;With VLLM stopped and the new data generated, the training process can be launched using the &lt;code&gt;ilab train&lt;/code&gt; command. By default, the training process saves a model checkpoint after every 4999 samples. You can adjust this using the &lt;code&gt;--num-samples&lt;/code&gt; parameter. Additionally, training defaults to running for 10 epochs, which can also be adjusted with the &lt;code&gt;--num-epochs&lt;/code&gt; parameter. Generally, more epochs are better, but after a certain point, more epochs will result in overfitting. It is typically recommended to stay within 10 or fewer epochs and to look at different sample points to find the best result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ilab train --num-epochs 9&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;RunningAvgSamplesPerSec=149.4829861942806, CurrSamplesPerSec=161.99957513920629, MemAllocated=22.45GB, MaxMemAllocated=29.08GB&#xA;throughput: 161.84935045724643 samples/s, lr: 1.3454545454545455e-05, loss: 0.840185821056366 cuda_mem_allocated: 22.45188570022583 GB cuda_malloc_retries: 0 num_loss_counted_tokens: 8061.0 batch_size: 96.0 total loss: 0.8581467866897583&#xA;Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84/84 [01:09&amp;lt;00:00,  1.20it/s]&#xA; total length: 2527 num samples 15 - rank: 6 max len: 187 min len: 149&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Serving the New Model&lt;/h4&gt; &#xA;&lt;p&gt;Once the training process has completed, the new model entries will be stored in the models directory with locations printed to the terminal&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;Generated model in /root/workspace/models/tuned-0504-0051:&#xA;.&#xA;./samples_4992&#xA;./samples_9984&#xA;./samples_14976&#xA;./samples_19968&#xA;./samples_24960&#xA;./samples_29952&#xA;./samples_34944&#xA;./samples_39936&#xA;./samples_44928&#xA;./samples_49920&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The same &lt;code&gt;ilab serve&lt;/code&gt; command can be used to serve the new model by passing the ‚Äìmodel option with the name and sample&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ilab serve --model tuned-0504-0051/samples_49920&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Chatting with the New Model&lt;/h4&gt; &#xA;&lt;p&gt;After VLLM has started with the new model, a chat session can be launched by creating a new terminal session and passing the same &lt;code&gt;--model&lt;/code&gt; parameter to chat (Note that if this does not match, you will receive a 404 error message). Ask it a question related to your taxonomy contributions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ilab chat --model tuned-0504-0051/samples_49920&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Example Chat Session with the New Model&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ system ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ&#xA;‚îÇ Welcome to InstructLab Chat w/                                        ‚îÇ&#xA;‚îÇ /INSTRUCTLAB/MODELS/TUNED-0504-0051/SAMPLES_49920 (type /h for help)  ‚îÇ&#xA;‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ&#xA;&#xA;&amp;gt;&amp;gt;&amp;gt; What are tonsils?&#xA;‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ /instructlab/models/tuned-0504-0051/samples_49920 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ&#xA;‚îÇ                                                                       ‚îÇ&#xA;‚îÇ Tonsils are a type of mucosal lymphatic tissue found in the           ‚îÇ&#xA;‚îÇ aerodigestive tracts of various mammals, including humans. In the     ‚îÇ&#xA;‚îÇ human body, the tonsils play a crucial role in protecting the body    ‚îÇ&#xA;‚îÇ from infections, particularly those caused by bacteria and viruses.   ‚îÇ&#xA;‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ elapsed 0.469 seconds ‚îÄ‚ïØ&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;To exit the session, type &lt;code&gt;exit&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Summary&lt;/h3&gt; &#xA;&lt;p&gt;That‚Äôs it! The purpose of a Developer Preview is to get something out to our users for early feedback. We realize there may be bugs. And we appreciate your time and effort if you‚Äôve made it this far. Chances are you hit some issues or needed to troubleshoot. We encourage you to file bug reports, feature requests, and ask us questions. See the contact information below for how to do that. Thank you!&lt;/p&gt; &#xA;&lt;h3&gt;How to contact us&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To report bugs or request features, use the GitHub issues page.&lt;/li&gt; &#xA; &lt;li&gt;For questions: send us an email &lt;a href=&#34;mailto:help-rhelai-devpreview@redhat.com&#34;&gt;help-rhelai-devpreview@redhat.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;For InstructLab: please see the community documentation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Known Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We have not tried this with Fedora (coming soon!)&lt;/li&gt; &#xA; &lt;li&gt;We intend to include a toolbox container inside the bootc container. For now, you can pull any toolbox image (e.g., Fedora Toolbx).&lt;/li&gt; &#xA; &lt;li&gt;RHUI-entitled hosts (e.g., on AWS) will require additional configuration to move from RHUI cloud auto-registration to Red Hat standard registration.&lt;/li&gt; &#xA; &lt;li&gt;Use subscription-manager with username/password or activation key, then run the following command: &lt;code&gt;$ sudo subscription-manager config --rhsm.manage_repos=1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;nvidia-smi&lt;/code&gt; to make sure the drivers work and can see the GPUs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nvtop&lt;/code&gt; (available in EPEL) to see whether the GPUs are being used (some code paths have CPU fallback, which we don‚Äôt want here)&lt;/li&gt; &#xA; &lt;li&gt;‚Äúno space left on device‚Äù errors (or similar) during container builds Ensure your build host has 400GB of storage.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make prune&lt;/code&gt; out of the training subdirectory. This will clean up old build artifacts.&lt;/li&gt; &#xA; &lt;li&gt;Sometimes, interrupting the container build process may lead to wanting a complete restart of the process. For those cases, we can instruct Podman to start from scratch and discard the cached layers. This is possible by passing the &lt;code&gt;--no-cache&lt;/code&gt; parameter to the build process&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make nvidia-bootc CONTAINER_TOOL_EXTRA_ARGS=&#34;--no-cache&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The building of accelerated images requires a lot of temporary disk space. In case you need to specify a directory for temporary storage, this can be done with the &lt;code&gt;TMPDIR&lt;/code&gt; environment variable:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;make &amp;lt;platform&amp;gt; TMPDIR=/path/to/tmp&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>