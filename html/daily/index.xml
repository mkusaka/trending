<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-30T01:29:53Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>wangyuchi369/InstructAvatar</title>
    <updated>2024-05-30T01:29:53Z</updated>
    <id>tag:github.com,2024-05-30:/wangyuchi369/InstructAvatar</id>
    <link href="https://github.com/wangyuchi369/InstructAvatar" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of the paper &#39;InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation&#39;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wangyuchi369/InstructAvatar/master/img/star.png&#34; alt=&#34;InstructAvatar&#34; style=&#34;height: 2rem;&#34;&gt; InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation &lt;/h1&gt; &#xA;&lt;!-- &lt;h4 align=&#34;center&#34;&gt;&#xA;  &lt;a href=&#34;https://wangyuchi369.github.io/&#34;&gt;Yuchi Wang&lt;/a&gt; &amp;nbsp; &#xA;  &lt;a href=&#34;https://renshuhuai-andy.github.io/&#34;&gt;Junliang Guo&lt;/a&gt; &amp;nbsp;&#xA;  Rundong Gao &amp;nbsp;&#xA;  &lt;a href=&#34;https://yaolinli.github.io/&#34;&gt;Linli Yao&lt;/a&gt; &amp;nbsp;&#xA;  &lt;a href=&#34;https://beeevita.github.io/&#34;&gt;Qingyan Guo&lt;/a&gt;&#xA;  &lt;/h4&gt;&#xA;  &lt;h4 align=&#34;center&#34;&gt;&#xA;  Kaikai An &amp;nbsp;&#xA;  &lt;a href=&#34;https://jianhongbai.github.io/&#34;&gt;Jianhong Bai&lt;/a&gt; &amp;nbsp;&#xA;  &lt;a href=&#34;https://xusun26.github.io/&#34;&gt;Xu Sun &lt;sup&gt;&amp;dagger;&lt;/sup&gt;&lt;/a&gt;&#xA;&lt;/h4&gt; --&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2405.15758&#34;&gt;[ArXiv]&lt;/a&gt; &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://wangyuchi369.github.io/InstructAvatar/&#34;&gt;[Demo Page]&lt;/a&gt; &amp;nbsp; &lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/wangyuchi369/InstructAvatar/master/img/demo.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- &lt;br&gt; --&gt; &#xA;&lt;h2&gt;‚ÄºÔ∏è Attention&lt;/h2&gt; &#xA;&lt;p&gt;This repository &lt;strong&gt;currently contains only the implementation of the demo website&lt;/strong&gt; for the paper &#34;InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation.&#34; ‚ö†Ô∏è&lt;/p&gt; &#xA;&lt;p&gt;Since InstructAvatar can generate and control photorealistic talking videos, it poses significant ethical risks of unintended misuse. We are currently undergoing the release process and internal compliance reviews. Please stay tuned!&lt;/p&gt; &#xA;&lt;p&gt;In the meantime, feel free to raise any issues in this repo if you have any questions! ü§ó&lt;/p&gt; &#xA;&lt;h2&gt;‚òï Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our projects helpful to your research, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{wang2024instructavatar,&#xA;      title={InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation}, &#xA;      author={Yuchi Wang and Junliang Guo and Jianhong Bai and Runyi Yu and Tianyu He and Xu Tan and Xu Sun and Jiang Bian},&#xA;      year={2024},&#xA;      eprint={2405.15758},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;!-- &lt;div align=center&gt;&#xA;&lt;img src=&#34;img/model.png&#34; width=&#34;800&#34; &gt;&#xA;&lt;/div&gt; --&gt; &#xA;&lt;!-- ## üí° Introduction&#xA;&#xA;&#xA;Diffusion models have demonstrated remarkable capabilities in text-to-image generation. However, their performance in image-to-text generation has lagged behind Auto-Regressive (AR) models, raising doubts about their applicability for such tasks. In this study, we revisit diffusion models, emphasizing their unique advantages compared to AR methods. We meticulously design a novel latent diffusion-based architecture, LaDiC, to further amplify the previously untapped potential of diffusion models in image-to-text generation.&#xA;&#xA;## üöÄ Method&#xA;&#xA;&lt;div align=center&gt;&#xA;&lt;img src=&#34;img/model_arch.png&#34; width=&#34;800&#34; &gt;&#xA;&lt;/div&gt;&#xA;&#xA;An overview of our LaDiC model. It mainly consists of the Image Encoder, Text Encoder, Diffuser, and Text Decoder. The diffusion process is depicted on the left, while the denoising process is depicted on the right. Initially, the caption $c$ is encoded into a text latent $x_0$ by the text encoder. Subsequently, diffusion process occurs within the textual latent space $\mathcal{X}$, where a diffuser is trained to restore the noisy text latent $x_t$ to its clean counterparts $\hat{x}_0$, guided by the associated image. Finally, the denoised text latent $\hat{x}_0$ is passed through a NAR text decoder to generate the final caption $\hat{c}$.&#xA;&#xA;## üìñ Experimental Results&#xA;&#xA;&lt;div align=center&gt;&#xA;&lt;img src=&#34;img/results.png&#34; width=&#34;800&#34; &gt;&#xA;&lt;/div&gt;&#xA;&#xA;Comparison results on COCO dataset. We can see that our model **achieves state-of-the-art performance across various metrics for both diffusion-based and traditional NAR models, and exhibits comparable performance with some well-established pretraining auto-regressive frameworks**, despite being trained on significantly less data.&#xA;&#xA;&lt;div align=center&gt;&#xA;&lt;img src=&#34;img/intro.png&#34; width=&#34;800&#34; &gt;&#xA;&lt;/div&gt;&#xA;&#xA;Apart from achieving exceptional performance, compare to AR methods, we also observe the superiority of our model in:&#xA;&#xA;- **Parallel Token Emission**: Diffusion-based model emits all tokens in parallel, effectively reducing the inference latency compared to autoregressive models, particularly as the length of the caption increases.&#xA;&#xA;- **Holistic Context Consideration**: Diffusion model takes into account a more comprehensive context, thereby helping to alleviate the error accumulation issues inherent in autoregressive models.&#xA;&#xA;- **Flexible Generation Approach**: In contrast to the unidirectional generation approach of AR models, the diffusion model adopts a more flexible manner of generation.&#xA;## ‚öôÔ∏è Environment&#xA;&#xA;Required packages and dependencies are listed in the `ladic.yaml` file. You can install the environment using Conda with the following command:&#xA;&#xA;```bash&#xA;conda env create -f ladic.yaml&#xA;conda activate ladic&#xA;```&#xA;&#xA;We also provide docker image as follows:&#xA;&#xA;```bash&#xA;docker pull wangyuchi/diffcap:python3.8&#xA;```&#xA;### Accelerate Configuration&#xA;&#xA;We use [accelerate package](https://huggingface.co/docs/accelerate/index) developed by Huggingface.&#xA;&#xA;Configure Accelerate by using the following command in the command line:&#xA;&#xA;```bash&#xA;accelerate config&#xA;```&#xA;&#xA;Answer the questions based on your actual setup. You will be prompted to specify the GPU to use, and other configurations can be left as default. For more information, refer to [this link](https://huggingface.co/docs/accelerate/v0.13.2/en/quicktour#launching-your-distributed-script).&#xA;&#xA;## üö¢ Datasets&#xA;&#xA;We test on the COCO dataset. You can download [MSCOCO dataset](https://cocodataset.org/#download) and place it into `datasets` folder.&#xA;&#xA;Meanwhile, we follow Karpathy split, and its annotation files can be found in its [orginial paper](https://cs.stanford.edu/people/karpathy/deepimagesent/). Our code will also automatically download these files and you may find them in `datasets/` folder.&#xA;&#xA;## üß∞ Required pretrained models&#xA;&#xA;In our LaDiC model, Text Encoder and Decoder are initialized from BERT-base-uncased, which can be downloaded from [Huggingface](https://huggingface.co/bert-base-uncased).&#xA;&#xA;As for image encoder, we utilized pretrained ViT in BLIP. You may download from [here](https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth) and put it into `pretrained_ckpt` folder. More information can be found in [BLIP&amp;#39;s official repo](https://github.com/salesforce/BLIP).&#xA;&#xA;&#xA;## üéá Training&#xA;&#xA;Launch the `main.py` script using Accelerate with the following command:&#xA;&#xA;```bash&#xA;accelerate launch main.py [--args]&#xA;```&#xA;&#xA;We list some important optional parameters as follows. The `notes` parameter is both a note to be placed at the top of the filename and the running name for [wandb](https://wandb.ai/site). More hyperparameters and their description can be found in `configs/`&#xA;&#xA;```bash&#xA;parser.add_argument(&#39;--notes&#39;, type=str, default=None, help=&#39;Note to be included in the trial name&#39;)&#xA;parser.add_argument(&#39;--bsz&#39;, type=int, default=5, help=&#39;batch size&#39;)&#xA;parser.add_argument(&#39;--seqlen&#39;, type=int, default=80, help=&#39;sequence length&#39;)&#xA;parser.add_argument(&#39;--epoch&#39;, type=int, default=10, help=&#39;epoch num&#39;)&#xA;parser.add_argument(&#39;--resume_epoch&#39;, type=int, default=0, help=&#39;start epoch of resume&#39;)&#xA;parser.add_argument(&#39;--resume_ckpt&#39;, type=str, default=None, help=&#39;resume or not&#39;)&#xA;parser.add_argument(&#39;--logdir&#39;, type=str, default=&#39;checkpoint&#39;, help=&#39;logdir&#39;)&#xA;```&#xA;&#xA;## ‚öñÔ∏è Evaluation&#xA;&#xA;Specify `MODEL_NAME` and `RESULLT_FILE` in `coco_eval.py` representing checkpoint to be evaluated and output path respectively. Then you can run&#xA;&#xA;```bash&#xA;python coco_eval.py&#xA;```&#xA;&#xA;## üìÜ TODO List&#xA;- [ ] Add more scripts for more flexible testing.&#xA;- [ ] Provide pretrained checkpoint.&#xA;- [x] Provide training and testing code.&#xA;- [x] Paper released on arXiv.&#xA;&#xA;## ‚òï Citation&#xA; If you find our projects helpful to your research, please consider citing our paper:&#xA;```&#xA;@misc{wang2024ladic,&#xA;      title={LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?}, &#xA;      author={Yuchi Wang and Shuhuai Ren and Rundong Gao and Linli Yao and Qingyan Guo and Kaikai An and Jianhong Bai and Xu Sun},&#xA;      year={2024},&#xA;      eprint={2404.10763},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;```&#xA;For any issues or further discussions, feel free to contact wangyuchi369@gmail.com&#xA;## üçÉ Acknowledgements&#xA;Our code is heavily based on projects like [diffusion-image-captioning](https://github.com/xu-shitong/diffusion-image-captioning), [BLIP](https://github.com/salesforce/BLIP) and [Huggingface transformers](https://github.com/huggingface/transformers). Thanks for their splendid works! --&gt;</summary>
  </entry>
  <entry>
    <title>xyhelper/chatgpt-share-server</title>
    <updated>2024-05-30T01:29:53Z</updated>
    <id>tag:github.com,2024-05-30:/xyhelper/chatgpt-share-server</id>
    <link href="https://github.com/xyhelper/chatgpt-share-server" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT-Share-Server&lt;/h1&gt; &#xA;&lt;p&gt;‰æøÊç∑ÁöÑË¥¶Âè∑ÂÖ±‰∫´ÊúçÂä°&lt;/p&gt; &#xA;&lt;h2&gt;‰ΩøÁî®ÂâçÂøÖÁúã&lt;/h2&gt; &#xA;&lt;h3&gt;Êú¨ÊúçÂä°‰∏∫ÂïÜ‰∏öÊúçÂä°ÔºåËá™2024Âπ¥4Êúà16Êó•0:00‰∏çÂÜçÊèê‰æõÂÖçË¥πÊé•ÂÖ•ÁÇπ„ÄÇÈúÄ‰ªòË¥π‰ΩøÁî®„ÄÇ&lt;/h3&gt; &#xA;&lt;p&gt;1.ÊµÅÈáèÂ§ß‰ΩøÁî®‰ªòË¥πÊé•ÂÖ•ÁÇπÊàñËÄÖÂ∞èÊµÅÈáèÊãºËΩ¶‰ªòË¥πÊé•ÂÖ•ÁÇπÁöÑÔºåÂèØÊâìÂºÄ‰ª•‰∏ãÈìæÊé•ÈÄâÊã©ÈÄÇÂêàËá™Â∑±ÁöÑÊñπÊ°à &lt;a href=&#34;https://xyhelper.cn/access&#34;&gt;https://xyhelper.cn/access&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;2.ÊµÅÈáèÂ∞èÊàñËÄÖÂ∞èÂõ¢ÈòüËá™Áî®ÁöÑÔºåÂèØ‰ΩøÁî®ËüëËûÇv1ÔºåÁõÆÂâçËüëËûÇv1ÂèØÊ≠£Â∏∏‰ΩøÁî®Ôºå‰∫ÜËß£ËØ¶ÊÉÖËØ∑ËÆøÈóÆ: CockroachAiÔºàÂèàÂêçËüëËûÇÔºâ &lt;a href=&#34;https://github.com/cockroachai/cockroachai&#34;&gt;https://github.com/cockroachai/cockroachai&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ÂºÄÊ∫êÈÉ®ÂàÜ‰ª£Á†ÅÂèØÁî®‰∫é‰∫åÊ¨°ÂºÄÂèëÔºå‰ΩÜÂ¶ÇÊûúÊÇ®ÁöÑ‰∫åÊ¨°ÂºÄÂèëËÑ±Á¶ªÊú¨Á´ôÁöÑÊúçÂä°ÔºåÊÇ®ÈúÄË¶ÅËøõË°å‰ª•‰∏ãÊìç‰ΩúÔºö&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;‰øÆÊîπ‰ª£Á†Å‰∏≠ÁöÑÊâÄÊúâ‰∏éÊú¨Á´ôÁõ∏ÂÖ≥ÁöÑÊñáÂ≠ó„ÄÅÂõæÁâá„ÄÅÈìæÊé•Á≠â&lt;/li&gt; &#xA; &lt;li&gt;‰øÆÊîπ‰ª£Á†Å‰∏≠ÁöÑÊâÄÊúâ‰∏éÊú¨Á´ôÁõ∏ÂÖ≥ÁöÑÊé•Âè£Âú∞ÂùÄ&lt;/li&gt; &#xA; &lt;li&gt;‰øÆÊîπ‰ª£Á†Å‰∏≠ÁöÑÊâÄÊúâ‰∏éÊú¨Á´ôÁõ∏ÂÖ≥ÁöÑÈÖçÁΩÆ&lt;/li&gt; &#xA; &lt;li&gt;‰øÆÊîπ‰ª£Á†Å‰∏≠ÁöÑÊâÄÊúâ‰∏éÊú¨Á´ôÁõ∏ÂÖ≥ÁöÑÂüüÂêç&lt;/li&gt; &#xA; &lt;li&gt;ÂèñÊ∂àÊâÄÊúâÂØπÊú¨Á´ôËµÑÊ∫êÁöÑË∞ÉÁî®(ËµÑÊ∫êËåÉÂõ¥ÂåÖÊã¨ xyhelper.cn closeai.biz freegpts.org Á≠â)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ÈÉ®ÁΩ≤&lt;/h2&gt; &#xA;&lt;p&gt;Âø´ÈÄüÈÉ®ÁΩ≤&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sSfL https://raw.githubusercontent.com/xyhelper/chatgpt-share-server/deploy/quick-install.sh | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ÊñáÊ°£&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chatgpt-share-server.xyhelper.cn&#34;&gt;https://chatgpt-share-server.xyhelper.cn&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;È¢ÑËßà&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chatgpt-share-server.xyhelper.cn/preview&#34;&gt;https://chatgpt-share-server.xyhelper.cn/preview&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xyhelper/chatgpt-share-server/master/docs/.vuepress/public/images/chat.png&#34; alt=&#34;preview&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>