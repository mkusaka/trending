<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-13T01:39:44Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rlleshi/phar</title>
    <updated>2022-06-13T01:39:44Z</updated>
    <id>tag:github.com,2022-06-13:/rlleshi/phar</id>
    <link href="https://github.com/rlleshi/phar" rel="alternate"></link>
    <summary type="html">&lt;p&gt;deep learning porn classifier&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;P-HAR: Porn Human Action Recognition&lt;/h1&gt; &#xA;&lt;p&gt;This is just a fun, side-project to see how State-of-the-art (SOTA) Human Action Recognition (HAR) models fare in the pornographic domain. HAR is a relatively new, active field of research in the deep learning domain, its goal being the identification of human actions from various input streams (e.g. video or sensor).&lt;/p&gt; &#xA;&lt;p&gt;The pornography domain is interesting from a technical perspective because of its inherent difficulties. Light variations, occlusions, and a tremendous variations of different camera angles and filming techniques (POV, dedicated camera person) make position (action) recognition hard. We can have two identical positions (actions) and yet be captured in such a different camera perspective to entirely confuse the model in its predictions.&lt;/p&gt; &#xA;&lt;p&gt;This repository uses three different input streams in order to get the best possible results: rgb frames, human skeleton, and audio. Correspondingly three different models are trained on these input streams and their results are merged through late fusion.&lt;/p&gt; &#xA;&lt;p&gt;The best current accuracy reached by this multi-model model currently is &lt;strong&gt;75.64%&lt;/strong&gt;, which is promising considering the small &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/#dataset&#34;&gt;training set&lt;/a&gt;. This result will be improved in the future.&lt;/p&gt; &#xA;&lt;p&gt;The models work on spatio-temporal data, meaning that they processes video clips rather than single images (&lt;a href=&#34;https://github.com/ryanjay0/miles-deep&#34;&gt;miles-deep&lt;/a&gt; is using single images for example). This is an inherently superior way of performing action recognition.&lt;/p&gt; &#xA;&lt;p&gt;Currently, 17 actions are supported. You can find the complete list &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/annotations/annotations.txt&#34;&gt;here&lt;/a&gt;. More data would be needed to further improve the models (help is welcomed). Read on for more information!&lt;/p&gt; &#xA;&lt;h2&gt;Supported Features&lt;/h2&gt; &#xA;&lt;p&gt;First download the HAR models &lt;a href=&#34;https://github.com/rlleshi/phar/releases/tag/v1.0.0&#34;&gt;here&lt;/a&gt;. Then move them inside the &lt;code&gt;checkpoints/har&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Video Demo&lt;/h3&gt; &#xA;&lt;p&gt;Input a video and get a demo with the top predictions every 7 seconds by default.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python src/demo/multimodial_demo.py video.mp4 demo.mp4&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, the results can also be dumped in a json file by specifying the output file as such.&lt;/p&gt; &#xA;&lt;p&gt;If you only want to use the RGB &amp;amp; Skeleton model, then you can disable the audio model like so:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python src/demo/multimodial_demo.py video.mp4 demo.json --audio-checkpoint &#39;&#39; --coefficients 0.5 1.0 --verbose&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/#multimodial-demo&#34;&gt;detailed usage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Timestamp Generator&lt;/h3&gt; &#xA;&lt;p&gt;Use the flag &lt;code&gt;--timestamps&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python src/demo/multimodial_demo.py video.mp4 demo.json --timestamps&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Tag Generator&lt;/h3&gt; &#xA;&lt;p&gt;Given the predictions generated by the multimodial demo (in json), we can grab the top 3 tags (by default) like so:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python src/top_tags.py demo.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checkout the &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/#late-fusion&#34;&gt;detailed usage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Content Filtering&lt;/h3&gt; &#xA;&lt;p&gt;TODO: depending if people need it.&lt;/p&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;Depends if people find this project useful. Currently one has to install the relevant libraries to use these models. See the installation section below.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation &amp;amp; Usages&lt;/h2&gt; &#xA;&lt;p&gt;The idea behind this project is to try and apply the latest deep learning techniques (i.e. &lt;a href=&#34;https://scholar.google.com/scholar?hl=en&amp;amp;as_sdt=0%2C5&amp;amp;q=human+action+recognition&amp;amp;btnG=&#34;&gt;human action recognition&lt;/a&gt;) in the pornographic domain.&lt;/p&gt; &#xA;&lt;p&gt;Once we have detailed information about the kind of actions/positions that are happening in a video a number of uses-cases can apply:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Improving the recommender system&lt;/li&gt; &#xA; &lt;li&gt;Automatic tag generator&lt;/li&gt; &#xA; &lt;li&gt;Automatic timestamp generator (when does an action start and finish)&lt;/li&gt; &#xA; &lt;li&gt;Cutting content out (for example non-sexual content)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on &lt;a href=&#34;https://github.com/open-mmlab/mmaction2&#34;&gt;MMAction2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The following installation instructions are for ubuntu (hence should also work for Windows WSL). Check the links for details if you are interested in other operating systems.&lt;/p&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Clone this repo and its submodules: &lt;code&gt;git clone --recurse-submodules git@github.com:rlleshi/phar.git&lt;/code&gt; and then create and environment with python 3.8+.&lt;/li&gt; &#xA; &lt;li&gt;Install torch (of course, it is recommended that you have CUDA &amp;amp; CUDNN installed).&lt;/li&gt; &#xA; &lt;li&gt;Install Mim: &lt;code&gt;pip install git+https://github.com/open-mmlab/mim.git&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install MMAction2 through &lt;code&gt;mim&lt;/code&gt;: &lt;code&gt;mim install mmaction2 -f https://github.com/open-mmlab/mmaction2.git&lt;/code&gt;. If installing with mim fails, then checkout the detailed installation steps &lt;a href=&#34;https://mmaction2.readthedocs.io/en/latest/install.html&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install MMPose, &lt;a href=&#34;https://mmpose.readthedocs.io/en/latest/install.html&#34;&gt;link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install MMDetection, &lt;a href=&#34;https://mmdetection.readthedocs.io/en/latest/get_started.html#installation&#34;&gt;link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Install extra dependencies: &lt;code&gt;pip install -r requirements/extra.txt&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;The SOTA results are archieved by late-fusing three models based on three input streams. This results in significant improvements compared to only using an RGB-based model. Since more than one action might happen at the same time (and moreover, currently, some of the actions/positions have are conceptually overlapping), it is best to consider the top 2 accuracy as a performance measurement. Hence, currently the multimodial model has a &lt;code&gt;~75%&lt;/code&gt; accuracy. However, since the dataset is quite small and in total only ~50 experiments have been performed, there is a lot of room for improvement.&lt;/p&gt; &#xA;&lt;h3&gt;Multi-Modial (Rgb + Skeleton + Audio)&lt;/h3&gt; &#xA;&lt;p&gt;The best performing models (performance &amp;amp; runtime wise) are &lt;code&gt;timesformer&lt;/code&gt; for the RGB stream, &lt;code&gt;poseC3D&lt;/code&gt; for the skeleton stream, and &lt;code&gt;resnet101&lt;/code&gt; for the Audio stream. The results of these models are fused together through late fusion. The models do not have the same importance in the late fusion scoring scheme. Currently the fine-tuned weights are: &lt;code&gt;0.5; 0.6; 1.0&lt;/code&gt; for the RGB, skeleton &amp;amp; audio model respectively.&lt;/p&gt; &#xA;&lt;p&gt;Another approach would be to train a model with two of the input streams at a time (i.e. rgb+skeleton &amp;amp; rgb+audio) and then perhaps combine their results. But this wouldn&#39;t work due to the nature of the data. When it comes to the audio input streams, it can only be exploited for certain actions (e.g. &lt;code&gt;deepthroat&lt;/code&gt; due to the gag reflex or &lt;code&gt;anal&lt;/code&gt; due to a higher pitch), while for others it&#39;s not possible to derive any insight from their audio (e.g. missionary, doggy and cowgirl do not have any special characteristics to set them apart from an audio perspective).&lt;/p&gt; &#xA;&lt;p&gt;Likewise, the skeleton-based model can only be used in those instances where the pose estimation is accurate above a certain confidence threshold (for these experiments the threshold used was 0.4). For example, for actions such as &lt;code&gt;scoop-up&lt;/code&gt; or &lt;code&gt;the-snake&lt;/code&gt; it&#39;s hard to get an accurate pose estimation in most camera angles due to the proximity of the human bodies in the frame (the poses get fuzzy and mixed up). This then influences the accuracy of the HAR model negatively. However, for actions such as doggy, cowgirl or missionary, the pose estimation is generally good enough to train a HAR model.&lt;/p&gt; &#xA;&lt;p&gt;However, if we have a bigger dataset, then we will probably have enough instances of clean samples for the difficult actions such as to train all (17) of them with a skeleton-based model. Skeleton based models are according to the current SOTA literature superior to the rgb-based ones. Ideally of course, the pose estimation models should also be fine tuned in the sex domain in order to get a better overall pose estimation.&lt;/p&gt; &#xA;&lt;h4&gt;Metrics&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Weights&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Top 1 Accuracy: 0.6362 &lt;br&gt; Top 2 Accuracy: 0.7524 &lt;br&gt; Top 3 Accuracy: 0.8155 &lt;br&gt; Top 4 Accuracy: 0.8521 &lt;br&gt; Top 5 Accuracy: 0.8771&lt;/td&gt; &#xA;   &lt;td&gt;Rgb: 0.5 &lt;br&gt; Skeleton: 0.6 &lt;br&gt; Audio: 1.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;RGB model - &lt;a href=&#34;https://arxiv.org/abs/2102.05095&#34;&gt;TimeSformer&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The best results for a 3D RGB model are achieved by the attention-based TimeSformer architecture. This model is also very fast in inference (~0.53s / 7s clips).&lt;/p&gt; &#xA;&lt;h4&gt;Metrics&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Training Speed&lt;/th&gt; &#xA;   &lt;th&gt;Complexity&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;top1_acc 0.5669 &lt;br&gt; top2_acc 0.6834 &lt;br&gt; top3_acc 0.7632 &lt;br&gt; top4_acc 0.8096 &lt;br&gt; top5_acc 0.8411&lt;/td&gt; &#xA;   &lt;td&gt;Avg iter time: 0.3472 s/iter&lt;/td&gt; &#xA;   &lt;td&gt;Flops: 100.96 GFLOPs &lt;br&gt; Params: 121.27 M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Loss&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/metrics/timesformer_loss.jpg&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Classes&lt;/h4&gt; &#xA;&lt;p&gt;All 17 annotations. See &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/annotations/annotations.txt&#34;&gt;annotations&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Skeleton model - &lt;a href=&#34;https://arxiv.org/abs/2104.13586&#34;&gt;PoseC3D&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;The best results for a skeleton-based model are achieved by the CNN-based PoseC3D architecture. This model is also fast in inference (~3.3s / 7s clips).&lt;/p&gt; &#xA;&lt;h4&gt;Metrics&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Training Speed&lt;/th&gt; &#xA;   &lt;th&gt;Complexity&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;top1_acc 0.8130 &lt;br&gt; top2_acc 0.9191 &lt;br&gt; top3_acc 0.9748&lt;/td&gt; &#xA;   &lt;td&gt;Avg iter time: 0.8616 s/iter&lt;/td&gt; &#xA;   &lt;td&gt;Flops: 17.83 GFLOPs &lt;br&gt; Params: 2.0 M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/metrics/skeleton_cm.png&#34;&gt;confusion matrix&lt;/a&gt; for a detailed overview of the performance.&lt;/p&gt; &#xA;&lt;h4&gt;Loss&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/metrics/posec3d_loss.jpg&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Classes&lt;/h4&gt; &#xA;&lt;p&gt;6 annotations. See &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/annotations/annotations_pose.txt&#34;&gt;annotations&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Audio Model - Simple ResNet based on &lt;a href=&#34;https://arxiv.org/abs/2001.08740&#34;&gt;Audiovisual SlowFast&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;A simple ResNet 101 (with some small tweaks) was used. This model definitely needs to be swapped with a better architecture. It is very fast in inference (0.05s / 7s audio clips).&lt;/p&gt; &#xA;&lt;h4&gt;Metrics&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Accuracy&lt;/th&gt; &#xA;   &lt;th&gt;Training Speed&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;top1_acc 0.6867 &lt;br&gt; top2_acc 0.9038 &lt;br&gt; top3_acc 0.9663&lt;/td&gt; &#xA;   &lt;td&gt;Avg iter time: 0.2747 s/iter&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/metrics/audio_cm.png&#34;&gt;confusion matrix&lt;/a&gt; for a detailed overview of the performance.&lt;/p&gt; &#xA;&lt;h4&gt;Loss&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/metrics/audio_loss.jpg&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Classes&lt;/h4&gt; &#xA;&lt;p&gt;4 annotations. See &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/annotations/annotations_audio.txt&#34;&gt;annotations&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;First things first, &lt;a href=&#34;https://www.womenshealthmag.com/sex-and-love/a19943165/sex-positions-guide/&#34;&gt;here&lt;/a&gt; is a list of definitions of the sex positions used in this project in case there is any confusion. &lt;code&gt;fondling&lt;/code&gt;, in addition to the meaning of the word, was also thought of as a general placeholder, e.g. when it is unclear what action there is. In reality, however, its ability to be a general placeholder is limited because I only got 48 minutes of data for this action.&lt;/p&gt; &#xA;&lt;p&gt;The gathered dataset is very inclusive and consists of a variety of recordings such as POV, professionally filmed, amateur, with or without a dedicated camera person, etc. It also includes all kinds of environments, people, and camera angles. The problem is probably much easier to solve if only professional recordings with a dedicated camera person are used and hence this was avoided.&lt;/p&gt; &#xA;&lt;p&gt;In general, a train/val split of 0.8/0.2 was used for all the datasets. The length of the clips in training &amp;amp; validation sets currently is 7 seconds (the main motivation was to include the more ephemeral actions such as &lt;code&gt;cumshot&lt;/code&gt; or &lt;code&gt;kissing&lt;/code&gt;). In total there were around 600 videos amounting to &lt;strong&gt;2674&lt;/strong&gt; minutes of footage. Check out the &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/annotation_distribution(min).json&#34;&gt;annotation distribution&lt;/a&gt; in time (minutes) for each of the 17 classes for more information. The dataset was not perfectly annotated but the number of wrong annotations should be small and hence the drop in performance should be minimal.&lt;/p&gt; &#xA;&lt;p&gt;In general, it can be said that this is a small dataset. Normally ~44 hours of footage would be enough for 17 actions. However, each position has a tremendous variety when it comes to camera perspectives, which makes the recognition task hard if there aren&#39;t enough samples. This would also mean that we should ideally have the same amount of footage for each different perspective. However, labeling the dataset was already very time-consuming and I didn&#39;t keep track of this point.&lt;/p&gt; &#xA;&lt;p&gt;A HAR model trained on 3D poses might be able solve this camera-perspective problem. However, due to the fact that 3D pose estimation is less accurate than 2D pose estimation, and I already noticed problems with the accuracy of the 2D (see &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/#2d-pose&#34;&gt;here&lt;/a&gt;), this has not been tried (yet). Ideally, however, if the dataset is big enough then the camera perspective problem should be naturally solved.&lt;/p&gt; &#xA;&lt;p&gt;The dataset is also slightly imbalanced, which actually makes the rgb models slightly biased towards the positions (actions) that have more data.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to help with doubling the current size of the dataset, please do open an issue.&lt;/p&gt; &#xA;&lt;h3&gt;RGB&lt;/h3&gt; &#xA;&lt;p&gt;In total there are ~17.6K training clips and ~4.9k val clips. &lt;a href=&#34;https://raw.githubusercontent.com/rlleshi/phar/master/resources/ann_dist_clip.jpg&#34;&gt;This&lt;/a&gt; plot shows the number of clips for each class. The RGB is considered the kernel input modality given that the audio modality is only applied to four classes and that the skeleton modality is rather fickle because of the accuracy of 2D pose estimation. Various data augmentation techniques were applied such as rescaling, cropping, flipping, color inversion, gaussian blur, elastic transformation, affine transformation, etc. This further improves the accuracy of the model.&lt;/p&gt; &#xA;&lt;h3&gt;(2D) Pose&lt;/h3&gt; &#xA;&lt;p&gt;Due to the variety of positions and camera angles, which make the pose estimation difficult as human bodies overlap and are too close, it&#39;s only feasible to apply HAR on skeleton data on a few of the actions. The clips generated for the RGB dataset were filtered based on two criteria:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;The confidence of the pose information. Minimal confidence of 0.4 was chosen.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The number of frames in a clip that have a confidence higher than the minimal confidence score. Here a 0.4 rate was also used. In other words, if we have a 7s clip of 210 frames and only 70 frames have pose information with confidence higher than 0.4, then we exclude this clip from the pose dataset because only 33% of the frames have a confidence higher than 0.4 and our minimum threshold is 40%.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;As a result, the pose dataset is significantly smaller than the original RGB dataset. Whereas there are about 4.9K testing clips for the RGB dataset, the pose dataset has only 815 clips. Therefore a bigger dataset is a must here so that we are able to train the skeleton model on all 17 actions.&lt;/p&gt; &#xA;&lt;h3&gt;Audio&lt;/h3&gt; &#xA;&lt;p&gt;As a preliminary pre-processing step audios that are not loud enough were first pruned from the dataset. The best results were achieved by prunning the bottom 20% of the quietest audios.&lt;/p&gt; &#xA;&lt;p&gt;In total there are about 5.9K training clips &amp;amp; 1.5K validation clips.&lt;/p&gt; &#xA;&lt;h2&gt;Script Docs&lt;/h2&gt; &#xA;&lt;h3&gt;Multimodial Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python src/demo/multimodial_demo.py ${VIDEO_FILE} ${OUTPUT_FILE}&#xA;    [--det-config ${HUMAN_DETECTION_CONFIG_FILE}] \&#xA;    [--det-checkpoint ${HUMAN_DETECTION_CHECKPOINT}] \&#xA;    [--pose-config ${HUMAN_POSE_ESTIMATION_CONFIG_FILE}] \&#xA;    [--pose-checkpoint ${HUMAN_POSE_ESTIMATION_CHECKPOINT}] \&#xA;    [--skeleton-config ${SKELETON_BASED_ACTION_RECOGNITION_CONFIG_FILE}] \&#xA;    [--skeleton-checkpoint ${SKELETON_BASED_ACTION_RECOGNITION_CHECKPOINT}] \&#xA;    [--rgb-config ${RGB_BASED_ACTION_RECOGNITION_CONFIG_FILE}] \&#xA;    [--rgb-checkpoint ${RGB_BASED_ACTION_RECOGNITION_CHECKPOINT}] \&#xA;    [--audio-config ${AUDIO_BASED_ACTION_RECOGNITION_CONFIG_FILE}] \&#xA;    [--audio-checkpoint ${AUDIO_BASED_ACTION_RECOGNITION_CHECKPOINT}] \&#xA;    [--det-score-thr ${HUMAN_DETECTION_SCORE_THRE}] \&#xA;    [--label-maps ${LIST_OF_ACTION_ANNOTATION_FILES}] \&#xA;    [--num-processes ${NUM_PROC_USED_FOR_SUBCLIP_EXTRACTION}] \&#xA;    [--subclip-len ${PREDICTION_WINDOW}] \&#xA;    [--device ${DEVICE}] \&#xA;    [--coefficients ${COEFFICIENT_WEIGHTS}] \&#xA;    [--pose-score-thr ${POSE_ESTIMATION_SCORE_THRESHOLD}] \&#xA;    [--correct-rate ${RATE_OF_CORRECT_FRAMES_FOR_SKELETON_RECOGNITION}] \&#xA;    [--loudness-weights ${LOUDNESS_THRESHOLD_FOR_AUDIOS}] \&#xA;    [--topk ${TOP_K_ACCURACY}]&#xA;    [--timestamps]&#xA;    [--verbose]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Late Fusion&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python src/top_tags.py ${JSON_FILE}&#xA;    [--topk ${TOP_K_ACCURACY}]&#xA;    [--label-map ${ANNOTATION_FILE}]&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>osrf/gazebo_models</title>
    <updated>2022-06-13T01:39:44Z</updated>
    <id>tag:github.com,2022-06-13:/osrf/gazebo_models</id>
    <link href="https://github.com/osrf/gazebo_models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Gazebo database of SDF models. This is a predecessor to https://app.gazebosim.org&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This repository holds the &lt;a href=&#34;http://gazebosim.org&#34;&gt;Gazebo&lt;/a&gt; model database.&lt;/p&gt; &#xA;&lt;p&gt;Learn more about the database &lt;a href=&#34;http://gazebosim.org/tutorials?tut=model_structure&amp;amp;cat=build_robot&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Learn how to contribute models &lt;a href=&#34;http://gazebosim.org/tutorials?tut=model_contrib&amp;amp;cat=build_robot&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>RootMyTV/RootMyTV.github.io</title>
    <updated>2022-06-13T01:39:44Z</updated>
    <id>tag:github.com,2022-06-13:/RootMyTV/RootMyTV.github.io</id>
    <link href="https://github.com/RootMyTV/RootMyTV.github.io" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RootMyTV is a user-friendly exploit for rooting/jailbreaking LG webOS smart TVs.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/RootMyTV/RootMyTV.github.io/main/img/header_logo.png&#34; alt=&#34;RootMyTV header image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;RootMyTV is a user-friendly exploit for rooting/jailbreaking LG webOS smart TVs.&lt;/p&gt; &#xA;&lt;p&gt;It bootstraps the installation of the &lt;a href=&#34;https://github.com/webosbrew/webos-homebrew-channel&#34;&gt;webOS Homebrew Channel&lt;/a&gt;, and allows it to run with elevated privileges. The Homebrew Channel is a community-developed open source app, that makes it easier to develop and install 3rd party software. &lt;a href=&#34;https://github.com/webosbrew/webos-homebrew-channel&#34;&gt;Find out more about it here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you want the full details of how the exploit works, &lt;a href=&#34;https://raw.githubusercontent.com/RootMyTV/RootMyTV.github.io/main/#research-summary-and-timeline&#34;&gt;skip ahead to our writeup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Is my TV vulnerable?&lt;/h1&gt; &#xA;&lt;p&gt;At the time of writing the original exploit (RootMyTV v1 - 2021-05-15), all webOS versions between 3.4 and 6.0 we tested (TVs released between mid-2017 and early-2021) are supported by this exploit chain. Around June-July 2021 LG started rolling out updates which added some minor mitigations that broke our original exploit chain.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;At the time of writing (RootMyTV v2 - 2022-01-05)&lt;/strong&gt;, all webOS versions between 4.x and 6.2+ we tested (TVs released between early-2018 and late-2021) are supported by the new exploit chain.&lt;/p&gt; &#xA;&lt;p&gt;Some versions between 3.4 and 3.9 may be supported by RootMyTV v2, but your mileage may vary.&lt;/p&gt; &#xA;&lt;p&gt;Note: this versioning refers to the &#34;webOS TV Version&#34; field in the settings menu, &lt;em&gt;not&lt;/em&gt; the &#34;Software Version&#34; field.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you want to protect your TV against remote exploitation, please see the &lt;a href=&#34;https://raw.githubusercontent.com/RootMyTV/RootMyTV.github.io/main/#mitigation-note&#34;&gt;relevant section&lt;/a&gt; of our writeup and/or await an update from LG.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Usage Instructions&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step Zero (disclaimer):&lt;/strong&gt; Be aware of the risks. Rooting your TV is (unfortunately) not supported by LG, and although we&#39;ve done our best to minimise the risk of damage, we cannot make any guarantees. This may void your warranty.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;(Pre-webOS 4.0) Make sure &#34;Settings → Network → LG Connect Apps&#34; feature is enabled.&lt;/li&gt; &#xA; &lt;li&gt;Developer Mode app &lt;strong&gt;must be uninstalled before rooting&lt;/strong&gt;. Having this application installed will interfere with RootMyTV v2 exploit, and its full functionality is replaced by Homebrew Channel built-in SSH server.&lt;/li&gt; &#xA; &lt;li&gt;Open the TV&#39;s web browser app and navigate to &lt;a href=&#34;https://rootmy.tv&#34;&gt;https://rootmy.tv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&#34;Slide to root&#34; using a Magic Remote or press button &#34;5&#34; on your remote.&lt;/li&gt; &#xA; &lt;li&gt;Accept the security prompt.&lt;/li&gt; &#xA; &lt;li&gt;The exploit will proceed automatically. The TV will reboot itself once during this process, and optionally a second time to finalize the installation of the Homebrew Channel. On-screen notifications will indicate the exploit&#39;s progress. On webOS 6.x &lt;strong&gt;Home Screen needs to be opened&lt;/strong&gt; for notifications/prompts to show up.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Your TV should now have Homebrew Channel app installed.&lt;/p&gt; &#xA;&lt;p&gt;By default system updates and remote root access are disabled on install. If you want to change these settings go to Homebrew Channel → Settings. Options there are applied after a reboot.&lt;/p&gt; &#xA;&lt;p&gt;For exploiting broken TVs, check out the information &lt;a href=&#34;https://raw.githubusercontent.com/RootMyTV/RootMyTV.github.io/main/docs/HEADLESS.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Why rooting&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Unlimited &#34;Developer Mode&#34; access&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;While LG allows willing Homebrew developers/users to install unofficial applications onto their TVs, official method requires manual renewal of &#34;developer mode session&#34;, which expires after 50 hours of inactivity.&lt;/li&gt; &#xA;   &lt;li&gt;Some of the &lt;a href=&#34;https://repo.webosbrew.org&#34;&gt;amazing homebrew&lt;/a&gt; that has been built/ported onto webOS would likely never be accepted onto LG&#39;s official Content Store.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Lower level user/application access&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This allows willing developers to research webOS system internals, which will result in creation of amazing projects, like &lt;a href=&#34;https://github.com/TBSniller/piccap&#34;&gt;PicCap&lt;/a&gt; (high performance video capture used for DIY immersive ambient lighting setups), or access to some interesting features like customization of system UI, remote adjustment of certain TV configuration options, and others.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Is it safe?&lt;/h3&gt; &#xA;&lt;p&gt;While we cannot take any responsibility for Your actions, we have not encountered any bricks due to rooting. If you only use trusted software from &lt;a href=&#34;https://repo.webosbrew.org&#34;&gt;official Homebrew Channel repository&lt;/a&gt;, then you should be safe.&lt;/p&gt; &#xA;&lt;h3&gt;Will this void my warranty?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is not a legal advice.&lt;/strong&gt; At least in the EU, &lt;a href=&#34;https://piana.eu/root/&#34;&gt;rooting and other software modifications are generally deemed to be legal&lt;/a&gt; and should not be a basis for voiding your warranty.&lt;/p&gt; &#xA;&lt;h3&gt;How do I get rid of this?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.lg.com/us/support/video-tutorials/lg-tv-how-to-reset-my-lg-smart-tv-CT10000020-1441914092672&#34;&gt;Factory reset&lt;/a&gt; should remove all root-related configuration files.&lt;/p&gt; &#xA;&lt;p&gt;We don&#39;t have a convenient tool for root removal &lt;em&gt;without factory reset&lt;/em&gt;, though a knowledgable person may be able to &lt;a href=&#34;https://github.com/webosbrew/webos-homebrew-channel/issues/11&#34;&gt;remove our customizations manually&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Are system updates possible?&lt;/h3&gt; &#xA;&lt;p&gt;While updates are technically possible, if LG patches the exploit, you might end up &#34;locked out&#34; and unable to re-root your TV if you somehow lose access. We also can&#39;t predict how future updates will affect our techniques used to elevate and operate the Homebrew Channel app.&lt;/p&gt; &#xA;&lt;h3&gt;Will this break Netflix/YouTube/AmazonVideo?&lt;/h3&gt; &#xA;&lt;p&gt;No. This does not break or limit access to subscription services or other DRMed content.&lt;/p&gt; &#xA;&lt;p&gt;However, staying on very old firmware version (which may be required for keeping root access persistent) may limit Your access to LG Content Store application installs, updates, or (rarely) launches. Workarounds for this &lt;a href=&#34;https://github.com/webosbrew/webos-homebrew-channel/issues/75&#34;&gt;are in the works&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How do I update from RootMyTV v1? (released 2021/05)&lt;/h3&gt; &#xA;&lt;p&gt;If you are not going to update your TV Software Version to the one that is already patched (most 4.x+ released after 2021/06) there is no need to update. New chain does not bring any new features - the most sensible thing you can do is to update your Homebrew Channel app.&lt;/p&gt; &#xA;&lt;p&gt;If you are already rooted on downgraded/pre-2021-06 firmware version and want to upgrade further, doing an official software update will remove existing root files and homebrew applications. Running RootMyTV v2 then will reenable root access again. You will need to reinstall removed applications yourself.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If you know what you are doing&lt;/strong&gt; and want to persist installed applications, you need to remove &lt;code&gt;/media/cryptofs/apps/usr/palm/services/com.palmdts.devmode.service/start-devmode.sh&lt;/code&gt; file right before an update (without rebooting inbetween), and then run RootMyTV v2 right on first boot after software update.&lt;/p&gt; &#xA;&lt;h3&gt;I quickly turned my TV on and off and it&#39;s really angry about Failsafe Mode&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;If &#34;Failsafe Mode&#34; got tripped on your TV and it&#39;s showing angry notifications, go to Homebrew Channel → Settings, switch &#34;Failsafe Mode&#34; off and press &#34;Reboot&#34;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&#34;Failsafe Mode&#34; is a mode where none of our system customizations are enabled and only an emergency remote access server gets started up.&lt;/p&gt; &#xA;&lt;p&gt;This mode gets enabled automatically when the TV crashes, gets its power removed or is shut down during early system startup. In order to reduce chances of that happening we recommend enabling &#34;Quick Start+&#34; setting in webOS System Settings General tab. This will make the TV only go to &#34;sleep mode&#34; (which doesn&#39;t take much more power) instead of doing a full shutdown, and will not need to restart our services on every suspend. This will also make TV startup much faster.&lt;/p&gt; &#xA;&lt;h3&gt;I want to run some commands as root during boot!&lt;/h3&gt; &#xA;&lt;p&gt;Our &lt;a href=&#34;https://github.com/webosbrew/webos-homebrew-channel/raw/main/services/startup.sh#L77-L80&#34;&gt;startup script&lt;/a&gt; runs all executable files in &lt;code&gt;/var/lib/webosbrew/init.d&lt;/code&gt; on boot (via &lt;code&gt;run-parts&lt;/code&gt; - filenames may only contain &lt;code&gt;a-zA-Z0-9-_&lt;/code&gt; letters!) - create your own scripts there.&lt;/p&gt; &#xA;&lt;p&gt;Create any customizations there and &lt;strong&gt;do not&lt;/strong&gt; modify existing RootMyTV/Homebrew Channel scripts, since these may be overwritten on future updates.&lt;/p&gt; &#xA;&lt;p&gt;If you are a homebrew developer - create a symlink to a script in your own app path there, and &lt;strong&gt;do not&lt;/strong&gt; copy over anything there.&lt;/p&gt; &#xA;&lt;h3&gt;I want to support you financially!&lt;/h3&gt; &#xA;&lt;p&gt;If you want, you can support this project via GitHub Sponsors - see &#34;Sponsor&#34; button in upper right corner.&lt;/p&gt; &#xA;&lt;h2&gt;Post-Installation Advice (IMPORTANT!)&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Don&#39;t update your TV. While updates are technically possible, if LG patches the exploit, you might end up &#34;locked out&#34; and unable to re-root your TV if you somehow lose access. We also can&#39;t predict how future updates will affect our techniques used to elevate and operate the Homebrew Channel app. &lt;strong&gt;&#34;Block system updates&#34; option in Homebrew Channel will disable firmware update checks.&lt;/strong&gt; Make sure &#34;Automatic system updates&#34; option in webOS System Settings is disabled as well.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It is &lt;strong&gt;required&lt;/strong&gt; to remove &#34;Developer Mode&#34; app before rooting. Otherwise it will interfere with the startup script used to bootstrap the jailbreak. SSH service exposed by Homebrew Channel is compatible with webOS SDK tooling.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you need remote root shell access and know how to use SSH, you can enable it in Homebrew Channel settings. Default password is &lt;code&gt;alpine&lt;/code&gt;, but we recommend setting up SSH Public Key authentication by copying your SSH Public Key over to &lt;code&gt;/home/root/.ssh/authorized_keys&lt;/code&gt; on the TV. This will disable password authentication after a reboot.&lt;/p&gt; &lt;p&gt;GitHub user registered keys can be installed using the following snippet:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mkdir -p ~/.ssh &amp;amp;&amp;amp; curl https://github.com/USERNAME.keys &amp;gt; ~/.ssh/authorized_keys&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternative option is Telnet (can be enabled in Homebrew Channel → Settings → Telnet) though it is &lt;strong&gt;highly discouraged&lt;/strong&gt;, since this gives unauthenticated root shell to anyone on a local network.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;It is recommended to have &#34;Quick Start+&#34; functionality &lt;strong&gt;enabled&lt;/strong&gt;. This will make shutdown button on a remote not do a full system shutdown. If you quickly turn the TV on and off without Quick Start+, our &#34;Failsafe Mode&#34; may get triggered (which is there to prevent startup scripts bricking the TV) which will go away after switching relevant switch in Homebrew Channel Settings.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;In case of any problems &lt;a href=&#34;https://discord.gg/xWqRVEm&#34;&gt;join the OpenLGTV Discord server&lt;/a&gt; and ask for help on &lt;code&gt;#rootmytv&lt;/code&gt; channel, ask on &lt;a href=&#34;https://matrix.to/#/#openlgtv:netserve.live&#34;&gt;our &lt;code&gt;#openlgtv:netserve.live&lt;/code&gt; Matrix channel&lt;/a&gt;, or file a GitHub issue.&lt;/p&gt; &#xA;&lt;p&gt;Before asking for support, please consult our &lt;a href=&#34;https://raw.githubusercontent.com/RootMyTV/RootMyTV.github.io/main/docs/TROUBLESHOOTING.md&#34;&gt;Troubleshooting guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Research Summary and Timeline&lt;/h1&gt; &#xA;&lt;p&gt;RootMyTV is a chain of exploits. The discovery and development of these exploits has been a collaborative effort, with direct and indirect contributions from multiple researchers.&lt;/p&gt; &#xA;&lt;p&gt;On October 05, 2020, Andreas Lindh reported a root file overwrite vulnerability to LG. On February 03, 2021, Andreas &lt;a href=&#34;https://blog.recurity-labs.com/2021-02-03/webOS_Pt1.html&#34;&gt;published his findings&lt;/a&gt;, demonstrating a local root exploit against the webOS Emulator (a part of LG&#39;s development SDK). LG had boldly claimed that this issue did not affect their devices, and that they were going to patch their emulator.&lt;/p&gt; &#xA;&lt;p&gt;On February 15th, 2021, David Buchanan reported a vulnerability in LG&#39;s &#34;ThinQ login&#34; app, which allowed the app to be hijacked via a specific sequence of user inputs, allowing an attacker to call privileged APIs. On March 23rd 2021, David &lt;a href=&#34;https://forum.xda-developers.com/t/rootmy-tv-coming-soon-developer-pre-release-available-now.4232223/&#34;&gt;published a proof-of-concept exploit&lt;/a&gt;, which enabled users to gain root privileges on their LG smart TVs. This was made possible by combining it with the local root vulnerability previously reported by Andreas (Yes, the same one that LG said did not affect their devices!).&lt;/p&gt; &#xA;&lt;p&gt;Around March 28th 2021, Piotr Dobrowolski discovered a similar vulnerability in the &#34;Social login&#34; app, which is present across a wider range of webOS versions. More importantly, this exploit could be easily triggered over the local network, using SSAP (details below), making it much more reliable and user-friendly.&lt;/p&gt; &#xA;&lt;p&gt;At time of writing, the code in this repo is the combined work of David Buchanan (Web design, initial PoC exploit) and Piotr Dobrowolski (Improved &#34;v1&#34; exploit implementation, writeup, and &#34;v2&#34; research and implementation).&lt;/p&gt; &#xA;&lt;p&gt;We would like to thank:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Andreas Lindh for publishing his webOS research.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The wider webOS community, particularly the &lt;a href=&#34;https://forum.xda-developers.com/f/webos-software-and-hacking-general.1079/&#34;&gt;XDA forums&lt;/a&gt; and the &lt;a href=&#34;https://discord.gg/xWqRVEm&#34;&gt;OpenLGTV discord&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;All the contributors (present and future) to the Homebrew Channel, and development of other homebrew apps and software.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;LG, for patching symptoms of bugs rather than underlying causes...&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;The Technical Details&lt;/h1&gt; &#xA;&lt;h3&gt;Background&lt;/h3&gt; &#xA;&lt;p&gt;webOS, as the name suggests, is a Smart TV operating system mostly based on web technologies. Applications, both system and external are either run in a stripped down Chromium-based web browser (&#34;WebAppMgr&#34;) or in Qt QML runtime. Almost all system and external applications run in chroot-based jails as an additional security layer.&lt;/p&gt; &#xA;&lt;p&gt;&#34;Web apps&#34;, outside of standard web technologies, also get access to an API for communicating with &#34;Luna Service Bus&#34;. This is a bus, similar to D-Bus, used to exchange messages and provide various services across different security domains. Bus clients can expose some RPC methods to other applications (identified by URIs &lt;code&gt;luna://service-name/prefix-maybe/method-name&lt;/code&gt;) which accept JSON object message as their call parameters, and then can return one or many messages. (depending on the call being &#34;subscribable&#34; or not)&lt;/p&gt; &#xA;&lt;p&gt;While Luna bus seems to have extensive ACL handling, considering the &lt;a href=&#34;https://en.wikipedia.org/wiki/WebOS#History&#34;&gt;history of webOS IP transfers&lt;/a&gt;, seems like not many engineers fully understand its capabilities. Part of the bus is marked as &#34;private&#34;, which is only accessible by certain system applications, while most of the other calls are &#34;public&#34; and can be accessed by all apps.&lt;/p&gt; &#xA;&lt;p&gt;Unexpectedly, one of the internal services exposed on a bus is &#34;LunaDownloadMgr&#34; which provides a convenient API for file download, progress tracking, etc... Said service has been researched in the past and an identity confusion bug leading to an arbitrary unjailed root file write vulnerability has been &lt;a href=&#34;https://blog.recurity-labs.com/2021-02-03/webOS_Pt1.html&#34;&gt;publicly documented&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This in and of itself was not very helpful in production hardware, thus we needed to find a way of calling an arbitrary Luna service from an application with a &lt;code&gt;com.webos.&lt;/code&gt; / &lt;code&gt;com.palm.&lt;/code&gt; / &lt;code&gt;com.lge.&lt;/code&gt; application ID.&lt;/p&gt; &#xA;&lt;h3&gt;Step #0 - Getting in (index.html)&lt;/h3&gt; &#xA;&lt;p&gt;In order to gain initial programmatic control of the TV GUI, an interface called &#34;LG Connect Apps&#34; can be used. Its protocol, called &#34;SSAP&#34; (Simple Service Access Protocol), is a simple websocket-based RPC mechanism that can be used to indirectly interact with Luna Service bus, and has been extensively documented in various home-automation related contexts. We use that to launch a vulnerable system application which is not easily accessible with normal user interaction.&lt;/p&gt; &#xA;&lt;h4&gt;Step #0.1 - Escaping the origins&lt;/h4&gt; &#xA;&lt;p&gt;SSAP API is meant to be used from an external mobile app. For the sake of simplicity, though, we wanted to serve our exploit as a web page. This lead us to notice that, understandably, the SSAP server explicitly rejects any connections from (plaintext) HTTP origins. However, there was an additional exception to that rule, and seemingly the authors wanted to allow &lt;code&gt;file://&lt;/code&gt; origins, which present themselves to the server as &lt;code&gt;null&lt;/code&gt;. Turns out there&#39;s one other origin that can be used that is also reprted as &lt;code&gt;null&lt;/code&gt;, and that is &lt;code&gt;data:&lt;/code&gt; URIs.&lt;/p&gt; &#xA;&lt;p&gt;In order to exploit this, we&#39;ve created a minimal WebSocket API proxy implementation that opens a hidden iframe with a javascript payload (which is now running in a &lt;code&gt;data:&lt;/code&gt;/&lt;code&gt;null&lt;/code&gt; origin) and exchanges the messages with the main browser frame. This has been released as &lt;a href=&#34;https://github.com/Informatic/webos-ssap-web&#34;&gt;a separate library&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Step #0.2 - General Data Protocol Redirection&lt;/h4&gt; &#xA;&lt;p&gt;There&#39;s a minor problem with establishing the connection with the SSAP websocket server. While we all believe in utter chaos, we don&#39;t feel very comfortable with serving our exploit over plaintext HTTP, which would be the only way of avoiding Mixed Content prevention policies. (by default, https origins are not allowed to communicate with plaintext http endpoints)&lt;/p&gt; &#xA;&lt;p&gt;While &lt;a href=&#34;https://chromium.googlesource.com/chromium/src.git/+/130ee686fa00b617bfc001ceb3bb49782da2cb4e&#34;&gt;some newer Chromium versions&lt;/a&gt; do allow Mixed Content communication with &lt;code&gt;localhost&lt;/code&gt;, that was not the case when Chromium 38 was released (used in webOS 3.x). Thankfully, it seems like the system browser on webOS 3.x is also vulnerable to something that has been considered a security issue in most browsers for a while now - navigation to &lt;code&gt;data:&lt;/code&gt; URIs. Thus, when applicable, our exploits attempts to open itself as a &lt;code&gt;data:&lt;/code&gt; base64-encoded URI. This makes our browser no longer consider the origin being secure, and we can again access the plain-http WebSocket server.&lt;/p&gt; &#xA;&lt;h4&gt;Mitigation note&lt;/h4&gt; &#xA;&lt;p&gt;An observant reader may have noticed that the service we use is meant to be used remotely. While the connection itself needs a confirmation using a remote &lt;strong&gt;we highly recommend to disable LG Connect Apps functionality&lt;/strong&gt; in order to prevent remote exploitation. However, this option seems to only be present on webOS versions older than webOS 4.x - in such cases the only solutions are to either &lt;strong&gt;keep the TV on a separate network&lt;/strong&gt;, or disable SSAP service manually using the following command after rooting:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;luna-send -n 1 &#39;palm://com.webos.settingsservice/setSystemSettings&#39; &#39;{&#34;category&#34;:&#34;network&#34;,&#34;settings&#34;:{&#34;allowMobileDeviceAccess&#34;:false}}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step #1 - Social login escape (stage1.html)&lt;/h3&gt; &#xA;&lt;p&gt;Having some initial programmatic control of the TV via SSAP, we can execute any application present on the TV. All cross-application launches can contain an extra JSON object called &lt;code&gt;launchParams&lt;/code&gt;. This is used to eg. open a system browser with specific site open, or launch a predetermined YouTube video. Turns out this functionality is also used to select which social website to use in &lt;code&gt;com.webos.app.facebooklogin&lt;/code&gt;, which is the older sibling of &lt;code&gt;com.webos.app.iot-thirdparty-login&lt;/code&gt; used in initial exploit, present on all webOS versions up until (at least) 3.x.&lt;/p&gt; &#xA;&lt;p&gt;When launching social login via LG Account Management, this application accepts an argument called &lt;code&gt;server&lt;/code&gt;. This turns out to be a part of URL that &#34;web app&#34; browser is navigated to. Thus, using a properly prepared &lt;code&gt;launchParams&lt;/code&gt; we are able to open an arbitrary web page (with the only requirement being that it&#39;s served over &lt;code&gt;https&lt;/code&gt;) running as a system app that is considered by &lt;code&gt;LunaDownloadMgr&lt;/code&gt; a &#34;system&#34; app.&lt;/p&gt; &#xA;&lt;h3&gt;Step #2 - Download All The Things (stage2.html)&lt;/h3&gt; &#xA;&lt;p&gt;Since we are already running as a system application, we can download files (securely over https!) into arbitrary unjailed filesystem locations as root.&lt;/p&gt; &#xA;&lt;p&gt;We use that to download following files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;stage3.sh&lt;/code&gt; → &lt;code&gt;/media/cryptofs/apps/usr/palm/services/com.palmdts.devmode.service/start-devmode.sh&lt;/code&gt; - this is the script executed at startup by &lt;code&gt;/etc/init/devmode.conf&lt;/code&gt; as root, in order to run developer mode jailed SSH daemon.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;hbchannel.ipk&lt;/code&gt; → &lt;code&gt;/media/internal/downloads/hbchannel.ipk&lt;/code&gt; - since our end goal is intalling the Homebrew Channel app, we can also just download it during the earlier stages of an exploit and confirm it&#39;s actually downloaded.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;devmode_enabled&lt;/code&gt; → &lt;code&gt;/var/luna/preferences/devmode_enabled&lt;/code&gt; - this is the flag checked before running &lt;code&gt;start-devmode.sh&lt;/code&gt; script, and is just a dummy file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step #3 - Homebrew Channel Deployment (stage3.sh)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;stage3.sh&lt;/code&gt; script is a minimal tool that, after opening an emergency telnet shell and removing itself (in case something goes wrong and the user needs to reboot a TV - script keeps running but will no longer be executed on next startup), installs the homebrew channel app via standard devmode service calls and elevates its service to run unjailed as root as well.&lt;/p&gt; &#xA;&lt;h3&gt;2021/06: The Old-New Chain (RootMyTV v2)&lt;/h3&gt; &#xA;&lt;p&gt;Around 2021/06 LG started rolling out a patched version which involved some fixes for the tricks we used in this chain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Certain applications we used for private bus access have their permissions limited to &lt;code&gt;public&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;LunaDownloadMgr now checks target paths against a list of regular expressions in &lt;code&gt;/etc/palm/luna-downloadmgr/download.json&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;start-devmode.sh&lt;/code&gt; script is now shipped with a signature and is now verified using &lt;code&gt;openssl&lt;/code&gt; on each boot &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This one had an interesting side effect - it took approximately a month for LG to roll out a new Developer Mode application with signed &lt;code&gt;start-devmode.sh&lt;/code&gt;, during which time updated TVs were unable to use developer mode at all.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most of these mitigations are too trivial to work around, thus we still consider this chain unfixed.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There are still applications on the system that are vulnerable to XSS attacks with private bus permissions&lt;/li&gt; &#xA; &lt;li&gt;Regular expressions used to verify target paths are too broad, and thus still allow us to write to relevant paths&lt;/li&gt; &#xA; &lt;li&gt;There are multiple paths that are executed during bootup, so we don&#39;t even need to use &lt;code&gt;start-devmode.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our initial estimate for fixing these issues in our chain were &#34;a couple of hours&#34; - patches theorized on our side on 2021/05/27 turned out to be correct, but due to some strategic choices and lack of personal time, we decided to postpone testing and release for a couple of months. Sorry. :)&lt;/p&gt;</summary>
  </entry>
</feed>