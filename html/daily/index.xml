<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-06T01:32:55Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>netease-youdao/QAnything</title>
    <updated>2024-01-06T01:32:55Z</updated>
    <id>tag:github.com,2024-01-06:/netease-youdao/QAnything</id>
    <link href="https://github.com/netease-youdao/QAnything" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Question and Answer based on Anything.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/netease-youdao/QAnything&#34;&gt; &#xA;  &lt;!-- Please provide path to your logo here --&gt; &lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/qanything_logo.png&#34; alt=&#34;Logo&#34; width=&#34;800&#34;&gt; &lt;/a&gt; &#xA; &lt;h1&gt;&lt;strong&gt;Q&lt;/strong&gt;uestion and &lt;strong&gt;A&lt;/strong&gt;nswer based on &lt;strong&gt;Anything&lt;/strong&gt;&lt;/h1&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/README_zh.md&#34;&gt;简体中文&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://qanything.ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/try%20online-qanything.ai-purple&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA; &lt;a href=&#34;https://read.youdao.com#/home&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/try%20online-read.youdao.com-purple&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache--2.0-yellow&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://github.com/netease-youdao/QAnything/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-red&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://twitter.com/YDopensource&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/follow-%40YDOpenSource-1DA1F2?logo=twitter&amp;amp;style={style}&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;Table of Contents&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#What-is-QAnything&#34;&gt;What is QAnything&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#Key-features&#34;&gt;Key features&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#Architecture&#34;&gt;Architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#API-Document&#34;&gt;API Document&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#WeChat-Group&#34;&gt;WeChat Group&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#support&#34;&gt;Support&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/#Acknowledgments&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;What is QAnything?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q&lt;/strong&gt;uestion and &lt;strong&gt;A&lt;/strong&gt;nswer based on &lt;strong&gt;Anything&lt;/strong&gt; (&lt;code&gt;QAnything&lt;/code&gt;) is a local knowledge base question-answering system designed to support a wide range of file formats and databases, allowing for offline installation and use.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;QAnything&lt;/code&gt;, you can simply drop any locally stored file of any format and receive accurate, fast, and reliable answers.&lt;/p&gt; &#xA;&lt;p&gt;Currently supported formats include: &lt;strong&gt;PDF, Word (doc/docx), PPT, Markdown, Eml, TXT, Images (jpg, png, etc.), Web links&lt;/strong&gt; and more formats coming soon…&lt;/p&gt; &#xA;&lt;h3&gt;Key features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Data Security&lt;/strong&gt;, supports installation and usage with network cable unplugged throughout the process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cross-language QA support&lt;/strong&gt;, freely switch between Chinese and English QA, regardless of the language of the document.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supports massive data QA&lt;/strong&gt;, two-stage retrieval ranking, solving the degradation problem of large-scale data retrieval; the more data, the better the performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance production-grade system&lt;/strong&gt;, directly deployable for enterprise applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-friendly&lt;/strong&gt;, no need for cumbersome configurations, one-click installation and deployment, ready to use.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi knowledge base QA&lt;/strong&gt; Support selecting multiple knowledge bases for Q&amp;amp;A&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Architecture&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/qanything_arch.png&#34; width=&#34;700&#34; alt=&#34;qanything_system&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Why 2 stage retrieval?&lt;/h4&gt; &#xA;&lt;p&gt;In scenarios with a large volume of knowledge base data, the advantages of a two-stage approach are very clear. If only a first-stage embedding retrieval is used, there will be a problem of retrieval degradation as the data volume increases, as indicated by the green line in the following graph. However, after the second-stage reranking, there can be a stable increase in accuracy, &lt;strong&gt;the more data, the better the performance&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/two_stage_retrieval.jpg&#34; width=&#34;500&#34; alt=&#34;two stage retrievaal&#34; align=&#34;center&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;QAnything uses the retrieval component &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt;, which is distinguished for its bilingual and crosslingual proficiency. BCEmbedding excels in bridging Chinese and English linguistic gaps, which achieves&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A high performence on &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/tree/master?tab=readme-ov-file#evaluate-semantic-representation-by-mteb&#34; target=&#34;_Self&#34;&gt;Semantic Representation Evaluations in MTEB&lt;/a&gt;&lt;/strong&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A new benchmark in the realm of &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/tree/master?tab=readme-ov-file#evaluate-rag-by-llamaindex&#34; target=&#34;_Self&#34;&gt;RAG Evaluations in LlamaIndex&lt;/a&gt;&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;1st Retrieval（embedding）&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Retrieval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PairClassification&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Classification&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Clustering&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.45&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-base-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.60&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.72&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.56&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-en-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.09&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.00&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.68&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.82&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-large-zh-v1.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.54&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.73&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;79.14&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.88&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;jina-embeddings-v2-base-en&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.58&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.67&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.93&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64.08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.84&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.54&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;m3e-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.85&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.74&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.99&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.62&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-embedding-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.60&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;65.73&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.96&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;69.00&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.29&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;38.95&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;59.43&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/EvaluationSummary/embedding_eval_summary.md&#34;&gt;Embedding Models Evaluation Summary&lt;/a&gt;。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2nd Retrieval（rerank）&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Reranking&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Avg&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.78&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;bge-reranker-large&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.69&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;em&gt;&lt;strong&gt;bce-reranker-base_v1&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;em&gt;&lt;strong&gt;60.06&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More evaluation details please check &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/EvaluationSummary/reranker_eval_summary.md&#34;&gt;Reranker Models Evaluation Summary&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;RAG Evaluations in LlamaIndex（embedding and rerank）&lt;/h4&gt; &#xA;&lt;img src=&#34;https://github.com/netease-youdao/BCEmbedding/raw/master/Docs/assets/rag_eval_multiple_domains_summary.jpg&#34;&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In &lt;code&gt;WithoutReranker&lt;/code&gt; setting, our &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; outperforms all the other embedding models.&lt;/li&gt; &#xA; &lt;li&gt;With fixing the embedding model, our &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; achieves the best performence.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;The combination of &lt;code&gt;bce-embedding-base_v1&lt;/code&gt; and &lt;code&gt;bce-reranker-base_v1&lt;/code&gt; is SOTA&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If you want to use embedding and rerank separately, please refer to &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;LLM&lt;/h4&gt; &#xA;&lt;p&gt;The open source version of QAnything is based on QwenLM and has been fine-tuned on a large number of professional question-answering datasets. It greatly enhances the ability of question-answering. If you need to use it for commercial purposes, please follow the license of QwenLM. For more details, please refer to: &lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;QwenLM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://qanything.ai&#34;&gt;&lt;span&gt;👉&lt;/span&gt; try QAnything online&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Required item&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Minimum Requirement&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NVIDIA GPU Memory&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 16GB&lt;/td&gt; &#xA;   &lt;td&gt;NVIDIA 3090 recommended&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NVIDIA Driver Version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 525.105.17&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CUDA Version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;= 12.0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;docker compose version&lt;/td&gt; &#xA;   &lt;td&gt;&amp;gt;=1.27.4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://docs.docker.com/compose/install/&#34;&gt;docker compose install&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;step1: pull qanything repository&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/netease-youdao/QAnything.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;step2: download the model and unzip it to the root directory of the current project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd QAnything&#xA;&#xA;git clone https://www.modelscope.cn/netease-youdao/qanything_models.git&#xA;&#xA;unzip qanything_models/models.zip   # in root directory of the current project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;step3: change config&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;vim front_end/.env  # change 10.55.163.92 to your host&#xA;vim docker-compose.yaml # change CUDA_VISIBLE_DEVICES to your gpu device id&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;step4: start server&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After successful installation, you can experience the application by entering the following addresses in your web browser.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Frontend address: http://{your_host}:5052/qanything&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;API address: http://{your_host}:5052/api/&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For detailed API documentation, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/API.md&#34;&gt;QAnything API 文档&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Cross-lingual: Multiple English paper Q&amp;amp;A&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/8915277f-c136-42b8-9332-78f64bf5df22&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/multi_paper_qa.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Information extraction&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/b9e3be94-183b-4143-ac49-12fa005a8a9a&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/information_extraction.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Various files&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/7ede63c1-4c7f-4557-bd2c-7c51a44c8e0b&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/various_files_qa.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Web Q&amp;amp;A&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/assets/141105427/d30942f7-6dbd-4013-a4b6-82f7c2a5fbee&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/videos/web_qa.mp4&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;API Document&lt;/h3&gt; &#xA;&lt;p&gt;If you need to access the API, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/API.md&#34;&gt;QAnything API documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;WeChat Group&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to scan the QR code below and join the WeChat group.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/docs/images/Wechat.jpg&#34; width=&#34;20%&#34; height=&#34;auto&#34;&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;Reach out to the maintainer at one of the following places:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/netease-youdao/QAnything/issues&#34;&gt;Github issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Contact options listed on &lt;a href=&#34;https://github.com/netease-youdao&#34;&gt;this GitHub profile&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt; is licensed under &lt;a href=&#34;https://raw.githubusercontent.com/netease-youdao/QAnything/master/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;QAnything&lt;/code&gt; adopts dependencies from the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to our &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt; for the excellent embedding and rerank model.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/QwenLM/Qwen&#34;&gt;Qwen&lt;/a&gt; for strong base language models.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/triton-inference-server/server&#34;&gt;Triton Inference Server&lt;/a&gt; for providing great open source inference serving.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/NVIDIA/FasterTransformer&#34;&gt;FasterTransformer&lt;/a&gt; for highly optimized LLM inference backend.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;Langchain&lt;/a&gt; for the wonderful llm application framework.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/chatchat-space/Langchain-Chatchat&#34;&gt;Langchain-Chatchat&lt;/a&gt; for the inspiration provided on local knowledge base Q&amp;amp;A.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/milvus-io/milvus&#34;&gt;Milvus&lt;/a&gt; for the excellent semantic search library.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34;&gt;PaddleOCR&lt;/a&gt; for its ease-to-use OCR library.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/sanic-org/sanic&#34;&gt;Sanic&lt;/a&gt; for the powerful web service framework.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>