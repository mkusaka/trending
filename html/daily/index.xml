<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-28T01:29:40Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>wandb/openui</title>
    <updated>2024-05-28T01:29:40Z</updated>
    <id>tag:github.com,2024-05-28:/wandb/openui</id>
    <link href="https://github.com/wandb/openui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenUI let&#39;s you describe UI using your imagination, then see it rendered live.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenUI&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/openui.png&#34; width=&#34;150&#34; alt=&#34;OpenUI&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Building UI components can be a slog. OpenUI aims to make the process fun, fast, and flexible. It&#39;s also a tool we&#39;re using at &lt;a href=&#34;https://wandb.com&#34;&gt;W&amp;amp;B&lt;/a&gt; to test and prototype our next generation tooling for building powerful applications on top of LLM&#39;s.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/demo.gif&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenUI let&#39;s you describe UI using your imagination, then see it rendered live. You can ask for changes and convert HTML to React, Svelte, Web Components, etc. It&#39;s like &lt;a href=&#34;https://v0.dev&#34;&gt;v0&lt;/a&gt; but open source and not as polished &lt;span&gt;üòù&lt;/span&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Live Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openui.fly.dev&#34;&gt;Try the demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;You can also run OpenUI locally and use models available to &lt;a href=&#34;https://ollama.com&#34;&gt;Ollama&lt;/a&gt;. &lt;a href=&#34;https://ollama.com/download&#34;&gt;Install Ollama&lt;/a&gt; and pull a model like &lt;a href=&#34;https://ollama.com/library/codellama&#34;&gt;CodeLlama&lt;/a&gt;, then assuming you have git and python installed:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; There&#39;s a .python-version file that specifies &lt;strong&gt;openui&lt;/strong&gt; as the virtual env name. Assuming you have pyenv and pyenv-virtualenv you can run the following from the root of the repository or just run &lt;code&gt;pyenv local 3.X&lt;/code&gt; where X is the version of python you have installed.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pyenv virtualenv 3.12.2 openui&#xA;pyenv local openui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/wandb/openui&#xA;cd openui/backend&#xA;# You probably want to do this from a virtual environment&#xA;pip install .&#xA;# This must be set to use OpenAI models, find your api key here: https://platform.openai.com/api-keys&#xA;export OPENAI_API_KEY=xxx&#xA;# You may change the base url to use an OpenAI-compatible api by setting the OPENAI_BASE_URL environment variable&#xA;# export OPENAI_BASE_URL=https://api.myopenai.com/v1&#xA;python -m openui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Groq&lt;/h2&gt; &#xA;&lt;p&gt;To use the super fast &lt;a href=&#34;https://groq.com&#34;&gt;Groq&lt;/a&gt; models, set &lt;code&gt;GROQ_API_KEY&lt;/code&gt; to your Groq api key which you can &lt;a href=&#34;https://console.groq.com/keys&#34;&gt;find here&lt;/a&gt;. To use one of the Groq models, click the settings icon in the sidebar and choose from the list:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/settings.jpeg&#34; width=&#34;500&#34; alt=&#34;Select Groq models&#34;&gt; &#xA;&lt;p&gt;You can also change the default base url used for Groq (if necessary), i.e.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GROQ_BASE_URL=https://api.groq.com/openai/v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Compose&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;DISCLAIMER:&lt;/strong&gt; This is likely going to be very slow. If you have a GPU you may need to change the tag of the &lt;code&gt;ollama&lt;/code&gt; container to one that supports it. If you&#39;re running on a Mac, follow the instructions above and run Ollama natively to take advantage of the M1/M2.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;From the root directory you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d&#xA;docker exec -it openui-ollama-1 ollama pull llava&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have your OPENAI_API_KEY set in the environment already, just remove &lt;code&gt;=xxx&lt;/code&gt; from the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; line. You can also replace &lt;code&gt;llava&lt;/code&gt; in the command above with your open source model of choice &lt;em&gt;(&lt;a href=&#34;https://ollama.com/library/llava&#34;&gt;llava&lt;/a&gt; is one of the only Ollama models that support images currently)&lt;/em&gt;. You should now be able to access OpenUI at &lt;a href=&#34;http://localhost:7878&#34;&gt;http://localhost:7878&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you make changes to the frontend or backend, you&#39;ll need to run &lt;code&gt;docker-compose build&lt;/code&gt; to have them reflected in the service.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can build and run the docker file manually from the &lt;code&gt;/backend&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build . -t wandb/openui --load&#xA;docker run -p 7878:7878 -e OPENAI_API_KEY -e GROQ_API_KEY wandb/openui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can goto &lt;a href=&#34;http://localhost:7878&#34;&gt;http://localhost:7878&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://github.com/wandb/openui/raw/main/.devcontainer/devcontainer.json&#34;&gt;dev container&lt;/a&gt; is configured in this repository which is the quickest way to get started.&lt;/p&gt; &#xA;&lt;h3&gt;Codespace&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/codespace.png&#34; alt=&#34;New with options...&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;Choose more options when creating a Codespace, then select &lt;strong&gt;New with options...&lt;/strong&gt;. Select the US West region if you want a really fast boot time. You&#39;ll also want to configure your OPENAI_API_KEY secret or just set it to &lt;code&gt;xxx&lt;/code&gt; if you want to try Ollama &lt;em&gt;(you&#39;ll want at least 16GB of Ram)&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once inside the code space you can run the server in one terminal: &lt;code&gt;python -m openui --dev&lt;/code&gt;. Then in a new terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /workspaces/openui/frontend&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should open another service on port 5173, that&#39;s the service you&#39;ll want to visit. All changes to both the frontend and backend will automatically be reloaded and reflected in your browser.&lt;/p&gt; &#xA;&lt;h3&gt;Ollama&lt;/h3&gt; &#xA;&lt;p&gt;The codespace installs ollama automaticaly and downloads the &lt;code&gt;llava&lt;/code&gt; model. You can verify Ollama is running with &lt;code&gt;ollama list&lt;/code&gt; if that fails, open a new terminal and run &lt;code&gt;ollama serve&lt;/code&gt;. In Codespaces we pull llava on boot so you should see it in the list. You can select Ollama models from the settings gear icon in the upper left corner of the application. Any models you pull i.e. &lt;code&gt;ollama pull llama&lt;/code&gt; will show up in the settings modal.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/ollama.png&#34; width=&#34;500&#34; alt=&#34;Select Ollama models&#34;&gt; &#xA;&lt;h3&gt;Gitpod&lt;/h3&gt; &#xA;&lt;p&gt;You can easily use Open UI via Gitpod, preconfigured with Open AI.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitpod.io/#https://github.com/wandb/openui&#34;&gt;&lt;img src=&#34;https://gitpod.io/button/open-in-gitpod.svg?sanitize=true&#34; alt=&#34;Open in Gitpod&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;On launch Open UI is automatically installed and launched.&lt;/p&gt; &#xA;&lt;p&gt;Before you can use Gitpod:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure you have a Gitpod account.&lt;/li&gt; &#xA; &lt;li&gt;To use Open AI models set up the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable in your Gitpod &lt;a href=&#34;https://gitpod.io/user/variables&#34;&gt;User Account&lt;/a&gt;. Set the scope to &lt;code&gt;wandb/openui&lt;/code&gt; (or your repo if you forked it).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE: Other (local) models might also be used with a bigger Gitpod instance type. Required models are not preconfigured in Gitpod but can easily be added as documented above.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;p&gt;See the readmes in the &lt;a href=&#34;https://raw.githubusercontent.com/wandb/openui/main/frontend/README.md&#34;&gt;frontend&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/wandb/openui/main/backend/README.md&#34;&gt;backend&lt;/a&gt; directories.&lt;/p&gt;</summary>
  </entry>
</feed>