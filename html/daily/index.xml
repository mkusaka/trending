<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-13T01:31:22Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>radames/Real-Time-Latent-Consistency-Model</title>
    <updated>2023-11-13T01:31:22Z</updated>
    <id>tag:github.com,2023-11-13:/radames/Real-Time-Latent-Consistency-Model</id>
    <link href="https://github.com/radames/Real-Time-Latent-Consistency-Model" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Demo showcasing ~real-time Latent Consistency Model pipeline with Diffusers and a MJPEG stream server&lt;/p&gt;&lt;hr&gt;&lt;hr&gt; &#xA;&lt;h2&gt;title: Real-Time Latent Consistency Model Image-to-Image ControlNet emoji: üñºÔ∏èüñºÔ∏è colorFrom: gray colorTo: indigo sdk: docker pinned: false suggested_hardware: a10g-small&lt;/h2&gt; &#xA;&lt;h1&gt;Real-Time Latent Consistency Model&lt;/h1&gt; &#xA;&lt;p&gt;This demo showcases &lt;a href=&#34;https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7&#34;&gt;Latent Consistency Model (LCM)&lt;/a&gt; using &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples/community#latent-consistency-pipeline&#34;&gt;Diffusers&lt;/a&gt; with a MJPEG stream server.&lt;/p&gt; &#xA;&lt;p&gt;You need a webcam to run this demo. ü§ó&lt;/p&gt; &#xA;&lt;p&gt;See a collecting with live demos &lt;a href=&#34;https://huggingface.co/collections/latent-consistency/latent-consistency-model-demos-654e90c52adb0688a0acbe6f&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;You need CUDA and Python 3.10, Mac with an M1/M2/M3 chip or Intel Arc GPU&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;TIMEOUT&lt;/code&gt;: limit user session timeout&lt;br&gt; &lt;code&gt;SAFETY_CHECKER&lt;/code&gt;: disabled if you want NSFW filter off&lt;br&gt; &lt;code&gt;MAX_QUEUE_SIZE&lt;/code&gt;: limit number of users on current app instance&lt;br&gt; &lt;code&gt;TORCH_COMPILE&lt;/code&gt;: enable if you want to use torch compile for faster inference works well on A100 GPUs&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv venv&#xA;source venv/bin/activate&#xA;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;LCM&lt;/h1&gt; &#xA;&lt;h3&gt;Image to Image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uvicorn &#34;app-img2img:app&#34; --host 0.0.0.0 --port 7860 --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Image to Image ControlNet Canny&lt;/h3&gt; &#xA;&lt;p&gt;Based pipeline from &lt;a href=&#34;https://github.com/taabata/LCM_Inpaint_Outpaint_Comfy&#34;&gt;taabata&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uvicorn &#34;app-controlnet:app&#34; --host 0.0.0.0 --port 7860 --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Text to Image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uvicorn &#34;app-txt2img:app&#34; --host 0.0.0.0 --port 7860 --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;LCM + LoRa&lt;/h1&gt; &#xA;&lt;p&gt;Using LCM-LoRA, giving it the super power of doing inference in as little as 4 steps. &lt;a href=&#34;https://huggingface.co/blog/lcm_lora&#34;&gt;Learn more here&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/papers/2311.05556&#34;&gt;technical report&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image to Image ControlNet Canny LoRa&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uvicorn &#34;app-controlnetlora:app&#34; --host 0.0.0.0 --port 7860 --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Text to Image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uvicorn &#34;app-txt2imglora:app&#34; --host 0.0.0.0 --port 7860 --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Setting environment variables&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;TIMEOUT=120 SAFETY_CHECKER=True MAX_QUEUE_SIZE=4 uvicorn &#34;app-img2img:app&#34; --host 0.0.0.0 --port 7860 --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re running locally and want to test it on Mobile Safari, the webserver needs to be served over HTTPS.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openssl req -newkey rsa:4096 -nodes -keyout key.pem -x509 -days 365 -out certificate.pem&#xA;uvicorn &#34;app-img2img:app&#34; --host 0.0.0.0 --port 7860 --reload --log-level info --ssl-certfile=certificate.pem --ssl-keyfile=key.pem&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;You need NVIDIA Container Toolkit for Docker&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t lcm-live .&#xA;docker run -ti -p 7860:7860 --gpus all lcm-live&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or with environment variables&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -ti -e TIMEOUT=0 -e SAFETY_CHECKER=False -p 7860:7860 --gpus all lcm-live&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Demo on Hugging Face&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/radames/Real-Time-Latent-Consistency-Model&#34;&gt;https://huggingface.co/spaces/radames/Real-Time-Latent-Consistency-Model&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/radames/Real-Time-Latent-Consistency-Model/assets/102277/c4003ac5-e7ff-44c0-97d3-464bb659de70&#34;&gt;https://github.com/radames/Real-Time-Latent-Consistency-Model/assets/102277/c4003ac5-e7ff-44c0-97d3-464bb659de70&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>