<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-11T02:31:43Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deep-learning-mit/staging</title>
    <updated>2023-11-11T02:31:43Z</updated>
    <id>tag:github.com,2023-11-11:/deep-learning-mit/staging</id>
    <link href="https://github.com/deep-learning-mit/staging" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;6.s898 2023 Blogposts&lt;/h1&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;For a hands-on walkthrough of al-folio installation, check out &lt;a href=&#34;https://www.youtube.com/watch?v=g6AJ9qPPoyc&#34;&gt;this cool video tutorial&lt;/a&gt; by one of the community members! üé¨ üçø&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Local setup using Docker&lt;/h4&gt; &#xA;&lt;p&gt;You need to take the following steps to get &lt;code&gt;al-folio&lt;/code&gt; up and running in your local machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;First, &lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;install docker&lt;/a&gt; (Install Docker Desktop). Make sure that Docker Destop is open/running on your computer, and if it hangs you may need to restart your computer once after the installation&lt;/li&gt; &#xA; &lt;li&gt;Then, clone this repository to your machine:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone git@github.com:&amp;lt;your-username&amp;gt;/&amp;lt;your-repo-name&amp;gt;.git&#xA;$ cd &amp;lt;your-repo-name&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, run the following command that will pull a pre-built image from DockerHub and will run your website.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ ./bin/dockerhub_run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;Local setup without Docker&lt;/h4&gt; &#xA;&lt;p&gt;Assuming you have &lt;a href=&#34;https://www.ruby-lang.org/en/downloads/&#34;&gt;Ruby&lt;/a&gt; and &lt;a href=&#34;https://bundler.io/&#34;&gt;Bundler&lt;/a&gt; installed on your system (&lt;em&gt;hint: for ease of managing ruby gems, consider using &lt;a href=&#34;https://github.com/rbenv/rbenv&#34;&gt;rbenv&lt;/a&gt;&lt;/em&gt;), do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone git@github.com:&amp;lt;your-username&amp;gt;/&amp;lt;your-repo-name&amp;gt;.git&#xA;$ cd &amp;lt;your-repo-name&amp;gt;&#xA;$ bundle install&#xA;$ bundle exec jekyll serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;blockquote&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The theme is available as open source under the terms of the &lt;a href=&#34;https://github.com/alshedivat/al-folio/raw/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Originally, &lt;strong&gt;al-folio&lt;/strong&gt; was based on the &lt;a href=&#34;https://github.com/bogoli/-folio&#34;&gt;*folio theme&lt;/a&gt; (published by &lt;a href=&#34;https://liabogoev.com&#34;&gt;Lia Bogoev&lt;/a&gt; and under the MIT license). Since then, it got a full re-write of the styles and many additional cool features.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/LLaVA-Med</title>
    <updated>2023-11-11T02:31:43Z</updated>
    <id>tag:github.com,2023-11-11:/microsoft/LLaVA-Med</id>
    <link href="https://github.com/microsoft/LLaVA-Med" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large Language-and-Vision Assistant for BioMedicine, built towards multimodal GPT-4 level capabilities.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLaVA-Med: Large Language and Vision Assistant for BioMedicine&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Visual instruction tuning towards buiding large language and vision models with GPT-4 level capabilities in the biomedicine space.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;Paper, NeurIPS 2023 Datasets and Benchmarks Track (Spotlight)&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;!-- [[Data](https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K)] [[Model](https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0)] --&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/strong&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=Sl05ifcAAAAJ&amp;amp;hl=en&#34;&gt;Cliff Wong*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=-LVEXQ8AAAAJ&amp;amp;hl=en&#34;&gt;Sheng Zhang*&lt;/a&gt;, &lt;a href=&#34;https://www.microsoft.com/en-us/research/people/naotous/&#34;&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href=&#34;https://hliu.cc&#34;&gt;Haotian Liu&lt;/a&gt;, &lt;a href=&#34;https://jwyang.github.io/&#34;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=cjlSeqwAAAAJ&amp;amp;hl=en&#34;&gt;Tristan Naumann&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=yqqmVbkAAAAJ&amp;amp;hl=en&#34;&gt;Hoifung Poon&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=CQ1cqKkAAAAJ&amp;amp;hl=en&#34;&gt;Jianfeng Gao&lt;/a&gt; (*Equal Contribution)&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/images/llava_med_logo.png&#34; width=&#34;50%&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;Generated by &lt;a href=&#34;https://gligen.github.io/&#34;&gt;GLIGEN&lt;/a&gt; using the grounded inpainting mode, with three boxes: &lt;code&gt;white doctor coat&lt;/code&gt;, &lt;code&gt;stethoscope&lt;/code&gt;, &lt;code&gt;white doctor hat with a red cross sign&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[Nov 8] LLaVA-Med is open-sourced under the MSR release policy. Huge thanks to commitment of the team, and patience of the community.&lt;/li&gt; &#xA; &lt;li&gt;[Sept] LLaVA-Med is accepted in NeurIPS 2023 Datasets and Benchmarks Track, as a spotlight presentation.&lt;/li&gt; &#xA; &lt;li&gt;[June 1] üî• We released &lt;strong&gt;LLaVA-Med: Large Language and Vision Assistant for Biomedicine&lt;/strong&gt;, a step towards building biomedical domain large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2306.00890&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/images/llava_med_pipeline.png&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;LLaVA-Med was initialized with the general-domain LLaVA and then continuously trained in a curriculum learning fashion (first biomedical concept alignment then full-blown instruction-tuning). We evaluated LLaVA-Med on standard visual conversation and question answering tasks.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/Research%20License.docx&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Microsoft%20Research-red&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/deed.en&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data, code, and model checkpoints are intended and licensed for research use only. They are also subject to additional restrictions dictated by the Terms of Use: LLaMA, Vicuna and GPT-4 respectively. The data is made available under CC BY NC 4.0. The data, code, and model checkpoints may be used for non-commercial purposes and any models trained using the dataset should be used only for research purposes. It is expressly prohibited for models trained on this data to be used in clinical care or for any clinical decision making purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/#data-download&#34;&gt;Data Download&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/#serving&#34;&gt;Serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/#model-description&#34;&gt;Model Description&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Download&lt;/h2&gt; &#xA;&lt;h3&gt;LLaVA-Med Dataset&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/images/llava_med_dataset.png&#34; width=&#34;90%&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;The data statistics of biomedical multimodal instruction-following data: (a,b) The root verb-noun pairs of instruction and responses, where the inner circle of the plot represents the root verb of the output response, and the outer circle represents the direct nouns. (c) The distribution of images and QA pairs on the five domains, one image is shown per domain.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Data Download&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Alignment data files&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/alignment/llava_med_alignment_500k.json&#34;&gt;llava_med_alignment_500k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;341.52 MiB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Instruction-Tuning data files&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/instruct/llava_med_instruct_10k.json&#34;&gt;llava_med_instruct_10k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;19.24 MiB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/instruct/llava_med_instruct_60k.json&#34;&gt;llava_med_instruct_60k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.65 MiB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/instruct/llava_med_instruct_60k_inline_mention.json&#34;&gt;llava_med_instruct_60k_inline_mention.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.61 MiB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/instruct/llava_med_instruct_fig_captions.json&#34;&gt;llava_med_instruct_fig_captions.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;161.39 MiB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Evaluation files&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/eval/llava_med_eval_qa50_qa.jsonl&#34;&gt;llava_med_eval_qa50_qa.jsonl&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;256.18 KiB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/eval/llava_med_eval_qa50_fig_captions.json&#34;&gt;llava_med_eval_qa50_fig_captions.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;51.82 KiB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/eval/llava_med_qa50_instruct_caption_in_text_cleaned-60k-3epoch.json&#34;&gt;llava_med_qa50_instruct_caption_in_text_cleaned-60k-3epoch.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;100.97 KiB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Image URLS&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/llava_med_image_urls.jsonl&#34;&gt;llava_med_image_urls.jsonl&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;122.82 MiB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/llava/data/download_images.py&#34;&gt;download_images.py&lt;/a&gt; is used to download the PMC articles using the above image_urls file and extract the images&lt;/p&gt; &#xA;&lt;p&gt;To download our langauge-image multimodal instruction-folllowing dataset, please run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh download_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GPT-4 Assisted Instruct Data Generation&lt;/h3&gt; &#xA;&lt;p&gt;We provide our prompts and few-shot samples for GPT-4 queries, to better facilitate research in this domain. Please check out the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/llava/instruct/&#34;&gt;&lt;code&gt;llava/instruct/&lt;/code&gt;&lt;/a&gt; folder for the instruct data &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/llava/instruct/instruct_generate.py&#34;&gt;generation&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/llava/instruct/instruct_postprocess.py&#34;&gt;filtering&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To generate medical instruction tuning for 60k samples and with in-text mentions:&lt;/p&gt; &#xA;&lt;p&gt;Fill in your OpenAI API parameters in the file &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/llava/openai_api.py&#34;&gt;llava/openai_api.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;openai.api_type = &#34;azure&#34;&#xA;openai.api_key = &#39;...&#39;&#xA;openai.api_base = &#39;https://example-endpoint.openai.azure.com/&#39;&#xA;openai.api_version = &#34;2023-03-15-preview&#34;&#xA;DEPLOYMENT_ID=&#34;deployment-name&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Generate visual instruct tuning conversations using GPT-4&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/instruct/instruct_generate.py \&#xA;    --input_path data/instruct/llava_med_instruct_fig_captions.json \&#xA;    --output_path data/instruct/llava_med_instruct_60k_inline_mentions_gen.jsonl \&#xA;    --max-size 60000 \&#xA;    --use_inline_mentions True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Postprocessing of GPT-4 generated conversations&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/instruct/instruct_postprocess.py \&#xA;    --input_path data/instruct/llava_med_instruct_60k_inline_mentions_gen.jsonl \&#xA;    --output_path data/instruct/llava_med_instruct_60k_inline_mentions_post.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The file llava_med_instruct_60k_inline_mentions.json in the download is generated the same way as llava_med_instruct_60k_inline_mentions_post.json output file above.&lt;/p&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to LLaVA-Med folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://github.com/microsoft/LLaVA-Med.git&#xA;cd LLaVA-Med&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package: Create conda environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n llava-med python=3.10 -y&#xA;conda activate llava-med&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install additional packages for training cases&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;pip uninstall torch torchvision -y&#xA;pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117&#xA;pip install openai==0.27.8&#xA;pip uninstall transformers -y&#xA;pip install git+https://github.com/huggingface/transformers@cae78c46&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install einops ninja open-clip-torch&#xA;pip install flash-attn --no-build-isolation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Initialization from LLaVA-7B Weights&lt;/h3&gt; &#xA;&lt;p&gt;To ensure the smooth adaptation in terms of the multimodal chat capability, we initialize model weights from the general-domain &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt;. The delta weights of LLaVA comply with the LLaMA model license. You can add the delta to the original LLaMA weights to obtain the LLaVA weights.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the original LLaMA weights in the huggingface format by following the instructions &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the following scripts to get LLaVA weights ``LLaVA-7b-v0&#39;&#39; by applying our delta &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-7b-delta-v0&#34;&gt;LLaVA-7b-delta-v0&lt;/a&gt;). It will automatically download delta weights from our Hugging Face account.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This conversion command needs around 30 GB of CPU RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m llava.model.apply_delta \&#xA;    --base /path/to/llama-7b \&#xA;    --target /output/path/to/LLaVA-7b-v0 \&#xA;    --delta /huggingface.co/liuhaotian/LLaVA-7b-delta-v0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLaVA-Med Training&lt;/h3&gt; &#xA;&lt;p&gt;LLaVA-Med is trained on 8 A100 GPUs with 40GB memory with the following code. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly to keep the global batch size the same.&lt;/p&gt; &#xA;&lt;h4&gt;- Stage 1 (Optional): Medical Concept Alignment&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Med-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Pretrain: LLaVA-Med-7B, 8x A100 (40G). Time: ~7 hours.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llava-7b-v0 \&#xA;    --data_path /path/to/pubmed_600k.json \&#xA;    --image_folder /path/to/pubmed_600k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-med-7b-pretrain \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 2 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to none&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;You may run this with a single A100 GPU for the debugging purpose. Please note that the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; * &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; can be reduced to load model checkpoint into GPU memory. But the decreased global batch size increase the total training.&lt;/p&gt; &#xA;&lt;h4&gt;- Stage 2: Medical Visual Instruct Tuning&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-Med-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path /path/to/llama-med-vicuna-7b \&#xA;    --data_path /path/to/llava_med_instruct_60k_inline_mention_post.jsonl \&#xA;    --image_folder /data/to/llava_med_instruct_images \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end True \&#xA;    --bf16 True \&#xA;    --output_dir /path/to/checkpoint_llava_med_instruct_60k_inline_mention \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 1 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 5000 \&#xA;    --save_total_limit 3 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may directly perform medical instruction tuning on &lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/instruct/llava_med_instruct_60k_inline_mention.json&#34;&gt;&lt;code&gt;medical instruct data&lt;/code&gt;&lt;/a&gt;, by skipping Stage 1, and replacing Stage-1 checkpoint with the pretrained LLaVA checkpoint (LLaVA-7b-v0). Please see an example running script at &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/scripts/chunyl/run_training_llava_med.sh&#34;&gt;&lt;code&gt;run_training_llava_med.sh&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Serving&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Delta Weights&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/models/llava_med_in_text_60k_delta.zip&#34;&gt;llava_med_in_text_60k_delta.zip&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;11.06 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The model weights above are &lt;em&gt;delta&lt;/em&gt; weights. The usage of LLaVA-Med checkpoints should comply with the base LLM&#39;s model license: &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;LLaMA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the delta weights &lt;a href=&#34;https://hanoverprod.z21.web.core.windows.net/med_llava/models/llava_med_in_text_60k_delta.zip&#34;&gt;llava_med_in_text_60k_delta.zip&lt;/a&gt; and unzip.&lt;/li&gt; &#xA; &lt;li&gt;Get the original LLaMA weights in the huggingface format by following the instructions &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the following scripts to get LLaVA-Med weights by applying our delta. In the script below, set the --delta argument to the path of the unzipped &lt;code&gt;llava_med_in_text_60k_delta&lt;/code&gt; directory. It can be adapted for other delta weights by changing the &lt;code&gt;--delta&lt;/code&gt; argument (and base/target accordingly).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m llava.model.apply_delta \&#xA;    --base /path/to/llama-7b \&#xA;    --target /output/path/to/llava_med_in_text_60k \&#xA;    --delta path/to/llava_med_in_text_60k_delta&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-Med-7B --multi-modal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker (Multiple GPUs, when GPU VRAM &amp;lt;= 24GB)&lt;/h4&gt; &#xA;&lt;p&gt;If your the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-Med-7B --multi-modal --num-gpus 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;.&lt;/p&gt; &#xA;&lt;h4&gt;Send a test message&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.test_message --model-name LLaVA-Med-7B --controller http://localhost:10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.gradio_web_server --controller http://localhost:10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;You can open your browser and chat with a model now.&lt;/h4&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;Medical Visual Chat (GPT-assisted Evaluation)&lt;/h3&gt; &#xA;&lt;p&gt;Our GPT-assisted evaluation pipeline for multimodal modeling is provided for a comprehensive understanding of the capabilities of vision-language models. Please see our paper for more details.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate LLaVA-Med responses&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python model_vqa.py \&#xA;    --model-name ./checkpoints/LLaVA-7B-v0 \&#xA;    --question-file data/eval/llava_med_eval_qa50_qa.jsonl \&#xA;    --image-folder data/images/ \&#xA;    --answers-file /path/to/answer-file.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Evaluate the generated responses. In our case, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/data/eval/llava_med_eval_qa50_qa.jsonl&#34;&gt;&lt;code&gt;llava_med_eval_qa50_qa.jsonl&lt;/code&gt;&lt;/a&gt; contains the questions, context (captions and inline-mentions) and responses generated by text-only GPT-4 (0314), which we treat as ground truth.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/eval/eval_multimodal_chat_gpt_score.py \&#xA;    --question_input_path data/eval/llava_med_eval_qa50_qa.jsonl \&#xA;    --input_path /path/to/answer-file.jsonl \&#xA;    --output_path /path/to/save/gpt4-eval-for-individual-answers.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Summarize the evaluation results&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python summarize_gpt_review.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Medical VQA&lt;/h3&gt; &#xA;&lt;p&gt;Three Medical VQA datasets are considered in our experiments, including VQA-Rad, SLAKE, Pathology-VQA. We use VQA-Rad as the running example to illustrate how LLaVA-Med is applied to a downstream scenario.&lt;/p&gt; &#xA;&lt;h4&gt;- Prepare Data&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Please see VQA-Rad &lt;a href=&#34;https://paperswithcode.com/dataset/vqa-rad&#34;&gt;repo&lt;/a&gt; for setting up the dataset.&lt;/li&gt; &#xA; &lt;li&gt;Generate VQA-Rad dataset for LLaVA-Med conversation-style format (the same format with instruct tuning). For each dataset, we process it into three components: &lt;code&gt;train.json&lt;/code&gt;, &lt;code&gt;test.json&lt;/code&gt;, &lt;code&gt;images&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;- Fine-tuning&lt;/h4&gt; &#xA;&lt;p&gt;To achieve the higher performance for given a downstream dataset, the same full-model tuning script with instruct tuning is used to continue train LLaVA-Med.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Detailed script to fine-tune to downstream datasets: LLaVA-Med-7B, 8x A100 (40G). Time: ~1 hour.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path /path/to/checkpoint_llava_med_instruct_60k_inline_mention \&#xA;    --data_path /path/to/eval/vqa_rad/train.json \&#xA;    --image_folder /path/to/eval/vqa_rad/images \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end True \&#xA;    --bf16 True \&#xA;    --output_dir /path/to/checkpoint_llava_med_instruct_60k_inline_mention/eval/fine_tuned/vqa_rad \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 1 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 8 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 5000 \&#xA;    --save_total_limit 3 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;- Evaluation&lt;/h4&gt; &#xA;&lt;p&gt;Depending on which checkpoint is employed in evaluation, zero-shot performance is reported on medical instruct tuned checkpoint (eg, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/path/to/checkpoint_llava_med_instruct_60k_inline_mention&#34;&gt;LLaVA-Med-7B&lt;/a&gt;), and fine-tuned performance is reported on checkpoint that has been further tuned on training set of the downstream datasets (eg, &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/path/to/checkpoint_llava_med_instruct_60k_inline_mention/fine_tuned/vqa_rad&#34;&gt;LLaVA-Med-7B-VQA-Rad&lt;/a&gt; ).&lt;/p&gt; &#xA;&lt;p&gt;(a) Generate LLaVA responses on ScienceQA dataset&lt;/p&gt; &#xA;&lt;p&gt;(a.1). [Option 1] Multiple-GPU inference You may evaluate this with multiple GPUs, and concatenate the generated jsonl files. Please refer to our script for &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/scripts/chunyl/finetune_on_benchmarks/eval_med_dataset_batch.sh&#34;&gt;batch evaluation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/eval/run_med_datasets_eval_batch.py --num-chunks 8  --model-name /path/to/checkpoint_llava_med_instruct_60k_inline_mention/eval/fine_tuned/vqa_rad \&#xA;    --question-file path/to/eval/vqa_rad/test.json \&#xA;    --image-folder path/to/eval/vqa_rad/images \&#xA;    --answers-file /path/to/checkpoint_llava_med_instruct_60k_inline_mention/eval/fine_tuned/vqa_rad/test-answer-file.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(a.2). [Option 2] Single-GPU inference&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/eval/model_vqa_med.py --model-name /path/to/checkpoint_llava_med_instruct_60k_inline_mention/eval/fine_tuned/vqa_rad \&#xA;    --question-file path/to/eval/vqa_rad/test.json \&#xA;    --image-folder path/to/eval/vqa_rad/images \&#xA;    --answers-file /path/to/checkpoint_llava_med_instruct_60k_inline_mention/eval/fine_tuned/vqa_rad/test-answer-file.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(b) Evaluate the generated responses&lt;/p&gt; &#xA;&lt;p&gt;(b.1). [Option 1] Evaluation for all three VQA datasets&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;&#xA;python llava/eval/run_eval_batch.py \&#xA;    --pred_file_parent_path /path/to/llava-med \&#xA;    --target_test_type test-answer-file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It collects the decoding results of all predictions files under the project path, computes the corresponding evaluation metrics, and outputs the results in &#34;&lt;code&gt;eval_results_med_datasets.jsonl&lt;/code&gt;&#34;. To analyze the score, we provdie ipython notebook &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/llava/notebook/run_eval_metrics.ipynb&#34;&gt;run_eval_metrics.ipynb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;(b.2). [Option 2] Evaluation for on one specific VQA dataset&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python llava/eval/run_eval.py \&#xA;    --gt /path/to/eval/vqa_rad/test.json \&#xA;    --pred /path/to/checkpoint_llava_med_instruct_60k_inline_mention/eval/fine_tuned/vqa_rad/test-answer-file.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please find the LLaVA-Med performance in &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/LLaVA-Med/main/docs/llava_med_performance.md&#34;&gt;llava_med_performance.md&lt;/a&gt; or in the paper.&lt;/p&gt; &#xA;&lt;h2&gt;Model Description&lt;/h2&gt; &#xA;&lt;p&gt;Large Language and Vision Assistant for bioMedicine (i.e., ‚ÄúLLaVA-Med‚Äù) is a large language and vision model trained using a curriculum learning method for adapting LLaVA to the biomedical domain. It is an open-source release intended for research use only to facilitate reproducibility of the corresponding paper which claims improved performance for open-ended biomedical questions answering tasks, including common visual question answering (VQA) benchmark datasets such as PathVQA and VQA-RAD.&lt;/p&gt; &#xA;&lt;h3&gt;Model Uses&lt;/h3&gt; &#xA;&lt;h4&gt;Intended Use&lt;/h4&gt; &#xA;&lt;p&gt;The data, code, and model checkpoints are intended to be used solely for (I) future research on visual-language processing and (II) reproducibility of the experimental results reported in the reference paper. The data, code, and model checkpoints are not intended to be used in clinical care or for any clinical decision making purposes.&lt;/p&gt; &#xA;&lt;h4&gt;Primary Intended Use&lt;/h4&gt; &#xA;&lt;p&gt;The primary intended use is to support AI researchers reproducing and building on top of this work. LLaVA-Med and its associated models should be helpful for exploring various biomedical vision-language processing (VLP ) and vision question answering (VQA) research questions.&lt;/p&gt; &#xA;&lt;h4&gt;Out-of-Scope Use&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Any&lt;/strong&gt; deployed use case of the model --- commercial or otherwise --- is out of scope. Although we evaluated the models using a broad set of publicly-available research benchmarks, the models and evaluations are intended &lt;em&gt;for research use only&lt;/em&gt; and not intended for deployed use cases. Please refer to &lt;a href=&#34;https://aka.ms/llava-med&#34;&gt;the associated paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;This model builds upon &lt;a href=&#34;https://aka.ms/biomedclip-paper&#34;&gt;PMC-15M dataset&lt;/a&gt;, which is a large-scale parallel image-text dataset for biomedical vision-language processing. It contains 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central. It covers a diverse range of biomedical image types, such as microscopy, radiography, histology, and more.&lt;/p&gt; &#xA;&lt;h3&gt;Limitations&lt;/h3&gt; &#xA;&lt;p&gt;This model was developed using English corpora, and thus may be considered English-only. This model is evaluated on a narrow set of biomedical benchmark tasks, described in &lt;a href=&#34;https://aka.ms/llava-med&#34;&gt;LLaVA-Med paper&lt;/a&gt;. As such, it is not suitable for use in any clinical setting. Under some conditions, the model may make inaccurate predictions and display limitations, which may require additional mitigation strategies. In particular, this model is likely to carry many of the limitations of the model from which it is derived, &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Further, this model was developed in part using the &lt;a href=&#34;https://aka.ms/biomedclip-paper&#34;&gt;PMC-15M&lt;/a&gt; dataset. The figure-caption pairs that make up this dataset may contain biases reflecting the current practice of academic publication. For example, the corresponding papers may be enriched for positive findings, contain examples of extreme cases, and otherwise reflect distributions that are not representative of other sources of biomedical data.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Our project is built upon &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;LLaVA&lt;/a&gt; and &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: They provide our base models with the amazing multimodal and langauge capabilities, respectively!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you find LLaVA-Med useful for your your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{li2023llavamed,&#xA;  title={Llava-med: Training a large language-and-vision assistant for biomedicine in one day},&#xA;  author={Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},&#xA;  journal={arXiv preprint arXiv:2306.00890},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224&#34;&gt;BioMed CLIP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>genius-space/genius-lesson-project</title>
    <updated>2023-11-11T02:31:43Z</updated>
    <id>tag:github.com,2023-11-11:/genius-space/genius-lesson-project</id>
    <link href="https://github.com/genius-space/genius-lesson-project" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;–°—É—á–∞—Å–Ω–∞ –ø–µ–∫–∞—Ä–Ω—è &#34;SHOP bakery&#34;&lt;/h1&gt;</summary>
  </entry>
</feed>