<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-04T01:31:16Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>VA3HDL/hamdashboard</title>
    <updated>2024-06-04T01:31:16Z</updated>
    <id>tag:github.com,2024-06-04:/VA3HDL/hamdashboard</id>
    <link href="https://github.com/VA3HDL/hamdashboard" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Customizable Dashboard for Ham Radio&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=sIdqMQTGNSc&#34;&gt;YouTube - Presentation video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Instructions:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Just download the files from the Github repository (hamdash.html, config.js, and wheelzoom.js) and keep them together on the same folder.&lt;/li&gt; &#xA; &lt;li&gt;Open hamdash.html with any browser of your preference and you done.&lt;/li&gt; &#xA; &lt;li&gt;With any text editor (like Notepad) you can change the source images (can be more than one per box) or the menu options from the config.js file.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/9ZZXg60tN-o&#34;&gt;YouTube - Configuration instructions contributed by Jason KM4ACK&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Quick Help:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Double click on an image to expand to full screen.&lt;/li&gt; &#xA; &lt;li&gt;Double click again to close full screen view.&lt;/li&gt; &#xA; &lt;li&gt;Right click on an image to display the next one. (In the latest version is possible to add multiple images per box.)&lt;/li&gt; &#xA; &lt;li&gt;The content refreshes automatically every 5 minutes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Is that easy!&lt;/p&gt; &#xA;&lt;p&gt;73 de Pablo, VA3HDL&lt;/p&gt; &#xA;&lt;h3&gt;Fix for Pi-Star iFrame embedding issues:&lt;/h3&gt; &#xA;&lt;p&gt;This error can occur if the server has certain security measures in place, such as the x-frame-options header, which prevents its content from being embedded on other websites using iframes.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Login via ssh to the pi-star then run this command to switch to Read/Write mode:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; rpi-rw&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the file /etc/nginx/default.d/security.conf comment the line below with a &#34;#&#34; in front, like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # add_header X-Frame-Options  &#34;SAMEORIGIN&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run this command to switch back to Read Only mode:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; rpi-ro&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then reboot the pi-star&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;More on iFrame embedding:&lt;/h3&gt; &#xA;&lt;p&gt;There is very little that can be done on the client side if the source site does not allow embedding the site inside another page (like the dashboard!) specially if the user can&#39;t change the server settings (most cases.)&lt;/p&gt; &#xA;&lt;p&gt;As a workaround for these issues, I&#39;ve tested running a local proxy on my computer to strip out the x-frame-options header coming from the source server and it worked well on some cases. But setting up a proxy adds another layer of complexity to the setup.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;h3&gt;2024.05.27 Changelog:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Moved the configuration parts of the JavaScript code to its own file &#34;config.js&#34; so it is easy to upgrade after updates to the main code. Suggested by Lou KI5FTY.&lt;/li&gt; &#xA; &lt;li&gt;Improved menu usability&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2024.05.25 Changelog:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Removed dependencies to local installed fonts. Fonts now are loaded from Google Fonts directly to ensure consistency.&lt;/li&gt; &#xA; &lt;li&gt;Ability to add multiple images per position. Images are rotated automatically every 30 seconds.&lt;/li&gt; &#xA; &lt;li&gt;Autorefresh is now paused automatically when switching to a website (from menu) or when an image is zoomed-in to full screen&lt;/li&gt; &#xA; &lt;li&gt;Moved configuration variables to the top of the script and added extra commentary to ease the initial setup&lt;/li&gt; &#xA; &lt;li&gt;Added menu to the right of the page. Now the left menu has ham radio links and right menu has weather links&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Samples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/VA3HDL/hamdashboard/raw/main/examples/dashboard_sample.png?raw=true&#34; alt=&#34;VA3HDL Sample Dashboard&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/VA3HDL/hamdashboard/raw/main/examples/N4NBC-sample.jpg?raw=true&#34; alt=&#34;N4NBC Sample Dashboard&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/VA3HDL/hamdashboard/raw/main/examples/KM4ACK-sample.png?raw=true&#34; alt=&#34;KM4ACK Sample Dashboard&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dual menu example:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/VA3HDL/hamdashboard/raw/main/examples/DualMenu.png?raw=true&#34; alt=&#34;Dual side Menu Sample Dashboard&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>YingqingHe/Awesome-LLMs-meet-Multimodal-Generation</title>
    <updated>2024-06-04T01:31:16Z</updated>
    <id>tag:github.com,2024-06-04:/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation</id>
    <link href="https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üî•üî•üî• A curated list of papers on LLMs-based multimodal generation (image, video, 3D and audio).&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt; LLMs Meet Multimodal Generation and Editing: A Survey &lt;/h2&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2405.19334&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ArXiv-2211.14758-red&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;ü§ó Introduction&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This repository contains a curated list of &lt;strong&gt;LLMs meet multimodal generation&lt;/strong&gt;. Modalities consist of visual (including image, video and 3D) and audio (including sound, speech and music). The survey paper will be released soon. &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/assets/fig.jpg&#34; width=&#34;300&amp;quot;&amp;quot;&#34;&gt; &lt;/p&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We welcome any contributions and suggestions to our repository or the addition of your own work. Feel free to make a pull request or leave your comments!!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìã Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-introduction&#34;&gt;ü§ó Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-contents&#34;&gt;üìã Contents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-tips&#34;&gt;üíò Tips&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-multimodal-generation&#34;&gt;üìç Multimodal Generation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#image-generation&#34;&gt;Image Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-llm-based&#34;&gt;üîÖ LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#non-llm-based-clipt5&#34;&gt;Non-LLM-based (Clip/T5)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#datasets&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#video-generation&#34;&gt;Video Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-llm-based-1&#34;&gt;üîÖ LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#non-llm-based&#34;&gt;Non-LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#datasets-1&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#3d-generation&#34;&gt;3D Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-llm-based-2&#34;&gt;üîÖ LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#non-llm-based-clipt5-1&#34;&gt;Non-LLM-based (Clip/T5)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#datasets-2&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#audio-generation&#34;&gt;Audio Generation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-llm-based-3&#34;&gt;üîÖ LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#non-llm-based-1&#34;&gt;Non-LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#datasets-3&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#generation-with-multiple-modalities&#34;&gt;Generation with Multiple Modalities&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-llm-based-4&#34;&gt;üîÖ LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#non-llm-based-2&#34;&gt;Non-LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-multimodal-editing&#34;&gt;üìç Multimodal Editing&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#image-editing&#34;&gt;Image Editing&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-llm-based-5&#34;&gt;üîÖ LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#non-llm-based-clipt5-2&#34;&gt;Non-LLM-based (Clip/T5)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#video-editing&#34;&gt;Video Editing&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-llm-based-6&#34;&gt;üîÖ LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#non-llm-based-clipt5-3&#34;&gt;Non-LLM-based (Clip/T5)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#3d-editing&#34;&gt;3D Editing&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-llm-based-7&#34;&gt;üîÖ LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#non-llm-based-clipt5-4&#34;&gt;Non-LLM-based (Clip/T5)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#audio-editing&#34;&gt;Audio Editing&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-llm-based-8&#34;&gt;üîÖ LLM-based&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#non-llm-based-clipt5-5&#34;&gt;Non-LLM-based (Clip/T5)&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-multimodal-agents&#34;&gt;üìç Multimodal Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-multimodal-understanding-with-llms&#34;&gt;üìç Multimodal Understanding with LLMs&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#image-understanding&#34;&gt;Image Understanding&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#video-understanding&#34;&gt;Video Understanding&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#3d-understanding&#34;&gt;3D Understanding&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#audio-understanding&#34;&gt;Audio Understanding&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-multimodal-llm-safety&#34;&gt;üìç Multimodal LLM Safety&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#attack&#34;&gt;Attack&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#defense-and-detect&#34;&gt;Defense and Detect&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#alignment&#34;&gt;Alignment&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#datasets-4&#34;&gt;Datasets&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#3d-video-and-audio-safety&#34;&gt;3D, Video and Audio Safety&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-related-surveys&#34;&gt;üìç Related Surveys&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#llm&#34;&gt;LLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#vision&#34;&gt;Vision&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-team&#34;&gt;üë®‚Äçüíª Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#-citation&#34;&gt;üòâ Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation/main/#%EF%B8%8F-star-history&#34;&gt;‚≠êÔ∏è Star History&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üíò Tips&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;‚úÖ Paper searching via catatogue&lt;/strong&gt;: directly clicking the content of the catatogue to select the area of your research and browse related papers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;‚úÖ Paper searching via author name&lt;/strong&gt;: Free feel to search papers of a specific author via &lt;code&gt;ctrl + F&lt;/code&gt; and then type the author name. The dropdown list of authors will automatically expand when searching.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;‚úÖ Paper searching via tag&lt;/strong&gt;: You can also search the related papers via the following tags: &lt;code&gt;customization&lt;/code&gt;, &lt;code&gt;iteractive&lt;/code&gt;, &lt;code&gt;human motion generation&lt;/code&gt;. (More tags are ongoing)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìç Multimodal Generation&lt;/h1&gt; &#xA;&lt;h2&gt;Image Generation&lt;/h2&gt; &#xA;&lt;h3&gt;üîÖ LLM-based&lt;/h3&gt; &#xA;&lt;!-- + **Genie: Generative Interactive Environments** (26 Feb 2024)&lt;details&gt;&lt;summary&gt;Jake Bruce, Michael Dennis, Ashley Edwards, et al.&lt;/summary&gt; Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rockt√§schel&lt;/details&gt;&#xA;[![Paper](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2402.15391v1) --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chameleon: Mixed-Modal Early-Fusion Foundation Models&lt;/strong&gt; (16 May 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Chameleon Team&lt;/summary&gt;&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/pdf/2405.09818&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/Chameleon%3A-Mixed-Modal-Early-Fusion-Foundation-Team/32112b798f70faab00e14806f51d46058cf5e597?utm_source=direct_link&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=32112b798f70faab00e14806f51d46058cf5e597&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Graphic Design with Large Multimodal Model&lt;/strong&gt; (22 Apr 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yutao Cheng, Zhao Zhang, Maoke Yang, et al.&lt;/summary&gt; Yutao Cheng, Zhao Zhang, Maoke Yang, Hui Nie, Chunyuan Li, Xinglong Wu, and Jie Shao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.14368&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9259b476c31ba52b7e9ed059e5fbce2125092738&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=9259b476c31ba52b7e9ed059e5fbce2125092738&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/graphic-design-ai/graphist&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/graphic-design-ai/graphist.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PMG : Personalized Multimodal Generation with Large Language Models&lt;/strong&gt; (7 Apr 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, et al.&lt;/summary&gt;Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, Jieming Zhu, Xi Xiao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.08677&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/PMG-%3A-Personalized-Multimodal-Generation-with-Large-Shen-Zhang/cfb9eba1b5c55bb0052df41eaaff8716f9c420bd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=cfb9eba1b5c55bb0052df41eaaff8716f9c420bd&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MineDreamer: Learning to Follow Instructions via Chain-of-Imagination for Simulated-World Control&lt;/strong&gt; (19 Mar 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Enshen Zhou, Yiran Qin, Zhenfei Yin, et al.&lt;/summary&gt;Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, Jing Shao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.12037&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ae06df762adcc4221162e83a737ea63cff47e65d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=ae06df762adcc4221162e83a737ea63cff47e65d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Zhoues/MineDreamer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Zhoues/MineDreamer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sites.google.com/view/minedreamer/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment&lt;/strong&gt; (8 Mar 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xiwei Hu, Rui Wang, Yixiao Fang, et al.&lt;/summary&gt; Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Gang Yu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.05135&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/da0d382c7fa981ba185ca633868442b75cb76de6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=da0d382c7fa981ba185ca633868442b75cb76de6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ELLA-Diffusion/ELLA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ELLA-Diffusion/ELLA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ella-diffusion.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis&lt;/strong&gt; (30 Jan 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zecheng Tang, Chenfei Wu, Zekai Zhang, et al.&lt;/summary&gt;Zecheng Tang, Chenfei Wu, Zekai Zhang, Mingheng Ni, Shengming Yin, Yu Liu, Zhengyuan Yang, Lijuan Wang, Zicheng Liu, Juntao Li, Nan Duan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.17093&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/StrokeNUWA%3A-Tokenizing-Strokes-for-Vector-Graphic-Tang-Wu/b2f6830afe63eb477294f17f0d3a6923135950f9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=b2f6830afe63eb477294f17f0d3a6923135950f9&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DiffusionGPT: LLM-Driven Text-to-Image Generation System&lt;/strong&gt; (18 Jan 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jie Qin, Jie Wu, Weifeng Chen, et al.&lt;/summary&gt; Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.10061&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d4b1a1c62a03ccffcf24983eb4fe22335cbb89b6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=d4b1a1c62a03ccffcf24983eb4fe22335cbb89b6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/DiffusionGPT/DiffusionGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/DiffusionGPT/DiffusionGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;StarVector: Generating Scalable Vector Graphics Code from Images&lt;/strong&gt; (17 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Juan A. Rodriguez, Shubham Agarwal, Issam H. Laradji, et al.&lt;/summary&gt; Juan A. Rodriguez, Shubham Agarwal, Issam H. Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, Marco Pedersoli&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.11556&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/60d3ade5c0085f5de1f5ab944cc058c78706ac66&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=60d3ade5c0085f5de1f5ab944cc058c78706ac66&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/joanrod/star-vector&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/joanrod/star-vector.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation&lt;/strong&gt; (14 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jinguo Zhu, Xiaohan Ding, Yixiao Ge, et al.&lt;/summary&gt; Jinguo Zhu, Xiaohan Ding, Yixiao Ge, Yuying Ge, Sijie Zhao, Hengshuang Zhao, Xiaohua Wang, Ying Shan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.09251&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AILab-CVC/VL-GPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AILab-CVC/VL-GPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;StoryGPT-V: Large Language Models as Consistent Story Visualizers&lt;/strong&gt; (13 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xiaoqian Shen, Mohamed Elhoseiny&lt;/summary&gt; Xiaoqian Shen, Mohamed Elhoseiny&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.02252&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e49cb2ab3a7990e3d05042197ae8b3fd934453de&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=e49cb2ab3a7990e3d05042197ae8b3fd934453de&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;GENIXER: Empowering Multimodal Large Language Models as a Powerful Data Generator&lt;/strong&gt; (11 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Henry Hengyuan Zhao, Pan Zhou, Mike Zheng Shou&lt;/summary&gt; Henry Hengyuan Zhao, Pan Zhou, Mike Zheng Shou&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.06731&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/cb2295766b2f8f35524f6a9f93ae39d948d50bd4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=cb2295766b2f8f35524f6a9f93ae39d948d50bd4&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Customization Assistant for Text-to-image Generation&lt;/strong&gt; (5 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, et al.&lt;/summary&gt; Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Tong Sun&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.03045&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f30bb09dbd95845d792bdac217a9a652635ee8a5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=f30bb09dbd95845d792bdac217a9a652635ee8a5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; Tags: &lt;code&gt;customization&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ChatIllusion: Efficient-Aligning Interleaved Generation ability with Visual Instruction Model&lt;/strong&gt; (29 Nov 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xiaowei Chi, Yijiang Liu, Zhengkai Jiang, et al.&lt;/summary&gt; Xiaowei Chi, Yijiang Liu, Zhengkai Jiang, Rongyu Zhang, Ziyi Lin, Renrui Zhang, Peng Gao, Chaoyou Fu, Shanghang Zhang, Qifeng Liu, Yike Guo&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.17963&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/22d55c52f43f59634586ab95fefbb7dba8c8b190&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=22d55c52f43f59634586ab95fefbb7dba8c8b190&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/litwellchi/ChatIllusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/litwellchi/ChatIllusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback&lt;/strong&gt; (29 Nov 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiao Sun, Deqing Fu, Yushi Hu, et al.&lt;/summary&gt;Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, Cyrus Rashtchian&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.17946&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d16f72b7be526dee5eb49e5afffeea2bddba5e66&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=d16f72b7be526dee5eb49e5afffeea2bddba5e66&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;COLE: A Hierarchical Generation Framework for Graphic Design&lt;/strong&gt; (28 Nov 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Peidong Jia, Chenxuan Li, Zeyu Liu, et al.&lt;/summary&gt;Peidong Jia, Chenxuan Li, Zeyu Liu, Yichao Shen, Xingru Chen, Yuhui Yuan, Yinglin Zheng, Dong Chen, Ji Li, Xiaodong Xie, Shanghang Zhang, Baining Guo&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.16974&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8441c30ad4abdca9ee380aa6f22ffd731b10231b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=8441c30ad4abdca9ee380aa6f22ffd731b10231b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://graphic-design-generation.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TextDiffuser-2: Unleashing the Power of Language Models for Text Rendering&lt;/strong&gt; (28 Nov 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jingye Chen, Yupan Huang, Tengchao Lv, et al.&lt;/summary&gt;Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.16465&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1c6e2a4da1ead685a95c079751bf4d7a727d8180&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=1c6e2a4da1ead685a95c079751bf4d7a727d8180&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jingyechen.github.io/textdiffuser2/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/textdiffuser-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/JingyeChen22/TextDiffuser-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLMGA: Multimodal Large Language Model based Generation Assistant&lt;/strong&gt; (27 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Bin Xia, Shiyin Wang, Yingfan Tao, et al.&lt;/summary&gt; Bin Xia, Shiyin Wang, Yingfan Tao, Yitong Wang, Jiaya Jia&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.16500&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/769a924d0af014acec326f50c15c5d70d258a969&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=769a924d0af014acec326f50c15c5d70d258a969&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/LLMGA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dvlab-research/LLMGA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llmga.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Self-correcting LLM-controlled Diffusion Models&lt;/strong&gt; (27 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Tsung-Han Wu, Long Lian, Joseph E. Gonzalez, et al.&lt;/summary&gt; Tsung-Han Wu, Long Lian, Joseph E. Gonzalez, Boyi Li, Trevor Darrell&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.16090&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/42c4315b5d2e33d7d9a0afdf84e6a47ccd7a700e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=42c4315b5d2e33d7d9a0afdf84e6a47ccd7a700e&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tsunghan-wu/SLD&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/tsunghan-wu/SLD.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tokenize and Embed ALL for Multi-modal Large Language Models&lt;/strong&gt; (8 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zhen Yang, Yingxue Zhang, Fandong Meng, et al.&lt;/summary&gt; Zhen Yang, Yingxue Zhang, Fandong Meng, Jie Zhou&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.04589&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/59d716b442ab760a78f58de6748c0fa1d507bfc1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=59d716b442ab760a78f58de6748c0fa1d507bfc1&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models&lt;/strong&gt; (20 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, et al.&lt;/summary&gt; Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Yusen Hu, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.18332&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/58b77dc0603eb52559d98a383bf9649fd31d0bc5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=58b77dc0603eb52559d98a383bf9649fd31d0bc5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts&lt;/strong&gt; (16 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, et al.&lt;/summary&gt;Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, Peter Wonka&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.10640&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4cb2c262ce34f41974f1b1623fc5a6e32956ded3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=4cb2c262ce34f41974f1b1623fc5a6e32956ded3&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hananshafi/llmblueprint&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hananshafi/llmblueprint.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Making Multimodal Generation Easier: When Diffusion Models Meet LLMs&lt;/strong&gt; (13 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xiangyu Zhao, Bo Liu, Qijiong Liu, et al.&lt;/summary&gt;Xiangyu Zhao, Bo Liu, Qijiong Liu, Guangyuan Shi, Xiao-Ming Wu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.08949v1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/833cdd713c27ab5899bb912a1d511c10af61cefb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=833cdd713c27ab5899bb912a1d511c10af61cefb&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zxy556677/EasyGen&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zxy556677/EasyGen.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation&lt;/strong&gt; (12 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zhengyuan Yang, Jianfeng Wang, Linjie Li, et al.&lt;/summary&gt;Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.08541&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1d14a708622917da4b9820ada6d32af24fc1651a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=1d14a708622917da4b9820ada6d32af24fc1651a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://idea2img.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zyang-ur/Idea2Img&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zyang-ur/Idea2Img.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation&lt;/strong&gt; (11 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jie An, Zhengyuan Yang, Linjie Li, et al.&lt;/summary&gt;Jie An, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Lijuan Wang, Jiebo Luo&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.07749&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-3-blue.svg?paper=7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models&lt;/strong&gt; (11 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zeqiang Lai, Xizhou Zhu, Jifeng Dai, et al.&lt;/summary&gt;Zeqiang Lai, Xizhou Zhu, Jifeng Dai, Yu Qiao, Wenhai Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.07653&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f669d7a6fab0147253178a6fc854e05e3d92fb3f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=f669d7a6fab0147253178a6fc854e05e3d92fb3f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://minidalle3.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Zeqiang-Lai/Mini-DALLE3&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Zeqiang-Lai/Mini-DALLE3.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[DALL-E 3] Improving Image Generation with Better Captions&lt;/strong&gt; &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;James Betker, Gabriel Goh, Li Jing, et al.&lt;/summary&gt;James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, Aditya Ramesh&#xA;  &lt;/details&gt; &lt;a href=&#34;https://cdn.openai.com/papers/dall-e-3.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/cfee1826dd4743eab44c6e27a0cc5970effa4d80&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-146-blue.svg?paper=cfee1826dd4743eab44c6e27a0cc5970effa4d80&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openai.com/dall-e-3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens&lt;/strong&gt; (3 Oct 2023)&lt;br&gt; Kaizhi Zheng, Xuehai He, Xin Eric Wang.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.02239&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e7d09b6f2bc878cf2c993acf675f409d0b55f35a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-26-blue.svg?paper=e7d09b6f2bc878cf2c993acf675f409d0b55f35a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://eric-ai-lab.github.io/minigpt-5.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/eric-ai-lab/MiniGPT-5&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/eric-ai-lab/MiniGPT-5.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Making LLaMA SEE and Draw with SEED Tokenizer&lt;/strong&gt; (2 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yuying Ge, Sijie Zhao, Ziyun Zeng, et al.&lt;/summary&gt;Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, Ying Shan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.01218&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5ba1525dc6d382ee0a4a1ca3c64fc5907ca64c67&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-19-blue.svg?paper=5ba1525dc6d382ee0a4a1ca3c64fc5907ca64c67&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ailab-cvc.github.io/seed/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AILab-CVC/SEED&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AILab-CVC/SEED.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dad1ed9a9fb76fe83b.gradio.live/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists&lt;/strong&gt; (30 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yulu Gan, Sungwoo Park, Alexander Schubert, et al.&lt;/summary&gt;Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, Ahmed M. Alaa&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.00390&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/819f477065088220a6f706cd9ef76dbcb4b4c134&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=819f477065088220a6f706cd9ef76dbcb4b4c134&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AlaaLab/InstructCV&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AlaaLab/InstructCV.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/alaa-lab/InstructCV&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition&lt;/strong&gt; (26 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Pan Zhang, Xiaoyi Dong, Bin Wang, et al.&lt;/summary&gt; Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.15112&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c1e450284e7d6cac1855330a1197df8537df653f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-48-blue.svg?paper=c1e450284e7d6cac1855330a1197df8537df653f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text-to-Image Generation for Abstract Concepts&lt;/strong&gt; (26 Sep 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiayi Liao, Xu Chen, Qiang Fu, et al.&lt;/summary&gt;Jiayi Liao, Xu Chen, Qiang Fu, Lun Du, Xiangnan He, Xiang Wang, Shi Han, Dongmei Zhang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.14623&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/0d38f1edac66b4645cf5fa05abaf9d92cba5d5d3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=0d38f1edac66b4645cf5fa05abaf9d92cba5d5d3&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamLLM: Synergistic Multimodal Comprehension and Creation&lt;/strong&gt; (20 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Runpei Dong, Chunrui Han, Yuang Peng, et al.&lt;/summary&gt;Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, Li Yi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.11499&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/7b689adb8c156d6158660f90d1c86888ee281f63&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-38-blue.svg?paper=7b689adb8c156d6158660f90d1c86888ee281f63&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dreamllm.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RunpeiDong/DreamLLM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RunpeiDong/DreamLLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SwitchGPT: Adapting Large Language Models for Non-Text Outputs&lt;/strong&gt; (14 Sep 2023)&lt;br&gt; Wang, Xinyu, Bohan Zhuang, and Qi Wu.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.07623&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/366564d210768814bc880e391b909cfbd95f8964&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=366564d210768814bc880e391b909cfbd95f8964&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xinke-wang/SwitchGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/xinke-wang/SwitchGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;NExT-GPT: Any-to-Any Multimodal LLM&lt;/strong&gt; (11 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shengqiong Wu, Hao Fei, Leigang Qu, et al.&lt;/summary&gt;Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.05519&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/fa75a55760e6ea49b39b83cb85c99a22e1088254&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-94-blue.svg?paper=fa75a55760e6ea49b39b83cb85c99a22e1088254&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://next-gpt.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NExT-GPT/NExT-GPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://9704af1b453125102e.gradio.live/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation&lt;/strong&gt; (9 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Leigang Qu, Shengqiong Wu, Hao Fei, et al. ACM MM 2023&lt;/summary&gt;Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, Tat-Seng Chua&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.05095&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/7d78238a9bad60433d616abdd93c735087d99670&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-10-blue.svg?paper=7d78238a9bad60433d616abdd93c735087d99670&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://layoutllm-t2i.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LayoutLLM-T2I/LayoutLLM-T2I&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/LayoutLLM-T2I/LayoutLLM-T2I.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Planting a SEED of Vision in Large Language Model&lt;/strong&gt; (16 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yuying Ge, Yixiao Ge, Ziyun Zeng, et al.&lt;/summary&gt;Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, Ying Shan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.08041&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/40298b8d50109c52fc10763eddc64a07cf8acb31&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-30-blue.svg?paper=40298b8d50109c52fc10763eddc64a07cf8acb31&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ailab-cvc.github.io/seed/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AILab-CVC/SEED&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AILab-CVC/SEED.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Generative Pretraining in Multimodality&lt;/strong&gt; (11 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Quan Sun, Qiying Yu, Yufeng Cui, et al.&lt;/summary&gt;Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.05222&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/94053805cd59f2e9a47fe3f080c7e7afefb337cc&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-43-blue.svg?paper=94053805cd59f2e9a47fe3f080c7e7afefb337cc&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/baaivision/Emu&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/baaivision/Emu.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://218.91.113.230:9002&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs&lt;/strong&gt; (30 Jun 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023 Spotlight] Lijun Yu, Yong Cheng, Zhiruo Wang, et al.&lt;/summary&gt;Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.17842&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/376f494126d1ea4f571ea0263c43ac2b6331800a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-12-blue.svg?paper=376f494126d1ea4f571ea0263c43ac2b6331800a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Controllable Text-to-Image Generation with GPT-4&lt;/strong&gt; (29 May 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Tianjun Zhang, Yi Zhang, Vibhav Vineet, et al.&lt;/summary&gt;Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, Xin Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.18583&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/3a79545719fb193a6b4042ef7d1d87cfd267be06&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-20-blue.svg?paper=3a79545719fb193a6b4042ef7d1d87cfd267be06&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tianjunz/Control-GPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Generating Images with Multimodal Language Models&lt;/strong&gt; (26 May 2023)&lt;br&gt; [NeurIPS 2023] Koh, Jing Yu, Daniel Fried, and Ruslan Salakhutdinov. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.17216&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6fb5c0eff3696ef252aca9638e10176ecce7cecb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-73-blue.svg?paper=6fb5c0eff3696ef252aca9638e10176ecce7cecb&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jykoh.com/gill&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kohjingyu/gill&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/kohjingyu/gill.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LayoutGPT: Compositional Visual Planning and Generation with Large Language Models&lt;/strong&gt; (24 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, et al.&lt;/summary&gt;Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, William Yang Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.15393&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/66d755730f5d08a6f4fcc5e81f24982ba389dca9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-31-blue.svg?paper=66d755730f5d08a6f4fcc5e81f24982ba389dca9&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://layoutgpt.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/weixi-feng/LayoutGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/weixi-feng/LayoutGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Programming for Text-to-Image Generation and Evaluation&lt;/strong&gt; (24 May 2023)&lt;br&gt; [NeurIPS 2023] Jaemin Cho, Abhay Zala, Mohit Bansal.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.15328&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9837349417e36ef5be06da0fd6c74042148bdaa2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-24-blue.svg?paper=9837349417e36ef5be06da0fd6c74042148bdaa2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://vp-t2i.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/j-min/VPGen&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/j-min/VPGen.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models&lt;/strong&gt; (23 May 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Long Lian, Boyi Li, Adam Yala, et al.&lt;/summary&gt;Long Lian, Boyi Li, Adam Yala, Trevor Darrell&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.13655&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-39-blue.svg?paper=e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llm-grounded-diffusion.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TonyLianLong/LLM-groundedDiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/TonyLianLong/LLM-groundedDiffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration&lt;/strong&gt; (22 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Qifan Yu, Juncheng Li, Wentao Ye, et al.&lt;/summary&gt;Qifan Yu, Juncheng Li, Wentao Ye, Siliang Tang, Yueting Zhuang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.12799&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/43a55dbd95c9d5cd82de8db276f41adeec4a937d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-7-blue.svg?paper=43a55dbd95c9d5cd82de8db276f41adeec4a937d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Yuqifan1117/Labal-Anything-Pipeline&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Yuqifan1117/Labal-Anything-Pipeline.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation&lt;/strong&gt; (18 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Yujie Lu, Xianjun Yang, Xiujun Li, et al.&lt;/summary&gt;Yujie Lu, Xianjun Yang, Xiujun Li, Xin Eric Wang, William Yang Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.11116&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/972501b057e2b84d6ce6506f70bcac697bab7872&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-10-blue.svg?paper=972501b057e2b84d6ce6506f70bcac697bab7872&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/YujieLu10/LLMScore&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/YujieLu10/LLMScore.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models&lt;/strong&gt; (9 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ACM MM 2023] Shanshan Zhong, Zhongzhan Huang, Wushao Wen, et al.&lt;/summary&gt;Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.05189&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Qrange-group/SUR-adapter&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Qrange-group/SUR-adapter.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Grounding Language Models to Images for Multimodal Inputs and Outputs&lt;/strong&gt; (31 Jan 2023)&lt;br&gt; [ICML 2023] Koh, Jing Yu, Ruslan Salakhutdinov, and Daniel Fried.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2301.13823&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6173520a1eb2814d067e8c5fd16212b7cbf6ee78&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-35-blue.svg?paper=6173520a1eb2814d067e8c5fd16212b7cbf6ee78&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jykoh.com/fromage&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/kohjingyu/fromage&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/kohjingyu/fromage.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[RPG-DiffusionMaster] Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs&lt;/strong&gt; (22 Jan 2024) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICML 2024] Ling Yang, Zhaochen Yu, Chenlin Meng, et al.&lt;/summary&gt;Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, Bin Cui&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.11708&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/140cfda71bfff852c3e205b7ad61854b78c76982&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-23-blue.svg?paper=140cfda71bfff852c3e205b7ad61854b78c76982&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/YangLing0818/RPG-DiffusionMaster&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/YangLing0818/RPG-DiffusionMaster.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;RealCompo: Balancing Realism and Compositionality Improves Text-to-Image Diffusion Models&lt;/strong&gt; (20 Feb 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xinchen Zhang, Ling Yang, Yaqi Cai, et al.&lt;/summary&gt;Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Kai-Ni Wang, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, Bin Cui&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.12908&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9c2ba04c376f127da506b63c566887fca2861b25&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=9c2ba04c376f127da506b63c566887fca2861b25&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cominclip.github.io/RealCompo_Page/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/YangLing0818/RealCompo&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/YangLing0818/RealCompo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Non-LLM-based (Clip/T5)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PIXART-Œ±: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis&lt;/strong&gt; (30 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Junsong Chen, Jincheng Yu, Chongjian Ge, et al.&lt;/summary&gt;Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.00426&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/7dfe1c9f1d7120102499c7e561efc2326e7a0358&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=7dfe1c9f1d7120102499c7e561efc2326e7a0358&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pixart-alpha.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PixArt-alpha/PixArt-alpha.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/PixArt-alpha/PixArt-alpha&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TextDiffuser: Diffusion Models as Text Painters&lt;/strong&gt; (18 May 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Jingye Chen, Yupan Huang, Tengchao Lv, et al.&lt;/summary&gt;Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.10855&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e779781f1bea273573fc9d3f1a5e874bcff2cd2b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-12-blue.svg?paper=e779781f1bea273573fc9d3f1a5e874bcff2cd2b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jingyechen.github.io/textdiffuser/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/unilm/tree/master/textdiffuser&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/JingyeChen22/TextDiffuser&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TiGAN: Text-Based Interactive Image Generation and Manipulation&lt;/strong&gt; (Dec 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[AAAI 2022] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, et al.&lt;/summary&gt;Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Chris Tensmeyer, Tong Yu,Changyou Chen, Jinhui Xu, Tong Sun&#xA;  &lt;/details&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/view/20270&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/839dc73c1adae268144d9cfb9d70985b2001304f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-9-blue.svg?paper=839dc73c1adae268144d9cfb9d70985b2001304f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; Tags: &lt;code&gt;iteractive&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multi-Concept Customization of Text-to-Image Diffusion&lt;/strong&gt; (8 Dec 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Nupur Kumari, Bingliang Zhang, Richard Zhang, et al.&lt;/summary&gt;Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.04488&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/144eca44e250cc462f6fc3a172abb865978f66f5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-307-blue.svg?paper=144eca44e250cc462f6fc3a172abb865978f66f5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.cs.cmu.edu/~custom-diffusion/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/adobe-research/custom-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/adobe-research/custom-diffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;br&gt; Tags: &lt;code&gt;customization&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/strong&gt; (25 Aug 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, et al.&lt;/summary&gt;Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.12242&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5b19bf6c3f4b25cac96362c98b930cf4b37f6744&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1090-blue.svg?paper=5b19bf6c3f4b25cac96362c98b930cf4b37f6744&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;br&gt; Tags: &lt;code&gt;customization&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&lt;/strong&gt; (2 Aug 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Rinon Gal, Yuval Alaluf, Yuval Atzmon, et al. &lt;/summary&gt;Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.01618&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5b19bf6c3f4b25cac96362c98b930cf4b37f6744&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1090-blue.svg?paper=5b19bf6c3f4b25cac96362c98b930cf4b37f6744&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rinongal/textual_inversion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/rinongal/textual_inversion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;br&gt; Tags: &lt;code&gt;customization&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding&lt;/strong&gt; (23 May 2022)&lt;br&gt; [NeurIPS 2022] &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Saharia, Chitwan Chan, William Saxena, Saurabh Li, Lala Whang, Jay Denton, Emily L Ghasemipour, Kamyar Gontijo Lopes, Raphael Karagol Ayan, Burcu Salimans, Tim others&lt;/summary&gt;&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9695824d7a01fad57ba9c01d7d76a519d78d65e7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2844-blue.svg?paper=9695824d7a01fad57ba9c01d7d76a519d78d65e7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imagen.research.google/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt; (20 Dec 2021)&lt;br&gt; [CVPR 2022 (Oral)] &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Rombach, Robin Blattmann, Andreas Lorenz, et al. &lt;/summary&gt;Rombach, Robin Blattmann, Andreas Lorenz, Dominik Esser, Patrick Ommer, Bj{&#34;o}rn&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c10075b3746a9f3dd5811970e93c8ca3ad39b39d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5641-blue.svg?paper=c10075b3746a9f3dd5811970e93c8ca3ad39b39d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/CompVis/stable-diffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MIMIC-IT: Multi-Modal In-Context Instruction Tuning&lt;/strong&gt; (8 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Bo Li, Yuanhan Zhang, Liangyu Chen, et al.&lt;/summary&gt;Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, Ziwei Liu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.05425&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d47524cd5c3c4b57af2e5a29f6f91c420310f236&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-78-blue.svg?paper=d47524cd5c3c4b57af2e5a29f6f91c420310f236&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Luodian/otter&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Luodian/otter.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[LAION-Glyph] GlyphControl: Glyph Conditional Control for Visual Text Generation&lt;/strong&gt; (29 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Yukang Yang, Dongnan Gui, Yuhui Yuan, et al.&lt;/summary&gt;Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han Hu, Kai Chen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.18259&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5fbe4c92791fbecb179c1ab79bba9a59b2e155ba&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-12-blue.svg?paper=5fbe4c92791fbecb179c1ab79bba9a59b2e155ba&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AIGText/GlyphControl-release&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AIGText/GlyphControl-release.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[MARIO-10M] TextDiffuser: Diffusion Models as Text Painters&lt;/strong&gt; (18 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Jingye Chen, Yupan Huang, Tengchao Lv, et al.&lt;/summary&gt;Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.14108&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e779781f1bea273573fc9d3f1a5e874bcff2cd2b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-12-blue.svg?paper=e779781f1bea273573fc9d3f1a5e874bcff2cd2b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jingyechen.github.io/textdiffuser/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/unilm&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DataComp: In search of the next generation of multimodal datasets&lt;/strong&gt; (27 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, et al.&lt;/summary&gt;Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.14108&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f9570989919338079088270a9cf1a7afc8db8093&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-103-blue.svg?paper=f9570989919338079088270a9cf1a7afc8db8093&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.datacomp.ai/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mlfoundations/datacomp&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mlfoundations/datacomp.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[LLava-instruct] Visual Instruction Tuning&lt;/strong&gt; (17 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Haotian Liu, Chunyuan Li, Qingyang Wu, et al.&lt;/summary&gt;Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/a5036f31f0e629dc661f120b8c3b1f374d479ab8&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-820-blue.svg?paper=a5036f31f0e629dc661f120b8c3b1f374d479ab8&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llava-vl.github.io//&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text&lt;/strong&gt; (14 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Wanrong Zhu, Jack Hessel, Anas Awadalla, et al.&lt;/summary&gt;Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, Yejin Choi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.06939&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/df958800014d310b6df34ad83d771314d68fbb2d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-64-blue.svg?paper=df958800014d310b6df34ad83d771314d68fbb2d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/mmc4&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/allenai/mmc4.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Language Is Not All You Need: Aligning Perception with Language Models&lt;/strong&gt; (27 Feb 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Shaohan Huang, Li Dong, Wenhui Wang, et al.&lt;/summary&gt;Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, Furu Wei&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.14045&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-261-blue.svg?paper=fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;COYO-700M: Image-Text Pair Dataset&lt;/strong&gt; (31 Aug 2022)&lt;br&gt; &lt;a href=&#34;https://github.com/kakaobrain/coyo-dataset&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/kakaobrain/coyo-dataset.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LAION-5B: An open large-scale dataset for training next generation image-text models&lt;/strong&gt; (16 Oct 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2022] Christoph Schuhmann, Romain Beaumont, Richard Vencu, et al. &lt;/summary&gt;Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, Jenia Jitsev&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.08402&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1288-blue.svg?paper=e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LAION COCO: 600M SYNTHETIC CAPTIONS FROM LAION2B-EN&lt;/strong&gt; (15 Sep 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Christoph Schuhmann, Andreas K√∂pf , Theo Coombes, et al.&lt;/summary&gt;Christoph Schuhmann, Andreas K√∂pf , Theo Coombes, Richard Vencu, Benjamin Trom , Romain Beaumont&#xA;  &lt;/details&gt; &lt;a href=&#34;https://laion.ai/blog/laion-coco/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[M3W] Flamingo: a Visual Language Model for Few-Shot Learning&lt;/strong&gt; (29 Apr 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2022] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, et al.&lt;/summary&gt;Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.14198&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/26218bdcc3945c7edae7aa2adbfba4cd820a2df3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1483-blue.svg?paper=26218bdcc3945c7edae7aa2adbfba4cd820a2df3&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[LAION-FACE]General Facial Representation Learning in a Visual-Linguistic Manner&lt;/strong&gt; (6 Dec 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2021] Yinglin Zheng, Hao Yang, Ting Zhang, et al.&lt;/summary&gt;Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, Fang Wen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.03109&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/037bab9d26ef7da11ee32d7682836604d2cc8a72&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-70-blue.svg?paper=037bab9d26ef7da11ee32d7682836604d2cc8a72&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/FacePerceiver/FaRL&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/FacePerceiver/FaRL.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[LAION-400M] Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs&lt;/strong&gt; (3 Nov 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2021] Christoph Schuhmann, Richard Vencu, Romain Beaumont, et al. &lt;/summary&gt;Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.02114&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-756-blue.svg?paper=b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://laion.ai/laion-400-open-dataset/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning&lt;/strong&gt; (2 Mar 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[SIGIR 2021] Krishna Srinivasan, Karthik Raman, Jiecao Chen, et al.&lt;/summary&gt;Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, Marc Najork&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.01913&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/98e565fa06f6c7bf7c46833b5106b26dc45130c4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-186-blue.svg?paper=98e565fa06f6c7bf7c46833b5106b26dc45130c4&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research-datasets/wit&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts&lt;/strong&gt; (17 Feb 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2021] Soravit Changpinyo, Piyush Sharma, Nan Ding, et al.&lt;/summary&gt;Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2102.08981&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/394be105b87e9bfe72c20efe6338de10604e1a11&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-617-blue.svg?paper=394be105b87e9bfe72c20efe6338de10604e1a11&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research-datasets/conceptual-12m&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[ALIGN] Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision&lt;/strong&gt; (11 Feb 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICML 2021] Chao Jia, Yinfei Yang, Ye Xia, et al. &lt;/summary&gt;Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2102.05918&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/141a5033d9994242b18bb3b217e79582f1ee9306&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2120-blue.svg?paper=141a5033d9994242b18bb3b217e79582f1ee9306&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[MS COCO] Microsoft COCO: Common Objects in Context&lt;/strong&gt; (1 May 2014)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ECCV 2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, et al. &lt;/summary&gt;Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, Piotr Doll√°r&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1405.0312&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-33630-blue.svg?paper=71b7178df5d2b112d07e45038cb5637208659ff7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cocodataset-org.translate.goog/?_x_tr_sl=en&amp;amp;_x_tr_tl=zh-CN&amp;amp;_x_tr_hl=zh-CN&amp;amp;_x_tr_pto=sc#home&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[Im2Text] Describing Images Using 1 Million Captioned Photographs&lt;/strong&gt; (12 Dec 2011)&lt;br&gt; [NeurIPS 2011] Vicente Ordonez, Girish Kulkarni, Tamara Berg&lt;br&gt; &lt;a href=&#34;https://papers.nips.cc/paper_files/paper/2011/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8e080b98efbe65c02a116439205ca2344b9f7cd4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1192-blue.svg?paper=8e080b98efbe65c02a116439205ca2344b9f7cd4&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Video Generation&lt;/h2&gt; &#xA;&lt;h3&gt;üîÖ LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation&lt;/strong&gt; (11 Mar 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, et al.&lt;/summary&gt;Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, Xingang Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.06845&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/DriveDreamer-2%3A-LLM-Enhanced-World-Models-for-Video-Zhao-Wang/b34fb645165da381e27077282d69ff224dd2d5f5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=b34fb645165da381e27077282d69ff224dd2d5f5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://drivedreamer2.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[Sora] Video generation models as world simulators&lt;/strong&gt; (15 Feb 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Tim Brooks, Bill Peebles, Connor Holmes, et al.&lt;/summary&gt;Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Ng and Ricky Wang and Aditya Ramesh&#xA;  &lt;/details&gt; &lt;a href=&#34;https://openai.com/research/video-generation-models-as-world-simulators&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[LGVI] Towards Language-Driven Video Inpainting via Multimodal Large Language Models&lt;/strong&gt; (18 Jan 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jianzong Wu, Xiangtai Li, Chenyang Si, et al.&lt;/summary&gt;Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.10226&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/Towards-Language-Driven-Video-Inpainting-via-Large-Wu-Li/02d96eb0da4a282831f14923d1a65976952b7177&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-3-blue.svg?paper=02d96eb0da4a282831f14923d1a65976952b7177&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jianzongwu.github.io/projects/rovi/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization: Content-Consistent Multi-Scene Video Generation with LLM&lt;/strong&gt; (2 Jan 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yang Jin, Zhicheng Sun, Kun Xu, et al.&lt;/summary&gt;Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.03161&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c1b5195bc09a2232ec2b69e5a2a6bd39b3162c62&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=c1b5195bc09a2232ec2b69e5a2a6bd39b3162c62&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://video-lavit.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM&lt;/strong&gt; (2 Jan 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Fuchen Long, Zhaofan Qiu, Ting Yao, et al.&lt;/summary&gt;Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.01256&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/fc84fcf269a37ed7ddcb1b0f2d7d1a00f677eaea&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=fc84fcf269a37ed7ddcb1b0f2d7d1a00f677eaea&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://videodrafter.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[PRO-Motion] Plan, Posture and Go: Towards Open-World Text-to-Motion Generation&lt;/strong&gt; (22 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jinpeng Liu, Wenxun Dai, Chunyu Wang, et al.&lt;/summary&gt;Jinpeng Liu, Wenxun Dai, Chunyu Wang, Yiji Cheng, Yansong Tang, Xin Tong&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.14828&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4599d5af850da482f591a02a3b17d56e0d358771&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=4599d5af850da482f591a02a3b17d56e0d358771&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://moonsliu.github.io/Pro-Motion/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VideoPoet: A Large Language Model for Zero-Shot Video Generation&lt;/strong&gt; (21 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Dan Kondratyuk, Lijun Yu, Xiuye Gu, et al.&lt;/summary&gt;Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos√© Lezama, Jonathan Huang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair Alon, Vighnesh Birodkar, Yong Cheng, Ming-Chang Chiu, Josh Dillon, Irfan Essa, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, David Ross, Grant Schindler, Mikhail Sirotenko, Kihyuk Sohn, Krishna Somandepalli, Huisheng Wang, Jimmy Yan, Ming-Hsuan Yang, Xuan Yang, Bryan Seybold, Lu Jiang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.14125&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/0c4f46e4dcae5527018e6432fb60cfe8c3354e97&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-18-blue.svg?paper=0c4f46e4dcae5527018e6432fb60cfe8c3354e97&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sites.research.google/videopoet/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax&lt;/strong&gt; (27 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[arXiv 2023] Yu Lu, Linchao Zhu, Hehe Fan, et al.&lt;/summary&gt;Yu Lu, Linchao Zhu, Hehe Fan, Yi Yang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.15813&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8feb33300c04fffa050e0dca59c3fdcafc920a3b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=9cdb7e415a96795dc6705e66f3b798238b4dec2c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InterControl: Generate Human Motion Interactions by Controlling Every Joint&lt;/strong&gt; (27 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zhenzhi Wang, Jingbo Wang, Dahua Lin, et al.&lt;/summary&gt;Zhenzhi Wang, Jingbo Wang, Dahua Lin, Bo Dai&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.15864&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9cdb7e415a96795dc6705e66f3b798238b4dec2c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=9cdb7e415a96795dc6705e66f3b798238b4dec2c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zhenzhiwang/intercontrol&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zhenzhiwang/intercontrol.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;br&gt; Tags: &lt;code&gt;human motion generation&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MotionLLM: Multimodal Motion-Language Learning with Large Language Models&lt;/strong&gt; (27 May 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Qi Wu, Yubo Zhao, Yifan Wang, et al.&lt;/summary&gt;Qi Wu, Yubo Zhao, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2405.17013&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/480da1ac2d39b5e036ce786af081366c23f08d1b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=480da1ac2d39b5e036ce786af081366c23f08d1b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://knoxzhao.github.io/MotionLLM/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;br&gt; Tags: &lt;code&gt;general human motion generation&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning&lt;/strong&gt; (21 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiaxi Lv, Yi Huang, Mingfu Yan, et al.&lt;/summary&gt;Jiaxi Lv, Yi Huang, Mingfu Yan, Jiancheng Huang, Jianzhuang Liu, Yifan Liu, Yafei Wen, Xiaoxin Chen, Shifeng Chen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.12631&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9cdb7e415a96795dc6705e66f3b798238b4dec2c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=9cdb7e415a96795dc6705e66f3b798238b4dec2c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gpt4motion.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[MAGVIT-v2] Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation&lt;/strong&gt; (9 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Lijun Yu, Jos√© Lezama, Nitesh B. Gundavarapu, et al.&lt;/summary&gt;Lijun Yu, Jos√© Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.05737&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/985f0c89c5a607742ec43c1fdc2cbfe54541cbad&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-15-blue.svg?paper=985f0c89c5a607742ec43c1fdc2cbfe54541cbad&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[LVD] LLM-grounded Video Diffusion Models&lt;/strong&gt; (29 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Long Lian, Baifeng Shi, Adam Yala, et al.&lt;/summary&gt;Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, Boyi Li&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.17444&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/87bf66eb6d22df17f70170a0e575b4f12c4813ef&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-10-blue.svg?paper=87bf66eb6d22df17f70170a0e575b4f12c4813ef&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llm-grounded-video-diffusion.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TonyLianLong/LLM-groundedVideoDiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/TonyLianLong/LLM-groundedVideoDiffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning&lt;/strong&gt; (26 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[arXiv 2023] Han Lin, Abhay Zala, Jaemin Cho, et al.&lt;/summary&gt;Han Lin, Abhay Zala, Jaemin Cho, Mohit Bansal&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.15091&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/16753e0317730e8c1b297338300a8c6163dd06f2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-12-blue.svg?paper=16753e0317730e8c1b297338300a8c6163dd06f2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://videodirectorgpt.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HL-hanlin/VideoDirectorGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/HL-hanlin/VideoDirectorGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator&lt;/strong&gt; (25 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NIPS 2023] Hanzhuo Huang, Yufan Feng, Cheng Shi, et al.&lt;/summary&gt;Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, Sibei Yang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.14494&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/120aca3e415b6641a0b0cd20695ab85ed7789612&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-24-blue.svg?paper=120aca3e415b6641a0b0cd20695ab85ed7789612&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/SooLab/Free-Bloom&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/SooLab/Free-Bloom.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[Dysen-VDM] Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models&lt;/strong&gt; (26 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Hao Fei, Shengqiong Wu, Wei Ji, et al.&lt;/summary&gt;Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Tat-Seng Chua&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.13812&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d0a7f7fe31e0e0c42b471b4c47a313bd8c8e5206&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=d0a7f7fe31e0e0c42b471b4c47a313bd8c8e5206&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://haofei.vip/Dysen-VDM/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/scofield7419/Dysen&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/scofield7419/Dysen.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[DirecT2V] Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation&lt;/strong&gt; (23 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[arXiv 2023] Susung Hong, Junyoung Seo, Sunghwan Hong, et al.&lt;/summary&gt;Susung Hong, Junyoung Seo, Sunghwan Hong, Heeseong Shin, Seungryong Kim&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.14330&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/b1750d2a6e3480e690999916a86c8b3876577b39&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-15-blue.svg?paper=b1750d2a6e3480e690999916a86c8b3876577b39&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KU-CVLAB/DirecT2V&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/KU-CVLAB/DirecT2V.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text2Motion: From Natural Language Instructions to Feasible Plans&lt;/strong&gt; (21 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[Autonomous Robots 2023] Kevin Lin, Christopher Agia, Toki Migimatsu, et al.&lt;/summary&gt;Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.12153&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8f2d4758e6d525509ae36bb30224dc9259027e6b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-110-blue.svg?paper=8f2d4758e6d525509ae36bb30224dc9259027e6b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sites.google.com/stanford.edu/text2motion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KU-CVLAB/DirecT2V&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/KU-CVLAB/DirecT2V.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Non-LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text&lt;/strong&gt; (21 Mar 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, et al.&lt;/summary&gt;Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, Humphrey Shi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.14773&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/StreamingT2V%3A-Consistent%2C-Dynamic%2C-and-Extendable-Henschel-Khachatryan/21a77ed349c8621d0a0ef8407eb744e3de3b13c5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=21a77ed349c8621d0a0ef8407eb744e3de3b13c5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Picsart-AI-Research/StreamingT2V&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Vchitect/VBench.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VBench: Comprehensive Benchmark Suite for Video Generative Models&lt;/strong&gt; (29 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Ziqi Huang, Yinan He, Jiashuo Yu, et al.&lt;/summary&gt;Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, Ziwei Liu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.17982&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4e9a8141da2a8c603722b07d096109207f8e0b66&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-7-blue.svg?paper=4e9a8141da2a8c603722b07d096109207f8e0b66&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://vchitect.github.io/VBench-project/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Vchitect/VBench&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Vchitect/VBench.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Vchitect/VBench_Leaderboard&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets&lt;/strong&gt; (25 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Andreas Blattmann, Tim Dockhorn, Sumith Kulal, et al.&lt;/summary&gt;Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, Robin Rombach&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.15127&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1206b05eae5a06ba662ae79fb291b50e359c4f42&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-51-blue.svg?paper=1206b05eae5a06ba662ae79fb291b50e359c4f42&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Stability-AI/generative-models.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VideoCrafter1: Open Diffusion Models for High-Quality Video Generation&lt;/strong&gt; (30 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Haoxin Chen, Menghan Xia, Yingqing He, et al.&lt;/summary&gt;Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, Ying Shan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.19512&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1891c3756f870d902a0b793a1dcd5cc34c778612&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-38-blue.svg?paper=1891c3756f870d902a0b793a1dcd5cc34c778612&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ailab-cvc.github.io/videocrafter/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AILab-CVC/VideoCrafter&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AILab-CVC/VideoCrafter.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/VideoCrafter/VideoCrafter&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling&lt;/strong&gt; (23 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Haonan Qiu, Menghan Xia, Yong Zhang, et al.&lt;/summary&gt;Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, Ziwei Liu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.15169&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d831988859f0c077b38094446d8585a8340af223&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-7-blue.svg?paper=d831988859f0c077b38094446d8585a8340af223&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://haonanqiu.com/projects/FreeNoise.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/arthur-qiu/LongerCrafter&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/arthur-qiu/LongerCrafter.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/MoonQiu/LongerCrafter&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation&lt;/strong&gt; (13 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yingqing He, Menghan Xia, Haoxin Chen, et al.&lt;/summary&gt;Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, Qifeng Chen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.06940&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/77040969110fab39a55699cb06f9edf68789445a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-18-blue.svg?paper=77040969110fab39a55699cb06f9edf68789445a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ailab-cvc.github.io/Animate-A-Story/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AILab-CVC/Animate-A-Story&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AILab-CVC/Animate-A-Story.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance&lt;/strong&gt; (1 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jinbo Xing, Menghan Xia, Yuxin Liu, et al.&lt;/summary&gt;Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, Ying Shan, Tien-Tsin Wong&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.00943&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/52b10ae66d025e99fbb602935e155f97f4f0696f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-31-blue.svg?paper=52b10ae66d025e99fbb602935e155f97f4f0696f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doubiiu.github.io/projects/Make-Your-Video/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AILab-CVC/Make-Your-Video&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AILab-CVC/Make-Your-Video.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos&lt;/strong&gt; (3 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yue Ma, Yingqing He, Xiaodong Cun, et al.&lt;/summary&gt;Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.01186&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ee73edebd42626d9c2d91e35fd2ed3cdb0fb26d0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-53-blue.svg?paper=ee73edebd42626d9c2d91e35fd2ed3cdb0fb26d0&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://follow-your-pose.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mayuelala/FollowYourPose&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mayuelala/FollowYourPose.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/YueMafighting/FollowYourPose&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation&lt;/strong&gt; (15 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zhengxiong Luo, Dayou Chen, Yingya Zhang, et al.&lt;/summary&gt;Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, Tieniu Tan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.08320&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/26c6090b7e7ba4513f82aa28d41360c60770c618&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-108-blue.svg?paper=26c6090b7e7ba4513f82aa28d41360c60770c618&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation&lt;/strong&gt; (13 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024 Spotlight] Yi Wang, Yinan He, Yizhuo Li, et al.&lt;/summary&gt;Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.06942&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/369b449415d50387fba048bbd4d26ee890df84b5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-36-blue.svg?paper=369b449415d50387fba048bbd4d26ee890df84b5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/InternVideo&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/InternVideo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/OpenGVLab/InternVid&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[HD-VG-130M] VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation&lt;/strong&gt; (18 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Wenjing Wang, Huan Yang, Zixi Tuo, et al.&lt;/summary&gt;Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, Jiaying Liu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.10874&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/50bbf2c11984d18aa14f964a4909ac25f07e50ea&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-43-blue.svg?paper=50bbf2c11984d18aa14f964a4909ac25f07e50ea&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/daooshee/HD-VG-130M?tab=readme-ov-file&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/InternVideo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[VideoCC3M] Learning Audio-Video Modalities from Image Captions&lt;/strong&gt; (18 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ECCV 2022] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, et al.&lt;/summary&gt;Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja Hauth, Santiago Manen, Chen Sun, Cordelia Schmid&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2204.00679&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/aa1b722485106c84e52c5e35b2d4b2f8c7fb3135&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-48-blue.svg?paper=aa1b722485106c84e52c5e35b2d4b2f8c7fb3135&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research-datasets/videoCC-data&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/InternVideo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CelebV-Text: A Large-Scale Facial Text-Video Dataset&lt;/strong&gt; (26 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Jianhui Yu, Hao Zhu, Liming Jiang, et al.&lt;/summary&gt;Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, Wayne Wu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.14717&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/484d2194ce8459bfa9da906e556f63812c6ca999&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-11-blue.svg?paper=484d2194ce8459bfa9da906e556f63812c6ca999&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://celebv-text.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/CelebV-Text/CelebV-Text&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/CelebV-Text/CelebV-Text.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=0TS1hQwjNWw&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[HD-VILA-100M] Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions&lt;/strong&gt; (19 Nov 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2022] Hongwei Xue, Tiankai Hang, Yanhong Zeng, et al. &lt;/summary&gt;Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, Baining Guo&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2111.10337&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e1a3e6856b6ac6af3600b5954392e5368603fd1b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-84-blue.svg?paper=e1a3e6856b6ac6af3600b5954392e5368603fd1b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/XPretrain/raw/main/hd-vila-100m/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/XPretrain.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[YT-Temporal-180M] MERLOT: Multimodal Neural Script Knowledge Models&lt;/strong&gt; (4 Jun 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2021] Rowan Zellers, Ximing Lu, Jack Hessel, et al. &lt;/summary&gt;Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, Yejin Choi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2106.02636&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/90357a6dc817e2f7cec477a51156675fbf545cf1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-277-blue.svg?paper=90357a6dc817e2f7cec477a51156675fbf545cf1&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rowanz/merlot&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/rowanz/merlot.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[WebVid-10M] Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval&lt;/strong&gt; (1 Apr 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2021] Max Bain, Arsha Nagrani, G√ºl Varol, et al. &lt;/summary&gt;Max Bain, Arsha Nagrani, G√ºl Varol, Andrew Zisserman&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.00650&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/bac87bdb1cabc35fafb8176a234d332ebcc02864&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-585-blue.svg?paper=bac87bdb1cabc35fafb8176a234d332ebcc02864&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[WTS70M] Learning Video Representations from Textual Web Supervision&lt;/strong&gt; (29 Jul 2020)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jonathan C. Stroud, Zhichao Lu, Chen Sun, et al.&lt;/summary&gt;Jonathan C. Stroud, Zhichao Lu, Chen Sun, Jia Deng, Rahul Sukthankar, Cordelia Schmid, David A. Ross&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2007.14937&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/da55208bc9b56b5f394c242239d8cd0734bd5a87&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-42-blue.svg?paper=da55208bc9b56b5f394c242239d8cd0734bd5a87&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips&lt;/strong&gt; (7 Jun 2019)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2019] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, et al. &lt;/summary&gt;Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, Josef Sivic&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1906.03327&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9311779489e597315488749ee6c386bfa3f3512e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-847-blue.svg?paper=9311779489e597315488749ee6c386bfa3f3512e&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.di.ens.fr/willow/research/howto100m/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/antoine77340/howto100m?tab=readme-ov-file&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/antoine77340/howto100m.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research&lt;/strong&gt; (6 Apr 2019)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2019 Oral] Xin Wang, Jiawei Wu, Junkun Chen, et al. &lt;/summary&gt;Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1904.03493&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/28b74bb7c8b08cceb2430ec2d54dfa0f3225d796&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-373-blue.svg?paper=28b74bb7c8b08cceb2430ec2d54dfa0f3225d796&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;How2: A Large-scale Dataset for Multimodal Language Understanding&lt;/strong&gt; (7 Jun 2019)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2018] Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, et al. &lt;/summary&gt;Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Lo√Øc Barrault, Lucia Specia, Florian Metze&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1811.00347&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f56cb5dc32b5b280546998418fda7769d0858629&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-223-blue.svg?paper=f56cb5dc32b5b280546998418fda7769d0858629&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://srvk.github.io/how2-dataset/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/srvk/how2-dataset&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/srvk/how2-dataset.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[ActivityNet Captions] Dense-Captioning Events in Videos&lt;/strong&gt; (2 May 2017)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2017] Ranjay Krishna, Kenji Hata, Frederic Ren, et al. &lt;/summary&gt;Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos Niebles&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1705.00754&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-934-blue.svg?paper=96dd1fc39a368d23291816d57763bc6eb4f7b8d6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cs.stanford.edu/people/ranjaykrishna/densevid/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[LSMDC] Movie Description&lt;/strong&gt; (12 May 2016)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[IJCV 2017] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, et al. &lt;/summary&gt;Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo Larochelle, Aaron Courville, Bernt Schiele&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1605.03705&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-278-blue.svg?paper=154c22ca5eef149aedc8a986fa684ca1fd14e7dc&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/vision-and-language/mpii-movie-description-dataset&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MSR-VTT: A Large Video Description Dataset for Bridging Video and Language&lt;/strong&gt; (1 Apr 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2016] Jun Xu , Tao Mei , Ting Yao, et al. &lt;/summary&gt;Jun Xu , Tao Mei , Ting Yao and Yong Rui&#xA;  &lt;/details&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1395-blue.svg?paper=b8e2e9f3ba008e28257195ec69a00e07f260131d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/crux82/msr-vtt-it&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/crux82/msr-vtt-it.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3D Generation&lt;/h2&gt; &#xA;&lt;h3&gt;üîÖ LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code&lt;/strong&gt; (2 Mar 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Ziniu Hu, Ahmet Iscen, Aashi Jain, et al. &lt;/summary&gt;Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.01248v1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MotionScript: Natural Language Descriptions for Expressive 3D Human Motions&lt;/strong&gt; (19 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Payam Jome Yazdian, Eric Liu, Li Cheng, et al. &lt;/summary&gt;Payam Jome Yazdian, Eric Liu, Li Cheng, Angelica Lim&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.12634&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/816792e66f463be2aa1888e4ecb51f8fb2b4dd79&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=816792e66f463be2aa1888e4ecb51f8fb2b4dd79&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HOLODECK: Language Guided Generation of 3D Embodied AI Environments&lt;/strong&gt; (19 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024]Yue Yang, Fan-Yun Sun, Luca Weihs, et al. &lt;/summary&gt;Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, Christopher Clark&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.09067&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1dbc2cdcae3e17c3d721d12a5a2d98ced727681a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=1dbc2cdcae3e17c3d721d12a5a2d98ced727681a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/Holodeck&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/allenai/Holodeck.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PoseGPT: Chatting about 3D Human Pose&lt;/strong&gt; (30 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yao Feng, Jing Lin, Sai Kumar Dwivedi, et al. &lt;/summary&gt;[CVPR 2024] Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Michael J. Black&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.18836&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4673c2ac4abb4b055da87171231acb60801ffe74&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=4673c2ac4abb4b055da87171231acb60801ffe74&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yfeng95/PoseGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yfeng95/PoseGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;3D-GPT: Procedural 3D MODELING WITH LARGE LANGUAGE MODELS&lt;/strong&gt; (19 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Chunyi Sun*, Junlin Han*, Weijian Deng, et al. &lt;/summary&gt;Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, Stephen Gould&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.12945&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/588930cdd801f335b5e524d13f99aa94136a20a0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-7-blue.svg?paper=588930cdd801f335b5e524d13f99aa94136a20a0&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Chuny1/3DGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Chuny1/3DGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Non-LLM-based (Clip/T5)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric Diffusion&lt;/strong&gt; (12 Mar 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yuanze Lin, Ronald Clark, Philip Torr. &lt;/summary&gt;Yuanze Lin, Ronald Clark, Philip Torr&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.17237&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/72e54db6eebb99d4039cd66cb5dad6a40b31cf87&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=72e54db6eebb99d4039cd66cb5dad6a40b31cf87&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yuanze-lin/DreamPolisher&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yuanze-lin/DreamPolisher.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior&lt;/strong&gt; (12 Mar 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zike Wu, Pan Zhou, Xuanyu Yi, et al. &lt;/summary&gt;[CVPR 2024]Zike Wu, Pan Zhou, Xuanyu Yi, Xiaoding Yuan, Hanwang Zhang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.09050&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/834f595fb25b8306b46f9e744ef8150f4971322f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=834f595fb25b8306b46f9e744ef8150f4971322f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/sail-sg/Consistent3D&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/sail-sg/Consistent3D.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AToM: Amortized Text-to-Mesh using 2D Diffusion&lt;/strong&gt; (1 Feb 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Guocheng Qian, Junli Cao, Aliaksandr Siarohin, et al. &lt;/summary&gt;Guocheng Qian, Junli Cao, Aliaksandr Siarohin, Yash Kant, Chaoyang Wang, Michael Vasilkovsky, Hsin-Ying Lee, Yuwei Fang, Ivan Skorokhodov, Peiye Zhuang, Igor Gilitschenski, Jian Ren, Bernard Ghanem, Kfir Aberman, Sergey Tulyakov&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.00867&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ce2edcc6a0ef7592c385c5d8fd0924f79707e223&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=ce2edcc6a0ef7592c385c5d8fd0924f79707e223&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/snap-research/AToM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/snap-research/AToM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior&lt;/strong&gt; ( 12 Mar 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Tianyu Huang, Yihan Zeng, Zhilu Zhang, et al. &lt;/summary&gt;[CVPR 2024]Tianyu Huang, Yihan Zeng, Zhilu Zhang, Wan Xu, Hang Xu, Songcen Xu, Rynson W. H. Lau, Wangmeng Zuo&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.06439&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c15380dcda5a010827e3b014dcebe95b1218c680&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=c15380dcda5a010827e3b014dcebe95b1218c680&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tyhuang0428/DreamControl&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/tyhuang0428/DreamControl.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation&lt;/strong&gt; (14 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zexiang Liu, Yangguang Li, Youtian Lin, et al. &lt;/summary&gt;Zexiang Liu, Yangguang Li, Youtian Lin, Xin Yu, Sida Peng, Yan-Pei Cao, Xiaojuan Qi, Xiaoshui Huang, Ding Liang, Wanli Ouyang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.08754&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/210e63599d49abdb848a4440d4244cdcdedeadff&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=210e63599d49abdb848a4440d4244cdcdedeadff&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/YG256Li/UniDream&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/YG256Li/UniDream.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior&lt;/strong&gt; (11 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Fangfu Liu, Diankun Wu, Yi Wei, et al. &lt;/summary&gt;Fangfu Liu, Diankun Wu, Yi Wei, Yongming Rao, Yueqi Duan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.06655&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/fe1b3f0d074974ce946f10f3bbf52e8351bc0156&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=fe1b3f0d074974ce946f10f3bbf52e8351bc0156&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/liuff19/Sherpa3D&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/liuff19/Sherpa3D.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting&lt;/strong&gt; (8 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xiaofeng Yang, Yiwen Chen, Cheng Chen, et al. &lt;/summary&gt;Xiaofeng Yang, Yiwen Chen, Cheng Chen, Chi Zhang, Yi Xu, Xulei Yang, Fayao Liu, Guosheng Lin&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.04820&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6d10f9b0e0a579a1359df7dfbdef00bc798d5714&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-3-blue.svg?paper=6d10f9b0e0a579a1359df7dfbdef00bc798d5714&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yangxiaofeng/LODS&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yangxiaofeng/LODS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling&lt;/strong&gt; (28 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Linqi Zhou, Andy Shih, Chenlin Meng, et al. &lt;/summary&gt;Linqi Zhou, Andy Shih, Chenlin Meng, Stefano Ermon&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.16918&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e88d5399956c9d9519a5cfd49308b7d439167543&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-3-blue.svg?paper=e88d5399956c9d9519a5cfd49308b7d439167543&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/alexzhou907/DreamPropeller&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/alexzhou907/DreamPropeller.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D&lt;/strong&gt; (28 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Lingteng Qiu, Guanying Chen, Xiaodong Gu, et al. &lt;/summary&gt;Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, Xiaoguang Han&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.17082&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/cf60a639add75cdea4273697269ee463024b7926&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-10-blue.svg?paper=cf60a639add75cdea4273697269ee463024b7926&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/richdreamer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/modelscope/richdreamer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models&lt;/strong&gt; (30 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Yukang Cao, Yan-Pei Cao, Kai Han, et al. &lt;/summary&gt;Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.00916&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/0fa1501c7378a0dca2ac913fce9dcdcc2b1958a7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-69-blue.svg?paper=0fa1501c7378a0dca2ac913fce9dcdcc2b1958a7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yukangcao/DreamAvatar&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yukangcao/DreamAvatar.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching&lt;/strong&gt; (2 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Yixun Liang, Xin Yang, Jiantao Lin, et al. &lt;/summary&gt;Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, Yingcong Chen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.11284&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6f709278506813d04a074e6fa20188cce9bb927b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-12-blue.svg?paper=6f709278506813d04a074e6fa20188cce9bb927b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/EnVision-Research/LucidDreamer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/EnVision-Research/LucidDreamer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models&lt;/strong&gt; (12 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Taoran Yi, Jiemin Fang, Junjie Wang, et al. &lt;/summary&gt;Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.08529&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c5e9fd131cde68c218d0ea69cd617a67c7f35d42&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-230-blue.svg?paper=c5e9fd131cde68c218d0ea69cd617a67c7f35d42&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hustvl/GaussianDreamer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hustvl/GaussianDreamer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text-to-3D using Gaussian Splatting&lt;/strong&gt; (28 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Zilong Chen, Feng Wang, Huaping Liu &lt;/summary&gt;Zilong Chen, Feng Wang, Huaping Liu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.16585&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/86b5318b0a69ccdeec17abb0120e4bd7688a4b59&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-57-blue.svg?paper=86b5318b0a69ccdeec17abb0120e4bd7688a4b59&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gsgen3d/gsgen&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/gsgen3d/gsgen.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior&lt;/strong&gt; (10 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Zhipeng Hu, Minda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng Li, Zeng Zhao, Changjie Fan, Xiaowei Zhou, Xin Yu&lt;/summary&gt;&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.13223&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/fec17239569efd6914f0df9e25b66b310969d3c5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-23-blue.svg?paper=fec17239569efd6914f0df9e25b66b310969d3c5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TADA! Text to Animatable Digital Avatars&lt;/strong&gt; (21 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[3DV 2024] Tingting Liao, Hongwei Yi, Yuliang Xiu, et al.&lt;/summary&gt;Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus Thies, Michael J. Black&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.10899&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/303f466fb823112f79a9f36637c7084dd8363fc5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-36-blue.svg?paper=303f466fb823112f79a9f36637c7084dd8363fc5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TingtingLiao/TADA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/TingtingLiao/TADA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D&lt;/strong&gt; (20 Oct 2023 )&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Weiyu Li, Rui Chen, Xuelin Chen, et al.&lt;/summary&gt;Weiyu Li, Rui Chen, Xuelin Chen, Ping Tan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.02596&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/438e9fb79c9e37d43223e61bb575ebd2dae0b0a7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-29-blue.svg?paper=438e9fb79c9e37d43223e61bb575ebd2dae0b0a7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wyysf-98/SweetDreamer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/wyysf-98/SweetDreamer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Noise-Free Score Distillation&lt;/strong&gt; (26 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Oren Katzir, Or Patashnik, Daniel Cohen-Or, et al.&lt;/summary&gt;Oren Katzir, Or Patashnik, Daniel Cohen-Or, Dani Lischinski&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.17590&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/85a70c0a048cba4f53dcf332ee73f6032a2e53bc&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-14-blue.svg?paper=85a70c0a048cba4f53dcf332ee73f6032a2e53bc&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/orenkatzir/nfsd&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/orenkatzir/nfsd.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text-to-3D with Classifier Score Distillation&lt;/strong&gt; (26 Oct 2023 )&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Xin Yu, Yuan-Chen Guo, Yangguang Li, et al. &lt;/summary&gt;Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, Xiaojuan Qi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.19415&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4e21879b564cc2e803b16edf0dda9f1edb91b497&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-13-blue.svg?paper=4e21879b564cc2e803b16edf0dda9f1edb91b497&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/CVMI-Lab/Classifier-Score-Distillation&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/CVMI-Lab/Classifier-Score-Distillation.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HiFA: High-fidelity Text-to-3D Generation with Advanced Diffusion Guidance&lt;/strong&gt; (28 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Junzhe Zhu, Peiye Zhuang. &lt;/summary&gt;Junzhe Zhu, Peiye Zhuang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.18766&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/daf3b117f789b2b95223e58592979fb57627515e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-17-blue.svg?paper=daf3b117f789b2b95223e58592979fb57627515e&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JunzheJosephZhu/HiFA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/JunzheJosephZhu/HiFA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MVDream: Multi-view Diffusion for 3D Generation&lt;/strong&gt; (31 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Yichun Shi, Peng Wang, Jianglong Ye, et al. &lt;/summary&gt;Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, Xiao Yang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.16512&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9aa01997226b5c4d705ae2e2f52c32681006654b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-154-blue.svg?paper=9aa01997226b5c4d705ae2e2f52c32681006654b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/bytedance/MVDream&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/bytedance/MVDream.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation&lt;/strong&gt; (28 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Jiaxiang Tang, Jiawei Ren, Hang Zhou, et al.&lt;/summary&gt;Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, Gang Zeng&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.16653&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/cc1a674bb164d09a060cf5b26fe518c02fae0ddc&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-99-blue.svg?paper=cc1a674bb164d09a060cf5b26fe518c02fae0ddc&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dreamgaussian/dreamgaussian&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dreamgaussian/dreamgaussian.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation&lt;/strong&gt; (11 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, et al.&lt;/summary&gt;Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Hyeonsu Kim, Jaehoon Ko, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.07937&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5356c3dac654854a0842753bcc2e3433dc4a2afd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-81-blue.svg?paper=5356c3dac654854a0842753bcc2e3433dc4a2afd&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/eladrich/latent-nerf&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/eladrich/latent-nerf.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;IT3D: Improved Text-to-3D Generation with Explicit View Synthesis&lt;/strong&gt; (22 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[AAAI 2024] Yiwen Chen, Chi Zhang, Xiaofeng Yang, et al. &lt;/summary&gt;Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, Guosheng Lin&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.11473&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/2b94785cbfd865a01cc68d7d4c7500b710e5e2fb&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-30-blue.svg?paper=2b94785cbfd865a01cc68d7d4c7500b710e5e2fb&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/buaacyw/IT3D-text-to-3D&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/buaacyw/IT3D-text-to-3D.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation&lt;/strong&gt; (30 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[WACV 2024] Jinbo Wu, Xiaobo Gao, Xing Liu, et al. &lt;/summary&gt;Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen Zhao, Haocheng Feng, Jingtuo Liu, Errui Ding&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.16183&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d8aaed01dffc621488aecbb0ef01b50f86e44bc1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-12-blue.svg?paper=d8aaed01dffc621488aecbb0ef01b50f86e44bc1&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond&lt;/strong&gt; (11 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Mohammadreza Armandpour, Ali Sadeghian, Huangjie Zheng, et al. &lt;/summary&gt;Mohammadreza Armandpour, Ali Sadeghian, Huangjie Zheng, Amir Sadeghian, Mingyuan Zhou&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.04968&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/b19ca192a5bebbc3473be61989baf085ff21daa5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-50-blue.svg?paper=b19ca192a5bebbc3473be61989baf085ff21daa5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Perp-Neg/Perp-Neg-stablediffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Perp-Neg/Perp-Neg-stablediffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures&lt;/strong&gt; (14 Nov 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Gal Metzer, Elad Richardson, Or Patashnik, et al.&lt;/summary&gt;Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.07600&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/793939b83e10903f58d8edbb7534963df627a1fe&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-212-blue.svg?paper=793939b83e10903f58d8edbb7534963df627a1fe&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/eladrich/latent-nerf&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/eladrich/latent-nerf.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Magic3D: High-Resolution Text-to-3D Content Creation&lt;/strong&gt; (18 Nov 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023 Highlight] Chen-Hsuan Lin, Jun Gao, Luming Tang, et al. &lt;/summary&gt;Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin&#xA;  &lt;/details&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Lin_Magic3D_High-Resolution_Text-to-3D_Content_Creation_CVPR_2023_paper.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/bdf4af8311637c681904e71cf50f96fd0026f578&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-466-blue.svg?paper=bdf4af8311637c681904e71cf50f96fd0026f578&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation&lt;/strong&gt; (1 Dec 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Haochen Wang, Xiaodan Du, Jiahao Li, et al. &lt;/summary&gt;Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, Greg Shakhnarovich&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.00774&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/fc011ed5ee986332523a62d2783adee1179dc1ed&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-238-blue.svg?paper=fc011ed5ee986332523a62d2783adee1179dc1ed&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pals-ttic/sjc&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/pals-ttic/sjc.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;High-fidelity 3D Face Generation from Natural Language Descriptions&lt;/strong&gt; (5 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Menghua Wu, Hao Zhu, Linjia Huang, et al. &lt;/summary&gt;Menghua Wu, Hao Zhu, Linjia Huang, Yiyu Zhuang, Yuanxun Lu, Xun Cao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.03302&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/012d7d3ee690e5acadf416787651a8fe425e8eb3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-13-blue.svg?paper=012d7d3ee690e5acadf416787651a8fe425e8eb3&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zhuhao-nju/describe3d&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zhuhao-nju/describe3d.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;RODIN: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion&lt;/strong&gt; (12 Dec 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023 Highlight] Tengfei Wang, Bo Zhang, Ting Zhang, et al. &lt;/summary&gt;Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, Baining Guo&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.06135&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/7e993a9ca01dcd4538362454aaac29a18a63c000&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-158-blue.svg?paper=7e993a9ca01dcd4538362454aaac29a18a63c000&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ClipFace: Text-guided Editing of Textured 3D Morphable Models&lt;/strong&gt; (24 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[SIGGRAPH 2023] Tengfei Wang, Bo Zhang, Ting Zhang, et al. &lt;/summary&gt;Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, Baining Guo&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.01406&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f21e8eddf42580d1f38a11ec5acd8891c0454a1f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-40-blue.svg?paper=f21e8eddf42580d1f38a11ec5acd8891c0454a1f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shivangi-aneja/ClipFace&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/shivangi-aneja/ClipFace.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/strong&gt; (29 Sep 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2023 Oral] Ben Poole, Ajay Jain, Jonathan T. Barron, et al.&lt;/summary&gt;Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2209.14988&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4c94d04afa4309ec2f06bdd0fe3781f91461b362&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-908-blue.svg?paper=4c94d04afa4309ec2f06bdd0fe3781f91461b362&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&lt;/strong&gt; (25 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023 Spotlight] Zhengyi Wang, Cheng Lu, Yikai Wang, et al. &lt;/summary&gt;Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.13873&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c5e9fd131cde68c218d0ea69cd617a67c7f35d42&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-230-blue.svg?paper=c5e9fd131cde68c218d0ea69cd617a67c7f35d42&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/KU-CVLAB/3DFuse-threestudio&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/KU-CVLAB/3DFuse-threestudio.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HeadSculpt: Crafting 3D Head Avatars with Text&lt;/strong&gt; (25 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Xiao Han, Yukang Cao, Kai Han, et al. &lt;/summary&gt;Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, Kwan-Yee K. Wong&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.03038&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4e8cf9602d4ef714dcdb8580de40e1a2a717ab11&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-230-blue.svg?paper=4e8cf9602d4ef714dcdb8580de40e1a2a717ab11&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BrandonHanx/HeadSculpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/BrandonHanx/HeadSculpt.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ATT3D: Amortized Text-to-3D Object Synthesis&lt;/strong&gt; (6 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2023] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, et al. &lt;/summary&gt;Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, James Lucas&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.07349&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1e8403af2e1e7a8f803d8df9e8daac584f99c2a0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-25-blue.svg?paper=1e8403af2e1e7a8f803d8df9e8daac584f99c2a0&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation&lt;/strong&gt; (24 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2023] Rui Chen, Yongwei Chen, Ningxin Jiao, et al. &lt;/summary&gt;Rui Chen, Yongwei Chen, Ningxin Jiao, Kui Jia&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.13873&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/0cbb518c364067200476a51e5ce7476a4f582770&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-202-blue.svg?paper=0cbb518c364067200476a51e5ce7476a4f582770&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Gorilla-Lab-SCUT/Fantasia3D&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Gorilla-Lab-SCUT/Fantasia3D.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models&lt;/strong&gt; (10 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2023] Lukas H√∂llein, Ang Cao, Andrew Owens, et al. &lt;/summary&gt;Lukas H√∂llein, Ang Cao, Andrew Owens, Justin Johnson, Matthias Nie√üner&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.11989&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/95aa6fa4e42387561cff22378348d528adea37f2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-55-blue.svg?paper=95aa6fa4e42387561cff22378348d528adea37f2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/lukasHoel/text2room&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/lukasHoel/text2room.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;X-Mesh: Towards Fast and Accurate Text-driven 3D Stylization via Dynamic Textual Guidance&lt;/strong&gt; (28 Mar 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2023] Yiwei Ma, Xiaioqing Zhang, Xiaoshuai Sun, et al.&lt;/summary&gt;Yiwei Ma, Xiaioqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei Wang, Guannan Jiang, Weilin Zhuang, Rongrong Ji&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.15764&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f8bf2225a2993e3ead73d886b5797378d6e53186&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-18-blue.svg?paper=f8bf2225a2993e3ead73d886b5797378d6e53186&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/xmu-xiaoma666/X-Mesh&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/xmu-xiaoma666/X-Mesh.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation&lt;/strong&gt; (31 May 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt; Chi Zhang, Yiwen Chen, Yijun Fu, et al.&lt;/summary&gt;Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.19012&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/b980d98c81252dfbed334728c46625e58f54dd9d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-13-blue.svg?paper=b980d98c81252dfbed334728c46625e58f54dd9d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/icoz69/StyleAvatar3D&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/icoz69/StyleAvatar3D.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TextMesh: Generation of Realistic 3D Meshes From Text Prompts&lt;/strong&gt; (24 Apr 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[3DV 2023] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, et al.&lt;/summary&gt;Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, Federico Tombari&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.12439&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/2c6392491b6a942e08db46c8fff0ef5ba1fd9de8&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-67-blue.svg?paper=2c6392491b6a942e08db46c8fff0ef5ba1fd9de8&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/threestudio-project/threestudio&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/threestudio-project/threestudio.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clip-forge: Towards zero-shot text-to-shape generation&lt;/strong&gt; (28 Apr 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2022] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, et al. &lt;/summary&gt;Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, Kamal Rahimi Malekshan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.02624&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/738e3e0623054da29dc57fc6aee5e6711867c4e8&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-197-blue.svg?paper=738e3e0623054da29dc57fc6aee5e6711867c4e8&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AutodeskAILab/Clip-Forge&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AutodeskAILab/Clip-Forge.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-Shot Text-Guided Object Generation with Dream Fields&lt;/strong&gt; (2 Dec 2021) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2022] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, et al.&lt;/summary&gt;Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, Ben Poole&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.01455&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/03e1c3b5fdad9b21bbed3d13af7e8d6c73cbcfa6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-345-blue.svg?paper=03e1c3b5fdad9b21bbed3d13af7e8d6c73cbcfa6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ajayj.com/dreamfields&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/google-research/google-research/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/google-research/google-research.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Text2Mesh: Text-Driven Neural Stylization for Meshes&lt;/strong&gt; (6 Dec 2021) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2022] Oscar Michel, Roi Bar-On, Richard Liu, et al. &lt;/summary&gt;Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, Rana Hanocka&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.02624&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d15b27edf3630728cdb40f49946365d9011641cf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-222-blue.svg?paper=d15b27edf3630728cdb40f49946365d9011641cf&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/threedle/text2mesh&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/threedle/text2mesh.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TANGO: Text-driven Photorealistic and Robust 3D Stylization via Lighting Decomposition&lt;/strong&gt; (20 Oct 2022) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2022 Spotlight] Yongwei Chen, Rui Chen, Jiabao Lei, et al. &lt;/summary&gt;Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, Kui Jia&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.11277&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/44e49f72fb6b97f52c25a30f0adc68c2384430ba&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-47-blue.svg?paper=44e49f72fb6b97f52c25a30f0adc68c2384430ba&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Gorilla-Lab-SCUT/tango&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Gorilla-Lab-SCUT/tango.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CLIP-Mesh: Generating textured meshes from text using pretrained image-text models&lt;/strong&gt; (24 Mar 2022) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[SIGGRAPH ASIA 2022] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, et al. &lt;/summary&gt;Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, Tiberiu Popa&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.13333&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8941e477b2f39eb92712f04400412da60d349ec1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-167-blue.svg?paper=8941e477b2f39eb92712f04400412da60d349ec1&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NasirKhalid24/CLIP-Mesh&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/NasirKhalid24/CLIP-Mesh.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MotionCLIP: Exposing Human Motion Generation to CLIP Space&lt;/strong&gt; (15 Mar 2022) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ECCV 2022] Guy Tevet, Brian Gordon, Amir Hertz, et al. &lt;/summary&gt;Guy Tevet, Brian Gordon, Amir Hertz, Amit H. Bermano, Daniel Cohen-Or&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2203.08063&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e82df4b6a3628501fce67835ad8316d6525ad133&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-140-blue.svg?paper=e82df4b6a3628501fce67835ad8316d6525ad133&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/GuyTevet/MotionCLIP&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/GuyTevet/MotionCLIP.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Objaverse-XL: A Universe of 10M+ 3D Objects&lt;/strong&gt; (11 Jul 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Matt Deitke, Dustin Schwenk, Jordi Salvador, et al. &lt;/summary&gt;Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, Ali Farhadi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.05663&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1b90e9e9734bed6b379ae87d688cb3b887baf597&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-70-blue.svg?paper=1b90e9e9734bed6b379ae87d688cb3b887baf597&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/objaverse-xl&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/allenai/objaverse-xl.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Objaverse: A Universe of Annotated 3D Objects&lt;/strong&gt; (15 Dec 2022) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Matt Deitke, Dustin Schwenk, Jordi Salvador, et al. &lt;/summary&gt;Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.08051&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1b31dbf44e68b698120552366df03e6e35a1e428&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-230-blue.svg?paper=1b31dbf44e68b698120552366df03e6e35a1e428&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/objaverse-xl&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/allenai/objaverse-xl.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Audio Generation&lt;/h2&gt; &#xA;&lt;h3&gt;üîÖ LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation&lt;/strong&gt; (27 Feb 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shuangrui Ding, Zihan Liu, Xiaoyi Dong, et al.&lt;/summary&gt;Shuangrui Ding, Zihan Liu, Xiaoyi Dong, Pan Zhang, Rui Qian, Conghui He, Dahua Lin, Jiaqi Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.17645&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e7c8a74423a5811a3aac5f33001fce32d2e2386c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=e7c8a74423a5811a3aac5f33001fce32d2e2386c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pjlab-songcomposer.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pjlab-songcomposer/songcomposer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/pjlab-songcomposer/songcomposer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ChatMusician: Understanding and Generating Music Intrinsically with LLM&lt;/strong&gt; (25 Feb 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Ruibin Yuan, Hanfeng Lin, Yi Wang, et al.&lt;/summary&gt;Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao Shen, Ge Zhang, Yuhang Wu, Cong Liu, Ziya Zhou, Ziyang Ma, Liumeng Xue, Ziyu Wang, Qin Liu, Tianyu Zheng, Yizhi Li, Yinghao Ma, Yiming Liang, Xiaowei Chi, Ruibo Liu, Zili Wang, Pengfei Li, Jingcheng Wu, Chenghua Lin, Qifeng Liu, Tao Jiang, Wenhao Huang, Wenhu Chen, Emmanouil Benetos, Jie Fu, Gus Xia, Roger Dannenberg, Wei Xue, Shiyin Kang, Yike Guo&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.16153&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/48494aa30f35a64858644aba839c8cba38c0cf2a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=48494aa30f35a64858644aba839c8cba38c0cf2a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://shanghaicannon.github.io/ChatMusician/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/hf-lin/ChatMusician&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hf-lin/ChatMusician.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/m-a-p/ChatMusician&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling&lt;/strong&gt; (19 Feb 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jun Zhan, Junqi Dai, Jiasheng Ye, et al.&lt;/summary&gt;Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, Xipeng Qiu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.12226&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/14191e9f12913ad8c7ac6e1188682afac04aad09&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=14191e9f12913ad8c7ac6e1188682afac04aad09&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://junzhan2000.github.io/AnyGPT.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenMOSS/AnyGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenMOSS/AnyGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Boosting Large Language Model for Speech Synthesis: An Empirical Study&lt;/strong&gt; (30 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Hongkun Hao, Long Zhou, Shujie Liu, et al.&lt;/summary&gt;Hongkun Hao, Long Zhou, Shujie Liu, Jinyu Li, Shujie Hu, Rui Wang, Furu Wei&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.00246&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c1dd77e48dd615ee6881b2cc876a00a92cae6eac&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=c1dd77e48dd615ee6881b2cc876a00a92cae6eac&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action&lt;/strong&gt; (28 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiasen Lu, Christopher Clark, Sangho Lee, et al.&lt;/summary&gt;Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.17172&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6c64ddd2190909de2c680dd18abc9b92e80c39f9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-8-blue.svg?paper=6c64ddd2190909de2c680dd18abc9b92e80c39f9&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://unified-io-2.allenai.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/unified-io-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/allenai/unified-io-2.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;M2UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models&lt;/strong&gt; (19 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, et al.&lt;/summary&gt;Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, Ying Shan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.11255&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1e84d7c45f70038574fcdb7bc1b20da9b348a092&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=1e84d7c45f70038574fcdb7bc1b20da9b348a092&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crypto-code.github.io/M2UGen-Demo/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shansongliu/M2UGen&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/shansongliu/M2UGen.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/M2UGen/M2UGen-Demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT&lt;/strong&gt; (7 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiaming Wang, Zhihao Du, Qian Chen, et al.&lt;/summary&gt;Jiaming Wang, Zhihao Du, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen Wang, Siqi Zheng, Chang Zhou, Zhijie Yan, Shiliang Zhang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.04673&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ffa05cb5504ba08254f498223f613b3ebcf87692&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-13-blue.svg?paper=ffa05cb5504ba08254f498223f613b3ebcf87692&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lauragpt.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLaSM: Large Language and Speech Model&lt;/strong&gt; (30 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yu Shu, Siwei Dong, Guangyao Chen, et al.&lt;/summary&gt;Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, Yemin Shi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.15930&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/7b22ecd9f1ced58c1704ac6191e029b98054e330&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-10-blue.svg?paper=7b22ecd9f1ced58c1704ac6191e029b98054e330&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/LinkSoul/LLaSM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LinkSoul-AI/LLaSM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/LinkSoul-AI/LLaSM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/LinkSoul/LLaSM&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AudioPaLM: A Large Language Model That Can Speak and Listen&lt;/strong&gt; (22 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, et al.&lt;/summary&gt;Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal√°n Borsos, F√©lix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimiroviƒá, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, Christian Frank&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.12925&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/3efb81de24eb88017d6dbcf22cb4215084223fd8&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-69-blue.svg?paper=3efb81de24eb88017d6dbcf22cb4215084223fd8&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://google-research.github.io/seanet/audiopalm/examples/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pengi: An Audio Language Model for Audio Tasks&lt;/strong&gt; (19 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Soham Deshmukh, Benjamin Elizalde, Rita Singh, et al.&lt;/summary&gt;Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.11834&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ad22af138fa1d1490cda0301abf8159a7c30c5a2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-35-blue.svg?paper=ad22af138fa1d1490cda0301abf8159a7c30c5a2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/Pengi&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/Pengi&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/Pengi.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities&lt;/strong&gt; (18 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Dong Zhang, Shimin Li, Xin Zhang, et al.&lt;/summary&gt;Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.11000&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5cac6430bd379c9d2fe13137dfd6ae7721a2679f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-76-blue.svg?paper=5cac6430bd379c9d2fe13137dfd6ae7721a2679f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://0nutation.github.io/SpeechGPT.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/0nutation/SpeechGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/0nutation/SpeechGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sparks of Artificial General Intelligence: Early experiments with GPT-4&lt;/strong&gt; (22 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, et al.&lt;/summary&gt;S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.12712&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/574beee702be3856d60aa482ec725168fe64fc99&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1407-blue.svg?paper=574beee702be3856d60aa482ec725168fe64fc99&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Non-LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Audiobox: Unified Audio Generation with Natural Language Prompts&lt;/strong&gt; (25 Dec 2023)&lt;br&gt; Apoorv Vyas, Bowen Shi, Matthew Le&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.15821&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f124ae1e4663359193be32adb37b07b3252d5329&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=f124ae1e4663359193be32adb37b07b3252d5329&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://audiobox.metademolab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://audiobox.metademolab.com/capabilities&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Music ControlNet: Multiple Time-varying Controls for Music Generation&lt;/strong&gt; (13 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shih-Lun Wu, Chris Donahue, Shinji Watanabe, et al.&lt;/summary&gt;Shih-Lun Wu, Chris Donahue, Shinji Watanabe, Nicholas J. Bryan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.07069&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/42239e71a712d70cd24e06ffc0cf0d22fc628a36&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-3-blue.svg?paper=42239e71a712d70cd24e06ffc0cf0d22fc628a36&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://musiccontrolnet.github.io/web/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing&lt;/strong&gt; (19 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yixiao Zhang, Akira Maezawa, Gus Xia, et al.&lt;/summary&gt;Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto, Simon Dixon&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.12404&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/cca4218dd7c10c1614bbd84aa7cd7e00027bdc7c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-3-blue.svg?paper=cca4218dd7c10c1614bbd84aa7cd7e00027bdc7c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sites.google.com/view/loop-copilot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ldzhangyx/loop-copilot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ldzhangyx/loop-copilot.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models&lt;/strong&gt; (18 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Dingyao Yu, Kaitao Song, Peiling Lu, et al.&lt;/summary&gt;Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, Jiang Bian&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.11954&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/beaf64df85f8204b8cd89a7f46827608e6d16922&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=beaf64df85f8204b8cd89a7f46827608e6d16922&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/muzic/tree/main/musicagent&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/muzic/tree/main/musicagent.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;UniAudio: An Audio Foundation Model Toward Universal Audio Generation&lt;/strong&gt; (1 Oct 2023)&lt;br&gt; Dongchao Yang, Jinchuan Tian, Xu Tan&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.00704&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/74bfbbb7307a7af2686043ea97ab8b34cb062ba8&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-15-blue.svg?paper=74bfbbb7307a7af2686043ea97ab8b34cb062ba8&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dongchaoyang.top/UniAudio_demo/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yangdongchao/UniAudio&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yangdongchao/UniAudio.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AudioLM: a Language Modeling Approach to Audio Generation&lt;/strong&gt; (7 Sep 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zal√°n Borsos, Rapha√´l Marinier, Damien Vincent, et al. (IEEE/ACM Transactions on Audio, Speech, and Language Processing)&lt;/summary&gt;Zal√°n Borsos, Rapha√´l Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, Neil Zeghidour&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.03143&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8c870bef01a4fbb20f60722ffc2f6bee3870b18b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-232-blue.svg?paper=8c870bef01a4fbb20f60722ffc2f6bee3870b18b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Wavjourney: Compositional audio creation with large language models&lt;/strong&gt; (26 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xubo Liu, Zhongkai Zhu, Haohe Liu, et al.&lt;/summary&gt;Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D. Plumbley, Wenwu Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.14335&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/aa7bcd1f9453c9096ec78900a7b94e816ed0e1c5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-9-blue.svg?paper=aa7bcd1f9453c9096ec78900a7b94e816ed0e1c5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://audio-agi.github.io/WavJourney_demopage/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Audio-AGI/WavJourney&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Audio-AGI/WavJourney.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Audio-AGI/WavJourney&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody&lt;/strong&gt; (16 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Sofoklis Kakouros, Juraj ≈†imko, Martti Vainio, et al. (2023 SSW)&lt;/summary&gt;Sofoklis Kakouros, Juraj ≈†imko, Martti Vainio, Antti Suni&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.09814&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/63aad36dc981348493be6743292a04234b29ba4e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=63aad36dc981348493be6743292a04234b29ba4e&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Simple and Controllable Music Generation&lt;/strong&gt; (8 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jade Copet, Felix Kreuk, Itai Gat, et al.&lt;/summary&gt;Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre D√©fossez&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.05284&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4cc8e18f5eece0b0d8e1abcb8ee10fb33680fbb2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-71-blue.svg?paper=4cc8e18f5eece0b0d8e1abcb8ee10fb33680fbb2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ai.honu.io/papers/musicgen/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/facebookresearch/audiocraft&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/facebookresearch/audiocraft.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/facebook/MusicGen&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation&lt;/strong&gt; (29 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiawei Huang, Yi Ren, Rongjie Huang, et al.&lt;/summary&gt;Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, Zhou Zhao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.18474&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/83d4b22d803ae856cf6b308482bd504fa151d39e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-10-blue.svg?paper=83d4b22d803ae856cf6b308482bd504fa151d39e&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://make-an-audio-2.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Jukebox: A Generative Model for Music&lt;/strong&gt; (30 Apr 2020)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Prafulla Dhariwal, Heewoo Jun, Christine Payne, et al.&lt;/summary&gt;Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2005.00341&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/67dea28495cab71703993d0d52ca4733b9a66077&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-477-blue.svg?paper=67dea28495cab71703993d0d52ca4733b9a66077&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openai.com/research/jukebox&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/openai/jukebox?tab=readme-ov-file&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/openai/jukebox?tab=readme-ov-file.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Audiogpt: Understanding and generating speech, music, sound, and talking head&lt;/strong&gt; (25 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Rongjie Huang, Mingze Li, Dongchao Yang, et al.&lt;/summary&gt;Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.12995&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8bc617c9139648d7a92991d70c671230bac7b2e2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-83-blue.svg?paper=8bc617c9139648d7a92991d70c671230bac7b2e2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AIGC-Audio/AudioGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TANGO: Text-to-Audio Generation using Instruction Tuned LLM and Latent Diffusion Model&lt;/strong&gt; (24 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, et al.&lt;/summary&gt;Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Soujanya Poria&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.13731&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f51bc74814a3452009ea5ca262d9768d08149ee6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-51-blue.svg?paper=f51bc74814a3452009ea5ca262d9768d08149ee6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://tango-web.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/declare-lab/tango&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/declare-lab/tango.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/declare-lab/tango&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface&lt;/strong&gt; (30 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yongliang Shen, Kaitao Song, Xu Tan, et al.&lt;/summary&gt;Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.17580&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-413-blue.svg?paper=d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/JARVIS&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/microsoft/HuggingGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Neural codec language models are zero-shot text to speech synthesizers&lt;/strong&gt; (5 Jan 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Chengyi Wang, Sanyuan Chen, Yu Wu, et al.&lt;/summary&gt;Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, Lei He, Sheng Zhao, Furu Wei&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c2f91f35df893714418cc29096083dce0b441229&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-203-blue.svg?paper=c2f91f35df893714418cc29096083dce0b441229&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.microsoft.com/en-us/research/project/vall-e-x/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MusicLM: Generating Music From Text&lt;/strong&gt; (26 Jan 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Andrea Agostinelli, Timo I. Denk, Zal√°n Borsos, et al.&lt;/summary&gt;Andrea Agostinelli, Timo I. Denk, Zal√°n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, Matt Sharifi, Neil Zeghidour, Christian Frank&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2301.11325&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/428854d9e75f94f0e61f37c6887c77800437d516&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-171-blue.svg?paper=428854d9e75f94f0e61f37c6887c77800437d516&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://google-research.github.io/seanet/musiclm/examples/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Libriheavy: a 50,000 hours ASR corpus with punctuation casing and context&lt;/strong&gt; (15 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Wei Kang, Xiaoyu Yang, Zengwei Yao, et al.&lt;/summary&gt;Wei Kang, Xiaoyu Yang, Zengwei Yao, Fangjun Kuang, Yifan Yang, Liyong Guo, Long Lin, Daniel Povey&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.08105&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e99b45179686982401d2d6ec919e42b327f04c0b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=e99b45179686982401d2d6ec919e42b327f04c0b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition&lt;/strong&gt; (7 Oct 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;BinBin Zhang, Hang Lv, Pengcheng Guo, et al.&lt;/summary&gt;BinBin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, Di wu, Zhendong Peng&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.03370&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9de3ac21af795dac56f6031e73db8198716bb352&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-103-blue.svg?paper=9de3ac21af795dac56f6031e73db8198716bb352&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://wenet.org.cn/WenetSpeech/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vggsound: A large-scale audio-visual dataset&lt;/strong&gt; (29 Apr 2020)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Honglie Chen, Weidi Xie, Andrea Vedaldi, et al. (ICASSP)&lt;/summary&gt;Honglie Chen, Weidi Xie, Andrea Vedaldi, Andrew Zisserman&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.14368&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/66831f683141c11ed7e20b0f2e8b40700740c164&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-316-blue.svg?paper=66831f683141c11ed7e20b0f2e8b40700740c164&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/data/vggsound/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Libri-Light: A Benchmark for ASR with Limited or No Supervision&lt;/strong&gt; (17 Dec 2019 )&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jacob Kahn, Morgane Rivi√®re, Weiyi Zheng, et al. (ICASSP)&lt;/summary&gt;Jacob Kahn, Morgane Rivi√®re, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar√©, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, Tatiana Likhomanenko, Gabriel Synnaeve, Armand Joulin, Abdel-rahman Mohamed, Emmanuel Dupoux&#xA;  &lt;/details&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9052942?casa_token=lrlqiDak4dkAAAAA:VfCRwwWhLiJyb61NkesOfzpobk4zjac1boi4PoJ7llh1SKSi5YJDt4DaozUQw_o8X4LvO1bK&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f59c038dee828e0a8c2fc28130d12e39ee4952d6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-449-blue.svg?paper=f59c038dee828e0a8c2fc28130d12e39ee4952d6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/facebookresearch/libri-light&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;The mtg-jamendo dataset for automatic music tagging&lt;/strong&gt; (15 Jun 2019)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Dmitry Bogdanov, Minz Won, Philip Tovstogan, et al. (ICML)&lt;/summary&gt;Dmitry Bogdanov, Minz Won, Philip Tovstogan, Alastair Porter, Xavier Serra&#xA;  &lt;/details&gt;&lt;a href=&#34;https://repositori.upf.edu/bitstream/handle/10230/42015/bogdanov_ICML2019__Jamendo.pdf?sequence=1&amp;amp;isAllowed=y&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/23037085b0815455e6d47333089b925c8c0e21d5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-102-blue.svg?paper=23037085b0815455e6d47333089b925c8c0e21d5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mtg.github.io/mtg-jamendo-dataset/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MTG/mtg-jamendo-dataset&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/MTG/mtg-jamendo-dataset.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech&lt;/strong&gt; (5 Apr 2019)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Heiga Zen, Viet Dang, Rob Clark, et al.&lt;/summary&gt;Heiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, Yonghui Wu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1904.02882&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/2789b6c84ba1422746246685001accba5563e7c1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-555-blue.svg?paper=2789b6c84ba1422746246685001accba5563e7c1&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.openslr.org/60/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset&lt;/strong&gt; (29 Oct 2018)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, et al.&lt;/summary&gt;Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, Cheng-Zhi Anna Huang, Sander Dieleman, Erich Elsen, Jesse Engel, Douglas Eck&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1810.12247&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/2603a68b4503ba949c91c7e00cd342624b4aae2f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-331-blue.svg?paper=2603a68b4503ba949c91c7e00cd342624b4aae2f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://magenta.tensorflow.org/datasets/maestro&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Audio Set: An ontology and human-labeled dataset for audio events&lt;/strong&gt; (05 Mar 2017)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, et al. (TASLP)&lt;/summary&gt;Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, Marvin Ritter&#xA;  &lt;/details&gt;&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45857.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5ba2218b708ca64ab556e39d5997202e012717d5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2338-blue.svg?paper=5ba2218b708ca64ab556e39d5997202e012717d5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://research.google.com/audioset/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Librispeech: An ASR corpus based on public domain audio books&lt;/strong&gt; (19 Apr2015)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Vassil Panayotov, Guoguo Chen, Daniel Povey, et al. (ICASSP)&lt;/summary&gt;Vassil Panayotov, Guoguo Chen, Daniel Povey, Sanjeev Khudanpur&#xA;  &lt;/details&gt;&lt;a href=&#34;https://ieeexplore.ieee.org/document/7178964&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/34038d9424ce602d7ac917a4e582d977725d4393&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4815-blue.svg?paper=34038d9424ce602d7ac917a4e582d977725d4393&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.openslr.org/12&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Evaluation of Algorithms Using Games: The Case of Music Tagging&lt;/strong&gt; (26 Oct 2009)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Edith Law, Kris West, Michael Mandel, et al. (ISMIR)&lt;/summary&gt;Edith Law, Kris West, Michael Mandel, Mert Bay J. Stephen Downie&#xA;  &lt;/details&gt;&lt;a href=&#34;https://ismir2009.ismir.net/proceedings/OS5-5.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8a1384e041cc6ea2735b01c734aeef666dc92884&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-214-blue.svg?paper=8a1384e041cc6ea2735b01c734aeef666dc92884&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mirg.city.ac.uk/codeapps/the-magnatagatune-dataset&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Generation with Multiple Modalities&lt;/h2&gt; &#xA;&lt;h3&gt;üîÖ LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation&lt;/strong&gt; (30 Nov 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zineng Tang, Ziyi Yang, Mahmoud Khademi, et al.&lt;/summary&gt;Zineng Tang, Ziyi Yang, Mahmoud Khademi, Yang Liu, Chenguang Zhu, Mohit Bansal&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.18775&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/78582ad19779a69d97b797a3c6eb2397f99398b6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=78582ad19779a69d97b797a3c6eb2397f99398b6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codi-2.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/CoDi-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models&lt;/strong&gt; (8 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zhen Yang, Yingxue Zhang, Fandong Meng, et al.&lt;/summary&gt;Zhen Yang, Yingxue Zhang, Fandong Meng, Jie Zhou&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.04589&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/TEAL%3A-Tokenize-and-Embed-ALL-for-Multi-modal-Large-Yang-Zhang/59d716b442ab760a78f58de6748c0fa1d507bfc1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=9f411fda2ad5b141a3115f707bcf5ee865b3fb94&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;NExT-GPT: Any-to-Any Multimodal LLM&lt;/strong&gt; (11 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shengqiong Wu, Hao Fei, Leigang Qu, et al.&lt;/summary&gt;Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.05519&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/fa75a55760e6ea49b39b83cb85c99a22e1088254&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-94-blue.svg?paper=fa75a55760e6ea49b39b83cb85c99a22e1088254&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://next-gpt.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/NExT-GPT/NExT-GPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/NExT-GPT/NExT-GPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://1ca8b1601858a12830.gradio.live/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CoDi: Any-to-Any Generation via Composable Diffusion&lt;/strong&gt; (19 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Zineng Tang, Ziyi Yang, Chenguang Zhu, et al.&lt;/summary&gt;Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, Mohit Bansal&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.11846&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9f411fda2ad5b141a3115f707bcf5ee865b3fb94&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-41-blue.svg?paper=9f411fda2ad5b141a3115f707bcf5ee865b3fb94&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-V3&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/i-Code.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codi-gen.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Non-LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation&lt;/strong&gt; (9 Jan 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Junming Chen, et al.&lt;/summary&gt;Junming Chen, Yunfei Liu, Jianan Wang, Ailing Zeng, Yu Li, Qifeng Chen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.04747&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1370d74ff6f7857a84da952e2f4cb6f42da40615&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=1370d74ff6f7857a84da952e2f4cb6f42da40615&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://jeremycjm.github.io/proj/DiffSHEG/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/JeremyCJM/DiffSHEG&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/JeremyCJM/DiffSHEG.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TAVGBench: Benchmarking Text to Audible-Video Generation&lt;/strong&gt; (22 Apr 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yuxin Mao, Xuyang Shen, Jing Zhang, et al.&lt;/summary&gt;Yuxin Mao, Xuyang Shen, Jing Zhang, Zhen Qin, Jinxing Zhou, Mochu Xiang, Yiran Zhong, Yuchao Dai&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.14381&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/TAVGBench%3A-Benchmarking-Text-to-Audible-Video-Mao-Shen/4ba90678411ddc0a2eb997e1184b059bdc955fd5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=21a77ed349c8621d0a0ef8407eb744e3de3b13c5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenNLPLab/TAVGBench&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Vchitect/VBench.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners&lt;/strong&gt; (27 Feb 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Yazhou Xing, Yingqing He, Zeyue Tian, et al.&lt;/summary&gt;Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, Qifeng Chen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.17723&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/Seeing-and-Hearing%3A-Open-domain-Visual-Audio-with-Xing-He/d9822d11ae4ead1f1d32c43124a6a0eb80ea4f0c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=21a77ed349c8621d0a0ef8407eb744e3de3b13c5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yzxing87/Seeing-and-Hearing&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Vchitect/VBench.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìç Multimodal Editing&lt;/h1&gt; &#xA;&lt;h2&gt;Image Editing&lt;/h2&gt; &#xA;&lt;h3&gt;üîÖ LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models&lt;/strong&gt; (11 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Yuzhou Huang, Liangbin Xie, Xintao Wang, et al.&lt;/summary&gt; Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, Ying Shan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.06739&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/388b0f44faf0a14cc402c2554ec36a868cf59129&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=388b0f44faf0a14cc402c2554ec36a868cf59129&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://yuzhou914.github.io/SmartEdit/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/SmartEdit&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/TencentARC/SmartEdit.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Self-correcting LLM-controlled Diffusion Models&lt;/strong&gt; (27 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Tsung-Han Wu, Long Lian, Joseph E. Gonzalez, et al.&lt;/summary&gt; Tsung-Han Wu, Long Lian, Joseph E. Gonzalez, Boyi Li, Trevor Darrell&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.16090&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/42c4315b5d2e33d7d9a0afdf84e6a47ccd7a700e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=42c4315b5d2e33d7d9a0afdf84e6a47ccd7a700e&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Emu Edit: Precise Image Editing via Recognition and Generation Tasks&lt;/strong&gt; (16 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ArXiv 2023] Shelly Sheynin, Adam Polyak, Uriel Singer, et al.&lt;/summary&gt; Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, Yaniv Taigman&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.10089&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5bcb0153dd0840113eb27d4d6f753414ef656a03&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-8-blue.svg?paper=5bcb0153dd0840113eb27d4d6f753414ef656a03&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://emu-edit.metademolab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Guiding Instruction-based Image Editing via Multimodal Large Language Models&lt;/strong&gt; &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024 (Spotlight)] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, et al.&lt;/summary&gt; Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, Zhe Gan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.17102v1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-11-blue.svg?paper=092245d86b77181c36f972b1b7a17a59cd989c4a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mllm-ie.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tsujuifu/pytorch_mgie&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/tsujuifu/pytorch_mgie.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CHATEDIT: Towards Multi-turn Interactive Facial Image Editing via Dialogue&lt;/strong&gt; (20 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[EMNLP 2023] Xing Cui, Zekun Li, Peipei Li, et al.&lt;/summary&gt; Xing Cui, Zekun Li, Peipei Li, Yibo Hu, Hailin Shi, Zhaofeng He&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.11108&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5a185965ad1e87367d044b47043706d00b85b007&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=5a185965ad1e87367d044b47043706d00b85b007&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/cuixing100876/ChatEdit&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/cuixing100876/ChatEdit.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HIVE: Harnessing Human Feedback for Instructional Visual Editing&lt;/strong&gt; (16 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shu Zhang, Xinyi Yang, Yihao Feng, et al.&lt;/summary&gt; Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu.&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.09618&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/372bc41602bbd21f192305775f0a58de9880e454&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-28-blue.svg?paper=372bc41602bbd21f192305775f0a58de9880e454&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://shugerdou.github.io/hive/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/salesforce/HIVE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/salesforce/HIVE.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/strong&gt; (8 Mar 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Chenfei Wu, Shengming Yin, Weizhen Qi, et al.&lt;/summary&gt; Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-337-blue.svg?paper=af997821231898a5f8d0fd78dad4eec526acabe5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/moymix/TaskMatrix&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/microsoft/visual_chatgpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InstructPix2Pix: Learning to Follow Image Editing Instructions&lt;/strong&gt; (17 Nov 2022)&lt;br&gt; [CVPR 2023 (Highlight)] Brooks, Tim, Aleksander Holynski, and Alexei A. Efros.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.09800&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/a2d2bbe4c542173662a444b33b76c66992697830&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-582-blue.svg?paper=a2d2bbe4c542173662a444b33b76c66992697830&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/timothybrooks/instruct-pix2pix.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Non-LLM-based (Clip/T5)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing&lt;/strong&gt; (4 Feb 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2024] Chong Mou, Xintao Wang, Jiechong Song, et al.&lt;/summary&gt;Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.02583&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/198b3d809594a76bc473927af37b858132ac7fdd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=198b3d809594a76bc473927af37b858132ac7fdd&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MC-E/DragonDiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/MC-E/DragonDiffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ZONE: Zero-Shot Instruction-Guided Local Editing&lt;/strong&gt; (28 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shanglin Li, Bohan Zeng, Yutang Feng, et al.&lt;/summary&gt;Shanglin Li, Bohan Zeng, Yutang Feng, Sicheng Gao, Xuhui Liu, Jiaming Liu, Li Lin, Xu Tang, Yao Hu, Jianzhuang Liu, Baochang Zhang. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.16794&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/05eb2ad3af471c05a24abbf70258688e579cdf22&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=05eb2ad3af471c05a24abbf70258688e579cdf22&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Watch Your Steps: Local Image and Scene Editing by Text Instructions&lt;/strong&gt; (17 Aug 2023 )&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A. Brubaker, et al.&lt;/summary&gt;Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G. Derpanis, Igor Gilitschenski. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.08947&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/737ad8905228cd410e3342b5cceefd4feb57d166&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-9-blue.svg?paper=737ad8905228cd410e3342b5cceefd4feb57d166&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ashmrz.github.io/WatchYourSteps/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dragondiffusion: Enabling drag-style manipulation on diffusion models&lt;/strong&gt; (5 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2024] Chong Mou, Xintao Wang, Jiechong Song, et al.&lt;/summary&gt;Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.14331&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/2cfaa5b3571d3b75f040f6d639359a3c673f5561&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-36-blue.svg?paper=2cfaa5b3571d3b75f040f6d639359a3c673f5561&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mc-e.github.io/project/DragonDiffusion/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MC-E/DragonDiffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/MC-E/DragonDiffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Differential Diffusion: Giving Each Pixel Its Strength&lt;/strong&gt; (1 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[Arxiv 2023] Thao Nguyen, Yuheng Li, Utkarsh Ojha, et al.&lt;/summary&gt;Thao Nguyen, Yuheng Li, Utkarsh Ojha, Yong Jae Lee&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.14331&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6e5760e5d4b468bbf01a95a6f64bd65c3aa3d798&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-3-blue.svg?paper=6e5760e5d4b468bbf01a95a6f64bd65c3aa3d798&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://differential-diffusion.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/exx8/differential-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/exx8/differential-diffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Instruction Inversion: Image Editing via Visual Prompting&lt;/strong&gt; (26 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ArXiv 2023] Thao Nguyen, Yuheng Li, Utkarsh Ojha, et al.&lt;/summary&gt; Thao Nguyen, Yuheng Li, Utkarsh Ojha, Yong Jae Lee. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.14331&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f4c62aa336de45273e0fdfcfbd65b3c2e552ad56&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-11-blue.svg?paper=f4c62aa336de45273e0fdfcfbd65b3c2e552ad56&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://thaoshibe.github.io/visii/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/thaoshibe/visii&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/thaoshibe/visii.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing&lt;/strong&gt; (17 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2023] Mingdeng Cao, Xintao Wang, Zhongang Qi, et al.&lt;/summary&gt; Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, Yinqiang Zheng. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.08947&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/85963807c11abe38e9a2797d9860e012238607ef&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-102-blue.svg?paper=85963807c11abe38e9a2797d9860e012238607ef&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ljzycmd.github.io/projects/MasaCtrl/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/TencentARC/MasaCtrl&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/TencentARC/MasaCtrl.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PAIR-Diffusion: A Comprehensive Multimodal Object-Level Image Editor&lt;/strong&gt; (30 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ArXiv 2023] Vidit Goel, Elia Peruzzo, Yifan Jiang, et al.&lt;/summary&gt; Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Xingqian Xu, Nicu Sebe, Trevor Darrell, Zhangyang Wang, Humphrey Shi. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.17546&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c614a4da924466f62ca39002af425c9d14d240a3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=c614a4da924466f62ca39002af425c9d14d240a3&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://vidit98.github.io/publication/conference-paper/pair_diff.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Picsart-AI-Research/PAIR-Diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/pix2pixzero/pix2pix-zero.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-shot Image-to-Image Translation&lt;/strong&gt; (6 Feb 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[SIGGRAPH 2023] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, et al.&lt;/summary&gt; Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, Jun-Yan Zhu. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.03027&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/daf61010eee0fbf6f9bab7db71c395ffca6f3ff3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-159-blue.svg?paper=daf61010eee0fbf6f9bab7db71c395ffca6f3ff3&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pix2pixzero.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pix2pixzero/pix2pix-zero&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/pix2pixzero/pix2pix-zero.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SINE: SINgle Image Editing with Text-to-Image Diffusion Models&lt;/strong&gt; (8 Dec 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Zhixing Zhang, Ligong Han, Arnab Ghosh, et al.&lt;/summary&gt; Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, Jian Ren. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.04489&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/a6ad30123bef4b19ee40c3d63cfabf00d211f0ef&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-69-blue.svg?paper=a6ad30123bef4b19ee40c3d63cfabf00d211f0ef&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zhang-zx.github.io/SINE/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zhang-zx/SINE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zhang-zx/SINE.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Interactive Image Manipulation with Complex Text Instructions&lt;/strong&gt; (25 Nov 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[WACV 2023] Ryugo Morita, Zhiqiang Zhang, Man M. Ho, et al.&lt;/summary&gt; Ryugo Morita, Zhiqiang Zhang, Man M. Ho, Jinjia Zhou. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.15352&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/387144d293567408c363313aac971294e7ec8547&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=387144d293567408c363313aac971294e7ec8547&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation&lt;/strong&gt; (22 Nov 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Narek Tumanyan, Michal Geyer, Shai Bagon, et al.&lt;/summary&gt; Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.12572&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/b000d6865db824af1563708fb7a545ddd65c6b3a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-224-blue.svg?paper=b000d6865db824af1563708fb7a545ddd65c6b3a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pnp-diffusion.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MichalGeyer/plug-and-play&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/MichalGeyer/plug-and-play.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Imagic: Text-Based Real Image Editing with Diffusion Models&lt;/strong&gt; (17 Oct 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Bahjat Kawar, Shiran Zada, Oran Lang, et al.&lt;/summary&gt; Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.09276&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/23e261a20a315059b4de5492ed071c97a20c12e7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-496-blue.svg?paper=23e261a20a315059b4de5492ed071c97a20c12e7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://imagic-editing.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- [![Code](https://img.shields.io/github/stars/pix2pixzero/pix2pix-zero.svg?style=social&amp;label=Star)](https://github.com/pix2pixzero/pix2pix-zero) --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Null-text Inversion for Editing Real Images using Guided Diffusion Models&lt;/strong&gt;&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2023] Ron Mokady, Amir Hertz, Kfir Aberman, et al.&lt;/summary&gt; Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.09794&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4de94949daf9bc8dd0e5161d20dfe83198d20ec1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=4de94949daf9bc8dd0e5161d20dfe83198d20ec1&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://null-text-inversion.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/google/prompt-to-prompt/#null-text-inversion-for-editing-real-images&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/google/prompt-to-prompt.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prompt-to-Prompt Image Editing with Cross Attention Control&lt;/strong&gt; &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2023] Amir Hertz, Ron Mokady, Jay Tenenbaum, et al.&lt;/summary&gt; Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-Or. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2208.01626&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/04e541391e8dce14d099d00fb2c21dbbd8afe87f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-717-blue.svg?paper=04e541391e8dce14d099d00fb2c21dbbd8afe87f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://prompt-to-prompt.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/google/prompt-to-prompt&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/google/prompt-to-prompt.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DiffEdit: Diffusion-based semantic image editing with mask guidance&lt;/strong&gt; (20 Oct 2022) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2023] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, et al.&lt;/summary&gt; Guillaume Couairon, Jakob Verbeek, Holger Schwenk, Matthieu Cord. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2210.11427&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/064ccebc03d3afabaae30fe29a457c1cfcdff7e3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-208-blue.svg?paper=064ccebc03d3afabaae30fe29a457c1cfcdff7e3&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- [![Project_Page](https://img.shields.io/badge/Project_Page-00CED1)](https://ashmrz.github.io/WatchYourSteps/) --&gt; &#xA;&lt;!-- [![Code](https://img.shields.io/github/stars/Xiang-cd/DiffEdit-stable-diffusion.svg?style=social&amp;label=Star)](https://github.com/Xiang-cd/DiffEdit-stable-diffusion) --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation&lt;/strong&gt; (6 Oct 2021)&lt;br&gt; [CVPR 2022] Gwanghyun Kim, Taesung Kwon, Jong Chul Ye.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2110.02711&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8f8dedb511c0324d1cb7f9750560109ca9290b5f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-318-blue.svg?paper=8f8dedb511c0324d1cb7f9750560109ca9290b5f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/gwang-kim/DiffusionCLIP&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/gwang-kim/DiffusionCLIP.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations&lt;/strong&gt; (2 Aug 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICLR 2022] Chenlin Meng, Yutong He, Yang Song, et al.&lt;/summary&gt; Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2108.01073&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f671a09e3e5922e6d38cb77dda8d76d5ceac2a27&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-604-blue.svg?paper=f671a09e3e5922e6d38cb77dda8d76d5ceac2a27&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sde-image-editing.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ermongroup/SDEdit&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ermongroup/SDEdit.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Video Editing&lt;/h2&gt; &#xA;&lt;h3&gt;üîÖ LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CONSISTENT VIDEO-TO-VIDEO TRANSFER USING SYNTHETIC DATASET&lt;/strong&gt; (1 Nov 2023)&lt;br&gt; Jiaxin Cheng, Tianjun Xiao, Tong He.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.00213&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e8bbffb8413cb1f88e99a7ecbabd21a6eac82271&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=e8bbffb8413cb1f88e99a7ecbabd21a6eac82271&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/amazon-science/instruct-video-to-video&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/amazon-science/instruct-video-to-video.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InstructVid2Vid: Controllable Video Editing with Natural Language Instructions&lt;/strong&gt; (21 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Bosheng Qin, Juncheng Li, Siliang Tang, et al.&lt;/summary&gt;Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, Yueting Zhuang. &#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.12328&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/205d2ed0906440f07a0275d7d6a63bced60951fc&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=205d2ed0906440f07a0275d7d6a63bced60951fc&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- [![Code](https://img.shields.io/github/stars/duyguceylan/pix2video.svg?style=social&amp;label=Star)](https://github.com/duyguceylan/pix2video) --&gt; &#xA;&lt;h3&gt;Non-LLM-based (Clip/T5)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LATENTWARP: CONSISTENT DIFFUSION LATENTS FOR ZERO-SHOT VIDEO-TO-VIDEO TRANSLATION&lt;/strong&gt; (1 Nov 2023)&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yuxiang Bao, Di Qiu, Guoliang Kang, et al.&lt;/summary&gt;Yuxiang Bao, Di Qiu, Guoliang Kang, Baochang Zhang, Bo Jin, Kaiye Wang, Pengfei Yan. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.00353&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1b4323a5324ee20fe9b2ff2a65ec26550a51ec2c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=1b4323a5324ee20fe9b2ff2a65ec26550a51ec2c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- [![Project_Page](https://img.shields.io/badge/Project_Page-00CED1)](https://diffusion-tokenflow.github.io/) --&gt; &#xA;&lt;!-- [![Code](https://img.shields.io/github/stars/omerbt/TokenFlow.svg?style=social&amp;label=Star)](https://github.com/omerbt/TokenFlow) --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MagicStick: Controllable Video Editing via Control Handle Transformations&lt;/strong&gt; (1 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yue Ma, Xiaodong Cun, Yingqing He, et al.&lt;/summary&gt;Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, Qifeng Chen&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.03047&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/xxx&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=xxx&#34; alt=&#34;citation&#34;&gt; )&lt;/a&gt; ) &lt;a href=&#34;https://magic-stick-edit.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mayuelala/MagicStick&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mayuelala/MagicStick.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MagicEdit: High-Fidelity Temporally Coherent Video Editing&lt;/strong&gt; (28 Aug 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, et al.&lt;/summary&gt;Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, Jiashi Feng. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.14749&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8819777e104f8c4197c262e11a01b070b50007aa&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-20-blue.svg?paper=8819777e104f8c4197c262e11a01b070b50007aa&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://magic-edit.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/magic-research/magic-edit&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/magic-research/magic-edit.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;StableVideo: Text-driven Consistency-aware Diffusion Video Editing&lt;/strong&gt; (18 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2023] Wenhao Chai, Xun Guo, Gaoang Wang, et al.&lt;/summary&gt;Wenhao Chai, Xun Guo, Gaoang Wang, Yan Lu. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.09592&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/05cbac9a5101f47a6fabad72398616506572c9fa&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-41-blue.svg?paper=05cbac9a5101f47a6fabad72398616506572c9fa&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rese1f/StableVideo&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/rese1f/StableVideo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CoDeF: Content Deformation Fields for Temporally Consistent Video Processing&lt;/strong&gt; (15 Aug 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Hao Ouyang, Qiuyu Wang, Yuxi Xiao, et al.&lt;/summary&gt;Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.07926&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c2d65fc3a7fde3f7662c6ef9448e5737d7e5551f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-28-blue.svg?paper=c2d65fc3a7fde3f7662c6ef9448e5737d7e5551f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://qiuyu96.github.io/CoDeF/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/qiuyu96/CoDeF&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/qiuyu96/CoDeF.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TokenFlow: Consistent Diffusion Features for Consistent Video Editing&lt;/strong&gt; (19 Jul 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Michal Geyer, Omer Bar-Tal, Shai Bagon, et al.&lt;/summary&gt;Michal Geyer, Omer Bar-Tal, Shai Bagon, Tali Dekel. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.10373&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4761f173965195798cd3046ef4af608a83504e4d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-80-blue.svg?paper=4761f173965195798cd3046ef4af608a83504e4d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://diffusion-tokenflow.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/omerbt/TokenFlow&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/omerbt/TokenFlow.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation&lt;/strong&gt; (13 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shuai Yang, Yifan Zhou, Ziwei Liu, et al.&lt;/summary&gt;Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.07954&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1e09b83fe064826a9a1ac61a7bdc00f26be41aee&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-74-blue.svg?paper=1e09b83fe064826a9a1ac61a7bdc00f26be41aee&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.mmlab-ntu.com/project/rerender/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/williamyang1991/Rerender_A_Video.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing&lt;/strong&gt; (26 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Min Zhao, Rongzhen Wang, Fan Bao, et al.&lt;/summary&gt;Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, Jun Zhu. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.17098&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/14acc36d8c87f31f8dcbbf8433b91af70a2a516a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=14acc36d8c87f31f8dcbbf8433b91af70a2a516a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/controlvideo/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/thu-ml/controlvideo&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/thu-ml/controlvideo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts&lt;/strong&gt; (15 May 2023) Michal Geyer, Omer Bar-Tal, Shai Bagon, Tali Dekel.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.08850&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5f51eda9f7abddca027941d50fb0b6bf6f508eff&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-13-blue.svg?paper=5f51eda9f7abddca027941d50fb0b6bf6f508eff&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://make-a-protagonist.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/HeliosZhao/Make-A-Protagonist&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/HeliosZhao/Make-A-Protagonist.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pix2Video: Video Editing using Image Diffusion&lt;/strong&gt; (22 Mar 2023)&lt;br&gt; [ICCV 2023] Ceylan, Duygu, Chun-Hao P. Huang, and Niloy J. Mitra.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.12688&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/32a3c2fbd3e733bd0eea938517fec2ff8dc7c701&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-92-blue.svg?paper=32a3c2fbd3e733bd0eea938517fec2ff8dc7c701&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://duyguceylan.github.io/pix2video.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/duyguceylan/pix2video&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/duyguceylan/pix2video.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;FateZero: Fusing Attentions for Zero-shot Text-based Video Editing&lt;/strong&gt; (16 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2023] Chenyang Qi, Xiaodong Cun, Yong Zhang, et al.&lt;/summary&gt;Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, Qifeng Chen. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.09535&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/14ccb8bcceb6de10eda6ad08bec242a4f2946497&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-133-blue.svg?paper=14ccb8bcceb6de10eda6ad08bec242a4f2946497&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fate-zero-edit.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ChenyangQiQi/FateZero&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ChenyangQiQi/FateZero.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Video-P2P: Video Editing with Cross-attention Control&lt;/strong&gt; (8 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shaoteng Liu, Yuechen Zhang, Wenbo Li, et al.&lt;/summary&gt;Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, Jiaya Jia. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.04761&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6283502d6900a0b403e2454b1cb1cf16ddefd5a7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-81-blue.svg?paper=6283502d6900a0b403e2454b1cb1cf16ddefd5a7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://video-p2p.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ShaoTengLiu/Video-P2P&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ShaoTengLiu/Video-P2P.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dreamix: Video Diffusion Models are General Video Editors&lt;/strong&gt; (2 Feb 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Eyal Molad, Eliahu Horwitz, Dani Valevski, et al.&lt;/summary&gt;Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, Yedid Hoshen. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.01329&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9758ddd6ffbaac75aa0447a9664e6989811a05e2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-107-blue.svg?paper=9758ddd6ffbaac75aa0447a9664e6989811a05e2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dreamix-video-editing.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation&lt;/strong&gt; (22 Dec 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2023] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, et al.&lt;/summary&gt;Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.11565&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1367dcff4ccb927a5e95c452041288b3f0dd0eff&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-275-blue.svg?paper=1367dcff4ccb927a5e95c452041288b3f0dd0eff&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fate-zero-edit.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://tuneavideo.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/showlab/Tune-A-Video.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformers&lt;/strong&gt; (2 Apr 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2022] Tsu-Jui Fu, Xin Eric Wang, Scott T. Grafton, et al.&lt;/summary&gt;Tsu-Jui Fu, Xin Eric Wang, Scott T. Grafton, Miguel P. Eckstein, William Yang Wang. &#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.01122&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/81349524489f8ba0812ac2529eac92ec45959782&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-9-blue.svg?paper=81349524489f8ba0812ac2529eac92ec45959782&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3D Editing&lt;/h2&gt; &#xA;&lt;h3&gt;üîÖ LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code&lt;/strong&gt; (2 Mar 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Ziniu Hu, Ahmet Iscen, Aashi Jain, et al. &lt;/summary&gt;Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, Alireza Fathi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.01248v1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;3D-GPT: Procedural 3D MODELING WITH LARGE LANGUAGE MODELS&lt;/strong&gt; (19 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Chunyi Sun*, Junlin Han*, Weijian Deng, et al. &lt;/summary&gt;Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, Stephen Gould&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.12945&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/588930cdd801f335b5e524d13f99aa94136a20a0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-7-blue.svg?paper=588930cdd801f335b5e524d13f99aa94136a20a0&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Chuny1/3DGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Chuny1/3DGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Non-LLM-based (Clip/T5)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Paint3D: Paint Anything 3D with Lighting-Less Texture Diffusion Models&lt;/strong&gt; (16 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xianfang Zeng, Xin Chen, Zhongqi Qi, et al.&lt;/summary&gt;Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, Gang Yu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.13913&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e90883da4ee8c947a8b97422c95bde905a257a74&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-7-blue.svg?paper=e90883da4ee8c947a8b97422c95bde905a257a74&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenTexture/Paint3D&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenTexture/Paint3D?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation&lt;/strong&gt; (16 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Dale Decatur, Itai Lang, Kfir Aberman, et al.&lt;/summary&gt;Dale Decatur, Itai Lang, Kfir Aberman, Rana Hanocka&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.09571&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/496bdd2804a231a3336463fca8e0a4c6a46f0304&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=496bdd2804a231a3336463fca8e0a4c6a46f0304&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/threedle/3d-paintbrush&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/threedle/3d-paintbrush?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields&lt;/strong&gt; (23 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Hyeonseop Song, Seokhun Choi, Hoseok Do, et al. &lt;/summary&gt;Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee, Taehyeong Kim&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.11974&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/bf7f31e07d9b128a0f555c275bc3fdb851f725b8&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-9-blue.svg?paper=bf7f31e07d9b128a0f555c275bc3fdb851f725b8&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field&lt;/strong&gt; (23 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2023] Chong Bao, Yinda Zhang, Bangbang Yang, et al.&lt;/summary&gt;Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, Zhaopeng Cui&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.13277&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/222c47b81fe04598fd84fe8b9a43f694415ec7e9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-44-blue.svg?paper=222c47b81fe04598fd84fe8b9a43f694415ec7e9&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zju3dv/SINE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zju3dv/SINE?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TextDeformer: Geometry Manipulation using Text Guidance&lt;/strong&gt; (26 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt; [TVCG 2022] William Gao, Noam Aigerman, Thibault Groueix, et al.&lt;/summary&gt;William Gao, Noam Aigerman, Thibault Groueix, Vladimir G. Kim, Rana Hanocka&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.13348&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4974186c3b5b50112cfd909de115d5fbe25411fd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-21-blue.svg?paper=4974186c3b5b50112cfd909de115d5fbe25411fd&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/threedle/TextDeformer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/threedle/TextDeformer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&lt;/strong&gt; (22 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[SIGGRAPH Asia 2023] Ayaan Haque, Matthew Tancik, Alexei A. Efros, et al. &lt;/summary&gt;Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.12789&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/26c22380282a00166273038bc5ba785d845d61ad&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-131-blue.svg?paper=26c22380282a00166273038bc5ba785d845d61ad&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ayaanzhaque/instruct-nerf2nerf&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ayaanzhaque/instruct-nerf2nerf.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;DreamEditor: Text-Driven 3D Scene Editing with Neural Fields&lt;/strong&gt; (23 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[SIGGRAPH Asia 2023] Jingyu Zhuang, Chen Wang, Lingjie Liu, et al. &lt;/summary&gt;Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, Guanbin Li&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.13455&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/029f3e2c215edac138be26ade67b3d70b8f74dd7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-39-blue.svg?paper=029f3e2c215edac138be26ade67b3d70b8f74dd7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zjy526223908/DreamEditor&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/zjy526223908/DreamEditor.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SKED: Sketch-guided Text-based 3D Editing&lt;/strong&gt; (19 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCV 2023] Aryan Mikaeili, Or Perel, Mehdi Safaee, et al.&lt;/summary&gt;Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.10735&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6ebec1ece44daa090158ff2531d6fabb94a4e683&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-27-blue.svg?paper=6ebec1ece44daa090158ff2531d6fabb94a4e683&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/aryanmikaeili/SKED&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/aryanmikaeili/SKED.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields&lt;/strong&gt; (22 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[ICCVW 2023] Ori Gordon, Omri Avrahami, Dani Lischinski.&lt;/summary&gt;Ori Gordon, Omri Avrahami, Dani Lischinski&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.12760&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/3a5d4352d3dd53148a9544233bb59f88d2504910&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-12-blue.svg?paper=3a5d4352d3dd53148a9544233bb59f88d2504910&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ClipFace: Text-guided Editing of Textured 3D Morphable Modelssting Neural Radiance Fields&lt;/strong&gt; (2 Dec 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[SIGGRAPH 2023] Shivangi Aneja, Justus Thies, Angela Dai, et al. &lt;/summary&gt;Shivangi Aneja, Justus Thies, Angela Dai, Matthias Nie√üner&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.01406&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f21e8eddf42580d1f38a11ec5acd8891c0454a1f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-31-blue.svg?paper=f21e8eddf42580d1f38a11ec5acd8891c0454a1f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/cassiePython/CLIPNeRF&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/cassiePython/CLIPNeRF.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fieldsadiance Fields&lt;/strong&gt; (9 Dec 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2022] Can Wang, Menglei Chai, Mingming He, et al. &lt;/summary&gt;Can Wang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.05139&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/0483be6c3ec6cd41ffe248f86effc7468d3ac7be&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-244-blue.svg?paper=0483be6c3ec6cd41ffe248f86effc7468d3ac7be&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shivangi-aneja/ClipFace&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/shivangi-aneja/ClipFace.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Audio Editing&lt;/h2&gt; &#xA;&lt;h3&gt;üîÖ LLM-based&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing&lt;/strong&gt; (19 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yixiao Zhang, Akira Maezawa, Gus Xia, et al.&lt;/summary&gt;Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto, Simon Dixon&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.12404&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/cca4218dd7c10c1614bbd84aa7cd7e00027bdc7c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-3-blue.svg?paper=cca4218dd7c10c1614bbd84aa7cd7e00027bdc7c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://sites.google.com/view/loop-copilot&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ldzhangyx/loop-copilot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ldzhangyx/loop-copilot/.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;UniAudio: An Audio Foundation Model Toward Universal Audio Generation&lt;/strong&gt; (1 Oct 2023)&lt;br&gt; Dongchao Yang, Jinchuan Tian, Xu Tan&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.00704&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/74bfbbb7307a7af2686043ea97ab8b34cb062ba8&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-15-blue.svg?paper=74bfbbb7307a7af2686043ea97ab8b34cb062ba8&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://dongchaoyang.top/UniAudio_demo/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yangdongchao/UniAudio&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yangdongchao/UniAudio.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Non-LLM-based (Clip/T5)&lt;/h3&gt; &#xA;&lt;h1&gt;üìç Multimodal Agents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing&lt;/strong&gt; (1 Nov 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, et al.&lt;/summary&gt; Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, Chunyuan Li&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.00571&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c020f15be1dee20f9e2e0c5a6f05f272b5508325&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-7-blue.svg?paper=c020f15be1dee20f9e2e0c5a6f05f272b5508325&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llava-vl.github.io/llava-interactive&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LLaVA-VL/LLaVA-Interactive-Demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/LLaVA-VL/LLaVA-Interactive-Demo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llavainteractive.ngrok.app/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Tags:&lt;/strong&gt; &lt;code&gt;Image Chat&lt;/code&gt; &lt;code&gt;Image Segmentation&lt;/code&gt;, &lt;code&gt;Image Generation&lt;/code&gt; &lt;code&gt;Image Editing&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ControlLLM: Augment Language Models with Tools by Searching on Graphs&lt;/strong&gt; (26 Oct 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, et al.&lt;/summary&gt;Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Ziheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, Wenhai Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.17796&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/288e7224d53d68669eb67f2496e068dc965c639e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-11-blue.svg?paper=288e7224d53d68669eb67f2496e068dc965c639e&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://controlllm.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/ControlLLM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/ControlLLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cllm.opengvlab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Tags:&lt;/strong&gt; &lt;code&gt;Image Understanding&lt;/code&gt; &lt;code&gt;Image Generation&lt;/code&gt; &lt;code&gt;Image Editing&lt;/code&gt; &lt;code&gt;Video Understanding&lt;/code&gt; &lt;code&gt;Video Generation&lt;/code&gt; &lt;code&gt;Video Editing&lt;/code&gt; &lt;code&gt;Audio Understanding&lt;/code&gt; &lt;code&gt;Audio Generation&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ImageBind-LLM: Multi-modality Instruction Tuning&lt;/strong&gt; (7 Sep 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiaming Han, Renrui Zhang, Wenqi Shao, et al.&lt;/summary&gt;Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, Xudong Lu, Shuai Ren, Yafei Wen, Xiaoxin Chen, Xiangyu Yue, Hongsheng Li, Yu Qiao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.03905&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-33-blue.svg?paper=54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/LLaMA-Adapter&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/LLaMA-Adapter.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Modalities:&lt;/strong&gt; &lt;code&gt;text&lt;/code&gt; &lt;code&gt;image&lt;/code&gt; &lt;code&gt;video&lt;/code&gt; &lt;code&gt;audio&lt;/code&gt; &lt;code&gt;point cloud&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models&lt;/strong&gt; (2 Sep 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Chenliang Li, Hehong Chen, Ming Yan, et al.&lt;/summary&gt;Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang, Fei Huang, Jingren Zhou&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.00986&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e2f1f04f648a8863d11439aa4c80ee65d6caccda&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=e2f1f04f648a8863d11439aa4c80ee65d6caccda&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/modelscope/modelscope-agent&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/modelscope/modelscope-agent.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language&lt;/strong&gt; (9 May 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zhaoyang Liu, Yinan He, Wenhai Wang, et al.&lt;/summary&gt;Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, Yu Qiao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.05662&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-40-blue.svg?paper=54a8b153ed04a872da878d695239bdc413dc782c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/InternGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/InternGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://igpt.opengvlab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Condition Modality:&lt;/strong&gt; &lt;code&gt;text&lt;/code&gt; &lt;code&gt;image&lt;/code&gt; &lt;code&gt;video&lt;/code&gt; &lt;code&gt;audio&lt;/code&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face&lt;/strong&gt; (30 Mar 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yongliang Shen, Kaitao Song, Xu Tan, et al.&lt;/summary&gt;Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.17580&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-413-blue.svg?paper=d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/JARVIS&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/microsoft/HuggingGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/strong&gt; (8 Mar 2023) &lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Chenfei Wu, Shengming Yin, Weizhen Qi, et al.&lt;/summary&gt;Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-337-blue.svg?paper=af997821231898a5f8d0fd78dad4eec526acabe5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/moymix/TaskMatrix&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/microsoft/visual_chatgpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AutoGPT: build &amp;amp; use AI agents&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://news.agpt.co/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Significant-Gravitas/AutoGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Significant-Gravitas/AutoGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìç Multimodal Understanding with LLMs&lt;/h1&gt; &#xA;&lt;h2&gt;Image Understanding&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks&lt;/strong&gt; (21 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zhe Chen, Jiannan Wu, Wenhai Wang, et al.&lt;/summary&gt;Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.14238&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6a33e58ef961a3a0a5657518b2be86395eb7c8d0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-10-blue.svg?paper=6a33e58ef961a3a0a5657518b2be86395eb7c8d0&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/InternVL&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/InternVL.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://internvl.opengvlab.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models&lt;/strong&gt; (28 Nov 2023)&lt;br&gt; Yanwei Li, Chengyao Wang, Jiaya Jia&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.17043&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/486c2df78cbb770a90a55f7fa3fe19102fba2c24&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=486c2df78cbb770a90a55f7fa3fe19102fba2c24&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llama-vid.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/LLaMA-VID&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://103.170.5.190:7864/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;CogVLM: Visual Expert for Pretrained Language Models&lt;/strong&gt; (6 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Weihan Wang, Qingsong Lv, Wenmeng Yu, et al.&lt;/summary&gt;Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, Jie Tang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.03079&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/3bf842dec99016da2d309ea8cbd7e25343032317&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=3bf842dec99016da2d309ea8cbd7e25343032317&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/THUDM/CogVLM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/THUDM/CogVLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;http://36.103.203.44:7861/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning&lt;/strong&gt; (14 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jun Chen, Deyao Zhu, Xiaoqian Shen, et al.&lt;/summary&gt;Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.09478&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1ddbd08ad8cf22a5c66c4242194c4286328533bf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-97-blue.svg?paper=1ddbd08ad8cf22a5c66c4242194c4286328533bf&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://minigpt-v2.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue&lt;/strong&gt; (21 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Weihao Gao, Zhuo Deng, Zhiyuan Niu, et al.&lt;/summary&gt;Weihao Gao, Zhuo Deng, Zhiyuan Niu, Fuju Rong, Chucheng Chen, Zheng Gong, Wenze Zhang, Daimin Xiao, Fang Li, Zhenjie Cao, Zhaoyi Ma, Wenbin Wei, Lan Ma&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.12174&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/0f8d12775a4685575f1489796b5dee9e11fbdfb5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-7-blue.svg?paper=0f8d12775a4685575f1489796b5dee9e11fbdfb5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://minigpt-v2.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ML-AILab/OphGLM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ML-AILab/OphGLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition&lt;/strong&gt; (26 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Pan Zhang, Xiaoyi Dong, Bin Wang, et al.&lt;/summary&gt; Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Haodong Duan, Songyang Zhang, Shuangrui Ding, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.15112&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c1e450284e7d6cac1855330a1197df8537df653f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-48-blue.svg?paper=c1e450284e7d6cac1855330a1197df8537df653f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/InternLM/InternLM-XComposer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;[LaVIT] Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization&lt;/strong&gt; (9 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yang Jin, Kun Xu, Kun Xu, et al.&lt;/summary&gt;Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, Yadong Mu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.04669&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/bcac614f9774488447221ebb4f16f05e3975ec1e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=bcac614f9774488447221ebb4f16f05e3975ec1e&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/jy0205/LaVIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/jy0205/LaVIT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond&lt;/strong&gt; (24 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jinze Bai, Shuai Bai, Shusheng Yang, et al.&lt;/summary&gt;Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.12966&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/fc6a2f7478f68adefd69e2071f27e38aa1647f2f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-69-blue.svg?paper=fc6a2f7478f68adefd69e2071f27e38aa1647f2f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/QwenLM/Qwen-VL/raw/master/TUTORIAL.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/QwenLM/Qwen-VL&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/QwenLM/Qwen-VL.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks&lt;/strong&gt; (18 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023] Wenhai Wang, Zhe Chen, Xiaokang Chen, et al.&lt;/summary&gt;Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.11175&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-134-blue.svg?paper=42a30dc5470f54ec249f25d3c31e05d7c376c8e3&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/VisionLLM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/VisionLLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/InternGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning&lt;/strong&gt; (11 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Wenliang Dai, Junnan Li, Dongxu Li, et al.&lt;/summary&gt;Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.06500&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8bd6a2a89503be083176f2cc26fabedb79238cbd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-474-blue.svg?paper=8bd6a2a89503be083176f2cc26fabedb79238cbd&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/QwenLM/Qwen-VL&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/QwenLM/Qwen-VL.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models&lt;/strong&gt; (20 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Deyao Zhu, Jun Chen, Xiaoqian Shen, et al.&lt;/summary&gt;Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.10592&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ca6a2bc279be5a3349a22bfd6866ed633d18734b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-649-blue.svg?paper=ca6a2bc279be5a3349a22bfd6866ed633d18734b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://minigpt-4.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Vision-CAIR/MiniGPT-v2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Visual Instruction Tuning&lt;/strong&gt; (17 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023 (Oral)] Liu, Haotian, et al.&lt;/summary&gt;Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Video Understanding&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PLLaVA: Parameter-free LLaVA Extension from Images to Videos for Video Dense Captioning&lt;/strong&gt; (25 Apr 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Lin Xu, Yilin Zhao, Daquan Zhou, et al.&lt;/summary&gt;Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, Jiashi Feng&#xA;  &lt;/details&gt; &lt;a href=&#34;https://pllava.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/PLLaVA-%3A-Parameter-free-LLaVA-Extension-from-Images-Xu-Zhao/9d29da83aba362c728c36f4dea9dde678ae3e2b2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=9d29da83aba362c728c36f4dea9dde678ae3e2b2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/magic-research/PLLaVA?tab=readme-ov-file&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/magic-research/PLLaVA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MovieChat: From Dense Token to Sparse Memory for Long Video Understanding&lt;/strong&gt; (3 Dec 2023) &lt;br&gt; Enxin, Song, et al. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.16449&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6f9b7c8cde1be2e62a503c31cac883c6d44c9d0d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-16-blue.svg?paper=6f9b7c8cde1be2e62a503c31cac883c6d44c9d0d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/rese1f/MovieChat&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/rese1f/MovieChat.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models&lt;/strong&gt; (28 Nov 2023) &lt;br&gt; Yanwei, Li, et al. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.17043&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/486c2df78cbb770a90a55f7fa3fe19102fba2c24&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=486c2df78cbb770a90a55f7fa3fe19102fba2c24&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dvlab-research/LLaMA-VID&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/dvlab-research/LLaMA-VID.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models&lt;/strong&gt; (27 Nov 2023)&lt;br&gt; Ning, Munan, et al. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.16103&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/b037bb09aa162d8a543e64ec777ca0edc732d2af&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=b037bb09aa162d8a543e64ec777ca0edc732d2af&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-Bench&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/Video-Bench.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PG-Video-LLaVA: Pixel Grounding Large Video-Language Models&lt;/strong&gt; (22 Nov 2023)&lt;br&gt; Munasinghe, Shehan, et al. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.13435&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4edbb942c2d20a6f5a4e3caa763a9761be953231&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=4edbb942c2d20a6f5a4e3caa763a9761be953231&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mbzuai-oryx/Video-LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mbzuai-oryx/Video-LLaVA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mbzuai-oryx.github.io/Video-LLaVA/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Video-LLaVA: Learning United Visual Representation by Alignment Before Projection&lt;/strong&gt; (16 Nov 2023)&lt;br&gt; Lin, Bin, et al. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.10122&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/107fb6eec2febbae12db29bf3e311aaf5680027c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-19-blue.svg?paper=107fb6eec2febbae12db29bf3e311aaf5680027c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/Video-LLaVA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/LanguageBind/Video-LLaVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding&lt;/strong&gt; (14 Nov 2023)&lt;br&gt; Jin, Peng, et al. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.08046&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/aad3d2e690f6c73f04a14622ceff51464bbc560e&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-11-blue.svg?paper=aad3d2e690f6c73f04a14622ceff51464bbc560e&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Chat-UniVi&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/Chat-UniVi.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Chat-UniVi/Chat-UniVi&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding&lt;/strong&gt; (5 Jun 2023)&lt;br&gt; Zhang, Hang, Xin Li, and Lidong Bing. EMNLP 2023&#39;s demo track. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.02858&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5d321194696f1f75cf9da045e6022b2f20ba5b9c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-176-blue.svg?paper=5d321194696f1f75cf9da045e6022b2f20ba5b9c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/DAMO-NLP-SG/Video-LLaMA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?&lt;/strong&gt; (31 Jul 2023)&lt;br&gt; Zhao, Qi, et al.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.16368&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6024f320e0a5b9b8fc29b86903aa9a96956b26dd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=6024f320e0a5b9b8fc29b86903aa9a96956b26dd&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://brown-palm.github.io/AntGPT/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Valley: Video Assistant with Large Language model Enhanced ability&lt;/strong&gt; (12 Jun 2023)&lt;br&gt; Luo, Ruipu, et al.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.07207&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4c4d176c6e28f48041f215d563f6ee8633534cff&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-42-blue.svg?paper=4c4d176c6e28f48041f215d563f6ee8633534cff&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://valley-vl.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RupertLuo/Valley&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RupertLuo/Valley.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models&lt;/strong&gt; (8 Jun 2023)&lt;br&gt; Muhammad Maaz, Hanoona Rasheed, Salman Khan, et al.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.05424&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/bf7025a2e5dbb3c09deae02a1aa98a256ca559e2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-107-blue.svg?paper=bf7025a2e5dbb3c09deae02a1aa98a256ca559e2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VideoChat: Chat-Centric Video Understanding&lt;/strong&gt; (10 May 2023)&lt;br&gt; Li, KunChang, et al. &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.06355&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-133-blue.svg?paper=d48cb91b9e555194f7494c4d4bb9815021d3ee45&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenGVLab/Ask-Anything&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;VideoLLM: Modeling Video Sequence with Large Language Models&lt;/strong&gt; (22 May 2023)&lt;br&gt; Chen, Guo, et al.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.13292&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f9bfc6d9ba1665b73af3323d46c7642b852759ef&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-33-blue.svg?paper=f9bfc6d9ba1665b73af3323d46c7642b852759ef&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/cg1177/VideoLLM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/cg1177/VideoLLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Learning video embedding space with Natural Language Supervision&lt;/strong&gt; (25 Mar 2023)&lt;br&gt; Uppala, Phani Krishna, Shriti Priya, and Vaidehi Joshi.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.14584&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4e54a45d2118b61ae1baec07308af3fdd2c48759&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=4e54a45d2118b61ae1baec07308af3fdd2c48759&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3D Understanding&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning&lt;/strong&gt; (30 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR2024]Sijin Chen, Xin Chen, Chi Zhang, et al. &lt;/summary&gt;[CVPR 2024] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, Tao Chen&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.18651&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/fc53f8f3a84f1fc4993689d8f98cf6551d07a22d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=fc53f8f3a84f1fc4993689d8f98cf6551d07a22d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Open3DA/LL3DA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Open3DA/LL3DA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding&lt;/strong&gt; (21 Dec 2023)&lt;br&gt; Senqiao Yang*, Jiaming Liu*, Ray Zhang, et al.&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.14074&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5edf706467dc76cd09319592d18db0ad4e1fb64d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=5edf706467dc76cd09319592d18db0ad4e1fb64d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;3D-LLM: Injecting the 3D World into Large Language Models&lt;/strong&gt; (24 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023 Spotlight] Yining Hong, Haoyu Zhen, Peihao Chen, et al.&lt;/summary&gt;Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, Chuang Gan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.12981&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/7637ed79d30d0139901175ae4abedd822c217ab4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-48-blue.svg?paper=7637ed79d30d0139901175ae4abedd822c217ab4&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/UMass-Foundation-Model/3D-LLM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/UMass-Foundation-Model/3D-LLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PointLLM: Empowering Large Language Models to Understand Point Clouds&lt;/strong&gt; (31 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[NeurIPS 2023 Spotlight] Runsen Xu, Xiaolong Wang, Tai Wang, et al.&lt;/summary&gt;Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/pdf/2308.16911.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6bcc6ab9c28805d4067e99b2cdc7524550fe80e1&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-31-blue.svg?paper=6bcc6ab9c28805d4067e99b2cdc7524550fe80e1&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/OpenRobotLab/PointLLM&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenRobotLab/PointLLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;PointCLIP: Point Cloud Understanding by CLIP&lt;/strong&gt; (31 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[CVPR 2022] Renrui Zhang, Ziyu Guo, Wei Zhang,, et al. &lt;/summary&gt;Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, Hongsheng Li&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/pdf/2112.02413.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f3ce9ba3fcec362b70263a7ed63d9404975496a0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-215-blue.svg?paper=f3ce9ba3fcec362b70263a7ed63d9404975496a0&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ZrrSkywalker/PointCLIP&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ZrrSkywalker/PointCLIP.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Audio Understanding&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action&lt;/strong&gt; (28 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiasen Lu, Christopher Clark, Sangho Lee, et al.&lt;/summary&gt;Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, Aniruddha Kembhavi&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.17172&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6c64ddd2190909de2c680dd18abc9b92e80c39f9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-8-blue.svg?paper=6c64ddd2190909de2c680dd18abc9b92e80c39f9&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://unified-io-2.allenai.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/unified-io-2&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/allenai/unified-io-2.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;M2UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models&lt;/strong&gt; (19 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, et al.&lt;/summary&gt;Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, Ying Shan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.11255&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/1e84d7c45f70038574fcdb7bc1b20da9b348a092&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=1e84d7c45f70038574fcdb7bc1b20da9b348a092&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crypto-code.github.io/M2UGen-Demo/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shansongliu/M2UGen&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/shansongliu/M2UGen.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/M2UGen/M2UGen-Demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models&lt;/strong&gt; (14 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yunfei Chu, Jin Xu, Xiaohuan Zhou, et al.&lt;/summary&gt;Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, Jingren Zhou&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.07919&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f90595f99a0c66d2bb6d0f230f17c7cd8c58f44d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-14-blue.svg?paper=f90595f99a0c66d2bb6d0f230f17c7cd8c58f44d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/QwenLM/Qwen-Audio&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;SALMONN: Towards Generic Hearing Abilities for Large Language Models&lt;/strong&gt; (20 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Changli Tang, Wenyi Yu, Guangzhi Sun, et al.&lt;/summary&gt;Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.13289&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f72be31de9f9a09d4410fd38bc717efe43444827&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-9-blue.svg?paper=f72be31de9f9a09d4410fd38bc717efe43444827&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/tsinghua-ee/SALMONN-7B-gradio&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/bytedance/SALMONN&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/bytedance/SALMONN.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/tsinghua-ee/SALMONN&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models&lt;/strong&gt; (18 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Dingyao Yu, Kaitao Song, Peiling Lu, et al.&lt;/summary&gt;Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, Jiang Bian&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.11954&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/beaf64df85f8204b8cd89a7f46827608e6d16922&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=beaf64df85f8204b8cd89a7f46827608e6d16922&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/muzic/tree/main/musicagent&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/muzic/tree/main/musicagent.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Llark: A multimodal foundation model for music&lt;/strong&gt; (11 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Josh Gardner, Simon Durand, Daniel Stoller, et al.&lt;/summary&gt;Josh Gardner, Simon Durand, Daniel Stoller, Rachel M. Bittner&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.07160&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/86e75cf15a838ed7d672fb114beff727d7210ca5&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=86e75cf15a838ed7d672fb114beff727d7210ca5&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://storage.googleapis.com/music2text-public/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/spotify-research/llark&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/spotify-research/llark.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT&lt;/strong&gt; (7 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiaming Wang, Zhihao Du, Qian Chen, et al.&lt;/summary&gt;Jiaming Wang, Zhihao Du, Qian Chen, Yunfei Chu, Zhifu Gao, Zerui Li, Kai Hu, Xiaohuan Zhou, Jin Xu, Ziyang Ma, Wen Wang, Siqi Zheng, Chang Zhou, Zhijie Yan, Shiliang Zhang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.04673&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ffa05cb5504ba08254f498223f613b3ebcf87692&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-13-blue.svg?paper=ffa05cb5504ba08254f498223f613b3ebcf87692&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://lauragpt.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation&lt;/strong&gt; (29 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shih-Lun Wu, Xuankai Chang, Gordon Wichern, et al.&lt;/summary&gt;Shih-Lun Wu, Xuankai Chang, Gordon Wichern, Jee-weon Jung, Fran√ßois Germain, Jonathan Le Roux, Shinji Watanabe&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.17352&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8f0a24d1678e4d0e584b0932196cd257d5c53c7d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=8f0a24d1678e4d0e584b0932196cd257d5c53c7d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Connecting Speech Encoder and Large Language Model for ASR&lt;/strong&gt; (25 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Wenyi Yu, Changli Tang, Guangzhi Sun, et al.&lt;/summary&gt;Wenyi Yu, Changli Tang, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.13963&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5596bd3e26ec2207666ec1ff3db4415d212f14b9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-11-blue.svg?paper=5596bd3e26ec2207666ec1ff3db4415d212f14b9&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Can Whisper perform speech-based in-context learning&lt;/strong&gt; (13 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Siyin Wang, Chao-Han Huck Yang, Ji Wu, et al.&lt;/summary&gt;Siyin Wang, Chao-Han Huck Yang, Ji Wu, Chao Zhang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.07081&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/3a944ddba8b6fbaaac36126fc955f181f8b8b06a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-8-blue.svg?paper=3a944ddba8b6fbaaac36126fc955f181f8b8b06a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Music understanding LLaMA: Advancing text-to-music generation with question answering and captioning&lt;/strong&gt; (22 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, et al.&lt;/summary&gt;Shansong Liu, Atin Sakkeer Hussain, Chenshuo Sun, Ying Shan&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.11276&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/a33b437618be733fea7176bd98e18b6362af0838&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-11-blue.svg?paper=a33b437618be733fea7176bd98e18b6362af0838&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crypto-code.github.io/MU-LLaMA-Demo/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/crypto-code/MU-LLaMA&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/crypto-code/MU-LLaMA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/mu-llama/MusicQA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;On decoder-only architecture for speech-to-text and large language model integration&lt;/strong&gt; (8 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jian Wu, Yashesh Gaur, Zhuo Chen, et al.&lt;/summary&gt;Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, Yu Wu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.03917&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8e1868f84091272544cb4209c4ccaad7cc88af27&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-24-blue.svg?paper=8e1868f84091272544cb4209c4ccaad7cc88af27&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AudioPaLM: A Large Language Model That Can Speak and Listen&lt;/strong&gt; (22 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, et al.&lt;/summary&gt;Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal√°n Borsos, F√©lix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara Sainath, Johan Schalkwyk, Matt Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimiroviƒá, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, Christian Frank&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.12925&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/3efb81de24eb88017d6dbcf22cb4215084223fd8&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-69-blue.svg?paper=3efb81de24eb88017d6dbcf22cb4215084223fd8&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://google-research.github.io/seanet/audiopalm/examples/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface&lt;/strong&gt; (30 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yongliang Shen, Kaitao Song, Xu Tan, et al.&lt;/summary&gt;Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.17580&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-413-blue.svg?paper=d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/JARVIS&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/microsoft/HuggingGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sparks of Artificial General Intelligence: Early experiments with GPT-4&lt;/strong&gt; (22 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, et al.&lt;/summary&gt;S√©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.12712&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/574beee702be3856d60aa482ec725168fe64fc99&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1407-blue.svg?paper=574beee702be3856d60aa482ec725168fe64fc99&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Listen, Think, and Understand&lt;/strong&gt; (18 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yuan Gong, Hongyin Luo, Alexander H. Liu, et al.&lt;/summary&gt;Yuan Gong, Hongyin Luo, Alexander H. Liu, Leonid Karlinsky, James Glass&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.10790&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4bb0b12803791764d641a4cef1e0ce39cf049542&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-33-blue.svg?paper=4bb0b12803791764d641a4cef1e0ce39cf049542&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/yuangongfdu/LTU&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities&lt;/strong&gt; (18 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Dong Zhang, Shimin Li, Xin Zhang, et al.&lt;/summary&gt;Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.11000&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5cac6430bd379c9d2fe13137dfd6ae7721a2679f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-76-blue.svg?paper=5cac6430bd379c9d2fe13137dfd6ae7721a2679f&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://0nutation.github.io/SpeechGPT.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/0nutation/SpeechGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/0nutation/SpeechGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Audiogpt: Understanding and generating speech, music, sound, and talking head&lt;/strong&gt; (25 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Rongjie Huang, Mingze Li, Dongchao Yang, et al.&lt;/summary&gt;Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.12995&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8bc617c9139648d7a92991d70c671230bac7b2e2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-83-blue.svg?paper=8bc617c9139648d7a92991d70c671230bac7b2e2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/AIGC-Audio/AudioGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/AIGC-Audio/AudioGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Demo-EEAD0E&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìç Multimodal LLM Safety&lt;/h1&gt; &#xA;&lt;h2&gt;Attack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Jailbreaking gpt-4v via self-adversarial attacks with system prompts.&lt;/strong&gt; (20 Jan 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yuanwei Wu, Xiang Li, Yixin Liu, et al.&lt;/summary&gt;Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, Lichao Sun&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.09127.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/18a8b97d75a87e8fef07542d8875d4a62b553744&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-8-blue.svg?paper=18a8b97d75a87e8fef07542d8875d4a62b553744&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ThuCCSLab/lm-ssp&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ThuCCSLab/lm-ssp.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Defending chatgpt against jailbreak attack via self-reminders.&lt;/strong&gt; (1 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yueqi Xie, Jingwei Yi, Jiawei Shao, et al.&lt;/summary&gt;Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, Fangzhao Wu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e762f92273cd96f63b7788c0173b9b6450adedd7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-32-blue.svg?paper=e762f92273cd96f63b7788c0173b9b6450adedd7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/yjw1029/Self-Reminder&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yjw1029/Self-Reminder.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Misusing Tools in Large Language Models With Visual Adversarial Examples&lt;/strong&gt; (4 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xiaohan Fu, Zihan Wang, Shuheng Li, et al.&lt;/summary&gt;Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.03185.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ac5b4df0e398ca48388330ac5c795b6fe708793c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-6-blue.svg?paper=ac5b4df0e398ca48388330ac5c795b6fe708793c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Image Hijacks: Adversarial Images can Control Generative Models at Runtime.&lt;/strong&gt; (18 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Luke Bailey, Euan Ong, Stuart Russell, et al.&lt;/summary&gt;Luke Bailey, Euan Ong, Stuart Russell, Scott Emmons&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.00236.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/5bdaadb84db0cbf72aaebda9f55f4288b63c6e9b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-22-blue.svg?paper=5bdaadb84db0cbf72aaebda9f55f4288b63c6e9b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/euanong/image-hijacks&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/euanong/image-hijacks.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Universal and Transferable Adversarial Attacks on Aligned Language Models&lt;/strong&gt; (27 Jul 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Andy Zou, Zifan Wang, Nicholas Carlini, et al.&lt;/summary&gt;Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, Matt Fredrikson&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.15043.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/47030369e97cc44d4b2e3cf1be85da0fd134904a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-309-blue.svg?paper=47030369e97cc44d4b2e3cf1be85da0fd134904a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/llm-attacks/llm-attacks&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/llm-attacks/llm-attacks.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prompt injection attack against llm-integrated applications&lt;/strong&gt; (8 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yi Liu, Gelei Deng, Yuekang Li, et al.&lt;/summary&gt;Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, Yang Liu&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2306.05499.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/db4cf9f6a653d5c15973e836c800ea47743251ae&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-77-blue.svg?paper=db4cf9f6a653d5c15973e836c800ea47743251ae&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/LLMSecurity/HouYi&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/LLMSecurity/HouYi.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Automatically Auditing Large Language Models via Discrete Optimization&lt;/strong&gt; (8 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Erik Jones, Anca Dragan, Aditi Raghunathan, et al.&lt;/summary&gt;Erik Jones, Anca Dragan, Aditi Raghunathan, Jacob Steinhardt&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.04381.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/2f94f03fdac62d05f0f416b7b3855d1f597afee9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-64-blue.svg?paper=2f94f03fdac62d05f0f416b7b3855d1f597afee9&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ejones313/auditing-llms&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ejones313/auditing-llms.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Poisoning Web-Scale Training Datasets is Practical&lt;/strong&gt; (20 Feb 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, et al.&lt;/summary&gt;Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, Florian Tram r&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.10149.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/2cf43a61d0937ad25f23eaef7c90253ab799b3c7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-61-blue.svg?paper=2cf43a61d0937ad25f23eaef7c90253ab799b3c7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Exploiting programmatic behavior of llms: Dual-use through standard security attacks.&lt;/strong&gt; (11 Feb 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Daniel Kang, Xuechen Li, Ion Stoica, et al.&lt;/summary&gt;Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, Tatsunori Hashimoto&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.05733.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/0cf694b8f85ab2e11d45595de211a15cfbadcd22&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-94-blue.svg?paper=0cf694b8f85ab2e11d45595de211a15cfbadcd22&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ignore previous prompt: Attack techniques for language models&lt;/strong&gt; (17 Nov 2022)&lt;br&gt; F bio Perez, Ian Ribeiro (NeurIPS 2022 Workshop)&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2211.09527.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-151-blue.svg?paper=9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/agencyenterprise/PromptInject&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/agencyenterprise/PromptInject.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Universal Adversarial Triggers for Attacking and Analyzing NLP&lt;/strong&gt; (20 Aug 2019)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Eric Wallace, Shi Feng, Nikhil Kandpal, et al. (EMNLP 2019)&lt;/summary&gt;Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1908.07125.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/18a1c21f35153c45d0ef30c564bffb7d70a13ccc&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-613-blue.svg?paper=18a1c21f35153c45d0ef30c564bffb7d70a13ccc&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Eric-Wallace/universal-triggers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Eric-Wallace/universal-triggers.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Adversarial Examples for Evaluating Reading Comprehension Systems&lt;/strong&gt; (23 Jul 2017)&lt;br&gt; Robin Jia, Percy Liang (EMNLP 2017)&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/1707.07328.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ffb949d3493c3b2f3c9acf9c75cb03938933ddf0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1445-blue.svg?paper=ffb949d3493c3b2f3c9acf9c75cb03938933ddf0&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/robinjia/adversarial-squad&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/robinjia/adversarial-squad.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Defense and Detect&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Detecting and correcting hate speech in multimodal memes with large visual language model.&lt;/strong&gt; (12 Nov 2023)&lt;br&gt; Minh-Hao Van, Xintao Wu&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.06737.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/60f4dc690ea42fb77b04fc685e9d9c3a1e209319&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-5-blue.svg?paper=60f4dc690ea42fb77b04fc685e9d9c3a1e209319&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Detecting Pretraining Data from Large Language Models&lt;/strong&gt; (3 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Weijia Shi, Anirudh Ajith, Mengzhou Xia, et al.&lt;/summary&gt;Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, Luke Zettlemoyer&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.16789.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/3422d5e0cdfdc935d6a84a1e3d3f96659265fe3a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-20-blue.svg?paper=3422d5e0cdfdc935d6a84a1e3d3f96659265fe3a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/swj0419/detect-pretrain-code&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/swj0419/detect-pretrain-code.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Jailbreak and guard aligned language models with only few in-context demonstrations&lt;/strong&gt; (10 Oct 2023)&lt;br&gt; Zeming Wei, Yifei Wang, Yisen Wang&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.06387.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6b135e922a0c673aeb0b05c5aeecdb6c794791c6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-43-blue.svg?paper=6b135e922a0c673aeb0b05c5aeecdb6c794791c6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Smoothllm: Defending large language models against jailbreaking attacks.&lt;/strong&gt; (5 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Alexander Robey, Eric Wong, Hamed Hassani, et al.&lt;/summary&gt;Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.03684.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/8cf9b49698fdb1b754df2556576412a7b44929f6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-46-blue.svg?paper=8cf9b49698fdb1b754df2556576412a7b44929f6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/arobey1/smooth-llm&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/arobey1/smooth-llm.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;A Watermark for Large Language Models&lt;/strong&gt; (6 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;John Kirchenbauer, Jonas Geiping, Yuxin Wen, et al. (ICML 2023)&lt;/summary&gt;John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.10226.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/cb5b71a622aff47014d4f28a958679629a8b6363&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-199-blue.svg?paper=cb5b71a622aff47014d4f28a958679629a8b6363&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BrianPulfer/LMWatermark&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/BrianPulfer/LMWatermark.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models&lt;/strong&gt; (23 May 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yiting Qu, Xinyue Shen, Xinlei He, et al. (ACM CCS 2023)&lt;/summary&gt;Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, Yang Zhang&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.13873.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c9e548d72f5ad72215025602be36f72042219baf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-29-blue.svg?paper=c9e548d72f5ad72215025602be36f72042219baf&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/YitingQu/unsafe-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/YitingQu/unsafe-diffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;TRAK: Attributing Model Behavior at Scale&lt;/strong&gt; (3 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Sung Min Park, Kristian Georgiev, Andrew Ilyas, et al.&lt;/summary&gt;Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, Aleksander Madry&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2303.14186.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/4f2ae5fa2dc74af9c36ee57b359a4b3241006a92&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-32-blue.svg?paper=4f2ae5fa2dc74af9c36ee57b359a4b3241006a92&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/MadryLab/trak&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/MadryLab/trak.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Poisoning Web-Scale Training Datasets is Practical&lt;/strong&gt; (20 Feb 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, et al.&lt;/summary&gt;Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, Florian Tram r&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.10149.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/2cf43a61d0937ad25f23eaef7c90253ab799b3c7&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-61-blue.svg?paper=2cf43a61d0937ad25f23eaef7c90253ab799b3c7&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mitigating Inappropriate Degeneration in Diffusion Models&lt;/strong&gt; (9 Nov 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Patrick Schramowski, Manuel Brack, Bj?rn Deiseroth, et al. (CVPR 2023)&lt;/summary&gt;Patrick Schramowski, Manuel Brack, Bj?rn Deiseroth, Kristian Kersting&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.05105.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/0231f2aed9a96cb516242fb57f2cb63f5651c4d8&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-76-blue.svg?paper=0231f2aed9a96cb516242fb57f2cb63f5651c4d8&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ml-research/safe-latent-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ml-research/safe-latent-diffusion.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Extracting Training Data from Large Language Models&lt;/strong&gt; (15 Jun 2021)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Nicholas Carlini, Florian Tramer, Eric Wallace, et al.&lt;/summary&gt;Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2012.07805.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/df7d26339adf4eb0c07160947b9d2973c24911ba&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1005-blue.svg?paper=df7d26339adf4eb0c07160947b9d2973c24911ba&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/shreyansh26/Extracting-Training-Data-from-Large-Langauge-Models&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/shreyansh26/Extracting-Training-Data-from-Large-Langauge-Models.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Alignment&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Direct Preference Optimization: Your Language Model is Secretly a Reward Model&lt;/strong&gt; (13 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Rafael Rafailov, Archit Sharma, Eric Mitchell, et al.&lt;/summary&gt;Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.18290.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-450-blue.svg?paper=0d1c76d45afa012ded7ab741194baf142117c495&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Raft: Reward ranked fine tuning for generative foundation model alignment&lt;/strong&gt; (1 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Hanze Dong, Wei Xiong, Deepanshu Goyal, et al. (Transactions on Machine Learning Research (TMLR))&lt;/summary&gt;Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.06767.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/3ab661db57d924f4ff1706e05ac807873ca00e0a&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-116-blue.svg?paper=3ab661db57d924f4ff1706e05ac807873ca00e0a&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Better aligning text-to-image models with human preference&lt;/strong&gt; (22 Aug 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xiaoshi Wu, Keqiang Sun, Feng Zhu, et al. (ICCV 2023)&lt;/summary&gt;Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.14420.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/14c3cf58192774b9b6fc6188df99efd6ab5fc739&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-44-blue.svg?paper=14c3cf58192774b9b6fc6188df99efd6ab5fc739&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tgxs002/align_sd&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/tgxs002/align_sd.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Scalable agent alignment via reward modeling: a research direction&lt;/strong&gt; (19 Nov 2018)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jan Leike, David Krueger, Tom Everitt, et al.&lt;/summary&gt;Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, Shane Legg&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1811.07871.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c6f913e4baa7f2c85363c0625c87003ad3b3a14c&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-244-blue.svg?paper=c6f913e4baa7f2c85363c0625c87003ad3b3a14c&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Proximal policy optimization algorithms&lt;/strong&gt; (20 Jul 2017)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;John Schulman, Filip Wolski, Prafulla Dhariwal, et al.&lt;/summary&gt;John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/1707.06347.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-12282-blue.svg?paper=dce6f9d4017b1785979e7520fd0834ef8cf02f4b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/morikatron/PPO&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/morikatron/PPO.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Goat-bench: Safety insights to large multimodal models through meme-based social abuse.&lt;/strong&gt; (7 Jan 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Hongzhan Lin, Ziyang Luo, Bo Wang, et al.&lt;/summary&gt;Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, Jing Ma&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.01523.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/d98aa44f79fe798ad5ff0cac6e7bf32ee30bd156&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=d98aa44f79fe798ad5ff0cac6e7bf32ee30bd156&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/isXinLiu/MLLM-Safety-Collection&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/isXinLiu/MLLM-Safety-Collection.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tovilag: Your visual-language generative model is also an evildoer.&lt;/strong&gt; (13 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xinpeng Wang, Xiaoyuan Yi, Han Jiang, et al. (EMNLP 2023 Oral)&lt;/summary&gt;Xinpeng Wang, Xiaoyuan Yi, Han Jiang, Shanlin Zhou, Zhihua Wei, Xing Xie&#xA;  &lt;/details&gt;&lt;a href=&#34;http://export.arxiv.org/abs/2312.11523&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/10280c290825fc0b0c884e988f4f1dedb80e4e80&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-1-blue.svg?paper=10280c290825fc0b0c884e988f4f1dedb80e4e80&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/victorup/ToViLaG&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/victorup/ToViLaG.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Figstep: Jailbreaking large vision-language models via typographic visual prompts.&lt;/strong&gt; (13 Dec 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yichen Gong, Delong Ran, Jinyuan Liu, et al.&lt;/summary&gt;Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, Xiaoyun Wang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.05608.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/b78b5ce5f21f46d8149824463f8eebd6103d49aa&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-7-blue.svg?paper=b78b5ce5f21f46d8149824463f8eebd6103d49aa&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ThuCCSLab/FigStep&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ThuCCSLab/FigStep.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Query-relevant images jailbreak large multi-modal models.&lt;/strong&gt; (29 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Xin Liu, Yichen Zhu, Yunshi Lan, et al.&lt;/summary&gt;Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.17600.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/74423a9ee66085e74cd2b2e42303f28359c74eb6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=74423a9ee66085e74cd2b2e42303f28359c74eb6&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/isXinLiu/MM-SafetyBench&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/isXinLiu/MM-SafetyBench.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dress: Instructing large vision-language models to align and interact with humans via natural language feedback.&lt;/strong&gt; (16 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yangyi Chen, Karan Sikka, Michael Cogswell, et al.&lt;/summary&gt;Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2311.10081.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/391eaeb1092c2b145ff0e5a2fa61637a42921fce&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-9-blue.svg?paper=391eaeb1092c2b145ff0e5a2fa61637a42921fce&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Beavertails: Towards improved safety alignment of llm via a human-preference dataset&lt;/strong&gt; (7 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Jiaming Ji, Mickel Liu, Juntao Dai, et al. (NeurIPS 2023)&lt;/summary&gt;Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.04657.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/92930ed3560ea6c86d53cf52158bc793b089054d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-56-blue.svg?paper=92930ed3560ea6c86d53cf52158bc793b089054d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/GaryYufei/AlignLLMHumanSurvey&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/GaryYufei/AlignLLMHumanSurvey.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Can pre-trained vision and language models answer visual information-seeking questions?&lt;/strong&gt; (17 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yang Chen, Hexiang Hu, Yi Luan, et al. (EMNLP 2023)&lt;/summary&gt;Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, Ming-Wei Chang&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.11713.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/f890b4dfe915174b23db909b07c515d465eaeff2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-19-blue.svg?paper=f890b4dfe915174b23db909b07c515d465eaeff2&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/edchengg/infoseek_eval&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/edchengg/infoseek_eval.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Can language models be instructed to protect personal information?&lt;/strong&gt; (3 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Yang Chen, Ethan Mendes, Sauvik Das, et al.&lt;/summary&gt;Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, Alan Ritter&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2310.02224.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/2403c8e72a90d9c778970fc0812ecdcc58800c5d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-14-blue.svg?paper=2403c8e72a90d9c778970fc0812ecdcc58800c5d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ethanm88/llm-access-control&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ethanm88/llm-access-control.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safetybench: Evaluating the safety of large language models with multiple choice questions&lt;/strong&gt; (13 Sep 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Zhexin Zhang, Leqi Lei, Lindong Wu, et al.&lt;/summary&gt;Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.07045.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/9b9a4fa3ed510fc6eb1bf831979235f3d9f8b556&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-13-blue.svg?paper=9b9a4fa3ed510fc6eb1bf831979235f3d9f8b556&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/thu-coai/SafetyBench&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/thu-coai/SafetyBench.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Safety assessment of chinese large language models&lt;/strong&gt; (20 Apr 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Hao Sun, Zhexin Zhang, Jiawen Deng, et al.&lt;/summary&gt;Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang&#xA;  &lt;/details&gt; &lt;a href=&#34;https://arxiv.org/abs/2304.10436.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/59fc49dfd81b92661437eaf7e339c0792ccd8755&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-33-blue.svg?paper=59fc49dfd81b92661437eaf7e339c0792ccd8755&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/thu-coai/Safety-Prompts&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/thu-coai/Safety-Prompts.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;3D, Video and Audio Safety&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators&lt;/strong&gt; (25 Jan 2024)&lt;br&gt; Wiebke Hutiri, Oresiti Papakyriakopoulos, Alice Xiang&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.01708&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/Not-My-Voice!-A-Taxonomy-of-Ethical-an&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-N/A-blue.svg?paper=Not-My-Voice!-A-Taxonomy-of-Ethical-an&#34; alt=&#34;citation&#34;&gt; )&lt;/a&gt; )&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Adv3D: Generating 3D Adversarial Examples in Driving Scenarios with NeRF&lt;/strong&gt; (4 Sep 2023)&lt;br&gt; Leheng Li, Qing Lian, Ying-Cong Chen&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2309.01351&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/daa6a6b2c495d002d72075c6203c98061d1e35f9&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-2-blue.svg?paper=daa6a6b2c495d002d72075c6203c98061d1e35f9&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/EnVision-Research/Adv3D&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/EnVision-Research/Adv3D.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deepfake Video Detection Using Generative Convolutional Vision Transformer&lt;/strong&gt; (13 Jul 2023)&lt;br&gt; Deressa Wodajo, Solomon Atnafu, Zahid Akhtar&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2307.07036&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/86301139cc02eb53247e63fca91b916348591505&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=86301139cc02eb53247e63fca91b916348591505&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/erprogs/GenConViT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/erprogs/GenConViT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection&lt;/strong&gt; (19 Apr 2022)&lt;br&gt; Junke Wang, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, Ser-Nam Lim, Yu-Gang Jiang&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2104.09770&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/21e0858665cddf51689fc680f72ec4e00b68ae04&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-132-blue.svg?paper=21e0858665cddf51689fc680f72ec4e00b68ae04&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/wangjk666/M2TR-Multi-modal-Multi-scale-Transformers-for-Deepfake-Detection&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/wangjk666/M2TR-Multi-modal-Multi-scale-Transformers-for-Deepfake-Detection.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deepfake Video Detection Using Convolutional Vision Transformer&lt;/strong&gt; (11 Mar 2021)&lt;br&gt; Deressa Wodajo, Solomon Atnafu&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2102.11126&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/86301139cc02eb53247e63fca91b916348591505&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-0-blue.svg?paper=86301139cc02eb53247e63fca91b916348591505&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/erprogs/GenConViT&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/erprogs/GenConViT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&#34;Deepfakes Generation and Detection: State-of-the-art, open challenges, countermeasures, and way forward&#34;&lt;/strong&gt; (25 Feb 2021)&lt;br&gt; Momina Masood, Marriam Nawaz, Khalid Mahmood Malik, Ali Javed, Aun Irtaza&lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.00484&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/e8f1c51c4e881345c0588bec8aa8bc6d9164a535&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-123-blue.svg?paper=e8f1c51c4e881345c0588bec8aa8bc6d9164a535&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìç Related Surveys&lt;/h1&gt; &#xA;&lt;h2&gt;LLM&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MM-LLMs: Recent Advances in MultiModal Large Language Models&lt;/strong&gt; (24 Jan 2024)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Duzhen Zhang, Yahan Yu, Chenxing Li&lt;/summary&gt;Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, Dong Yu&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.13601&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/a050c9b0c321839e4427ab9defa3463be7825ac4&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-4-blue.svg?paper=a050c9b0c321839e4427ab9defa3463be7825ac4&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://mm-llms.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project_Page-00CED1&#34; alt=&#34;Project_Page&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;A Survey on Multimodal Large Language Models&lt;/strong&gt; (23 Jun 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Shukang Yin, Chaoyou Fu, Sirui Zhao, et al.&lt;/summary&gt;Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.13549&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-114-blue.svg?paper=ebedc4d7a2356090904baba4104ef0832bc236df&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/BradyFU/Awesome-Multimodal-Large-Language-Models.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multimodal Large Language Models: A Survey&lt;/strong&gt; (22 Nov 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[IEEE BigData 2023] Jiayang Wu, Wensheng Gan, Zefeng Chen, et al.&lt;/summary&gt;Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, Philip S. Yu&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2311.13165&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/52941cadbd340344f3e0a6f50719fe55b3de5088&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-14-blue.svg?paper=52941cadbd340344f3e0a6f50719fe55b3de5088&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;A Survey of Large Language Models&lt;/strong&gt; (31 Mar 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Wayne Xin Zhao, Kun Zhou, Junyi Li, et al.&lt;/summary&gt;Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, Ji-Rong Wen&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.18223&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/c61d54644e9aedcfc756e5d6fe4cc8b78c87755d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-875-blue.svg?paper=c61d54644e9aedcfc756e5d6fe4cc8b78c87755d&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/RUCAIBox/LLMSurvey?tab=readme-ov-file#timeline-of-llms&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RUCAIBox/LLMSurvey.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Vision&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;State of the Art on Diffusion Models for Visual Computing&lt;/strong&gt; (11 Oct 2023)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;Ryan Po, Wang Yifan, Vladislav Golyanik, et al.&lt;/summary&gt;Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T. Barron, Amit H. Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias Nie√üner, Bj√∂rn Ommer, Christian Theobalt, Peter Wonka, Gordon Wetzstein&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.07204&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/6487ec82f6d8082a5b402a5416ea03009acb1679&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-27-blue.svg?paper=6487ec82f6d8082a5b402a5416ea03009acb1679&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Diffusion Models in Vision: A Survey&lt;/strong&gt; (10 Sep 2022)&lt;/p&gt;&#xA;  &lt;details&gt;&#xA;   &lt;summary&gt;[TPAMI 2023] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, et al. &lt;/summary&gt;Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah&#xA;  &lt;/details&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.04747&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-b31b1b.svg?sanitize=true&#34; alt=&#34;Paper&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.semanticscholar.org/paper/efa1647594b236361610a20d507127f0586a379b&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/citation-371-blue.svg?paper=efa1647594b236361610a20d507127f0586a379b&#34; alt=&#34;citation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/CroitoruAlin/Diffusion-Models-in-Vision-A-Survey.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Code&#34;&gt;&lt;/a&gt;&lt;p&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üë®‚Äçüíª Team&lt;/h1&gt; &#xA;&lt;p&gt;Here is the list of our contributors in each modality of this repository.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Modality/Task&lt;/th&gt; &#xA;   &lt;th&gt;Contributors&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image Generation&lt;/td&gt; &#xA;   &lt;td&gt;Jingye Chen, Xiaowei Chi, Yingqing He&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Video Generation&lt;/td&gt; &#xA;   &lt;td&gt;Yingqing He, Xiaowei Chi, Jingye Chen&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Image and Video Editing&lt;/td&gt; &#xA;   &lt;td&gt;Yazhou Xing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3D Generation and Editing&lt;/td&gt; &#xA;   &lt;td&gt;Hongyu Liu&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Audio Generation and Editing&lt;/td&gt; &#xA;   &lt;td&gt;Zeyue Tian, Ruibin Yuan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;Zhaoyang Liu&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Safety&lt;/td&gt; &#xA;   &lt;td&gt;Runtao Liu&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Leaders&lt;/td&gt; &#xA;   &lt;td&gt;Yingqing He, Zhaoyang Liu&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;üòâ Citation&lt;/h1&gt; &#xA;&lt;p&gt;If you find this work useful in your research, Please cite the paper as below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@article{he2024llms,&#xA;    title={LLMs Meet Multimodal Generation and Editing: A Survey},&#xA;    author={He, Yingqing and Liu, Zhaoyang and Chen, Jingye and Tian, Zeyue and Liu, Hongyu and Chi, Xiaowei and Liu, Runtao and Yuan, Ruibin and Xing, Yazhou and Wang, Wenhai and Dai, Jifeng and Zhang, Yong and Xue, Wei and Liu, Qifeng and Guo, Yike and Chen, Qifeng},&#xA;    journal={arXiv preprint arXiv:2405.19334},&#xA;    year={2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;‚≠êÔ∏è Star History&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#YingqingHe/Awesome-LLMs-meet-Multimodal-Generation&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=YingqingHe/Awesome-LLMs-meet-Multimodal-Generation&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>