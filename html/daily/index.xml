<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-27T01:35:05Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>gradio-app/gradio</title>
    <updated>2022-11-27T01:35:05Z</updated>
    <id>tag:github.com,2022-11-27:/gradio-app/gradio</id>
    <link href="https://github.com/gradio-app/gradio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Create UIs for your machine learning model in Python in 3 minutes&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gradio.app&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/gradio.svg?sanitize=true&#34; alt=&#34;gradio&#34; width=&#34;300&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;em&gt;Build &amp;amp; share delightful machine learning apps easily&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/gradio-app/gradio&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/gradio-app/gradio.svg?style=svg&#34; alt=&#34;circleci&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.codecov.io/gh/gradio-app/gradio&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/gradio-app/gradio/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/gradio/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/gradio&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/gradio/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/gradio&#34; alt=&#34;PyPI downloads&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/python-3.7+-important&#34; alt=&#34;Python version&#34;&gt; &lt;a href=&#34;https://twitter.com/gradio&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/gradio?style=social&amp;amp;label=follow&#34; alt=&#34;Twitter follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gradio.app&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://gradio.app/docs/&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://gradio.app/guides/&#34;&gt;Guides&lt;/a&gt; | &lt;a href=&#34;https://gradio.app/getting_started/&#34;&gt;Getting Started&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/&#34;&gt;Examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Gradio: Build Machine Learning Web Apps ‚Äî in Python&lt;/h1&gt; &#xA;&lt;p&gt;Gradio is an open-source Python library that is used to build machine learning and data science demos and web applications.&lt;/p&gt; &#xA;&lt;p&gt;With Gradio, you can quickly create a beautiful user interface around your machine learning models or data science workflow and let people &#34;try it out&#34; by dragging-and-dropping in their own images, pasting text, recording their own voice, and interacting with your demo, all through the browser.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/header-image.jpg&#34; alt=&#34;Interface montage&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gradio is useful for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Demoing&lt;/strong&gt; your machine learning models for clients/collaborators/users/students.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deploying&lt;/strong&gt; your models quickly with automatic shareable links and getting feedback on model performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Debugging&lt;/strong&gt; your model interactively during development using built-in manipulation and interpretation tools.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisite&lt;/strong&gt;: Gradio requires Python 3.7 or higher, that&#39;s all!&lt;/p&gt; &#xA;&lt;h3&gt;What Does Gradio Do?&lt;/h3&gt; &#xA;&lt;p&gt;One of the &lt;em&gt;best ways to share&lt;/em&gt; your machine learning model, API, or data science workflow with others is to create an &lt;strong&gt;interactive app&lt;/strong&gt; that allows your users or colleagues to try out the demo in their browsers.&lt;/p&gt; &#xA;&lt;p&gt;Gradio allows you to &lt;strong&gt;build demos and share them, all in Python.&lt;/strong&gt; And usually in just a few lines of code! So let&#39;s get started.&lt;/p&gt; &#xA;&lt;h3&gt;Hello, World&lt;/h3&gt; &#xA;&lt;p&gt;To get Gradio running with a simple &#34;Hello, World&#34; example, follow these three steps:&lt;/p&gt; &#xA;&lt;p&gt;1. Install Gradio using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install gradio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;2. Run the code below as a Python script or in a Jupyter Notebook (or &lt;a href=&#34;https://colab.research.google.com/drive/18ODkJvyxHutTN0P5APWyGFO_xwNcgHDZ?usp=sharing&#34;&gt;Google Colab&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gradio as gr&#xA;&#xA;def greet(name):&#xA;    return &#34;Hello &#34; + name + &#34;!&#34;&#xA;&#xA;demo = gr.Interface(fn=greet, inputs=&#34;text&#34;, outputs=&#34;text&#34;)&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;3. The demo below will appear automatically within the Jupyter Notebook, or pop in a browser on &lt;a href=&#34;http://localhost:7860&#34;&gt;http://localhost:7860&lt;/a&gt; if running from a script:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/hello_world/screenshot.gif&#34; alt=&#34;hello_world demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;The &lt;code&gt;Interface&lt;/code&gt; Class&lt;/h3&gt; &#xA;&lt;p&gt;You&#39;ll notice that in order to make the demo, we created a &lt;code&gt;gradio.Interface&lt;/code&gt;. This &lt;code&gt;Interface&lt;/code&gt; class can wrap any Python function with a user interface. In the example above, we saw a simple text-based function, but the function could be anything from music generator to a tax calculator to the prediction function of a pretrained machine learning model.&lt;/p&gt; &#xA;&lt;p&gt;The core &lt;code&gt;Interface&lt;/code&gt; class is initialized with three required parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;fn&lt;/code&gt;: the function to wrap a UI around&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;inputs&lt;/code&gt;: which component(s) to use for the input (e.g. &lt;code&gt;&#34;text&#34;&lt;/code&gt;, &lt;code&gt;&#34;image&#34;&lt;/code&gt; or &lt;code&gt;&#34;audio&#34;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;outputs&lt;/code&gt;: which component(s) to use for the output (e.g. &lt;code&gt;&#34;text&#34;&lt;/code&gt;, &lt;code&gt;&#34;image&#34;&lt;/code&gt; or &lt;code&gt;&#34;label&#34;&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let&#39;s take a closer look at these components used to provide input and output.&lt;/p&gt; &#xA;&lt;h3&gt;Components Attributes&lt;/h3&gt; &#xA;&lt;p&gt;We saw some simple &lt;code&gt;Textbox&lt;/code&gt; components in the previous examples, but what if you want to change how the UI components look or behave?&lt;/p&gt; &#xA;&lt;p&gt;Let&#39;s say you want to customize the input text field ‚Äî for example, you wanted it to be larger and have a text placeholder. If we use the actual class for &lt;code&gt;Textbox&lt;/code&gt; instead of using the string shortcut, you have access to much more customizability through component attributes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gradio as gr&#xA;&#xA;def greet(name):&#xA;    return &#34;Hello &#34; + name + &#34;!&#34;&#xA;&#xA;demo = gr.Interface(&#xA;    fn=greet,&#xA;    inputs=gr.Textbox(lines=2, placeholder=&#34;Name Here...&#34;),&#xA;    outputs=&#34;text&#34;,&#xA;)&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/hello_world_2/screenshot.gif&#34; alt=&#34;hello_world_2 demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multiple Input and Output Components&lt;/h3&gt; &#xA;&lt;p&gt;Suppose you had a more complex function, with multiple inputs and outputs. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number. Take a look how you pass a list of input and output components.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gradio as gr&#xA;&#xA;def greet(name, is_morning, temperature):&#xA;    salutation = &#34;Good morning&#34; if is_morning else &#34;Good evening&#34;&#xA;    greeting = f&#34;{salutation} {name}. It is {temperature} degrees today&#34;&#xA;    celsius = (temperature - 32) * 5 / 9&#xA;    return greeting, round(celsius, 2)&#xA;&#xA;demo = gr.Interface(&#xA;    fn=greet,&#xA;    inputs=[&#34;text&#34;, &#34;checkbox&#34;, gr.Slider(0, 100)],&#xA;    outputs=[&#34;text&#34;, &#34;number&#34;],&#xA;)&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/hello_world_3/screenshot.gif&#34; alt=&#34;hello_world_3 demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You simply wrap the components in a list. Each component in the &lt;code&gt;inputs&lt;/code&gt; list corresponds to one of the parameters of the function, in order. Each component in the &lt;code&gt;outputs&lt;/code&gt; list corresponds to one of the values returned by the function, again in order.&lt;/p&gt; &#xA;&lt;h3&gt;An Image Example&lt;/h3&gt; &#xA;&lt;p&gt;Gradio supports many types of components, such as &lt;code&gt;Image&lt;/code&gt;, &lt;code&gt;DataFrame&lt;/code&gt;, &lt;code&gt;Video&lt;/code&gt;, or &lt;code&gt;Label&lt;/code&gt;. Let&#39;s try an image-to-image function to get a feel for these!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import gradio as gr&#xA;&#xA;def sepia(input_img):&#xA;    sepia_filter = np.array([&#xA;        [0.393, 0.769, 0.189], &#xA;        [0.349, 0.686, 0.168], &#xA;        [0.272, 0.534, 0.131]&#xA;    ])&#xA;    sepia_img = input_img.dot(sepia_filter.T)&#xA;    sepia_img /= sepia_img.max()&#xA;    return sepia_img&#xA;&#xA;demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), &#34;image&#34;)&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/sepia_filter/screenshot.gif&#34; alt=&#34;sepia_filter demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;When using the &lt;code&gt;Image&lt;/code&gt; component as input, your function will receive a NumPy array with the shape &lt;code&gt;(width, height, 3)&lt;/code&gt;, where the last dimension represents the RGB values. We&#39;ll return an image as well in the form of a NumPy array.&lt;/p&gt; &#xA;&lt;p&gt;You can also set the datatype used by the component with the &lt;code&gt;type=&lt;/code&gt; keyword argument. For example, if you wanted your function to take a file path to an image instead of a NumPy array, the input &lt;code&gt;Image&lt;/code&gt; component could be written as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;gr.Image(type=&#34;filepath&#34;, shape=...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also note that our input &lt;code&gt;Image&lt;/code&gt; component comes with an edit button üñâ, which allows for cropping and zooming into images. Manipulating images in this way can help reveal biases or hidden flaws in a machine learning model!&lt;/p&gt; &#xA;&lt;p&gt;You can read more about the many components and how to use them in the &lt;a href=&#34;https://gradio.app/docs&#34;&gt;Gradio docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Blocks: More Flexibility and Control&lt;/h3&gt; &#xA;&lt;p&gt;Gradio offers two classes to build apps:&lt;/p&gt; &#xA;&lt;p&gt;1. &lt;strong&gt;Interface&lt;/strong&gt;, that provides a high-level abstraction for creating demos that we&#39;ve been discussing so far.&lt;/p&gt; &#xA;&lt;p&gt;2. &lt;strong&gt;Blocks&lt;/strong&gt;, a low-level API for designing web apps with more flexible layouts and data flows. Blocks allows you to do things like feature multiple data flows and demos, control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. If this customizability is what you need, try &lt;code&gt;Blocks&lt;/code&gt; instead!&lt;/p&gt; &#xA;&lt;h3&gt;Hello, Blocks&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s take a look at a simple example. Note how the API here differs from &lt;code&gt;Interface&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gradio as gr&#xA;&#xA;def greet(name):&#xA;    return &#34;Hello &#34; + name + &#34;!&#34;&#xA;&#xA;with gr.Blocks() as demo:&#xA;    name = gr.Textbox(label=&#34;Name&#34;)&#xA;    output = gr.Textbox(label=&#34;Output Box&#34;)&#xA;    greet_btn = gr.Button(&#34;Greet&#34;)&#xA;    greet_btn.click(fn=greet, inputs=name, outputs=output)&#xA;&#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/hello_blocks/screenshot.gif&#34; alt=&#34;hello_blocks demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Things to note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Blocks&lt;/code&gt; are made with a &lt;code&gt;with&lt;/code&gt; clause, and any component created inside this clause is automatically added to the app.&lt;/li&gt; &#xA; &lt;li&gt;Components appear vertically in the app in the order they are created. (Later we will cover customizing layouts!)&lt;/li&gt; &#xA; &lt;li&gt;A &lt;code&gt;Button&lt;/code&gt; was created, and then a &lt;code&gt;click&lt;/code&gt; event-listener was added to this button. The API for this should look familiar! Like an &lt;code&gt;Interface&lt;/code&gt;, the &lt;code&gt;click&lt;/code&gt; method takes a Python function, input components, and output components.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;More Complexity&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s an app to give you a taste of what&#39;s possible with &lt;code&gt;Blocks&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;import gradio as gr&#xA;&#xA;def flip_text(x):&#xA;    return x[::-1]&#xA;&#xA;def flip_image(x):&#xA;    return np.fliplr(x)&#xA;&#xA;with gr.Blocks() as demo:&#xA;    gr.Markdown(&#34;Flip text or image files using this demo.&#34;)&#xA;    with gr.Tabs():&#xA;        with gr.TabItem(&#34;Flip Text&#34;):&#xA;            text_input = gr.Textbox()&#xA;            text_output = gr.Textbox()&#xA;            text_button = gr.Button(&#34;Flip&#34;)&#xA;        with gr.TabItem(&#34;Flip Image&#34;):&#xA;            with gr.Row():&#xA;                image_input = gr.Image()&#xA;                image_output = gr.Image()&#xA;            image_button = gr.Button(&#34;Flip&#34;)&#xA;    &#xA;    text_button.click(flip_text, inputs=text_input, outputs=text_output)&#xA;    image_button.click(flip_image, inputs=image_input, outputs=image_output)&#xA;    &#xA;demo.launch()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/demo/blocks_flipper/screenshot.gif&#34; alt=&#34;blocks_flipper demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A lot more going on here! We&#39;ll cover how to create complex &lt;code&gt;Blocks&lt;/code&gt; apps like this in the &lt;a href=&#34;https://github.com/gradio-app/gradio/tree/main/guides/3)building_with_blocks&#34;&gt;building with blocks&lt;/a&gt; section for you.&lt;/p&gt; &#xA;&lt;p&gt;Congrats, you&#39;re now familiar with the basics of Gradio! ü•≥ Go to our &lt;a href=&#34;https://gradio.app/key_features&#34;&gt;next guide&lt;/a&gt; to learn more about the key features of Gradio.&lt;/p&gt; &#xA;&lt;h2&gt;Open Source Stack&lt;/h2&gt; &#xA;&lt;p&gt;Gradio is built with many wonderful open-source libraries, please support them as well!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/huggingface_mini.svg?sanitize=true&#34; alt=&#34;huggingface&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/python.svg?sanitize=true&#34; alt=&#34;python&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fastapi.tiangolo.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/fastapi.svg?sanitize=true&#34; alt=&#34;fastapi&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.encode.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/encode.svg?sanitize=true&#34; alt=&#34;encode&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://svelte.dev&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/svelte.svg?sanitize=true&#34; alt=&#34;svelte&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://vitejs.dev&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/vite.svg?sanitize=true&#34; alt=&#34;vite&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pnpm.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/pnpm.svg?sanitize=true&#34; alt=&#34;pnpm&#34; height=&#34;40&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://tailwindcss.com&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/readme_files/tailwind.svg?sanitize=true&#34; alt=&#34;tailwind&#34; height=&#34;40&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Gradio is licensed under the Apache License 2.0 found in the &lt;a href=&#34;https://raw.githubusercontent.com/gradio-app/gradio/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file in the root directory of this repository.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Also check out the paper &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/1906.02569&#34;&gt;Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild&lt;/a&gt;, ICML HILL 2019&lt;/em&gt;, and please cite it if you use Gradio in your work.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{abid2019gradio,&#xA;  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},&#xA;  author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},&#xA;  journal = {arXiv preprint arXiv:1906.02569},&#xA;  year = {2019},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>collabnix/kubetools</title>
    <updated>2022-11-27T01:35:05Z</updated>
    <id>tag:github.com,2022-11-27:/collabnix/kubetools</id>
    <link href="https://github.com/collabnix/kubetools" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Kubetools - Curated List of Kubernetes Tools&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kubetools - A Curated List of Kubernetes Tools&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/collabnix/kubetools&#34; alt=&#34;stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/collabnix/kubetools&#34; alt=&#34;forks&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/collabnix/kubetools&#34; alt=&#34;issues&#34;&gt; &lt;img src=&#34;https://shields-io-visitor-counter.herokuapp.com/badge?page=collabnix.kubetools&#34; alt=&#34;Visitor count&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/contributors/collabnix/kubetools&#34; alt=&#34;GitHub contributors&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/kubetools?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/collabnix/kubetools/master/img/kubetool.jpg&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;There are more than 300+ &lt;a href=&#34;https://www.cncf.io/certification/kcsp/&#34;&gt;Kubernetes Certified Service Providers&lt;/a&gt; and &lt;a href=&#34;https://kubernetes.io/partners/&#34;&gt;tons of Kubernetes Certified distributions&lt;/a&gt;. Choosing a right distribution can be a daunting task. Kubetools is built with a purpose to build a curated list of popular Kubernetes tools. It is actively maintained by &lt;a href=&#34;https://collabnix.com&#34;&gt;Collabnix Slack Community&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Follow the Kubetools &lt;a href=&#34;https://twitter.com/kubetools&#34;&gt;Twitter&lt;/a&gt; account for updates on new list additions.&lt;/p&gt; &#xA;&lt;p&gt;Have Questions? Join us over &lt;a href=&#34;https://launchpass.com/collabnix&#34;&gt;Slack&lt;/a&gt; and get chance to be a part of 7200+ DevOps enthusiasts.&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cluster Management&lt;/h2&gt; &#xA;&lt;p&gt;kops - &lt;a href=&#34;https://github.com/kubernetes/kops&#34;&gt;Production Grade K8s Installation, Upgrades, and Management&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;silver-surfer - &lt;a href=&#34;https://github.com/devtron-labs/silver-surfer&#34;&gt;Check ApiVersion compatibility and provide Migration path for Kubernetes objects when upgrading Kubernetes to latest versions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kube-ops-view - &lt;a href=&#34;https://github.com/hjacobs/kube-ops-view&#34;&gt;Kubernetes Operational View - read-only system dashboard for multiple K8s clusters&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kubeprompt - &lt;a href=&#34;https://github.com/jlesquembre/kubeprompt&#34;&gt;Kubernetes prompt info&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Metalk8s - &lt;a href=&#34;https://github.com/scality/metalk8s&#34;&gt;An opinionated Kubernetes distribution with a focus on long-term on-prem deployments&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kind - &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34;&gt;Kubernetes IN Docker - local clusters for testing Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clusterman - &lt;a href=&#34;https://github.com/Yelp/clusterman&#34;&gt;Cluster Autoscaler for Kubernetes and Mesos&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cert-manager - &lt;a href=&#34;https://github.com/jetstack/cert-manager&#34;&gt;Automatically provision and manage TLS certificates&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Goldilocks - &lt;a href=&#34;https://github.com/FairwindsOps/goldilocks&#34;&gt;Get your resource requests &#34;Just Right&#34;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;katafygio - &lt;a href=&#34;https://github.com/bpineau/katafygio&#34;&gt;Dump, or continuously backup Kubernetes objets as yaml files in git&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Rancher - &lt;a href=&#34;https://github.com/rancher/rancher&#34;&gt;Complete container management platform&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sealed Secrets - &lt;a href=&#34;https://github.com/bitnami-labs/sealed-secrets&#34;&gt;A Kubernetes controller and tool for one-way encrypted Secrets&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenKruise/Kruise - &lt;a href=&#34;https://github.com/openkruise/kruise&#34;&gt;Automate application workloads management on Kubernetes https://openkruise.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kubectl snapshot - &lt;a href=&#34;https://github.com/fbrubbo/kubectl-snapshot&#34;&gt;Take Cluster Snapshots&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kapp - &lt;a href=&#34;https://github.com/k14s/kapp&#34;&gt;simple deployment tool focused on the concept of &#34;Kubernetes application&#34; ‚Äî a set of resources with the same label https://get-kapp.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;keda - &lt;a href=&#34;https://keda.sh/&#34;&gt;Event-driven autoscaler for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Octant - &lt;a href=&#34;https://github.com/vmware-tanzu/octant&#34;&gt;To better understand the complexity of Kubernetes clusters&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Portainer - &lt;a href=&#34;https://github.com/portainer/k8s&#34;&gt;Portainer inside a Kubernetes environment&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gardener - &lt;a href=&#34;https://gardener.cloud/&#34;&gt;Deliver fully-managed clusters at scale everywhere with your own Kubernetes-as-a-Service&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;xlskubectl - &lt;a href=&#34;https://github.com/learnk8s/xlskubectl&#34;&gt;xlskubectl ‚Äî a spreadsheet to control your Kubernetes cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Cluster with Core CLI tools&lt;/h2&gt; &#xA;&lt;p&gt;Bootkube - &lt;a href=&#34;https://github.com/kubernetes-sigs/bootkube&#34;&gt;bootkube - Launch a self-hosted Kubernetes cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kubectx + kubens - &lt;a href=&#34;https://github.com/ahmetb/kubectx&#34;&gt;Switch faster between clusters and namespaces in kubectl&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kube-shell - &lt;a href=&#34;https://github.com/cloudnativelabs/kube-shell&#34;&gt;Kubernetes shell: An integrated shell for working with the Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kuttle: kubectl wrapper for sshuttle without SSH - &lt;a href=&#34;https://github.com/kayrus/kuttle&#34;&gt;Kubernetes wrapper for sshuttle&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kubectl sudo - &lt;a href=&#34;https://github.com/postfinance/kubectl-sudo&#34;&gt;Run kubernetes commands with the security privileges of another user&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;K9s - &lt;a href=&#34;https://github.com/derailed/k9s&#34;&gt;Kubernetes CLI To Manage Your Clusters In Style!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ktunnel - &lt;a href=&#34;https://github.com/omrikiei/ktunnel&#34;&gt;A cli that exposes your local resources to kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;KubeOperator - &lt;a href=&#34;https://github.com/KubeOperator/webkubectl&#34;&gt;Run kubectl command in Web Browser. https://kubeoperator.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Vimkubectl - &lt;a href=&#34;https://github.com/rottencandy/vimkubectl&#34;&gt;Manage any Kubernetes resource from Vim https://www.vim.org/scripts/script.ph&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;KubeHelper - &lt;a href=&#34;https://github.com/KubeHelper/kubehelper&#34;&gt;KubeHelper - simplifies many daily Kubernetes cluster tasks through a web interface.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Alert and Monitoring&lt;/h2&gt; &#xA;&lt;p&gt;Thanos - &lt;a href=&#34;https://github.com/thanos-io/thanos&#34;&gt;Highly available Prometheus setup with long term storage capabilities. CNCF Sandbox project. https://thanos.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prometheus - &lt;a href=&#34;https://github.com/prometheus/prometheus&#34;&gt;The Prometheus monitoring system and time series database.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Grafana - &lt;a href=&#34;https://github.com/grafana/grafana&#34;&gt;The tool for beautiful monitoring and metric analytics &amp;amp; dashboards for Graphite, InfluxDB &amp;amp; Prometheus &amp;amp; More&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubetail - &lt;a href=&#34;https://github.com/johanhaleby/kubetail&#34;&gt;Bash script to tail Kubernetes logs from multiple pods at the same time&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Searchlight - &lt;a href=&#34;https://github.com/searchlight/searchlight&#34;&gt;Alerts for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;linkerd2 Monitoring Mixin for Grafana - &lt;a href=&#34;https://github.com/andrew-waters/linkerd2-mixin&#34;&gt;Grafana dashboards for linkerd2 monitoring and can work in standalone (default) or in multi cluster setup&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kuberhaus - &lt;a href=&#34;https://github.com/stevelacy/kuberhaus&#34;&gt;Kubernetes resource dashboard with node/pod layout and resource requests&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubernetes Job/CronJob Notifier - &lt;a href=&#34;https://github.com/sukeesh/k8s-job-notify&#34;&gt;This tool sends an alert to slack whenever there is a Kubernetes cronJob/Job failure/success&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Argus - &lt;a href=&#34;https://clustergarage.io/argus/docs/overview&#34;&gt;This tool monitors changes in the filesystem on specified paths&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kube-Scout - &lt;a href=&#34;https://github.com/ReallyLiri/kubescout&#34;&gt;Scout for alarming issues across your Kubernetes clusters&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Logging and Tracing&lt;/h2&gt; &#xA;&lt;p&gt;Jaeger - &lt;a href=&#34;https://github.com/jaegertracing/jaeger&#34;&gt;CNCF Jaeger, a Distributed Tracing Platform&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kiali - &lt;a href=&#34;https://github.com/kiali/kiali&#34;&gt;Kiali project, observability for the Istio service mesh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ELK - &lt;a href=&#34;https://github.com/elastic&#34;&gt;Elasticsearch, Logstash, Kibana&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;fluentbit - &lt;a href=&#34;https://github.com/fluent/fluent-bit&#34;&gt;Fast and Lightweight Log processor and forwarder for Linux, BSD and OSX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Loki - &lt;a href=&#34;https://github.com/grafana/loki&#34;&gt;Like Prometheus, but for logs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Kubectl-debug - &lt;a href=&#34;https://github.com/aylei/kubectl-debug&#34;&gt;Allows you to run a new container with all the troubleshooting tools installed in running pod for debugging purposed&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PowerfulSeal - &lt;a href=&#34;https://github.com/bloomberg/powerfulseal&#34;&gt;A powerful testing tool for Kubernetes clustersd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Crash-diagnostic - &lt;a href=&#34;https://github.com/vmware-tanzu/crash-diagnostics&#34;&gt;Crash-Diagnostics is a tool to help investigate, analyze, and troubleshoot unresponsive or crashed Kubernetes clustersd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;K9s - &lt;a href=&#34;https://github.com/derailed/k9s&#34;&gt;Kubernetes CLI To Manage Your Clusters In Style!d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubernetes CLI Plugin - Doctor - &lt;a href=&#34;https://github.com/emirozer/kubectl-doctor&#34;&gt;kubectl cluster triage plugin for k8s - üè• (brew doctor equivalent)d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Knative Inspect - &lt;a href=&#34;https://github.com/nimakaviani/knative-inspect&#34;&gt;A light-weight debugging tool for Knative&#39;s system componentsd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubeman - &lt;a href=&#34;https://github.com/walmartlabs/kubeman&#34;&gt;To find information from Kubernetes clusters, and to investigate issues related to Kubernetes and Istiod&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kpexec - &lt;a href=&#34;https://github.com/ssup2/kpexec&#34;&gt;kpexec is a kubernetes cli that runs commands in a container with high privilegesd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Koolkits - &lt;a href=&#34;https://github.com/lightrun-platform/koolkits&#34;&gt;üß∞ Opinionated, language-specific, batteries-included debug container images for Kubernetes.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Developement Tools/Kit&lt;/h2&gt; &#xA;&lt;p&gt;Okteto: A Tool for Cloud Native Developers - &lt;a href=&#34;https://github.com/okteto/okteto&#34;&gt;Build better applications by developing and testing your code directly in Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tilt: Tilt manages local development instances for teams that deploy to Kubernetes - &lt;a href=&#34;https://github.com/windmilleng/tilt&#34;&gt;Local Kubernetes development with no stress&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Garden: Kubernetes from source to finish - &lt;a href=&#34;https://github.com/garden-io/garden&#34;&gt;Development orchestrator for Kubernetes, containers and functions.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;KuberNix - &lt;a href=&#34;https://github.com/saschagrunert/kubernix&#34;&gt;Single dependency Kubernetes clusters for local testing, experimenting and development&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Copper - &lt;a href=&#34;https://github.com/cloud66-oss/copper&#34;&gt;A configuration file validator for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ko - &lt;a href=&#34;https://github.com/google/ko&#34;&gt;Build and deploy Go applications on Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dekorate - &lt;a href=&#34;https://github.com/dekorateio/dekorate&#34;&gt;Java annotation processors for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lens IDE - &lt;a href=&#34;https://k8slens.dev/&#34;&gt;A powerful interface and toolkit for managing, visualizing, and interacting with multiple Kubernetes clusters&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kosko - &lt;a href=&#34;https://kosko.dev/&#34;&gt;Organize Kubernetes manifests in JavaScript&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Telepresence - &lt;a href=&#34;https://www.telepresence.io/&#34;&gt;Fast, local development for Kubernetes and Openshift microservices&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Monokle - &lt;a href=&#34;https://github.com/kubeshop/monokle/&#34;&gt;Desktop UI for managing Kubernetes manifests&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;KuberEz - &lt;a href=&#34;https://github.com/uengine-oss/kuber-ez&#34;&gt;Graphical modeling tool for Kubernetes manifest&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;mirrord - &lt;a href=&#34;https://github.com/metalbear-co/mirrord&#34;&gt;Run your local process in the context of your cloud cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Alternative Tools for Developement&lt;/h2&gt; &#xA;&lt;p&gt;Minikube - &lt;a href=&#34;https://github.com/kubernetes/minikube&#34;&gt;minikube implements a local Kubernetes clusterd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;KubeSphere - &lt;a href=&#34;https://github.com/kubesphere/kubesphere&#34;&gt;Easy-to-use Production Ready Container Platform https://kubesphere.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;skippbox - &lt;a href=&#34;https://github.com/skippbox/skippbox&#34;&gt;A Desktop application for k8sd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kind - &lt;a href=&#34;https://github.com/kubernetes-sigs/kind&#34;&gt;Kubernetes IN Docker - local clusters for testing Kubernetes https://kind.sigs.k8s.io/d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;k3d - &lt;a href=&#34;https://k3d.io/&#34;&gt;k3d is a lightweight wrapper to run k3s (Rancher Lab‚Äôs minimal Kubernetes distribution) in docker.d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Systemk: virtual kubelet for systemd - &lt;a href=&#34;https://github.com/virtual-kubelet/systemk&#34;&gt;Systemk is a systemd backend for the virtual-kubelet. Instead of starting containers, you start systemd units&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;CI/CD integration Tools&lt;/h2&gt; &#xA;&lt;p&gt;HybridK8s Droid - &lt;a href=&#34;https://hybridk8s.tech/&#34;&gt;Intelligence foor your favourite Delivery Platform&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Devtron - &lt;a href=&#34;https://github.com/devtron-labs/devtron&#34;&gt;Software Delivery Workflow for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Skaffold - &lt;a href=&#34;https://github.com/GoogleContainerTools/skaffold&#34;&gt;Easy and Repeatable Kubernetes Development&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Apollo - &lt;a href=&#34;https://github.com/logzio/apollo&#34;&gt;Apollo - The logz.io continuous deployment solution over kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Helm Cabin - &lt;a href=&#34;https://github.com/Nick-Triller/helm-cabin&#34;&gt;Web UI that visualizes Helm releases in a Kubernetes cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;flagger - &lt;a href=&#34;https://github.com/weaveworks/flagger&#34;&gt;Progressive delivery Kubernetes operator (Canary, A/B Testing and Blue/Green deployments)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubeform - &lt;a href=&#34;https://github.com/kubeform/kubeform&#34;&gt;Kubernetes CRDs for Terraform providers https://kubeform.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Spinnaker - &lt;a href=&#34;https://github.com/spinnaker/spinnaker&#34;&gt;Spinnaker is an open source, multi-cloud continuous delivery platform for releasing software changes with high velocity and confidence. http://www.spinnaker.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;werf - &lt;a href=&#34;https://github.com/werf/werf&#34;&gt;GitOps tool to deliver apps to Kubernetes and integrate this process with GitLab and other CI tools&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Flux - &lt;a href=&#34;https://github.com/fluxcd/flux&#34;&gt;GitOps Kubernetes operator&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Argo CD - &lt;a href=&#34;https://github.com/argoproj/argo-cd&#34;&gt;Declarative continuous deployment for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Tekton - &lt;a href=&#34;https://github.com/tektoncd/pipeline&#34;&gt;A cloud native continuous integration and delivery (CI/CD) solution&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Jenkins X - &lt;a href=&#34;https://github.com/jenkins-x/jx&#34;&gt;Jenkins X provides automated CI+CD for Kubernetes with Preview Environments on Pull Requests using Tekton, Knative, Lighthouse, Skaffold and Helm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Drone - &lt;a href=&#34;https://github.com/harness/drone&#34;&gt;Drone is a Container-Native, Continuous Delivery Platform&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Security Tools&lt;/h2&gt; &#xA;&lt;p&gt;TerraScan - &lt;a href=&#34;https://github.com/accurics/terrascan&#34;&gt;Detect compliance and security violations across Infrastructure as Code to mitigate risk before provisioning cloud native infrastructure.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;klum - &lt;a href=&#34;https://github.com/ibuildthecloud/klum&#34;&gt;Kubernetes Lazy User Manager&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kube2iam - &lt;a href=&#34;https://github.com/jtblin/kube2iam&#34;&gt;IAM credentials to containers running inside a kubernetes cluster based on annotations.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kyverno - &lt;a href=&#34;https://github.com/nirmata/kyverno&#34;&gt;Kubernetes Native Policy Management https://kyverno.io&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kiosk - &lt;a href=&#34;https://github.com/kiosk-sh/kiosk&#34;&gt;kiosk office Multi-Tenancy Extension For Kubernetes - Secure Cluster Sharing &amp;amp; Self-Service Namespace Provisioning&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kube-bench - &lt;a href=&#34;https://github.com/aquasecurity/kube-bench&#34;&gt;CIS Kubernetes Benchmark tool&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kube-hunter - &lt;a href=&#34;https://github.com/aquasecurity/kube-hunter&#34;&gt;Pentesting tool - Hunts for security weaknesses in Kubernetes clusters&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kube-who-can - &lt;a href=&#34;https://github.com/aquasecurity/kubectl-who-can&#34;&gt;Show who has RBAC permissions to perform actions on different resources in Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;starboard - &lt;a href=&#34;https://github.com/aquasecurity/starboard&#34;&gt;Kubernetes-native security toolkit&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Simulator - &lt;a href=&#34;https://github.com/kubernetes-simulator/simulator&#34;&gt;Kubernetes Security Training Platform - Focussing on security mitigation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;RBAC Lookup - &lt;a href=&#34;https://github.com/FairwindsOps/rbac-lookup&#34;&gt;Easily find roles and cluster roles attached to any user, service account, or group name in your Kubernetes cluster https://fairwinds.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubeaudit - &lt;a href=&#34;https://github.com/Shopify/kubeaudit&#34;&gt;kubeaudit helps you audit your Kubernetes clusters against common security controls&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gangway - &lt;a href=&#34;https://github.com/heptiolabs/gangway&#34;&gt;An application that can be used to easily enable authentication flows via OIDC for a kubernetes cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Audit2rbac - &lt;a href=&#34;https://github.com/liggitt/audit2rbac&#34;&gt;Autogenerate RBAC policies based on Kubernetes audit logs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chartsec - &lt;a href=&#34;https://github.com/banzaicloud/chartsec&#34;&gt;Helm Chart security scanner&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kubestriker - &lt;a href=&#34;https://github.com/vchinnipilli/kubestriker&#34;&gt;Security Auditing tool&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Datree - &lt;a href=&#34;https://datree.io/&#34;&gt;CLI tool to prevent K8s misconfigurations by ensuring that manifests and Helm charts follow best practices as well as your organization‚Äôs policies&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Krane - &lt;a href=&#34;https://github.com/appvia/krane&#34;&gt;Kubernetes RBAC static Analysis &amp;amp; visualisation tool&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Falco - &lt;a href=&#34;https://falco.org/&#34;&gt;The Falco Project - Cloud-Native runtime security&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clair - &lt;a href=&#34;https://github.com/quay/clair&#34;&gt;Vulnerability Static Analysis for Containers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Network Policies&lt;/h2&gt; &#xA;&lt;p&gt;trireme-kubernetes - &lt;a href=&#34;https://github.com/aporeto-inc/trireme-kubernetes&#34;&gt;Aporeto integration with Kubernetes Network Policies&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Calico - &lt;a href=&#34;https://github.com/projectcalico/calico&#34;&gt;Cloud native connectivity and network policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kubepox - &lt;a href=&#34;https://github.com/aporeto-inc/kubepox&#34;&gt;Kubernetes network Policy eXploration tool&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kokotap - &lt;a href=&#34;https://github.com/redhat-nfvpe/kokotap&#34;&gt;Tools for kubernetes pod network tapping&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Submariner - &lt;a href=&#34;https://github.com/submariner-io/submariner&#34;&gt;Connect all your Kubernetes clusters, no matter where they are in the world&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;egress-operator - &lt;a href=&#34;https://github.com/monzo/egress-operator&#34;&gt;An operator to produce egress gateway pods and control access to them with network policies&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kubefwd (Kube Forward) - &lt;a href=&#34;https://github.com/txn2/kubefwd&#34;&gt;Bulk port forwarding Kubernetes services for local development&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Testing Tools&lt;/h2&gt; &#xA;&lt;p&gt;k6d - &lt;a href=&#34;https://github.com/loadimpact/k6&#34;&gt;A modern load testing tool, using Go and JavaScript&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Network bandwith and load testingd - &lt;a href=&#34;https://github.com/mrahbar/k8s-testsuite&#34;&gt;Test suite for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;test-infrad - &lt;a href=&#34;https://github.com/kubernetes/test-infra&#34;&gt;Test infrastructure for the Kubernetes project&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kube-scored - &lt;a href=&#34;https://github.com/zegl/kube-score&#34;&gt;Kubernetes object analysis with recommendations for improved reliability and security&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Litmusd - &lt;a href=&#34;https://github.com/litmuschaos/litmus&#34;&gt;Cloud-Native Chaos Engineering; Kubernetes-Native Chaos Engineering; Chaos Engineering for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;PowerfulSeald - &lt;a href=&#34;https://github.com/bloomberg/powerfulseal&#34;&gt;A powerful testing tool for Kubernetes clusters&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kube-burnerd - &lt;a href=&#34;https://kube-burner.readthedocs.io/en/latest/&#34;&gt;Kube-burner is a tool aimed at stressing kubernetes clusters&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Service Mesh&lt;/h2&gt; &#xA;&lt;p&gt;Istio - &lt;a href=&#34;https://github.com/istio/istio&#34;&gt;Connect, secure, control, and observe services&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Traefik - &lt;a href=&#34;https://github.com/containous/traefik&#34;&gt;The Cloud Native Edge Router&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NGINX Ingress Controller - &lt;a href=&#34;https://github.com/nginxinc/kubernetes-ingress&#34;&gt;NGINX and NGINX Plus Ingress Controllers for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Autopilot - &lt;a href=&#34;https://docs.solo.io/autopilot/latest&#34;&gt;THE SERVICE MESH SDK&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;linkerd-config - &lt;a href=&#34;https://github.com/ihcsim/linkerd-config&#34;&gt;A Kubernetes controller that knows how to reconcile the Linkerd configuration&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kong - &lt;a href=&#34;https://github.com/Kong/kubernetes-ingress-controller&#34;&gt;Kong for Kubernetes: the official Ingress Controller for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OSM - &lt;a href=&#34;https://github.com/openservicemesh/osm&#34;&gt;Open Service Mesh (OSM) is a lightweight, extensible, cloud native service mesh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Layer5 - &lt;a href=&#34;https://github.com/layer5io/layer5&#34;&gt;Layer5, the service mesh company, representing every service mesh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gloo Mesh - &lt;a href=&#34;https://github.com/solo-io/gloo-mesh&#34;&gt;The Service Mesh Orchestration Platform&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;APISIX - &lt;a href=&#34;https://github.com/apache/apisix&#34;&gt;Apache APISIX is a dynamic, real-time, high-performance API gateway.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Contour - &lt;a href=&#34;https://projectcontour.io/&#34;&gt;High performance ingress controller for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kusk Gateway - &lt;a href=&#34;https://kusk.io/kusk-gateway&#34;&gt;OpenAPI-driven Ingress Controller for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Observability&lt;/h2&gt; &#xA;&lt;p&gt;Kubespy - &lt;a href=&#34;https://github.com/pulumi/kubespy&#34;&gt;Tools for observing Kubernetes resources in real time&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Popeye - &lt;a href=&#34;https://github.com/derailed/popeye&#34;&gt;A Kubernetes cluster resource sanitizer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stern - &lt;a href=&#34;https://github.com/wercker/stern&#34;&gt;Multi pod and container log tailing for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cri-tools - &lt;a href=&#34;https://github.com/kubernetes-sigs/cri-tools&#34;&gt;CLI and validation tools for Kubelet Container Runtime Interface (CRI)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubebox - &lt;a href=&#34;https://github.com/astefanutti/kubebox&#34;&gt;Terminal and Web console for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubewatch - &lt;a href=&#34;https://github.com/bitnami-labs/kubewatch&#34;&gt;Watch k8s events and trigger Handlers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kube-state-metrics - &lt;a href=&#34;https://github.com/kubernetes/kube-state-metrics&#34;&gt;Add-on agent to generate and expose cluster-level metrics&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Sloop - &lt;a href=&#34;https://github.com/salesforce/sloop?utm_sq=g90yo8t8s1&#34;&gt;Kubernetes History Visualization&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kubectl tree üéÑ - &lt;a href=&#34;https://github.com/ahmetb/kubectl-tree&#34;&gt;Kubectl plugin to observe object hierarchies through ownerReferences&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;chaoskube - &lt;a href=&#34;https://github.com/linki/chaoskube&#34;&gt;chaoskube periodically kills random pods in your Kubernetes cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;BotKube - &lt;a href=&#34;https://www.botkube.io/&#34;&gt;Helps you monitor your Kubernetes cluster(s), debug critical deployments and gives recommendations for standard practices&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubestone - &lt;a href=&#34;https://kubestone.io/en/latest&#34;&gt;Kubestone is a benchmarking Operator that can evaluate the performance of Kubernetes installations&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chaos Mesh - &lt;a href=&#34;https://github.com/pingcap/chaos-mesh&#34;&gt;A Chaos Engineering Platform for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Lemur - &lt;a href=&#34;https://github.com/turbonomic/lemur&#34;&gt;LEMUR: Observability and Context&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kubernetes-event-exporter - &lt;a href=&#34;https://github.com/opsgenie/kubernetes-event-exporter&#34;&gt;Export Kubernetes events to multiple destinations with routing and filtering&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubevious - &lt;a href=&#34;https://kubevious.io/&#34;&gt;Kubevious is an app-centric assurance, validation, and introspection platform for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenTelemetry - &lt;a href=&#34;https://opentelemetry.io/&#34;&gt;High-quality, ubiquitous, and portable telemetry to enable effective observability&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Grafana Tempo - &lt;a href=&#34;https://github.com/grafana/tempo&#34;&gt;Grafana Tempo is a high volume, minimal dependency distributed tracing backend&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Machine Learning/Deep Learning&lt;/h2&gt; &#xA;&lt;p&gt;Kubeflow - &lt;a href=&#34;https://github.com/kubeflow/kubeflow&#34;&gt;Machine Learning Toolkit for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Volcano - &lt;a href=&#34;https://github.com/volcano-sh/volcano&#34;&gt;A Kubernetes Native Batch System&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Compute Edge Tools&lt;/h2&gt; &#xA;&lt;p&gt;KubeEdge - &lt;a href=&#34;https://github.com/kubeedge/kubeedge&#34;&gt;Kubernetes Native Edge Computing Framework&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubeless - &lt;a href=&#34;https://github.com/kubeless/kubeless&#34;&gt;Kubernetes Native Serverless Framework&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Kubernetes Tools for Specific Cloud&lt;/h2&gt; &#xA;&lt;p&gt;Kubernetes on AWS (kube-aws) - &lt;a href=&#34;https://github.com/kubernetes-incubator/kube-aws&#34;&gt;A command-line tool to declaratively manage Kubernetes clusters on AWS&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Draft: Streamlined Kubernetes Development - &lt;a href=&#34;https://github.com/azure/draft&#34;&gt;A tool for developers to create cloud-native applications on Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;helm-ssm - &lt;a href=&#34;https://github.com/totango/helm-ssm&#34;&gt;A low dependency tool for retrieving and injecting secrets from AWS SSM into Helm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Skupper - &lt;a href=&#34;https://skupper.io/&#34;&gt;Multicloud communication for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Storage Providers&lt;/h2&gt; &#xA;&lt;p&gt;ChubaoFS - &lt;a href=&#34;https://github.com/chubaofs/chubaofs&#34;&gt;distributed file system and object storage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Longhorn - &lt;a href=&#34;https://github.com/longhorn/longhorn&#34;&gt;Cloud-Native distributed block storage built on and for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenEBS - &lt;a href=&#34;https://github.com/openebs/openebs&#34;&gt;Kubernetes native - hyperconverged block storage with multiple storage engines&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Rook - &lt;a href=&#34;https://github.com/rook/rook&#34;&gt;Storage Orchestration for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;SeaweedFS - &lt;a href=&#34;https://github.com/chrislusf/seaweedfs&#34;&gt;Distributed file system supports read-write many volumes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;TiKV - &lt;a href=&#34;https://github.com/tikv/tikv&#34;&gt;Distributed transactional key-value database&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;TopoLVM - &lt;a href=&#34;https://github.com/topolvm/topolvm&#34;&gt;Capacity-aware CSI plugin for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;velero - &lt;a href=&#34;https://github.com/vmware-tanzu/velero&#34;&gt;Backup and migrate Kubernetes applications and their persistent volumes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Vitess - &lt;a href=&#34;https://github.com/vitessio/vitess&#34;&gt;Vitess is a database clustering system for horizontal scaling of MySQL&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kaDalu - &lt;a href=&#34;https://github.com/kadalu/kadalu&#34;&gt;A lightweight Persistent storage solution for Kubernetes / OpenShift using GlusterFS in background&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Multiple Tools Repo&lt;/h2&gt; &#xA;&lt;p&gt;Chaos Toolkit Kubernetes Support - &lt;a href=&#34;https://github.com/chaostoolkit/chaostoolkit-kubernetes&#34;&gt;Kubernetes driver extension of the Chaos Toolkit probes and actions API&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;k14s - &lt;a href=&#34;https://github.com/k14s&#34;&gt;Kubernetes Tools that follow Unix philosophy to be simple and composable&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Pulumi - &lt;a href=&#34;https://github.com/pulumi/pulumi&#34;&gt;Pulumi - Modern Infrastructure as Code. Any cloud, any language. Give your team cloud superpowers rocket https://www.pulumi.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Non-Categorize&lt;/h2&gt; &#xA;&lt;p&gt;Rudr - &lt;a href=&#34;https://github.com/oam-dev/rudr&#34;&gt;A Kubernetes implementation of the Open Application Model specification&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Keel - &lt;a href=&#34;https://keel.sh/&#34;&gt;Kubernetes Operator to automate Helm, DaemonSet, StatefulSet &amp;amp; Deployment updates&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cabin, the mobile app for Kubernetes - &lt;a href=&#34;https://github.com/bitnami-labs/cabin&#34;&gt;The Mobile Dashboard for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Funktion - &lt;a href=&#34;https://github.com/funktionio/funktion&#34;&gt;CLI tool for working with funktion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Alterant - &lt;a href=&#34;https://github.com/cloud66-oss/alterant&#34;&gt;A simple Kubernetes configuration modifier&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;BUCK - &lt;a href=&#34;https://github.com/brigadecore/buck&#34;&gt;Brigade Universal Controller for Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kube-fledged - &lt;a href=&#34;https://github.com/senthilrch/kube-fledged&#34;&gt;A kubernetes add-on for creating and managing a cache of container images directly on the cluster worker nodes, so application pods start almost instantly&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Kubecost - &lt;a href=&#34;https://github.com/kubecost/cost-model&#34;&gt;Cross-cloud cost allocation models for workloads running on Kubernetes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;kpt - &lt;a href=&#34;https://github.com/GoogleContainerTools/kpt&#34;&gt;toolkit to help you manage, manipulate, customize, and apply Kubernetes Resource configuration&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;capsule - &lt;a href=&#34;https://github.com/clastix/capsule&#34;&gt;Capsule helps to implement a multi-tenancy and policy-based environment in your Kubernetes cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;KubeSlice - &lt;a href=&#34;https://github.com/kubeslice&#34;&gt;KubeSlice enables Kubernetes pods and services to communicate seamlessly across clusters, clouds, edges, and data centers by creating logical application boundaries known as Slices&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Maintainer&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/apurvabhandari-linux/&#34;&gt;Apurva Bhandari&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.linkedin.com/in/ajeetsraina&#34;&gt;Ajeet Singh Raina&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Join Discord Channel&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/ztZpXzjSmF&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/34368930/198940642-50d0e7f0-c670-4800-b0ea-5b95d56aaf0e.png&#34; alt=&#34;Title&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>omerbsezer/Fast-Kubernetes</title>
    <updated>2022-11-27T01:35:05Z</updated>
    <id>tag:github.com,2022-11-27:/omerbsezer/Fast-Kubernetes</id>
    <link href="https://github.com/omerbsezer/Fast-Kubernetes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repo covers Kubernetes Environment with LABs: Kubectl, Pod, Deployment, Service, PV, PVC, Kubeadm, Helm, etc.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Fast-Kubernetes&lt;/h1&gt; &#xA;&lt;p&gt;This repo covers Kubernetes objects&#39; and components&#39; details (Kubectl, Pod, Deployment, Service, ConfigMap, Volume, PV, PVC, Daemonset, Secret, Affinity, Taint-Toleration, Helm, etc.) fastly, and possible example usage scenarios (HowTo: Hands-on LAB) in a nutshell. Possible usage scenarios are aimed to update over time.&lt;/p&gt; &#xA;&lt;h2&gt;Prerequisite&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Have a knowledge of Container Technology (Docker). You can learn it from here =&amp;gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Docker&#34;&gt;Fast-Docker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; Containerization, Kubernetes, Kubectl, Pod, Deployment, Service, ConfigMap, ReplicaSet, Volume, Cheatsheet.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; K8s objects and objects feature can be updated/changed in time. While creating this repo, the version of K8s is v1.22.3.&lt;/p&gt; &#xA;&lt;h1&gt;Quick Look (HowTo): Scenarios - Hands-on LAB&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-CreatingPod-Imperative.md&#34;&gt;LAB: K8s Creating Pod - Imperative Way&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8-CreatingPod-Declerative.md&#34;&gt;LAB: K8s Creating Pod - Declarative Way (With File) - Environment Variable&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Multicontainer-Sidecar.md&#34;&gt;LAB: K8s Multicontainer - Sidecar - Emptydir Volume - Port-Forwarding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Deployment.md&#34;&gt;LAB: K8s Deployment - Scale Up/Down - Bash Connection - Port Forwarding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Rollout-Rollback.md&#34;&gt;LAB: K8s Rollout - Rollback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Service-App.md&#34;&gt;LAB: K8s Service Implementations (ClusterIp, NodePort and LoadBalancer)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Liveness-App.md&#34;&gt;LAB: K8s Liveness Probe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Secret.md&#34;&gt;LAB: K8s Secret (Declarative and Imperative Way)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Configmap.md&#34;&gt;LAB: K8s Config Map&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Node-Affinity.md&#34;&gt;LAB: K8s Node Affinity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Taint-Toleration.md&#34;&gt;LAB: K8s Taint-Toleration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Daemon-Sets.md&#34;&gt;LAB: K8s Daemonset - Creating 3 nodes on Minikube&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-PersistantVolume.md&#34;&gt;LAB: K8s Persistent Volume and Persistent Volume Claim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Statefulset.md&#34;&gt;LAB: K8s Stateful Sets - Nginx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Job.md&#34;&gt;LAB: K8s Job&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-CronJob.md&#34;&gt;LAB: K8s Cron Job&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Ingress.md&#34;&gt;LAB: K8s Ingress&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/Helm.md&#34;&gt;LAB: Helm Install &amp;amp; Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Kubeadm-Cluster-Setup.md&#34;&gt;LAB: K8s Cluster Setup with Kubeadm and Containerd&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Kubeadm-Cluster-Docker.md&#34;&gt;LAB: K8s Cluster Setup with Kubeadm and Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Helm-Jenkins.md&#34;&gt;LAB: Helm-Jenkins on running K8s Cluster (2 Node Multipass VM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Enable-Dashboard-On-Cluster.md&#34;&gt;LAB: Enable Dashboard on Real K8s Cluster&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Monitoring-Prometheus-Grafana.md&#34;&gt;LAB: K8s Monitoring - Prometheus and Grafana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/KubernetesCommandCheatSheet.md&#34;&gt;Kubectl Commands Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/HelmCheatsheet.md&#34;&gt;Helm Commands Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#motivation&#34;&gt;Motivation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#containerization&#34;&gt;What is Containerization? What is Container Orchestration?&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#whatIsKubernetes&#34;&gt;What is Kubernetes?&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#architecture&#34;&gt;Kubernetes Architecture&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#components&#34;&gt;Kubernetes Components&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#kubectl&#34;&gt;Kubectl Config ‚Äì Usage&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#pod&#34;&gt;Pod: Creating, Yaml, LifeCycle&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#multicontainerpod&#34;&gt;MultiContainer Pod, Init Container&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#labelselector&#34;&gt;Label and Selector, Annotation, Namespace&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#deployment&#34;&gt;Deployment&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#replicaset&#34;&gt;Replicaset&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#rollout-rollback&#34;&gt;Rollout and Rollback&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#network-service&#34;&gt;Network, Service&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#liveness-readiness&#34;&gt;Liveness and Readiness Probe&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#environmentvariable&#34;&gt;Resource Limit, Environment Variable&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#volume&#34;&gt;Volume&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#secret&#34;&gt;Secret&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#configmap&#34;&gt;ConfigMap&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#node-pod-affinity&#34;&gt;Node ‚Äì Pod Affinity&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#taint-tolereation&#34;&gt;Taint and Toleration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#daemon-set&#34;&gt;Deamon Set&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#pvc&#34;&gt;Persistent Volume and Persistent Volume Claim&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#storageclass&#34;&gt;Storage Class&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#statefulset&#34;&gt;Stateful Set&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#job&#34;&gt;Job, CronJob&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#authentication&#34;&gt;Authentication, Role Based Access Control, Service Account&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#ingress&#34;&gt;Ingress&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#dashboard&#34;&gt;Dashboard&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#playWithKubernetes&#34;&gt;Play With Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#helm&#34;&gt;Helm: Kuberbetes Package Manager&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#cheatsheet&#34;&gt;Kubernetes Commands Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#helm_cheatsheet&#34;&gt;Helm Commands Cheatsheet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#cluster_setup&#34;&gt;Kubernetes Cluster Setup: Kubeadm, Containerd, Multipass&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#prometheus_grafana&#34;&gt;Monitoring Kubernetes Cluster with SSH, Prometheus and Grafana&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#resource&#34;&gt;Other Useful Resources Related Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/omerbsezer/Fast-Kubernetes/main/#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Motivation &lt;a name=&#34;motivation&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Why should we use Kubernetes? &#34;Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.&#34; (Ref: Kubernetes.io)&lt;/p&gt; &#xA;&lt;h3&gt;What is Containerization? What is Container Orchestration? &lt;a name=&#34;containerization&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Containerization is an operating system-level virtualization or application-level virtualization over multiple network resources so that software applications can run in isolated user spaces called containers in any cloud or non-cloud environment&#34; (wikipedia)&lt;/li&gt; &#xA; &lt;li&gt;With Docker Environment, we can create containers.&lt;/li&gt; &#xA; &lt;li&gt;Kubernetes and Docker Swarm are the container orchestration and management tools that automate and schedule the deployment, management, scaling, and networking of containers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/146249579-b4221dc1-bad7-4da5-831a-849a71fa849e.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Features &lt;a name=&#34;features&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Service discovery and load balancing:&lt;/strong&gt; Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Storage orchestration:&lt;/strong&gt; Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automated rollouts and rollbacks:&lt;/strong&gt;&amp;nbsp; You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automatic bin packing:&lt;/strong&gt; You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-monitoring:&lt;/strong&gt;&amp;nbsp;Kubernetes checks constantly the health of nodes and containers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Self-healing:&lt;/strong&gt; Kubernetes restarts containers that fail, replaces containers, kills containers that don&#39;t respond to your user-defined health check&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Automates various manual processes:&lt;/strong&gt;&amp;nbsp;for instance, Kubernetes will control for you which server will host the container, how it will be launched etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interacts with several groups of containers:&lt;/strong&gt;&amp;nbsp;Kubernetes is able to manage more cluster at the same time&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Provides additional services:&lt;/strong&gt;&amp;nbsp;as well as the management of containers, Kubernetes offers security, networking and storage services&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Horizontal scaling:&lt;/strong&gt;&amp;nbsp;Kubernetes allows you scaling resources not only vertically but also horizontally, easily and quickly&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Container balancing:&lt;/strong&gt;&amp;nbsp;Kubernetes always knows where to place containers, by calculating the ‚Äúbest location‚Äù for them&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run everywhere:&lt;/strong&gt;&amp;nbsp;Kubernetes is an open source tool and gives you the freedom to take advantage of on-premises, hybrid, or public cloud infrastructure, letting you move workloads to anywhere you want&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Secret and configuration management:&lt;/strong&gt; Kubernetes lets you store and manage sensitive information&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is Kubertenes? &lt;a name=&#34;whatIsKubernetes&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Kubernetes is a portable, extensible, open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.&#34; (Ref: Kubernetes.io)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/146247396-5bc3bbf9-41fa-47ff-b10d-cac305379e21.png&#34; alt=&#34;image&#34;&gt; (Ref: Kubernetes.io)&lt;/p&gt; &#xA;&lt;h3&gt;Kubertenes Architecture &lt;a name=&#34;architecture&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/146250114-18759a06-e6a6-4554-bc7f-b23a13534f77.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Kubernetes Components &lt;a name=&#34;components&#34;&gt;&lt;/a&gt; (Ref: Kubernetes.io)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Control Plane:&lt;/strong&gt; User enters commands and configuration files from control plane. It controls all cluster. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;API Server:&lt;/strong&gt; &#34;It exposes the Kubernetes API. The API server is the front end for the Kubernetes control plane.&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Etcd:&lt;/strong&gt; &#34;Consistent and highly-available key value store used as Kubernetes&#39; backing store for all cluster data (meta data, objects, etc.).&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Scheduler:&lt;/strong&gt; &#34;It watches for newly created Pods with no assigned node, and selects a node for them to run on. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Factors taken into account for scheduling decisions include: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;individual and collective resource requirements,&lt;/li&gt; &#xA;       &lt;li&gt;hardware/software/policy constraints,&lt;/li&gt; &#xA;       &lt;li&gt;affinity and anti-affinity specifications,&lt;/li&gt; &#xA;       &lt;li&gt;data locality,&lt;/li&gt; &#xA;       &lt;li&gt;inter-workload interference,&lt;/li&gt; &#xA;       &lt;li&gt;deadlines.&#34;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Controller Manager:&lt;/strong&gt; &#34;It runs controller processes. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Logically, each controller is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.&lt;/li&gt; &#xA;     &lt;li&gt;Some types of these controllers are: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Node controller: Responsible for noticing and responding when nodes go down.&lt;/li&gt; &#xA;       &lt;li&gt;Job controller: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.&lt;/li&gt; &#xA;       &lt;li&gt;Endpoints controller: Populates the Endpoints object (that is, joins Services &amp;amp; Pods).&lt;/li&gt; &#xA;       &lt;li&gt;Service Account &amp;amp; Token controllers: Create default accounts and API access tokens for new namespaces&#34;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Cloud Controller Manager:&lt;/strong&gt; &#34;It embeds cloud-specific control logic. The cloud controller manager lets you link your cluster into your cloud provider&#39;s API, and separates out the components that interact with that cloud platform from components that only interact with your cluster. The cloud-controller-manager only runs controllers that are specific to your cloud provider &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;The following controllers can have cloud provider dependencies: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding&lt;/li&gt; &#xA;       &lt;li&gt;Route controller: For setting up routes in the underlying cloud infrastructure&lt;/li&gt; &#xA;       &lt;li&gt;Service controller: For creating, updating and deleting cloud provider load balancers.&#34;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Node:&lt;/strong&gt; &#34;Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.&#34; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Kubelet:&lt;/strong&gt; &#34;An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod. The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy.&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Kube-proxy:&lt;/strong&gt; &#34;It is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;It maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.&lt;/li&gt; &#xA;     &lt;li&gt;It uses the operating system packet filtering layer if there is one and it&#39;s available. Otherwise, kube-proxy forwards the traffic itself.&#34;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Container Runtime:&lt;/strong&gt; &#34;The container runtime is the software that is responsible for running containers. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Kubernetes supports several container runtimes: &lt;strong&gt;Docker, containerd, CRI-O,&lt;/strong&gt; and any implementation of the Kubernetes CRI (Container Runtime Interface)&#34;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/146250916-a9298521-526b-451a-9810-6813e4165db5.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Installation &lt;a name=&#34;installation&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Download:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Kubectl:&lt;/strong&gt; The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Minikube:&lt;/strong&gt; It is a tool that lets you run Kubernetes locally. It runs a single-node Kubernetes cluster on your personal computer (&lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34;&gt;https://minikube.sigs.k8s.io/docs/start/&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;KubeAdm:&lt;/strong&gt; You can use the kubeadm tool to create and manage Kubernetes clusters. This is for creating cluster with computers (Goto: &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Kubeadm-Cluster-Setup.md&#34;&gt;LAB: K8s Kubeadm Cluster Setup&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;from here=&amp;gt; &lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/&#34;&gt;https://kubernetes.io/docs/tasks/tools/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For learning K8s and running on a computer, &lt;strong&gt;Kubectl and Minikube&lt;/strong&gt; are enough to install.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; Cloud providers (Azure, Google Cloud, AWS) offer managed K8s (control plane is managed by cloud provides). You can easily create your cluster (number of computer and details) and make connection with Kubectl (using CLI get-credentials of cluster on the cloud)&lt;/p&gt; &#xA;&lt;h3&gt;Kubectl Config ‚Äì Usage &lt;a name=&#34;kubectl&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Config File&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can communicate with K8s cluster in different ways: REST API, Command Line Tool (CLI-Kubectl), GUI (kube-dashboard, etc.)&lt;/li&gt; &#xA; &lt;li&gt;After installation, you can find the kubernetes config file (C:\Users\User.kube\config) that is YAML file.&lt;/li&gt; &#xA; &lt;li&gt;Config file contains 3 main parts: Clusters (cluster certificate data, server, name), Context (cluster and user, namespace), Users (name, config features, certificates, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Usage&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Kubectl is our main command line tool that connects minikube. There are many combination of commands. So it is not possible to list all commands.&lt;/li&gt; &#xA; &lt;li&gt;When run &#34;kubectl&#34; on the terminal, it can be seen some simple commands. Also &#34;kubectl &lt;command&gt; --help&#34; gives more information.&lt;/li&gt; &#xA; &lt;li&gt;Pattern: kubectl [get|delete|edit|apply] [pods|deployment|services] [podName|serviceName|deploymentName]&lt;/li&gt; &#xA; &lt;li&gt;Example: &#34;kubectl get pods podName&#34;, &#34;kubectl delete pods test_pod&#34;, &#34;kubectl describe pods firstpod&#34;, etc.&lt;/li&gt; &#xA; &lt;li&gt;All necessary/most usable commands are listed in the &#34;&lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/KubernetesCommandCheatSheet.md&#34;&gt;Kubernetes Commands Cheatsheet&lt;/a&gt;&#34;. Please have a look to get more information and usage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pod: Creating, Yaml, LifeCycle &lt;a name=&#34;pod&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pod is the smallest unit that is created and managed in K8s.&lt;/li&gt; &#xA; &lt;li&gt;Pods may contain more than 1 container, but mostly pods contain only 1 container.&lt;/li&gt; &#xA; &lt;li&gt;Each pod has unique id (uid).&lt;/li&gt; &#xA; &lt;li&gt;Each pod has unique IP address.&lt;/li&gt; &#xA; &lt;li&gt;Containers in the same Pod run on the same Node (computer), and these containers can communicate with each other on the localhost.&lt;/li&gt; &#xA; &lt;li&gt;Creation of the first pod, IMPERATIVE WAY (with command):&lt;/li&gt; &#xA; &lt;li&gt;Please have a look Scenario (&lt;strong&gt;Creating Pod - Imperative way&lt;/strong&gt;, below link) to learn more information about the pod&#39;s kubectl commands. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;how to create basic K8s pod using imperative commands,&lt;/li&gt; &#xA;   &lt;li&gt;how to get more information about pod (to solve troubleshooting),&lt;/li&gt; &#xA;   &lt;li&gt;how to run commands in pod,&lt;/li&gt; &#xA;   &lt;li&gt;how to delete pod.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-CreatingPod-Imperative.md&#34;&gt;LAB: K8s Creating Pod - Imperative Way&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Pod: YAML File&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Imperative way could be difficult to store and manage process. Every time we have to enter commands. To prevent this, we can use YAML file to define pods and pods&#39; feature. This way is called &#34;Declarative Way&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Declarative way (with file), Imperative way (with command)&lt;/li&gt; &#xA; &lt;li&gt;Sample Yaml File:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153674712-426a262d-d13e-489d-9c86-63ac22114d75.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please have a look Scenario (&lt;strong&gt;Creating Pod - Declarative way&lt;/strong&gt;, below link) to learn more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8-CreatingPod-Declerative.md&#34;&gt;LAB: K8s Creating Pod - Declarative Way (With File) - Environment Variable&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Pod: Life Cycle&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pending:&lt;/strong&gt; API-&amp;gt;etcd, pod created, pod id created, but not running on the node.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Creating:&lt;/strong&gt; Scheduler take pod from etcd, assing on node. Kubelet on the Node pull images from docker registry or repository.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ImagePullBackOff:&lt;/strong&gt; Kubelet can not pull image from registry. E.g. Image name is fault (typo error), Authorization Failure, Username/Pass error.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Running:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Container closes in 3 ways: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;App completes the mission and closes automatically without giving error,&lt;/li&gt; &#xA;     &lt;li&gt;Use or System sends close signal and closes automatically without giving error,&lt;/li&gt; &#xA;     &lt;li&gt;Giving error, collapsed and closes with giving error code.&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Restart Policies (it can defined in the pod definition): &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Always: Default value, kubelet starts always when closing with or without error,&lt;/li&gt; &#xA;     &lt;li&gt;On-failure: It starts again when it gets only error,&lt;/li&gt; &#xA;     &lt;li&gt;Never: It never restarts in any case.&lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Successed (completed)&lt;/strong&gt;: If the container closes successfully without error and restart policy is configured as on-failure/never, it converts to succeed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Failed&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;CrashLoopBackOff:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If restart policy is configured as always and container closes again and again, container restarts again and again (Restart waiting duration before restarting again: 10 sec -&amp;gt; 20 sec -&amp;gt; 40 sec -&amp;gt; .. -&amp;gt; 5mins), It runs every 5 mins if the pod is crashed.&lt;/li&gt; &#xA;   &lt;li&gt;If container runs more than 10 mins, status converted from &#39;CrashLoopBackOff&#39; to &#39;Running&#39;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;MultiContainer Pod, Init Container &lt;a name=&#34;multicontainerpod&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Best Practice: 1 Container runs in 1 Pod normally, because the smallest element in K8s is Pod (Pod can be scaled up/down).&lt;/li&gt; &#xA; &lt;li&gt;Multicontainers run in the same Pod when containers are dependent of each other.&lt;/li&gt; &#xA; &lt;li&gt;Multicontainers in one Pod have following features: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Multi containers that run on the same Pod run on the same Node.&lt;/li&gt; &#xA;   &lt;li&gt;Containers in the same Pod run/pause/deleted at the same time.&lt;/li&gt; &#xA;   &lt;li&gt;Containers in the same Pod communicate with each other on localhost, there is not any network isolation.&lt;/li&gt; &#xA;   &lt;li&gt;Containers in the same Pod use one volume commonly and they can reasch same files in the volume.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Init Container&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Init container is used for configuration of app before running app container.&lt;/li&gt; &#xA; &lt;li&gt;Init container handle what it should run, then it closes successfully, after init container close, app container starts.&lt;/li&gt; &#xA; &lt;li&gt;Example below shows how to define init containers in one Pod. There are 2 containers: appcontainer and initcontainer. Initcontainer is polling the service (myservice). When it finds, it closes and app container starts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: initcontainerpod&#xA;spec:&#xA;  containers:&#xA;  - name: appcontainer            # after initcontainer closed successfully, appcontainer starts.&#xA;    image: busybox&#xA;    command: [&#39;sh&#39;, &#39;-c&#39;, &#39;echo The app is running! &amp;amp;&amp;amp; sleep 3600&#39;]&#xA;  initContainers:&#xA;  - name: initcontainer&#xA;    image: busybox                # init container starts firstly and look up myservice is up or not in every 2 seconds, if there is myservice available, initcontainer closes. &#xA;    command: [&#39;sh&#39;, &#39;-c&#39;, &#34;until nslookup myservice; do echo waiting for myservice; sleep 2; done&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# save as service.yaml and run after pod creation&#xA;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: myservice&#xA;spec:&#xA;  ports:&#xA;  - protocol: TCP&#xA;    port: 80&#xA;    targetPort: 9376&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please have a look Scenario (below link) to learn more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Multicontainer-Sidecar.md&#34;&gt;LAB: K8s Multicontainer - Sidecar - Emptydir Volume - Port-Forwarding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Label and Selector, Annotation, Namespace &lt;a name=&#34;labelselector&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Label&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Label is important to reach the K8s objects with key:value pairs.&lt;/li&gt; &#xA; &lt;li&gt;key:value is used for labels. E.g. tier:frontend, stage:test, name:app1, team:development&lt;/li&gt; &#xA; &lt;li&gt;prefix may also be used for optional with key:value. E.g. example.com/tier:front-end, kubernetes.io/ , k8s.io/&lt;/li&gt; &#xA; &lt;li&gt;In the file (declerative way), labels are added under metadata. It is possible to add multiple labels.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153675164-62265978-60c3-4167-ad0c-4bfbbf1f704b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In the command (imperative way), we can also add label to the pods.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl label pods pod1 team=development  #adding label team=development on pod1&#xA;kubectl get pods --show-labels&#xA;kubectl label pods pod1 team-  #remove team (key:value) from pod1&#xA;kubectl label --overwrite pods pod1 team=test #overwrite/change label on pod1&#xA;kubectl label pods --all foo=bar  # add label foo=bar for all pods&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Selector&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We can select/filter pod with kubectl command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl get pods -l &#34;app=firstapp&#34; --show-labels&#xA;kubectl get pods -l &#34;app=firstapp,tier=frontend&#34; --show-labels&#xA;kubectl get pods -l &#34;app=firstapp,tier!=frontend&#34; --show-labels&#xA;kubectl get pods -l &#34;app,tier=frontend&#34; --show-labels #equality-based selector&#xA;kubectl get pods -l &#34;app in (firstapp)&#34; --show-labels  #set-based selector&#xA;kubectl get pods -l &#34;app not in (firstapp)&#34; --show-labels  #set-based selector&#xA;kubectl get pods -l &#34;app=firstapp,app=secondapp&#34; --show-labels # comma means and =&amp;gt; firstapp and secondapp&#xA;kubectl get pods -l &#34;app in (firstapp,secondapp)&#34; --show-labels # it means or =&amp;gt; firstapp or secondapp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Node Selector&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With Node Selector, we can choose which pod run on which Node.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153676102-03b2137b-ecc8-4802-9a9f-41694e1ce6fa.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is also possible to label nodes with imperative way.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl apply -f podnode.yaml&#xA;kubectl get pods -w #always watch&#xA;kubectl label nodes minikube hddtype=ssd #after labelling node, pod11 configuration can run, because node is labelled with hddtype:ssd &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Annotation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is similar to label, but it is used for the detailed information (e.g. owner, notification-email, releasedate, etc.) that are not used for linking objects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153675516-4b71b55a-f7ec-40a4-9e32-0b794208e6ae.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl apply -f podannotation.yaml&#xA;kubectl describe pod annotationpod&#xA;kubectl annotate pods annotationpod foo=bar #imperative way&#xA;kubectl delete -f podannotation.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Namespaces&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Namespaces provides a mechanism for isolating groups of resources within a single cluster. It provides a scope for names.&lt;/li&gt; &#xA; &lt;li&gt;Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.&lt;/li&gt; &#xA; &lt;li&gt;Kubectl commands run on default namespaces if it is not determined in the command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/148784384-96681287-e4c4-46e8-b63f-5953270a5b28.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl get pods --namespaces kube-system  #get all pods in the kube-system namespaces&#xA;kubectl get pods --all-namespaces  # get pods from all namespaces&#xA;kubectl create namespace development  #create new development namespace in imperative way&#xA;kubectl get pods -n development  # get pods from all namespace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In declerative way, it is possible to create namespaces and run pod on the related namespace.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153675331-ee6ccfb6-b186-4e29-8e85-55adee465a53.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl apply -f namespace.yaml&#xA;kubectl get pods -n development  #get pods in the development namespace&#xA;kubectl exec -it namespacedpod -n development -- /bin/sh  #run namespacepod in development namespace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We can avoid to use -n &#xA;  &lt;namespacename&gt;&#xA;    for all command with changing of default namespace (because, if we don&#39;t use -n namespace, kubectl commands run on the default namespace).&#xA;  &lt;/namespacename&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl config set-context --current  --namespace=development  #now default namespace is development&#xA;kubectl get pods     #returns pods in the development namespace  &#xA;kubectl config set-context --current  --namespace=default  #now namespace is default &#xA;kubectl delete namespaces development  #delete development namespace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deployment &lt;a name=&#34;deployment&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A Deployment provides declarative updates for Pods and ReplicaSets.&lt;/li&gt; &#xA; &lt;li&gt;We define states in the deployment, deployment controller compares desired state and take necessary actions to keep desire state.&lt;/li&gt; &#xA; &lt;li&gt;Deployment object is the higher level K8s object that controls and keeps state of single or multiple pods automatically.&lt;/li&gt; &#xA; &lt;li&gt;Imperative way:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl create deployment firstdeployment --image=nginx:latest --replicas=2 &#xA;kubectl get deployments&#xA;kubectl get pods -w    #on another terminal&#xA;kubectl delete pods &amp;lt;oneofthepodname&amp;gt; #we can see another terminal, new pod will be created (to keep 2 replicas)  &#xA;kubectl scale deployments firstdeployment --replicas=5&#xA;kubectl delete deployments firstdeployment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Please have a look Scenario (below link) to learn more about the deployment and declerative way of creating deployment.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Deployment.md&#34;&gt;LAB: K8s Deployment - Scale Up/Down - Bash Connection - Port Forwarding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Replicaset &lt;a name=&#34;replicaset&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deployment object create Replicaset object. Deployment provides the transition of the different replicaset automatically.&lt;/li&gt; &#xA; &lt;li&gt;Replicaset is the responsible for the management of replica creation and remove. But, when the pods are updated (e.g. image changed), it can not update replicaset pods. However, deployment can update for all change. So, best practice is to use deployment directly, not to use replicaset directly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Important:&lt;/strong&gt; It can be possible to create replicaset directly, but we could not use rollout/rollback, undo features with replicaset. Deployment provide to use rollout/rollback, undo features.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/148804992-8ad27155-1c1e-436f-949e-4aec9a1a9d05.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Rollout and Rollback &lt;a name=&#34;rollout-rollback&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rollout and Rollback enable to update and return back containers that run under the deployment.&lt;/li&gt; &#xA; &lt;li&gt;2 strategy for rollout: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Recreate Strategy:&lt;/strong&gt; Delete all pods firstly and create Pods from scratch. If two different version of SW affect each other negatively, this strategy could be used.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;RollingUpdate Strategy (default)&lt;/strong&gt;: It updates pods step by step. Pods are updated step by step, all pods are not deleted at the same time. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;maxUnavailable:&lt;/strong&gt; At the update duration, it shows the max number of deleted containers (total:10 container; if maxUn:2, min:8 containers run in that time period)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;maxSurge:&lt;/strong&gt; At the update duration, it shows that the max number of containers run on the cluster (total:10 container; if maxSurge:2, max:12 containers run in a time)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl set image deployment rolldeployment nginx=httpd:alpine --record     # change image of deployment&#xA;kubectl rollout history deployment rolldeployment                           #shows record/history revisions &#xA;kubectl rollout history deployment rolldeployment --revision=2              #select the details of the one of the revisions&#xA;kubectl rollout undo deployment rolldeployment                              #returns back to previous deployment revision&#xA;kubectl rollout undo deployment rolldeployment --to-revision=1              #returns back to the selected revision=1&#xA;kubectl rollout status deployment rolldeployment -w                         #show live status of the rollout deployment&#xA;kubectl rollout pause deployment rolldeployment                             #pause the rollout while updating pods &#xA;kubectl rollout resume deployment rolldeployment                            #resume the rollout if rollout paused&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Rollout-Rollback.md&#34;&gt;LAB: K8s Rollout - Rollback&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Network, Service &lt;a name=&#34;network-service&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;K8s Networking Requirements&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Each pod has unique and own IP address (Containers within a pod share network namespaces).&lt;/li&gt; &#xA; &lt;li&gt;All PODs can communicate with all other pods without NAT (Network Address Translation)&lt;/li&gt; &#xA; &lt;li&gt;All NODEs can communicate with all pods without NAT.&lt;/li&gt; &#xA; &lt;li&gt;The IP of the POD is same throughout the cluster.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;CNI (Container Network Interface)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Networking of container and nodes with different vendors and devices is difficult to handle. So K8s give this responsibility to CNI plugins to handle networking requirements.&lt;/li&gt; &#xA; &lt;li&gt;&#34;CNI (Container Network Interface), a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins.&#34; =&amp;gt; &lt;a href=&#34;https://github.com/containernetworking/cni&#34;&gt;https://github.com/containernetworking/cni&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;K8s has CNI plugins that are selected by the users. Some of the CNI methods are: Flannel, calico, weave, and canal.&lt;/li&gt; &#xA; &lt;li&gt;Calico (&lt;a href=&#34;https://github.com/projectcalico/calico&#34;&gt;https://github.com/projectcalico/calico&lt;/a&gt;) is the one of the popular and open source CNI method/plugin in K8s. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Network Management in the cluster: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;IP assignments to Pods&lt;/li&gt; &#xA;     &lt;li&gt;IP Table Management&lt;/li&gt; &#xA;     &lt;li&gt;Overlay definition between Nodes without using NAT (e.g. --pod-network-cidr management)&lt;/li&gt; &#xA;     &lt;li&gt;Vxlan Interface implementation and etc.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Service&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;An abstract way to expose an application running on a set of Pods as a network service.&lt;/li&gt; &#xA; &lt;li&gt;Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.&lt;/li&gt; &#xA; &lt;li&gt;Type values and their behaviors are: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ClusterIP:&lt;/strong&gt; Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NodePort:&lt;/strong&gt; Exposes the Service on each Node&#39;s IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You&#39;ll be able to contact the NodePort Service, from outside the cluster, by requesting &#xA;    &lt;nodeip&gt;&#xA;     :&#xA;     &lt;nodeport&gt;&#xA;      .&#xA;     &lt;/nodeport&gt;&#xA;    &lt;/nodeip&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;LoadBalancer:&lt;/strong&gt; Exposes the Service externally using a cloud provider&#39;s load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;ExternalName:&lt;/strong&gt; Maps the Service to the contents of the externalName field (e.g. foo.bar.example.com), by returning a CNAME record with its value. No proxying of any kind is set up.&#34; (Ref: Kubernetes.io)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Example of Service Object Definition: (Selector binds service to the related pods, get traffic from port 80 to port 9376)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Service&#xA;metadata:&#xA;  name: my-service&#xA;spec:&#xA;  selector:&#xA;    app: MyApp&#xA;  ports:&#xA;    - protocol: TCP&#xA;      port: 80&#xA;      targetPort: 9376&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Service-App.md&#34;&gt;LAB: K8s Service Implementations (ClusterIp, NodePort and LoadBalancer)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Liveness and Readiness Probe &lt;a name=&#34;liveness-readiness&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Liveness Probe&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;The kubelet uses liveness probes to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress.&#34; (Ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;There are different ways of controlling Pods: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;httpGet,&lt;/li&gt; &#xA;   &lt;li&gt;exec command,&lt;/li&gt; &#xA;   &lt;li&gt;tcpSocket,&lt;/li&gt; &#xA;   &lt;li&gt;grpc, etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;initialDelaySeconds: waiting some period of time after starting. e.g. 5sec, after 5 sec start to run command&lt;/li&gt; &#xA; &lt;li&gt;periodSeconds: in a period of time, run command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Liveness-App.md&#34;&gt;LAB: K8s Liveness Probe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Readiness Probe&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;Sometimes, applications are temporarily unable to serve traffic. For example, an application might need to load large data or configuration files during startup, or depend on external services after startup. In such cases, you don&#39;t want to kill the application, but you don&#39;t want to send it requests either. Kubernetes provides readiness probes to detect and mitigate these situations. A pod with containers reporting that they are not ready does not receive traffic through Kubernetes Services.&#34; (Ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;Readiness probe is similar to liveness pod. Only difference is to define &#34;readinessProbe&#34; instead of &#34;livenessProbe&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Resource Limit, Environment Variable &lt;a name=&#34;environmentvariable&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Resource Limit&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pods can consume resources (cpu, memory) up to physical resource limits, if there was not any limitation.&lt;/li&gt; &#xA; &lt;li&gt;Pods&#39; used resources can be limited. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;use 1 cpu core =&amp;gt; cpu = &#34;1&#34; = &#34;1000&#34; = &#34;1000m&#34;&lt;/li&gt; &#xA;   &lt;li&gt;use 10% of 1 cpu core =&amp;gt; cpu = &#34;0.1&#34; = &#34;100&#34; = &#34;100m&#34;&lt;/li&gt; &#xA;   &lt;li&gt;use 64 MB =&amp;gt; memory: &#34;64M&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;CPU resources are exactly limited when it defines.&lt;/li&gt; &#xA; &lt;li&gt;When pod requests memory resource more than limitation, pod changes its status to &#34;OOMKilled&#34; and restarts itself to limit memory usage.&lt;/li&gt; &#xA; &lt;li&gt;Example (below), pod requests 64MB memory and 0.25 CPU core, uses maximum 256MB memory and 0.5 CPU core.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153676383-eb783491-79da-4886-9728-55977b6bbd88.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Environment Variable&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Environment Variables can be defined for each pods in the YAML file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/153676628-d103de1d-e223-451b-8337-cdfe1cebee66.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8-CreatingPod-Declerative.md&#34;&gt;LAB: K8s Creating Pod - Declarative Way - Environment Variable&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Volume &lt;a name=&#34;volume&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ephemeral volume (Temporary volume): Multiple containers reach ephemeral volume in the pod. When the pod is deleted/killed, volume is also deleted. But when container is restarted, volume is still available because pod still runs.&lt;/li&gt; &#xA; &lt;li&gt;There are 2 types of ephemeral volumes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Emptydir&lt;/li&gt; &#xA;   &lt;li&gt;Hostpath &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Directory&lt;/li&gt; &#xA;     &lt;li&gt;DirectoryOrCreate&lt;/li&gt; &#xA;     &lt;li&gt;FileOrCreate&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Emptydir Volume&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Emptydir (empty directory on the node) is created on which node the pod is created on and it is mounted on the container using &#34;volumeMounts&#34;. Multiple containers in the pod can reach this volume (read/write).&lt;/li&gt; &#xA; &lt;li&gt;Emptydir volume is dependent of Pod Lifecycle. If the pod is deleted, emptydir is also deleted.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;spec: &#xA;  containers:&#xA;  - name: sidecar&#xA;    image: busybox&#xA;    command: [&#34;/bin/sh&#34;]&#xA;    args: [&#34;-c&#34;, &#34;sleep 3600&#34;]&#xA;    volumeMounts:                # volume is mounted under &#34;volumeMounts&#34; &#xA;    - name: cache-vol            # &#34;name&#34; of the volume type&#xA;      mountPath: /tmp/log        # &#34;mountPath&#34; is the path in the container.&#xA;  volumes:&#xA;  - name: cache-vol              &#xA;    emptyDir: {}                 # &#34;volume&#34; type &#34;emptydir&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Multicontainer-Sidecar.md&#34;&gt;LAB: K8s Multicontainer - Sidecar - Emptydir Volume - Port-Forwarding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Hostpath Volume&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is similar to emtpydir, hostpath is also created on which node the pod is created on. In addition, the hostpath is specifically defined path on the node.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: hostpath&#xA;spec:&#xA;  containers:&#xA;  - name: hostpathcontainer&#xA;    image: ImageName                  # e.g. nginx&#xA;    volumeMounts:&#xA;    - name: directory-vol             # container connects &#34;volume&#34; name    &#xA;      mountPath: /dir1                # on the container which path this volume is mounted&#xA;    - name: dircreate-vol&#xA;      mountPath: /cache               # on the container which path this volume is mounted&#xA;    - name: file-vol&#xA;      mountPath: /cache/config.json   # on the container which file this volume is mounted     &#xA;  volumes:&#xA;  - name: directory-vol               # &#34;volume&#34; name&#xA;    hostPath:                         # &#34;volume&#34; type &#34;hostpath&#34;&#xA;      path: /tmp                      # &#34;path&#34; on the node, &#34;/tmp&#34; is defined volume&#xA;      type: Directory                 # &#34;hostpath&#34; type &#34;Directory&#34;, existed directory&#xA;  - name: dircreate-vol&#xA;    hostPath:                         # &#34;volume&#34; type &#34;hostpath&#34;&#xA;      path: /cache                    # &#34;path&#34; on the node&#xA;      type: DirectoryOrCreate         # &#34;hostpath&#34; type &#34;DirectoryOrCreate&#34;, if it is not existed, create directory&#xA;  - name: file-vol&#xA;    hostPath:                         # &#34;volume&#34; type &#34;hostpath&#34;&#xA;      path: /cache/config.json        # &#34;path&#34; on the node&#xA;      type: FileOrCreate              # &#34;hostpath&#34; type &#34;FileOrCreate&#34;,  if it is not existed, create file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154715083-f5972de0-d95e-47f2-bc6d-92cf7b8a182a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Secret &lt;a name=&#34;secret&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Secret objects store the sensitive and secure information like username, password, ssh-tokens, certificates.&lt;/li&gt; &#xA; &lt;li&gt;Secrets (that you defined) and pods (that you defined) should be in the same namespace (e.g. if defined secret is in the &#34;default&#34; namespace, pod should be also in the &#34;default&#34; namepace).&lt;/li&gt; &#xA; &lt;li&gt;There are 8 different secret types (basic-auth, tls, ssh-auth, token, service-account-token, dockercfg, dockerconfigjson, opaque). Opaque type is the default one and mostly used.&lt;/li&gt; &#xA; &lt;li&gt;Secrets are called by the pod in 2 different ways: volume and environment variable&lt;/li&gt; &#xA; &lt;li&gt;Imperative way, run on the terminal (geneneric in the command = opaque):&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl create secret generic mysecret2 --from-literal=db_server=db.example.com --from-literal=db_username=admin --from-literal=db_password=P@ssw0rd!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Imperative way with file to hide pass in the command history&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl create secret generic mysecret3 --from-file=db_server=server.txt --from-file=db_username=username.txt --from-file=db_password=password.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Imperative way with json file to hide pass in the command history&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;kubectl create secret generic mysecret4 --from-file=config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Secret.md&#34;&gt;LAB: K8s Secret (Declarative and Imperative Way)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ConfigMap &lt;a name=&#34;configmap&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is same as &#34;secrets&#34;. The difference is that configmap does not save sensitive information. It stores config variables.&lt;/li&gt; &#xA; &lt;li&gt;Configmap stores data with key-value pairs.&lt;/li&gt; &#xA; &lt;li&gt;Configmaps are called by the pod in 2 different ways: volume and environment variable&lt;/li&gt; &#xA; &lt;li&gt;Scenario shows the usage of configmaps.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Configmap.md&#34;&gt;LAB: K8s Config Map&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Node ‚Äì Pod Affinity &lt;a name=&#34;node-pod-affinity&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Affinity means closeness, proximity, familarity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Node Affinity&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;With node affinity, specific pods can enable to run on the desired node (Node selector also supports that feature, but node affinity is more flexible).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If node is labelled with key-value, we can run some of the pods on that specific node.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Terms for Node Affinity:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;requiredDuringSchedulingIgnoredDuringExecution:&lt;/strong&gt; Find a node during scheduling according to &#34;matchExpression&#34; and run pod on that node. If it is not found, do not run this pod until finding specific node &#34;matchExpression&#34;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;IgnoredDuringExecution:&lt;/strong&gt; After scheduling, if the node label is removed/deleted from node, ignore it while executing.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;preferredDuringSchedulingIgnoredDuringExecution:&lt;/strong&gt; Find a node during scheduling according to &#34;matchExpression&#34; and run pod on that node. If it is not found, run this pod wherever it finds. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;weight:&lt;/strong&gt; Preference weight. If weight is more than other weights, this weight is higher priority than others.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To understand better, please have a look &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Node-Affinity.md&#34;&gt;LAB: K8s Node Affinity&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Node-Affinity.md&#34;&gt;LAB: K8s Node Affinity&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Pod Affinity&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Some of the pods should run with other pods on same node or same availability zone (e.g. frontend pods run with cache pod on the same availability zone)&lt;/li&gt; &#xA; &lt;li&gt;If pod affinity is defined for one pod, that pod runs with the related pod on same node or same availability zone.&lt;/li&gt; &#xA; &lt;li&gt;Each node in the cluster is labelled with default labels. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;kubernetes.io/hostname&#34;: e.g &#34;kubernetes.io/hostname=minikube&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;kubernetes.io/arch&#34;: e.g &#34;kubernetes.io/arch=amd64&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;kubernetes.io/os&#34;: e.g &#34;kubernetes.io/os=linux&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Each node in the cluster that runs on the Cloud is labelled with following labels. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;topology.kubernetes.io/region&#34;: e.g. &#34;topology.kubernetes.io/region=northeurope&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;topology.kubernetes.io/zone&#34;: e.g. &#34;topology.kubernetes.io/zone=northeurope-1&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: frontendpod&#xA;  labels:&#xA;    app: frontend                                     # defined labels&#xA;    deployment: test                      &#xA;spec:&#xA;  containers:&#xA;  - name: nginx&#xA;    image: nginx:latest&#xA;    ports:&#xA;    - containerPort: 80&#xA;---&#xA;apiVersion: v1&#xA;kind: Pod&#xA;metadata:&#xA;  name: cachepod&#xA;spec:&#xA;  affinity:&#xA;    podAffinity:&#xA;      requiredDuringSchedulingIgnoredDuringExecution:    # required: if not find, not run this pod on any node&#xA;      - labelSelector:&#xA;          matchExpressions:&#xA;          - key: app&#xA;            operator: In&#xA;            values:&#xA;            - frontend&#xA;        topologyKey: kubernetes.io/hostname               # run this pod with the POD which includes &#34;app=frontend&#34; on the same worker NODE  &#xA;      preferredDuringSchedulingIgnoredDuringExecution:    # preferred: if not find, run this pod on any node&#xA;      - weight: 1&#xA;        podAffinityTerm:&#xA;          labelSelector:&#xA;            matchExpressions:&#xA;            - key: branch&#xA;              operator: In&#xA;              values:&#xA;              - develop&#xA;          topologyKey: topology.kubernetes.io/zone         # run this pod with the POD which includes &#34;branch=develop&#34; on the any NODE in the same ZONE &#xA;    podAntiAffinity:                                       # anti-affinity: NOT run this pod with the following match &#34;&#34;&#xA;      preferredDuringSchedulingIgnoredDuringExecution:&#xA;      - weight: 100&#xA;        podAffinityTerm:&#xA;          labelSelector:&#xA;            matchExpressions:&#xA;            - key: deployment&#xA;              operator: In&#xA;              values:&#xA;              - prod&#xA;          topologyKey: topology.kubernetes.io/zone         # NOT run this pod with the POD which includes &#34;deployment=prod&#34; on the any NODE in the same ZONE   &#xA;  containers:&#xA;  - name: cachecontainer                                   # cache image and container name&#xA;    image: redis:6-alpine&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154729871-1294d423-1429-4a00-9d2b-78cfcdace18a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154730052-19e96985-1452-4d93-9fc3-d70ea06ceb8a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Taint and Toleration &lt;a name=&#34;taint-tolereation&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node affinity is a property of Pods that attracts/accepts them to a set of nodes. Taints are the opposite, they allow a node to repel/reject a set of pods.&lt;/li&gt; &#xA; &lt;li&gt;TAINTs are assigned to the NODEs. TOLERATIONs assigned to the PODs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;kubectl describe nodes minikube&#34;, at taints section, it can be seen taints.&lt;/li&gt; &#xA;   &lt;li&gt;To add taint to the node with commmand: &#34;kubectl taint node minikube app=production:NoSchedule&#34;&lt;/li&gt; &#xA;   &lt;li&gt;To delete taint to the node with commmand: &#34;kubectl taint node minikube app-&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If pod has not any toleration for related taint, it can not be started on the tainted node (status of pod remains pending)&lt;/li&gt; &#xA; &lt;li&gt;Taint Types: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;key1=value1:effect&lt;/strong&gt;: (e.g.&#34;kubectl taint node minikube app=production:NoSchedule&#34;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Taint &#34;effect&#34; types: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NoSchedule:&lt;/strong&gt; If pod is not tolerated with this effect, it can not run on the related node (status will be pending, until toleration/untaint)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;PreferNoSchedule:&lt;/strong&gt; If pod is not tolerated with this effect and if there is not any untainted node, it can run on the related node.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;NoExecute:&lt;/strong&gt; If pod is not tolerated with this effect, it can not run on the related node. If there are pods running on the node before assigning &#34;NoExecute&#34; taint, after tainting &#34;NoExecute&#34;, untolerated pods stopped on this node.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;For clarification, please have a look &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Taint-Toleration.md&#34;&gt;LAB: K8s Taint Toleration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the Scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Taint-Toleration.md&#34;&gt;LAB: K8s Taint-Toleration&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Deamon Set &lt;a name=&#34;daemon-set&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It provides to run pods on EACH nodes. It can be configured to run only specific nodes.&lt;/li&gt; &#xA; &lt;li&gt;For example, you can run log application that runs on each node in the cluster and app sends these logs to the main log server. Manual configuration of each nodes could be headache in this sceneario, so using deamon sets would be beneficial to save time and effort.&lt;/li&gt; &#xA; &lt;li&gt;If the new nodes are added on the cluster and running deamon sets on the cluster at that time period, default pods which are defined on deamon sets also run on the new nodes without any action.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Daemon-Sets.md&#34;&gt;LAB: K8s Daemonset - Creating 3 nodes on Minikube&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Persistent Volume and Persistent Volume Claim &lt;a name=&#34;pvc&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Volumes are ephemeral/temporary area that stores data. Emptydir and hostpath create volume on node which runs related pod.&lt;/li&gt; &#xA; &lt;li&gt;In the scenario of creating Mysql pod on cluster, we can not use emptydir and hostpath for long term. Because they don&#39;t provide the long term/persistent volume.&lt;/li&gt; &#xA; &lt;li&gt;Persistent volume provides long term storage area that runs out of the cluster.&lt;/li&gt; &#xA; &lt;li&gt;There are many storage solutions that can be enabled on the cluster: nfs, iscsi, azure disk, aws ebs, google pd, cephfs.&lt;/li&gt; &#xA; &lt;li&gt;Container Storage Interface (CSI) provides the connection of K8s cluster and different storage solution.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Persistent Volume&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;accessModes&#34; types: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;ReadWriteOnce&#34;: read/write for only 1 node.&lt;/li&gt; &#xA;   &lt;li&gt;&#34;ReadOnlyMany&#34; : only read for many nodes.&lt;/li&gt; &#xA;   &lt;li&gt;&#34;ReadWriteMany&#34;: read/write for many nodes.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&#34;persistentVolumeReclaimPolicy&#34; types: it defines the behaviour of volume after the end of using volume. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;Retain&#34; : volume remains with all data after using it.&lt;/li&gt; &#xA;   &lt;li&gt;&#34;Recycle&#34;: volume is not deleted but all data in the volume is deleted. We get empty volume if it is chosen.&lt;/li&gt; &#xA;   &lt;li&gt;&#34;Delete&#34; : volume is deleted after using it.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Creating Persistent Volume on NFS Server on the network    &#xA;apiVersion: v1                               &#xA;kind: PersistentVolume&#xA;metadata:&#xA;   name: mysqlpv&#xA;   labels:&#xA;     app: mysql                                # labelled PV with &#34;mysql&#34;&#xA;spec:&#xA;  capacity:&#xA;    storage: 5Gi                               # 5Gibibyte = power of 2; 5GB= power of 10&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  persistentVolumeReclaimPolicy: Recycle       # volume is not deleted, all data in the volume will be deleted.&#xA;  nfs:&#xA;    path: /tmp                                 # binds the path on the NFS Server&#xA;    server: 10.255.255.10                      # IP of NFS Server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154734368-323af0cc-e745-4aa0-b844-65b4a410426d.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Persistent Volume Claim (PVC)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We should create PVC to use volume. With PVC, existed PVs can be chosen.&lt;/li&gt; &#xA; &lt;li&gt;The reason why K8s manage volume with 2 files (PVC and PV) is to seperate the management of K8s Cluster (PV) and using of volume (PVC).&lt;/li&gt; &#xA; &lt;li&gt;If there is seperate role of system management of K8s cluster, system manager creates PV (to connect different storage vendors), developers only use existed PVs with PVCs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: PersistentVolumeClaim&#xA;metadata:&#xA;  name: mysqlclaim&#xA;spec:&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  volumeMode: Filesystem                    # VolumeMode&#xA;  resources:&#xA;    requests:&#xA;      storage: 5Gi&#xA;  storageClassName: &#34;&#34;&#xA;  selector:&#xA;    matchLabels:                          &#xA;      app: mysql                            # chose/select &#34;mysql&#34; PV that is defined above.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154735404-80221355-1493-4043-ba7a-8c7a4ddc8df0.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-PersistantVolume.md&#34;&gt;LAB: K8s Persistant Volume and Persistant Volume Claim&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Storage Class &lt;a name=&#34;storageclass&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Creating volume with PV is manual way of creating volume. With storage classes, it can be automated.&lt;/li&gt; &#xA; &lt;li&gt;Cloud providers provide storage classes on their infrastructure.&lt;/li&gt; &#xA; &lt;li&gt;When pod/deployment is created, storage class is triggered to create PV automatically (Trigger order: Pod -&amp;gt; PVC -&amp;gt; Storage Class -&amp;gt; PV).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Storage Class Creation on Azure&#xA;apiVersion: storage.k8s.io/v1&#xA;kind: StorageClass&#xA;metadata:&#xA;  name: standarddisk&#xA;parameters:&#xA;  cachingmode: ReadOnly&#xA;  kind: Managed&#xA;  storageaccounttype: StandardSSD_LRS&#xA;provisioner: kubernetes.io/azure-disk&#xA;reclaimPolicy: Delete&#xA;volumeBindingMode: WaitForFirstConsumer    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;storageClassName&#34; is added into PVC file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: v1&#xA;kind: PersistentVolumeClaim&#xA;metadata:&#xA;  name: mysqlclaim&#xA;spec:&#xA;  accessModes:&#xA;    - ReadWriteOnce&#xA;  volumeMode: Filesystem&#xA;  resources:&#xA;    requests:&#xA;      storage: 5Gi&#xA;  storageClassName: &#34;standarddisk&#34;               # selects/binds to storage class (defined above)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When deployment/pod request PVC (claim), storage class provides volume on the infrastructure automatically.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Stateful Set &lt;a name=&#34;statefulset&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pods/Deployments are stateless objects. Stateful set provides to run stateful apps.&lt;/li&gt; &#xA; &lt;li&gt;Differences between Deployment and Statefulset: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Name of the pods in the statefulset are not assigned randomly. It gives name statefulsetName_0,1,2,3.&lt;/li&gt; &#xA;   &lt;li&gt;Pods in the statefulset are not created at the same time. Pods are created in order (new pod creation waits until previous pod&#39;s running status).&lt;/li&gt; &#xA;   &lt;li&gt;When scaling down of statefulset, pods are deleted in random. Pods are deleted in order.&lt;/li&gt; &#xA;   &lt;li&gt;If PVC is defined in the statefulset, each pod in the statefulset has own PV&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Statefulset.md&#34;&gt;LAB: K8s Stateful Sets - Nginx&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Job, CronJob &lt;a name=&#34;job&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Job Object&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate&#34;. If the container is not successfully completed, it will recreated again.&lt;/li&gt; &#xA; &lt;li&gt;&#34;When a specified number of successful completions is reached, the task (ie, Job) is complete.&#34;&lt;/li&gt; &#xA; &lt;li&gt;After finishing job, pods are not deleted. Logs in the pods can be viewed.&lt;/li&gt; &#xA; &lt;li&gt;Job is used for the task that runs once (e.g. maintanence scripts, scripts that are used for creating DB)&lt;/li&gt; &#xA; &lt;li&gt;Job is also used for processing tasks that are stored in queue or bucket.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;spec:&#xA;  parallelism: 2               # each step how many pods start in parallel at a time&#xA;  completions: 10              # number of pods that run and complete job at the end of the time&#xA;  backoffLimit: 5              # to tolerate fail number of job, after 5 times of failure, not try to continue job, fail the job&#xA;  activeDeadlineSeconds: 100   # if this job is not completed in 100 seconds, fail the job&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154946885-80e87f3c-5120-4c09-bde2-a35cd09a7383.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Job.md&#34;&gt;LAB: K8s Job&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Cron Job Object&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cron job is a scheduled job that can be started in scheduled time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ minute (0 - 59)&#xA;# ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ hour (0 - 23)&#xA;# ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the month (1 - 31)&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ month (1 - 12)&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ day of the week (0 - 6) (Sunday to Saturday;&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ                                   7 is also Sunday on some systems)&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ&#xA;# ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ&#xA;# * * * * *&#xA;#&#xA;# https://crontab.guru/ &#xA;# Examples: &#xA;#   5 * * * *   : (means) For every day start at minute 5: 00:05 - Second day 00:05 ....&#xA;#   */5 * * * * : (means) At every 5th minute: 00:05 - 00:10 - 00:15 ... &#xA;#   0 */2 * * * : (means) At minute 0 pass every 2d hour: 00:00 - 02:00 - 04:00 ... &#xA;#  &#34;*&#34; means &#34;every&#34;&#xA;#  &#34;/&#34; means &#34;repetitive&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;spec:&#xA;  schedule: &#34;*/1 * * * *&#34;                        # At every 1st minute: 00:01 - 00:02 ...&#xA;  jobTemplate:&#xA;    spec:&#xA;      template:&#xA;        spec:&#xA;          containers:&#xA;          - name: hello&#xA;            image: busybox&#xA;            imagePullPolicy: IfNotPresent&#xA;            command:                             # start shell and echo  &#xA;            - /bin/sh&#xA;            - -c&#xA;            - date; echo Hello from the Kubernetes cluster &#xA;          restartPolicy: OnFailure&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154948618-8b71bf38-62a7-44de-bdd2-ac40a1709eb4.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-CronJob.md&#34;&gt;LAB: K8s Cron Job&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Authentication, Role Based Access Control, Service Account &lt;a name=&#34;authentication&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h4&gt;Authentication&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is related to authenticate user to use specific cluster.&lt;/li&gt; &#xA; &lt;li&gt;Theory of the creating authentication is explained in short: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;user creates .key (key file) and .csr (certificate signing request file includes username and roles) with openssl application&lt;/li&gt; &#xA;   &lt;li&gt;user sends .csr file to the K8s admin&lt;/li&gt; &#xA;   &lt;li&gt;K8s admin creates a K8s object with this .csr file and creates .crt file (certification file) to give user&lt;/li&gt; &#xA;   &lt;li&gt;user gets this .crt file (certification file) and creates credential (set-credentials) in user&#39;s pc with certification.&lt;/li&gt; &#xA;   &lt;li&gt;user creates context (set-context) with cluster and credential, and uses this context.&lt;/li&gt; &#xA;   &lt;li&gt;now it requires to get/create authorization for the user.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Role Based Access Control (RBAC, Authorization)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It provides to give authorization (role) to the specific user.&lt;/li&gt; &#xA; &lt;li&gt;&#34;Role&#34;, &#34;RoleBinding&#34; K8s objects are used to bind users for specific &#34;namespace&#34;.&lt;/li&gt; &#xA; &lt;li&gt;&#34;ClusterRole&#34;, &#34;ClusterRoleBinding&#34; K8s objects are used to bind users for specific &#34;namespace&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: Role&#xA;metadata:&#xA;  namespace: default&#xA;  name: pod-reader&#xA;rules:&#xA;- apiGroups: [&#34;&#34;]                            # &#34;&#34; indicates the core API group&#xA;  resources: [&#34;pods&#34;]                        # &#34;services&#34;, &#34;endpoints&#34;, &#34;pods&#34;, &#34;pods/log&#34; etc.&#xA;  verbs: [&#34;get&#34;, &#34;watch&#34;, &#34;list&#34;]            # &#34;get&#34;, &#34;list&#34;, &#34;watch&#34;, &#34;create&#34;, &#34;update&#34;, &#34;patch&#34;, &#34;delete&#34;  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154953311-84f616cf-3a25-486f-beb9-e2d6a3a2e01a.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: RoleBinding&#xA;metadata:&#xA;  name: read-pods&#xA;  namespace: default&#xA;subjects:&#xA;- kind: User&#xA;  name: username@hostname.net                 # &#34;name&#34; is case sensitive, this name was defined while creating .csr file&#xA;  apiGroup: rbac.authorization.k8s.io&#xA;roleRef:&#xA;  kind: Role #this must be Role or ClusterRole&#xA;  name: pod-reader                            # this must match the name of the Role or ClusterRole you wish to bind to&#xA;  apiGroup: rbac.authorization.k8s.io    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154953439-1dd52309-611b-48bf-8f7b-51433b678f8c.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRole&#xA;metadata:&#xA;  name: secret-reader&#xA;rules:&#xA;- apiGroups: [&#34;&#34;]&#xA;  resources: [&#34;secrets&#34;]&#xA;  verbs: [&#34;get&#34;, &#34;watch&#34;, &#34;list&#34;]    &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154953542-3723d691-632e-41d6-908f-5b15080ffa7b.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;apiVersion: rbac.authorization.k8s.io/v1&#xA;kind: ClusterRoleBinding&#xA;metadata:&#xA;  name: read-secrets-global&#xA;subjects:&#xA;- kind: Group&#xA;  name: DevTeam                              # Name is case sensitive&#xA;  apiGroup: rbac.authorization.k8s.io&#xA;roleRef:&#xA;  kind: ClusterRole&#xA;  name: secret-reader&#xA;  apiGroup: rbac.authorization.k8s.io &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/154953630-dcd71073-6de6-4194-955e-9b50a0f9c978.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Service Account&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;RBACs are used for real people.&lt;/li&gt; &#xA; &lt;li&gt;Service accounts are used for pods/apps that can connect K8s API to create K8s objects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Ingress &lt;a name=&#34;ingress&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&#34;An API object that manages external access to the services in a cluster, typically HTTP.&#34; (ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;&#34;Ingress may provide load balancing, SSL termination and name-based virtual hosting&#34; (ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;Ingress is not a Service type, but it acts as the entry point for your cluster.&lt;/li&gt; &#xA; &lt;li&gt;Ingress resource only supports rules for directing HTTP(S) (L7) traffic.&lt;/li&gt; &#xA; &lt;li&gt;&#34;Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.&#34; (ref: Kubernetes.io)&lt;/li&gt; &#xA; &lt;li&gt;Ingress controller is a L7 Application Loadbalancer that works in K8s according to K8s specification. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Ingress Controllers: Nginx, HAproxy, Traefik&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/152972977-5cfb148f-4ac7-4fb6-b68b-9a576e199e68.png&#34; alt=&#34;image&#34;&gt; (ref: Kubernetes.io)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Simple Ingress Object Definition    &#xA;apiVersion: networking.k8s.io/v1&#xA;kind: Ingress&#xA;metadata:&#xA;  name: minimal-ingress&#xA;  annotations:&#xA;    nginx.ingress.kubernetes.io/rewrite-target: /&#xA;spec:&#xA;  ingressClassName: nginx-example&#xA;  rules:&#xA;  - http:&#xA;      paths:&#xA;      - path: /testpath&#xA;        pathType: Prefix&#xA;        backend:&#xA;          service:&#xA;            name: test&#xA;            port:&#xA;              number: 80&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Ingress.md&#34;&gt;LAB: K8s Ingress&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Dashboard &lt;a name=&#34;dashboard&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can view followings using default K8s dashboard: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;All Workloads on Cluster: Memory and CPU usages, update time, image name, node name, status&lt;/li&gt; &#xA;   &lt;li&gt;Cron Jobs and Jobs&lt;/li&gt; &#xA;   &lt;li&gt;Daeamon Sets&lt;/li&gt; &#xA;   &lt;li&gt;Deployments, Replicasets&lt;/li&gt; &#xA;   &lt;li&gt;Pods, Stateful Sets&lt;/li&gt; &#xA;   &lt;li&gt;Services, Endpoints, IPs, Ports,&lt;/li&gt; &#xA;   &lt;li&gt;Persistent Volume Claims, Persisten Volumes&lt;/li&gt; &#xA;   &lt;li&gt;Config Maps,&lt;/li&gt; &#xA;   &lt;li&gt;Secrets, Storage Classes&lt;/li&gt; &#xA;   &lt;li&gt;Cluster Roles and Role Binding&lt;/li&gt; &#xA;   &lt;li&gt;Namespaces&lt;/li&gt; &#xA;   &lt;li&gt;Network Policies&lt;/li&gt; &#xA;   &lt;li&gt;Nodes&lt;/li&gt; &#xA;   &lt;li&gt;Roles and Role Bindings&lt;/li&gt; &#xA;   &lt;li&gt;Service Accounts&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# if working on minikube&#xA;minikube addons enable dashboard&#xA;minikube addons enable metrics-server&#xA;minikube dashboard&#xA;# if running on WSL/WSL2 to open browser&#xA;sensible-browser http://127.0.0.1:45771/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;to see better resolution, click on it&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/152148024-6ec65b33-9fd0-42eb-89c3-927e453553a2.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/10358317/152147845-017c6c10-a687-4ee3-b868-a08d96f6d884.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Enable-Dashboard-On-Cluster.md&#34;&gt;LAB: Enable Dashboard on Real Cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Play With Kubernetes &lt;a name=&#34;playWithKubernetes&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://labs.play-with-k8s.com/&#34;&gt;https://labs.play-with-k8s.com/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Helm &lt;a name=&#34;helm&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Helm is the package manager of K8s (&lt;a href=&#34;https://helm.sh/&#34;&gt;https://helm.sh/&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&#34;Helm installs charts into Kubernetes, creating a new release for each installation. And to find new charts, you can search Helm chart repositories.&#34; (Ref: Helm.sh)&lt;/li&gt; &#xA; &lt;li&gt;With Helm, it is easy to install best-practice K8s designs and products. Search K8s packages =&amp;gt; &lt;a href=&#34;https://artifacthub.io/&#34;&gt;https://artifacthub.io/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Detailed Tutorial =&amp;gt; &lt;a href=&#34;https://helm.sh/docs/intro/quickstart/&#34;&gt;https://helm.sh/docs/intro/quickstart/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Important Terms:&lt;/strong&gt; (Ref: Helm.sh) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Chart:&lt;/strong&gt; &#34;A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an Apt dpkg, or a Yum RPM file.&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &#34;A Repository is the place where charts can be collected and shared&#34;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Release:&lt;/strong&gt; &#34;A Release is an instance of a chart running in a Kubernetes cluster. One chart can often be installed many times into the same cluster. And each time it is installed, a new release is created. Consider a MySQL chart. If you want two databases running in your cluster, you can install that chart twice. Each one will have its own release, which will in turn have its own release name.&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/Helm.md&#34;&gt;LAB: HELM Install &amp;amp; Usage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto the scenario:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Helm-Jenkins.md&#34;&gt;LAB: Helm-Jenkins on running K8s Cluster (2 Node Multipass VM)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/HelmCheatsheet.md&#34;&gt;Helm Commands Cheatsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Kubernetes Commands Cheatsheet &lt;a name=&#34;cheatsheet&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/KubernetesCommandCheatSheet.md&#34;&gt;Kubernetes Commands Cheatsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Helm Commands Cheatsheet &lt;a name=&#34;helm_cheatsheet&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/HelmCheatsheet.md&#34;&gt;Helm Commands Cheatsheet&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Kubernetes Cluster Setup: Kubeadm, Containerd, Multipass &lt;a name=&#34;cluster_setup&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Kubeadm-Cluster-Setup.md&#34;&gt;LAB: K8s Kubeadm Cluster Setup&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Monitoring Kubernetes Cluster with SSH, Prometheus and Grafana &lt;a name=&#34;prometheus_grafana&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Monitoring-Prometheus-Grafana.md&#34;&gt;LAB: K8s Monitoring - Prometheus and Grafana&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goto:&lt;/strong&gt; &lt;a href=&#34;https://github.com/omerbsezer/Fast-Kubernetes/raw/main/K8s-Enable-Dashboard-On-Cluster.md&#34;&gt;LAB: Enable Dashboard on Real K8s Cluster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Other Useful Resources Related Docker &lt;a name=&#34;resource&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34;&gt;KubernetesTutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Docker and Kubernetes Tutorial - Youtube: &lt;a href=&#34;https://www.youtube.com/watch?v=bhBSlnQcq2k&amp;amp;t=3088s&#34;&gt;https://www.youtube.com/watch?v=bhBSlnQcq2k&amp;amp;t=3088s&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References &lt;a name=&#34;references&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/overview/&#34;&gt;Kubernetes.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tutorials/&#34;&gt;KubernetesTutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.udemy.com/course/kubernetes-temelleri/&#34;&gt;udemy-course:Kubernetes-Temelleri&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://helm.sh/&#34;&gt;Helm.sh&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>