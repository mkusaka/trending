<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-06T01:32:37Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>shuding/react-wrap-balancer</title>
    <updated>2023-01-06T01:32:37Z</updated>
    <id>tag:github.com,2023-01-06:/shuding/react-wrap-balancer</id>
    <link href="https://github.com/shuding/react-wrap-balancer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple React Component That Makes Titles More Readable&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://react-wrap-balancer.vercel.app&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shuding/react-wrap-balancer/main/.github/card.png&#34; alt=&#34;React Wrap Balancer - Simple React Component That Makes Titles More Readable&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://react-wrap-balancer.vercel.app&#34;&gt;&lt;strong&gt;React Wrap Balancer&lt;/strong&gt;&lt;/a&gt; is a simple React Component that makes your titles more readable in different viewport sizes. It improves the wrapping to avoid situations like single word in the last line, makes the content more “balanced”:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shuding/react-wrap-balancer/main/.github/demo.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To start using the library, install it to your project:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm i react-wrap-balancer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And wrap text content with it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsx&#34;&gt;import Balancer from &#39;react-wrap-balancer&#39;&#xA;&#xA;// ...&#xA;&#xA;function Title() {&#xA;  return (&#xA;    &amp;lt;h1&amp;gt;&#xA;      &amp;lt;Balancer&amp;gt;My Awesome Title&amp;lt;/Balancer&amp;gt;&#xA;    &amp;lt;/h1&amp;gt;&#xA;  )&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have multiple &lt;code&gt;&amp;lt;Balancer&amp;gt;&lt;/code&gt; components used, it’s recommended (but optional) to also use &lt;code&gt;&amp;lt;Provider&amp;gt;&lt;/code&gt; to wrap the entire app. This will make them share the re-balance logic and reduce the HTML size:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-jsx&#34;&gt;import { Provider } from &#39;react-wrap-balancer&#39;&#xA;&#xA;// ...&#xA;&#xA;function App() {&#xA;  return (&#xA;    &amp;lt;Provider&amp;gt;&#xA;      &amp;lt;MyApp/&amp;gt;&#xA;    &amp;lt;/Provider&amp;gt;&#xA;  )&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For full documentation and use cases, please visit &lt;a href=&#34;https://react-wrap-balancer.vercel.app&#34;&gt;&lt;strong&gt;react-wrap-balancer.vercel.app&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;This project was inspired by Adobe’s &lt;a href=&#34;https://github.com/adobe/balance-text&#34;&gt;balance-text&lt;/a&gt; project, NYT’s &lt;a href=&#34;https://github.com/nytimes/text-balancer&#34;&gt;text-balancer&lt;/a&gt; project, and Daniel Aleksandersen’s &lt;a href=&#34;https://www.ctrl.blog/entry/text-wrap-balance.html&#34;&gt;Improving the New York Times’ line wrap balancer&lt;/a&gt;. If you want to learn more, you can also take a look at the &lt;a href=&#34;https://drafts.csswg.org/css-text-4/#text-wrap&#34;&gt;text-wrap: balance&lt;/a&gt; proposal.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://twitter.com/emilkowalski_&#34;&gt;Emil Kowalski&lt;/a&gt; for testing and feedback.&lt;/p&gt; &#xA;&lt;p&gt;Created by &lt;a href=&#34;https://twitter.com/shuding_&#34;&gt;Shu Ding&lt;/a&gt; in 2022, released under the MIT license.&lt;/p&gt; &#xA;&lt;a aria-label=&#34;Vercel logo&#34; href=&#34;https://vercel.com&#34;&gt; &lt;img src=&#34;https://badgen.net/badge/icon/Made%20by%20Vercel?icon=zeit&amp;amp;label&amp;amp;color=black&amp;amp;labelColor=black&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>vlievin/medical-reasoning</title>
    <updated>2023-01-06T01:32:37Z</updated>
    <id>tag:github.com,2023-01-06:/vlievin/medical-reasoning</id>
    <link href="https://github.com/vlievin/medical-reasoning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Medical reasoning using large language models&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; Work in progress&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Medical Reasoning using GPT-3.5&lt;/h1&gt; &#xA;&lt;p&gt;Official repository for the paper &lt;a href=&#34;https://arxiv.org/abs/2207.08143&#34;&gt;Can large language models reason about medical questions?&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Although large language models (LLMs) often produce impressive outputs, it remains unclear how they perform in real-world scenarios requiring strong reasoning skills and expert domain knowledge. We set out to investigate whether GPT-3.5 (Codex and InstructGPT) can be applied to answer and reason about difficult real-world-based questions. We utilize two multiple-choice medical exam questions (USMLE and MedMCQA) and a medical reading comprehension dataset (PubMedQA). We investigate multiple prompting scenarios: Chain-of-Thought (CoT, think step-by-step), zero- and few-shot (prepending the question with question-answer exemplars) and retrieval augmentation (injecting Wikipedia passages into the prompt). For a subset of the USMLE questions, a medical expert reviewed and annotated the model&#39;s CoT. We found that InstructGPT can often read, reason and recall expert knowledge. Failure are primarily due to lack of knowledge and reasoning errors and trivial guessing heuristics are observed, e.g.\ too often predicting labels A and D on USMLE. Sampling and combining many completions overcome some of these limitations. Using 100 samples, Codex 5-shot CoT not only gives close to well-calibrated predictive probability but also achieves human-level performances on the three datasets. USMLE: 60.2%, MedMCQA: 57.5% and PubMedQA: 78.2%.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;CoT Samples&lt;/h2&gt; &#xA;&lt;p&gt;Samples of generated CoTs for the USMLE, MedMCQA and PubMedQA datasets can be accessed &lt;a href=&#34;https://vlievin.github.io/medical-reasoning&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;More samples will be made available through &lt;a href=&#34;https://github.com/OpenBioLink/ThoughtSource&#34;&gt;ThoughtSource ⚡&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Install poetry&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Install dependencies&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Setup Elasticsearch&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.14.1-linux-x86_64.tar.gz&#xA;tar -xzf elasticsearch-7.14.1-linux-x86_64.tar.gz&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To run ElasticSearch navigate to the &lt;code&gt;elasticsearch-7.14.1&lt;/code&gt; folder in the terminal and run &lt;code&gt;./bin/elasticsearch&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Running one experiment&lt;/h2&gt; &#xA;&lt;p&gt;Use &lt;code&gt;poetry run&lt;/code&gt; to load and run using the &lt;code&gt;poetry&lt;/code&gt; environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;poetry run experiment &amp;lt;args&amp;gt;&#xA;# Example&#xA;poetry run experiment engine=code dataset.name=medqa_us dataset.subset=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running a group of experiments&lt;/h2&gt; &#xA;&lt;p&gt;Groups of experiments are defined in &lt;code&gt;pyproject.toml&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;poetry run poe medqa_test&#xA;poetry run poe medmcqa_valid&#xA;poetry run poe pubmedqa_test&#xA;poetry run poe mmlu_test_code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{https://doi.org/10.48550/arxiv.2207.08143,&#xA;  doi = {10.48550/ARXIV.2207.08143},&#xA;  url = {https://arxiv.org/abs/2207.08143},&#xA;  author = {Liévin, Valentin and Hother, Christoffer Egeberg and Winther, Ole},&#xA;  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.1; I.2.7},&#xA;  title = {Can large language models reason about medical questions?},&#xA;  publisher = {arXiv},&#xA;  year = {2022},&#xA;  copyright = {arXiv.org perpetual, non-exclusive license}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Tencent/tdesign-miniprogram</title>
    <updated>2023-01-06T01:32:37Z</updated>
    <id>tag:github.com,2023-01-06:/Tencent/tdesign-miniprogram</id>
    <link href="https://github.com/Tencent/tdesign-miniprogram" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Wechat MiniProgram UI components lib for TDesign.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://tdesign.tencent.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;TDesign Logo&#34; width=&#34;200&#34; src=&#34;https://tdesign.gtimg.com/site/TDesign.png&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Tencent/tdesign-miniprogram/raw/develop/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/l/tdesign-miniprogram.svg?sanitize=true&#34; alt=&#34;License&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/tdesign-miniprogram&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/v/tdesign-miniprogram.svg?sanitize=true&#34; alt=&#34;Version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/tdesign-miniprogram&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/dw/tdesign-miniprogram&#34; alt=&#34;Downloads&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/tdesign&#34;&gt;TDesign&lt;/a&gt; 适配微信小程序的组件库。&lt;/p&gt; &#xA;&lt;h2&gt;预览&lt;/h2&gt; &#xA;&lt;p&gt;小程序组件示例小程序，请使用微信扫码预览 ↓ &lt;br&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;260&#34; src=&#34;https://user-images.githubusercontent.com/7017290/146479952-b05298e8-f6ac-44a1-b73c-7abd8b9b3914.jpeg&#34;&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;h3&gt;使用 NPM&lt;/h3&gt; &#xA;&lt;p&gt;小程序已经支持使用 NPM 安装第三方包。&lt;/p&gt; &#xA;&lt;p&gt;具体使用方式，可以参考小程序官网文档： &lt;a href=&#34;https://developers.weixin.qq.com/miniprogram/dev/devtools/npm.html?search-key=npm&#34;&gt;《NPM 支持》&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm i tdesign-miniprogram -S --production&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;建议使用 NPM，不再推荐“源码拷贝的方式”&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;使用组件&lt;/h2&gt; &#xA;&lt;p&gt;以按钮组件为例，只需要在 &lt;code&gt;JSON&lt;/code&gt; 文件中引入按钮对应的自定义组件即可&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;usingComponents&#34;: {&#xA;    &#34;t-button&#34;: &#34;tdesign-miniprogram/button/button&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;在开发者工具中预览&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 安装项目依赖&#xA;npm install&#xA;&#xA;# 执行组件编译&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;打开&lt;a href=&#34;https://mp.weixin.qq.com/debug/wxadoc/dev/devtools/download.html&#34;&gt;微信开发者工具&lt;/a&gt;，把 &lt;code&gt;_example&lt;/code&gt; 目录添加进去就可以预览示例了。&lt;/p&gt; &#xA;&lt;h2&gt;基础库版本&lt;/h2&gt; &#xA;&lt;p&gt;最低基础库版本&lt;code&gt;^2.6.5&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;开源协议&lt;/h2&gt; &#xA;&lt;p&gt;TDesign 遵循 &lt;a href=&#34;https://github.com/Tencent/tdesign-miniprogram/LICENSE&#34;&gt;MIT 协议&lt;/a&gt;。&lt;/p&gt;</summary>
  </entry>
</feed>