<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-02T01:33:43Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ChartsCSS/charts.css</title>
    <updated>2022-11-02T01:33:43Z</updated>
    <id>tag:github.com,2022-11-02:/ChartsCSS/charts.css</id>
    <link href="https://github.com/ChartsCSS/charts.css" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open source CSS framework for data visualization.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Charts.css&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/ChartsCSS/charts.css?style=for-the-badge&#34; alt=&#34;GitHub Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/bundlephobia/min/charts.css?style=for-the-badge&#34; alt=&#34;Minified Size&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/ChartsCSS/charts.css?label=GitHub%20Stars&amp;amp;style=for-the-badge&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/ChartsCSS/charts.css?style=for-the-badge&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Charts.css is an open source CSS framework for data visualization.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Visualization help end-users understand data. &lt;strong&gt;Charts.css&lt;/strong&gt; help frontend developers turn data into beautiful charts and graphs using simple &lt;strong&gt;CSS classes&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;No dependencies. 72kb file size. Less than 6kb gzipped file size!&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Check the full documentation on &lt;a href=&#34;https://ChartsCSS.org/&#34;&gt;ChartsCSS.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chartscss.org/docs/&#34;&gt;Get Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chartscss.org/components/&#34;&gt;Components&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chartscss.org/charts/&#34;&gt;Charts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chartscss.org/customization/&#34;&gt;Customization&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chartscss.org/development/&#34;&gt;Development&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chartscss.org/examples/&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;CDN&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;a href=&#34;https://www.jsdelivr.com/package/npm/charts.css&#34;&gt;jsdelivr&lt;/a&gt; CDN:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;link rel=&#34;stylesheet&#34; href=&#34;https://cdn.jsdelivr.net/npm/charts.css/dist/charts.min.css&#34;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or &lt;a href=&#34;https://unpkg.com/browse/charts.css/&#34;&gt;unpkg&lt;/a&gt; CDN:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;link rel=&#34;stylesheet&#34; href=&#34;https://unpkg.com/charts.css/dist/charts.min.css&#34;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Package Manager&lt;/h3&gt; &#xA;&lt;p&gt;Install using &lt;a href=&#34;https://www.npmjs.com/package/charts.css&#34;&gt;npm&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install charts.css&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or using &lt;a href=&#34;https://classic.yarnpkg.com/en/package/charts.css&#34;&gt;yarn&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yarn add charts.css&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The data is structured using semantic HTML tags and styled using CSS classes which change the visual representation displayed to the end user.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;table class=&#34;charts-css [ column ] [ show-primary-axis show-4-secondary-axes ] [ data-spacing-4 reverse-data ]&#34;&amp;gt;&#xA;&#xA;  &amp;lt;caption&amp;gt; Front End Developer Salary &amp;lt;/caption&amp;gt;&#xA;&#xA;  &amp;lt;thead&amp;gt;&#xA;    &amp;lt;tr&amp;gt;&#xA;      &amp;lt;th scope=&#34;col&#34;&amp;gt; Year &amp;lt;/th&amp;gt;&#xA;      &amp;lt;th scope=&#34;col&#34;&amp;gt; Income &amp;lt;/th&amp;gt;&#xA;    &amp;lt;/tr&amp;gt;&#xA;  &amp;lt;/thead&amp;gt;&#xA;&#xA;  &amp;lt;tbody&amp;gt;&#xA;    &amp;lt;tr&amp;gt;&#xA;      &amp;lt;th scope=&#34;row&#34;&amp;gt; 2016 &amp;lt;/th&amp;gt;&#xA;      &amp;lt;td style=&#34;--size: calc( 40 / 100 );&#34;&amp;gt; $ 40K &amp;lt;/td&amp;gt;&#xA;    &amp;lt;/tr&amp;gt;&#xA;    &amp;lt;tr&amp;gt;&#xA;      &amp;lt;th scope=&#34;row&#34;&amp;gt; 2017 &amp;lt;/th&amp;gt;&#xA;      &amp;lt;td style=&#34;--size: calc( 60 / 100 );&#34;&amp;gt; $ 60K &amp;lt;/td&amp;gt;&#xA;    &amp;lt;/tr&amp;gt;&#xA;    &amp;lt;tr&amp;gt;&#xA;      &amp;lt;th scope=&#34;row&#34;&amp;gt; 2018 &amp;lt;/th&amp;gt;&#xA;      &amp;lt;td style=&#34;--size: calc( 75 / 100 );&#34;&amp;gt; $ 75K &amp;lt;/td&amp;gt;&#xA;    &amp;lt;/tr&amp;gt;&#xA;    &amp;lt;tr&amp;gt;&#xA;      &amp;lt;th scope=&#34;row&#34;&amp;gt; 2019 &amp;lt;/th&amp;gt;&#xA;      &amp;lt;td style=&#34;--size: calc( 90 / 100 );&#34;&amp;gt; $ 90K &amp;lt;/td&amp;gt;&#xA;    &amp;lt;/tr&amp;gt;&#xA;    &amp;lt;tr&amp;gt;&#xA;      &amp;lt;th scope=&#34;row&#34;&amp;gt; 2020 &amp;lt;/th&amp;gt;&#xA;      &amp;lt;td style=&#34;--size: calc( 100 / 100 );&#34;&amp;gt; $ 100K &amp;lt;br&amp;gt; ðŸ‘‘ &amp;lt;/td&amp;gt;&#xA;    &amp;lt;/tr&amp;gt;&#xA;  &amp;lt;/tbody&amp;gt;&#xA;&#xA;&amp;lt;/table&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The framework offers developers flexibility. You choose what components to display and how to style them. Each component offers several CSS classes and CSS variables to customizes your style.&lt;/p&gt; &#xA;&lt;p&gt;The key feature is the ability to customize everything using basic CSS. Frontend developers can target any HTML element and customize it. This philosophical guideline is what makes the framework so flexible, easy and fun to use.&lt;/p&gt; &#xA;&lt;h2&gt;Questions&lt;/h2&gt; &#xA;&lt;p&gt;For questions and support please use the &lt;a href=&#34;https://github.com/ChartsCSS/charts.css/discussions&#34;&gt;official forum&lt;/a&gt; on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Liked Charts.css?&lt;/h2&gt; &#xA;&lt;p&gt;If you like the project, &lt;strong&gt;please consider to star the repo on GitHub&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Charts.css is licensed under the &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>orpatashnik/StyleCLIP</title>
    <updated>2022-11-02T01:33:43Z</updated>
    <id>tag:github.com,2022-11-02:/orpatashnik/StyleCLIP</id>
    <link href="https://github.com/orpatashnik/StyleCLIP" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Implementation for &#34;StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery&#34; (ICCV 2021 Oral)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery (ICCV 2021 Oral)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://replicate.ai/orpatashnik/styleclip&#34;&gt;Run this model on Replicate&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Optimization: &lt;a href=&#34;http://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/optimization_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Mapper: &lt;a href=&#34;https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/mapper_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Global directions Torch: &lt;a href=&#34;https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global_torch.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; Global directions TF1: &lt;a href=&#34;https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=5icI0NgALnQ&#34;&gt;&lt;img src=&#34;https://github.com/orpatashnik/StyleCLIP/raw/main/img/StyleCLIP_gif.gif&#34; width=&#34;600&#34;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt;Full Demo Video: &lt;a href=&#34;https://www.youtube.com/watch?v=5icI0NgALnQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-YouTube-red?&amp;amp;style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white&#34; height=&#34;20&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp; ICCV Video &lt;a href=&#34;https://www.youtube.com/watch?v=PhR1gpXDu0w&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-YouTube-red?&amp;amp;style=for-the-badge&amp;amp;logo=youtube&amp;amp;logoColor=white&#34; height=&#34;20&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/teaser.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery&lt;/strong&gt;&lt;br&gt; Or Patashnik*, Zongze Wu*, Eli Shechtman, Daniel Cohen-Or, Dani Lischinski &lt;br&gt; *Equal contribution, ordered alphabetically &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2103.17249&#34;&gt;https://arxiv.org/abs/2103.17249&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Inspired by the ability of StyleGAN to generate highly realistic images in a variety of domains, much recent work has focused on understanding how to use the latent spaces of StyleGAN to manipulate generated and real images. However, discovering semantically meaningful latent manipulations typically involves painstaking human examination of the many degrees of freedom, or an annotated collection of images for each desired manipulation. In this work, we explore leveraging the power of recently introduced Contrastive Language-Image Pre-training (CLIP) models in order to develop a text-based interface for StyleGAN image manipulation that does not require such manual effort. We first introduce an optimization scheme that utilizes a CLIP-based loss to modify an input latent vector in response to a user-provided text prompt. Next, we describe a latent mapper that infers a text-guided latent manipulation step for a given input image, allowing faster and more stable textbased manipulation. Finally, we present a method for mapping a text prompts to input-agnostic directions in StyleGANâ€™s style space, enabling interactive text-driven image manipulation. Extensive results and comparisons demonstrate the effectiveness of our approaches.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;Official Implementation of StyleCLIP, a method to manipulate images using a driving text. Our method uses the generative power of a pretrained StyleGAN generator, and the visual-language power of CLIP. In the paper we present three methods:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Latent vector optimization.&lt;/li&gt; &#xA; &lt;li&gt;Latent mapper, trained to manipulate latent vectors according to a specific text description.&lt;/li&gt; &#xA; &lt;li&gt;Global directions in the StyleSpace.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;31/10/2022&lt;/strong&gt; Add support for global direction with torch implementation&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;15/8/2021&lt;/strong&gt; Add support for StyleSpace in optimization and latent mapper methods&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;6/4/2021&lt;/strong&gt; Add mapper training and inference (including a jupyter notebook) code&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;6/4/2021&lt;/strong&gt; Add support for custom StyleGAN2 and StyleGAN2-ada models, and also custom images&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2/4/2021&lt;/strong&gt; Add the global directions code (a local GUI and a colab notebook)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;31/3/2021&lt;/strong&gt; Upload paper to arxiv, and video to YouTube&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;14/2/2021&lt;/strong&gt; Initial version&lt;/p&gt; &#xA;&lt;h2&gt;Setup (for all three methods)&lt;/h2&gt; &#xA;&lt;p&gt;For all the methods described in the paper, is it required to have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anaconda&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;CLIP&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Specific requirements for each method are described in its section. To install CLIP please run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=&amp;lt;CUDA_VERSION&amp;gt;&#xA;pip install ftfy regex tqdm gdown&#xA;pip install git+https://github.com/openai/CLIP.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Editing via Latent Vector Optimization&lt;/h2&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;Here, the code relies on the &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch/&#34;&gt;Rosinality&lt;/a&gt; pytorch implementation of StyleGAN2. Some parts of the StyleGAN implementation were modified, so that the whole implementation is native pytorch.&lt;/p&gt; &#xA;&lt;p&gt;In addition to the requirements mentioned before, a pretrained StyleGAN2 generator will attempt to be downloaded, (or manually download from &lt;a href=&#34;https://drive.google.com/file/d/1EM87UquaoQmk17Q8d5kYIAHqu0dkYqdT/view?usp=sharing&#34;&gt;here&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;Given a textual description, one can both edit a given image, or generate a random image that best fits to the description. Both operations can be done through the &lt;code&gt;main.py&lt;/code&gt; script, or the &lt;code&gt;optimization_playground.ipynb&lt;/code&gt; notebook (&lt;a href=&#34;http://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/optimization_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h4&gt;Editing&lt;/h4&gt; &#xA;&lt;p&gt;To edit an image set &lt;code&gt;--mode=edit&lt;/code&gt;. Editing can be done on both provided latent vector, and on a random latent vector from StyleGAN&#39;s latent space. It is recommended to adjust the &lt;code&gt;--l2_lambda&lt;/code&gt; according to the desired edit.&lt;/p&gt; &#xA;&lt;h4&gt;Generating Free-style Images&lt;/h4&gt; &#xA;&lt;p&gt;To generate a free-style image set &lt;code&gt;--mode=free_generation&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Editing via Latent Mapper&lt;/h2&gt; &#xA;&lt;p&gt;Here, we provide the code for the latent mapper. The mapper is trained to learn &lt;em&gt;residuals&lt;/em&gt; from a given latent vector, according to the driving text. The code for the mapper is in &lt;code&gt;mapper/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;As in the optimization, the code relies on &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch/&#34;&gt;Rosinality&lt;/a&gt; pytorch implementation of StyleGAN2. In addition the the StyleGAN weights, it is neccessary to have weights for the facial recognition network used in the ID loss. The weights can be downloaded from &lt;a href=&#34;https://drive.google.com/file/d/1KW7bjndL3QG3sxBbZxreGHigcCCpsDgn/view?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The mapper is trained on latent vectors. It is recommended to train on &lt;em&gt;inverted real images&lt;/em&gt;. To this end, we provide the CelebA-HQ that was inverted by e4e: &lt;a href=&#34;https://drive.google.com/file/d/1gof8kYc_gDLUT4wQlmUdAtPnQIlCO26q/view?usp=sharing&#34;&gt;train set&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1j7RIfmrCoisxx3t-r-KC02Qc8barBecr/view?usp=sharing&#34;&gt;test set&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The main training script is placed in &lt;code&gt;mapper/scripts/train.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Training arguments can be found at &lt;code&gt;mapper/options/train_options.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Intermediate training results are saved to opts.exp_dir. This includes checkpoints, train outputs, and test outputs. Additionally, if you have tensorboard installed, you can visualize tensorboard logs in opts.exp_dir/logs. Note that&lt;/li&gt; &#xA; &lt;li&gt;To resume a training, please provide &lt;code&gt;--checkpoint_path&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--description&lt;/code&gt; is where you provide the driving text.&lt;/li&gt; &#xA; &lt;li&gt;If you perform an edit that is not supposed to change &#34;colors&#34; in the image, it is recommended to use the flag &lt;code&gt;--no_fine_mapper&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example for training a mapper for the moahwk hairstyle:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd mapper&#xA;python train.py --exp_dir ../results/mohawk_hairstyle --no_fine_mapper --description &#34;mohawk hairstyle&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All configurations for the examples shown in the paper are provided there.&lt;/p&gt; &#xA;&lt;h4&gt;Inference&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The main inferece script is placed in &lt;code&gt;mapper/scripts/inference.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Inference arguments can be found at &lt;code&gt;mapper/options/test_options.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Adding the flag &lt;code&gt;--couple_outputs&lt;/code&gt; will save image containing the input and output images side-by-side.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Pretrained models for variuos edits are provided. Please refer to &lt;code&gt;utils.py&lt;/code&gt; for the complete links list.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a notebook for performing inference with the mapper Mapper notebook: &lt;a href=&#34;https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/mapper_playground.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Editing via Global Direction&lt;/h2&gt; &#xA;&lt;p&gt;Here we provide GUI for editing images with the global directions. We provide both a jupyter notebook &lt;a href=&#34;https://colab.research.google.com/github/orpatashnik/StyleCLIP/blob/main/notebooks/StyleCLIP_global.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;, and the GUI used in the &lt;a href=&#34;https://www.youtube.com/watch?v=5icI0NgALnQ&#34;&gt;video&lt;/a&gt;. For both, the linear direction are computed in &lt;strong&gt;real time&lt;/strong&gt;. The code is located at &lt;code&gt;global_directions/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;Here, we rely on the &lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;official&lt;/a&gt; TensorFlow implementation of StyleGAN2.&lt;/p&gt; &#xA;&lt;p&gt;It is required to have TensorFlow, version 1.14 or 1.15 (&lt;code&gt;conda install -c anaconda tensorflow-gpu==1.14&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;h4&gt;Local GUI&lt;/h4&gt; &#xA;&lt;p&gt;To start the local GUI please run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd global_directions&#xA;&#xA;# input dataset name &#xA;dataset_name=&#39;ffhq&#39; &#xA;&#xA;# pretrained StyleGAN2 model from standard [NVlabs implementation](https://github.com/NVlabs/stylegan2) will be download automatically.&#xA;# pretrained StyleGAN2-ada model could be download from https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ .&#xA;# for custom StyleGAN2 or StyleGAN2-ada model, please place the model under ./StyleCLIP/global_directions/model/ folder.&#xA;&#xA;&#xA;# input prepare data &#xA;python GetCode.py --dataset_name $dataset_name --code_type &#39;w&#39;&#xA;python GetCode.py --dataset_name $dataset_name --code_type &#39;s&#39;&#xA;python GetCode.py --dataset_name $dataset_name --code_type &#39;s_mean_std&#39;&#xA;&#xA;# preprocess (this may take a few hours). &#xA;# we precompute the results for StyleGAN2 on ffhq, StyleGAN2-ada on afhqdog, afhqcat. For these model, we can skip the preprocess step.&#xA;python SingleChannel.py --dataset_name $dataset_name&#xA;&#xA;# generated image to be manipulated &#xA;# this operation will generate and replace the w_plu.npy and .jpg images in &#39;./data/dataset_name/&#39; folder. &#xA;# if you you want to keep the original data, please rename the original folder.&#xA;# to use custom images, please use e4e encoder to generate latents.pt, and place it in &#39;./data/dataset_name/&#39; folder, and add --real flag while running this function.&#xA;# you may skip this step if you want to manipulate the real human faces we prepare in ./data/ffhq/ folder.   &#xA;python GetGUIData.py --dataset_name $dataset_name&#xA;&#xA;# interactively manipulation &#xA;python PlayInteractively.py --dataset_name $dataset_name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As shown in the video, to edit an image it is requires to write a &lt;em&gt;neutral text&lt;/em&gt; and a &lt;em&gt;target text&lt;/em&gt;. To operate the GUI, please do the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Maximize the window size&lt;/li&gt; &#xA; &lt;li&gt;Double click on the left square to choose an image. The images are taken from &lt;code&gt;global_directions/data/ffhq&lt;/code&gt;, and the corresponding latent vectors are in &lt;code&gt;global_directions/data/ffhq/w_plus.npy&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Type a neutral text, then press enter&lt;/li&gt; &#xA; &lt;li&gt;Modify the target text so that it will contain the target edit, then press enter.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can now play with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Manipulation strength&lt;/em&gt; - positive values correspond to moving along the target direction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;Disentanglement threshold&lt;/em&gt; - large value means more disentangled edit, just a few channels will be manipulated so only the target attribute will change (for example, grey hair). Small value means less disentangled edit, a large number of channels will be manipulated, related attributes will also change (such as wrinkle, skin color, glasses).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Examples:&lt;/h5&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Edit&lt;/th&gt; &#xA;   &lt;th&gt;Neutral Text&lt;/th&gt; &#xA;   &lt;th&gt;Target Text&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Smile&lt;/td&gt; &#xA;   &lt;td&gt;face&lt;/td&gt; &#xA;   &lt;td&gt;smiling face&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gender&lt;/td&gt; &#xA;   &lt;td&gt;female face&lt;/td&gt; &#xA;   &lt;td&gt;male face&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Blonde hair&lt;/td&gt; &#xA;   &lt;td&gt;face with hair&lt;/td&gt; &#xA;   &lt;td&gt;face with blonde hair&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hi-top fade&lt;/td&gt; &#xA;   &lt;td&gt;face with hair&lt;/td&gt; &#xA;   &lt;td&gt;face with Hi-top fade hair&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Blue eyes&lt;/td&gt; &#xA;   &lt;td&gt;face with eyes&lt;/td&gt; &#xA;   &lt;td&gt;face with blue eyes&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;More examples could be found in the &lt;a href=&#34;https://www.youtube.com/watch?v=5icI0NgALnQ&#34;&gt;video&lt;/a&gt; and in the paper.&lt;/p&gt; &#xA;&lt;h5&gt;Pratice Tips:&lt;/h5&gt; &#xA;&lt;p&gt;In the terminal, for every manipulation, the number of channels being manipulated is printed (the number is controlled by the attribute (neutral, target) and the disentanglement threshold).&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For color transformation, usually 10-20 channels is enough. For large structure change (for example, Hi-top fade), usually 100-200 channels are required.&lt;/li&gt; &#xA; &lt;li&gt;For an attribute (neutral, target), if you give a low disentanglement threshold, there are just few channels (&amp;lt;20) being manipulated, and usually it is not enough for performing the desired edit.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Notebook&lt;/h4&gt; &#xA;&lt;p&gt;Open the notebook in colab and run all the cells. In the last cell you can play with the image.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;beta&lt;/code&gt; corresponds to the &lt;em&gt;disentanglement threshold&lt;/em&gt;, and &lt;code&gt;alpha&lt;/code&gt; to the &lt;em&gt;manipulation strength&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After you set the desired set of parameters, please run again the last cell to generate the image.&lt;/p&gt; &#xA;&lt;h2&gt;Editing Examples&lt;/h2&gt; &#xA;&lt;p&gt;In the following, we show some results obtained with our methods. All images are real, and were inverted into the StyleGAN&#39;s latent space using &lt;a href=&#34;https://github.com/omertov/encoder4editing&#34;&gt;e4e&lt;/a&gt;. The driving text that was used for each edit appears below or above each image.&lt;/p&gt; &#xA;&lt;h4&gt;Latent Optimization&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/me.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/ariana.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/federer.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/styles.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Latent Mapper&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/mapper_hairstyle.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Global Directions&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/global_example_1.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/global_example_2.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/global_example_3.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/orpatashnik/StyleCLIP/main/img/global_example_4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Related Works&lt;/h2&gt; &#xA;&lt;p&gt;The global directions we find for editing are direction in the &lt;em&gt;S Space&lt;/em&gt;, which was introduced and analyzed in &lt;a href=&#34;https://arxiv.org/abs/2011.12799&#34;&gt;StyleSpace&lt;/a&gt; (Wu et al).&lt;/p&gt; &#xA;&lt;p&gt;To edit real images, we inverted them to the StyleGAN&#39;s latent space using &lt;a href=&#34;https://arxiv.org/abs/2102.02766&#34;&gt;e4e&lt;/a&gt; (Tov et al.).&lt;/p&gt; &#xA;&lt;p&gt;The code strcuture of the mapper is heavily based on &lt;a href=&#34;https://github.com/eladrich/pixel2style2pixel&#34;&gt;pSp&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this code for your research, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@InProceedings{Patashnik_2021_ICCV,&#xA;    author    = {Patashnik, Or and Wu, Zongze and Shechtman, Eli and Cohen-Or, Daniel and Lischinski, Dani},&#xA;    title     = {StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery},&#xA;    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},&#xA;    month     = {October},&#xA;    year      = {2021},&#xA;    pages     = {2085-2094}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>kishandiemm/html-portfolio-site</title>
    <updated>2022-11-02T01:33:43Z</updated>
    <id>tag:github.com,2022-11-02:/kishandiemm/html-portfolio-site</id>
    <link href="https://github.com/kishandiemm/html-portfolio-site" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
</feed>