<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-27T01:31:26Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lumiere-video/lumiere-video.github.io</title>
    <updated>2024-01-27T01:31:26Z</updated>
    <id>tag:github.com,2024-01-27:/lumiere-video/lumiere-video.github.io</id>
    <link href="https://github.com/lumiere-video/lumiere-video.github.io" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>swyxio/ai-notes</title>
    <updated>2024-01-27T01:31:26Z</updated>
    <id>tag:github.com,2024-01-27:/swyxio/ai-notes</id>
    <link href="https://github.com/swyxio/ai-notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;notes for software engineers getting up to speed on new AI developments. Serves as datastore for https://latent.space writing, and product brainstorming, but has cleaned up canonical references under the /Resources folder.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Notes&lt;/h1&gt; &#xA;&lt;p&gt;notes on AI state of the art, with a focus on generative and large language models. These are the &#34;raw materials&#34; for the &lt;a href=&#34;https://lspace.swyx.io/&#34;&gt;https://lspace.swyx.io/&lt;/a&gt; newsletter.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repo used to be called &lt;a href=&#34;https://github.com/sw-yx/prompt-eng&#34;&gt;https://github.com/sw-yx/prompt-eng&lt;/a&gt;, but was renamed because &lt;a href=&#34;https://twitter.com/swyx/status/1596184757682941953&#34;&gt;Prompt Engineering is Overhyped&lt;/a&gt;. This is now an &lt;a href=&#34;https://www.latent.space/p/ai-engineer&#34;&gt;AI Engineering&lt;/a&gt; notes repo.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This Readme is just the high level overview of the space; you should see the most updates in the OTHER markdown files in this repo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;TEXT.md&lt;/code&gt; - text generation, mostly with GPT-4 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;TEXT_CHAT.md&lt;/code&gt; - information on ChatGPT and competitors, as well as derivative products&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;TEXT_SEARCH.md&lt;/code&gt; - information on GPT-4 enabled semantic search and other info&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;TEXT_PROMPTS.md&lt;/code&gt; - a small &lt;a href=&#34;https://www.swyx.io/swipe-files-strategy&#34;&gt;swipe file&lt;/a&gt; of good GPT3 prompts&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;INFRA.md&lt;/code&gt; - raw notes on AI Infrastructure, Hardware and Scaling&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;AUDIO.md&lt;/code&gt; - tracking audio/music/voice transcription + generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CODE.md&lt;/code&gt; - codegen models, like Copilot&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;IMAGE_GEN.md&lt;/code&gt; - the most developed file, with the heaviest emphasis notes on Stable Diffusion, and some on midjourney and dalle. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;IMAGE_PROMPTS.md&lt;/code&gt; - a small &lt;a href=&#34;https://www.swyx.io/swipe-files-strategy&#34;&gt;swipe file&lt;/a&gt; of good image prompts&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Resources&lt;/strong&gt;: standing, cleaned up resources that are meant to be permalinked to&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;stub notes&lt;/strong&gt; - very small/lightweight proto pages of future coverage areas - &lt;code&gt;AGENTS.md&lt;/code&gt; - tracking &#34;agentic AI&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;blog ideas&lt;/strong&gt;- potential blog post ideas derived from these notes bc&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;!-- DON&#39;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Table of Contents&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/swyxio/ai-notes/main/#motivational-use-cases&#34;&gt;Motivational Use Cases&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/swyxio/ai-notes/main/#top-ai-reads&#34;&gt;Top AI Reads&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/swyxio/ai-notes/main/#communities&#34;&gt;Communities&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/swyxio/ai-notes/main/#people&#34;&gt;People&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/swyxio/ai-notes/main/#misc&#34;&gt;Misc&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/swyxio/ai-notes/main/#quotes-reality--demotivation&#34;&gt;Quotes, Reality &amp;amp; Demotivation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/swyxio/ai-notes/main/#legal-ethics-and-privacy&#34;&gt;Legal, Ethics, and Privacy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;h2&gt;Motivational Use Cases&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;images &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mpost.io/best-100-stable-diffusion-prompts-the-most-beautiful-ai-text-to-image-prompts&#34;&gt;https://mpost.io/best-100-stable-diffusion-prompts-the-most-beautiful-ai-text-to-image-prompts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/Warvito/status/1570691960792580096?&#34;&gt;3D MRI synthetic brain images&lt;/a&gt; - &lt;a href=&#34;https://twitter.com/danCMDstat/status/1572312699853312000?s=20&amp;amp;t=x-ouUbWA5n0-PxTGZcy2iA&#34;&gt;positive reception from neuroimaging statistician&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/huggingface-projects/stable-diffusion-multiplayer?roomid=room-0&#34;&gt;multiplayer stable diffusion&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;video &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;img2img of famous movie scenes (&lt;a href=&#34;https://twitter.com/TomLikesRobots/status/1565678995986911236&#34;&gt;lalaland&lt;/a&gt;) &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/LighthiserScott/status/1567355079228887041?s=20&amp;amp;t=cBH4EGPC4r0Earm-mDbOKA&#34;&gt;img2img transforming actor&lt;/a&gt; with ebsynth + koe_recast&lt;/li&gt; &#xA;     &lt;li&gt;how ebsynth works &lt;a href=&#34;https://twitter.com/TomLikesRobots/status/1612047103806545923?s=20&#34;&gt;https://twitter.com/TomLikesRobots/status/1612047103806545923?s=20&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;virtual fashion (&lt;a href=&#34;https://twitter.com/karenxcheng/status/1564626773001719813&#34;&gt;karenxcheng&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/replicatehq/status/1568288903177859072?s=20&amp;amp;t=sRd3HRehPMcj1QfcOwDMKg&#34;&gt;seamless tiling images&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;evolution of scenes (&lt;a href=&#34;https://twitter.com/xsteenbrugge/status/1558508866463219712&#34;&gt;xander&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;outpainting &lt;a href=&#34;https://twitter.com/orbamsterdam/status/1568200010747068417?s=21&amp;amp;t=rliacnWOIjJMiS37s8qCCw&#34;&gt;https://twitter.com/orbamsterdam/status/1568200010747068417?s=21&amp;amp;t=rliacnWOIjJMiS37s8qCCw&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;webUI img2img collaboration &lt;a href=&#34;https://twitter.com/_akhaliq/status/1563582621757898752&#34;&gt;https://twitter.com/_akhaliq/status/1563582621757898752&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;image to video with rotation &lt;a href=&#34;https://twitter.com/TomLikesRobots/status/1571096804539912192&#34;&gt;https://twitter.com/TomLikesRobots/status/1571096804539912192&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&#34;prompt paint&#34; &lt;a href=&#34;https://twitter.com/1littlecoder/status/1572573152974372864&#34;&gt;https://twitter.com/1littlecoder/status/1572573152974372864&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;audio2video animation of your face &lt;a href=&#34;https://twitter.com/siavashg/status/1597588865665363969&#34;&gt;https://twitter.com/siavashg/status/1597588865665363969&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;physical toys to 3d model + animation &lt;a href=&#34;https://twitter.com/sergeyglkn/status/1587430510988611584&#34;&gt;https://twitter.com/sergeyglkn/status/1587430510988611584&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;music videos &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=WJaxFbdjm8c&#34;&gt;video killed the radio star&lt;/a&gt;, &lt;a href=&#34;https://colab.research.google.com/github/dmarx/video-killed-the-radio-star/blob/main/Video_Killed_The_Radio_Star_Defusion.ipynb&#34;&gt;colab&lt;/a&gt; This uses OpenAI&#39;s Whisper speech-to-text, allowing you to take a YouTube video &amp;amp; create a Stable Diffusion animation prompted by the lyrics in the YouTube video&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/nateraw/stable-diffusion-videos/blob/main/stable_diffusion_videos.ipynb&#34;&gt;Stable Diffusion Videos&lt;/a&gt; generates videos by interpolating between prompts and audio&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;direct text2video project &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/_akhaliq/status/1575546841533497344&#34;&gt;https://twitter.com/_akhaliq/status/1575546841533497344&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://makeavideo.studio/&#34;&gt;https://makeavideo.studio/&lt;/a&gt; - explorer &lt;a href=&#34;https://webvid.datasette.io/webvid/videos&#34;&gt;https://webvid.datasette.io/webvid/videos&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://phenaki.video/&#34;&gt;https://phenaki.video/&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/CogVideo&#34;&gt;https://github.com/THUDM/CogVideo&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://imagen.research.google/video/&#34;&gt;https://imagen.research.google/video/&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;text-to-3d &lt;a href=&#34;https://twitter.com/_akhaliq/status/1575541930905243652&#34;&gt;https://twitter.com/_akhaliq/status/1575541930905243652&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;https://dreamfusion3d.github.io/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;open source impl: &lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;https://github.com/ashawkey/stable-dreamfusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;demo &lt;a href=&#34;https://twitter.com/_akhaliq/status/1578035919403503616&#34;&gt;https://twitter.com/_akhaliq/status/1578035919403503616&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;text products &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;has a list of usecases at the end &lt;a href=&#34;https://huyenchip.com/2023/04/11/llm-engineering.html&#34;&gt;https://huyenchip.com/2023/04/11/llm-engineering.html&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Jasper&lt;/li&gt; &#xA; &lt;li&gt;GPT for Obsidian &lt;a href=&#34;https://reasonabledeviations.com/2023/02/05/gpt-for-second-brain/&#34;&gt;https://reasonabledeviations.com/2023/02/05/gpt-for-second-brain/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;gpt3 email &lt;a href=&#34;https://github.com/sw-yx/gpt3-email&#34;&gt;https://github.com/sw-yx/gpt3-email&lt;/a&gt; and &lt;a href=&#34;https://github.com/danielgross/embedland/raw/main/bench.py#L281&#34;&gt;email clustering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;gpt3() in google sheet &lt;a href=&#34;https://twitter.com/pavtalk/status/1285410751092416513?s=20&amp;amp;t=ppZhNO_OuQmXkjHQ7dl4wg&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/shubroski/status/1587136794797244417&#34;&gt;2022&lt;/a&gt; - &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1YzeQLG_JVqHKz5z4QE9wUsYbLoVZZxbGDnj7wCf_0QQ/edit&#34;&gt;sheet&lt;/a&gt; google sheets &lt;a href=&#34;https://twitter.com/mehran__jalali/status/1608159307513618433&#34;&gt;https://twitter.com/mehran__jalali/status/1608159307513618433&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://gpt3demo.com/apps/google-sheets&#34;&gt;https://gpt3demo.com/apps/google-sheets&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Charm &lt;a href=&#34;https://twitter.com/shubroski/status/1620139262925754368?s=20&#34;&gt;https://twitter.com/shubroski/status/1620139262925754368?s=20&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.summari.com/&#34;&gt;https://www.summari.com/&lt;/a&gt; Summari helps busy people read more&lt;/li&gt; &#xA; &lt;li&gt;market maps/landscapes &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;sequoia market map &lt;a href=&#34;https://twitter.com/sonyatweetybird/status/1584580362339962880&#34;&gt;jan 2023&lt;/a&gt;, &lt;a href=&#34;https://www.sequoiacap.com/article/llm-stack-perspective/&#34;&gt;july 2023&lt;/a&gt;, &lt;a href=&#34;https://www.sequoiacap.com/article/generative-ai-act-two/&#34;&gt;sep 2023&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;base10 market map &lt;a href=&#34;https://twitter.com/letsenhance_io/status/1594826383305449491&#34;&gt;https://twitter.com/letsenhance_io/status/1594826383305449491&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;matt shumer market map &lt;a href=&#34;https://twitter.com/mattshumer_/status/1620465468229451776&#34;&gt;https://twitter.com/mattshumer_/status/1620465468229451776&lt;/a&gt; &lt;a href=&#34;https://docs.google.com/document/d/1sewTBzRF087F6hFXiyeOIsGC1N4N3O7rYzijVexCgoQ/edit&#34;&gt;https://docs.google.com/document/d/1sewTBzRF087F6hFXiyeOIsGC1N4N3O7rYzijVexCgoQ/edit&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;nfx &lt;a href=&#34;https://www.nfx.com/post/generative-ai-tech-5-layers?ref=context-by-cohere&#34;&gt;https://www.nfx.com/post/generative-ai-tech-5-layers?ref=context-by-cohere&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;a16z &lt;a href=&#34;https://a16z.com/2023/01/19/who-owns-the-generative-ai-platform/&#34;&gt;https://a16z.com/2023/01/19/who-owns-the-generative-ai-platform/&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/&#34;&gt;https://a16z.com/2023/06/20/emerging-architectures-for-llm-applications/&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;madrona &lt;a href=&#34;https://www.madrona.com/foundation-models/&#34;&gt;https://www.madrona.com/foundation-models/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;coatue &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.coatue.com/blog/perspective/ai-the-coming-revolution-2023&#34;&gt;https://www.coatue.com/blog/perspective/ai-the-coming-revolution-2023&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://x.com/Sam_Awrabi/status/1742324900034150646?s=20&#34;&gt;https://x.com/Sam_Awrabi/status/1742324900034150646?s=20&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;game assets - &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;emad thread &lt;a href=&#34;https://twitter.com/EMostaque/status/1591436813750906882&#34;&gt;https://twitter.com/EMostaque/status/1591436813750906882&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;scenario.gg &lt;a href=&#34;https://twitter.com/emmanuel_2m/status/1593356241283125251&#34;&gt;https://twitter.com/emmanuel_2m/status/1593356241283125251&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.traffickinggame.com/ai-assisted-graphics/&#34;&gt;3d game character modeling example&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;MarioGPT &lt;a href=&#34;https://arxiv.org/pdf/2302.05981.pdf&#34;&gt;https://arxiv.org/pdf/2302.05981.pdf&lt;/a&gt; &lt;a href=&#34;https://www.slashgear.com/1199870/mariogpt-uses-ai-to-generate-endless-super-mario-levels-for-free/&#34;&gt;https://www.slashgear.com/1199870/mariogpt-uses-ai-to-generate-endless-super-mario-levels-for-free/&lt;/a&gt; &lt;a href=&#34;https://github.com/shyamsn97/mario-gpt/raw/main/mario_gpt/level.py&#34;&gt;https://github.com/shyamsn97/mario-gpt/blob/main/mario_gpt/level.py&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=36295227&#34;&gt;https://news.ycombinator.com/item?id=36295227&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Top AI Reads&lt;/h2&gt; &#xA;&lt;p&gt;The more advanced GPT3 reads have been split out to &lt;a href=&#34;https://github.com/sw-yx/ai-notes/raw/main/TEXT.md&#34;&gt;https://github.com/sw-yx/ai-notes/blob/main/TEXT.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gwern.net/GPT-3#prompts-as-programming&#34;&gt;https://www.gwern.net/GPT-3#prompts-as-programming&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learnprompting.org/&#34;&gt;https://learnprompting.org/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Beginner Reads&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gatesnotes.com/The-Age-of-AI-Has-Begun&#34;&gt;Bill Gates on AI&lt;/a&gt; (&lt;a href=&#34;https://twitter.com/gdb/status/1638310597325365251?s=20&#34;&gt;tweet&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&#34;The development of AI is as fundamental as the creation of the microprocessor, the personal computer, the Internet, and the mobile phone. It will change the way people work, learn, travel, get health care, and communicate with each other.&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://about.sourcegraph.com/blog/cheating-is-all-you-need&#34;&gt;Steve Yegge on AI for developers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zjkBMFhNj_g&#34;&gt;Karpathy 2023 intro to LLMs&lt;/a&gt; (notes from &lt;a href=&#34;https://twitter.com/SarahChieng/status/1729569057475879103&#34;&gt;Sarah Chieng&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/SarahChieng/status/1741926266087870784&#34;&gt;Prompt Engineering guide from OpenAI at NeurIPS&lt;/a&gt; via Sarah Chieng&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.thenewatlantis.com/publications/why-this-ai-moment-may-be-the-real-deal&#34;&gt;Why this AI moment might be the real deal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sam Altman - &lt;a href=&#34;https://moores.samaltman.com/&#34;&gt;Moore&#39;s Law for Everything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;excellent introduction to foundation models from MSR &lt;a href=&#34;https://youtu.be/HQI6O5DlyFc&#34;&gt;https://youtu.be/HQI6O5DlyFc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;openAI prompt tutorial &lt;a href=&#34;https://beta.openai.com/docs/quickstart/add-some-examples&#34;&gt;https://beta.openai.com/docs/quickstart/add-some-examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;google LAMDA intro &lt;a href=&#34;https://aitestkitchen.withgoogle.com/how-lamda-works&#34;&gt;https://aitestkitchen.withgoogle.com/how-lamda-works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;karpathy gradient descent course&lt;/li&gt; &#xA; &lt;li&gt;FT visual storytelling on &#34;&lt;a href=&#34;https://ig.ft.com/generative-ai/&#34;&gt;how transformers work&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;DALLE2 prompt writing book &lt;a href=&#34;http://dallery.gallery/wp-content/uploads/2022/07/The-DALL%C2%B7E-2-prompt-book-v1.02.pdf&#34;&gt;http://dallery.gallery/wp-content/uploads/2022/07/The-DALL%C2%B7E-2-prompt-book-v1.02.pdf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/nerd-for-tech/prompt-engineering-the-career-of-future-2fb93f90f117&#34;&gt;https://medium.com/nerd-for-tech/prompt-engineering-the-career-of-future-2fb93f90f117&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.oneusefulthing.org/p/how-to-use-ai-to-do-stuff-an-opinionated&#34;&gt;How to use AI to do stuff&lt;/a&gt; across getting information, working with data, and making images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ourworldindata.org/brief-history-of-ai&#34;&gt;https://ourworldindata.org/brief-history-of-ai&lt;/a&gt; ai progress overview with nice charts&lt;/li&gt; &#xA; &lt;li&gt;Jon Stokes&#39; &lt;a href=&#34;https://www.jonstokes.com/p/ai-content-generation-part-1-machine&#34;&gt;AI Content Generation, Part 1: Machine Learning Basics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5p248yoa3oE&#34;&gt;Andrew Ng - Opportunities in AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://txt.cohere.ai/what-are-transformer-models/&#34;&gt;What are transformer models and how do they work?&lt;/a&gt; - maybe &lt;a href=&#34;https://news.ycombinator.com/item?id=35577138&#34;&gt;a bit too high level&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;text generation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;humanloop&#39;s &lt;a href=&#34;https://website-olo3k29b2-humanloopml.vercel.app/blog/prompt-engineering-101&#34;&gt;prompt engineering 101&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Stephen Wolfram&#39;s explanations &lt;a href=&#34;https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/&#34;&gt;https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;equivalent from jon stokes jonstokes.com/p/the-chat-stack-gpt-4-and-the-near&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://andymatuschak.org/prompts/&#34;&gt;https://andymatuschak.org/prompts/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;cohere&#39;s LLM university &lt;a href=&#34;https://docs.cohere.com/docs/llmu&#34;&gt;https://docs.cohere.com/docs/llmu&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Jay alammar&#39;s guide to all the things: &lt;a href=&#34;https://llm.university/&#34;&gt;https://llm.university/&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.jonstokes.com/p/chatgpt-explained-a-guide-for-normies&#34;&gt;https://www.jonstokes.com/p/chatgpt-explained-a-guide-for-normies&lt;/a&gt; for normies&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;image generation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://wiki.installgentoo.com/wiki/Stable_Diffusion&#34;&gt;https://wiki.installgentoo.com/wiki/Stable_Diffusion&lt;/a&gt; overview&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/x41n87/how_to_get_images_that_dont_suck_a/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/x41n87/how_to_get_images_that_dont_suck_a/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://mpost.io/best-100-stable-diffusion-prompts-the-most-beautiful-ai-text-to-image-prompts/&#34;&gt;https://mpost.io/best-100-stable-diffusion-prompts-the-most-beautiful-ai-text-to-image-prompts/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html&#34;&gt;https://www.kdnuggets.com/2021/03/beginners-guide-clip-model.html&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;for nontechnical &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.jonstokes.com/p/ai-content-generation-part-1-machine&#34;&gt;https://www.jonstokes.com/p/ai-content-generation-part-1-machine&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.protocol.com/generative-ai-startup-landscape-map&#34;&gt;https://www.protocol.com/generative-ai-startup-landscape-map&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/saranormous/status/1572791179636518913&#34;&gt;https://twitter.com/saranormous/status/1572791179636518913&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Intermediate Reads&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;State of AI Report&lt;/strong&gt;: &lt;a href=&#34;https://www.stateof.ai/2018&#34;&gt;2018&lt;/a&gt;, &lt;a href=&#34;https://www.stateof.ai/2019&#34;&gt;2019&lt;/a&gt;, &lt;a href=&#34;https://www.stateof.ai/2020&#34;&gt;2020&lt;/a&gt;, &lt;a href=&#34;https://www.stateof.ai/2021&#34;&gt;2021&lt;/a&gt;, &lt;a href=&#34;https://www.stateof.ai/&#34;&gt;2022&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;reverse chronological major events &lt;a href=&#34;https://bleedingedge.ai/&#34;&gt;https://bleedingedge.ai/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://willthompson.name/what-we-know-about-llms-primer#block-920907dc37394adcac5bf4e7318adc10&#34;&gt;What we Know about LLMs&lt;/a&gt; - great recap of research&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zjkBMFhNj_g&#34;&gt;Karpathy&#39;s 1hr guide to LLMs&lt;/a&gt; - summary &lt;a href=&#34;https://twitter.com/SarahChieng/status/1729569057475879103&#34;&gt;from Sarah Chieng&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;What is a large language model (LLM)?&lt;/li&gt; &#xA;    &lt;/ol&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;There are two main components of an LLM &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;What does an LLM do?&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;How do you create an LLM? &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Stage 1: Model Pre-Training&lt;/li&gt; &#xA;     &lt;li&gt;Stage 2: Model Fine-tuning &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Stage 2b: [Optional] Additional Fine-tuning&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Stage 3: Model Inference&lt;/li&gt; &#xA;     &lt;li&gt;Stage 4: [Optional] Supercharging LLMs with Customization&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;The Current LLM “Leaderboard”&lt;/li&gt; &#xA;   &lt;li&gt;The Future of LLMs: What’s Next? &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;How to improve LLM performance? &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;LLM Scaling Laws&lt;/li&gt; &#xA;       &lt;li&gt;Self-Improvement&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;How to improve LLM abilities? &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Multimodality&lt;/li&gt; &#xA;       &lt;li&gt;System 1 + 2 Thinking&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;The LLM Dark Arts &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Jailbreaking&lt;/li&gt; &#xA;     &lt;li&gt;Prompt Injecting&lt;/li&gt; &#xA;     &lt;li&gt;Data Poisoning &amp;amp; Backdoor Attacks&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/jeremyphoward/status/1705883362991472984?s=20&#34;&gt;A Hacker&#39;s Guide to Language Models&lt;/a&gt; (&lt;a href=&#34;https://youtu.be/jkrNMKz9pWU?si=BNz-v6VmdbX7QDtr&#34;&gt;youtube&lt;/a&gt;) Jeremy Howard&#39;s 90min complete overview of LLM learnings - starting at the basics: the 3-step pre-training / fine-tuning / classifier ULMFiT approach used in all modern LLMs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://simonwillison.net/2023/Aug/3/weird-world-of-llms/&#34;&gt;&#34;Catching up on the weird world of LLMs&#34;&lt;/a&gt; - Simon Willison&#39;s 40min overview + &lt;a href=&#34;https://www.youtube.com/watch?v=AjLVoAu-u-Q&#34;&gt;Open Questions for AI Engineers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flyte.org/blog/getting-started-with-large-language-models-key-things-to-know#what-are-llms&#34;&gt;LLMs overview from Flyte&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://eugeneyan.com/writing/llm-patterns/&#34;&gt;Patterns for building LLM-based systems and products&lt;/a&gt; - great recap &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://eugeneyan.com/writing/llm-patterns/#evals-to-measure-performance&#34;&gt;Evals&lt;/a&gt;: To measure performance&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://eugeneyan.com/writing/llm-patterns/#retrieval-augmented-generation-to-add-knowledge&#34;&gt;RAG&lt;/a&gt;: To add recent, external knowledge&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://eugeneyan.com/writing/llm-patterns/#fine-tuning-to-get-better-at-specific-tasks&#34;&gt;Fine-tuning&lt;/a&gt;: To get better at specific tasks&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://eugeneyan.com/writing/llm-patterns/#caching-to-reduce-latency-and-cost&#34;&gt;Caching&lt;/a&gt;: To reduce latency &amp;amp; cost&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://eugeneyan.com/writing/llm-patterns/#guardrails-to-ensure-output-quality&#34;&gt;Guardrails&lt;/a&gt;: To ensure output quality&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://eugeneyan.com/writing/llm-patterns/#defensive-ux-to-anticipate--handle-errors-gracefully&#34;&gt;Defensive UX&lt;/a&gt;: To anticipate &amp;amp; manage errors gracefully&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://eugeneyan.com/writing/llm-patterns/#collect-user-feedback-to-build-our-data-flywheel&#34;&gt;Collect user feedback&lt;/a&gt;: To build our data flywheel&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tge-data-web.nyc3.digitaloceanspaces.com/docs/Vector%20Databases%20-%20A%20Technical%20Primer.pdf&#34;&gt;Vector Databases: A Technical Primer [pdf]&lt;/a&gt; very nice slides on Vector DBs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Missing coverage of hybrid search (vector + lexical). &lt;a href=&#34;https://news.ycombinator.com/item?id=38971221&#34;&gt;Further discussions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;A16z AI Canon &lt;a href=&#34;https://a16z.com/2023/05/25/ai-canon/&#34;&gt;https://a16z.com/2023/05/25/ai-canon/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://karpathy.medium.com/software-2-0-a64152b37c35&#34;&gt;Software 2.0&lt;/a&gt;&lt;/strong&gt;: Andrej Karpathy was one of the first to clearly explain (in 2017!) why the new AI wave really matters. His argument is that AI is a new and powerful way to program computers. As LLMs have improved rapidly, this thesis has proven prescient, and it gives a good mental model for how the AI market may progress.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&#34;&gt;State of GPT&lt;/a&gt;&lt;/strong&gt;: Also from Karpathy, this is a very approachable explanation of how ChatGPT / GPT models in general work, how to use them, and what directions R&amp;amp;D may take.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/&#34;&gt;&lt;strong&gt;What is ChatGPT doing … and why does it work?&lt;/strong&gt;&lt;/a&gt;: Computer scientist and entrepreneur Stephen Wolfram gives a long but highly readable explanation, from first principles, of how modern AI models work. He follows the timeline from early neural nets to today’s LLMs and ChatGPT.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://daleonai.com/transformers-explained&#34;&gt;Transformers, explained&lt;/a&gt;&lt;/strong&gt;: This post by Dale Markowitz is a shorter, more direct answer to the question “what is an LLM, and how does it work?” This is a great way to ease into the topic and develop intuition for the technology. It was written about GPT-3 but still applies to newer models.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://mccormickml.com/2022/12/21/how-stable-diffusion-works/&#34;&gt;How Stable Diffusion works&lt;/a&gt;&lt;/strong&gt;: This is the computer vision analogue to the last post. Chris McCormick gives a layperson’s explanation of how Stable Diffusion works and develops intuition around text-to-image models generally. For an even&amp;nbsp;&lt;em&gt;gentler&lt;/em&gt;&amp;nbsp;introduction, check out this&amp;nbsp;&lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/zs5dk5/i_made_an_infographic_to_explain_how_stable/&#34;&gt;comic&lt;/a&gt;&amp;nbsp;from r/StableDiffusion.&lt;/li&gt; &#xA;   &lt;li&gt;Explainers &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/deep-learning-nutshell-core-concepts/&#34;&gt;&lt;strong&gt;Deep learning in a nutshell: core concepts&lt;/strong&gt;&lt;/a&gt;: This four-part series from Nvidia walks through the basics of deep learning as practiced in 2015, and is a good resource for anyone just learning about AI.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://course.fast.ai/&#34;&gt;Practical deep learning for coders&lt;/a&gt;&lt;/strong&gt;: Comprehensive, free course on the fundamentals of AI, explained through practical examples and code.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://towardsdatascience.com/word2vec-explained-49c52b4ccb71&#34;&gt;Word2vec explained&lt;/a&gt;&lt;/strong&gt;: Easy introduction to embeddings and tokens, which are building blocks of LLMs (and all language models).&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b&#34;&gt;Yes you should understand backprop&lt;/a&gt;&lt;/strong&gt;: More in-depth post on back-propagation if you want to understand the details. If you want even more, try the&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=i94OvYb6noo&#34;&gt;Stanford CS231n lecture&lt;/a&gt; (&lt;a href=&#34;http://cs231n.stanford.edu/2016/&#34;&gt;course here&lt;/a&gt;)&amp;nbsp;on Youtube.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Courses &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&#34;&gt;Stanford CS229&lt;/a&gt;&lt;/strong&gt;: Introduction to Machine Learning with Andrew Ng, covering the fundamentals of machine learning.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&#34;&gt;Stanford CS224N&lt;/a&gt;&lt;/strong&gt;: NLP with Deep Learning with Chris Manning, covering NLP basics through the first generation of LLMs.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mlabonne/llm-course&#34;&gt;https://github.com/mlabonne/llm-course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cims.nyu.edu/~sbowman/eightthings.pdf&#34;&gt;https://cims.nyu.edu/~sbowman/eightthings.pdf&lt;/a&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;LLMs predictably get more capable with increasing investment, even without targeted innovation.&lt;/li&gt; &#xA;   &lt;li&gt;Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment.&lt;/li&gt; &#xA;   &lt;li&gt;LLMs often appear to learn and use representations of the outside world.&lt;/li&gt; &#xA;   &lt;li&gt;There are no reliable techniques for steering the behavior of LLMs.&lt;/li&gt; &#xA;   &lt;li&gt;Experts are not yet able to interpret the inner workings of LLMs.&lt;/li&gt; &#xA;   &lt;li&gt;Human performance on a task isn’t an upper bound on LLM performance.&lt;/li&gt; &#xA;   &lt;li&gt;LLMs need not express the values of their creators nor the values encoded in web text.&lt;/li&gt; &#xA;   &lt;li&gt;Brief interactions with LLMs are often misleading.&lt;/li&gt; &#xA;   &lt;li&gt;simonw highlights &lt;a href=&#34;https://fedi.simonwillison.net/@simon/110144185463887790&#34;&gt;https://fedi.simonwillison.net/@simon/110144185463887790&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;10 open challenges in LLM research &lt;a href=&#34;https://huyenchip.com/2023/08/16/llm-research-open-challenges.html&#34;&gt;https://huyenchip.com/2023/08/16/llm-research-open-challenges.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;openai prompt eng cookbook &lt;a href=&#34;https://github.com/openai/openai-cookbook/raw/main/techniques_to_improve_reliability.md&#34;&gt;https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;on prompt eng overview &lt;a href=&#34;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&#34;&gt;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know/&#34;&gt;https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know/&lt;/a&gt; comparing search vs ai&lt;/li&gt; &#xA; &lt;li&gt;Recap of 2022&#39;s major AI developments &lt;a href=&#34;https://www.deeplearning.ai/the-batch/issue-176/&#34;&gt;https://www.deeplearning.ai/the-batch/issue-176/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DALLE2 asset generation + inpainting &lt;a href=&#34;https://twitter.com/aifunhouse/status/1576202480936886273?s=20&amp;amp;t=5EXa1uYDPVa2SjZM-SxhCQ&#34;&gt;https://twitter.com/aifunhouse/status/1576202480936886273?s=20&amp;amp;t=5EXa1uYDPVa2SjZM-SxhCQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;suhail journey &lt;a href=&#34;https://twitter.com/Suhail/status/1541276314485018625?s=20&amp;amp;t=X2MVKQKhDR28iz3VZEEO8w&#34;&gt;https://twitter.com/Suhail/status/1541276314485018625?s=20&amp;amp;t=X2MVKQKhDR28iz3VZEEO8w&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;composable diffusion - &#34;AND&#34; instead of &#34;and&#34; &lt;a href=&#34;https://twitter.com/TomLikesRobots/status/1580293860902985728&#34;&gt;https://twitter.com/TomLikesRobots/status/1580293860902985728&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;on BPE tokenization &lt;a href=&#34;https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0&#34;&gt;https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0&lt;/a&gt; see also google sentencepiece and openai tiktoken &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;source in GPT2 source &lt;a href=&#34;https://github.com/openai/gpt-2/raw/master/src/encoder.py&#34;&gt;https://github.com/openai/gpt-2/blob/master/src/encoder.py&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;note that BPEs are suboptimal &lt;a href=&#34;https://www.lesswrong.com/posts/dFbfCLZA4pejckeKc/a-mechanistic-explanation-for-solidgoldmagikarp-like-tokens?commentId=9jNdKscwEWBB4GTCQ&#34;&gt;https://www.lesswrong.com/posts/dFbfCLZA4pejckeKc/a-mechanistic-explanation-for-solidgoldmagikarp-like-tokens?commentId=9jNdKscwEWBB4GTCQ&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;causes math and string character issues &lt;a href=&#34;https://news.ycombinator.com/item?id=35363769&#34;&gt;https://news.ycombinator.com/item?id=35363769&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=39086318&#34;&gt;glitch tokens&lt;/a&gt; happen when tokenizer has different dataset than LLM&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/tokenizer&#34;&gt;https://platform.openai.com/tokenizer&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;https://github.com/openai/tiktoken&lt;/a&gt; (more up to date: &lt;a href=&#34;https://tiktokenizer.vercel.app/&#34;&gt;https://tiktokenizer.vercel.app/&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Wordpiece -&amp;gt; BPE -&amp;gt; SentenceTransformer &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526?gi=ee46baab0d8f&#34;&gt;Preliminary reading on Embeddings&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://youtu.be/QdDoFfkVkcw?si=qefZSDDSpxDNd313&#34;&gt;https://youtu.be/QdDoFfkVkcw?si=qefZSDDSpxDNd313&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/mteb&#34;&gt;Huggingface MTEB Benchmark of a bunch of Embeddings&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/Nils_Reimers/status/1487014195568775173&#34;&gt;notable issues with GPT3 Embeddings&lt;/a&gt;&amp;nbsp;and alternatives to consider&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://observablehq.com/@simonw/gpt-3-token-encoder-decoder&#34;&gt;https://observablehq.com/@simonw/gpt-3-token-encoder-decoder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;karpathy wants tokenization to go away &lt;a href=&#34;https://twitter.com/karpathy/status/1657949234535211009&#34;&gt;https://twitter.com/karpathy/status/1657949234535211009&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;positional encoding not needed for decoder only &lt;a href=&#34;https://twitter.com/a_kazemnejad/status/1664277559968927744?s=20&#34;&gt;https://twitter.com/a_kazemnejad/status/1664277559968927744?s=20&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;creates its own language &lt;a href=&#34;https://twitter.com/giannis_daras/status/1531693104821985280&#34;&gt;https://twitter.com/giannis_daras/status/1531693104821985280&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Google Cloud Generative AI Learning Path &lt;a href=&#34;https://www.cloudskillsboost.google/paths/118&#34;&gt;https://www.cloudskillsboost.google/paths/118&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;img2img &lt;a href=&#34;https://andys.page/posts/how-to-draw/&#34;&gt;https://andys.page/posts/how-to-draw/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;on language modeling &lt;a href=&#34;https://lena-voita.github.io/nlp_course/language_modeling.html&#34;&gt;https://lena-voita.github.io/nlp_course/language_modeling.html&lt;/a&gt; and approachable but technical explanation of language generation including sampling from distributions and some mechanistic intepretability (finding neuron that tracks quote state)&lt;/li&gt; &#xA; &lt;li&gt;quest for photorealism &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/x9zmjd/quest_for_ultimate_photorealism_part_2_colors/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/x9zmjd/quest_for_ultimate_photorealism_part_2_colors/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://medium.com/merzazine/prompt-design-for-dall-e-photorealism-emulating-reality-6f478df6f186&#34;&gt;https://medium.com/merzazine/prompt-design-for-dall-e-photorealism-emulating-reality-6f478df6f186&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;settings tweaking &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/x3k79h/the_feeling_of_discovery_sd_is_like_a_great_proc/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/x3k79h/the_feeling_of_discovery_sd_is_like_a_great_proc/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;seed selection &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/x8szj9/tutorial_seed_selection_and_the_impact_on_your/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/x8szj9/tutorial_seed_selection_and_the_impact_on_your/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;minor parameter parameter difference study (steps, clamp_max, ETA, cutn_batches, etc) &lt;a href=&#34;https://twitter.com/KyrickYoung/status/1500196286930292742&#34;&gt;https://twitter.com/KyrickYoung/status/1500196286930292742&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Generative AI: Autocomplete for everything &lt;a href=&#34;https://noahpinion.substack.com/p/generative-ai-autocomplete-for-everything?sd=pf&#34;&gt;https://noahpinion.substack.com/p/generative-ai-autocomplete-for-everything?sd=pf&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1&#34;&gt;How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources&lt;/a&gt; good paper with the development history of the GPT family of models and how the capabilities developed&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://barryz-architecture-of-agentic-llm.notion.site/Almost-Everything-I-know-about-LLMs-d117ca25d4624199be07e9b0ab356a77&#34;&gt;https://barryz-architecture-of-agentic-llm.notion.site/Almost-Everything-I-know-about-LLMs-d117ca25d4624199be07e9b0ab356a77&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Advanced Reads&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Mooler0410/LLMsPracticalGuide&#34;&gt;https://github.com/Mooler0410/LLMsPracticalGuide&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;good curated list of all the impt papers&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/eleutherAI/cookbook#the-cookbook&#34;&gt;https://github.com/eleutherAI/cookbook#the-cookbook&lt;/a&gt; Eleuther AI&#39;s list of resources for training. compare to &lt;a href=&#34;https://github.com/google-research/tuning_playbook&#34;&gt;https://github.com/google-research/tuning_playbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;anti hype LLM reading list &lt;a href=&#34;https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e&#34;&gt;https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/_jasonwei/status/1729585618311950445&#34;&gt;6 papers from Jason Wei of OpenAI&lt;/a&gt; (&lt;a href=&#34;https://www.jasonwei.net/blog/some-intuitions-about-large-language-models&#34;&gt;blog&lt;/a&gt;) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GPT-3 paper (&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;https://arxiv.org/abs/2005.14165&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;chain-of-thought prompting (&lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;https://arxiv.org/abs/2201.11903&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;scaling laws, (&lt;a href=&#34;https://arxiv.org/abs/2001.08361&#34;&gt;https://arxiv.org/abs/2001.08361&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;emergent abilities (&lt;a href=&#34;https://arxiv.org/abs/2206.07682&#34;&gt;https://arxiv.org/abs/2206.07682&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;language models can follow both flipped labels and semantically-unrelated labels (&lt;a href=&#34;https://arxiv.org/abs/2303.03846&#34;&gt;https://arxiv.org/abs/2303.03846&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Transformers from scratch &lt;a href=&#34;https://e2eml.school/transformers.html&#34;&gt;https://e2eml.school/transformers.html&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;transformers vs LSTM &lt;a href=&#34;https://medium.com/analytics-vidhya/why-are-lstms-struggling-to-matchup-with-transformers-a1cc5b2557e3&#34;&gt;https://medium.com/analytics-vidhya/why-are-lstms-struggling-to-matchup-with-transformers-a1cc5b2557e3&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;transformer code walkthru &lt;a href=&#34;https://twitter.com/mark_riedl/status/1555188022534176768&#34;&gt;https://twitter.com/mark_riedl/status/1555188022534176768&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;transformer familyi &lt;a href=&#34;https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/&#34;&gt;https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;carmack paper list &lt;a href=&#34;https://news.ycombinator.com/item?id=34639634&#34;&gt;https://news.ycombinator.com/item?id=34639634&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Transformer models: an introduction and catalog &lt;a href=&#34;https://arxiv.org/abs/2302.07730&#34;&gt;https://arxiv.org/abs/2302.07730&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Deepmind - formal algorithms for transformers &lt;a href=&#34;https://arxiv.org/pdf/2207.09238.pdf&#34;&gt;https://arxiv.org/pdf/2207.09238.pdf&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Jay Alammar explainers &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&#34;&gt;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;karpathy on transformers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Convergence&lt;/strong&gt;: The ongoing consolidation in AI is incredible. When I started ~decade ago vision, speech, natural language, reinforcement learning, etc. were completely separate; You couldn&#39;t read papers across areas - the approaches were completely different, often not even ML based. In 2010s all of these areas started to transition 1) to machine learning and specifically 2) neural nets. The architectures were diverse but at least the papers started to read more similar, all of them utilizing large datasets and optimizing neural nets. But as of approx. last two years, even the neural net architectures across all areas are starting to look identical - a Transformer (definable in ~200 lines of PyTorch &lt;a href=&#34;https://t.co/xQL5NyJkLE&#34;&gt;https://github.com/karpathy/minGPT/blob/master/mingpt/model.py…&lt;/a&gt;), with very minor differences. Either as a strong baseline or (often) state of the art. (&lt;a href=&#34;https://twitter.com/karpathy/status/1468370605229547522?s=20&#34;&gt;tweetstorm&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Why Transformers won&lt;/strong&gt;: The Transformer is a magnificient neural network architecture because it is a general-purpose differentiable computer. It is simultaneously: 1) expressive (in the forward pass) 2) optimizable (via backpropagation+gradient descent) 3) efficient (high parallelism compute graph) &lt;a href=&#34;https://twitter.com/karpathy/status/1582807367988654081&#34;&gt;tweetstorm&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/karpathy/status/1593417989830848512?s=20&#34;&gt;https://twitter.com/karpathy/status/1593417989830848512?s=20&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;elaborated in &lt;a href=&#34;https://www.youtube.com/watch?v=XfpMkf4rD6E&#34;&gt;1hr stanford lecture&lt;/a&gt; and &lt;a href=&#34;https://www.youtube.com/watch?v=9uw3F6rndnA&#34;&gt;8min lex fridman summary&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/karpathy/status/1645115622517542913&#34;&gt;BabyGPT&lt;/a&gt; with two tokens 0/1 and context length of 3, viewing it as a finite state markov chain. It was trained on the sequence &#34;111101111011110&#34; for 50 iterations. The parameters and the architecture of the Transformer modifies the probabilities on the arrows.&lt;/li&gt; &#xA;   &lt;li&gt;Build GPT from scratch &lt;a href=&#34;https://www.youtube.com/watch?v=kCc8FmEb1nY&#34;&gt;https://www.youtube.com/watch?v=kCc8FmEb1nY&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;different GPT from scratch in 60 LOC &lt;a href=&#34;https://jaykmody.com/blog/gpt-from-scratch/&#34;&gt;https://jaykmody.com/blog/gpt-from-scratch/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jasonwei.net/blog/emergence&#34;&gt;137 emergent abilities of large language models&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Emergent few-shot prompted tasks: BIG-Bench and MMLU benchmarks&lt;/li&gt; &#xA;   &lt;li&gt;Emergent prompting strategies &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=gEZrGCozdqR&#34;&gt;Instruction-following&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=iedYJm92o0a&#34;&gt;Scratchpad&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.11446&#34;&gt;Using open-book knowledge for fact checking&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;Chain-of-thought prompting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.06991&#34;&gt;Differentiable search index&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.11171&#34;&gt;Self-consistency&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.02329&#34;&gt;Leveraging explanations in prompting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.10625&#34;&gt;Least-to-most prompting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.11916&#34;&gt;Zero-shot chain-of-thought&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.05221&#34;&gt;Calibration via P(True)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.03057&#34;&gt;Multilingual chain-of-thought&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02441&#34;&gt;Ask-me-anything prompting&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;some pushback - are they a mirage? just dont use harsh metrics &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.jasonwei.net/blog/common-arguments-regarding-emergent-abilities&#34;&gt;https://www.jasonwei.net/blog/common-arguments-regarding-emergent-abilities&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://hai.stanford.edu/news/ais-ostensible-emergent-abilities-are-mirage&#34;&gt;https://hai.stanford.edu/news/ais-ostensible-emergent-abilities-are-mirage&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Images &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Eugene Yan explanation of the Text to Image stack &lt;a href=&#34;https://eugeneyan.com/writing/text-to-image/&#34;&gt;https://eugeneyan.com/writing/text-to-image/&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;VQGAN/CLIP &lt;a href=&#34;https://minimaxir.com/2021/08/vqgan-clip/&#34;&gt;https://minimaxir.com/2021/08/vqgan-clip/&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;10 years of Image generation history &lt;a href=&#34;https://zentralwerkstatt.org/blog/ten-years-of-image-synthesis&#34;&gt;https://zentralwerkstatt.org/blog/ten-years-of-image-synthesis&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Vision Transformers (ViT) Explained &lt;a href=&#34;https://www.pinecone.io/learn/vision-transformers/&#34;&gt;https://www.pinecone.io/learn/vision-transformers/&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;negative prompting &lt;a href=&#34;https://minimaxir.com/2022/11/stable-diffusion-negative-prompt/&#34;&gt;https://minimaxir.com/2022/11/stable-diffusion-negative-prompt/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;best papers of 2022 &lt;a href=&#34;https://www.yitay.net/blog/2022-best-nlp-papers&#34;&gt;https://www.yitay.net/blog/2022-best-nlp-papers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2202.07785.pdf&#34;&gt;Predictability and Surprise in Large Generative Models&lt;/a&gt; - good survey paper of what we know about scaling and capabilities and rise of LLMs so far&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;more prompt eng papers &lt;a href=&#34;https://github.com/dair-ai/Prompt-Engineering-Guide&#34;&gt;https://github.com/dair-ai/Prompt-Engineering-Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://creator.nightcafe.studio/vqgan-clip-keyword-modifier-comparison&#34;&gt;https://creator.nightcafe.studio/vqgan-clip-keyword-modifier-comparison&lt;/a&gt; VQGAN+CLIP Keyword Modifier Comparison&lt;/li&gt; &#xA; &lt;li&gt;History of Transformers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;richard socher on their contribution to attention mechanism leading up to transformers &lt;a href=&#34;https://overcast.fm/+r1P4nKfFU/1:00:00&#34;&gt;https://overcast.fm/+r1P4nKfFU/1:00:00&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://kipp.ly/blog/transformer-taxonomy/&#34;&gt;https://kipp.ly/blog/transformer-taxonomy/&lt;/a&gt; This document is my running literature review for people trying to catch up on AI. It covers 22 models, 11 architectural changes, 7 post-pre-training techniques and 3 training techniques (and 5 things that are none of the above)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://magazine.sebastianraschka.com/p/understanding-large-language-models&#34;&gt;Understanding Large Language Models A Cross-Section of the Most Relevant Literature To Get Up to Speed&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;giving credit to Bandanau et al (2014), which I believe first proposed the concept of applying a Softmax function over token scores to compute attention, setting the stage for the original transformer by Vaswani et al (2017). &lt;a href=&#34;https://news.ycombinator.com/item?id=35589756&#34;&gt;https://news.ycombinator.com/item?id=35589756&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://finbarrtimbers.substack.com/p/five-years-of-progress-in-gpts&#34;&gt;https://finbarrtimbers.substack.com/p/five-years-of-progress-in-gpts&lt;/a&gt; GPT1/2/3, Megatron, Gopher, Chinchilla, PaLM, LLaMa&lt;/li&gt; &#xA;   &lt;li&gt;good summary paper (8 things to know) &lt;a href=&#34;https://cims.nyu.edu/~sbowman/eightthings.pdf&#34;&gt;https://cims.nyu.edu/~sbowman/eightthings.pdf&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/moe&#34;&gt;Huggingface MOE explainer&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We compared 126 keyword modifiers with the same prompt and initial image. These are the results.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://creator.nightcafe.studio/collection/8dMYgKm1eVXG7z9pV23W&#34;&gt;https://creator.nightcafe.studio/collection/8dMYgKm1eVXG7z9pV23W&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Google released PartiPrompts as a benchmark: &lt;a href=&#34;https://parti.research.google/&#34;&gt;https://parti.research.google/&lt;/a&gt; &#34;PartiPrompts (P2) is a rich set of over 1600 prompts in English that we release as part of this work. P2 can be used to measure model capabilities across various categories and challenge aspects.&#34;&lt;/li&gt; &#xA; &lt;li&gt;Video tutorials &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Pixel art &lt;a href=&#34;https://www.youtube.com/watch?v=UvJkQPtr-8s&amp;amp;feature=youtu.be&#34;&gt;https://www.youtube.com/watch?v=UvJkQPtr-8s&amp;amp;feature=youtu.be&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;History of papers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;2008: Unified Architecture for NLP (Collobert-Weston) &lt;a href=&#34;https://twitter.com/ylecun/status/1611921657802768384&#34;&gt;https://twitter.com/ylecun/status/1611921657802768384&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;2015: &lt;a href=&#34;https://arxiv.org/abs/1511.01432&#34;&gt;Semi-supervised sequence learning&lt;/a&gt; &lt;a href=&#34;https://twitter.com/deliprao/status/1611896130589057025?s=20&#34;&gt;https://twitter.com/deliprao/status/1611896130589057025?s=20&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;2017: Transformers (Vaswani et al)&lt;/li&gt; &#xA;   &lt;li&gt;2018: GPT (Radford et al)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Misc &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;StabilityAI CIO perspective &lt;a href=&#34;https://danieljeffries.substack.com/p/the-turning-point-for-truly-open?sd=pf&#34;&gt;https://danieljeffries.substack.com/p/the-turning-point-for-truly-open?sd=pf&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/awesome-stable-diffusion/awesome-stable-diffusion&#34;&gt;https://github.com/awesome-stable-diffusion/awesome-stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/LMOps&#34;&gt;https://github.com/microsoft/LMOps&lt;/a&gt; guide to msft prompt research&lt;/li&gt; &#xA;   &lt;li&gt;gwern&#39;s behind the scenes discussion of Bing, GPT4, and the Microsoft-OpenAI relationship &lt;a href=&#34;https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned&#34;&gt;https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;other lists like this&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/rain-1/eebd5e5eb2784feecf450324e3341c8d&#34;&gt;https://gist.github.com/rain-1/eebd5e5eb2784feecf450324e3341c8d&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/underlines/awesome-marketing-datascience/raw/master/awesome-ai.md#llama-models&#34;&gt;https://github.com/underlines/awesome-marketing-datascience/blob/master/awesome-ai.md#llama-models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/imaurer/awesome-decentralized-llm&#34;&gt;https://github.com/imaurer/awesome-decentralized-llm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Communities&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Discords (see &lt;a href=&#34;https://buttondown.email/ainews&#34;&gt;https://buttondown.email/ainews&lt;/a&gt; for daily email recaps) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://discord.gg/xJJMRaWCRt&#34;&gt;Latent Space Discord&lt;/a&gt; (ours!)&lt;/li&gt; &#xA;   &lt;li&gt;General hacking and learning &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.chatgpthackers.dev/&#34;&gt;ChatGPT Hackers Discord&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://discord.com/invite/k36qjUxyJC&#34;&gt;Alignment Lab AI Discord&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/swyxio/ai-notes/main/%5Bhttps://discord.gg/T3kTZfYzs6%5D(https://t.co/D3omqAxP04)&#34;&gt;Nous Research Discord&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://discord.com/invite/vGRFMnS6c2&#34;&gt;DiscoLM Discord&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://discord.gg/3zy8kqD9Cp&#34;&gt;Karpathy Discord&lt;/a&gt; (inactive):&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://discuss.huggingface.co/t/join-the-hugging-face-discord/11263&#34;&gt;HuggingFace Discord&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://discord.gg/3Sfmpd3Njt&#34;&gt;Skunkworks AI Discord&lt;/a&gt; (new)&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/wangzjeff&#34;&gt;Jeff Wang/LLM Perf enthusiasts discord&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Art &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://discord.com/invite/stablediffusion&#34;&gt;StableDiffusion Discord&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Deforum Discord &lt;a href=&#34;https://discord.gg/upmXXsrwZc&#34;&gt;https://discord.gg/upmXXsrwZc&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Lexica Discord &lt;a href=&#34;https://discord.com/invite/bMHBjJ9wRh&#34;&gt;https://discord.com/invite/bMHBjJ9wRh&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;AI research &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;LAION discord &lt;a href=&#34;https://discord.gg/xBPBXfcFHd&#34;&gt;https://discord.gg/xBPBXfcFHd&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Eleuther discord: &lt;a href=&#34;https://www.eleuther.ai/get-involved/&#34;&gt;https://www.eleuther.ai/get-involved/&lt;/a&gt; (&lt;a href=&#34;https://blog.eleuther.ai/year-one/&#34;&gt;primer&lt;/a&gt;)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Various startups &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Perplexity Discord &lt;a href=&#34;https://discord.com/invite/kWJZsxPDuX&#34;&gt;https://discord.com/invite/kWJZsxPDuX&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Midjourney&#39;s discord &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;how to use midjourney v4 &lt;a href=&#34;https://twitter.com/fabianstelzer/status/1588856386540417024?s=20&amp;amp;t=PlgLuGAEEds9HwfegVRrpg&#34;&gt;https://twitter.com/fabianstelzer/status/1588856386540417024?s=20&amp;amp;t=PlgLuGAEEds9HwfegVRrpg&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stablehorde.net/&#34;&gt;https://stablehorde.net/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Agents &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;AutoGPT discord&lt;/li&gt; &#xA;     &lt;li&gt;BabyAGI discord&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Reddit &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://reddit.com/r/stableDiffusion&#34;&gt;https://reddit.com/r/stableDiffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/LocalLLaMA/&#34;&gt;https://www.reddit.com/r/LocalLLaMA/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/bing&#34;&gt;https://www.reddit.com/r/bing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/openai&#34;&gt;https://www.reddit.com/r/openai&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;People&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;*Unknown to many people, a growing amount of alpha is now outside of Arxiv, sources include but are not limited to: &lt;a href=&#34;https://github.com/trending&#34;&gt;https://github.com/trending&lt;/a&gt;, HN, that niche Discord server, anime profile picture anons on X, reddit *- &lt;a href=&#34;https://twitter.com/karpathy/status/1733968385472704548&#34;&gt;K&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This list will be out of date but will get you started. My live list of people to follow is at: &lt;a href=&#34;https://twitter.com/i/lists/1585430245762441216&#34;&gt;https://twitter.com/i/lists/1585430245762441216&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Researchers/Developers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/_jasonwei&#34;&gt;https://twitter.com/_jasonwei&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/johnowhitaker/status/1565710033463156739&#34;&gt;https://twitter.com/johnowhitaker/status/1565710033463156739&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/altryne/status/1564671546341425157&#34;&gt;https://twitter.com/altryne/status/1564671546341425157&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/SchmidhuberAI&#34;&gt;https://twitter.com/SchmidhuberAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/nearcyan&#34;&gt;https://twitter.com/nearcyan&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/karinanguyen&#34;&gt;https://twitter.com/karinanguyen&lt;/a&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/abhi_venigalla&#34;&gt;https://twitter.com/abhi_venigalla&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/advadnoun&#34;&gt;https://twitter.com/advadnoun&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/polynoamial&#34;&gt;https://twitter.com/polynoamial&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/vovahimself&#34;&gt;https://twitter.com/vovahimself&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/sarahookr&#34;&gt;https://twitter.com/sarahookr&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/shaneguML&#34;&gt;https://twitter.com/shaneguML&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/MaartenSap&#34;&gt;https://twitter.com/MaartenSap&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/ethanCaballero&#34;&gt;https://twitter.com/ethanCaballero&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/ShayneRedford&#34;&gt;https://twitter.com/ShayneRedford&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/seb_ruder&#34;&gt;https://twitter.com/seb_ruder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/rasbt&#34;&gt;https://twitter.com/rasbt&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/wightmanr&#34;&gt;https://twitter.com/wightmanr&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/GaryMarcus&#34;&gt;https://twitter.com/GaryMarcus&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/ylecun&#34;&gt;https://twitter.com/ylecun&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/karpathy&#34;&gt;https://twitter.com/karpathy&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/pirroh&#34;&gt;https://twitter.com/pirroh&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/eerac&#34;&gt;https://twitter.com/eerac&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/teknium&#34;&gt;https://twitter.com/teknium&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/alignment_lab&#34;&gt;https://twitter.com/alignment_lab&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/picocreator&#34;&gt;https://twitter.com/picocreator&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/charlespacker&#34;&gt;https://twitter.com/charlespacker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/ldjconfirmed&#34;&gt;https://twitter.com/ldjconfirmed&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/nisten&#34;&gt;https://twitter.com/nisten&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/far__el&#34;&gt;https://twitter.com/far__el&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/i/lists/1713824630241202630&#34;&gt;https://twitter.com/i/lists/1713824630241202630&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;News/Aggregators &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/ai__pub&#34;&gt;https://twitter.com/ai__pub&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/WeirdStableAI&#34;&gt;https://twitter.com/WeirdStableAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/multimodalart&#34;&gt;https://twitter.com/multimodalart&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/LastWeekinAI&#34;&gt;https://twitter.com/LastWeekinAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/paperswithcode&#34;&gt;https://twitter.com/paperswithcode&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/DeepLearningAI&#34;&gt;https://twitter.com/DeepLearningAI&lt;/a&gt;_&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/dl_weekly&#34;&gt;https://twitter.com/dl_weekly&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/slashML&#34;&gt;https://twitter.com/slashML&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/_akhaliq&#34;&gt;https://twitter.com/_akhaliq&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/aaditya_ai&#34;&gt;https://twitter.com/aaditya_ai&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/bentossell&#34;&gt;https://twitter.com/bentossell&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/johnvmcdonnell&#34;&gt;https://twitter.com/johnvmcdonnell&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Founders/Builders/VCs &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/levelsio&#34;&gt;https://twitter.com/levelsio&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/goodside&#34;&gt;https://twitter.com/goodside&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/c_valenzuelab&#34;&gt;https://twitter.com/c_valenzuelab&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/Raza_Habib496&#34;&gt;https://twitter.com/Raza_Habib496&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/sharifshameem/status/1562455690714775552&#34;&gt;https://twitter.com/sharifshameem/status/1562455690714775552&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/genekogan/status/1555184488606564353&#34;&gt;https://twitter.com/genekogan/status/1555184488606564353&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/levelsio/status/1566069427501764613?s=20&amp;amp;t=camPsWtMHdSSEHqWd0K7Ig&#34;&gt;https://twitter.com/levelsio/status/1566069427501764613?s=20&amp;amp;t=camPsWtMHdSSEHqWd0K7Ig&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/amanrsanger&#34;&gt;https://twitter.com/amanrsanger&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/ctjlewis&#34;&gt;https://twitter.com/ctjlewis&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/sarahcat21&#34;&gt;https://twitter.com/sarahcat21&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/jackclarkSF&#34;&gt;https://twitter.com/jackclarkSF&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/alexandr_wang&#34;&gt;https://twitter.com/alexandr_wang&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/rameerez&#34;&gt;https://twitter.com/rameerez&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/scottastevenson&#34;&gt;https://twitter.com/scottastevenson&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/denisyarats&#34;&gt;https://twitter.com/denisyarats&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Stability &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/StabilityAI&#34;&gt;https://twitter.com/StabilityAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/StableDiffusion&#34;&gt;https://twitter.com/StableDiffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/hardmaru&#34;&gt;https://twitter.com/hardmaru&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/JJitsev&#34;&gt;https://twitter.com/JJitsev&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;OpenAI &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/sama&#34;&gt;https://twitter.com/sama&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/ilyasut&#34;&gt;https://twitter.com/ilyasut&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/miramurati&#34;&gt;https://twitter.com/miramurati&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;HuggingFace &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/younesbelkada&#34;&gt;https://twitter.com/younesbelkada&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Artists &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/karenxcheng/status/1564626773001719813&#34;&gt;https://twitter.com/karenxcheng/status/1564626773001719813&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/TomLikesRobots&#34;&gt;https://twitter.com/TomLikesRobots&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Other &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Companies &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/AnthropicAI&#34;&gt;https://twitter.com/AnthropicAI&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/AssemblyAI&#34;&gt;https://twitter.com/AssemblyAI&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/CohereAI&#34;&gt;https://twitter.com/CohereAI&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/MosaicML&#34;&gt;https://twitter.com/MosaicML&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/MetaAI&#34;&gt;https://twitter.com/MetaAI&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/DeepMind&#34;&gt;https://twitter.com/DeepMind&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/HelloPaperspace&#34;&gt;https://twitter.com/HelloPaperspace&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Bots and Apps &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/dreamtweetapp&#34;&gt;https://twitter.com/dreamtweetapp&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/aiarteveryhour&#34;&gt;https://twitter.com/aiarteveryhour&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quotes, Reality &amp;amp; Demotivation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Narrow, tedium domain usecases &lt;a href=&#34;https://twitter.com/WillManidis/status/1584900092615528448&#34;&gt;https://twitter.com/WillManidis/status/1584900092615528448&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/WillManidis/status/1584900100480192516&#34;&gt;https://twitter.com/WillManidis/status/1584900100480192516&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;antihype &lt;a href=&#34;https://twitter.com/alexandr_wang/status/1573302977418387457&#34;&gt;https://twitter.com/alexandr_wang/status/1573302977418387457&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;antihype &lt;a href=&#34;https://twitter.com/fchollet/status/1612142423425138688?s=46&amp;amp;t=pLCNW9pF-co4bn08QQVaUg&#34;&gt;https://twitter.com/fchollet/status/1612142423425138688?s=46&amp;amp;t=pLCNW9pF-co4bn08QQVaUg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;prompt eng memes &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/_jasonwei/status/1516844920367054848&#34;&gt;https://twitter.com/_jasonwei/status/1516844920367054848&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;things stablediffusion struggles with &lt;a href=&#34;https://opguides.info/posts/aiartpanic/&#34;&gt;https://opguides.info/posts/aiartpanic/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;New Google &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/alexandr_wang/status/1585022891594510336&#34;&gt;https://twitter.com/alexandr_wang/status/1585022891594510336&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;New Powerpoint&lt;/li&gt; &#xA; &lt;li&gt;via emad&lt;/li&gt; &#xA; &lt;li&gt;Appending prompts by default in UI&lt;/li&gt; &#xA; &lt;li&gt;DALLE: &lt;a href=&#34;https://twitter.com/levelsio/status/1588588688115912705?s=20&amp;amp;t=0ojpGmH9k6MiEDyVG2I6gg&#34;&gt;https://twitter.com/levelsio/status/1588588688115912705?s=20&amp;amp;t=0ojpGmH9k6MiEDyVG2I6gg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;There have been two previous winters, one 1974-1980 and one 1987-1993. &lt;a href=&#34;https://www.erichgrunewald.com/posts/the-prospect-of-an-ai-winter/&#34;&gt;https://www.erichgrunewald.com/posts/the-prospect-of-an-ai-winter/&lt;/a&gt;. bit more commentary &lt;a href=&#34;https://news.ycombinator.com/item?id=37474528&#34;&gt;here&lt;/a&gt;. related - &lt;a href=&#34;https://www.sequoiacap.com/article/ai-paradox-perspective/&#34;&gt;AI Effect&lt;/a&gt; - &#34;once it works its not AI&#34;&lt;/li&gt; &#xA; &lt;li&gt;It&#39;s just matrix multiplication/stochastic parrots &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Even LLM skeptic Yann LeCun says LLMs have some level of understanding: &lt;a href=&#34;https://twitter.com/ylecun/status/1667947166764023808&#34;&gt;https://twitter.com/ylecun/status/1667947166764023808&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Gary Marcus’ “Deep Learning is Hitting a Wall” &lt;a href=&#34;https://nautil.us/deep-learning-is-hitting-a-wall-238440/&#34;&gt;https://nautil.us/deep-learning-is-hitting-a-wall-238440/&lt;/a&gt; pushed symbolic systems&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Legal, Ethics, and Privacy&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NSFW filter &lt;a href=&#34;https://vickiboykis.com/2022/11/18/some-notes-on-the-stable-diffusion-safety-filter/&#34;&gt;https://vickiboykis.com/2022/11/18/some-notes-on-the-stable-diffusion-safety-filter/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;On &#34;AI Art Panic&#34; &lt;a href=&#34;https://opguides.info/posts/aiartpanic/&#34;&gt;https://opguides.info/posts/aiartpanic/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://old.reddit.com/r/blender/comments/121lhfq/i_lost_everything_that_made_me_love_my_job/&#34;&gt;I lost everything that made me love my job through Midjourney&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Yannick influencing OPENRAIL-M &lt;a href=&#34;https://www.youtube.com/watch?v=W5M-dvzpzSQ&#34;&gt;https://www.youtube.com/watch?v=W5M-dvzpzSQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;art schools accepting AI art &lt;a href=&#34;https://twitter.com/DaveRogenmoser/status/1597746558145265664&#34;&gt;https://twitter.com/DaveRogenmoser/status/1597746558145265664&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DRM issues &lt;a href=&#34;https://undeleted.ronsor.com/voice.ai-gpl-violations-with-a-side-of-drm/&#34;&gt;https://undeleted.ronsor.com/voice.ai-gpl-violations-with-a-side-of-drm/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;stealing art &lt;a href=&#34;https://stablediffusionlitigation.com/&#34;&gt;https://stablediffusionlitigation.com&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;http://www.stablediffusionfrivolous.com/&#34;&gt;http://www.stablediffusionfrivolous.com/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;stable attribution &lt;a href=&#34;https://news.ycombinator.com/item?id=34670136&#34;&gt;https://news.ycombinator.com/item?id=34670136&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;coutner argument for disney &lt;a href=&#34;https://twitter.com/jonst0kes/status/1616219435492163584?s=46&amp;amp;t=HqQqDH1yEwhWUSQxYTmF8w&#34;&gt;https://twitter.com/jonst0kes/status/1616219435492163584?s=46&amp;amp;t=HqQqDH1yEwhWUSQxYTmF8w&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;research on stable diffusion copying &lt;a href=&#34;https://twitter.com/officialzhvng/status/1620535905298817024?s=20&amp;amp;t=NC-nW7pfDa8nyRD08Lx1Nw&#34;&gt;https://twitter.com/officialzhvng/status/1620535905298817024?s=20&amp;amp;t=NC-nW7pfDa8nyRD08Lx1Nw&lt;/a&gt; This paper used Stable Diffusion to generate 175 million images over 350,000 prompts and only found 109 near copies of training data. Am I right that my main takeaway from this is how good Stable Diffusion is at &lt;em&gt;not&lt;/em&gt; memorizing training examples?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;scraping content &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://blog.ericgoldman.org/archives/2023/08/web-scraping-for-me-but-not-for-thee-guest-blog-post.htm&#34;&gt;https://blog.ericgoldman.org/archives/2023/08/web-scraping-for-me-but-not-for-thee-guest-blog-post.htm&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;sarah silverman case - openai response &lt;a href=&#34;https://arstechnica.com/tech-policy/2023/08/openai-disputes-authors-claims-that-every-chatgpt-response-is-a-derivative-work/&#34;&gt;https://arstechnica.com/tech-policy/2023/08/openai-disputes-authors-claims-that-every-chatgpt-response-is-a-derivative-work/&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Licensing &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://opencoreventures.com/blog/2023-06-27-ai-weights-are-not-open-source/&#34;&gt;AI weights are not open &#34;source&#34; - Sid Sijbrandij&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Diversity and Equity &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;sexualizing minorities &lt;a href=&#34;https://twitter.com/lanadenina/status/1680238883206832129&#34;&gt;https://twitter.com/lanadenina/status/1680238883206832129&lt;/a&gt; the reason is &lt;a href=&#34;https://twitter.com/levelsio/status/1680665706235404288&#34;&gt;porn is good at bodies&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/rzhang88/status/1549472829304741888?s=20&#34;&gt;OpenAI tacking on &#34;black&#34; randomly to make DallE diverse&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Privacy - confidential computing &lt;a href=&#34;https://www.edgeless.systems/blog/how-confidential-computing-and-ai-fit-together/&#34;&gt;https://www.edgeless.systems/blog/how-confidential-computing-and-ai-fit-together/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Alignment, Safety&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Anthropic - &lt;a href=&#34;https://arxiv.org/pdf/2112.00861.pdf&#34;&gt;https://arxiv.org/pdf/2112.00861.pdf&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Helpful: attempt to do what is ask. concise, efficient. ask followups. redirect bad questions.&lt;/li&gt; &#xA;   &lt;li&gt;Honest: give accurate information, express uncertainty. don&#39;t imitate responses expected from an expert if it doesn&#39;t have the capabilities/knowledge&lt;/li&gt; &#xA;   &lt;li&gt;Harmless: not offensive/discriminatory. refuse to assist dangerous acts. recognize when providing sensitive/consequential advice&lt;/li&gt; &#xA;   &lt;li&gt;criticism and boundaries as future direction &lt;a href=&#34;https://twitter.com/davidad/status/1628489924235206657?s=46&amp;amp;t=TPVwcoqO8qkc7MuaWiNcnw&#34;&gt;https://twitter.com/davidad/status/1628489924235206657?s=46&amp;amp;t=TPVwcoqO8qkc7MuaWiNcnw&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Just Eliezer entire body of work &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/esyudkowsky/status/1625922986590212096&#34;&gt;https://twitter.com/esyudkowsky/status/1625922986590212096&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;agi list of lethalities &lt;a href=&#34;https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities&#34;&gt;https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;note that eliezer has made controversial comments &lt;a href=&#34;https://twitter.com/johnnysands42/status/1641349759754485760?s=46&amp;amp;t=90xQ8sGy63D2OtiaoGJuww&#34;&gt;in the past&lt;/a&gt; and also in &lt;a href=&#34;https://twitter.com/lorakolodny/status/1641448759086415875?s=46&amp;amp;t=90xQ8sGy63D2OtiaoGJuww&#34;&gt;recent times&lt;/a&gt; (&lt;a href=&#34;https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/&#34;&gt;TIME article&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Connor Leahy may be a more sane/measured/technically competent version of yud &lt;a href=&#34;https://overcast.fm/+aYlOEqTJ0&#34;&gt;https://overcast.fm/+aYlOEqTJ0&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;it&#39;s not just paperclip factories&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like&#34;&gt;https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;the 6 month pause letter &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://futureoflife.org/open-letter/pause-giant-ai-experiments/&#34;&gt;https://futureoflife.org/open-letter/pause-giant-ai-experiments/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;yann lecun vs andrew ng &lt;a href=&#34;https://www.youtube.com/watch?v=BY9KV8uCtj4&#34;&gt;https://www.youtube.com/watch?v=BY9KV8uCtj4&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://scottaaronson.blog/?p=7174&#34;&gt;https://scottaaronson.blog/?p=7174&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/emilymbender/status/1640920936600997889&#34;&gt;emily bender response&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=35771104&#34;&gt;Geoffrey Hinton leaving Google&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;followed up by one sentence public letter &lt;a href=&#34;https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html&#34;&gt;https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;xrisk - Is avoiding extinction from AI really an urgent priority? (&lt;a href=&#34;https://link.mail.beehiiv.com/ss/c/5J8WPrGlKFK1BUsRYoWIfdCHPD-3Xbi8FugDN8_LxoMLoHhMJlEG7wG6Qm_xTk5kjhv7y5vwidMdRiSXu8XoBiq8nEOR34GaAFwHPM3qm-KgbLw6_hl3AQd9rRxt7mbTHvXRNeF6hfODzGg5z4t8D3ZdIldVTpoAGQ-KmKNEnmzBudTJIJtP1kjZLr1QqJYX/3wo/z-oFlqV_RUGtJd6OO2FogA/h13/XrV7_YgyheO615JC1X8VasmPENc7KRnJrp03iAlmoXw&#34;&gt;link&lt;/a&gt;)&amp;nbsp; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;AI Is not an arms race. (&lt;a href=&#34;https://link.mail.beehiiv.com/ss/c/znicDlvJFyGBhcMAVWxZFpwlt5VC0YnUsV4gzm_4ut3qiUuoiY9_n0aSS6Uv0inD2_kx5JhKOVXSRbXMrV7VwL_fuIMlfwAiTSTTCxo56Xv58IWHdUClCfyt4alUnKRf2MV5a7rIM0KG4vwVLObEua0i3t5UIvPlbHybyFluj52xGYswNiQUMZl2OrDzh1u4oLAvnCVkTUi5vCX0i6-N8A/3wo/z-oFlqV_RUGtJd6OO2FogA/h14/K2LmS7FyAGW-u4j6oHnp_bKapwqFG_Gb4MC5XPpKJsM&#34;&gt;link&lt;/a&gt;)&amp;nbsp;&lt;/li&gt; &#xA;   &lt;li&gt;If we’re going to label AI an ‘extinction risk,’ we need to clarify how it could happen. (&lt;a href=&#34;https://link.mail.beehiiv.com/ss/c/znicDlvJFyGBhcMAVWxZFsLJphRoW5fZiwv4ALj3pNMBRHKVGkJIME1sXnwK-P46O3jH_jtoC_wqyCeroi2bRUKEUKd_QQvXSoMgu3Nqbw99wsPjSDl_Lt6RSk7bni0KT4c1-gstNpWdPoUbj3air5NbOAbvtp5P9ds1xCm4qG-6dvoJELH0HHB7G9FO2ZFlXPTm37nswLD77q6opSiWnrTEHhHsCo37yO01bFol4LeaSr8F4e_WynvF0QrKLNaSKf0rDpyMSn__lxmbRl6M1A/3wo/z-oFlqV_RUGtJd6OO2FogA/h15/SYpE89X1W3Z_qSjH8YJmhLYYRRgjUHJzn2WILhBIcxw&#34;&gt;link&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;OpenAI superalignment &lt;a href=&#34;https://www.youtube.com/watch?v=ZP_N4q5U3eE&#34;&gt;https://www.youtube.com/watch?v=ZP_N4q5U3eE&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;regulation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;chinese regulation &lt;a href=&#34;https://www.chinalawtranslate.com/en/overview-of-draft-measures-on-generative-ai/&#34;&gt;https://www.chinalawtranslate.com/en/overview-of-draft-measures-on-generative-ai/&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/mmitchell_ai/status/1647697067006111745?s=46&amp;amp;t=90xQ8sGy63D2OtiaoGJuww&#34;&gt;https://twitter.com/mmitchell_ai/status/1647697067006111745?s=46&amp;amp;t=90xQ8sGy63D2OtiaoGJuww&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;China is the only major world power that explicitly&amp;nbsp;&lt;a href=&#34;https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVt-xv1bfQFxVTDC381T3c1vVLsY3L4_gp1wN1FQ0th3q3n_V1-WJV7CgH8lW4wLFDD1Q5sD1W6QG0gj2gQKZ5W2WNS9Z5gKTB8W6jF2Dc8ltmWfW1kwRcc4LNmnNW2_F-zw6rWXtDN8M32V9_0Z1cN1gwSlkLF9WBW6yYMS68JLJYjN1wstfhr0tvgW5DCclJ4zMFhNN6tQ4vt1P5bVW5w-L-275lv9LW5zhjMk7CCjjcW20ChgZ57-8l2W50dQgR1_tfL-VqXDdY2t227nVzlNDX4m43yWW4D6GXl6Mf9JvW3qShZ085BMXqW5S2j7D4VWf5lW4c37Wn4lbf-NW4W6Hxl3CCDHRW451x4X8wNPKHW5zc90X90FjXcW97Qn_B7RdzpP3nQX1&#34;&gt;regulates&lt;/a&gt;&amp;nbsp;generative AI&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;italy banning chatgpt&lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;At its annual meeting in Japan, the Group of Seven (G7), an informal bloc of industrialized democratic governments,&amp;nbsp;&lt;a href=&#34;https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVt-xv1bfQFxVTDC381T3c1vVLsY3L4_gp1wN1FQ0s_3q3nJV1-WJV7CgFv5W51g32V2hBgR-N3j2W3szNMJlW80w4Xv5Gg2S8N4_ZHQFYd4cRW8yvm4F2zg5qpW5xfrS61fJ8H4W49Nj5Y2zWcRbW97ym606Vq3X6W2-51W529GnLcW2zlMRl3qKmBCW8jd69B7nRzmFV5K0lP4FzrchW6nxHbj1vFJPqN3sbnlvFM2WhW6PNj-t5YfVS3W6pl7681yBKGxN1R1Mbj8wWj4W22BS_g1BH_1yW7pT8c47QKBQFW64WfHc80PxjRV6dQN42mCqRMW3yJrxC3DX4_5W5yqFbL34kwc0W770qZv2fjyv03bJQ1&#34;&gt;announced&lt;/a&gt;&amp;nbsp;the Hiroshima Process, an intergovernmental task force empowered to investigate risks of generative AI. G7 members, which include Canada, France, Germany, Italy, Japan, the United Kingdom, and the United States, vowed to craft mutually compatible laws and regulate AI according to democratic values. These include fairness, accountability, transparency, safety, data privacy, protection from abuse, and respect for human rights.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;U.S. President Joe Biden&amp;nbsp;&lt;a href=&#34;https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVt-xv1bfQFxVTDC381T3c1vVLsY3L4_gp1wN1FQ0s55nCT_V3Zsc37CgQX9W7wTfL38m-2KKW3mGNtx8sgMgJW10rjg65dMw5qN3jtZLMqRgQbV_3DXH2yr2HbW4vs2Tm43thGvW6fK8f72N6w37N53TdBst-8D1W6yzHrb70MHkTW1ckbRd5NfDP9W2j6yWK34KFvtW18lscs3lQ0G6W4GFgyx486-vdW5NJBQv4tvxYpW36FqGc4md2XfW2Fgj6n2fd-BSW3PyPVH9bD8W3N61PDTSyzVy1W2QSSm07tHjwWW8zG-Kl3TPwmfVMNjLb7Nnhk4W2B_zlf7n91mNW806djL3zxyMFW5RpR1Q9kcL0yW7ss_7m92D7Z-W4fWJYk3xBb3yN5bZbNkSvb14N2kgsftyLf7cN1WmZDl5Sw63W4FcWFn65g7DsVzPJZP2qtH36W3vfw782XRtSbW834rhB5jGZ7RW6K9z1d87ns4N38SY1&#34;&gt;issued&lt;/a&gt;&amp;nbsp;a strategic plan for AI. The initiative calls on U.S. regulatory agencies to develop public datasets, benchmarks, and standards for training, measuring, and evaluating AI systems.&lt;/li&gt; &#xA; &lt;li&gt;Earlier this month, France’s data privacy regulator&amp;nbsp;&lt;a href=&#34;https://info.deeplearning.ai/e3t/Ctc/LX+113/cJhC404/VVt-xv1bfQFxVTDC381T3c1vVLsY3L4_gp1wN1FQ0s_3q3nJV1-WJV7CgTpxW8C6yq247bfj8W4mQv0-4hl35_W8SPtZ52JXPlxW1Fkb5p54f30RW6sj0m71XsJ4yF7-b6kBx5vTW7cwGKJ6RcqpFW5325sQ2R54VbW79rbsP4wh6MyW2MwyS_6CSJfwW8VBz1y1M5_4nW2nhxPD5vZw17MCVDrTvH8ljW1JYH0t8DPm23W3BPQvW69f5TFW5ms3_413vDbJVw9GyW1yMYBfW6zpGVw12swbdV_wmsh11rtb0Vlzk0b6ZkhpZW1XWkdG7yNYpsW38p95C5jXCx7W4qrc4w1_q_sdW5RD3Jv7bdxpv2Gp1&#34;&gt;announced&lt;/a&gt;&amp;nbsp;a framework for regulating generative AI.&lt;/li&gt; &#xA; &lt;li&gt;regulation vs Xrisk &lt;a href=&#34;https://1a3orn.com/sub/essays-regulation-stories.html&#34;&gt;https://1a3orn.com/sub/essays-regulation-stories.html&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://news.ycombinator.com/item?id=37877605&#34;&gt;Multimodal Prompt Injection in GPT4V&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Misc&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Whisper &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/sensahin/YouWhisper&#34;&gt;https://huggingface.co/spaces/sensahin/YouWhisper&lt;/a&gt; YouWhisper converts Youtube videos to text using openai/whisper.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://twitter.com/jeffistyping/status/1573145140205846528&#34;&gt;https://twitter.com/jeffistyping/status/1573145140205846528&lt;/a&gt; youtube whipserer&lt;/li&gt; &#xA;   &lt;li&gt;multilingual subtitles &lt;a href=&#34;https://twitter.com/1littlecoder/status/1573030143848722433&#34;&gt;https://twitter.com/1littlecoder/status/1573030143848722433&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;video subtitles &lt;a href=&#34;https://twitter.com/m1guelpf/status/1574929980207034375&#34;&gt;https://twitter.com/m1guelpf/status/1574929980207034375&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;you can join whisper to stable diffusion for reasons &lt;a href=&#34;https://twitter.com/fffiloni/status/1573733520765247488/photo/1&#34;&gt;https://twitter.com/fffiloni/status/1573733520765247488/photo/1&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;known problems &lt;a href=&#34;https://twitter.com/lunixbochs/status/1574848899897884672&#34;&gt;https://twitter.com/lunixbochs/status/1574848899897884672&lt;/a&gt; (edge case with catastrophic failures)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;textually guided audio &lt;a href=&#34;https://twitter.com/FelixKreuk/status/1575846953333579776&#34;&gt;https://twitter.com/FelixKreuk/status/1575846953333579776&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Codegen &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;CodegeeX &lt;a href=&#34;https://twitter.com/thukeg/status/1572218413694726144&#34;&gt;https://twitter.com/thukeg/status/1572218413694726144&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/salesforce/CodeGen&#34;&gt;https://github.com/salesforce/CodeGen&lt;/a&gt; &lt;a href=&#34;https://joel.tools/codegen/&#34;&gt;https://joel.tools/codegen/&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;pdf to structured data - Impira used t to do it (dead link: &lt;a href=&#34;https://www.impira.com/blog/hey-machine-whats-my-invoice-total&#34;&gt;https://www.impira.com/blog/hey-machine-whats-my-invoice-total&lt;/a&gt;) but if you look hard enough on twitter there are some alternatives&lt;/li&gt; &#xA; &lt;li&gt;text to Human Motion diffusion &lt;a href=&#34;https://twitter.com/GuyTvt/status/1577947409551851520&#34;&gt;https://twitter.com/GuyTvt/status/1577947409551851520&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;abs: &lt;a href=&#34;https://arxiv.org/abs/2209.14916&#34;&gt;https://arxiv.org/abs/2209.14916&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;project page: &lt;a href=&#34;https://guytevet.github.io/mdm-page/&#34;&gt;https://guytevet.github.io/mdm-page/&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Corgea/retriever</title>
    <updated>2024-01-27T01:31:26Z</updated>
    <id>tag:github.com,2024-01-27:/Corgea/retriever</id>
    <link href="https://github.com/Corgea/retriever" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Secure secret sharing through the browser using web crypto. No server required!&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Corgea/retriever/main/img/logo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Retriever&lt;/h1&gt; &#xA;&lt;p&gt;Secure secret sharing through the browser using web crypto. No server required!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://retriever.corgea.io&#34;&gt;Try it here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://retriever.corgea.io/why.html&#34;&gt;Why did we build this?&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;100% client-side&lt;/li&gt; &#xA; &lt;li&gt;Uses standard browser web crypto APIs&lt;/li&gt; &#xA; &lt;li&gt;Links are secure and won&#39;t decrypt the secret&lt;/li&gt; &#xA; &lt;li&gt;Your secrets and the private keys that encrypt them are never sent to a server by Retriever&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Corgea/retriever/raw/main/img/encryption_flow.png?raw=true&#34; alt=&#34;How retriever works&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for larger secrets&lt;/li&gt; &#xA; &lt;li&gt;File sharing&lt;/li&gt; &#xA; &lt;li&gt;Bi-directional sharing&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Analytics disclosure&lt;/h2&gt; &#xA;&lt;p&gt;Retriever does use Mixpanel to help the Corgea team know if it&#39;s getting traffic. We do not transmit any of your secrets and private keys to Mixpanel. It is only used if you use &lt;a href=&#34;https://retriever.corgea.io/&#34;&gt;https://retriever.corgea.io/&lt;/a&gt;. If you run this locally it will not send any analytics to Mixpanel and you can choose to remove it.&lt;/p&gt;</summary>
  </entry>
</feed>