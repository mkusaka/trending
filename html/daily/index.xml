<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-16T01:31:34Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lm-rebooter/NuggetsBooklet</title>
    <updated>2025-02-16T01:31:34Z</updated>
    <id>tag:github.com,2025-02-16:/lm-rebooter/NuggetsBooklet</id>
    <link href="https://github.com/lm-rebooter/NuggetsBooklet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;掘金小册&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;juejin-download&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;掘金小册下载&lt;/p&gt; &#xA;&lt;h2&gt;使用方法&lt;/h2&gt; &#xA;&lt;h3&gt;安装依赖&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;获取 Cookie&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;安装 &lt;a href=&#34;https://chrome.google.com/webstore/detail/cookie-editor/hlkenndednhfkekhgcdicdfddnkalmdm&#34;&gt;Chrome&lt;/a&gt; 或 &lt;a href=&#34;https://addons.mozilla.org/en-US/firefox/addon/cookie-editor/&#34;&gt;Firefox&lt;/a&gt; 的 cookie editor 扩展&lt;/li&gt; &#xA; &lt;li&gt;打开 &lt;a href=&#34;https://juejin.cn&#34;&gt;掘金&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;打开扩展程序&lt;/li&gt; &#xA; &lt;li&gt;点击右下角的 &#34;Export&#34; -&amp;gt; &#34;Export as JSON&#34; (保存到剪贴板)&lt;/li&gt; &#xA; &lt;li&gt;把你剪贴板上的内容粘贴到 &lt;code&gt;cookies.json&lt;/code&gt; 文件中&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;执行脚本&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;node main.js&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;声明&lt;/h2&gt; &#xA;&lt;p&gt;本项目只做个人学习研究之用，不得用于商业用途！&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mlfoundations/evalchemy</title>
    <updated>2025-02-16T01:31:34Z</updated>
    <id>tag:github.com,2025-02-16:/mlfoundations/evalchemy</id>
    <link href="https://github.com/mlfoundations/evalchemy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automatic Evals for LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🧪 Evalchemy&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A unified and easy-to-use toolkit for evaluating post-trained language models&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mlfoundations/evalchemy/raw/main/image.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Evalchemy is developed by the &lt;a href=&#34;https://datacomp.ai&#34;&gt;DataComp community&lt;/a&gt; and &lt;a href=&#34;https://bespokelabs.ai&#34;&gt;Bespoke Labs&lt;/a&gt; and builds on the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;LM-Eval-Harness&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🎉 What&#39;s New&lt;/h2&gt; &#xA;&lt;h4&gt;[2025.01.30] API Model Support&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bespokelabsai/curator/&#34;&gt;API models via Curator&lt;/a&gt;: With &lt;code&gt;--model curator&lt;/code&gt; you can now evaluate with even more API based models via &lt;a href=&#34;https://github.com/bespokelabsai/curator/&#34;&gt;Curator&lt;/a&gt;, including all those supported by &lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;LiteLLM&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;  python -m eval.eval \&#xA;        --model curator  \&#xA;        --tasks AIME24,MATH500,GPQA-Diamond \&#xA;        --model_name &#34;gemini/gemini-2.0-flash-thinking-exp-01-21&#34; \&#xA;        --apply_chat_template False \&#xA;        --model_args &#39;tokenized_requests=False&#39; \&#xA;        --output_path logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;[2025.01.29] New Reasoning Benchmarks&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AIME24, AMC23, MATH500, LiveCodeBench, GPQA-Diamond, HumanEvalPlus, MBPPPlus, BigCodeBench, MultiPL-E, and CRUXEval have been added to our growing list of &lt;a href=&#34;https://github.com/mlfoundations/evalchemy?tab=readme-ov-file#built-in-benchmarks&#34;&gt;available benchmarks&lt;/a&gt;. This is part of the effort in the &lt;a href=&#34;https://github.com/open-thoughts/open-thoughts&#34;&gt;Open Thoughts&lt;/a&gt; project. See the &lt;a href=&#34;https://www.open-thoughts.ai/blog/measure&#34;&gt;our blog post&lt;/a&gt; on using Evalchemy for measuring reasoning models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;[2025.01.28] New Model Support&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.vllm.ai/2023/06/20/vllm.html&#34;&gt;vLLM models&lt;/a&gt;: High-performance inference and serving engine with PagedAttention technology&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eval.eval \&#xA;    --model vllm \&#xA;    --tasks alpaca_eval \&#xA;    --model_args &#34;pretrained=meta-llama/Meta-Llama-3-8B-Instruct&#34; \&#xA;    --batch_size 16 \&#xA;    --output_path logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI models&lt;/a&gt;: Full support for OpenAI&#39;s model lineup&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eval.eval \&#xA;    --model openai-chat-completions \&#xA;    --tasks alpaca_eval \&#xA;    --model_args &#34;model=gpt-4o-mini-2024-07-18,num_concurrent=32&#34; \&#xA;    --batch_size 16 \&#xA;    --output_path logs &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Key Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unified Installation&lt;/strong&gt;: One-step setup for all benchmarks, eliminating dependency conflicts&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parallel Evaluation&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Data-Parallel: Distribute evaluations across multiple GPUs for faster results&lt;/li&gt; &#xA;   &lt;li&gt;Model-Parallel: Handle large models that don&#39;t fit on a single GPU&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simplified Usage&lt;/strong&gt;: Run any benchmark with a consistent command-line interface&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Results Management&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Local results tracking with standardized output format&lt;/li&gt; &#xA;   &lt;li&gt;Optional database integration for systematic tracking&lt;/li&gt; &#xA;   &lt;li&gt;Leaderboard submission capability (requires database setup)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;⚡ Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;We suggest using conda (&lt;a href=&#34;https://docs.anaconda.com/miniconda/install/#quick-command-line-install&#34;&gt;installation instructions&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create and activate conda environment&#xA;conda create --name evalchemy python=3.10&#xA;conda activate evalchemy&#xA;&#xA;# Clone the repo&#xA;git clone git@github.com:mlfoundations/evalchemy.git   &#xA;cd evalchemy&#xA;&#xA;# Install dependencies&#xA;pip install -e &#34;.[eval]&#34;&#xA;pip install -e eval/chat_benchmarks/alpaca_eval&#xA;&#xA;# Log into HuggingFace for datasets and models.&#xA;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📚 Available Tasks&lt;/h2&gt; &#xA;&lt;h3&gt;Built-in Benchmarks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;All tasks from &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;LM Evaluation Harness&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Custom instruction-based tasks (found in &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/evalchemy/main/eval/chat_benchmarks/&#34;&gt;&lt;code&gt;eval/chat_benchmarks/&lt;/code&gt;&lt;/a&gt;): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;MTBench&lt;/strong&gt;: &lt;a href=&#34;https://github.com/mtbench101/mt-bench-101&#34;&gt;Multi-turn dialogue evaluation benchmark&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;WildBench&lt;/strong&gt;: &lt;a href=&#34;https://github.com/allenai/WildBench&#34;&gt;Real-world task evaluation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;RepoBench&lt;/strong&gt;: &lt;a href=&#34;https://github.com/Leolty/repobench&#34;&gt;Code understanding and repository-level tasks&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;MixEval&lt;/strong&gt;: &lt;a href=&#34;https://github.com/Psycoy/MixEval&#34;&gt;Comprehensive evaluation across domains&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;IFEval&lt;/strong&gt;: &lt;a href=&#34;https://github.com/google-research/google-research/tree/master/instruction_following_eval&#34;&gt;Instruction following capability evaluation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;AlpacaEval&lt;/strong&gt;: &lt;a href=&#34;https://github.com/tatsu-lab/alpaca_eval&#34;&gt;Instruction following evaluation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;HumanEval&lt;/strong&gt;: &lt;a href=&#34;https://github.com/openai/human-eval&#34;&gt;Code generation and problem solving&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;HumanEvalPlus&lt;/strong&gt;: &lt;a href=&#34;https://github.com/evalplus/evalplus&#34;&gt;HumanEval with more test cases&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;ZeroEval&lt;/strong&gt;: &lt;a href=&#34;https://github.com/WildEval/ZeroEval&#34;&gt;Logical reasoning and problem solving&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;MBPP&lt;/strong&gt;: &lt;a href=&#34;https://github.com/google-research/google-research/tree/master/mbpp&#34;&gt;Python programming benchmark&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;MBPPPlus&lt;/strong&gt;: &lt;a href=&#34;https://github.com/evalplus/evalplus&#34;&gt;MBPP with more test cases&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;BigCodeBench:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2406.15877&#34;&gt;Benchmarking Code Generation with Diverse Function Calls and Complex Instructions&lt;/a&gt;&lt;/p&gt; &#xA;    &lt;blockquote&gt; &#xA;     &lt;p&gt;&lt;strong&gt;🚨 Warning:&lt;/strong&gt; for BigCodeBench evaluation, we strongly recommend using a Docker container since the execution of LLM generated code on a machine can lead to destructive outcomes. More info is &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/evalchemy/main/eval/chat_benchmarks/BigCodeBench/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;    &lt;/blockquote&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;MultiPL-E:&lt;/strong&gt; &lt;a href=&#34;https://github.com/nuprl/MultiPL-E/&#34;&gt;Multi-Programming Language Evaluation of Large Language Models of Code&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;CRUXEval:&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.03065&#34;&gt;Code Reasoning, Understanding, and Execution Evaluation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;AIME24&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/datasets/AI-MO/aimo-validation-aime&#34;&gt;Math Reasoning Dataset&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;AMC23&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/datasets/AI-MO/aimo-validation-amc&#34;&gt;Math Reasoning Dataset&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;MATH500&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/MATH-500&#34;&gt;Math Reasoning Dataset&lt;/a&gt; split from &lt;a href=&#34;https://github.com/openai/prm800k/tree/main?tab=readme-ov-file#math-splits&#34;&gt;Let&#39;s Verify Step by Step&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;LiveCodeBench&lt;/strong&gt;: &lt;a href=&#34;https://livecodebench.github.io/&#34;&gt;Benchmark of LLMs for code&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;LiveBench&lt;/strong&gt;: &lt;a href=&#34;https://livebench.ai/#/&#34;&gt;A benchmark for LLMs designed with test set contamination and objective evaluation in mind&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;GPQA Diamond&lt;/strong&gt;: &lt;a href=&#34;https://huggingface.co/datasets/Idavidrein/gpqa&#34;&gt;A Graduate-Level Google-Proof Q&amp;amp;A Benchmark&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Arena-Hard-Auto&lt;/strong&gt; (Coming soon): &lt;a href=&#34;https://github.com/lmarena/arena-hard-auto&#34;&gt;Automatic evaluation tool for instruction-tuned LLMs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;SWE-Bench&lt;/strong&gt; (Coming soon): &lt;a href=&#34;https://github.com/princeton-nlp/SWE-bench&#34;&gt;Evaluating large language models on real-world software issues&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;SafetyBench&lt;/strong&gt; (Coming soon): &lt;a href=&#34;https://github.com/thu-coai/SafetyBench&#34;&gt;Evaluating the safety of LLMs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;SciCode Bench&lt;/strong&gt; (Coming soon): &lt;a href=&#34;https://github.com/scicode-bench/SciCode&#34;&gt;Evaluate language models in generating code for solving realistic scientific research problems&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Berkeley Function Calling Leaderboard&lt;/strong&gt; (Coming soon): &lt;a href=&#34;https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html&#34;&gt;Evaluating ability of LLMs to use APIs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We have recorded reproduced results against published numbers for these benchmarks in &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/evalchemy/main/reproduced_benchmarks.md&#34;&gt;&lt;code&gt;reproduced_benchmarks.md&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Basic Usage&lt;/h3&gt; &#xA;&lt;p&gt;Make sure your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; is set in your environment before running evaluations.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eval.eval \&#xA;    --model hf \&#xA;    --tasks HumanEval,mmlu \&#xA;    --model_args &#34;pretrained=mistralai/Mistral-7B-Instruct-v0.3&#34; \&#xA;    --batch_size 2 \&#xA;    --output_path logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results will be written out in &lt;code&gt;output_path&lt;/code&gt;. If you have &lt;code&gt;jq&lt;/code&gt; &lt;a href=&#34;https://jqlang.github.io/jq/download/&#34;&gt;installed&lt;/a&gt;, you can view the results easily after evaluation. Example: &lt;code&gt;jq &#39;.results&#39; logs/Qwen__Qwen2.5-7B-Instruct/results_2024-11-17T17-12-28.668908.json&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Args&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model&lt;/code&gt;: Which model type or provider is evaluated (example: hf)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--tasks&lt;/code&gt;: Comma-separated list of tasks to be evaluated.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--model_args&lt;/code&gt;: Model path and parameters. Comma-separated list of parameters passed to the model constructor. Accepts a string of the format &lt;code&gt;&#34;arg1=val1,arg2=val2,...&#34;&lt;/code&gt;. You can find the list supported arguments &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness/raw/365fcda9b85bbb6e0572d91976b8daf409164500/lm_eval/models/huggingface.py#L66&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch_size&lt;/code&gt;: Batch size for inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--output_path&lt;/code&gt;: Directory to save evaluation results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example running multiple benchmarks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eval.eval \&#xA;    --model hf \&#xA;    --tasks MTBench,WildBench,alpaca_eval \&#xA;    --model_args &#34;pretrained=mistralai/Mistral-7B-Instruct-v0.3&#34; \&#xA;    --batch_size 2 \&#xA;    --output_path logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Config shortcuts&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;p&gt;To be able to reuse commonly used settings without having to manually supply full arguments every time, we support reading eval configs from YAML files. These configs replace the &lt;code&gt;--batch_size&lt;/code&gt;, &lt;code&gt;--tasks&lt;/code&gt;, and &lt;code&gt;--annoator_model&lt;/code&gt; arguments. Some example config files can be found in &lt;code&gt;./configs&lt;/code&gt;. To use these configs, you can use the &lt;code&gt;--config&lt;/code&gt; flag as shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eval.eval \&#xA;    --model hf \&#xA;    --model_args &#34;pretrained=mistralai/Mistral-7B-Instruct-v0.3&#34; \&#xA;    --output_path logs \&#xA;    --config configs/light_gpt4omini0718.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We add several more command examples in &lt;a href=&#34;https://github.com/mlfoundations/Evalchemy/tree/main/eval/examples&#34;&gt;&lt;code&gt;eval/examples&lt;/code&gt;&lt;/a&gt; to help you start using Evalchemy.&lt;/p&gt; &#xA;&lt;h2&gt;🔧 Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Support for different models&lt;/h3&gt; &#xA;&lt;p&gt;Through LM-Eval-Harness, we support all HuggingFace models and are currently adding support for all LM-Eval-Harness models, such as OpenAI and VLLM. For more information on such models, please check out the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/models&#34;&gt;models page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To choose a model, simply set &#39;pretrained=&#xA; &lt;name of hf model&gt;&#xA;  &#39; where the model name can either be a HuggingFace model name or a path to a local model.&#xA; &lt;/name&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Multi-GPU Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;For faster evaluation using data parallelism (recommended):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch --num-processes &amp;lt;num-gpus&amp;gt; --num-machines &amp;lt;num-nodes&amp;gt; \&#xA;    --multi-gpu -m eval.eval \&#xA;    --model hf \&#xA;    --tasks MTBench,alpaca_eval \&#xA;    --model_args &#39;pretrained=mistralai/Mistral-7B-Instruct-v0.3&#39; \&#xA;    --batch_size 2 \&#xA;    --output_path logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Large Model Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;For models that don&#39;t fit on a single GPU, use model parallelism:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eval.eval \&#xA;    --model hf \&#xA;    --tasks MTBench,alpaca_eval \&#xA;    --model_args &#39;pretrained=mistralai/Mistral-7B-Instruct-v0.3,parallelize=True&#39; \&#xA;    --batch_size 2 \&#xA;    --output_path logs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;💡 Note&lt;/strong&gt;: While &#34;auto&#34; batch size is supported, we recommend manually tuning the batch size for optimal performance. The optimal batch size depends on the model size, GPU memory, and the specific benchmark. We used a maximum of 32 and a minimum of 4 (for RepoBench) to evaluate Llama-3-8B-Instruct on 8xH100 GPUs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Output Log Structure&lt;/h3&gt; &#xA;&lt;p&gt;Our generated logs include critical information about each evaluation to help inform your experiments. We highlight important items in our generated logs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Model Configuration&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: Model framework used&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model_args&lt;/code&gt;: Model arguments for the model framework&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt;: Size of processing batches&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;device&lt;/code&gt;: Computing device specification&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;annotator_model&lt;/code&gt;: Model used for annotation (&#34;gpt-4o-mini-2024-07-18&#34;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Seed Configuration&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;random_seed&lt;/code&gt;: General random seed&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;numpy_seed&lt;/code&gt;: NumPy-specific seed&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;torch_seed&lt;/code&gt;: PyTorch-specific seed&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;fewshot_seed&lt;/code&gt;: Seed for few-shot examples&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Model Details&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;model_num_parameters&lt;/code&gt;: Number of model parameters&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model_dtype&lt;/code&gt;: Model data type&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model_revision&lt;/code&gt;: Model version&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model_sha&lt;/code&gt;: Model commit hash&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Version Control&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;git_hash&lt;/code&gt;: Repository commit hash&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;date&lt;/code&gt;: Unix timestamp of evaluation&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;transformers_version&lt;/code&gt;: Hugging Face Transformers version&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Tokenizer Configuration&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;tokenizer_pad_token&lt;/code&gt;: Padding token details&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;tokenizer_eos_token&lt;/code&gt;: End of sequence token&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;tokenizer_bos_token&lt;/code&gt;: Beginning of sequence token&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;eot_token_id&lt;/code&gt;: End of text token ID&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;max_length&lt;/code&gt;: Maximum sequence length&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Model Settings&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;model_source&lt;/code&gt;: Model source platform&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model_name&lt;/code&gt;: Full model identifier&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model_name_sanitized&lt;/code&gt;: Sanitized model name for file system usage&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;chat_template&lt;/code&gt;: Conversation template&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;chat_template_sha&lt;/code&gt;: Template hash&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Timing Information&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;start_time&lt;/code&gt;: Evaluation start timestamp&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;end_time&lt;/code&gt;: Evaluation end timestamp&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;total_evaluation_time_seconds&lt;/code&gt;: Total duration&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Hardware Environment&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PyTorch version and build configuration&lt;/li&gt; &#xA;   &lt;li&gt;Operating system details&lt;/li&gt; &#xA;   &lt;li&gt;GPU configuration&lt;/li&gt; &#xA;   &lt;li&gt;CPU specifications&lt;/li&gt; &#xA;   &lt;li&gt;CUDA and driver versions&lt;/li&gt; &#xA;   &lt;li&gt;Relevant library versions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Customizing Evaluation&lt;/h3&gt; &#xA;&lt;h4&gt;🤖 Change Annotator Model&lt;/h4&gt; &#xA;&lt;p&gt;As part of Evalchemy, we want to make swapping in different Language Model Judges for standard benchmarks easy. Currently, we support two judge settings. The first is the default setting, where we use a benchmark&#39;s default judge. To activate this, you can either do nothing or pass in&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--annotator_model auto&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In addition to the default assignments, we support using gpt-4o-mini-2024-07-18 as a judge:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;--annotator_model gpt-4o-mini-2024-07-18&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We are planning on adding support for different judges in the future!&lt;/p&gt; &#xA;&lt;h3&gt;⏱️ Runtime and Cost Analysis&lt;/h3&gt; &#xA;&lt;p&gt;Evalchemy makes running common benchmarks simple, fast, and versatile! We list the speeds and costs for each benchmark we achieve with Evalchemy for Meta-Llama-3-8B-Instruct on 8xH100 GPUs.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Benchmark&lt;/th&gt; &#xA;   &lt;th&gt;Runtime (8xH100)&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;Total Tokens&lt;/th&gt; &#xA;   &lt;th&gt;Default Judge Cost ($)&lt;/th&gt; &#xA;   &lt;th&gt;GPT-4o-mini Judge Cost ($)&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MTBench&lt;/td&gt; &#xA;   &lt;td&gt;14:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;~196K&lt;/td&gt; &#xA;   &lt;td&gt;6.40&lt;/td&gt; &#xA;   &lt;td&gt;0.05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WildBench&lt;/td&gt; &#xA;   &lt;td&gt;38:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;~2.2M&lt;/td&gt; &#xA;   &lt;td&gt;30.00&lt;/td&gt; &#xA;   &lt;td&gt;0.43&lt;/td&gt; &#xA;   &lt;td&gt;Using GPT-4-mini judge&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RepoBench&lt;/td&gt; &#xA;   &lt;td&gt;46:00&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Lower batch size due to memory&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MixEval&lt;/td&gt; &#xA;   &lt;td&gt;13:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;~4-6M&lt;/td&gt; &#xA;   &lt;td&gt;3.36&lt;/td&gt; &#xA;   &lt;td&gt;0.76&lt;/td&gt; &#xA;   &lt;td&gt;Varies by judge model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AlpacaEval&lt;/td&gt; &#xA;   &lt;td&gt;16:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;~936K&lt;/td&gt; &#xA;   &lt;td&gt;9.40&lt;/td&gt; &#xA;   &lt;td&gt;0.14&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HumanEval&lt;/td&gt; &#xA;   &lt;td&gt;4:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;No API costs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IFEval&lt;/td&gt; &#xA;   &lt;td&gt;1:30&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;No API costs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ZeroEval&lt;/td&gt; &#xA;   &lt;td&gt;1:44:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;Longest runtime&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MBPP&lt;/td&gt; &#xA;   &lt;td&gt;6:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;No API costs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MMLU&lt;/td&gt; &#xA;   &lt;td&gt;7:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;No API costs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARC&lt;/td&gt; &#xA;   &lt;td&gt;4:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;No API costs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DROP&lt;/td&gt; &#xA;   &lt;td&gt;20:00&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;No API costs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notes:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Runtimes measured using 8x H100 GPUs with Meta-Llama-3-8B-Instruct model&lt;/li&gt; &#xA; &lt;li&gt;Batch sizes optimized for memory and speed&lt;/li&gt; &#xA; &lt;li&gt;API costs vary based on judge model choice&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cost-Saving Tips:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use gpt-4o-mini-2024-07-18 judge when possible for significant cost savings&lt;/li&gt; &#xA; &lt;li&gt;Adjust batch size based on available memory&lt;/li&gt; &#xA; &lt;li&gt;Consider using data-parallel evaluation for faster results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;🔐 Special Access Requirements&lt;/h3&gt; &#xA;&lt;h4&gt;ZeroEval Access&lt;/h4&gt; &#xA;&lt;p&gt;To run ZeroEval benchmarks, you need to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Request access to the &lt;a href=&#34;https://huggingface.co/datasets/allenai/ZebraLogicBench-private&#34;&gt;ZebraLogicBench-private dataset&lt;/a&gt; on Hugging Face&lt;/li&gt; &#xA; &lt;li&gt;Accept the terms and conditions&lt;/li&gt; &#xA; &lt;li&gt;Log in to your Hugging Face account when running evaluations&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;🛠️ Implementing Custom Evaluations&lt;/h2&gt; &#xA;&lt;p&gt;To add a new evaluation system:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a new directory under &lt;code&gt;eval/chat_benchmarks/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Implement &lt;code&gt;eval_instruct.py&lt;/code&gt; with two required functions: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;eval_instruct(model)&lt;/code&gt;: Takes an LM Eval Model, returns results dict&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;evaluate(results)&lt;/code&gt;: Takes results dictionary, returns evaluation metrics&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Adding External Evaluation Repositories&lt;/h3&gt; &#xA;&lt;p&gt;Use git subtree to manage external evaluation code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Add external repository&#xA;git subtree add --prefix=eval/chat_benchmarks/new_eval https://github.com/original/repo.git main --squash&#xA;&#xA;# Pull updates&#xA;git subtree pull --prefix=eval/chat_benchmarks/new_eval https://github.com/original/repo.git main --squash&#xA;&#xA;# Push contributions back&#xA;git subtree push --prefix=eval/chat_benchmarks/new_eval https://github.com/original/repo.git contribution-branch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;🔍 Debug Mode&lt;/h3&gt; &#xA;&lt;p&gt;To run evaluations in debug mode, add the &lt;code&gt;--debug&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m eval.eval \&#xA;    --model hf \&#xA;    --tasks MTBench \&#xA;    --model_args &#34;pretrained=mistralai/Mistral-7B-Instruct-v0.3&#34; \&#xA;    --batch_size 2 \&#xA;    --output_path logs \&#xA;    --debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is particularly useful when testing new evaluation implementations, debugging model configurations, verifying dataset access, and testing database connectivity.&lt;/p&gt; &#xA;&lt;h3&gt;🚀 Performance Tips&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Utilize batch processing for faster evaluation:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;all_instances.append(&#xA;    Instance(&#xA;        &#34;generate_until&#34;,&#xA;        example,&#xA;        (&#xA;            inputs,&#xA;            {&#xA;                &#34;max_new_tokens&#34;: 1024,&#xA;                &#34;do_sample&#34;: False,&#xA;            },&#xA;        ),&#xA;        idx,&#xA;    )&#xA;)&#xA;&#xA;outputs = self.compute(model, all_instances)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Use the LM-eval logger for consistent logging across evaluations&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;🔧 Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;Evalchemy has been tested on CUDA 12.4. If you run into issues like this: &lt;code&gt;undefined symbol: __nvJitLinkComplete_12_4, version libnvJitLink.so.12&lt;/code&gt;, try updating your CUDA version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64/cuda-keyring_1.1-1_all.deb&#xA;sudo dpkg -i cuda-keyring_1.1-1_all.deb&#xA;sudo add-apt-repository contrib&#xA;sudo apt-get update&#xA;sudo apt-get -y install cuda-toolkit-12-4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🏆 Leaderboard Integration&lt;/h2&gt; &#xA;&lt;p&gt;To track experiments and evaluations, we support logging results to a PostgreSQL database. Details on the entry schemas and database setup can be found in &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/evalchemy/main/database/&#34;&gt;&lt;code&gt;database/&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thank you to all the contributors for making this project possible! Please follow &lt;a href=&#34;https://raw.githubusercontent.com/mlfoundations/evalchemy/main/CONTRIBUTING.md&#34;&gt;these instructions&lt;/a&gt; on how to contribute.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find Evalchemy useful, please consider citing us!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{Evalchemy,&#xA;  author = {Guha, Etash and Raoof, Negin and Mercat, Jean and Frankel, Eric and Keh, Sedrick and Grover, Sachin and Smyrnis, George and Vu, Trung and Marten, Ryan and Saad-Falcon, Jon and Choi, Caroline and Arora, Kushal and Merrill, Mike and Deng, Yichuan and Suvarna, Ashima and Bansal, Hritik and Nezhurina, Marianna and Choi, Yejin and Heckel, Reinhard and Oh, Seewong and Hashimoto, Tatsunori and Jitsev, Jenia and Shankar, Vaishaal and Dimakis, Alex and Sathiamoorthy, Mahesh and Schmidt, Ludwig},&#xA;  month = nov,&#xA;  title = {{Evalchemy}},&#xA;  year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>