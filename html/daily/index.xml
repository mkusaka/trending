<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-20T01:31:16Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>docker/labs-ai-tools-for-devs</title>
    <updated>2025-03-20T01:31:16Z</updated>
    <id>tag:github.com,2025-03-20:/docker/labs-ai-tools-for-devs</id>
    <link href="https://github.com/docker/labs-ai-tools-for-devs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An MCP server &amp; prompt runner for all of Docker. Simple Markdown. BYO LLM.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;This README is an agentic workflow&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;AI Tools for Developers&lt;/h1&gt; &#xA;&lt;p&gt;Agentic AI workflows enabled by Docker containers.&lt;/p&gt; &#xA;&lt;p&gt;Just Docker. Just Markdown. BYOLLM.&lt;/p&gt; &#xA;&lt;h2&gt;MCP&lt;/h2&gt; &#xA;&lt;p&gt;Any prompts you write and their tools can now be used as &lt;a href=&#34;https://www.anthropic.com/news/model-context-protocol&#34;&gt;MCP servers&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use serve mode with &lt;code&gt;--mcp&lt;/code&gt; flag. Then, register prompts via git ref or path with &lt;code&gt;--register &amp;lt;ref&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# ...&#xA;serve&#xA;--mcp&#xA;--register github:docker/labs-ai-tools-for-devs?path=prompts/examples/generate_dockerfile.md&#xA;--register /Users/ai-overlordz/some/local/prompt.md&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/docker/labs-ai-tools-for-devs/main/img1.png&#34; alt=&#34;overall architecture diagram preview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Source for many experiments in our &lt;a href=&#34;https://www.linkedin.com/newsletters/docker-labs-genai-7204877599427194882/&#34;&gt;LinkedIn newsletter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/docker/labs-ai-tools-vscode&#34;&gt;&lt;strong&gt;VSCode Extension&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://vonwig.github.io/prompts.docs/&#34;&gt;&lt;strong&gt;Docs&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is this?&lt;/h1&gt; &#xA;&lt;p&gt;This is a simple Docker image which enables infinite possibilities for novel workflows by combining Dockerized Tools, Markdown, and the LLM of your choice.&lt;/p&gt; &#xA;&lt;h2&gt;Markdown is the language&lt;/h2&gt; &#xA;&lt;p&gt;Humans already speak it. So do LLM&#39;s. This software allows you to write complex workflows in a markdown files, and then run them with your own LLM in your editor or terminal...or any environment, thanks to Docker.&lt;/p&gt; &#xA;&lt;h2&gt;Dockerized Tools&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/docker/labs-ai-tools-for-devs/main/img4.png&#34; alt=&#34;dockerized tools&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenAI API compatiable LLM&#39;s already support tool calling. We believe these tools could just be Docker images. Some of the benefits using Docker based on our &lt;a href=&#34;https://www.linkedin.com/newsletters/docker-labs-genai-7204877599427194882/&#34;&gt;research&lt;/a&gt; are enabling the LLM to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;take more complex actions&lt;/li&gt; &#xA; &lt;li&gt;get more context with fewer tokens&lt;/li&gt; &#xA; &lt;li&gt;work across a wider range of environments&lt;/li&gt; &#xA; &lt;li&gt;operate in a sandboxed environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Conversation &lt;em&gt;Loop&lt;/em&gt;&lt;/h2&gt; &#xA;&lt;p&gt;The conversation loop is the core of each workflow. Tool results, agent responses, and of course, the markdown prompts, are all passed through the loop. If an agent sees an error, it will try running the tool with different parameters, or even different tools until it gets the right result.&lt;/p&gt; &#xA;&lt;h2&gt;Multi-Model Agents&lt;/h2&gt; &#xA;&lt;p&gt;Each prompt can be configured to be run with different LLM models, or even different model families. This allows you to use the best tool for the job. When you combine these tools, you can create multi-agent workflows where each agent runs with the model best suited for that task.&lt;/p&gt; &#xA;&lt;p&gt;With Docker, it is possible to have frontier models plan, while lightweight local models execute.&lt;/p&gt; &#xA;&lt;h2&gt;Project-First Design&lt;/h2&gt; &#xA;&lt;p&gt;To get help from an assistant in your software development loop, the only context necessary is the project you are working on.&lt;/p&gt; &#xA;&lt;h3&gt;Extracting project context&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/docker/labs-ai-tools-for-devs/main/img2.png&#34; alt=&#34;extractor architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;An extractor is a Docker image that runs against a project and extracts information into a JSON context.&lt;/p&gt; &#xA;&lt;h2&gt;Prompts as a trackable artifact&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/docker/labs-ai-tools-for-devs/main/img3.png&#34; alt=&#34;prompts as a trackable artifact&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompts are stored in a git repo and can be versioned, tracked, and shared for anyone to run in their own environment.&lt;/p&gt; &#xA;&lt;h1&gt;Get Started&lt;/h1&gt; &#xA;&lt;p&gt;We highly recommend using the VSCode extension to get started. It will help you create prompts, and run them with your own LLM.&lt;/p&gt; &#xA;&lt;h2&gt;Running your first loop&lt;/h2&gt; &#xA;&lt;h3&gt;VSCode&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install Extension&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Get the &lt;a href=&#34;https://github.com/docker/labs-ai-tools-vscode/releases/latest&#34;&gt;latest release&lt;/a&gt; and install with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;code --install-extension &#39;labs-ai-tools-vscode-&amp;lt;version&amp;gt;.vsix&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open an existing markdown file, or create a new markdown file in VSCode.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can even run &lt;em&gt;this&lt;/em&gt; markdown file directly!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Run command &lt;code&gt;&amp;gt;Docker AI: Set OpenAI API Key&lt;/code&gt; to set an OpenAI API key, or use a dummy value for local models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run command &lt;code&gt;&amp;gt;Docker AI: Select target project&lt;/code&gt; to select a project to run the prompt against.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run command &lt;code&gt;&amp;gt;Docker AI: Run Prompt&lt;/code&gt; to start the conversation loop.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;CLI&lt;/h3&gt; &#xA;&lt;p&gt;Instructions assume you have a terminal open, and Docker Desktop running.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set OpenAI key&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;echo $OPENAI_API_KEY &amp;gt; $HOME/.openai-api-key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: we assume this file exists, so you must set a dummy value for local models.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the container in your project directory&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker run &#xA;  --rm \&#xA;  --pull=always \&#xA;  -it \&#xA;  -v /var/run/docker.sock:/var/run/docker.sock \&#xA;  --mount type=volume,source=docker-prompts,target=/prompts \&#xA;  --mount type=bind,source=$HOME/.openai-api-key,target=/root/.openai-api-key \&#xA;  vonwig/prompts:latest \&#xA;    run \&#xA;    --host-dir $PWD \&#xA;    --user $USER \&#xA;    --platform &#34;$(uname -o)&#34; \&#xA;    --prompts &#34;github:docker/labs-githooks?ref=main&amp;amp;path=prompts/git_hooks&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://vonwig.github.io/prompts.docs/#/page/running%20the%20prompt%20engine&#34;&gt;docs&lt;/a&gt; for more details on how to run the conversation loop.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;#docker:command=build&#xA;&#xA;docker build -t vonwig/prompts:local -f Dockerfile .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, for the agentic workflow...&lt;/p&gt; &#xA;&lt;h1&gt;prompt system&lt;/h1&gt; &#xA;&lt;p&gt;You are an expert at reading readmes.&lt;/p&gt; &#xA;&lt;p&gt;Use curl to get the readme for &lt;a href=&#34;https://github.com/docker/labs-ai-tools-for-devs&#34;&gt;https://github.com/docker/labs-ai-tools-for-devs&lt;/a&gt; before answering the following questions.&lt;/p&gt; &#xA;&lt;h1&gt;prompt user&lt;/h1&gt; &#xA;&lt;p&gt;What is this project?&lt;/p&gt;</summary>
  </entry>
</feed>