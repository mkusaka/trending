<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-07T01:37:43Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>open-source-labs/ReacType</title>
    <updated>2023-07-07T01:37:43Z</updated>
    <id>tag:github.com,2023-07-07:/open-source-labs/ReacType</id>
    <link href="https://github.com/open-source-labs/ReacType" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🧪 Prototyping Tool for exporting React/Typescript Applications!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ReacType&lt;/h1&gt; &#xA;&lt;!-- &lt;p align=&#34;center&#34;&gt;&#xA;  &lt;img width=&#34;1000&#34; src=&#34;https://i.imgur.com/enAcYvB.png&#34;&gt;&#xA;&lt;/p&gt; --&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/open-source-labs/ReacType/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/open-source-labs/ReacType&#34; alt=&#34;StarShield&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-source-labs/ReacType/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/open-source-labs/ReacType&#34; alt=&#34;ContributorShield&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-source-labs/ReacType/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/open-source-labs/ReacType&#34; alt=&#34;ForksShield&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/version-16.0.0-orange&#34; alt=&#34;Version: 14.0.0&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- &lt;p align=&#34;center&#34;&gt;&#xA;  &lt;img width=&#34;1000&#34; src=&#34;https://i.imgur.com/FPizsat.png&#34;&gt;&#xA;&lt;/p&gt; --&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;1000&#34; src=&#34;https://i.imgur.com/FIX8skV.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/javascript-%23323330.svg?style=for-the-badge&amp;amp;logo=javascript&amp;amp;logoColor=%23F7DF1E&#34; alt=&#34;JavaScript&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/typescript-%23007ACC.svg?style=for-the-badge&amp;amp;logo=typescript&amp;amp;logoColor=white&#34; alt=&#34;TypeScript&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/node.js-6DA55F?style=for-the-badge&amp;amp;logo=node.js&amp;amp;logoColor=white&#34; alt=&#34;NodeJS&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/express.js-%23404d59.svg?style=for-the-badge&amp;amp;logo=express&amp;amp;logoColor=%2361DAFB&#34; alt=&#34;Express.js&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/react-%2320232a.svg?style=for-the-badge&amp;amp;logo=react&amp;amp;logoColor=%2361DAFB&#34; alt=&#34;React&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/redux-%23593d88.svg?style=for-the-badge&amp;amp;logo=redux&amp;amp;logoColor=white&#34; alt=&#34;Redux&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Socket.io-black?style=for-the-badge&amp;amp;logo=socket.io&amp;amp;badgeColor=010101&#34; alt=&#34;Socket.io&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/-jest-%23C21325?style=for-the-badge&amp;amp;logo=jest&amp;amp;logoColor=white&#34; alt=&#34;Jest&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Babel-F9DC3e?style=for-the-badge&amp;amp;logo=babel&amp;amp;logoColor=black&#34; alt=&#34;Babel&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/git-%23F05033.svg?style=for-the-badge&amp;amp;logo=git&amp;amp;logoColor=white&#34; alt=&#34;Git&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/MUI-%230081CB.svg?style=for-the-badge&amp;amp;logo=mui&amp;amp;logoColor=white&#34; alt=&#34;MUI&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Electron-191970?style=for-the-badge&amp;amp;logo=Electron&amp;amp;logoColor=white&#34; alt=&#34;Electron.js&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/MongoDB-%234ea94b.svg?style=for-the-badge&amp;amp;logo=mongodb&amp;amp;logoColor=white&#34; alt=&#34;MongoDB&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/AWS-%23FF9900.svg?style=for-the-badge&amp;amp;logo=amazon-aws&amp;amp;logoColor=white&#34; alt=&#34;AWS&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&amp;amp;logo=docker&amp;amp;logoColor=white&#34; alt=&#34;Docker&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;1000&#34; src=&#34;https://raw.githubusercontent.com/open-source-labs/ReacType/master/resources/demo.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReacType&lt;/strong&gt; is a rapid prototyping tool that allows users &lt;em&gt;visualize&lt;/em&gt; their application architecture dynamically, employing a &lt;em&gt;drag-and-drop canvas display&lt;/em&gt; and an interactive, &lt;em&gt;real-time component code preview&lt;/em&gt; that can be exported as a &lt;strong&gt;React&lt;/strong&gt; app for developers employing React component architecture alongside the comprehensive type-checking of &lt;strong&gt;TypeScript&lt;/strong&gt;. In other words, &lt;strong&gt;you can draw prototypes and export React / TypeScript code!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://reactype.dev&#34;&gt;reactype.dev&lt;/a&gt; to learn more about the product.&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/reactype&#34;&gt;@ReacType&lt;/a&gt; on Twitter for important announcements.&lt;/p&gt; &#xA;&lt;h3&gt;Documentation&lt;/h3&gt; &#xA;&lt;p&gt;If you want to read about using ReacType, the &lt;a href=&#34;https://reactype-1.herokuapp.com/#/tutorial&#34;&gt;User Manual&lt;/a&gt; is free and available online now.&lt;/p&gt; &#xA;&lt;h2&gt;Changes with version 16.0.0&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Improved Testing Coverage&lt;/strong&gt;: Testing coverage has now doubled since version 15, and now sits at just over 50% coverage. Version 16 introduces end-to-end testing with Playwright and adds additional unit testing with React Testing Library.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Major Bug Fixes&lt;/strong&gt;: Manage Project Features now work as expected. State Manager now deletes state from parent components. Context Manager Display Tab and CSS Editor now rendering as expected.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Typescript Conversion&lt;/strong&gt;: Typescript coverage has improved from 30% to 80% with additional interfaces added for quality improvements.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Live CSS Demo Rendering&lt;/strong&gt;: CSS Editor changes now rendered visually in the demo page on save.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Universal Exports on Web App&lt;/strong&gt;: Export feature on web app now allows users to download the current project as a zip file with modularized component folder, html, and css file included. Export feature is now available to all users including guests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;UI Improvements&lt;/strong&gt;: Fixed multiple contrast issues with white text displaying on white background. Adjusted context manager interface for improved UX. Fixed border styling within modals and error messages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;And more:&lt;/strong&gt; See &lt;a href=&#34;https://github.com/open-source-labs/ReacType/raw/master/CHANGE_LOG.md&#34;&gt;change log&lt;/a&gt; for more details on what was changed from the previous versions as well as plans for upcoming features!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;File Structure courtesy of Reactype version 14.0.0&lt;/h2&gt; &#xA;&lt;p&gt;Here is the main file structure:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;1000&#34; src=&#34;https://i.imgur.com/RdK8QzW.jpg&#34;&gt; &lt;/p&gt; Please refer to the link: https://excalidraw.com/#json=JKwzVD5qx6lsfiHW1_pQ9,XJ6uDoehVu-1bsx0SMlC6w for more details. &#xA;&lt;h2&gt;Run ReacType using CLI&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fork&lt;/strong&gt; and &lt;strong&gt;Clone&lt;/strong&gt; Repository.&lt;/li&gt; &#xA; &lt;li&gt;Open project directory&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To run the production build&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run prod&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To run tests&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To run the development build&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Note that a .env with DEV_PORT, and a NODE_ENV flag (=production or development) are needed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Please note that the development build is not connected to the production server. &lt;code&gt;npm run dev&lt;/code&gt; should spin up the development server from the server folder of this repo. For additional information, the readme is &lt;a href=&#34;https://github.com/open-source-labs/ReacType/raw/master/server/README.md&#34;&gt;here&lt;/a&gt;. Alternatively, you can select &#34;Continue as guest&#34; on the login page of the app, which will not use any features that rely on the server (authentication and saving project data.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To run the development build of electron app&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev&#xA;npm run electron-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run Exported App&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open exported project directory&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build the app&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start an instance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open browser and navigate to localhost at specified port&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Stack&lt;/h2&gt; &#xA;&lt;p&gt;Typescript, React.js, Redux Toolkit, Javascript, ESM, Node.js (Express), HTML, CSS, MUI, GraphQL, Next.js, Gatsby.js, Electron, NoSQL, Webpack, TDD (Jest, React Testing Library, Playwright), OAuth 2.0, Websocket, Continuous Integration (Github Actions), Docker, AWS (ECR, Elastic Beanstalk), Ace Editor, Google Charts, React DnD&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Here is the up to date &lt;a href=&#34;https://github.com/open-source-labs/ReacType/raw/master/contributors.md&#34;&gt;list&lt;/a&gt; of all co-developers of this product. Please visit our &lt;a href=&#34;https://github.com/open-source-labs/ReacType/raw/master/contribution_documentation.md&#34;&gt;contribution documentation&lt;/a&gt; for more information on how you can contribute to ReacType!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://github.com/team-reactype/ReacType/raw/development/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>diff-usion/Awesome-Diffusion-Models</title>
    <updated>2023-07-07T01:37:43Z</updated>
    <id>tag:github.com,2023-07-07:/diff-usion/Awesome-Diffusion-Models</id>
    <link href="https://github.com/diff-usion/Awesome-Diffusion-Models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of resources and papers on Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/hee9joon/Awesome-Diffusion-Models&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chetanraj/awesome-github-badges&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Made%20With-Love-red.svg?sanitize=true&#34; alt=&#34;Made With Love&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains a collection of resources and papers on &lt;em&gt;&lt;strong&gt;Diffusion Models&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#resources&#34;&gt;Resources&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#introductory-posts&#34;&gt;Introductory Posts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#introductory-papers&#34;&gt;Introductory Papers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#introductory-videos&#34;&gt;Introductory Videos&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#introductory-lectures&#34;&gt;Introductory Lectures&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#tutorial-and-jupyter-notebook&#34;&gt;Tutorial and Jupyter Notebook&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#papers&#34;&gt;Papers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#survey&#34;&gt;Survey&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#vision&#34;&gt;Vision&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#generation&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#classification&#34;&gt;Classification&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#segmentation&#34;&gt;Segmentation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#image-translation&#34;&gt;Image Translation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#inverse-problems&#34;&gt;Inverse Problems&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#medical-imaging&#34;&gt;Medical Imaging&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#multi-modal-learning&#34;&gt;Multi-modal Learning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#3d-vision&#34;&gt;3D Vision&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#adversarial-attack&#34;&gt;Adversarial Attack&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#miscellany&#34;&gt;Miscellany&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#audio&#34;&gt;Audio&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#generation-1&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#conversion&#34;&gt;Conversion&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#enhancement&#34;&gt;Enhancement&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#separation&#34;&gt;Separation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#text-to-speech&#34;&gt;Text-to-Speech&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#miscellany-1&#34;&gt;Miscellany&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#natural-language&#34;&gt;Natural Language&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#tabular-and-time-series&#34;&gt;Tabular and Time Series&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#generation-2&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#forecasting&#34;&gt;Forecasting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#imputation&#34;&gt;Imputation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#miscellany-2&#34;&gt;Miscellany&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#graph&#34;&gt;Graph&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#generation-3&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#molecular-and-material-generation&#34;&gt;Molecular and Material Generation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#theory&#34;&gt;Theory&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;h2&gt;Introductory Posts&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;⏩&lt;/span&gt; DiffusionFastForward: 01-Diffusion-Theory&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mikolaj Czerkawski (@mikonvergence)&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/mikonvergence/DiffusionFastForward/raw/master/notes/01-Diffusion-Theory.md&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 4 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How diffusion models work: the math from scratch&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sergios Karagiannakos,Nikolas Adaloglou&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://theaisummer.com/diffusion-models/?fbclid=IwAR1BIeNHqa3NtC8SL0sKXHATHklJYphNH-8IGNoO3xZhSKM_GYcvrrQgB0o&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 24 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Path to the Variational Diffusion Loss&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Alemi&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://blog.alexalemi.com/diffusion.html&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/github/google-research/vdm/blob/main/colab/SimpleDiffusionColab.ipynb&#34;&gt;Colab&lt;/a&gt;] &lt;br&gt; 15 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Annotated Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Niels Rogge, Kashif Rasul&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://huggingface.co/blog/annotated-diffusion&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 06 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The recent rise of diffusion-based models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Maciej Domagała&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://maciejdomagala.github.io/generative_models/2022/06/06/The-recent-rise-of-diffusion-based-models.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 06 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Introduction to Diffusion Models for Machine Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryan O&#39;Connor&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 12 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Models as an Alternative To GANs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arash Vahdat and Karsten Kreis&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/&#34;&gt;Website-Part 1&lt;/a&gt;] [&lt;a href=&#34;https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-2/&#34;&gt;Website-Part 2&lt;/a&gt;] &lt;br&gt; 26 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An introduction to Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayan Das&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 04 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Introduction to deep generative modeling: Diffusion-based Deep Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jakub Tomczak&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://jmtomczak.github.io/blog/10/10_ddgms_lvm_p2.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 30 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What are Diffusion Models?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lilian Weng&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 11 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models as a kind of VAE&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Angus Turner&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://angusturner.github.io/generative_models/2021/06/29/diffusion-probabilistic-models-I.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 29 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Modeling by Estimating Gradients of the Data Distribution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://yang-song.github.io/blog/2021/score/&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 5 May 2021&lt;/p&gt; &#xA;&lt;h2&gt;Introductory Papers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding Diffusion Models: A Unified Perspective&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Calvin Luo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.11970&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to Train Your Energy-Based Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Diederik P. Kingma&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2101.03288&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jan 2021&lt;/p&gt; &#xA;&lt;h2&gt;Introductory Videos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;⏩&lt;/span&gt; DiffusionFastForward&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mikolaj Czerkawski (@mikonvergence)&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/playlist?list=PL5RHjmn-MVHDMcqx-SI53mB7sFOqPK6gN&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 4 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models from scratch in PyTorch&lt;/strong&gt; &lt;br&gt; &lt;em&gt;DeepFindr&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=a4Yfz2FxXiY&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 18 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models | Paper Explanation | Math Explained&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Outlier&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=HoKDTa5jHvg&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 6 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What are Diffusion Models?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ari Seff&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=fbLgFrlTnGU&amp;amp;list=LL&amp;amp;index=2&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 20 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models explained&lt;/strong&gt; &lt;br&gt; &lt;em&gt;AI Coffee Break with Letitia&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=344w5h24-h8&amp;amp;ab_channel=AICoffeeBreakwithLetitia&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 23 Mar 2022&lt;/p&gt; &#xA;&lt;h2&gt;Introductory Lectures&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion-based Generative Modeling: Foundations and Applications&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Karsten Kreis, Ruiqi Gao, Arash Vahdat&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://cvpr2022-tutorial-diffusion-models.github.io/&#34;&gt;Page&lt;/a&gt;] &lt;br&gt; 19 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jascha Sohl-Dickstein, MIT 6.S192 - Lecture 22&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=XCUlnHP1TNM&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 19 Apr 2022&lt;/p&gt; &#xA;&lt;h2&gt;Tutorial and Jupyter Notebook&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;⏩&lt;/span&gt; DiffusionFastForward: train from scratch in colab&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mikolaj Czerkawski (@mikonvergence)&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/mikonvergence/DiffusionFastForward&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mikonvergence/DiffusionFastForward#computer-code&#34;&gt;notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;diffusion-for-beginners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;ozanciga&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/ozanciga/diffusion-for-beginners&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Beyond Diffusion: What is Personalized Image Generation and How Can You Customize Image Synthesis?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;J. Rafid Siddiqui&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/azad-academy/personalized-diffusion&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://medium.com/mlearning-ai/beyond-diffusion-what-is-personalized-image-generation-and-how-can-you-customize-image-synthesis-26a89d5b335&#34;&gt;Medium&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion_models_tutorial&lt;/strong&gt; &lt;br&gt; &lt;em&gt;FilippoMB&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/FilippoMB/Diffusion_models_tutorial&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ScoreDiffusionModel&lt;/strong&gt; &lt;br&gt; &lt;em&gt;JeongJiHeon&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/JeongJiHeon/ScoreDiffusionModel&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Minimal implementation of diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;VSehwag&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/VSehwag/minimal-diffusion&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;diffusion_tutorial&lt;/strong&gt; &lt;br&gt; &lt;em&gt;sunlin-ai&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/sunlin-ai/diffusion_tutorial&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising diffusion probabilistic models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;acids-ircam&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/acids-ircam/diffusion_models&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Centipede Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zalring&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/Zalring/Centipede_Diffusion/blob/main/Centipede_Diffusion.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deforum Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;deforum&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stable Diffusion Interpolation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;None&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/drive/1EHZtFjQoRr-bns1It5mTcOVyZzZD9bBc?usp=sharing&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Keras Stable Diffusion: GPU starter example&lt;/strong&gt; &lt;br&gt; &lt;em&gt;None&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/drive/1zVTa4mLeM_w44WaFwl7utTaa6JcaH1zK&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Huemin Jax Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;huemin-art&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/huemin-art/jax-guided-diffusion/blob/v2.7/Huemin_Jax_Diffusion_2_7.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disco Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;alembics&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Simplified Disco Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;entmike&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/entmike/disco-diffusion-1/blob/main/Simplified_Disco_Diffusion.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WAS&#39;s Disco Diffusion - Portrait Generator Playground&lt;/strong&gt; &lt;br&gt; &lt;em&gt;WASasquatch&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/WASasquatch/disco-diffusion-portrait-playground/blob/main/WAS&#39;s_Disco_Diffusion_v5_6_9_%5BPortrait_Generator_Playground%5D.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusers - Hugging Face&lt;/strong&gt; &lt;br&gt; &lt;em&gt;huggingface&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h1&gt;Papers&lt;/h1&gt; &#xA;&lt;h2&gt;Survey&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Design Fundamentals of Diffusion Models: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyi Chang, George A. Koulieris, Hubert P. H. Shum&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04542&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models in NLP: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao Zou, Zae Myung Kim, Dongyeop Kang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Time Series Applications: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, Junbin Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00624&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Comprehensive Survey on Knowledge Distillation of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weijian Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04262&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, Chaoning Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01565&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenshuang Zhang, Chaoning Zhang, Sheng Zheng, Mengchun Zhang, Maryam Qamar, Sung-Ho Bae, In So Kweon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13336&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models in NLP: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuansong Zhu, Yu Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07576&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-image Diffusion Model in Generative AI: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In So Kweon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07909&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Non-autoregressive Text Generation: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06574&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models in Bioinformatics: A New Wave of Deep Learning Revolution in Action&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiye Guo, Jian Liu, Yanli Wang, Mengrui Chen, Duolin Wang, Dong Xu, Jianlin Cheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10907&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Diffusion Models on Graphs: Methods and Applications&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenqi Fan, Chengyi Liu, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, Qing Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02591&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Medical Image Analysis: A Comprehensive Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, Dorit Merhof&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.07804&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Diffusion Models for Vision: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anwaar Ulhaq, Naveed Akhtar, Ganna Pogrebna&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.09292&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models in Vision: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.04747&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Survey on Generative Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong Chen, Pheng-Ann Heng, Stan Z. Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.02646&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models: A Comprehensive Survey of Methods and Applications&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ling Yang, Zhilong Zhang, Shenda Hong, Wentao Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.00796&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2022&lt;/p&gt; &#xA;&lt;h2&gt;Vision&lt;/h2&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spiking Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiahang Cao, Ziqing Wang, Hanzhong Guo, Hao Cheng, Qiang Zhang, Renjing Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17046&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DomainStudio: Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14153&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Decoupled Diffusion Models with Explicit Transition Probability&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuhang Huang, Zheng Qin, Xinwang Liu, Kai Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13720&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Continuous Layout Editing of Single Images with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyuan Zhang, Zhitong Huang, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13078&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semi-Implicit Denoising Diffusion Models (SIDDMs)&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, kayhan Batmanghelich, Tingbo Hou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12511&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Eliminating Lipschitz Singularities in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhantao Yang, Ruili Feng, Han Zhang, Yujun Shen, Kai Zhu, Lianghua Huang, Yifei Zhang, Yu Liu, Deli Zhao, Jingren Zhou, Fan Cheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11251&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GD-VDM: Generated Depth for better Diffusion-based Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ariel Lapid, Idan Achituve, Lior Bracha, Ethan Fetaya&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11173&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Harmonization with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiajie Li, Jian Wang, Chen Wang, Jinjun Xiong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10441&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training Diffusion Classifiers with Denoising Assistance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chandramouli Sastry, Sri Harsha Dumpala, Sageev Oore&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09192&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Human Sketch Synthesis with Explicit Abstraction Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dar-Yen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09274&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Training of Diffusion Models with Masked Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongkai Zheng, Weili Nie, Arash Vahdat, Anima Anandkumar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09305&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Anima-Lab/MaskDiT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Relation-Aware Diffusion Model for Controllable Poster Layout Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fengheng Li, An Liu, Wei Feng, Honghe Zhu, Yaoyu Li, Zheng Zhang, Jingjing Lv, Xin Zhu, Junjie Shen, Zhangang Lin, Jingping Shao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09086&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OMS-DPM: Optimizing the Model Schedule for Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, Yu Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08860&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Allan Jabri, Sjoerd van Steenkiste, Emiel Hoogeboom, Mehdi S. M. Sajjadi, Thomas Kipf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08068&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zike Wu, Pan Zhou, Kenji Kawaguchi, Hanwang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.06991&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sail-sg/FDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changyao Tian, Chenxin Tao, Jifeng Dai, Hao Li, Ziheng Li, Lewei Lu, Xiaogang Wang, Hongsheng Li, Gao Huang, Xizhou Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05423&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Architecture Multi-Expert Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunsung Lee, Jin-Young Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, Seungtaek Choi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04990&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting and Improving Diffusion Models Using the Euclidean Distance Function&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Frank Permenter, Chenyang Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04848&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video Diffusion Models with Local-Global Context Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siyuan Yang, Lu Zhang, Yu Liu, Zhizhuo Jiang, You He&lt;/em&gt; &lt;br&gt; IJCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02562&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/exisas/LGC-VD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrew F. Luo, Margaret M. Henderson, Leila Wehbe, Michael J. Tarr&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03089&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Faster Training of Diffusion Models and Improved Density Estimation via Parallel Score Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Etrit Haxholli, Marco Lorenzi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02658&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Temporal Dynamic Quantization for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, Eunhyeok Park&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Generation from Unconditional Diffusion Models using Denoiser Representations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Graikos, Srikar Yellapragada, Dimitris Samaras&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01900&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditioning Diffusion Models via Attributes and Semantic Masks for Face Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nico Giambi, Giuseppe Lisanti&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00914&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Differential Diffusion: Giving Each Pixel Its Strength&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eran Levin, Ohad Fried&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00950&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Addressing Discrepancies in Semantic and Visual Alignment in Neural Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Natalie Abreu, Nathan Vaska, Victoria Helus&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01148&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Addressing Negative Transfer in Diffusion Models&lt;/strong&gt; \ &lt;em&gt;Hyojun Go, JinYoung Kim, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, Hyeongdon Moon, Seungtaek Choi&lt;/em&gt; \ arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00354&#34;&gt;Paper&lt;/a&gt;] \ 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Geometric Perspective on Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Defang Chen, Zhenyu Zhou, Jian-Ping Mei, Chunhua Shen, Chun Chen, Can Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19947&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spontaneous symmetry breaking in generative diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gabriel Raya, Luca Ambrogioni&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19693&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifei Liu, Rex Shen, Xiaotong Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One-Line-of-Code Data Mollification Improves Optimization of Likelihood-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ba-Hien Tran, Giulio Franzese, Pietro Michiardi, Maurizio Filippone&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18900&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ambient Diffusion: Learning Clean Distributions from Corrupted Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alexandros G. Dimakis, Adam Klivans&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19256&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Accurate Data-free Quantization for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18723&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BRIGHT: Bi-level Feature Representation of Image Collections using Groups of Hash Tables&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dingdong Yang, Yizhi Wang, Ali Mahdavi-Amiri, Hao Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18601&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bright-project01.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, Zhihua Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18455&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianqi Chen, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18375&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tqch/poisson-jump&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reconstructing the Mind&#39;s Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paul S. Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan J. Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth A. Norman, Tanishq Mathew Abraham&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18274&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://medarc-ai.github.io/mindeye/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyuan Sun, Mingxiao Li, Zijiao Chen, Yunhao Zhang, Shaonan Wang, Marie-Francine Moens&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17214&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Parallel Sampling of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, Nima Anari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16317&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AndyShih12/paradigms&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Trans-Dimensional Generative Modeling via Jump Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrew Campbell, William Harvey, Christian Weilbach, Valentin De Bortoli, Tom Rainforth, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UDPM: Upsampling Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shady Abu-Hussein, Raja Giryes&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16269&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying GANs and Score-Based Diffusion as Generative Particle Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bézenac, Mickaël Chen, Alain Rakotomamonjy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16150&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DuDGAN: Improving Class-Conditional GANs via Dual-Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Taesun Yeom, Minhyeok Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14849&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingxiao Li, Tingyu Qu, Wei Sun, Marie-Francine Moens&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15583&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Robust Classification via a Single Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15241&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Generalization of Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingyang Yi, Jiacheng Sun, Zhenguo Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14712&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VDT: An Empirical Study on Video Diffusion with Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, Mingyu Ding&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13311&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RERV/VDT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijiao Chen, Jiaxin Qing, Juan Helen Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11675&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mind-video.com/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PTQD: Accurate Post-Training Quantization for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, Bohan Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10657&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Javier E Santos, Zachary R. Fox, Nicholas Lubbers, Yen Ting Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11089&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Structural Pruning for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gongfan Fang, Xinyin Ma, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10924&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/VainF/Diff-Pruning&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Shao, Xu Dai, Shouyi Yin, Lujun Li, Huanran Chen, Yang Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10769&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Mind Visual Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang Liu, Baochang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10135&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analyzing Bias in Diffusion-based Face Generation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Malsha V. Perera, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06402&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaiwen Zheng, Cheng Lu, Jianfei Chen, Jun Zhu&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03935&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LEO: Generative Latent Image Animator for Human Video Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaohui Wang, Xin Ma, Xinyuan Chen, Antitza Dantcheva, Bo Dai, Yu Qiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03989&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://wyhsirius.github.io/LEO-project/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wyhsirius/LEO&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Iterative α-(de)Blending: a Minimalist Deterministic Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eric Heitz, Laurent Belcour, Thomas Chambon&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03486&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reconstructing seen images from human brain activity via guided stochastic search&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Reese Kneeland, Jordyn Ojeda, Ghislain St-Yves, Thomas Naselaris&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00556&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Motion-Conditioned Diffusion Model for Controllable Video Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, Ming-Hsuan Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14404&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tsaishien-chen.github.io/MCDiff/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 27 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zihao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13224&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Compositional Visual Generation with Latent Classifier Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changhao Shi, Haomiao Ni, Kai Li, Shaobo Han, Mingfu Liang, Martin Renqiang Min&lt;/em&gt; &lt;br&gt; CVPR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12536&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao Wang, Pengcheng He, Zhangyang Wang, Weizhu Chen, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12526&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Variational Diffusion Auto-encoder: Deep Latent Variable Model with Unconditional Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12141&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LaMD: Latent Motion Diffusion for Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaosi Hu, Zhenzhong Chen, Chong Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11603&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lookahead Diffusion Probabilistic Models for Refining Mean Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guoqiang Zhang, Niwa Kenta, W. Bastiaan Kleijn&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11312&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/guoqiang-zhang-x/LA-DPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, Sanja Fidler&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09787&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attributing Image Generative Models using Latent Fingerprints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guangyu Nie, Changhoon Kim, Yezhou Yang, Yi Ren&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09752&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Identity Encoder for Personalized Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Chuan Su, Kelvin C.K. Chan, Yandong Li, Yang Zhao, Han Zhang, Boqing Gong, Huisheng Wang, Xuhui Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07429&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Memory Efficient Diffusion Probabilistic Models via Patch-based Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shinei Arakawa, Hideki Tsunashima, Daichi Horita, Keitaro Tanaka, Shigeo Morishima&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07087&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DCFace: Synthetic Face Generation with Dual Condition Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minchul Kim, Feng Liu, Anil Jain, Xiaoming Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07060&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mk-minchul/dcface&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li, Zhenguo Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06648&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06767&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Johanna Karras, Aleksander Holynski, Ting-Chun Wang, Ira Kemelmacher-Shlizerman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06025&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://grail.cs.washington.edu/projects/dreampose/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/johannakarras/DreamPose&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reflected Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aaron Lou, Stefano Ermon&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04740&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://aaronlou.com/blog/2023/reflected-diffusion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/louaaron/Reflected-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Binary Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ze Wang, Jiang Wang, Zicheng Liu, Qiang Qiu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04820&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models as Masked Autoencoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, Christoph Feichtenhofer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03283&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://weichen582.github.io/diffmae.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot Semantic Image Synthesis with Class Affinity Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marlène Careil, Jakob Verbeek, Stéphane Lathuilière&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02321&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EGC: Image Generation and Classification via a Diffusion Energy-Based Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiushan Guo, Chuofan Ma, Yi Jiang, Zehuan Yuan, Yizhou Yu, Ping Luo&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02012&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://guoqiushan.github.io/egc.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Token Merging for Fast Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daniel Bolya, Judy Hoffman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17604&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/dbolya/tomesd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Closer Look at Parameter-Efficient Tuning in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.18181&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;-Diff: Infinite Resolution Diffusion with Subsampled Mollified States&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sam Bond-Taylor, Chris G. Willcocks&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.18242&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-aware Image Generation using 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianfeng Xiang, Jiaolong Yang, Binbin Huang, Xin Tong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17905&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jeffreyxiang.github.io/ivid/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistent View Synthesis with Pose-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, Johannes Kopf&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17598&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCollage: Parallel Generation of Large Content with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, Ming-Yu Liu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17076&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/dir/diffcollage/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Masked Diffusion Transformer is a Strong Image Synthesizer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shanghua Gao, Pan Zhou, Ming-Ming Cheng, Shuicheng Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14389&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sail-sg/MDT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Image-to-Video Generation with Latent Flow Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haomiao Ni, Changhao Shi, Kai Li, Sharon X. Huang, Martin Renqiang Min&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13744&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nihaomiao/CVPR23_LFDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12346&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://msra-nuwa.azurewebsites.net/#/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Object-Centric Slot Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jindong Jiang, Fei Deng, Gautam Singh, Sungjin Ahn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10834&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LDMVFI: Video Frame Interpolation with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Duolikun Danier, Fan Zhang, David Bull&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09508&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Diffusion Training via Min-SNR Weighting Strategy&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, Baining Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09556&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation&lt;/strong&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08320&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpretable ODE-style Generative Diffusion Model via Force Field Construction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weiyang Jin, Yongpei Zhu, Yuxi Peng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08063&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Regularized Vector Quantization for Tokenized Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiahui Zhang, Fangneng Zhan, Christian Theobalt, Shijian Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06424&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PARASOL: Parametric Style Control for Diffusion Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gemma Canet Tarrés, Dan Ruta, Tu Bui, John Collomosse&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06464&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain-Diffuser: Natural scene reconstruction from fMRI signals using generative latent diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Furkan Ozcelik, Rufin VanRullen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05334&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paul Hagemann, Lars Ruthotto, Gabriele Steidl, Nicole Tianjiao Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04772&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, Eric Gu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04248&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Diffusions in Augmented Spaces: A Complete Recipe&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kushagra Pandey, Stephan Mandt&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01748&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistency Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Prafulla Dhariwal, Mark Chen, Ilya Sutskever&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01469&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peiye Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing, Joshua M. Susskind, Miguel Ángel Bautista&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.00165&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Semantic Latent Directions in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yong-Hyun Park, Mingi Kwon, Junghyo Jo, Youngjung Uh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12469&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11552&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://energy-based-model.github.io/reduce-reuse-recycle/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning 3D Photography Videos via Self-supervised Diffusion on Single Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaodong Wang, Chenfei Wu, Shengming Yin, Minheng Ni, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Fan Yang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10781&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Calibrating Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianyu Pang, Cheng Lu, Chao Du, Min Lin, Shuicheng Yan, Zhijie Deng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10688&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/thudzj/Calibrated-DPMs&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zebin You, Yong Zhong, Fan Bao, Jiacheng Sun, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10586&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cross-domain Compositing with Pretrained Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roy Hachnochi, Mingrui Zhao, Nadav Orzech, Rinon Gal, Ali Mahdavi-Amiri, Daniel Cohen-Or, Amit Haim Bermano&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10167&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cross-domain-compositing/cross-domain-compositing&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoration based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaemoo Choi, Yesom Park, Myungjoo Kang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05456&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giannis Daras, Yuval Dagan, Alexandros G. Dimakis, Constantinos Daskalakis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.09057&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/giannisdaras/cdm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun Xiao, Mu Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08908&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video Probabilistic Diffusion Models in Projected Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sihyun Yu, Kihyuk Sohn, Subin Kim, Jinwoo Shin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07685&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sihyun.me/PVDM/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFaceSketch: High-Fidelity Face Image Synthesis with Sketch-Guided Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yichen Peng, Chunqi Zhao, Haoran Xie, Tsukasa Fukusato, Kazunori Miyata&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.06908&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where to Diffuse, How to Diffuse, and How to Get Back: Automated Learning for Multivariate Diffusions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Raghav Singhal, Mark Goldstein, Rajesh Ranganath&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Preconditioned Score-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Li Zhang, Hengyuan Ma, Xiatian Zhu, Jianfeng Feng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.06504&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://github.com/fudan-zvg/PDS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Star-Shaped Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrey Okhotin, Dmitry Molchanov, Vladimir Arkhipkin, Grigory Bartosh, Aibek Alanov, Dmitry Vetrov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05259&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04867&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://unipc.ivg-research.xyz&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wl-zhao/UniPC&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Geometry of Score Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sandesh Ghimire, Jinyang Liu, Armand Comas, Davin Hill, Aria Masoomi, Octavia Camps, Jennifer Dy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04411&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q-Diffusion: Quantizing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, Kurt Keutzer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04304&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PFGM++: Unlocking the Potential of Physics-Inspired Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04265&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Newbeeer/pfgmpp&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Long Horizon Temperature Scaling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andy Shih, Dorsa Sadigh, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03686&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spatial Functa: Scaling Functa to ImageNet Classification and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Schwarz, Hyunjik Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03130&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijian Zhang, Zhou Zhao, Jun Yu, Qi Tian&lt;/em&gt; &lt;br&gt; AAAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02373&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Divide and Compose with Score Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sandesh Ghimire, Armand Comas, Davin Hill, Aria Masoomi, Octavia Camps, Jennifer Dy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02272&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sandeshgh/Score-based-disentanglement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stable Target Field for Reduced Variance Score Estimation in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilun Xu, Shangyuan Tong, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00670&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Newbeeer/stf&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Yang, Yuwang Wang, Yan Lv, Nanning Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13721&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing DDPM Sampling with Shortcut Fine-Tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ying Fan, Kangwook Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13362&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Data Representations with Joint Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kamil Deja, Tomasz Trzcinski, Jakub M. Tomczak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13622&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengmeng Li, Luping Liu, Zenghao Chai, Runnan Li, Xu Tan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12935&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Don&#39;t Play Favorites: Minority Guidance for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Soobin Um, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12334&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sangyun884/fast-ode&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Guided Diffusion Sampling with Splitting Numerical Methods&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suttisak Wizadwongsa, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11558&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Input Perturbation Reduces Exposure Bias in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, Rita Cucchiara&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11706&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/forever208/DDPM-IP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Minimizing Trajectory Curvature of ODE-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sangyun Lee, Beomsu Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12003&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Importance of Noise Scheduling for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ting Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10972&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;simple diffusion: End-to-end diffusion for high resolution images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Jonathan Heek, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11093&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Inference in Denoising Diffusion Models via MMD Finetuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emanuele Aiello, Diego Valsesia, Enrico Magli&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.07969&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/diegovalsesia/MMD-DDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Transformer Backbones for Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Princy Chahal&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.14678&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijian Zhang, Zhou Zhao, Zhijie Lin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.12990&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scalable Adaptive Computation for Iterative Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Allan Jabri, David Fleet, Ting Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11972&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchically branched diffusion models for efficient and interpretable multi-class conditional generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex M. Tseng, Tommaso Biancalani, Max Shen, Gabriele Scalia&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10777&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, Baining Guo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09478&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/researchmm/MM-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scalable Diffusion Models with Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;William Peebles, Saining Xie&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09748&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.wpeebles.com/DiT&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyeongnyeon Kim, Wooseok Jang, Gyuseong Lee, Susung Hong, Junyoung Seo, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08861&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ku-cvlab.github.io/DAG/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Practical Plug-and-Play Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyojun Go, Yunsung Lee, Jin-Young Kim, Seunghyun Lee, Myeongho Jeong, Hyun Seung Lee, Seungtaek Choi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.05973&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Brain Decoding: from fMRI to conceptually similar image reconstruction of visual stimuli&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matteo Ferrante, Tommaso Boccato, Nicola Toschi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06726&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MAGVIT: Masked Generative Video Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, Lu Jiang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.05199&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://magvit.cs.cmu.edu/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 10 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyeongman Kim, Hajin Shim, Hyunsu Kim, Yunjey Choi, Junho Kim, Eunho Yang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02802&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Naoki Matsunaga, Masato Ishii, Akio Hayakawa, Kenji Suzuki, Takuya Narihira&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02024&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VIDM: Video Implicit Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kangfu Mei, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00235&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://kfmei.page/vidm/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MKFMIKU/VIDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why Are Conditional Generative Models Better Than Unconditional Ones?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00362&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity Guided Image Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaskirat Singh, Stephen Gould, Liang Zheng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.17084&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://1jsingh.github.io/gradop&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Continuous-time Discrete Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, Hanjun Dai&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16750&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wavelet Diffusion Models are fast and scalable Image Generators&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao Phung, Quan Dao, Anh Tran&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16152&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dimensionality-Varying Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Han Zhang, Ruili Feng, Zhantao Yang, Lianghua Huang, Yu Liu, Yifei Zhang, Yujun Shen, Deli Zhao, Jingren Zhou, Fan Cheng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16032&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongjun Kim, Yeongmin Kim, Wanmo Kang, Il-Chul Moon&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.17091&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Model Made Slim&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingyi Yang, Daquan Zhou, Jiashi Feng, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.17106&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Sampling of Diffusion Models via Operator Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, Anima Anandkumar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13449&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, Qifeng Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13221&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Example: Exemplar-based Image Editing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13227&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinDiffusion: Learning a Diffusion Model from a Single Natural Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/WeilunWang/SinDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Sampling with Classifier-based Feature Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wujie Sun, Defang Chen, Can Wang, Deshi Ye, Yan Feng, Chun Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12039&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SceneComposer: Any-Level Semantic Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Zeng, Zhe Lin, Jianming Zhang, Qing Liu, John Collomosse, Jason Kuen, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11742&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zengyu.me/scenec/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based Scene Graph to Image Generation with Masked Contrastive Pre-Training&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao Li, Wentao Zhang, Bin Cui, Bernard Ghanem, Ming-Hsuan Yang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11138&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinFusion: Training Diffusion Models on a Single Image or Video&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaniv Nikankin, Niv Haim, Michal Irani&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11743&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MagicVideo: Efficient Video Generation With Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, Jiashi Feng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11018&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://magicvideo.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, Juan Helen Zhou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06956&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mind-vis.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zjc062/mind-vis&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot Image Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.03264&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Denoising Diffusions to Denoising Markov Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.03595&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuyang-shi/generalized-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, Jun-Yan Zhu&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.02048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lmxyy/sige&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An optimal control perspective on diffusion-based generative modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julius Berner, Lorenz Richter, Karen Ullrich&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01364&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Entropic Neural Optimal Transport via Diffusion Processes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry Vetrov, Evgeny Burnaev&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01156&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; NeurIPS 2022 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2211.01095&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LuChengTHU/dpm-solver&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vikram Voleti, Christopher Pal, Adam Oberman&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12254&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Equilibrium Approaches to Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ashwini Pokle, Zhengyang Geng, Zico Kolter&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12867&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/locuslab/deq-ddim&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Representation Learning with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jeremias Traub&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.11058&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jeremiastraub/diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vincent Tao Hu, David W Zhang, Yuki M. Asano, Gertjan J. Burghouts, Cees G. M. Snoek&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.06462&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://taohu.me/sgdm/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GENIE: Higher-Order Denoising Diffusion Solvers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Dockhorn, Arash Vahdat, Karsten Kreis&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.05475&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/GENIE/&#34;&gt;Project&lt;/a&gt; [&lt;a href=&#34;https://github.com/nv-tlabs/GENIE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Angel Bautista, Josh Susskind&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.04955&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://jiataogu.me/fdm/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 10 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Distillation of Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.03142&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Sample Quality of Diffusion Model Using Self-Attention Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Susung Hong, Gyuseong Lee, Wooseok Jang, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.00939&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ku-cvlab.github.io/Self-Attention-Guidance/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 3 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OCD: Learning to Overfit with Conditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shahar Shlomo Lutati, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.00471&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ShaharLutatiPersonal/OCD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ali Borji&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.00586&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/aliborji/GFW&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising MCMC for Accelerating Diffusion-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Beomsu Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14593&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/1202kbs/DMCMC&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;All are Worth Words: a ViT Backbone for Score-based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Chongxuan Li, Yue Cao, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.12152&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Wavelet-domain Diffusion for 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ka-Hei Hui, Ruihui Li, Jingyu Hu, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08725&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can segmentation models be trained with fully synthetically generated data?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Petru-Daniel Tudosiu, Mark S Graham, Tom Vercauteren, M Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08256&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blurring Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.05557&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Soft Diffusion: Score Matching for General Corruptions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G. Dimakis, Peyman Milanfar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.05442&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Masked Image Generation with Token-Critic&lt;/strong&gt; &lt;br&gt; &lt;em&gt;José Lezama, Huiwen Chang, Lu Jiang, Irfan Essa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.04439&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let us Build Bridges: Understanding and Extending Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchao Liu, Lemeng Wu, Mao Ye, Qiang Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.14699&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu Cheng, Lu Yuan, Yu-Chiang Frank Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.13753&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adaptively-Realistic Image Generation from Stroke and Sketch with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shin-I Cheng, Yu-Jie Chen, Wei-Chen Chiu, Hsin-Ying Lee, Hung-Yu Tseng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.12675&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cyj407.github.io/DiSS/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09392&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arpitbansal297/Cold-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Roy Ganz, Michael Elad&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.08664&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your ViT is Secretly a Hybrid Discriminative-Generative Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiulong Yang, Sheng-Min Shih, Yinlin Fu, Xiaoting Zhao, Shihao Ji&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.07791&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sndnyang/Diffusion_ViT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Applying Regularized Schrödinger-Bridge-Based Stochastic Process in Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ki-Ung Song&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.07131&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KiUngSong/RSB&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ting Chen, Ruixiang Zhang, Geoffrey Hinton&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.04202&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pyramidal Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dohoon Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.01864&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sangyun Lee, Hyungjin Chung, Jaehyeon Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.11192&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sangyun884/blur-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Model Efficiency Through Patching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Troy Luhman, Eric Luhman&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.04316&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ericl122333/PatchDiffusion-Pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hengyuan Ma, Li Zhang, Xiatian Zhu, Jianfeng Feng&lt;/em&gt; &lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.02196&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPI-GAN: Distilling Score-based Generative Models with Straight-Path Interpolations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinsung Jeon, Noseong Park&lt;/em&gt; &lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.14464&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Entropy-driven Sampling and Training Scheme for Conditional Diffusion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengming Li, Guangcong Zheng, Hui Wang, Taiping Yao, Yang Chen, Shoudong Ding, Xi Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.11474&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Modelling With Inverse Heat Dissipation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Severi Rissanen, Markus Heinonen, Arno Solin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.13397&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://aaltoml.github.io/generative-inverse-heat-dissipation/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models as plug-and-play priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, Dimitris Samaras&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.09012&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alexgraikos/diffusion_priors&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Flexible Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weitao Du, Tao Yang, He Zhang, Yuanqi Du&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2206.10365&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lossy Compression with Gaussian Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lucas Theis, Tim Salimans, Matthew D. Hoffman, Fabian Mentzer&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.08889&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.08265&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LuChengTHU/mle_score_ode&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, Bo Zhang&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07309&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/baofff/Extended-Analytic-DPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Video Prediction and Infilling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, Andrea Dittadi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07696&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, Yan Yan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07771&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/L-YeZhu/CDCD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;gDDIM: Generalized denoising diffusion implicit models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Molei Tao, Yongxin Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.05564&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qsh-zh/gDDIM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How Much is Enough? A Study on Diffusion Times in Score-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giulio Franzese, Simone Rossi, Lixuan Yang, Alessandro Finamore, Dario Rossi, Maurizio Filippone, Pietro Michiardi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.05173&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Generation with Multimodal Priors using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.05039&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Score-based Generative Models for High-Resolution Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hengyuan Ma, Li Zhang, Xiatian Zhu, Jingfeng Zhang, Jianfeng Feng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.04029&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-GAN: Training GANs with Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.02262&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; NeurrIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00927&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LuChengTHU/dpm-solver&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Elucidating the Design Space of Diffusion-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Timo Aila, Samuli Laine&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00364&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kamil Deja, Anna Kuzina, Tomasz Trzciński, Jakub M. Tomczak&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00070&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-Shot Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giorgio Giannone, Didrik Nielsen, Ole Winther&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.15463&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Continuous Time Framework for Discrete Denoising Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.14987&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maximum Likelihood Training of Implicit Nonlinear Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, Il-Chul Moon&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.13699&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Models via Early Stop of the Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaoyang Lyu, Xudong XU, Ceyuan Yang, Dahua Lin, Bo Dai&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.12524&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flexible Diffusion Modeling of Long Videos&lt;/strong&gt; &lt;br&gt; &lt;em&gt;William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.11495&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/plai-group/flexible-video-diffusion-modeling&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vikram Voleti, Alexia Jolicoeur-Martineau, Christopher Pal&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.09853&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/voletiv/mcvd-pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Conditioning the Input Noise for Controlled Image Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vedant Singh, Surgan Jandial, Ayush Chopra, Siddharth Ramesh, Balaji Krishnamurthy, Vineeth N. Balasubramanian&lt;/em&gt; &lt;br&gt; CVPR Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.03859&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Subspace Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bowen Jing, Gabriele Corso, Renato Berlinghieri, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.01490&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bjing2016/subspace-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Sampling of Diffusion Models with Exponential Integrator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Yongxin Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.13902&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semi-Parametric Neural Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, Björn Ommer&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.11824&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J. Fleet&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.03458&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Perception Prioritized Training of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, Sungroh Yoon&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.00227&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jychoi118/P2-weighting&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating High Fidelity Data from Low-density Regions using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Ozgenel, Cristian Canton Ferrer&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.17260&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Counterfactual Explanations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Jeanneret, Loïc Simon, Frédéric Jurie&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.15636&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Likelihood Score Matching for Conditional Score-based Data Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, Chia-Ping Chen, Chun-Yi Lee&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.14206&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Modeling for Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruihan Yang, Prakhar Srivastava, Stephan Mandt&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.09481&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/buggyyang/rvd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dynamic Dual-Output Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaniv Benny, Lior Wolf&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.04304&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Simulation Using Diffusion Schrödinger Bridges&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.13460&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Causal Models for Counterfactual Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pedro Sanchez, Sotirios A. Tsaftaris&lt;/em&gt; &lt;br&gt; PMLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.10166&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pseudo Numerical Methods for Diffusion Models on Manifolds&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luping Liu, Yi Ren, Zhijie Lin, Zhou Zhao&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/luping-liu/PNDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Truncated Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.09671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding DDPM Latent Codes Through Optimal Transport&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Valentin Khrulkov, Ivan Oseledets&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.07477&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daniel Watson, William Chan, Jonathan Ho, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.05830&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion bridges vector quantized Variational AutoEncoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max Cohen, Guillaume Quispe, Sylvain Le Corff, Charles Ollion, Eric Moulines&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.04895&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Progressive Distillation for Fast Sampling of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Salimans, Jonathan Ho&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.00512&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Chongxuan Li, Jun Zhu, Bo Zhang&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.06503&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kushagra Pandey, Avideep Mukherjee, Piyush Rai, Abhishek Kumar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.00308&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/kpandey008/DiffuseVAE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Itô-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hideyuki Tachibana, Mocho Go, Muneyoshi Inahara, Yotaro Katayama, Yotaro Watanabe&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.13339&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen&lt;/em&gt; &lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.10741&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/glide-text2im&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Heavy-tailed denoising score matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jacob Deasy, Nikola Simidjievski, Pietro Liò&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.09788&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High Fidelity Visualization of What Your Self-Supervised Representation Knows About&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Florian Bordes, Randall Balestriero, Pascal Vincent&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.09164&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tackling the Generative Learning Trilemma with Denoising Diffusion GANs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhisheng Xiao, Karsten Kreis, Arash Vahdat&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.07804&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/denoising-diffusion-gan&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 15 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Generative Modeling with Critically-Damped Langevin Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Dockhorn, Arash Vahdat, Karsten Kreis&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.07068&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/CLD-SGM/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;More Control for Free! Image Synthesis with Semantic Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.05744&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Global Context with Discrete Diffusion in Vector Quantised Modelling for Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minghui Hu, Yujie Wang, Tat-Jen Cham, Jianfei Yang, P.N.Suganthan&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.01799&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Autoencoders: Toward a Meaningful and Decodable Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.15640&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diff-ae.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/phizaz/diffae&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Image Generation with Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.13606&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sam Bond-Taylor, Peter Hessey, Hiroshi Sasaki, Toby P. Breckon, Chris G. Willcocks&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.12701&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/samb-t/unleashing-transformers&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Normalizing Flow&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Yongxin Chen&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.07579&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qsh-zh/DiffFlow&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Gamma Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eliya Nachmani, Robin San Roman, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.05948&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Generative Neural Networks for Large-Scale Optimal Transport&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max Daniels, Tyler Maunu, Paul Hand&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.03237&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Generative Classifiers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roland S. Zimmermann, Lukas Schott, Yang Song, Benjamin A. Dunn, David A. Klindt&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.00473&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Classifier-Free Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Tim Salimans&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Sep 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bilateral Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max W. Y. Lam, Jun Wang, Rongjie Huang, Dan Su, Dong Yu&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.11514&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bilateral-denoising-diffusion-model.github.io&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Andreas Blattmann, Björn Ommer&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.08827&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/imagebart/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 19 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon&lt;/em&gt; &lt;br&gt; ICCV 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2108.02938&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jychoi118/ilvr_adm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2108.01073&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sde-image-editing.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/SDEdit&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Structured Denoising Diffusion Models in Discrete State-Spaces&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.03006&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Variational Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.00630&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/vdm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Priors In Variational Autoencoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Antoine Wehenkel, Gilles Louppe&lt;/em&gt; &lt;br&gt; ICML Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.15671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Generative Learning via Schrödinger Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, Can Yang&lt;/em&gt; &lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.10410&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Non Gaussian Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eliya Nachmani, Robin San Roman, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.07582&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://enk100.github.io/Non-Gaussian-Denoising-Diffusion-Models/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;D2C: Diffusion-Denoising Models for Few-shot Conditional Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Abhishek Sinha, Jiaming Song, Chenlin Meng, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.06819&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://d2c-model.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/d2c-model/d2c-model.github.io&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Generative Modeling in Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arash Vahdat, Karsten Kreis, Jan Kautz&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.05931&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning to Efficiently Sample from Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daniel Watson, Jonathan Ho, Mohammad Norouzi, William Chan&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.03802&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Variational Perspective on Diffusion-Based Generative Models and Score Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chin-Wei Huang, Jae Hyun Lim, Aaron Courville&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.02808&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CW-Huang/sdeflow-light&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Soft Truncation: A Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, Il-Chul Moon&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2106.05527&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Valentin De Bortoli, James Thornton, Jeremy Heng, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.01357&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jtt94.github.io/papers/schrodinger_bridge&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JTT94/diffusion_schrodinger_bridge&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Fast Sampling of Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhifeng Kong, Wei Ping&lt;/em&gt; &lt;br&gt; ICML Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.00132&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/FengNiMa/FastDPM_pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cascaded Diffusion Models for High Fidelity Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, Tim Salimans&lt;/em&gt; &lt;br&gt; JMLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.15282&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cascaded-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gotta Go Fast When Generating Data with Score-Based Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexia Jolicoeur-Martineau, Ke Li, Rémi Piché-Taillefer, Tal Kachman, Ioannis Mitliagkas&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.14080&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AlexiaJM/score_sde_fast_sampling&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models Beat GANs on Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Prafulla Dhariwal, Alex Nichol&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.05233&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Super-Resolution via Iterative Refinement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.07636&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://iterative-refinement.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Apr 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Noise Estimation for Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin San-Roman, Eliya Nachmani, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.02600&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Nichol, Prafulla Dhariwal&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.09672&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/improved-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Feb 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maximum Likelihood Training of Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Conor Durkan, Iain Murray, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.09258&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jan 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eric Luhman, Troy Luhman&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.02388&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tcl9876/Denoising_Student&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Jan 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Energy-Based Models by Diffusion Recovery Likelihood&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, Diederik P. Kingma&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.08125&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ruiqigao/recovery_likelihood&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Dec 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Generative Modeling through Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole&lt;/em&gt; &lt;br&gt; ICLR 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2011.13456&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yang-song/score_sde&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Nov 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Kun Xu, Chongxuan Li, Lanqing Hong, Jun Zhu, Bo Zhang&lt;/em&gt; &lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2010.08258&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Implicit Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaming Song, Chenlin Meng, Stefano Ermon&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2010.02502&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/ddim&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Oct 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial score matching and improved sampling for image generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Rémi Tachet des Combes, Ioannis Mitliagkas&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2009.05475&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AlexiaJM/AdversarialConsistentScoreMatching&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Sep 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Ajay Jain, Pieter Abbeel&lt;/em&gt; &lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hojonathanho/diffusion&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/pesser/pytorch_diffusion&#34;&gt;Github2&lt;/a&gt;] &lt;br&gt; 19 Jun 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Techniques for Training Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.09011&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/ncsnv2&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Jun 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Modeling by Estimating Gradients of the Data Distribution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS 2019. [&lt;a href=&#34;https://arxiv.org/abs/1907.05600&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yang-song.github.io/blog/2021/score/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/ncsn&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Jul 2019&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Belinda Tzen, Maxim Raginsky&lt;/em&gt; &lt;br&gt; arXiv 2019. [&lt;a href=&#34;https://arxiv.org/abs/1905.09883&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2019&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Unsupervised Learning using Nonequilibrium Thermodynamics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli&lt;/em&gt; &lt;br&gt; ICML 2015. [&lt;a href=&#34;https://arxiv.org/abs/1503.03585&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Mar 2015&lt;/p&gt; &#xA;&lt;h3&gt;Classification&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yingjun Du, Zehao Xiao, Shengcai Liao, Cees Snoek&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14770&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Masked Diffusion Models are Fast Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiachen Lei, Peng Cheng, Zhongjie Ba, Kui Ren&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11363&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Renderers are Good Zero-Shot Representation Learners: Exploring Diffusion Latents for Metric Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michael Tang, David Shustin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10721&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Big Data Myth: Using Diffusion Models for Dataset Generation to Train Deep Detection Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roy Voetman, Maya Aghaei, Klaas Dijkstra&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09762&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;When Hyperspectral Image Classification Meets Diffusion Models: An Unsupervised Feature Learning Framework&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyi Zhou, Jiamu Sheng, Jiayuan Fan, Peng Ye, Tong He, Bin Wang, Tao Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08964&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tal Daniel, Aviv Tamar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05957&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changyao Tian, Chenxin Tao, Jifeng Dai, Hao Li, Ziheng Li, Lewei Lu, Xiaogang Wang, Hongsheng Li, Gao Huang, Xizhou Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05423&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Generation from Unconditional Diffusion Models using Denoiser Representations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Graikos, Srikar Yellapragada, Dimitris Samaras&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01900&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, Xinxiao Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15957&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training on Thin Air: Improve Image Classification with Generated Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yongchao Zhou, Hshmat Sahak, Jimmy Ba&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15316&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/diffusion-inversion&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yongchao97/diffusion_inversion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheng Li, Yuxuan Li, Penghai Zhao, Renjie Song, Xiang Li, Jian Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12954&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhengli97/DM-KD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Human-Object Interaction Detection with Text-to-Image Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jie Yang, Bingliang Li, Fengyu Yang, Ailing Zeng, Lei Zhang, Ruimao Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12252&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Meta-DM: Applications of Diffusion Models on Few-Shot Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wentao Hu, Xiurong Jiang, Jiarun Liu, Yuqi Yang, Hui Tian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08092&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Class-Balancing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, Ya Zhang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00562&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic Data from Diffusion Models Improves ImageNet Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, David J. Fleet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08466&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OVTrack: Open-Vocabulary Multiple Object Tracking&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siyuan Li, Tobias Fischer, Lei Ke, Henghui Ding, Martin Danelljan, Fisher Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your Diffusion Model is Secretly a Zero-Shot Classifier&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, Deepak Pathak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16203&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusion-classifier.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Image Diffusion Models are Zero-Shot Classifiers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kevin Clark, Priyank Jaini&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15233&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nicola Franco, Daniel Korth, Jeanette Miriam Lorenz, Karsten Roscher, Stephan Guennemann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14961&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jordan J. Bird, Ahmad Lotfi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14126&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Autoencoders are Unified Self-supervised Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weilai Xiang, Hongyu Yang, Di Huang, Yunhong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09769&#34;&gt;Paper&lt;/a&gt;] )] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Zero-shot Classification with Synthetic Data Diversity via Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, Clinton Fookes&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03298&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fake it till you make it: Learning(s) from a synthetic ImageNet clone&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, Yannis Kalantidis&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.08420&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://europe.naverlabs.com/research/computer-vision/imagenet-sd/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffAlign : Few-shot learning using diffusion based synthesis and alignment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aniket Roy, Anshul Shah, Ketul Shah, Anirban Roy, Rama Chellappa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.05404&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luping Liu, Yi Ren, Xize Cheng, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11255&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/luping-liu/DiffOOD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionDet: Diffusion Model for Object Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shoufa Chen, Peize Sun, Yibing Song, Ping Luo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09788&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ShoufaChen/DiffusionDet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Models for Out-of-Distribution Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mark S. Graham, Walter H.L. Pinaya, Petru-Daniel Tudosiu, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.07740&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/marksgraham/ddpm-ood&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A simple, efficient and scalable contrastive masked autoencoder for learning visual representations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shlok Mishra, Joshua Robinson, Huiwen Chang, David Jacobs, Aaron Sarna, Aaron Maschinot, Dilip Krishnan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.16870&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Points to Functions: Infinite-dimensional Representations in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sarthak Mittal, Guillaume Lajoie, Stefan Bauer, Arash Mehrjou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.13774&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sarthmit/traj_drl&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boomerang: Local sampling on image manifolds using diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lorenzo Luzi, Ali Siahkoohi, Paul M Mayer, Josue Casco-Rodriguez, Richard Baraniuk&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12100&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/drive/1PV5Z6b14HYZNx1lHCaEVhId-Y4baKXwt&#34;&gt;Colab&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Meta-Learning via Classifier(-free) Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elvis Nava, Seijin Kobayashi, Yifei Yin, Robert K. Katzschmann, Benjamin F. Grewe&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.08942&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2022&lt;/p&gt; &#xA;&lt;h3&gt;Segmentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Better Certified Segmentation via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Othmane Laousy, Alexandre Araujo, Guillaume Chassagnon, Marie-Pierre Revel, Siddharth Garg, Farshad Khorrami, Maria Vakalopoulou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Zero-Shot Open-Vocabulary Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Laurynas Karazija, Iro Laina, Andrea Vedaldi, Christian Rupprecht&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tomer Amit, Shmuel Shichrur, Tal Shaharabany, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09004&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Semantic Communication: Diffusion Models Beyond Bit Recovery&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eleonora Grassucci, Sergio Barbarossa, Danilo Comminiello&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04321&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ispamm/GESCO&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinrong Hu, Yu-Jen Chen, Tsung-Yi Ho, Yiyu Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03878&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DFormer: Diffusion-guided Transformer for Universal Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hefeng Wang, Jiale Cao, Rao Muhammad Anwer, Jin Xie, Fahad Shahbaz Khan, Yanwei Pang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03437&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cp3wan/DFormer&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Semantic Segmentation with Mask Prior Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeqiang Lai, Yuchen Duan, Jifeng Dai, Ziheng Li, Ying Fu, Hongsheng Li, Yu Qiao, Wenhai Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01721&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Level Global Context Cross Consistency Model for Semi-Supervised Ultrasound Image Segmentation with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fenghe Tang, Jianrui Ding, Lingtao Wang, Min Xian, Chunping Ning&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09447&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/FengheTan9/Multi-Level-Global-Context-Cross-Consistency&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Stojanovski, Uxio Hermida, Pablo Lamata, Arian Beqiri, Alberto Gomez&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05424&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Personalize Segment Anything Model with One Shot&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZrrSkywalker/Personalize-SAM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Personalize Segment Anything Model with One Shot&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZrrSkywalker/Personalize-SAM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nurislam Tursynbek, Marc Niethammer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00067&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseExpand: Expanding dataset for 2D medical image segmentation using diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Shao, Xiaohan Yuan, Zhen Huang, Ziming Qiu, Shuai Wang, Kevin Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13416&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://anonymous.4open.science/r/DiffuseExpand/README.md&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Realistic Data Enrichment for Robust Image Segmentation in Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sarah Cechnicka, James Ball, Callum Arthurs, Candice Roufosse, Bernhard Kainz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09534&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Medical Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pham Ngoc Huy, Tran Minh Quan&lt;/em&gt; &lt;br&gt; IEEE ISBI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09383&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ambiguous Medical Image Segmentation using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aimon Rahman, Jeya Maria Jose Valanarasu, Ilker Hacihaliloglu, Vishal M Patel&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04745&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/aimansnigdha/Ambiguous-Medical-Image-Segmentation-using-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Chen, Chenhui Wang, Hongming Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04429&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haipeng Zhou, Lei Zhu, Yuyin Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12313&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral Fracture Grading&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthias Keicher, Matan Atad, David Schinz, Alexandra S. Gersing, Sarah C. Foreman, Sophia S. Goller, Juergen Weissinger, Jon Rischewski, Anna-Sophia Dietrich, Benedikt Wiestler, Jan S. Kirschke, Nassir Navab&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12031&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Koutilya Pnvr, Bharat Singh, Pallabi Ghosh, Behjat Siddiquie, David Jacobs&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12343&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, Chunhua Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11681&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://weijiawu.github.io/DiffusionMask/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Object-Centric Slot Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jindong Jiang, Fei Deng, Gautam Singh, Sungjin Ahn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10834&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-UNet: A Diffusion Embedded Network for Volumetric Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaohu Xing, Liang Wan, Huazhu Fu, Guang Yang, Lei Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10326&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ge-xing/Diff-UNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaofan Ma, Yuhuan Yang, Chen Ju, Fei Zhang, Jinxiang Liu, Yu Wang, Ya Zhang, Yanfeng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09813&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stochastic Segmentation with Conditional Categorical Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lukas Zbinden, Lars Doorenbos, Theodoros Pissas, Raphael Sznitman, Pablo Márquez-Neila&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08888&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LarsDoorenbos/ccdm-stochastic-segmentation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffBEV: Conditional Diffusion Model for Bird&#39;s Eye View Perception&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiayu Zou, Zheng Zhu, Yun Ye, Xingang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08333&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Importance of Aligning Training Strategy with Evaluation for Diffusion Models in 3D Multiclass Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunguan Fu, Yiwen Li, Shaheer U. Saeed, Matthew J. Clarkson, Yipeng Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06040&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mathpluscode/ImgX-DiffSeg&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model for Few-Shot Instance Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minh-Quan Le, Tam V. Nguyen, Trung-Nghia Le, Thanh-Toan Do, Minh N. Do, Minh-Triet Tran&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05105&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, Shalini De Mello&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04803&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jerryxu.net/ODISE/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, Yanwu Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11798&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionInst: Diffusion Model for Instance Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhangxuan Gu, Haoxing Chen, Zhuoer Xu, Jun Lan, Changhua Meng, Weiqiang Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02773&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chenhaoxing/DiffusionInst&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 DEc 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Class Segmentation from Aerial Views using Recursive Noise Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Benedikt Kolbeinsson, Krystian Mikolajczyk&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00787&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryan Burgert, Kanchana Ranasinghe, Xiang Li, Michael S. Ryoo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13224&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved HER2 Tumor Segmentation with Subtype Balancing using Deep Generative Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mathias Öttl, Jana Mönius, Matthias Rübner, Carol I. Geppert, Jingna Qiu, Frauke Wilm, Arndt Hartmann, Matthias W. Beckmann, Peter A. Fasching, Andreas Maier, Ramona Erber, Katharina Breininger&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06150&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junde Wu, Huihui Fang, Yu Zhang, Yehui Yang, Yanwu Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00611&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Models via Pre-segmentation Diffusion Sampling for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xutao Guo, Yanwu Yang, Chenfei Ye, Shang Lu, Yang Xiang, Ting Ma&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.17408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anatomically constrained CT image translation for heterogeneous blood vessel segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giammarco La Barbera, Haithem Boussaid, Francesco Maso, Sabine Sarnacki, Laurence Rouet, Pietro Gori, Isabelle Bloch&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.01713&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boah Kim, Yujin Oh, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14566&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can segmentation models be trained with fully synthetically generated data?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Petru-Daniel Tudosiu, Mark S Graham, Tom Vercauteren, M Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08256&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let us Build Bridges: Understanding and Extending Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchao Liu, Lemeng Wu, Mao Ye, Qiang Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.14699&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Image Synthesis via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.00050&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Remote Sensing Change Detection (Segmentation) using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wele Gedara Chaminda Bandara, Nithin Gopalakrishnan Nair, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.11892&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wgcban/ddpm-cd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models as plug-and-play priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, Dimitris Samaras&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.09012&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Walter H. L. Pinaya, Mark S. Graham, Robert Gray, Pedro F Da Costa, Petru-Daniel Tudosiu, Paul Wright, Yee H. Mah, Andrew D. MacKinnon, James T. Teo, Rolf Jager, David Werring, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardos&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.03461&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Decoder Denoising Pretraining for Semantic Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emmanuel Brempong Asiedu, Simon Kornblith, Ting Chen, Niki Parmar, Matthias Minderer, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.11423&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Implicit Image Segmentation Ensembles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe Valmaggia, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; MIDL 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.03145&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Label-Efficient Semantic Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, Artem Babenko&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.03126&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yandex-research/ddpm-segmentation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SegDiff: Image Segmentation with Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tomer Amit, Eliya Nachmani, Tal Shaharbany, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.00390&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, Max Welling&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.05379&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2021&lt;/p&gt; &#xA;&lt;h3&gt;Image Translation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14435&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ArtFusion: Controllable Arbitrary Style Transfer using Dual Conditional Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dar-Yen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09330&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenDarYen/ArtFusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, Volodymyr Kuleshov&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08757&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TryOnDiffusion: A Tale of Two UNets&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, Ira Kemelmacher-Shlizerman&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08276&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04396&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSketching: Sketch Control Image Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiang Wang, Di Kong, Fengyin Lin, Yonggang Qi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18812&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real-World Image Variation by Aligning Diffusion Inversion Chain&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuechen Zhang, Jinbo Xing, Eric Lo, Jiaya Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18729&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Photoswap: Personalized Subject Swapping in Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, Xin Eric Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18286&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://photoswap.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16289&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lisadunlap/ALIA&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unpaired Image-to-Image Translation via Neural Schr&#34;odinger Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Beomsu Kim, Gihyun Kwon, Kwanyoung Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15086&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cyclomon/UNSB&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SAR-to-Optical Image Translation via Thermodynamics-inspired Network&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingjin Zhang, Jiamin Xu, Chengyu He, Wenteng Shang, Yunsong Li, Xinbo Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13839&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Huang, Wenjing Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06710&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nulltextforcartoon.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NullTextforCartoon/NullTextforCartoon&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yupei Lin, Sen Zhang, Xiaojun Yang, Xiao Wang, Yukai Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04651&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yupeilin2388.github.io/publication/ReDiffuser&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeyu Lu, Chengyue Wu, Xinyuan Chen, Yaohui Wang, Yu Qiao, Xihui Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11829&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionRig: Learning Personalized Priors for Facial Appearance Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheng Ding, Xuaner Zhang, Zhihao Xia, Lars Jebe, Zhuowen Tu, Xiuming Zhang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06711&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusionrig.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adobe-research/diffusion-rig&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Animation with an Attribute-Guided Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bohan Zeng, Xuhui Liu, Sicheng Gao, Boyu Liu, Hong Li, Jianzhuang Liu, Baochang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03199&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reference-based Image Composition with Sketch via Structure-aware Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kangyeol Kim, Sunghyun Park, Junsoo Lee, Jaegul Choo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09748&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-free Style Transfer Emerges from h-space in Diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaeseok Jeong, Mingi Kwon, Youngjung Uh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15403&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://curryjung.github.io/DiffStyle/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/curryjung/DiffStyle_official&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Target Sampler for Unsupervised Domain Adaptation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yulong Zhang, Shuhao Chen, Yu Zhang, Jiangang Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12724&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyO: Stylize Your Face in Only One-Shot&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bonan Li, Zicheng Zhang, Xuecheng Nie, Congying Han, Yinhan Hu, Tiande Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03231&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shidong Cao, Wenhao Chai, Shengyu Hao, Yanting Zhang, Hangyue Chen, Gaoang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.06826&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I2SB: Image-to-Image Schrödinger Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A. Theodorou, Weili Nie, Anima Anandkumar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05872&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://i2sb.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zihao Wang, Yingyu Yang, Maxime Sermesant, Hervé Delingette, Ona Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13743&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFace: Diffusion-based Face Swapping with Facial Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kihong Kim, Yunho Kim, Seokju Cho, Junyoung Seo, Jisu Nam, Kychul Lee, Seungryong Kim, KwangHee Lee&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.13344&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://hxngiee.github.io/DiffFace/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 27 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HS-Diffusion: Learning a Semantic-Guided Diffusion Model for Head Swapping&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinghe Wang, Lijie Liu, Miao Hua, Qian He, Pengfei Zhu, Bing Cao, Qinghua Hu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06458&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inversion-Based Creativity Transfer with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, Changsheng Xu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.13203&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zyxElsa/InST&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Person Image Synthesis via Denoising Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Jorma Laaksonen, Mubarak Shah, Fahad Shahbaz Khan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12500&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Diffusion Models&#39; Latent Space, with Applications to CycleDiffusion and Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Henry Wu, Fernando De la Torre&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.05559&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/cycle-diffusion&#34;&gt;Github-1&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/unified-generative-zoo&#34;&gt;Github-2&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anatomically constrained CT image translation for heterogeneous blood vessel segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giammarco La Barbera, Haithem Boussaid, Francesco Maso, Sabine Sarnacki, Laurence Rouet, Pietro Gori, Isabelle Bloch&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.01713&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Image Translation using Disentangled Style and Content Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.15264&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyoung Seo, Gyuseong Lee, Seokju Cho, Jiyoung Lee, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.11047&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ku-cvlab.github.io/MIDMs/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ozan Özdenizci, Robert Legenstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.14626&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Non-Uniform Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.09786&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Medical Image Translation with Adversarial Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muzaffer Özbey, Salman UH Dar, Hasan A Bedel, Onat Dalmaz, Şaban Özturk, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08208&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Min Zhao, Fan Bao, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.06635&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, Yan Yan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07771&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/L-YeZhu/CDCD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pretraining is All You Need for Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, Fang Wen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.12952&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tengfei-wang.github.io/PITI/index.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PITI-Synthesis/PITI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VQBB: Image-to-image Translation with Vector Quantized Brownian Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Li, Kaitao Xue, Bin Liu, Yu-Kun Lai&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.07680&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.02641&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dual Diffusion Implicit Bridges for Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Su, Jiaming Song, Chenlin Meng, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08382&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Restoration Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Michael Elad, Stefano Ermon, Jiaming Song&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.11793&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseMorph: Unsupervised Deformable Image Registration Along Continuous Trajectory Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boah Kim, Inhwa Han, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.05149&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Autoencoders: Toward a Meaningful and Decodable Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.15640&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diff-ae.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Image Generation with Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.13606&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon&lt;/em&gt; &lt;br&gt; ICCV 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2108.02938&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jychoi118/ilvr_adm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hiroshi Sasaki, Chris G. Willcocks, Toby P. Breckon&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.05358&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2021&lt;/p&gt; &#xA;&lt;h3&gt;Inverse Problems&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Supervised MRI Reconstruction with Unrolled Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilmaz Korkmaz, Tolga Cukur, Vishal Patel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16654&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SVNR: Spatially-variant Noise Removal with Denoising Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Naama Pearl, Yaron Brodsky, Dana Berman, Assaf Zomet, Alex Rav Acha, Daniel Cohen-Or, Dani Lischinski&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16052&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Easing Color Shifts in Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Katherine Deck, Tobias Bischoff&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.15832&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model Based Low-Light Image Enhancement for Space Satellite&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiman Zhu, Lu Wang, Jingyi Yuan, Yu Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14227&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marco Aversa, Gabriel Nobis, Miriam Hägele, Kai Standvoss, Mihaela Chirica, Roderick Murray-Smith, Ahmed Alaa, Lukas Ruff, Daniela Ivanova, Wojciech Samek, Frederick Klauschen, Bruno Sanguinetti, Luis Oala&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13384&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wind Noise Reduction with a Diffusion-based Stochastic Regeneration Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jean-Marie Lemercier, Joachim Thiemann, Raphael Koning, Timo Gerkmann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12867&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseIR:Diffusion Models For Isotropic Reconstruction of 3D Microscopic Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingjie Pan, Yulu Gan, Fangxu Zhou, Jiaming Liu, Aimin Wang, Shanghang Zhang, Dawei Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12109&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HSR-Diff:Hyperspectral Image Super-Resolution via Conditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chanyue Wu, Dong Wang, Hanyu Mao, Ying Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12085&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Frédo Durand, William T. Freeman, Vincent Sitzmann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11719&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Ultrasound Denoising Using Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hojat Asgariandehkordi, Sobhan Goudarzi, Adrian Basarab, Hassan Rivaz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07440&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Visual Foundational Models of Physical Scenes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chethan Parameshwara, Alessandro Achille, Matthew Trager, Xiaolong Li, Jiawei Mo, Matthew Trager, Ashwin Swaminathan, CJ Taylor, Dheera Venkatraman, Xiaohan Fei, Stefano Soatto&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03727&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;INDigo: An INN-Guided Probabilistic Diffusion Algorithm for Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Di You, Andreas Floros, Pier Luigi Dragotti&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, David J. Fleet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01923&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dissecting Arbitrary-scale Super-resolution Capability from Pre-trained Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruibin Li, Qihua Zhou, Song Guo, Jie Zhang, Jingcai Guo, Xinyang Jiang, Yifei Shen, Zhenhua Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00714&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Low-Light Image Enhancement with Wavelet-based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hai Jiang, Ao Luo, Songchen Han, Haoqiang Fan, Shuaicheng Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00306&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Unified Conditional Framework for Diffusion-based Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Zhang, Xiaoyu Shi, Dasong Li, Xiaogang Wang, Jian Wang, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20049&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Direct Diffusion Bridge using Data Consistency for Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jeongsol Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19809&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gongye Liu, Haoze Sun, Jiayi Li, Fei Yin, Yujiu Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16965&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthew Chang, Aditya Prakash, Saurabh Gupta&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16301&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://matthewchang.github.io/vidm/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Diffusion Probabilistic Prior for Low-Dose CT Image Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15887&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiyang Ma, Huan Yang, Wenhan Yang, Jianlong Fu, Jiaying Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15357&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WaveDM: Wavelet-Based Diffusion Models for Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Huang, Jiancheng Huang, Jianzhuang Liu, Yu Dong, Jiaxi Lv, Shifeng Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13819&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dual-Diffusion: Dual Conditional Denoising Diffusion Probabilistic Models for Blind Super-Resolution Reconstruction in RSIs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengze Xu, Jie Ma, Yuanyuan Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12170&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Lincoln20030413/DDSR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, Ran Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11147&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pyramid Diffusion Models For Low-light Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dewei Zhou, Zongxin Yang, Yi Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10028&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruoqi Wang, Zhuoyang Chen, Qiong Luo, Feng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09121&#34;&gt;Paper&lt;/a&gt;] 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Models for Plug-and-Play Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, Luc Van Gool&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08995&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuanzhi-zhu/DiffPIR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploiting Diffusion Prior for Real-World Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K. Chan, Chen Change Loy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.07015&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://iceclear.github.io/projects/stablesr/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IceClear/StableSR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Atmospheric Turbulence Correction via Variational Deep Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xijun Wang, Santiago López-Tapia, Aggelos K. Katsaggelos&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05077&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Light Diffusion for Portraits&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Futschik, Kelvin Ritland, James Vecore, Sean Fanello, Sergio Orts-Escolano, Brian Curless, Daniel Sýkora, Rohit Pandey&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04745&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffBFR: Bootstrapping Diffusion Model Towards Blind Face Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinmin Qiu, Congying Han, ZiCheng Zhang, Bonan Li, Tiande Guo, Xuecheng Nie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04517&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real-World Denoising via Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Yang, Lijing Liang, Zhixun Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04457&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Variational Perspective on Solving Inverse Problems with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Morteza Mardani, Jiaming Song, Jan Kautz, Arash Vahdat&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04391&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Taofeng Xie, Chentao Cao, Zhuoxu Cui, Yu Guo, Caiying Wu, Xuemei Wang, Qingneng Li, Zhanli Hu, Tao Sun, Ziru Sang, Yihang Zhou, Yanjie Zhu, Dong Liang, Qiyu Jin, Guoqing Chen, Haifeng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03901&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DocDiff: Document Enhancement via Residual Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zongyuan Yang, Baolin Liu, Yongping Xiong, Lan Yi, Guibin Wu, Xiaojun Tang, Ziqi Liu, Junjie Zhou, Xing Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03892&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Royalvice/DocDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Asad Aali, Marius Arvinte, Sidharth Kumar, Jonathan I. Tamir&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01166&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-similarity-based super-resolution of photoacoustic angiography from hand-drawn doodles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanzheng Ma, Wangting Zhou, Rui Ma, Sihua Yang, Yansong Tang, Xun Guan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01165&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Diffusion Models as Principled Priors for Inverse Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Berthy T. Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L. Bouman, William T. Freeman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11751&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Diffusion-based Image Colorization via Piggybacked Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanyuan Liu, Jinbo Xing, Minshan Xie, Chengze Li, Tien-Tsin Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11105&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://piggyback-color.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiFaReli: Diffusion Face Relighting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Puntawat Ponglertnapakorn, Nontawat Tritrong, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09479&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusion-face-relighting.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inpaint Anything: Segment Anything Meets Image Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, Zhibo Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06790&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/geekyutao/Inpaint-Anything&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08291&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Algolzw/image-restoration-sde&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPIRiT-Diffusion: Self-Consistency Driven Diffusion Model for Accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Chentao Cao, Jing Cheng, Sen Jia, Hairong Zheng, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05060&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot CT Field-of-view Completion with Unconditional Generative Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaiwen Xu, Aravind R. Krishnan, Thomas Z. Li, Yuankai Huo, Kim L. Sandler, Fabien Maldonado, Bennett A. Landman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03760&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SketchFFusion: Sketch-guided image editing with diffusion model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weihang Mao, Bo Han, Zihao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03174&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inst-Inpaint: Instructing to Remove Objects with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut Erdem, Aysegul Dundar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03246&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://instinpaint.abyildirim.com/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Tommi Jaakkola, Shiyu Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03322&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UCSB-NLP-Chang/CoPaint/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Medical Image Translation via Frequency-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunxiang Li, Hua-Chieh Shao, Xiao Liang, Liyuan Chen, Ruiqi Li, Steve Jiang, Jing Wang, You Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02742&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Brian Moser, Stanislav Frolov, Federico Raue, Sebastian Palacio, Andreas Dengel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01994&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qi Gao, Zilong Li, Junping Zhang, Yi Zhang, Hongming Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01814&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Diffusion Prior for Unified Image Restoration and Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, Bo Dai&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Implicit Diffusion Models for Continuous Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yanjing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, Baochang Zhang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16491&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zalan Fabian, Berk Tinaz, Mahdi Soltanolkotabi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14353&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yizhuo Lu, Changde Du, Dianpeng Wang, Huiguang He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14139&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DisC-Diff: Disentangled Conditional Diffusion Model for Multi-Contrast MRI Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Mao, Lan Jiang, Xi Chen, Chao Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13933&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sub-volume-based Denoising Diffusion Probabilistic Model for Cone-beam CT Reconstruction from Incomplete Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Chuang Niu, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12861&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Perceptual Quality Assessment Exploration for AIGC Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zicheng Zhang, Chunyi Li, Wei Sun, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12618&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mauricio Delbracio, Peyman Milanfar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11435&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Neural Generation of 4K Masks for Homogeneous Diffusion Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Karl Schrader, Pascal Peter, Niklas Kämper, Joachim Weickert&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10096&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Post-Processing for Low-Light Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Savvas Panagiotou, Anna S. Bosman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09627&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SUD2: Supervision by Denoising Diffusion Models for Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthew A. Chan, Sean I. Young, Christopher A. Metzler&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09642&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffIR: Efficient Diffusion Model for Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, Luc Van Gool&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09472&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuyao Shang, Zhengyang Shan, Guangxing Liu, Jinglin Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08714&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jan Oscar Cross-Zamirski, Praveen Anand, Guy Williams, Elizabeth Mouchet, Yinhai Wang, Carola-Bibiane Schönlieb&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08863&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/crosszamirski/guided-I2I&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Contrast Harmonization of Magnetic Resonance Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alicia Durrer, Julia Wolleb, Florentin Bieder, Tim Sinnecker, Matthias Weigel, Robin Sandkühler, Cristina Granziera, Özgür Yaldizli, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08189&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing Realistic Image Restoration Training Pairs: A Diffusion Approach&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Yang, Peiran Ren, Xuansong xie, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06994&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhixin Wang, Xiaoyun Zhang, Ziying Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, Yanfeng Wang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06885&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuchun Miao, Lefei Zhang, Liangpei Zhang, Dacheng Tao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06682&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Suhyeon Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05754&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generalized Diffusion MRI Denoising and Super-Resolution using Swin Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amir Sadikov, Jamie Wren-Jarvis, Xinlei Pan, Lanya T. Cai, Pratik Mukherjee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05686&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiqun Duan, Zheng Zhu, Xianda Guo&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05021&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/duanyiqun/DiffusionDepth&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Enhancement From Degradation: A Diffusion Model For Fundus Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Puijin Cheng, Li Lin, Yijin Huang, Huaqing He, Wenhan Luo, Xiaoying Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04603&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/QtacierP/LED&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unlimited-Size Diffusion Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yinhuai Wang, Jiwen Yu, Runyi Yu, Jian Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.00354&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Out-of-Distribution Detection with Diffusion Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenzhen Liu, Jin Peng Zhou, Yufan Wang, Kilian Q. Weinberger&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10326&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoration based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaemoo Choi, Yesom Park, Myungjoo Kang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05456&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explicit Diffusion of Gaussian Mixture Model Based Image Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Martin Zach, Thomas Pock, Erich Kobler, Antonin Chambolle&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08411&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hshmat Sahak, Daniel Watson, Chitwan Saharia, David Fleet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07864&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Axi Niu, Kang Zhang, Trung X. Pham, Jinqiu Sun, Yu Zhu, In So Kweon, Yanning Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12831&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jacopo Teneggi, Matt Tivnan, J Webster Stayman, Jeremias Sulam&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03791&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tiange Xiang, Mahmut Yurt, Ali B Syed, Kawin Setsompop, Akshay Chaudhari&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03018&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/StanfordMIMI/DDM2&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model for Generative Image Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutong Xie, Minne Yuan, Bin Dong, Quanzheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02398&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Litu Rout, Advait Parulekar, Constantine Caramanis, Sanjay Shakkottai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01217&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Naoki Murata, Koichi Saito, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Yuki Mitsufuji, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12686&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Guided Diffusion Sampling with Splitting Numerical Methods&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suttisak Wizadwongsa, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11558&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Denoising for Low-Dose-CT Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Runyi Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11482&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Screen Space Indirect Lighting with Visibility Bitmask&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Olivier Therrien, Yannick Levesque, Guillaume Gilet&lt;/em&gt; &lt;br&gt; Visual Computer 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11376&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shangrong Yang, Chunyu Lin, Kang Liao, Yao Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11785&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RainDiffusion:When Unsupervised Learning Meets Diffusion Models for Real-world Image Deraining&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingqiang Wei, Yiyang Shen, Yongzhen Wang, Haoran Xie, Fu Lee Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.09430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingqiang Wei, Yiyang Shen, Yongzhen Wang, Haoran Xie, Fu Lee Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.09430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Removing Structured Noise with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tristan S.W. Stevens, Jean-Luc Robert, Faik C. Meral Jason Yu, Jun Seob Shin, Ruud J.G. van Sloun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05290&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Restoration with Mean-Reverting Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11699&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Algolzw/image-restoration-sde&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionCT: Latent Diffusion Model for CT Image Standardization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Md Selim, Jie Zhang, Michael A. Brooks, Ge Wang, Jin Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.08815&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Targeted Image Reconstruction by Sampling Pre-trained Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiageng Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.07557&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyutaek Oh, Jeong Eun Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03027&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Vision Transformers as Diffusion Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;He Cao, Jianan Wang, Tianhe Ren, Xianbiao Qi, Yihao Chen, Yuan Yao, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.13771&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Blind Watermarking: Combining Invertible and Non-invertible Mechanisms&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rui Ma, Mengxi Guo, Yi Hou, Fan Yang, Yuan Li, Huizhu Jia, Xiaodong Xie&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.12678&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rmpku/CIN&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bi-Noising Diffusion: Towards Conditional Diffusion Models with Generative Restoration Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kangfu Mei, Nithin Gopalakrishnan Nair, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07352&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://kfmei.page/bi-noising/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPIRiT-Diffusion: SPIRiT-driven Score-Based Generative Modeling for Vessel Wall imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chentao Cao, Zhuo-Xu Cui, Jing Cheng, Sen Jia, Hairong Zheng, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11274&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Universal Generative Modeling in Dual-domain for Dynamic MR Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chuanming Yu, Yu Guan, Ziwen Ke, Dong Liang, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07599&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DifFace: Blind Face Restoration with Diffused Error Contraction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zongsheng Yue, Chen Change Loy&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06512&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zsyOAOA/DifFace&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang, Yufei Wang, Hanspeter Pfister, Bihan Wen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04711&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Sample Diffusion Model in Projection Domain for Low-Dose CT Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bin Huang, Liu Zhang, Shiyu Lu, Boyu Lin, Weiwen Wu, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03630&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDM: Spatial Diffusion Model for Large Hole Image Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenbo Li, Xin Yu, Kun Zhou, Yibing Song, Zhe Lin, Jiaya Jia&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02963&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ADIR: Adaptive Diffusion for Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shady Abu-Hussein, Tom Tirer, Raja Giryes&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03221&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://shadyabh.github.io/ADIR/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Deblurring with Domain Generalizable Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido Gerig, Peyman Milanfar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.01789&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yinhuai Wang, Jiwen Yu, Jian Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00490&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wyhuai/DDNM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FREDSR: Fourier Residual Efficient Diffusive GAN for Single Image Super Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kyoungwan Woo, Achyuta Rajaram&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16678&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shichong Peng, Alireza Moazeni, Ke Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.14286&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaming Liu, Rushil Anirudh, Jayaraman J. Thiagarajan, Stewart He, K. Aditya Mohan, Ulugbek S. Kamilov, Hyojin Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12340&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangming Meng, Yoshiyuki Kabashima&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12343&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mengxiangming/dmps&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Parallel Diffusion Models of Operator and Image for Blind Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jeongsol Kim, Sehui Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10656&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Dohoon Ryu, Michael T. McCann, Marc L. Klasky, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10655&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Patch-Based Denoising Diffusion Probabilistic Model for Sparse-View CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10388&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Structure-Guided Diffusion Model for Large-Hole Diverse Image Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daichi Horita, Jiaolong Yang, Dong Chen, Yuki Koyama, Kiyoharu Aizawa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10437&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conffusion: Confidence Intervals for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eliahu Horwitz, Yedid Hoshen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09795&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Superresolution Reconstruction of Single Image for Latent features&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Wang, Jing-Ke Yan, Jing-Ye Cai, Jian-Hua Deng, Qin Qin, Qin Wang, Heng Xiao, Yao Cheng, Peng-Fei Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12845&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning to Kindle the Starlight&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Yuan, Jiaqi Wu, Lindong Wang, Zhongliang Jing, Henry Leung, Shuyuan Zhu, Han Pan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09206&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ShadowDiffusion: Diffusion-based Shadow Removal using Classifier-driven Attention and Structure Preservation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yeying Jin, Wenhan Yang, Wei Ye, Yuan Yuan, Robby T. Tan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.08089&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DriftRec: Adapting diffusion models to blind image restoration tasks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Simon Welker, Henry N. Chapman, Timo Gerkmann&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06757&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Denoising Diffusions to Denoising Markov Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.03595&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuyang-shi/generalized-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quantized Compressed Sensing with Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangming Meng, Yoshiyuki Kabashima&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13006&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mengxiangming/QCS-SGM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intelligent Painter: Picture Composition With Resampling Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wing-Fung Ku, Wan-Chi Siu, Xi Cheng, H. Anthony Chan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.17106&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pouria Rouzrokh, Bardia Khosravi, Shahriar Faghani, Mana Moassefi, Sanaz Vahdati, Bradley J. Erickson&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12113&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Mayo-Radiology-Informatics-Lab/MBTI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.08573&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Low-Dose CT Using Denoising Diffusion Probabilistic Model for 20× Speedup&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Qing Lyu, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.15136&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Posterior Sampling for General Noisy Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14687&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DPS2022/diffusion-posterior-sampling&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Super-Resolution Using Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marcelo dos Santos, Rayson Laroca, Rafael O. Ribeiro, João Neves, Hugo Proença, David Menotti&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.12064&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/marcelowds/sr-sde&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JPEG Artifact Correction using Denoising Diffusion Restoration Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Jiaming Song, Stefano Ermon, Michael Elad&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.11888&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;T2V-DDPM: Thermal to Visible Face Translation using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08814&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Delving Globally into Texture and Structure for Image Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haipeng Liu, Yang Wang, Meng Wang, Yong Rui&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08217&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/htyjers/DGTS-Inpainting&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PET image denoising based on denoising diffusion probabilistic models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kuang Gong, Keith A. Johnson, Georges El Fakhri, Quanzheng Li, Tinsu Pan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.06167&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Score: Self-Supervised Learning on Score-Based Models for MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Chentao Cao, Shaonan Liu, Qingyong Zhu, Jing Cheng, Haifeng Wang, Yanjie Zhu, Dong Liang&lt;/em&gt; &lt;br&gt; IEEE TMI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.00835&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AT-DDPM: Restoring Faces degraded by Atmospheric Turbulence using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Kangfu Mei, Vishal M Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.11284&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09392&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arpitbansal297/Cold-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Frequency Space Diffusion Models for Accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chentao Cao, Zhuo-Xu Cui, Shaonan Liu, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.05481&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ozan Özdenizci, Robert Legenstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.14626&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IGITUGraz/WeatherDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Non-Uniform Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.09786&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Medical Image Translation with Adversarial Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muzaffer Özbey, Salman UH Dar, Hasan A Bedel, Onat Dalmaz, Şaban Özturk, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08208&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adaptive Diffusion Priors for Accelerated MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Salman UH Dar, Şaban Öztürk, Yilmaz Korkmaz, Gokberk Elmas, Muzaffer Özbey, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.05876&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangxi Meng, Yuning Gu, Yongsheng Pan, Nizhuan Wang, Peng Xue, Mengkang Lu, Xuming He, Yiqiang Zhan, Dinggang Shen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.03430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SAR Despeckling using a Denoising Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Malsha V. Perera, Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.04514&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Models for Inverse Problems using Manifold Constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00941&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.02641&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Eun Sun Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.12621&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards performant and reliable undersampled MR reconstruction via diffusion model sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Peng, Pengfei Guo, S. Kevin Zhou, Vishal Patel, Rama Chellappa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.04292&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cpeng93/diffuserecon&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutong Xie, Quanzheng Li&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.03623&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Theodore-PKU/MC-DDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MRI Reconstruction via Data Driven Markov Chain with Joint Uncertainty Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guanxiong Luo, Martin Heide, Martin Uecker&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.01479&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mrirecon/spreco&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Denoising of Retinal OCT with Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dewei Hu, Yuankai K. Tao, Ipek Oguz&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.11760&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DeweiHu/OCT_DDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Restoration Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Michael Elad, Stefano Ermon, Jiaming Song&lt;/em&gt; &lt;br&gt; ICLR 2022 Workshop (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2201.11793&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RePaint: Inpainting using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, Luc Van Gool&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.09865&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/andreas128/RePaint&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kushagra Pandey, Avideep Mukherjee, Piyush Rai, Abhishek Kumar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.00308&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/kpandey008/DiffuseVAE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Byeongsu Sim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.05146&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deblurring via Stochastic Refinement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, Peyman Milanfar&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.02475&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Image Generation with Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.13606&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems in Medical Imaging with Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Liyue Shen, Lei Xing, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.08005&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yang-song/score_inverse_problems&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;S3RP: Self-Supervised Super-Resolution and Prediction for Advection-Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chulin Wang, Kyongmin Yeo, Xiao Jin, Andres Codas, Levente J. Klein, Bruce Elmegreen&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.04639&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based diffusion models for accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jong chul Ye&lt;/em&gt; &lt;br&gt; MIA 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.05243&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HJ-harry/score-MRI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Autoregressive Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.02037&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon&lt;/em&gt; &lt;br&gt; ICCV 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2108.02938&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jychoi118/ilvr_adm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cascaded Diffusion Models for High Fidelity Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.15282&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cascaded-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhihai Xu, Qi Li, Yueting Chen&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.14951&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Apr 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Super-Resolution via Iterative Refinement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.07636&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://iterative-refinement.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Apr 2021&lt;/p&gt; &#xA;&lt;h3&gt;Medical Imaging&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Supervised MRI Reconstruction with Unrolled Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilmaz Korkmaz, Tolga Cukur, Vishal Patel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16654&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DoseDiff: Distance-aware Diffusion Model for Dose Prediction in Radiotherapy&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiwen Zhang, Chuanpu Li, Liming Zhong, Zeli Chen, Wei Yang, Xuetao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16324&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffMix: Diffusion Model-based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyun-Jic Oh, Won-Ki Jeong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14132&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marco Aversa, Gabriel Nobis, Miriam Hägele, Kai Standvoss, Mihaela Chirica, Roderick Murray-Smith, Ahmed Alaa, Lukas Ruff, Daniela Ivanova, Wojciech Samek, Frederick Klauschen, Bruno Sanguinetti, Luis Oala&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13384&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseIR:Diffusion Models For Isotropic Reconstruction of 3D Microscopic Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingjie Pan, Yulu Gan, Fangxu Zhou, Jiaming Liu, Aimin Wang, Shanghang Zhang, Dawei Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12109&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TauPETGen: Text-Conditional Tau PET Image Synthesis Based on Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Se-In Jang, Cristina Lois, Emma Thibault, J. Alex Becker, Yafei Dong, Marc D. Normandin, Julie C. Price, Keith A. Johnson, Georges El Fakhri, Kuang Gong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11984&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shenghuan Sun, Gregory M. Goldgof, Atul Butte, Ahmed M. Alaa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12438&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tomer Amit, Shmuel Shichrur, Tal Shaharabany, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09004&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Ultrasound Denoising Using Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hojat Asgariandehkordi, Sobhan Goudarzi, Adrian Basarab, Hassan Rivaz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07440&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinrong Hu, Yu-Jen Chen, Tsung-Yi Ho, Yiyu Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03878&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpretable Alzheimer&#39;s Disease Classification Via a Contrastive Diffusion Autoencoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayodeji Ijishakin, Ahmed Abdulaal, Adamos Hadjivasiliou, Sophie Martin, James Cole&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03022&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing Sampling Patterns for Compressed Sensing MRI with Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sriram Ravula, Brett Levac, Ajil Jalal, Jonathan I. Tamir, Alexandros G. Dimakis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03284&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain tumor segmentation using synthetic MR images -- A comparison of GANs and diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muhammad Usman Akbar, Måns Larsson, Anders Eklund&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02986&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Anomaly Detection in Medical Images Using Masked Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hasan Iqbal, Umar Khalid, Jing Hua, Chen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19867&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cosmin I. Bercea, Michael Neumayr, Daniel Rueckert, Julia A. Schnabel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19643&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic CT Generation from MRI using 3D Transformer-based Denoising Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoyan Pan, Elham Abouei, Jacob Wynne, Tonghe Wang, Richard L. J. Qiu, Yuheng Li, Chih-Wei Chang, Junbo Peng, Justin Roper, Pretesh Patel, David S. Yu, Hui Mao, Xiaofeng Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19467&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Models for Semantic 3D Medical Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zolnamar Dorjsembe, Hsing-Kuo Pao, Sodtavilan Odonchimed, Furen Xiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18453&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GenerateCT: Text-Guided 3D Chest CT Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ibrahim Ethem Hamamci, Sezgin Er, Enis Simsar, Alperen Tezcan, Ayse Gulnihan Simsek, Furkan Almas, Sevval Nil Esirgun, Hadrien Reynaud, Sarthak Pati, Christian Bluethgen, Bjoern Menze&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16037&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ibrahimethemhamamci/GenerateCT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Diffusion Probabilistic Prior for Low-Dose CT Image Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15887&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Level Global Context Cross Consistency Model for Semi-Supervised Ultrasound Image Segmentation with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fenghe Tang, Jianrui Ding, Lingtao Wang, Min Xian, Chunping Ning&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09447&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/FengheTan9/Multi-Level-Global-Context-Cross-Consistency&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muhammad Usman Akbar, Wuhao Wang, Anders Eklund&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.07644&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generation of Structurally Realistic Retinal Fundus Images with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sojung Go, Younghoon Ji, Sang Jun Park, Soochahn Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06813&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Stojanovski, Uxio Hermida, Pablo Lamata, Arian Beqiri, Alberto Gomez&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05424&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Taofeng Xie, Chentao Cao, Zhuoxu Cui, Yu Guo, Caiying Wu, Xuemei Wang, Qingneng Li, Zhanli Hu, Tao Sun, Ziru Sang, Yihang Zhou, Yanjie Zhu, Dong Liang, Qiyu Jin, Guoqing Chen, Haifeng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03901&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Asad Aali, Marius Arvinte, Sidharth Kumar, Jonathan I. Tamir&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01166&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-similarity-based super-resolution of photoacoustic angiography from hand-drawn doodles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanzheng Ma, Wangting Zhou, Rui Ma, Sihua Yang, Yansong Tang, Xun Guan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01165&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity Image Synthesis from Pulmonary Nodule Lesion Maps using Semantic Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Zhao, Benjamin Hou&lt;/em&gt; &lt;br&gt; MIDL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01138&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nurislam Tursynbek, Marc Niethammer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00067&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cycle-guided Denoising Diffusion Probability Model for 3D Cross-modality MRI Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoyan Pan, Chih-Wei Chang, Junbo Peng, Jiahan Zhang, Richard L.J. Qiu, Tonghe Wang, Justin Roper, Tian Liu, Hui Mao, Xiaofeng Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00042&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseExpand: Expanding dataset for 2D medical image segmentation using diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Shao, Xiaohan Yuan, Zhen Huang, Ziming Qiu, Shuai Wang, Kevin Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13416&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://anonymous.4open.science/r/DiffuseExpand/README.md&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Realistic Data Enrichment for Robust Image Segmentation in Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sarah Cechnicka, James Ball, Callum Arthurs, Candice Roufosse, Bernhard Kainz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09534&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Medical Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pham Ngoc Huy, Tran Minh Quan&lt;/em&gt; &lt;br&gt; IEEE ISBI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09383&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Multi-Institutional Open-Source Benchmark Dataset for Breast Cancer Clinical Decision Support using Synthetic Correlated Diffusion Imaging Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chi-en Amy Tai, Hayden Gunraj, Alexander Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05623&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cancer-Net BCa-S: Breast Cancer Grade Prediction using Volumetric Deep Radiomic Features from Synthetic Correlated Diffusion Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chi-en Amy Tai, Hayden Gunraj, Alexander Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05899&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPIRiT-Diffusion: Self-Consistency Driven Diffusion Model for Accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Chentao Cao, Jing Cheng, Sen Jia, Hairong Zheng, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05060&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mask-conditioned latent diffusion for generating gastrointestinal polyp images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roman Macháček, Leila Mozaffari, Zahra Sepasdar, Sravanthi Parasa, Pål Halvorsen, Michael A. Riegler, Vajira Thambawita&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05233&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Chen, Chenhui Wang, Hongming Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04429&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ambiguous Medical Image Segmentation using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aimon Rahman, Jeya Maria Jose Valanarasu, Ilker Hacihaliloglu, Vishal M Patel&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04745&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/aimansnigdha/Ambiguous-Medical-Image-Segmentation-using-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kun Han, Yifeng Xiong, Chenyu You, Pooya Khosravi, Shanlin Sun, Xiangyi Yan, James Duncan, Xiaohui Xie&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04106&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://krishan999.github.io/MedGen3D/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Realistic Ultrasound Fetal Brain Imaging Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michelle Iskandar, Harvey Mannering, Zhanxiang Sun, Jacqueline Matthew, Hamideh Kerdegari, Laura Peralta, Miguel Xochicale&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03941&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/budai4medtech/midl2023&#34;&gt;Gitub&lt;/a&gt;] &lt;br&gt; 8 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot CT Field-of-view Completion with Unconditional Generative Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaiwen Xu, Aravind R. Krishnan, Thomas Z. Li, Yuankai Huo, Kim L. Sandler, Fabien Maldonado, Bennett A. Landman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03760&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Medical Image Translation via Frequency-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunxiang Li, Hua-Chieh Shao, Xiao Liang, Liyuan Chen, Ruiqi Li, Steve Jiang, Jing Wang, You Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02742&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qi Gao, Zilong Li, Junping Zhang, Yi Zhang, Hongming Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01814&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology Image Analysis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Xu, Saarthak Kapse, Rajarsi Gupta, Prateek Prasanna&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01053&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pay Attention: Accuracy Versus Interpretability Trade-off in Fine-tuned Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mischa Dombrowski, Hadrien Reynaud, Johanna P. Müller, Matthew Baugh, Bernhard Kainz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17908&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDMM-Synth: A Denoising Diffusion Model for Cross-modal Medical Image Synthesis with Sparse-view Measurement Embedding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoyue Li, Kai Shang, Gaoang Wang, Mark D. Butala&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15770&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Memory-efficient Processing of 3D Medical Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Florentin Bieder, Julia Wolleb, Alicia Durrer, Robin Sandkühler, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; MIDL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15288&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaofei Wang, Stephen Price, Chao Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14845&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoLa-Diff: Conditional Latent Diffusion Model for Multi-Modal MRI Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lan Jiang, Ye Mao, Xi Chen, Xiangfeng Wang, Chao Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14081&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DisC-Diff: Disentangled Conditional Diffusion Model for Multi-Contrast MRI Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Mao, Lan Jiang, Xi Chen, Chao Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13933&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Medical diffusion on a budget: textual inversion for medical image generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bram de Wilde, Anindo Saha, Richard P.G. ten Broek, Henkjan Huisman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sub-volume-based Denoising Diffusion Probabilistic Model for Cone-beam CT Reconstruction from Incomplete Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Chuang Niu, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12861&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hadrien Reynaud, Mengyun Qiao, Mischa Dombrowski, Thomas Day, Reza Razavi, Alberto Gomez, Paul Leeson, Bernhard Kainz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12644&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haipeng Zhou, Lei Zhu, Yuyin Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12313&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral Fracture Grading&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthias Keicher, Matan Atad, David Schinz, Alexandra S. Gersing, Sarah C. Foreman, Sophia S. Goller, Juergen Weissinger, Jon Rischewski, Anna-Sophia Dietrich, Benedikt Wiestler, Jan S. Kirschke, Nassir Navab&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12031&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aman Shrivastava, P. Thomas Fletcher&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11477&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cascaded Latent Diffusion Models for High-Resolution Chest X-ray Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tobias Weber, Michael Ingrisch, Bernd Bischl, David Rügamer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11224&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yijun Yang, Huazhu Fu, Angelica Aviles-Rivero, Carola-Bibiane Schönlieb, Lei Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10610&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-UNet: A Diffusion Embedded Network for Volumetric Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaohu Xing, Liang Wan, Huazhu Fu, Guang Yang, Lei Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10326&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ge-xing/Diff-UNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cosmin I Bercea, Benedikt Wiestler, Daniel Rueckert, Julia A Schnabel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08452&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suhyeon Lee, Hyungjin Chung, Minyoung Park, Jonghyuk Park, Wi-Sun Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08440&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jan Oscar Cross-Zamirski, Praveen Anand, Guy Williams, Elizabeth Mouchet, Yinhai Wang, Carola-Bibiane Schönlieb&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08863&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/crosszamirski/guided-I2I&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stochastic Segmentation with Conditional Categorical Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lukas Zbinden, Lars Doorenbos, Theodoros Pissas, Raphael Sznitman, Pablo Márquez-Neila&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08888&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LarsDoorenbos/ccdm-stochastic-segmentation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Contrast Harmonization of Magnetic Resonance Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alicia Durrer, Julia Wolleb, Florentin Bieder, Tim Sinnecker, Matthias Weigel, Robin Sandkühler, Cristina Granziera, Özgür Yaldizli, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08189&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer&#39;s Disease Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikhil J. Dhinagar, Sophia I. Thomopoulos, Emily Laltoo, Paul M. Thompson&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08216&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based Hierarchical Multi-Label Object Detection to Analyze Panoramic Dental X-rays&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ibrahim Ethem Hamamci, Sezgin Er, Enis Simsar, Anjany Sekuboyina, Mustafa Gundogar, Bernd Stadlinger, Albert Mehl, Bjoern Menze&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06500&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AugDiff: Diffusion based Feature Augmentation for Multiple Instance Learning in Whole Slide Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuchen Shao, Liuxi Dai, Yifeng Wang, Haoqian Wang, Yongbing Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06371&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuhang Chen, Baiying Lei, Chi-Man Pun, Shuqiang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06410&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Suhyeon Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05754&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generalized Diffusion MRI Denoising and Super-Resolution using Swin Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amir Sadikov, Jamie Wren-Jarvis, Xinlei Pan, Lanya T. Cai, Pratik Mukherjee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05686&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Importance of Aligning Training Strategy with Evaluation for Diffusion Models in 3D Multiclass Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunguan Fu, Yiwen Li, Shaheer U. Saeed, Matthew J. Clarkson, Yipeng Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06040&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mathpluscode/ImgX-DiffSeg&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Patched Diffusion Models for Unsupervised Anomaly Detection in Brain MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Finn Behrendt, Debayan Bhattacharya, Julia Krüger, Roland Opfer, Alexander Schlaefer&lt;/em&gt; &lt;br&gt; MIDL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03758&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bi-parametric prostate MR image synthesis using pathology and sequence-conditioned stable diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaheer U. Saeed, Tom Syer, Wen Yan, Qianye Yang, Mark Emberton, Shonit Punwani, Matthew J. Clarkson, Dean C. Barratt, Yipeng Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.02094&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jian Shi, Pengyi Zhang, Ni Zhang, Hakim Ghazzai, Yehia Massoud&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.14696&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tiange Xiang, Mahmut Yurt, Ali B Syed, Kawin Setsompop, Akshay Chaudhari&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03018&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/StanfordMIMI/DDM2&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zihao Wang, Yingyu Yang, Maxime Sermesant, Hervé Delingette, Ona Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13743&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Denoising for Low-Dose-CT Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Runyi Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11482&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionCT: Latent Diffusion Model for CT Image Standardization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Md Selim, Jie Zhang, Michael A. Brooks, Ge Wang, Jin Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.08815&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, Yanwu Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11798&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The role of noise in denoising models for anomaly detection in medical images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Antanas Kascenas, Pedro Sanchez, Patrick Schrempf, Chaoyang Wang, William Clackett, Shadia S. Mikhael, Jeremy P. Voisey, Keith Goatman, Alexander Weir, Nicolas Pugeault, Sotirios A. Tsaftaris, Alison Q. O&#39;Neil&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.08330&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AntanasKascenas/DenoisingAE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohamed Akrout, Bálint Gyepesi, Péter Holló, Adrienn Poór, Blága Kincső, Stephen Solis, Katrina Cirone, Jeremy Kawahara, Dekker Slade, Latif Abid, Máté Kovács, István Fazekas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.04802&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyutaek Oh, Jeong Eun Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03027&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dennis Eschweiler, Johannes Stegmaier&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10227&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shizhan Gong, Cheng Chen, Yuqi Gong, Nga Yan Chan, Wenao Ma, Calvin Hoi-Kwan Mak, Jill Abrigo, Qi Dou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.00409&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SADM: Sequence-Aware Diffusion Model for Longitudinal Medical Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jee Seok Yoon, Chenghao Zhang, Heung-Il Suk, Jia Guo, Xiaoxiao Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08228&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Universal Generative Modeling in Dual-domain for Dynamic MR Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chuanming Yu, Yu Guan, Ziwen Ke, Dong Liang, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07599&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Realistic 3D Brain MRIs Using a Conditional Diffusion Probabilistic Model&lt;/strong&gt; \ &lt;em&gt;Wei Peng, Ehsan Adeli, Qingyu Zhao, Kilian M. Pohl&lt;/em&gt; \ arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08034&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Project-MONAI/GenerativeModels/tree/260-add-cdpm-model&#34;&gt;Github&lt;/a&gt;] \ 15 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPIRiT-Diffusion: SPIRiT-driven Score-Based Generative Modeling for Vessel Wall imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chentao Cao, Zhuo-Xu Cui, Jing Cheng, Sen Jia, Hairong Zheng, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11274&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Models beat GANs on Medical Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gustav Müller-Franzes, Jan Moritz Niehues, Firas Khader, Soroosh Tayebi Arasteh, Christoph Haarburger, Christiane Kuhl, Tianci Wang, Tianyu Han, Sven Nebelung, Jakob Nikolas Kather, Daniel Truhn&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07501&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Sample Diffusion Model in Projection Domain for Low-Dose CT Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bin Huang, Liu Zhang, Shiyu Lu, Boyu Lin, Weiwen Wu, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03630&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Cell Video Synthesis via Optical-Flow Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Manuel Serna-Aguilera, Khoa Luu, Nathaniel Harris, Min Zou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03250&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving dermatology classifiers across populations using images generated by large diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luke W. Sagers, James A. Diao, Matthew Groh, Pranav Rajpurkar, Adewole S. Adamson, Arjun K. Manrai&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13352&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RoentGen: Vision-Language Foundation Model for Chest X-ray Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pierre Chambon, Christian Bluethgen, Jean-Benoit Delbrouck, Rogier Van der Sluijs, Małgorzata Połacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis P. Langlotz, Akshay Chaudhari&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12737&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaming Liu, Rushil Anirudh, Jayaraman J. Thiagarajan, Stewart He, K. Aditya Mohan, Ulugbek S. Kamilov, Hyojin Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12340&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Dohoon Ryu, Michael T. McCann, Marc L. Klasky, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10655&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Patch-Based Denoising Diffusion Probabilistic Model for Sparse-View CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10388&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain PET Synthesis from MRI Using Joint Probability Distribution of Diffusion Model at Ultrahigh Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xie Taofeng, Cao Chentao, Cui Zhuoxu, Li Fanshi, Wei Zidong, Zhu Yanjie, Li Ye, Liang Dong, Jin Qiyu, Chen Guoqing, Wang Haifeng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.08901&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved HER2 Tumor Segmentation with Subtype Balancing using Deep Generative Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mathias Öttl, Jana Mönius, Matthias Rübner, Carol I. Geppert, Jingna Qiu, Frauke Wilm, Arndt Hartmann, Matthias W. Beckmann, Peter A. Fasching, Andreas Maier, Ramona Erber, Katharina Breininger&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06150&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An unobtrusive quality supervision approach for medical image annotation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sonja Kunzmann, Mathias Öttl, Prathmesh Madhu, Felix Denzinger, Andreas Maier&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06146&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Medical Diffusion -- Denoising Diffusion Probabilistic Models for 3D Medical Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Firas Khader, Gustav Mueller-Franzes, Soroosh Tayebi Arasteh, Tianyu Han, Christoph Haarburger, Maximilian Schulze-Hagen, Philipp Schad, Sandy Engelhardt, Bettina Baessler, Sebastian Foersch, Johannes Stegmaier, Christiane Kuhl, Sven Nebelung, Jakob Nikolas Kather, Daniel Truhn&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.03364&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generation of Anonymous Chest Radiographs Using Latent Diffusion Models for Training Thoracic Abnormality Classification Systems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Packhäuser, Lukas Folle, Florian Thamm, Andreas Maier&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01323&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spot the fake lungs: Generating Synthetic Medical Images using Neural Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hazrat Ali, Shafaq Murad, Zubair Shah&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00902&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.kaggle.com/datasets/hazrat/awesomelungs&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junde Wu, Huihui Fang, Yu Zhang, Yehui Yang, Yanwu Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00611&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Models via Pre-segmentation Diffusion Sampling for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xutao Guo, Yanwu Yang, Chenfei Ye, Shang Lu, Yang Xiang, Ting Ma&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.17408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pouria Rouzrokh, Bardia Khosravi, Shahriar Faghani, Mana Moassefi, Sanaz Vahdati, Bradley J. Erickson&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12113&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Mayo-Radiology-Informatics-Lab/MBTI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pierre Chambon, Christian Bluethgen, Curtis P. Langlotz, Akshay Chaudhari&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.04133&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anatomically constrained CT image translation for heterogeneous blood vessel segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giammarco La Barbera, Haithem Boussaid, Francesco Maso, Sabine Sarnacki, Laurence Rouet, Pietro Gori, Isabelle Bloch&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.01713&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Low-Dose CT Using Denoising Diffusion Probabilistic Model for 20× Speedup&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Qing Lyu, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.15136&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boah Kim, Yujin Oh, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14566&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conversion Between CT and MRI Images Using Diffusion and Score-Matching Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qing Lyu, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.12104&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain Imaging Generation with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Walter H. L. Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.07162&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PET image denoising based on denoising diffusion probabilistic models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kuang Gong, Keith A. Johnson, Georges El Fakhri, Quanzheng Li, Tinsu Pan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.06167&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Score: Self-Supervised Learning on Score-Based Models for MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Chentao Cao, Shaonan Liu, Qingyong Zhu, Jing Cheng, Haifeng Wang, Yanjie Zhu, Dong Liang&lt;/em&gt; &lt;br&gt; IEEE TMI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.00835&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Frequency Space Diffusion Models for Accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chentao Cao, Zhuo-Xu Cui, Shaonan Liu, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.05481&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What is Healthy? Generative Counterfactual Diffusion for Lesion Localization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pedro Sanchez, Antanas Kascenas, Xiao Liu, Alison Q. O&#39;Neil, Sotirios A. Tsaftaris&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.12268&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vios-s/Diff-SCM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Medical Image Translation with Adversarial Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muzaffer Özbey, Salman UH Dar, Hasan A Bedel, Onat Dalmaz, Şaban Özturk, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08208&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adaptive Diffusion Priors for Accelerated MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Salman UH Dar, Şaban Öztürk, Yilmaz Korkmaz, Gokberk Elmas, Muzaffer Özbey, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.05876&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangxi Meng, Yuning Gu, Yongsheng Pan, Nizhuan Wang, Peng Xue, Mengkang Lu, Xuming He, Yiqiang Zhan, Dinggang Shen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.03430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cross-Modal Transformer GAN: A Brain Structure-Function Deep Fusing Framework for Alzheimer&#39;s Disease&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junren Pan, Shuqiang Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.13393&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Deformable Model for 4D Temporal Medical Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boah Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.13295&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/torchddm/ddm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Walter H. L. Pinaya, Mark S. Graham, Robert Gray, Pedro F Da Costa, Petru-Daniel Tudosiu, Paul Wright, Yee H. Mah, Andrew D. MacKinnon, James T. Teo, Rolf Jager, David Werring, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardos&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.03461&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Models for Inverse Problems using Manifold Constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00941&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AnoDDPM: Anomaly Detection with Denoising Diffusion Probabilistic Models using Simplex Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julian Wyatt, Adam Leach, Sebastian M. Schmon, Chris G. Willcocks&lt;/em&gt; &lt;br&gt; CVPR Workshop 2022. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Wyatt_AnoDDPM_Anomaly_Detection_With_Denoising_Diffusion_Probabilistic_Models_Using_Simplex_CVPRW_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Julian-Wyatt/AnoDDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.02641&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Eun Sun Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.12621&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Medical Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Florentin Bieder, Robin Sandkühler, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.04306&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JuliaWolleb/diffusion-anomaly&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards performant and reliable undersampled MR reconstruction via diffusion model sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Peng, Pengfei Guo, S. Kevin Zhou, Vishal Patel, Rama Chellappa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.04292&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cpeng93/diffuserecon&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutong Xie, Quanzheng Li&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.03623&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Theodore-PKU/MC-DDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MRI Reconstruction via Data Driven Markov Chain with Joint Uncertainty Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guanxiong Luo, Martin Heide, Martin Uecker&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.01479&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mrirecon/spreco&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Denoising of Retinal OCT with Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dewei Hu, Yuankai K. Tao, Ipek Oguz&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.11760&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DeweiHu/OCT_DDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Byeongsu Sim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.05146&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems in Medical Imaging with Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Liyue Shen, Lei Xing, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.08005&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yang-song/score_inverse_problems&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based diffusion models for accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jong chul Ye&lt;/em&gt; &lt;br&gt; MIA 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.05243&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HJ-harry/score-MRI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Oct 2021&lt;/p&gt; &#xA;&lt;h3&gt;Multi-modal Learning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, Shenghua Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17115&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjing Huang, Shikui Tu, Lei Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16894&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ximing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian Yu, Dong Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14685&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A-STAR: Test-time Attention Segregation and Retention for Text-to-image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aishwarya Agarwal, Srikrishna Karanam, K J Joseph, Apoorv Saxena, Koustava Goswami, Balaji Vasan Srinivasan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14544&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Decompose and Realign: Tackling Condition Misalignment in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luozhou Wang, Guibao Shen, Yijun Li, Ying-cong Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot spatial layout conditioning for text-to-image diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Couairon, Marlène Careil, Matthieu Cord, Stéphane Lathuilière, Jakob Verbeek&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13754&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12422&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Align, Adapt and Inject: Sound-guided Unified Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yue Yang, Kaipeng Zhang, Yuying Ge, Wenqi Shao, Zeyue Xue, Yu Qiao, Ping Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11504&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EMoG: Synthesizing Emotive Co-speech 3D Gesture with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lianying Yin, Yijun Wang, Tianyu He, Jinming Liu, Wei Zhao, Bohan Li, Xin Jin, Jianxin Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11496&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11300&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuqi Sun, Reian He, Weimin Tan, Bo Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10813&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Text Image Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanzhi Zhu, Zhaohai Li, Tianwei Wang, Mengchao He, Cong Yao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10804&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Point-Cloud Completion with Pretrained Text-to-image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yoni Kasten, Ohad Rahamim, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10533&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Geon Yeong Park, Jeongsol Kim, Beomsu Kim, Sang Wan Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09869&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongcheng Gao, Hao Zhang, Yinpeng Dong, Zhijie Deng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13103&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao-Wen Dong, Xiaoyu Liu, Jordi Pons, Gautam Bhattacharya, Santiago Pascual, Joan Serrà, Taylor Berg-Kirkpatrick, Julian McAuley&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09635&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming Diffusion Models for Music-driven Conducting Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuoran Zhao, Jinbin Bai, Delong Chen, Debang Wang, Yubo Pan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10065&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shivam Mehta, Siyang Wang, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09417&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Zero-Shot Open-Vocabulary Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Laurynas Karazija, Iro Laina, Andrea Vedaldi, Christian Rupprecht&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08877&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training Multimedia Event Extraction With Generated Images and Captions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zilin Du, Yunxin Li, Xu Guo, Yidan Sun, Boyang Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08966&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paul Couairon, Clément Rambour, Jean-Emmanuel Haugeard, Nicolas Thome&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08707&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Norm-guided latent space exploration for text-to-image generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08687&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyu Jin, Xuli Shen, Bin Li, Xiangyang Xue&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08645&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GBSD: Generative Bokeh with Stage Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jieren Deng, Xin Zhou, Hao Tian, Zhihong Pan, Derek Aguiar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08251&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yongqi Yang, Ruoyu Wang, Zhihao Qian, Ye Zhu, Yu Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07954&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Zhang, Jiaxian Guo, Paul Yoo, Yutaka Matsuo, Yusuke Iwasawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07596&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controlling Text-to-Image Diffusion by Orthogonal Finetuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, Bernhard Schölkopf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07280&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi Tuo, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, Jianlong Fu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07257&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying Shan, Shenghua Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07154&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language-Guided Traffic Simulation via Scene-Level Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, Baishakhi Ray&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.06344&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, Josh Susskind&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05544&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Grounded Text-to-Image Synthesis with Attention Refocusing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Quynh Phung, Songwei Ge, Jia-Bin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05427&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuseung Lee, Kunho Kim, Hyunjin Kim, Minhyuk Sung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05178&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://syncdiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KAIST-Geometric-AI-Group/SyncDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Tuning-Free Real Image Editing with Proximal Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu, Qilong Zhangli, Anastasis Stathopoulos, Jindong Jiang, Zhaoyang Xia, Akash Srivastava, Dimitris Metaxas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05414&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changhoon Kim, Kyle Min, Maitreya Patel, Sheng Cheng, Yezhou Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04744&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Maitreya Patel, Tejas Gokhale, Chitta Baral, Yezhou Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04695&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Designing a Better Asymmetric VQGAN for StableDiffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, Gang Hua&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04632&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/buxiangzhiren/Asymmetric_VQGAN&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-modal Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mustapha Bounoua, Giulio Franzese, Pietro Michiardi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04445&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04607&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04396&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stable Diffusion is Unstable&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chengbin Du, Yanxi Li, Zhongwei Qiu, Chang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02583&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yochai Yemini, Aviv Shamsian, Lior Bracha, Sharon Gannot, Ethan Fetaya&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03258&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://lipvoicer.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HeadSculpt: Crafting 3D Head Avatars with Text&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03038&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://brandonhan.uk/HeadSculpt/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoxu Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02903&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuyu Yang, Yinan Zhou, Yaxiong Wang, Yujiao Wu, Li Zhu, Zhedong Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02898&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;User-friendly Image Editing with Minimal Text Input: Leveraging Captioning and Injection Techniques&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sunwoo Kim, Wooseok Jang, Hyunsu Kim, Junho Kim, Yunjey Choi, Seungryong Kim, Gayeong Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02717&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Detector Guidance for Multi-Object Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luping Liu, Zijian Zhang, Yi Ren, Rongjie Huang, Xiang Yin, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02236&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Word-Level Explanations for Analyzing Bias in Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexander Lin, Lucas Monteiro Paes, Sree Harsha Tanneru, Suraj Srinivas, Himabindu Lakkaraju&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05500&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Text-Guided 3D-Aware Portrait Generation with Score Distillation Sampling on Distribution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiji Cheng, Fei Yin, Xiaoke Huang, Xintong Yu, Jiaxiang Liu, Shikun Feng, Yujiu Yang, Yansong Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02083&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Probabilistic Adaptation of Text-to-Video Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengjiao Yang, Yilun Du, Bo Dai, Dale Schuurmans, Joshua B. Tenenbaum, Pieter Abbeel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01872&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://video-adapter.github.io/video-adapter/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video Colorization with Pre-trained Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanyuan Liu, Minshan Xie, Jinbo Xing, Chengze Li, Tien-Tsin Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01732&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio-Visual Speech Enhancement with Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julius Richter, Simone Frintrop, Timo Gerkmann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01432&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Privacy Distillation: Reducing Re-identification Risk of Multimodal Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Virginia Fernandez, Pedro Sanchez, Walter Hugo Lopez Pinaya, Grzegorz Jacenków, Sotirios A. Tsaftaris, Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01322&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00984&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Self-Guidance for Controllable Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, Aleksander Holynski&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00986&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dave.ml/selfguidance/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleDrop: Text-to-Image Generation in Any Style&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, Dilip Krishnan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00983&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://styledrop.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intriguing Properties of Text-guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, Alan Yuille&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00974&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Weidi Xie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00973&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://haoningwu3639.github.io/StoryGen_Webpage/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaozhe Hao, Kai Han, Shihao Zhao, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00971&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haoosz/ViCo&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Hidden Language of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00966&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://hila-chefer.github.io/Conceptor/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, Tat-Jen Cham&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00964&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mhh0318.github.io/cocktail/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mhh0318/Cocktail&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, Ying Shan, Tien-Tsin Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00943&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://doubiiu.github.io/projects/Make-Your-Video/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inserting Anybody in Diffusion Models via Celeb Basis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan, Huicheng Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00926&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://celeb-basis.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wuerstchen: Efficient Pretraining of Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pablo Pernias, Dominic Rampas, Marc Aubreville&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00637&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniDiff: Advancing Vision-Language Models with Generative and Discriminative Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiao Dong, Runhui Huang, Xiaoyong Wei, Zequn Jie, Jianxing Yu, Jian Yin, Xiaodan Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00813&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FigGen: Text to Scientific Figure Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Juan A. Rodriguez, David Vazquez, Issam Laradji, Marco Pedersoli, Pau Rodriguez&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00800&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peyman Gholami, Robert Xiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00219&#34;&gt;Paper&lt;/a&gt;] \ 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding and Mitigating Copying in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20086&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/somepago/DCR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, Yebin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20082&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://control4darxiv.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guian Fang, Zutao Jiang, Jianhua Han, Guansong Lu, Hang Xu, Xiaodan Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19599&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Enderfga/FineRewards&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifei Liu, Rex Shen, Xiaotong Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jialu Li, Mohit Bansal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19195&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pano-gen.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ernie Chu, Shuo-Yen Lin, Jun-Cheng Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19193&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nested Diffusion Processes for Anytime Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Noam Elata, Bahjat Kawar, Tomer Michaeli, Michael Elad&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19066&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19012&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junzhe Zhu, Peiye Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18766&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayerDiffusion: Layered Controlled Image Editing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pengzhi Li, QInxuan Huang, Yikang Ding, Zhiheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18676&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Text-to-Image Generation with GPT-4&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, Xin Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18583&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zizhao Hu, Mohammad Rostami&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18433&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, Ping Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18295&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, Yixiao Ge, Ying Shan, Mike Zheng Shou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18292&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://showlab.github.io/Mix-of-Show/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18264&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/G-U-N/Gen-L-Video&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Only Image Captioning with Multi-Context Data Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Feipeng Ma, Yizhou Zhou, Fengyun Rao, Yueyi Zhang, Xiaoyan Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18072&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qian Wang, Biao Zhang, Michael Birsak, Peter Wonka&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18047&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Score Guidance for Text-Driven Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyunsoo Lee, Minsoo Kang, Bohyung Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18007&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-image Editing by Image Information Removal&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhongping Zhang, Jian Zheng, Jacob Zhiyuan Fang, Bryan A. Plummer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17489&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Consistent Video Editing with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, Luoqi Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17431&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FISEdit: Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion Inference&lt;/strong&gt; \ &lt;em&gt;Zihao Yu, Haoyang Li, Fangcheng Fu, Xupeng Miao, Bin Cui&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17423&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17098&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/controlvideo/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Visual Story Generation with Adaptive Context Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhangyin Feng, Yuchen Ren, Xinmiao Yu, Xiaocheng Feng, Duyu Tang, Shuming Shi, Bing Qin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16811&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daiki Miyake, Akihiro Iohara, Yu Saito, Toshiyuki Tanaka&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16807&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Are Diffusion Models Vision-And-Language Reasoners?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Benno Krojer, Elinor Poole-Dayan, Vikram Voleti, Christopher Pal, Siva Reddy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16397&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/McGill-NLP/diffusion-itm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, Kimin Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16381&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16322&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://shihaozhaozsh.github.io/unicontrolnet/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ShihaoZhaoZSH/Uni-ControlNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Parallel Sampling of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, Nima Anari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16317&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AndyShih12/paradigms&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Break-A-Scene: Extracting Multiple Concepts from a Single Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, Dani Lischinski&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16311&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/break-a-scene/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16289&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lisadunlap/ALIA&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt-Free Diffusion: Taking &#34;Text&#34; out of Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16223&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SHI-Labs/Prompt-Free-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProSpect: Expanded Conditioning for the Personalization of Attribute-aware Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, Changsheng Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16225&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16213&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/prolificdreamer/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Architectural Compression of Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, Shinkook Choi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15798&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Yunjey Choi, Yunji Kim, Junho Kim, Sungroh Yoon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15779&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Björn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Koen Oostermeijer, Andres Felipe Cruz-Salinas, Patrick Schramowski, Kristian Kersting, Samuel Weinbach&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15296&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongxu Yue, Qin Guo, Munan Ning, Jiaxi Cui, Yuesheng Zhu, Li Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14742&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sungnyun Kim, Junsoo Lee, Kibeom Hong, Daesik Kim, Namhyuk Ahn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15194&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sungnyun/diffblender&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, Smaranda Muresan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14724&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongxu Li, Junnan Li, Steven C. H. Hoi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14720&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alicia Parrish, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, Rafael Mosquera, Addison Howard, Will Cukierski, D. Sculley, Vijay Janapa Reddi, Lora Aroyo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14384&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, Xiaodong Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13921&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, Yang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13873&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, Liang Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13840&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding Text-driven Motion Synthesis with Keyframe Collaboration via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dong Wei, Xiaoning Sun, Huaijiang Sun, Bin Li, Shengxiang Hu, Weiqing Li, Jianfeng Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13773&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Long Lian, Boyi Li, Adam Yala, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13655&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, Rita Cucchiara&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13501&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Megha Chakraborty, Khusbu Pahwa, Anku Rani, Adarsh Mahor, Aditya Pakala, Arghya Sarkar, Harshit Dave, Ishan Paul, Janvita Reddy, Preethi Gurumurthy, Ritvik G, Samahriti Mukherjee, Shreyas Chatterjee, Kinjal Sensharma, Dwip Dalal, Suryavardan S, Shreyash Mishra, Parth Patwa, Aman Chadha, Amit Sheth, Amitava Das&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05523&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training Diffusion Models with Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13301&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If at First You Don&#39;t Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, Zeynep Akata&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13308&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://rl-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ControlVideo: Training-free Controllable Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, Qi Tian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13077&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YBYBZhang/ControlVideo&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guy Yariv, Itai Gat, Lior Wolf, Yossi Adi, Idan Schwartz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13050&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The CLIP Model is Secretly an Image-to-Prompt Converter&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxuan Ding, Chunna Tian, Haoxuan Ding, Lingqiao Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12716&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructVid2Vid: Controllable Video Editing with Natural Language Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, Yueting Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12328&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models&#39; Safety Filters&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, Yinzhi Cao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12082&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Late-Constraint Diffusion Guidance for Controllable Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chang Liu, Dong Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11520&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://alonzoleeeooo.github.io/LCDG/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AlonzoLeeeooo/LCDG&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Any-to-Any Generation via Composable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, Mohit Bansal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11846&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://codi-gen.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-V3&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11588&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain Captioning: Decoding human brain activity into images and text&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matteo Ferrante, Furkan Ozcelik, Tommaso Boccato, Rufin VanRullen, Nicola Toschi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11560&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinyi Hu, Xu Han, Xiaoyuan Yi, Yutong Chen, Wenhao Li, Zhiyuan Liu, Maosong Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11540&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discriminative Diffusion Models as Few-shot Vision and Language Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuehai He, Weixi Feng, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, William Yang Wang, Xin Eric Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10722&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yihao Huang, Qing Guo, Felix Juefei-Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10701&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AIwriting: Relations Between Image Generation and Digital Writing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Scott Rettberg, Talan Memmott, Jill Walker Rettberg, Jason Nelson, Patrick Lichty&lt;/em&gt; &lt;br&gt; ISEA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10834&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TextDiffuser: Diffusion Models as Text Painters&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10855&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, Jiaying Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10874&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LDM3D: Latent Diffusion Model for 3D&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, Vasudev Lal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10853&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yixiong Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10843&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inspecting the Geographical Representativeness of Images from Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Abhipsa Basu, R. Venkatesh Babu, Danish Pruthi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11080&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, Yogesh Balaji&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10474&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/dir/pyoco/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AMD: Autoregressive Motion Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Han, Hao Peng, Minjing Dong, Chang Xu, Yi Ren, Yixuan Shen, Yuheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09381&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating coherent comic with rich story using ChatGPT and Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ze Jin, Zorina Song&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11067&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh, Sonal Gupta&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09662&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://azadis.github.io/make-an-animation/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li, Gim Hee Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08850&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://make-a-protagonist.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Make-A-Protagonist/Make-A-Protagonist&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Common Diffusion Noise Schedules and Sample Steps are Flawed&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shanchuan Lin, Bingchen Liu, Jiashi Li, Xiao Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08891&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interactive Fashion Content Generation Using LLMs and Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Krishna Sri Ipsit Mantri, Nevasini Sasikumar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05182&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Huang, Wenjing Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06710&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nulltextforcartoon.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NullTextforCartoon/NullTextforCartoon&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;iEdit: Localised Text-guided Image Editing with Weak Supervision&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rumeysa Bodur, Erhan Gundogdu, Binod Bhattarai, Tae-Kyun Kim, Michael Donoser, Loris Bazzani&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05947&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05189&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Qrange-group/SUR-adapter&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nisha Huang, Yuxin Zhang, Weiming Dong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05464&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseStyleGesture: Stylized Audio-Driven Co-Speech Gesture Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, Ming Cheng, Long Xiao&lt;/em&gt; &lt;br&gt; IJCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04919&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YoungSeng/DiffuseStyleGesture&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IIITD-20K: Dense captioning for Text-Image ReID&lt;/strong&gt; &lt;br&gt; &lt;em&gt;A V Subramanyam, Niranjan Sundararajan, Vibhu Dubey, Brejesh Lall&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04497&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yupei Lin, Sen Zhang, Xiaojun Yang, Xiao Wang, Yukai Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04651&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yupeilin2388.github.io/publication/ReDiffuser&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenkai Dong, Song Xue, Xiaoyue Duan, Shumin Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04441&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, Hang Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04175&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seungwoo Lee, Chaerin Kong, Donghyeon Jeon, Nojun Kwak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04001&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Curation for Image Captioning with Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenyan Li, Jonas F. Lotz, Chen Qiu, Desmond Elliott&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03610&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, Wenwu Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://disenbooth.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guided Image Synthesis via Initial Image Editing in Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiafeng Mao, Xueting Wang, Kiyoharu Aizawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03382&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seongmin Lee, Benjamin Hoover, Hendrik Strobelt, Zijie J. Wang, ShengYun Peng, Austin Wright, Kevin Li, Haekyu Park, Haoyang Yang, Duen Horng Chau&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03509&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://poloclub.github.io/diffusion-explainer/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chao Xu, Shaoting Zhu, Junwei Zhu, Tianxin Huang, Jiangning Zhang, Ying Tai, Yong Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.02594&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Data Augmentation for Image Captioning using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changrong Xiao, Sean Xin Xu, Kunpeng Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01855&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In-Context Learning Unlocked for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01115&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zhendong-wang.github.io/prompt-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Zhendong-Wang/Prompt-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Azade Farshad, Yousef Yeganeh, Yu Chi, Chengzhi Shen, Björn Ommer, Nassir Navab&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14573&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;It is all about where you start: Text-to-image generation with seed selection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14530&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edit Everything: A Text-Guided Generative System for Images Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Defeng Xie, Ruichen Wang, Jian Ma, Chen Chen, Haonan Lu, Dong Yang, Fobo Shi, Xiaodong Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14006&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DefengXie/Edit_Everything&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-Free Location-Aware Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiafeng Mao, Xueting Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13427&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TextMesh: Generation of Realistic 3D Meshes From Text Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, Federico Tombari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12439&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using Text-to-Image Generation for Architectural Design Ideation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ville Paananen, Jonas Oppenlaender, Aku Visuri&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10182&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anything-3D: Towards Single-view Anything Reconstruction in the Wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiuhong Shen, Xingyi Yang, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10261&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Anything-of-anything/Anything-3D&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Soon Yau Cheong, Armin Mustafa, Andrew Gilbert&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08870&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/soon-yau/upgpt&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuwei Yin, Jean Kaddour, Xiang Zhang, Yixin Nie, Zhenguang Liu, Lingpeng Kong, Qi Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08821&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/VideoLDM/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Performer: Text-Driven Human Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuming Jiang, Shuai Yang, Tong Liang Koh, Wayne Wu, Chen Change Loy, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08483&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yumingj.github.io/projects/Text2Performer.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, Xi Yin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08477&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://latent-shift.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, Yinqiang Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08465&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TencentARC/MasaCtrl&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Conditional Contextualized Avatars For Zero-Shot Personalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samaneh Azadi, Thomas Hayes, Akbar Shah, Guan Pang, Devi Parikh, Sonal Gupta&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07410&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Delta Denoising Score&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amir Hertz, Kfir Aberman, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07090&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://delta-denoising-score.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Expressive Text-to-Image Generation with Rich Text&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Songwei Ge, Taesung Park, Jun-Yan Zhu, Jia-Bin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06720&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://rich-text-to-image.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SongweiGe/rich-text-to-image&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Soundini: Sound-Guided Diffusion for Natural Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seung Hyun Lee, Sieun Kim, Innfarn Yoo, Feng Yang, Donghyeon Cho, Youngseo Kim, Huiwen Chang, Jinkyu Kim, Sangpil Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://kuai-lab.github.io/soundini-gallery/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Models for Scene Text Editing with Dual Encoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, Shiyu Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05568&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UCSB-NLP-Chang/DiffSTE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An Edit Friendly DDPM Noise Space: Inversion and Manipulations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Inbar Huberman-Spiegelglas, Vladimir Kulikov, Tomer Michaeli&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06140&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA&lt;/strong&gt; &lt;br&gt; &lt;em&gt;James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, Hongxia Jin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06027&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jamessealesmith.github.io/continual-diffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eslam Mohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan Farooq Khan, Li Erran Li, Mohamed Elhoseiny&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05390&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://eslambakr.github.io/hrsbench.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammadreza Armandpour, Huangjie Zheng, Ali Sadeghian, Amir Sadeghian, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04968&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikita Starodubcev, Dmitry Baranchuk, Valentin Khrulkov, Artem Babenko&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04344&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, Qiang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04269&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://idea-research.github.io/HumanSD/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiucheng Wu, Yujian Liu, Handong Zhao, Trung Bui, Zhe Lin, Yang Zhang, Shiyu Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03869&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Generative Model Adaptation via Image-specific Prompt Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiayi Guo, Chaofei Wang, You Wu, Eric Zhang, Kai Wang, Xingqian Xu, Shiji Song, Humphrey Shi, Gao Huang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03119&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-Free Layout Control with Cross-Attention Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minghao Chen, Iro Laina, Andrea Vedaldi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03373&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://silent-chen.github.io/layout-guidance/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/silent-chen/layout-guidance&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Benchmarking Robustness to Text-Guided Corruptions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammadreza Mofayezi, Yasamin Medghalchi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02963&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02827&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://janeyeon.github.io/ditto-nerf/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02642&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Diffusion-based Method for Multi-turn Compositional Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chao Wang, Xiaoyu Yang, Jinmiao Huang, Kevin Ferreira&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02192&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;viz2viz: Prompt-driven stylized visualization generation using a diffusion model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaqi Wu, John Joon Young Chung, Eytan Adar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01919&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alberto Baldrati, Davide Morelli, Giuseppe Cartella, Marcella Cornia, Marco Bertini, Rita Cucchiara&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02051&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Ji Ha Jang, Se Young Chun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01900&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gwang-kim.github.io/podia_3d/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaewoong Lee, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Yunji Kim, Jin-Hwa Kim, Jung-Woo Ha, Sung Ju Hwang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01515&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01116&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mingyuan-zhang/ReMoDiffuse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.00916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, Jingyi Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03117&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/dreamface&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, Xiaodong Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17870&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://1073521013.github.io/glyph-draw.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17606&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://avatar-craft.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/songrise/avatarcraft&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu Sebe, Trevor Darrell, Zhangyang Wang, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17546&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Picsart-AI-Research/PAIR-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Social Biases through the Text-to-Image Generation Lens&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ranjita Naik, Besmira Nushi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06034&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17591&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SHI-Labs/Forget-Me-Not&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCollage: Parallel Generation of Large Content with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, Ming-Yu Liu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17076&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/dir/diffcollage/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, Chunhua Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17599&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discriminative Class Tokens for Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Idan Schwartz, Vésteinn Snæbjarnarson, Sagie Benaim, Hila Chefer, Ryan Cotterell, Lior Wolf, Serge Belongie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17155&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenpng Du, Qi Chen, Tianyu He, Xu Tan, Xie Chen, Kai Yu, Sheng Zhao, Jiang Bian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17550&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, Xi Li&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17189&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZGCTroy/LayoutDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4D Facial Expression Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaifeng Zou, Sylvain Faisan, Boyang Yu, Sébastien Valette, Hyewon Seo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16611&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZOUKaifeng/4DFM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MDP: A Generalized Framework for Text-Guided Image Editing by Manipulating the Diffusion Path&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qian Wang, Biao Zhang, Michael Birsak, Peter Wonka&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16765&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/QianWangX/MDP-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Ishii, Takuya Narihira&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15780&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sony.github.io/Instruct3Dto3D-doc/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, Jian Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15649&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seer: Language Instructed Video Prediction with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xianfan Gu, Chuan Wen, Jiaming Song, Yang Gao&lt;/em&gt; &lt;br&gt; CVPR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14897&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Susung Hong, Donghoon Ahn, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15413&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anti-DreamBooth: Protecting users from personalized text-to-image synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, Anh Tran&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15433&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/VinAIResearch/Anti-DreamBooth&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tenglong Ao, Zeyi Zhang, Libin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14613&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Better Aligning Text-to-Image Models with Human Preference&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14420&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tgxs002.github.io/align_sd_web/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ISS++: Image as Stepping Stone for Text-Guided 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15181&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, Matthias Nießner&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14207&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tangjiapeng.github.io/projects/DiffuScene/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13843&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fantasia3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rui Chen, Yongwei Chen, Ningxin Jiao, Kui Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13873&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReVersion: Diffusion-Based Relation Inversion from Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13495&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ziqihuangg.github.io/projects/reversion.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ziqihuangg/ReVersion&#34;&gt;Github&lt;/a&gt;] 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ablating Concepts in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, Jun-Yan Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13516&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~concept-ablation/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nupurkmr9/concept-ablation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13439&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Picsart-AI-Research/Text2Video-Zero&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wenjing Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13126&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://magicfusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MagicFusion/MagicFusion.github.io&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pix2Video: Video Editing using Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Duygu Ceylan, Chun-Hao Paul Huang, Niloy J. Mitra&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12688&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://duyguceylan.github.io/pix2video.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12789&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instruct-nerf2nerf.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, Minhyuk Sung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12236&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://salad3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vox-E: Text-guided Voxel Editing of 3D Objects&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Etai Sella, Gal Fiebelman, Peter Hedman, Hadar Averbuch-Elor&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tau-vailab.github.io/Vox-E/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, Sangdoo Yun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Jhe Li, Kris Kitani&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11938&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Tex: Text-driven Texture Synthesis via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, Matthias Nießner&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11396&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://daveredrum.github.io/Text2Tex/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Localizing Object-level Shape Variations with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11306&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://orpatashnik.github.io/local-prompt-mixing/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SVDiff: Compact Parameter Space for Diffusion Fine-Tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, Feng Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11305&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Interpretable Directions in the Semantic Latent Space of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;René Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, Tomer Michaeli&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11073&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SKED: Sketch-guided Text-based 3D Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aryan Mikaeili, Or Perel, Daniel Cohen-Or, Ali Mahdavi-Amiri&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10735&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DialogPaint: A Dialog-based Image Editing Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingxuan Wei, Shiyu Wu, Xin Jiang, Yequan Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10073&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Can Qin, Ning Yu, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming Xiong, Ran Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10056&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionRet: Generative Text-Video Retrieval with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji, Chang Liu, Li Yuan, Jie Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09867&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, Jian Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09833&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vvictoryuki/FreeDoM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, Jiaying Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09319&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FateZero: Fusing Attentions for Zero-shot Text-based Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, Qifeng Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09535&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fate-zero-edit.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenyangQiQi/FateZero&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HIVE: Harnessing Human Feedback for Instructional Visual Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09618&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;P+: Extended Textual Conditioning in Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, Kfir Aberman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09522&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://prompt-plus.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Inhwa Han, Serin Yang, Taesung Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08767&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Aerial Diffusion: Text Guided Ground-to-Aerial View Translation from a Single Image using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Divya Kothandaraman, Tianyi Zhou, Ming Lin, Dinesh Manocha&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11444&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/divyakraman/AerialDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Serin Yang, Hyunmin Hwang, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08622&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edit-A-Video: Single Video Editing with Object-Aware Consistency&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, Sungroh Yoon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07945&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://edit-a-video.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Editing Implicit Assumptions in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hadas Orgad, Bahjat Kawar, Yonatan Belinkov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08084&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://time-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bahjat-kawar/time-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07937&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/visual-chatgpt&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video-P2P: Video Editing with Cross-attention Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, Jiaya Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04761&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://video-p2p.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Erasing Concepts from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, David Bau&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07345&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://erasing.baulab.info/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rohitgandikota/erasing&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06555&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/thu-ml/unidiffuser&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cones: Concept Neurons in Diffusion Models for Customized Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, Yang Cao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05125&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Prompt Log Analysis of Text-to-Image Generation Systems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutong Xie, Zhaoying Pan, Jinge Ma, Jie Luo, Qiaozhu Mei&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04587&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiwei Tang, Dmitry Rybin, Tsung-Hui Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03751&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TZW1998/Taming-Stable-Diffusion-with-Human-Ranking-Feedback&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unleashing Text-to-Image Diffusion Models for Visual Perception&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.02153&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wl-zhao/VPD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collage Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vishnu Sarukkai, Linden Li, Arden Ma, Christopher Ré, Kayvon Fatahalian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.00262&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Enhanced Controllability of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wonwoong Cho, Hareesh Ravi, Midhun Harikumar, Vinh Khuc, Krishna Kumar Singh, Jingwan Lu, David I. Inouye, Ajinkya Kale&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.14368&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Directed Diffusion: Direct Control of Object Placement through Attention Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wan-Duo Kurt Ma, J.P. Lewis, W. Bastiaan Kleijn, Thomas Leung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13153&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modulating Pretrained Diffusion Models for Multimodal Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cusuh Ham, James Hays, Jingwan Lu, Krishna Kumar Singh, Zhifei Zhang, Tobias Hinz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12764&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Region-Aware Diffusion for Zero-shot Text-driven Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nisha Huang, Fan Tang, Weiming Dong, Tong-Yee Lee, Changsheng Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11797&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controlled and Conditional Text to Image Generation with Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pranav Aggarwal, Hareesh Ravi, Naveen Marri, Sachin Kelkar, Fengbin Chen, Vinh Khuc, Midhun Harikumar, Ritiz Tambi, Sudharshan Reddy Kakumanu, Purvak Lapsiya, Alvin Ghouas, Sarah Saber, Malavika Ramprasad, Baldo Faieta, Ajinkya Kale&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11710&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11552&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://energy-based-model.github.io/reduce-reuse-recycle/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning 3D Photography Videos via Self-supervised Diffusion on Single Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaodong Wang, Chenfei Wu, Shengming Yin, Minheng Ni, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Fan Yang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10781&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Henry Kvinge, Davis Brown, Charles Godfrey&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.09301&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-driven Visual Synthesis with Latent Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ting-Hsuan Liao, Songwei Ge, Yiran Xu, Yao-Chih Lee, Badour AlBahar, Jia-Bin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08510&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://latent-diffusion-prior.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08453&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TencentARC/T2I-Adapter&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omer Bar-Tal, Lior Yariv, Yaron Lipman, Tali Dekel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08113&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://multidiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omerbt/MultiDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boundary Guided Mixing Trajectory for Semantic Control with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Zhu, Yu Wu, Zhiwei Deng, Olga Russakovsky, Yan Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08357&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dataset Interfaces: Diagnosing Model Failures Using Controllable Counterfactual Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joshua Vendrow, Saachi Jain, Logan Engstrom, Aleksander Madry&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07865&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MadryLab/dataset-interfaces&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PRedItOR: Text Guided Image Editing with Diffusion Prior&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hareesh Ravi, Sachin Kelkar, Midhun Harikumar, Ajinkya Kale&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07979&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Guided Scene Sketch-to-Photo Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;AprilPyone MaungMaung, Makoto Shing, Kentaro Mitsui, Kei Sawada, Fumio Okura&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.06883&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Universal Guidance for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07121&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arpitbansal297/Universal-Guided-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adding Conditional Control to Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lvmin Zhang, Maneesh Agrawala&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05543&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analyzing Multimodal Objectives Through the Lens of Generative Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaerin Kong, Nojun Kwak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10305&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is This Loss Informative? Speeding Up Textual Inversion with Deterministic Objective Evaluation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anton Voronov, Mikhail Khoroshikh, Artem Babenko, Max Ryabinin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04841&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q-Diffusion: Quantizing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, Kurt Keutzer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04304&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Xiuyu-Li/q-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben Y. Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04222&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyeonho Jeong, Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03900&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Felix Friedrich, Patrick Schramowski, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Sasha Luccioni, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10893&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03668&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YuxinWenRick/hard-prompts-made-easy&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, Jun-Yan Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03027&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Structure and Content-Guided Video Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03011&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.runwayml.com/gen1&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mixture of Diffusers for scene composition and high resolution image generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Álvaro Barbero Jiménez&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02412&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/albarji/mixture-of-diffusers&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kexun Zhang, Xianjun Yang, William Yang Wang, Lei Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02285&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Eliminating Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zuopeng Yang, Tianshu Chu, Xin Lin, Erdun Gao, Daqing Liu, Jie Yang, Chaoyue Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02394&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic-Guided Image Augmentation with Pre-trained Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bohan Li, Xinghao Wang, Xiao Xu, Yutai Hou, Yunlong Feng, Feng Wang, Wanxiang Che&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02070&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://texturepaper.github.io/TEXTurePaper/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TEXTure: Text-Guided Texturing of 3D Shapes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01721&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://texturepaper.github.io/TEXTurePaper/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TEXTurePaper/TEXTurePaper&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dreamix: Video Diffusion Models are General Video Editors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, Yedid Hoshen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01329&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreamix-video-editing.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Trash to Treasure: Using text-to-image models to inform the design of physical artefacts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amy Smith, Hope Schroeder, Ziv Epstein, Michael Cook, Simon Colton, Andrew Lippman&lt;/em&gt; &lt;br&gt; AAAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00561&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13826&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://attendandexcite.github.io/Attend-and-Excite/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AttendAndExcite/Attend-and-Excite&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero3D: Semantic-Driven Multi-Category 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Han, Yitong Liu, Yixuan Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13591&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shape-aware Text-driven Layered Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth Qiu, Jia-Bin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13173&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://text-video-edit.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PromptMix: Text-to-image diffusion models enhance the performance of lightweight networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arian Bakhtiarnia, Qi Zhang, Alexandros Iosifidis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12914&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gitlab.au.dk/maleci/promptmix&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ming Tao, Bing-Kun Bao, Hao Tang, Changsheng Xu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12959&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tobran/GALIP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SEGA: Instructing Diffusion using Semantic Dimensions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhixuan Liu, Youeun Shin, Beverley-Claire Okogwu, Youngsik Yun, Lia Coleman, Peter Schaldenbrand, Jihie Kim, Jean Oh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12073&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-To-4D Dynamic Scene Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, Yaniv Taigman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11280&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guiding Text-to-Image Diffusion Model Towards Grounded Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.05221&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://lipurple.github.io/Grounded_Diffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Driven Video Editing via an Audio-Conditioned Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dan Bigioi, Shubhajit Basak, Hugh Jordan, Rachel McDonnell, Peter Corcoran&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.04474&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://danbigioi.github.io/DiffusionVideoEditing/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DanBigioi/DiffusionVideoEditing&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual Story Generation Based on Emotion and Keywords&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuetian Chen, Ruohua Li, Bowen Shi, Peiru Liu, Mei Si&lt;/em&gt; &lt;br&gt; AIIDE INT 2022. [&lt;a href=&#34;https://arxiv.org/abs/2301.02777&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03786&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Driven Video Editing via an Audio-Conditioned Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dan Bigioi, Shubhajit Basak, Hugh Jordan, Rachel McDonnell, Peter Corcoran&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.04474&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michał Stypułkowski, Konstantinos Vougioukas, Sen He, Maciej Zięba, Stavros Petridis, Maja Pantic&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03396&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mstypulkowski.github.io/diffusedheads/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Muse: Text-To-Image Generation via Masked Generative Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, Dilip Krishnan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.00704&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://muse-model.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Shenghua Gao&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.14704&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bluestyle97.github.io/dream3d/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Vision Transformers as Diffusion Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;He Cao, Jianan Wang, Tianhe Ren, Xianbiao Qi, Yihao Chen, Yuan Yao, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.13771&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11565&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tuneavideo.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robert Wolfe, Yiwei Yang, Bill Howe, Aylin Caliskan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing Prompts for Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaru Hao, Zewen Chi, Li Dong, Furu Wei&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09611&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/microsoft/Promptist&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/LMOps/tree/main/promptist&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, Shiyu Chang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08698&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TeTIm-Eval: a novel curated evaluation data set for comparing text-to-image models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Federico A. Galatolo, Mario G. C. A. Cimino, Edoardo Cogotti&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07839&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Infinite Index: Information Retrieval on Generative Text-To-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast&lt;/em&gt; &lt;br&gt; CHIIR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.07476&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LidarCLIP or: How I Learned to Talk to Point Clouds&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georg Hess, Adam Tonderski, Christoffer Petersson, Lennart Svensson, Kalle Åström&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06858&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/atonderski/lidarclip&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, William Chan&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.06909&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Stable Artist: Steering Semantics in Diffusion Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Manuel Brack, Patrick Schramowski, Felix Friedrich, Dominik Hintersdorf, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06013&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, Kun Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.05034&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, William Yang Wang&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.05032&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weixi-feng/Structured-Diffusion-Guidance&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, Christian Theobalt&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04495&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/MoFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, Liangyan Gui&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04493&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yccyenchicheng.github.io/SDFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SINE: SINgle Image Editing with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, Jian Ren&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04489&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zhang-zx.github.io/SINE/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhang-zx/SINE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Concept Customization of Text-to-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04488&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~custom-diffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Guided Domain Adaptation of Image Generators&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris Metaxas, Ahmed Elgammal&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04473&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://styleganfusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Executing your Commands via Motion Diffusion in Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, Gang Yu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://chenxin.tech/mld/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn Wong, Baoyuan Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04248&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zxyin.github.io/TH-PAD/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magic: Multi Art Genre Intelligent Choreography Dataset and Network for 3D Dance Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Xiu Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03741&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Judge, Localize, and Edit: Ensuring Visual Commonsense Morality for Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seongbeom Park, Suhong Moon, Jinkyu Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03507&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Congyue Deng, Chiyu &#34;Max&#39;&#39; Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03267&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic-Conditional Diffusion Networks for Image Captioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng, Hongyang Chao, Tao Mei&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.03099&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-SDF: Text-to-Shape via Voxelized Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muheng Li, Yueqi Duan, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.03293&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ttlmh.github.io/DiffusionSDF/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ttlmh/Diffusion-SDF&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ADIR: Adaptive Diffusion for Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shady Abu-Hussein, Tom Tirer, Raja Giryes&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03221&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://shadyabh.github.io/ADIR/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;M-VADER: A Model for Diffusion with Multimodal Context&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samuel Weinbach, Marco Bellagente, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Björn Deiseroth, Koen Oostermeijer, Hannah Teufel, Andres Felipe Cruz-Salinas&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02936&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyeongman Kim, Hajin Shim, Hyunsu Kim, Yunjey Choi, Junho Kim, Eunho Yang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.02802&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diff-video-ae.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/man805/Diffusion-Video-Autoencoders&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unite and Conquer: Cross Dataset Multimodal Synthesis using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00793&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nithin-gk.github.io/projectpages/Multidiff/index.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shape-Guided Diffusion with Inside-Outside Attention&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dong Huk Park, Grace Luo, Clayton Toste, Samaneh Azadi, Xihui Liu, Maka Karalashvili, Anna Rohrbach, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00210&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://shape-guided-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinDDM: A Single Image Denoising Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vladimir Kulikov, Shahar Yadin, Matan Kleiner, Tomer Michaeli&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16582&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://matankleiner.github.io/sinddm/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Se Young Chun&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.16374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://datid-3d.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Refined Semantic Enhancement towards Frequency Diffusion for Video Captioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xian Zhong, Zipeng Li, Shuqin Chen, Kui Jiang, Chen Chen, Mang Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.15076&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lzp870/RSFD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unified Discrete Diffusion for Simultaneous Vision-Language Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minghui Hu, Chuanxia Zheng, Heliang Zheng, Tat-Jen Cham, Chaoyue Wang, Zuopeng Yang, Dacheng Tao, Ponnuthurai N. Suganthan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.14842&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, Dacheng Tao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.14108&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SpaText: Spatio-Textual Representation for Controllable Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, Xi Yin&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.14305&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/spatext/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sketch-Guided Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrey Voynov, Kfir Aberman, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sketch-guided-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shifted Diffusion for Text-to-image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yufan Zhou, Bingchen Liu, Yizhe Zhu, Xiao Yang, Changyou Chen, Jinhui Xu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.15388&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-A-Story: Visual Memory Conditioned Consistent Story Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, Leonid Sigal&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.13319&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Schrödinger&#39;s Bat: Diffusion Models Sometimes Generate Polysemous Words in Superposition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jennifer C. White, Ryan Cotterell&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13095&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EDICT: Exact Diffusion Inversion via Coupled Transformations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bram Wallace, Akash Gokul, Nikhil Naik&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12446&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/salesforce/EDICT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.12572&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MichalGeyer/plug-and-play&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Human Evaluation of Text-to-Image Models on a Multi-Task Benchmark&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vitali Petsiuk, Alexander E. Siemenn, Saisamrit Surbehera, Zad Chin, Keith Tyser, Gregory Hunter, Arvind Raghavan, Yann Hicke, Bryan A. Plummer, Ori Kerret, Tonio Buonassisi, Kate Saenko, Armando Solar-Lezama, Iddo Drori&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12112&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinDiffusion: Learning a Diffusion Model from a Single Natural Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/WeilunWang/SinDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinFusion: Training Diffusion Models on a Single Image or Video&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaniv Nikankin, Niv Haim, Michal Irani&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11743&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yanivnik.github.io/sinfusion/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Discrete Diffusion Models for Image Captioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixin Zhu, Yixuan Wei, Jianfeng Wang, Zhe Gan, Zheng Zhang, Le Wang, Gang Hua, Lijuan Wang, Zicheng Liu, Han Hu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11694&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/buxiangzhiren/DDCap&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Investigating Prompt Engineering in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sam Witteveen, Martin Andrews&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.15462&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ajay Jain, Amber Xie, Pieter Abbeel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11319&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ajayj.com/vectorfusion&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10950&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xichenpan/ARLDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffStyler: Controllable Dual Diffusion for Text-Driven Image Stylization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nisha Huang, Yuxin Zhang, Fan Tang, Chongyang Ma, Haibin Huang, Yong Zhang, Weiming Dong, Changsheng Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10682&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magic3D: High-Resolution Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.10440&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://deepimagination.cc/Magic3D/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Invariant Learning via Diffusion Dreamed Distribution Shifts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Priyatham Kattakinda, Alexander Levine, Soheil Feizi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10370&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Null-text Inversion for Editing Real Images using Guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09794&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructPix2Pix: Learning to Follow Image Editing Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Brooks, Aleksander Holynski, Alexei A. Efros&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.09800&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Versatile Diffusion: Text, Images and Variations All in One Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.08332&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SHI-Labs/Versatile-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Adham Elarabawy, Harish Kamath, Samuel Denton&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.07825&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Arbitrary Style Guidance for Enhanced Diffusion-Based Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhihong Pan, Xin Zhou, Hao Tian&lt;/em&gt; &lt;br&gt; WACV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.07751&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Patrick Schramowski, Manuel Brack, Björn Deiseroth, Kristian Kersting&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.05105&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ml-research/safe-latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lukas Struppek, Dominik Hintersdorf, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.02408&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LukasStruppek/Rickrolling-the-Artist&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01324&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://deepimagination.cc/eDiffi/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MagicMix: Semantic Mixing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jun Hao Liew, Hanshu Yan, Daquan Zhou, Jiashi Feng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.16056&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://magicmix.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wei Li, Xue Xu, Xinyan Xiao, Jiachen Liu, Hu Yang, Guohao Li, Zhanpeng Wang, Zhifan Feng, Qiaoqiao She, Yajuan Lyu, Hua Wu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.16031&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hritik Bansal, Da Yin, Masoud Monajatipoor, Kai-Wei Chang&lt;/em&gt; &lt;br&gt; EMNLP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.15230&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Hritikbansal/entigen_emnlp&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.15257&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, Duen Horng Chau&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.14896&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://poloclub.github.io/diffusiondb/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/poloclub/diffusiondb&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lafite2: Few-shot Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yufan Zhou, Chunyuan Li, Changyou Chen, Jianfeng Gao, Jinhui Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.14124&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Editing via Multi-Stage Blended Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Johannes Ackermann, Minjun Li&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12965&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/pfnet-research/multi-stage-blended-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion with Less Explicit Guidance via Model Predictive Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max W. Shen, Ehsan Hajiramezanali, Gabriele Scalia, Alex Tseng, Nathaniel Diamant, Tommaso Biancalani, Andreas Loukas&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12192&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Visual Tour Of Current Challenges In Multimodal Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shashank Sonkar, Naiming Liu, Richard G. Baraniuk&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12565&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffEdit: Diffusion-based semantic image editing with mask guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Couairon, Jakob Verbeek, Holger Schwenk, Matthieu Cord&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.11427&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models already have a Semantic Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingi Kwon, Jaeseok Jeong, Youngjung Uh&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.10960&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://kwonminki.github.io/Asyrp/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dani Valevski, Matan Kalman, Yossi Matias, Yaniv Leviathan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.09477&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruijun Li, Weihua Li, Yi Yang, Hanyu Wei, Jianhua Jiang, Quan Bai&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.09549&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imagic: Text-Based Real Image Editing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.09276&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://imagic-editing.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaerin Kong, DongHyeon Jeon, Ohjoon Kwon, Nojun Kwak&lt;/em&gt; &lt;br&gt; WACV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.05872&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Diffusion Models&#39; Latent Space, with Applications to CycleDiffusion and Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Henry Wu, Fernando De la Torre&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.05559&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/cycle-diffusion&#34;&gt;Github-1&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/unified-generative-zoo&#34;&gt;Github-2&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imagen Video: High Definition Video Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.02303&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ivan Kapelyukh, Vitalis Vosylius, Edward Johns&lt;/em&gt; &lt;br&gt; IEEE RA-L 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.02438&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LDEdit: Towards Generalized Text Guided Image Manipulation via Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paramanand Chandramouli, Kanchana Vaishnavi Gandikota&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.02249&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;clip2latent: Text driven sampling of a pre-trained StyleGAN using denoising diffusion and CLIP&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Justin N. M. Pinkney, Chuan Li&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.02347&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/justinpinkney/clip2latent&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Membership Inference Attacks Against Text-to-image Generation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yixin Wu, Ning Yu, Zheng Li, Michael Backes, Yang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.00968&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-A-Video: Text-to-Video Generation without Text-Video Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14792&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14988&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Re-Imagen: Retrieval-Augmented Text-to-Image Generator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenhu Chen, Hexiang Hu, Chitwan Saharia, William W. Cohen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14491&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Creative Painting with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xianchao Wu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14697&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nisha Huang, Fan Tang, Weiming Dong, Changsheng Xu&lt;/em&gt; &lt;br&gt; ACM MM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.13360&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Personalizing Text-to-Image Generation via Aesthetic Gradients&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Victor Gallego&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.12330&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vicgalle/stable-diffusion-aesthetic-gradients&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Best Prompts for Text-to-Image Models and How to Find Them&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikita Pavlichenko, Dmitry Ustalov&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.11711&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Biased Artist: Exploiting Cultural Biases via Homoglyphs in Text-Guided Image Generation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lukas Struppek, Dominik Hintersdorf, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08891&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LukasStruppek/The-Biased-Artist&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Henry Wu, Saman Motamed, Shaunak Srivastava, Fernando De la Torre&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.06970&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/Generative-Visual-Prompt&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ISS: Image as Stepping Stone for Text-Guided 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.04145&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/liuzhengzhe/ISS-Image-as-Stepping-Stone-for-Text-Guided-3D-Shape-Generation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2208.12242&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Victarry/stable-dreambooth&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Björn Ommer&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.13038&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, Yan Yan&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2206.07771&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/L-YeZhu/CDCD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blended Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omri Avrahami, Ohad Fried, Dani Lischinski&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.02779&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/blended-latent-diffusion-page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omriav/blended-latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional Visual Generation with Composable Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, Joshua B. Tenenbaum&lt;/em&gt; &lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.01714&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jie Shi, Chenfei Wu, Jian Liang, Xiang Liu, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00386&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Vector Quantized Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhicong Tang, Shuyang Gu, Jianmin Bao, Dong Chen, Fang Wen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.16007&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/VQ-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Human: Text-Driven Controllable Human Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, Ziwei Liu&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.15996&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yumingj/Text2Human&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/imagen-pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Retrieval-Augmented Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andreas Blattmann, Robin Rombach, Kaan Oktay, Björn Ommer&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.11824&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/retrieval-augmented-ddpm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.06125&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/DALLE2-pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;KNN-Diffusion: Image Generation via Large-Scale Retrieval&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Oron Ashual, Shelly Sheynin, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, Yaniv Taigman&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2204.02849&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;More Control for Free! Image Synthesis with Semantic Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, Trevor Darrell&lt;/em&gt; &lt;br&gt; WACV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.05744&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://xh-liu.github.io/sdg/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 10 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vector Quantized Diffusion Model for Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.14822&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/VQ-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blended Diffusion for Text-driven Editing of Natural Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omri Avrahami, Dani Lischinski, Ohad Fried&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.14818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/blended-diffusion-page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omriav/blended-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tackling the Generative Learning Trilemma with Denoising Diffusion GANs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhisheng Xiao, Karsten Kreis, Arash Vahdat&lt;/em&gt; &lt;br&gt; ICLR 2022 (Spotlight). [&lt;a href=&#34;https://arxiv.org/abs/2112.07804&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/denoising-diffusion-gan&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 15 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionCLIP: Text-guided Image Manipulation Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.02711&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gwang-kim/DiffusionCLIP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Oct 2021&lt;/p&gt; &#xA;&lt;h3&gt;3D Vision&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, Shenghua Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17115&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffComplete: Diffusion-based Generative 3D Shape Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruihang Chu, Enze Xie, Shentong Mo, Zhenguo Li, Matthias Nießner, Chi-Wing Fu, Jiaya Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16329&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12422&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EMoG: Synthesizing Emotive Co-speech 3D Gesture with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lianying Yin, Yijun Wang, Tianyu He, Jinming Liu, Wei Zhao, Bohan Li, Xin Jin, Jianxin Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11496&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Point-Cloud Completion with Pretrained Text-to-image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yoni Kasten, Ohad Rahamim, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10533&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, Xun Cao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09864&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edit-DiffNeRF: Editing 3D Neural Radiance Fields using 2D Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lu Yu, Wei Xiang, Kang Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09551&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adding 3D Geometry Control to Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wufei Ma, Qihao Liu, Jiahao Wang, Angtian Wang, Yaoyao Liu, Adam Kortylewski, Alan Yuille&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08103&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07881&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D molecule generation by denoising voxel grids&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pedro O. Pinheiro, Joshua Rackers, Joseph Kleinhenz, Michael Maser, Omar Mahmood, Andrew Martin Watkins, Stephen Ra, Vishnu Sresht, Saeed Saremi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07473&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying Shan, Shenghua Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07154&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchen Zhou, Ying He, F. Richard Yu, Jianqiang Li, You Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05668&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stochastic Multi-Person 3D Motion Forecasting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sirui Xu, Yu-Xiong Wang, Liang-Yan Gui&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05421&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chun-Han Yao, Amit Raj, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, Varun Jampani&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04619&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing realistic sand assemblies with denoising diffusion in latent space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikolaos N. Vlassis, WaiChing Sun, Khalid A. Alshibli, Richard A. Regueiro&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04411&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohit Mendiratta, Xingang Pan, Mohamed Elgharib, Kartik Teotia, Mallikarjun B R, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00547&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffRoom: Diffusion-based High-Quality 3D Room Reconstruction and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoliang Ju, Zhaoyang Huang, Yijin Li, Guofeng Zhang, Yu Qiao, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00519&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Shi, Jingbo Wang, Xuekun Jiang, Bo Dai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00416&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://controllablemdm.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing, Tai Chi-Keung Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00783&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junxing Hu, Hongwen Zhang, Zerui Chen, Mengcheng Li, Yunlong Wang, Yebin Liu, Zhenan Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20089&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://junxinghu.github.io/projects/hoi.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19012&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junzhe Zhu, Peiye Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18766&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Models for Semantic 3D Medical Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zolnamar Dorjsembe, Hsing-Kuo Pao, Sodtavilan Odonchimed, Furen Xiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18453&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenzhen Weng, Zeyu Wang, Serena Yeung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16411&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NAP: Neural 3D Articulation Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiahui Lei, Congyue Deng, Bokui Shen, Leonidas Guibas, Kostas Daniilidis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16315&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cis.upenn.edu/~leijh/projects/nap/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, Benjamin Busam&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16283&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16213&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/prolificdreamer/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, Xinxiao Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15957&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tsu-Ching Hsiao, Hao-Wei Chen, Hsuan-Kung Yang, Chun-Yi Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15873&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai, Chi-Keung Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15171&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Manifold Diffusion Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ahmed A. Elhag, Joshua M. Susskind, Miguel Angel Bautista&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15586&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rundi Wu, Ruoshi Liu, Carl Vondrick, Changxi Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15399&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sin3dm.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Sin3DM/Sin3DM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding Text-driven Motion Synthesis with Keyframe Collaboration via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dong Wei, Xiaoning Sun, Huaijiang Sun, Bin Li, Shengxiang Hu, Weiqing Li, Jianfeng Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13773&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lijun Li, Li&#39;an Zhuo, Bang Zhang, Liefeng Bo, Chen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13705&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GMD: Controllable Human Motion Synthesis via Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, Siyu Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12577&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://korrawe.github.io/gmd-project/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Globally Consistent Stochastic Human Motion Prediction via Motion Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiarui Sun, Girish Chowdhary&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12554&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11664&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Byungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi Lee, Sookwan Han, Daesik Kim, Hanbyul Joo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11870&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://snuvclab.github.io/chupa/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11588&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Liangchen Song, Liangliang Cao, Hongyu Xu, Kai Kang, Feng Tang, Junsong Yuan, Yang Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11337&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LDM3D: Latent Diffusion Model for 3D&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, Vasudev Lal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10853&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh, Sonal Gupta&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09662&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://azadis.github.io/make-an-animation/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FitMe: Deep Photorealistic 3D Morphable Model Avatars&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Baris Gecer, Jiankang Deng, Stefanos Zafeiriou&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09641&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://alexlattas.com/fitme&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AMD: Autoregressive Motion Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Han, Hao Peng, Minjing Dong, Chang Xu, Yi Ren, Yixuan Shen, Yuheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09381&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-guided High-definition Consistency Texture Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhibin Tang, Tiantong He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05901&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Relightify: Relightable 3D Faces from a Single Image via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, Stefanos Zafeiriou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06077&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://foivospar.github.io/Relightify/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 10 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CaloClouds: Fast Geometry-Independent Highly-Granular Calorimeter Simulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Erik Buhmann, Sascha Diefenbacher, Engin Eren, Frank Gaede, Gregor Kasieczka, Anatolii Korol, William Korcari, Katja Krüger, Peter McKeown&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04847&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Locally Attentional SDF Diffusion for Controllable 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, Heung-Yeung Shum&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04461&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui Huang, Shi-Min Hu, Ke Li, Leonidas J Guibas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01921&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://difffacto.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shap-E: Generating Conditional 3D Implicit Functions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Heewoo Jun, Alex Nichol&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.02463&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/shap-e&#34;&gt;Github&lt;/a&gt;] 3 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01618&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zehaozhu.github.io/ContactArt/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On without 3D Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mehmet Saygin Seyfioglu, Karim Bouyarmane, Suren Kumar, Amir Tavanaei, Ismail B. Tutar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01257&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning a Diffusion Prior for NeRFs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guandao Yang, Abhijit Kundu, Leonidas J. Guibas, Jonathan T. Barron, Ben Poole&lt;/em&gt; &lt;br&gt; ICLR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14473&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TextMesh: Generation of Realistic 3D Meshes From Text Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, Federico Tombari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12439&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Holynski, Angjoo Kanazawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10532&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ethanweber.me/nerfbusters/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ethanweber/nerfbusters&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Farm3D: Learning Articulated 3D Animals by Distilling 2D Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10535&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://farm3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anything-3D: Towards Single-view Anything Reconstruction in the Wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiuhong Shen, Xingyi Yang, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, Artsiom Sanakoyeu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08577&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dulucas.github.io/agrol/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Controllable Diffusion Models via Reward-Guided Exploration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hengtong Zhang, Tingyang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07132&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Controllable 3D Diffusion Models from Single-view Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen, Lingjie Liu, Josh Susskind&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06700&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jiataogu.me/control3diff/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, Hao Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06714&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://lakonik.github.io/ssdnerf/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, Siyu Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06024&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sanweiliti.github.io/egohmr/egohmr.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, Lan Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05684&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tr3e/InterGen&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, Siyu Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sanweiliti.github.io/egohmr/egohmr.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammadreza Armandpour, Huangjie Zheng, Ali Sadeghian, Amir Sadeghian, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04968&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://perp-neg.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRF applied to satellite imagery for surface reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Federico Semeraro, Yi Zhang, Wenying Wu, Patrick Carroll&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04133&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fsemerar/satnerf&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02827&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://janeyeon.github.io/ditto-nerf/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Novel View Synthesis with 3D-Aware Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02602&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/genvs/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, Or Litany&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01893&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/trace-pace/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Ji Ha Jang, Se Young Chun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01900&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gwang-kim.github.io/podia_3d/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01116&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mingyuan-zhang/ReMoDiffuse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Motion Synthesis and Reconstruction with Autoregressive Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjie Yin, Ruibo Tu, Hang Yin, Danica Kragic, Hedvig Kjellström, Mårten Björkman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04681&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.00916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, Jingyi Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03117&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/dreamface&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17606&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://avatar-craft.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/songrise/avatarcraft&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Animesh Karnewar, Andrea Vedaldi, David Novotny, Niloy Mitra&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16509&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://holodiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4D Facial Expression Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaifeng Zou, Sylvain Faisan, Boyang Yu, Sébastien Valette, Hyewon Seo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16611&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZOUKaifeng/4DFM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Ishii, Takuya Narihira&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15780&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sony.github.io/Instruct3Dto3D-doc/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://sony.github.io/Instruct3Dto3D-doc/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Novel View Synthesis of Humans using Differentiable Rendering&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Rochette, Chris Russell, Richard Bowden&lt;/em&gt; &lt;br&gt; IEEE T-BIOM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15880&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/GuillaumeRochette/HumanViewSynthesis&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Susung Hong, Donghoon Ahn, Seungryong Kim&lt;/em&gt; &lt;br&gt; CVPR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15413&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, Dong Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14184&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://make-it-3d.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://make-it-3d.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ISS++: Image as Stepping Stone for Text-Guided 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15181&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13843&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fantasia3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rui Chen, Yongwei Chen, Ningxin Jiao, Kui Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13873&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fantasia3d.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Gorilla-Lab-SCUT/Fantasia3D&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ce Zheng, Guo-Jun Qi, Chen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13397&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12789&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instruct-nerf2nerf.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianglong Ye, Naiyan Wang, Xiaolong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12786&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jianglongye.com/featurenerf/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vox-E: Text-guided Voxel Editing of 3D Objects&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Etai Sella, Gal Fiebelman, Peter Hedman, Hadar Averbuch-Elor&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tau-vailab.github.io/Vox-E/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional 3D Scene Generation using Locally Conditioned Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryan Po, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12218&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ryanpo.com/comp3d/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, Wen Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11579&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/paTRICK-swk/D3DP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Jhe Li, Kris Kitani&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11938&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Affordance Diffusion: Synthesizing Hand-Object Interactions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello, Stan Birchfield, Jiaming Song, Shubham Tulsiani, Sifei Liu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12538&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://judyye.github.io/affordiffusion-www/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, Minhyuk Sung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12236&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://salad3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning a 3D Morphable Face Reflectance Model from Low-cost Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxuan Han, Zhibo Wang, Feng Xu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11686&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yxuhan.github.io/ReflectanceMM/index.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Tex: Text-driven Texture Synthesis via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, Matthias Nießner&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11396&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://daveredrum.github.io/Text2Tex/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-1-to-3: Zero-shot One Image to 3D Object&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, Carl Vondrick&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11328&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zero123.cs.columbia.edu/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SKED: Sketch-guided Text-based 3D Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aryan Mikaeili, Or Perel, Daniel Cohen-Or, Ali Mahdavi-Amiri&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10735&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuhan Li, Yishun Dou, Xuanhong Chen, Bingbing Ni, Yilin Sun, Yutian Liu, Fuzhen Wang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10406&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/colorful-liyu/3DQD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, Lequan Yu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09119&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Advocate99/DiffGesture&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-HPC: Generating Synthetic Images with Realistic Humans&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenzhen Weng, Laura Bravo-Sánchez, Serena Yeung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09541&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZZWENG/Diffusion_HPC&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Svitov, Dmitrii Gudkov, Renat Bashirov, Victor Lempitsky&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09375&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suhyeon Lee, Hyungjin Chung, Minyoung Park, Jonghyuk Park, Wi-Sun Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08440&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Mesh Generation Through Sparse Latent Point Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua Lin, Bo Dai&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07938&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://slide-3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MeshDiffusion: Score-based Generative 3D Mesh Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, Weiyang Liu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08133&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://meshdiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lzzcd001/MeshDiffusion/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Point Cloud Diffusion Models for Automatic Implant Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paul Friedrich, Julia Wolleb, Florentin Bieder, Florian M. Thieringer, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08061&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07937&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KU-CVLAB/3DFuse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GECCO: Geometrically-Conditioned Point Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michał J. Tyszkiewicz, Pascal Fua, Eduard Trulls&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DGen: Triplane Latent Diffusion for Textured Mesh Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, Barlas Oğuz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05371&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Human Motion Diffusion as a Generative Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yonatan Shafir, Guy Tevet, Roy Kapon, Amit H. Bermano&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01418&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyemin Ahn, Esteve Valls Mascaro, Dongheui Lee&lt;/em&gt; &lt;br&gt; ICRA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.14503&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/diffusion-motion-prediction&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cotton-ahn/diffusion-motion-prediction&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jamie Wynn, Daniyar Turmukhambetov&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12231&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nianticlabs/diffusionerf&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lukemelas/projection-conditioned-point-cloud-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luke Melas-Kyriazi, Christian Rupprecht, Andrea Vedaldi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10668&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, Ravi Ramamoorthi&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10109&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jiataogu.me/nerfdiff/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinMDM: Single Motion Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H. Bermano, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05905&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sinmdm.github.io/SinMDM-page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SinMDM/SinMDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Colored Shape Reconstruction from a Single RGB Image through Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Li, Xiaolin Wei, Fengwei Chen, Bin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05573&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanMAC: Masked Motion Completion for Human Motion Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, Tongliang Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03665&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://lhchen.top/Human-MAC/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LinghaoChan/HumanMAC&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TEXTure: Text-Guided Texturing of 3D Shapes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01721&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://texturepaper.github.io/TEXTurePaper/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TEXTurePaper/TEXTurePaper&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero3D: Semantic-Driven Multi-Category 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Han, Yitong Liu, Yixuan Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13591&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Wavelet-domain Diffusion for 3D Shape Generation, Inversion, and Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Ruihui Li, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; SIGGRAPH ASIA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00190&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/edward1997104/Wavelet-Generation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Biao Zhang, Jiapeng Tang, Matthias Niessner, Peter Wonka&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://1zb.github.io/3DShape2VecSet/&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/1zb/3DShape2VecSet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Zhang, Naye Ji, Fuxing Gao, Yongping Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10047&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bipartite Graph Diffusion Model for Human Interaction Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Baptiste Chopin, Hao Tang, Mohamed Daoudi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10134&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Generation, Optimization, and Planning in 3D Scenes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, Song-Chun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.06015&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://scenediffuser.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/scenediffuser/Scene-Diffuser&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai, Nicu Sebe&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jumin Lee, Woobin Im, Sebin Lee, Sung-Eui Yoon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.00527&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zoomin-lee/scene-scale-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Shenghua Gao&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.14704&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bluestyle97.github.io/dream3d/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Point-E: A System for Generating 3D Point Clouds from Complex Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, Mark Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08751&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/point-e&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real-Time Rendering of Arbitrary Surface Geometries using Learnt Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sirikonda Dhawal, Aakash KT, P.J. Narayanan&lt;/em&gt; &lt;br&gt; ICVGIP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09315&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Human Motion Synthesis and Style Transfer with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyi Chang, Edmund J. C. Findlay, Haozheng Zhang, Hubert P. H. Shum&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08526&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, Baining Guo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06135&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://3d-avatar-diffusion.microsoft.com/#/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiabao Lei, Jiapeng Tang, Kui Jia&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.05993&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jblei.site/project-pages/rgbd-diffusion.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Karbo123/RGBD-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ego-Body Pose Estimation via Ego-Head Pose Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaman Li, C. Karen Liu, Jiajun Wu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.04636&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, Christian Theobalt&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.04495&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/MoFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, Liangyan Gui&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.04493&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yccyenchicheng.github.io/SDFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Executing your Commands via Motion Diffusion in Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, Gang Yu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.04048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://chenxin.tech/mld/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenFengYe/motion-latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magic: Multi Art Genre Intelligent Choreography Dataset and Network for 3D Dance Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Xiu Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03741&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Congyue Deng, Chiyu &#34;Max&#39;&#39; Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03267&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-SDF: Text-to-Shape via Voxelized Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muheng Li, Yueqi Duan, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.03293&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ttlmh/Diffusion-SDF&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pretrained Diffusion Models for Unified Human Motion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianxin Ma, Shuai Bai, Chang Zhou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02837&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ofa-sys.github.io/MoFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuPose: Monocular 3D Human Pose Estimation via Denoising Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jeongjun Choi, Dongseok Shim, H. Jin Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02796&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PhysDiff: Physics-Guided Human Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, Jan Kautz&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02500&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/PhysDiff/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Point Cloud Generation with Straight Flows&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lemeng Wu, Dilin Wang, Chengyue Gong, Xingchao Liu, Yunyang Xiong, Rakesh Ranjan, Raghuraman Krishnamoorthi, Vikas Chandra, Qiang Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.01747&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffRF: Rendering-Guided 3D Radiance Field Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Norman Müller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nießner&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.01206&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sirwyver.github.io/DiffRF/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, Paul Guerrero&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00842&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, Greg Shakhnarovich&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.00774&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pals.ttic.edu/p/score-jacobian-chaining&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhizhuo Zhou, Shubham Tulsiani&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.00792&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sparsefusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://sparsefusion.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Neural Field Generation using Triplane Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16677&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jryanshue.com/nfd/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffPose: Toward More Reliable 3D Pose Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hossein Rahmani, Jun Liu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.16940&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/GONGJIA0208/Diffpose&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Karl Holmquist, Bastian Wandt&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16487&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/paTRICK-swk/D3DP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Se Young Chun&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.16374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://datid-3d.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360° Views&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, Zhangyang Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16431&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vita-group.github.io/NeuralLift-360/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/VITA-Group/NeuralLift-360&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kui Zhang, Hang Zhou, Jie Zhang, Qidong Huang, Weiming Zhang, Nenghai Yu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UDE: A Unified Driving Engine for Human Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixiang Zhou, Baoyuan Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16016&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zixiangzhou916.github.io/UDE/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zixiangzhou916/UDE/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, Dacheng Tao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.14108&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionSDF: Conditional Generative Modeling of Signed Distance Functions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gene Chou, Yuval Bahat, Felix Heide&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13757&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/princeton-computational-imaging/Diffusion-SDF&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tetrahedral Diffusion Models for 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikolai Kalischek, Torben Peters, Jan D. Wegner, Konrad Schindler&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13220&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IC3D: Image-Conditioned 3D Diffusion for Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cristian Sbrolli, Paolo Cudrano, Matteo Frosi, Matteo Matteucci&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10865&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Listen, denoise, action! Audio-driven motion synthesis with diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Simon Alexanderson, Rajmund Nagy, Jonas Beskow, Gustav Eje Henter&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09707&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Titas Anciukevičius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy J. Mitra, Paul Guerrero&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.09869&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Anciukevicius/RenderDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.07600&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/eladrich/latent-nerf&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReFu: Refine and Fuse the Unobserved View for Detail-Preserving Single-Image 3D Human Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyumin Shim, Minsoo Lee, Jaegul Choo&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.04753&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StructDiffusion: Object-Centric Diffusion for Semantic Rearrangement of Novel Objects&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weiyu Liu, Tucker Hermans, Sonia Chernova, Chris Paxton&lt;/em&gt; &lt;br&gt; RSS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.04604&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyuan Ren, Zhihong Pan, Xin Zhou, Le Kang&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.12315&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LION: Latent Point Diffusion Models for 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2210.06978.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/LION/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Human Joint Kinematics Diffusion-Refinement for Stochastic Motion Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dong Wei, Huaijiang Sun, Bin Li, Jianfeng Lu, Weiqing Li, Xiaoning Sun, Shengxiang Hu&lt;/em&gt; &lt;br&gt; AAAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.05976&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A generic diffusion-based approach for 3D human pose prediction in the wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Saeed Saadatnejad, Ali Rasekh, Mohammadreza Mofayezi, Yasamin Medghalchi, Sara Rajabzadeh, Taylor Mordan, Alexandre Alahi&lt;/em&gt; &lt;br&gt; ICRA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.05669&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Novel View Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.04628&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Volumetric Mesh Generator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yan Zheng, Lemeng Wu, Xingchao Liu, Zhen Chen, Qiang Liu, Qixing Huang&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.03158&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models for Styled Walking Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Edmund J. C. Findlay, Haozheng Zhang, Ziyi Chang, Hubert P. H. Shum&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.14828&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Human Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H. Bermano, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14916&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://guytevet.github.io/mdm-page/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ISS: Image as Stepping Stone for Text-Guided 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.04145&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/liuzhengzhe/ISS-Image-as-Stepping-Stone-for-Text-Guided-3D-Shape-Generation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SE(3)-DiffusionFields: Learning cost functions for joint grasp and motion optimization through diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julen Urain, Niklas Funk, Georgia Chalvatzaki, Jan Peters&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.03855&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TheCamusean/grasp_diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;First Hitting Diffusion Models for Generating Manifold, Graph and Categorical Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mao Ye, Lemeng Wu, Qiang Liu&lt;/em&gt; &lt;br&gt; NeruIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.01170&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FLAME: Free-form Language-based Motion Synthesis &amp;amp; Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jihoon Kim, Jiseob Kim, Sungjoon Choi&lt;/em&gt; &lt;br&gt; AAAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.00349&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let us Build Bridges: Understanding and Extending Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchao Liu, Lemeng Wu, Mao Ye, Qiang Liu&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.14699&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.15001&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mingyuan-zhang.github.io/projects/MotionDiffuse.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dominik J. E. Waibel, Ernst Röell, Bastian Rieck, Raja Giryes, Carsten Marr&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.14125&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiachen Sun, Weili Nie, Zhiding Yu, Z. Morley Mao, Chaowei Xiao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09801&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, Dahua Lin&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.03530&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhaoyanglyu/point_diffusion_refinement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Point Cloud Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Luo, Wei Hu&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.10981&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/luost26/score-denoise&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang Sun, Yebin Liu&lt;/em&gt; &lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08000&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://liuyebin.com/diffustereo/diffustereo.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DSaurus/DiffuStereo&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Shape Generation and Completion through Point-Voxel Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Linqi Zhou, Yilun Du, Jiajun Wu&lt;/em&gt; &lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.03670&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://alexzhou907.github.io/pvd&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Apr 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Models for 3D Point Cloud Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Luo, Wei Hu&lt;/em&gt; &lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.01458&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/luost26/diffusion-point-cloud&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Mar 2021&lt;/p&gt; &#xA;&lt;h3&gt;Adversarial Attack&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;DIFFender: Diffusion-Based Adversarial Defense against Patch Attacks in the Physical World&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Caixin Kang, Yinpeng Dong, Zhengyi Wang, Shouwei Ruan, Hang Su, Xingxing Wei&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09124&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fei Kong, Jinhao Duan, RuiPeng Ma, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18355&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haotian Xue, Alexandre Araujo, Bin Hu, Yongxin Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16494&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xavihart/Diff-PGD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Differentially Private Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Saiyue Lyu, Margarita Vinaroz, Michael F. Liu, Mijung Park&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15759&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Magic: An Investigation into Adversarial Examples Crafted in the Semantic Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;BoYang Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12906&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mist: Towards Improved Adversarial Examples for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chumeng Liang, Xiaoyu Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12683&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Content-based Unrestricted Adversarial Attack&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaoyu Chen, Bo Li, Shuang Wu, Kaixun Jiang, Shouhong Ding, Wenqiang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10665&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yihao Huang, Qing Guo, Felix Juefei-Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10701&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Raising the Bar for Certified Adversarial Robustness with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Thomas Altstidl, David Dobre, Björn Eskofier, Gauthier Gidel, Leo Schwinn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10388&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Imperceptible and Transferable Adversarial Attack&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianqi Chen, Hao Chen, Keyan Chen, Yilan Zhang, Zhengxia Zou, Zhenwei Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08192&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/WindVChen/DiffAttack&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On enhancing the robustness of Vision Transformers: Defensive Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Raza Imam, Muhammad Huzaifa, Mohammed El-Amine Azz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08031&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Muhammad-Huzaifaa/Defensive_Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Steganography Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ping Wei, Qing Zhou, Zichi Wang, Zhenxing Qian, Xinpeng Zhang, Sheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03472&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haomin Zhuang, Yihua Zhang, Sijia Liu&lt;/em&gt; &lt;br&gt; CVPR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16378&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Black-box Backdoor Defense via Zero-shot Image Purification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Ninghao Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12175&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial Counterfactual Visual Explanations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Jeanneret, Loïc Simon, Frédéric Jurie&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09962&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/guillaumejs2403/ACE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Robust Evaluation of Diffusion-Based Adversarial Purification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minjong Lee, Dongwoo Kim&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09051&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Devil&#39;s Advocate: Shattering the Illusion of Unexploitable Data using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08500&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weixin Chen, Dawn Song, Bo Li&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05762&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chenweixin107/TrojDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Model-Based Attack on Learnable Image Encryption for Privacy-Preserving Deep Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;AprilPyone MaungMaung, Hitoshi Kiya&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05036&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Differentially Private Diffusion Models Generate Useful Synthetic Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De, Samuel L. Smith, Olivia Wiles, Borja Balle&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13861&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Derui Zhu, Dingfan Chen, Jens Grossklags, Mario Fritz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07801&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Raising the Cost of Malicious AI-Powered Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, Aleksander Madry&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.06588&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MadryLab/photoguard&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song, Zhengui Xue, Ruhui Ma, Haibing Guan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04578&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Better Diffusion Models Further Improve Adversarial Training&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, Shuicheng Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04638&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wzekai99/DM-Improves-AT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Membership Inference Attacks against Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tomoya Matsumoto, Takayuki Miura, Naoto Yanai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03262&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MorDIFF: Recognition Vulnerability and Attack Detectability of Face Morphing Attacks Created by Diffusion Autoencoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Naser Damer, Meiling Fang, Patrick Siebke, Jan Niklas Kolf, Marco Huber, Fadi Boutros&lt;/em&gt; &lt;br&gt; IWBF 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01843&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/naserdamer/mordiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Extracting Training Data from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00860&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Are Diffusion Models Vulnerable to Membership Inference Attacks?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinhao Duan, Fei Kong, Shiqi Wang, Xiaoshuang Shi, Kaidi Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Salient Conditional Diffusion for Defending Against Backdoor Attacks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Brandon B. May, N. Joseph Tatro, Piyush Kumar, Nathan Shnidman&lt;/em&gt; &lt;br&gt; ICLR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13721&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Extracting Training Data from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13188&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Membership Inference of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hailong Hu, Jun Pang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.09956&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models as a Defense against Adversarial Attacks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lars Lien Ankile, Anna Midgley, Sebastian Weisshaar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.06871&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ankile/Adversarial-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fight Fire With Fire: Reversing Skin Adversarial Examples by Multiscale Diffusive and Denoising Aggregation Mechanism&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yongwei Wang, Yuan Li, Zhiqi Shen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.10373&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DensePure: Understanding Diffusion Models towards Adversarial Robustness&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaowei Xiao, Zhongzhu Chen, Kun Jin, Jiongxiao Wang, Weili Nie, Mingyan Liu, Anima Anandkumar, Bo Li, Dawn Song&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00322&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Adversarial Robustness by Contrastive Guided Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yidong Ouyang, Liyan Xie, Guang Cheng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.09643&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Differentially Private Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Dockhorn, Tianshi Cao, Arash Vahdat, Karsten Kreis&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.09929&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/DPDM/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiachen Sun, Weili Nie, Zhiding Yu, Z. Morley Mao, Chaowei Xiao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09801&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Threat Model-Agnostic Adversarial Defense using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tsachi Blau, Roy Ganz, Bahjat Kawar, Alex Bronstein, Michael Elad&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08089&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tsachiblau/Threat-Model-Agnostic-Adversarial-Defense-using-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Back to the Source: Diffusion-Driven Test-Time Adaptation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan Shelhamer, Dequan Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.03442&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/shiyegao/DDA&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guided Diffusion Model for Adversarial Purification from Random Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Quanlin Wu, Hang Ye, Yuntian Gu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.10875&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Certified!!) Adversarial Robustness for Free!&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nicholas Carlini, Florian Tramer, Krishnamurthy (Dj)Dvijotham, J. Zico Kolter&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2206.10550&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guided Diffusion Model for Adversarial Purification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinyi Wang, Zhaoyang Lyu, Dahua Lin, Bo Dai, Hongfei Fu&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.14969&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jinyiw/guideddiffusionpur&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Adversarial Purification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, Anima Anandkumar&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.07460&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffpure.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/DiffPure&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TFDPM: Attack detection for cyber-physical systems with diffusion probabilistic models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tijin Yan, Tong Zhou, Yufeng Zhan, Yuanqing Xia&lt;/em&gt; &lt;br&gt; Elsveier Knowledge-Based Systems 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.10774&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial purification with Score-based generative models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jongmin Yoon, Sung Ju Hwang, Juho Lee&lt;/em&gt; &lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.06041&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jmyoon1/adp&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Jun 2021&lt;/p&gt; &#xA;&lt;h3&gt;Miscellany&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;ID-Pose: Sparse-view Camera Pose Estimation by Inverting Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weihao Cheng, Yan-Pei Cao, Ying Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17140&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhongwei Qiu, Qiansheng Yang, Jian Wang, Xiyu Wang, Chang Xu, Dongmei Fu, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17074&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionSTR: Diffusion Model for Scene Text Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Masato Fujitake&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16707&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Morphing Attack Detection with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marija Ivanovska, Vitomir Štruc&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.15733&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianyuan Wang, Christian Rupprecht, David Novotny&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.15667&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Majed El Helou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14891&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards More Realistic Membership Inference Attacks on Large Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jan Dubiński, Antoni Kowalczuk, Stanisław Pawlak, Przemysław Rokita, Tomasz Trzciński, Paweł Morawiecki&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12983&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffWA: Diffusion Models for Watermark Attack&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinyu Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12790&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving visual image reconstruction from human brain activity using latent diffusion models via multiple decoded inputs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Takagi, Shinji Nishimoto&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11536&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion model based data generation for partial differential equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rucha Apte, Sheel Nidhan, Rishikesh Ranade, Jay Pathak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11075&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GenPose: Generative Category-level Object Pose Estimation via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiyao Zhang, Mingdong Wu, Hao Dong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10531&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Drag-guided diffusion models for vehicle image generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikos Arechiga, Frank Permenter, Binyang Song, Chenyang Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09935&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;R2-Diff: Denoising by diffusion as a refinement of retrieved motion for image-based motion prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Takeru Oba, Norimichi Ukita&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09483&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Robustness of Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianping Zhang, Zhuoer Xu, Shiwen Cui, Changhua Meng, Weibin Wu, Michael R. Lyu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08257&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.06874&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting GUI Prototyping with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, Gérard Dray&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.06233&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Extraction and Recovery of Spatio-Temporal Structure in Latent Dynamics Alignment with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yule Wang, Zijing Wu, Chengrui Li, Anqi Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.06138&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alexwangNTL/ERDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yida Chen, Fernanda Viégas, Martin Wattenberg&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05720&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PriSampler: Mitigating Property Inference of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hailong Hu, Jun Pang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05208&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;George Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan Liu, Anthony L. Caterini, J. Eric T. Taylor, Gabriel Loaiza-Ganem&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04675&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/layer6ai-labs/dgm-eval&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Phoenix: A Federated Generative Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fiona Victoria Stanley Jothiraj, Afra Mashhadi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04098&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-dimensional and Permutation Invariant Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vinicius Mikuni, Benjamin Nachman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03933&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Emergent Correspondence from Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, Bharath Hariharan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03881&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Phoenix: A Federated Generative Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fiona Victoria Stanley Jothiraj, Afra Mashhadi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04098&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Change Diffusion: Change Detection Map Generation Based on Difference-Feature Guided DDPM&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yihan Wen, Jialu Sui, Xianping Ma, Wendi Liang, Xiaokang Zhang, Man-On Pun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03424&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Visual Foundational Models of Physical Scenes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chethan Parameshwara, Alessandro Achille, Matthew Trager, Xiaolong Li, Jiawei Mo, Matthew Trager, Ashwin Swaminathan, CJ Taylor, Dheera Venkatraman, Xiaohan Fei, Stefano Soatto&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03727&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Protecting the Intellectual Property of Diffusion Models by the Watermark Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sen Peng, Yufei Chen, Cong Wang, Xiaohua Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03436&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Emergent Correspondence from Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, Bharath Hariharan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03881&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusionfeatures.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhance Diffusion to Improve Robust Generalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianhui Sun, Sanchit Sinha, Aidong Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02618&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training Data Attribution for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheng Dai, David K Gifford&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02174&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Classifier Mimicry without Data Access&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Steven Braun, Martin Mundt, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02090&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quantifying Sample Anonymity in Score-Based Generative Models with Adversarial Fingerprinting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mischa Dombrowski, Bernhard Kainz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01363&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Autoencoders as Watermark Attackers: Analyses of Vulnerabilities and Threats&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuandong Zhao, Kexun Zhang, Yu-Xiang Wang, Lei Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01953&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiacheng Chen, Ruizhi Deng, Yasutaka Furukawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01461&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unlearnable Examples for Diffusion Models: Protect Data from Unauthorized Exploitation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengyue Zhao, Jinhao Duan, Xing Hu, Kaidi Xu, Chenan Wang, Rui Zhang, Zidong Du, Qi Guo, Yunji Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01902&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and Compatible Triggers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Yong Zhang, Yanbo Fan, Baoyuan Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00816&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20030&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YuxinWenRick/tree-ring-watermark&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GANDiffFace: Controllable Generation of Synthetic Datasets for Face Recognition with Realistic Variations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pietro Melzi, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Dominik Lawatsch, Florian Domin, Maxim Schaubert&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19962&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Handwritten OCR with Training Samples Generated by Glyph Conditional Denoising Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haisong Ding, Bozhi Luan, Dongnan Gui, Kai Chen, Qiang Huo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19543&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jian Chen, Ruiyi Zhang, Tong Yu, Rohan Sharma, Zhiqiang Xu, Tong Sun, Changyou Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19518&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/puar-playground/LRA-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffMatch: Diffusion Model for Dense Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jisu Nam, Gyuseong Lee, Sunwoo Kim, Hyeonsu Kim, Hyoungwon Cho, Seyeon Kim, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19094&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ku-cvlab.github.io/DiffMatch/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Calliffusion: Chinese Calligraphy Generation and Style Transfer with Diffusion Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qisheng Liao, Gus Xia, Zhinuo Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19124&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Stego: Training-free Diffusion Generative Steganography via Message Projection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daegyu Kim, Chaehun Shin, Jooyoung Choi, Dahuin Jung, Sungroh Yoon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18726&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Diffusion Modeling for Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Victor Livernoche, Vineet Jain, Yashar Hezaveh, Siamak Ravanbakhsh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18593&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Aligning Optimization Trajectories with Diffusion Models for Constrained Design Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giorgio Giannone, Akash Srivastava, Ole Winther, Faez Ahmed&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18470&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Driving Scenes with Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ethan Pronovost, Kai Wang, Nick Roy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18452&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Diffusion for 3D Turbulent Flows&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marten Lienen, Jan Hansen-Palmus, David Lüdke, Stephan Günnemann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01776&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GlyphControl: Glyph Conditional Control for Visual Text Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukang Yang, Dongnan Gui, Yuhui Yuan, Haisong Ding, Han Hu, Kai Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18259&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AIGText/GlyphControl-release&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity Image Compression with Score-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Eirikur Agustsson, Fabian Mentzer, Luca Versari, George Toderici, Lucas Theis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18231&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CamoDiffusion: Camouflaged Object Detection via Conditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhongxi Chen, Ke Sun, Xianming Lin, Rongrong Ji&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17932&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionNAG: Task-guided Neural Architecture Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sohyun An, Hayeon Lee, Jaehyeong Jo, Seanie Lee, Sung Ju Hwang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16943&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiwen Yu, Xuanyu Zhang, Youmin Xu, Jian Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16936&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vvictoryuki/CRoSS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yingqian Cui, Jie Ren, Han Xu, Pengfei He, Hui Liu, Lichao Sun, Jiliang Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04642&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Realistic Noise Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qi Wu, Mingyan Han, Ting Jiang, Haoqiang Fan, Bing Zeng, Shuaicheng Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14022&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anomaly Detection with Conditioned Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arian Mousakhan, Thomas Brox, Jawad Tayyub&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15956&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anomaly Detection in Satellite Videos using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Akash Awasthi, Son Ly, Jaer Nizam, Samira Zare, Videet Mehta, Safwan Ahmed, Keshav Shah, Ramakrishna Nemani, Saurabh Prasad, Hien Van Nguyen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05376&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Knowledge Diffusion for Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Huang, Yuan Zhang, Mingkai Zheng, Shan You, Fei Wang, Chen Qian, Chang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15712&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hunto/DiffKD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongnan Gui, Kai Chen, Haisong Ding, Qiang Huo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15660&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Semantic Correspondence Using Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15581&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, Ming-Hsuan Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15347&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sd-complements-dino.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14334&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusion-hyperfeatures.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial Privacy Protection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiang Liu, Chun Pong Lau, Rama Chellappa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13625&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GSURE-Based Diffusion Model Training with Corrupted Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Noam Elata, Tomer Michaeli, Michael Elad&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13128&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bahjat-kawar/gsure-diffusion/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Watermarking Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, Yang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12502&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffUCD:Unsupervised Hyperspectral Image Change Detection with Semantic Correlation Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangrong Zhang, Shunli Tian, Guanchun Wang, Huiyu Zhou, Licheng Jiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12410&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Incomplete Multi-view Clustering via Diffusion Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sifan Fang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11489&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SlotDiffusion: Object-Centric Generative Modeling with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, Animesh Garg&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11281&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09847&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Method for Training-free Person Image Picture Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianyu Chen&lt;/em&gt; &lt;br&gt; ICOAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09817&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructing a personalized AI assistant for shear wall layout using Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lufeng Wang, Jiepeng Liu, Guozhong Cheng, En Liu, Wei Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10830&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffUTE: Universal Text Editing Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen, Haoxing, Xu, Zhuoer, Gu, Zhangxuan, Lan, Jun, Zheng, Xing, Li, Yaohui, Meng, Changhua, Zhu, Huijia, Wang, Weiqiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10825&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chenhaoxing/DiffUTE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CDDM: Channel Denoising Diffusion Models for Wireless Communications&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tong Wu, Zhiyong Chen, Dazhi He, Liang Qian, Yin Xu, Meixia Tao, Wenjun Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09161&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unlearnable Examples Give a False Sense of Security: Piercing through Unexploitable Data with Learnable Examples&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wan Jiang, Yunfeng Diao, He Wang, Jianxin Sun, Meng Wang, Richang Hong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09241&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Dataset Generation: Towards Closing the Sim2Real Gap for Pedestrian Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrew Farley, Mohsen Zand, Michael Greenspan&lt;/em&gt; &lt;br&gt; CRV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09401&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Reproducible Extraction of Training Images from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryan Webster&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08694&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ryanwebster90/onestep-extraction&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Antoni Bigata Casademunt, Rodrigo Mira, Nikita Drobyshev, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08854&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Manipulating Visually-aware Federated Recommender Systems and Its Countermeasures&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wei Yuan, Shilong Yuan, Chaoqun Yang, Quoc Viet Hung Nguyen, Hongzhi Yin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08183&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Undercover Deepfakes: Detecting Fake Segments in Videos&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sanjay Saha, Rashindrie Perera, Sachith Seneviratne, Tamasha Malepathirana, Sanka Rasnayaka, Deshani Geethika, Terence Sim, Saman Halgamuge&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06564&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sanjaysaha1311/temporal-deepfake-segmentation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Comprehensive Dataset of Synthetic and Manipulated Overhead Imagery for Development and Evaluation of Forensic Tools&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Brandon B. May, Kirill Trapeznikov, Shengbang Fang, Matthew C. Stamm&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05784&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Žiga Babnik, Peter Peer, Vitomir Štruc&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05768&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, Hang Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04175&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring One-shot Semi-supervised Federated Learning with A Pre-trained Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04063&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Prompt-robust Face Privacy Protection via Adversarial Decoupling Augmentation Framework&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruijia Wu, Yuhang Wang, Huafeng Shi, Zhipeng Yu, Yichao Wu, Ding Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03980&#34;&gt;Paper&lt;/a&gt;] 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Feature Refinement for Continuous Sign Language Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Leming Guo, Wanli Xue, Qing Guo, Yuxi Zhou, Tiantian Yuan, Shengyong Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03614&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayoutDM: Transformer-based Diffusion Model for Layout Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shang Chai, Liansheng Zhuang, Fengying Yan&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.02567&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Long-Term Rhythmic Video Soundtracker&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiashuo Yu, Yaohui Wang, Xinyuan Chen, Xiao Sun, Yu Qiao&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01319&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OpenGVLab/LORIS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Putting People in Their Place: Affordance-Aware Human Insertion into Scenes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu, Jimei Yang, Jingwan Lu, Alexei A. Efros, Krishna Kumar Singh&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14406&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sumith1896.github.io/affordance-insertion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adobe-research/affordance-insertion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Single-View Height Estimation with Conditional Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Isaac Corley, Peyman Najafirad&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13214&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Model Based Accurate and High-Degree-of-Freedom Metasurface Inverse Design&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zezhou Zhang, Chuanchuan Yang, Yifeng Qin, Hao Feng, Jiqiang Feng, Hongbin Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13038&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Synthetically Generated Image Detection in Cross-Concept Settings&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pantelis Dogoulis, Giorgos Kordopatis-Zilos, Ioannis Kompatsiaris, Symeon Papadopoulos&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12053&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Diffusion Probabilistic Model Sampling through the lens of Backward Error Analysis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yansong Gao, Zhihong Pan, Xin Zhou, Le Kang, Pratik Chaudhari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11446&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via GPU-Aware Optimizations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Hui Chen, Raman Sarokin, Juhyun Lee, Jiuqiang Tang, Chuo-Ling Chang, Andrei Kulik, Matthias Grundmann&lt;/em&gt; &lt;br&gt; CVPR 2023 Workshop. [&lt;a href=&#34;https://arxiv.org/abs/2304.11267&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A data augmentation perspective on diffusion models and retrieval&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max F. Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, Chris Russell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10253&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models with location-scale noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexia Jolicoeur-Martineau, Kilian Fatras, Ke Li, Tal Kachman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05907&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Diffusion Models for Unsupervised Video Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anil Osman Tur, Nicola Dall&#39;Asen, Cigdem Beyan, Elisa Ricci&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05841&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CamDiff: Camouflage Image Augmentation via Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xue-Jing Luo, Shuo Wang, Zongwei Wu, Christos Sakaridis, Yun Cheng, Deng-Ping Fan, Luc Van Gool&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05469&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/drlxj/CamDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDRF: Denoising Diffusion Model for Remote Sensing Image Fusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;ZiHan Cao, ShiQi Cao, Xiao Wu, JunMing Hou, Ran Ran, Liang-Jian Deng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04774&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CCLAP: Controllable Chinese Landscape Painting Generation via Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhongqi Wang, Jie Zhang, Zhilong Ji, Jinfeng Bai, Shiguang Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04156&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ChiroDiff: Modelling chirographic data with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang, Yi-Zhe Song&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03785&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ayandas.me/chirodiff&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 7 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RoSteALS: Robust Steganography using Autoencoder Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tu Bui, Shruti Agarwal, Ning Yu, John Collomosse&lt;/em&gt; &lt;br&gt; CVPR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03400&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TuBui/RoSteALS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JPEG Compressed Images Can Bypass Protections Against AI Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pedro Sandoval-Segura, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02234&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning to Read Braille: Bridging the Tactile Reality Gap with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Carolina Higuera, Byron Boots, Mustafa Mukadam&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01182&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Textile Pattern Generation Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Halil Faruk Karagoz, Gulcin Baykal, Irem Arikan Eksi, Gozde Unal&lt;/em&gt; &lt;br&gt; ITFC 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.00520&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Parents and Children: Distinguishing Multimodal DeepFakes from Natural Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roberto Amoroso, Davide Morelli, Marcella Cornia, Lorenzo Baraldi, Alberto Del Bimbo, Rita Cucchiara&lt;/em&gt; &lt;br&gt; ACM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.00500&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeuroDAVIS: A neural network model for data visualization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chayan Maitra, Dibyendu B. Seal, Rajat K. De&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01222&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Action Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daochang Liu, Qiyue Li, AnhDung Dinh, Tingting Jiang, Mubarak Shah, Chang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17959&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One-shot Unsupervised Domain Adaptation with Personalized Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton, Stéphane Lathuilière&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.18080&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDP: Diffusion Model for Dense Visual Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, Ping Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17559&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WordStylist: Styled Verbatim Handwritten Text Generation with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Konstantina Nikolaidou, George Retsinas, Vincent Christlein, Mathias Seuret, Giorgos Sfikas, Elisa Barney Smith, Hamam Mokayed, Marcus Liwicki&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16576&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual Chain-of-Thought Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;William Harvey, Frank Wood&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16187&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sauradip Nag, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14863&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Stable Signature: Rooting Watermarks in Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pierre Fernandez, Guillaume Couairon, Hervé Jégou, Matthijs Douze, Teddy Furon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15435&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pierrefdz.github.io/publications/stablesignature/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Continual Learning of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michał Zając, Kamil Deja, Anna Kuzina, Jakub M. Tomczak, Tomasz Trzciński, Florian Shkurti, Piotr Miłoś&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15342&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Freestyle Layout-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Han Xue, Zhiwu Huang, Qianru Sun, Li Song, Wenjun Zhang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14412&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Inversion of Black-Box Face-Recognition Models via Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Manuel Kansy, Anton Raël, Graziana Mignone, Jacek Naruniec, Christopher Schroers, Markus Gross, Romann M. Weber&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13006&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;End-to-End Diffusion Latent Optimization Improves Classifier Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bram Wallace, Akash Gokul, Stefano Ermon, Nikhil Naik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13703&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffPattern: Layout Pattern Generation via Discrete Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixiao Wang, Yunheng Shen, Wenqian Zhao, Yang Bai, Guojin Chen, Farzan Farnia, Bei Yu&lt;/em&gt; &lt;br&gt; DAC 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13060&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffuse-Denoise-Count: Accurate Crowd-Counting with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yasiru Ranasinghe, Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12790&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyi Zhang, Jiaqi Guo, Shizhao Sun, Jian-Guang Lou, Dongmei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11589&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Positional Diffusion: Ordering Unordered Sets with Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Francesco Giuliari, Gianluca Scarpellini, Stuart James, Yiming Wang, Alessio Del Bue&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11120&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://iit-pavis.github.io/Positional_Diffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Leapfrog Diffusion Model for Stochastic Trajectory Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weibo Mao, Chenxin Xu, Qi Zhu, Siheng Chen, Yanfeng Wang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10895&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MediaBrain-SJTU/LED&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pluralistic Aging Diffusion Autoencoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peipei Li, Rui Wang, Huaibo Huang, Ran He, Zhaofeng He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11086&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AnimeDiffusion: Anime Face Line Drawing Colorization via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Cao, Xiangqiao Meng, P.Y. Mok, Xueting Liu, Tong-Yee Lee, Ping Li&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11137&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Document Layout Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Liu He, Yijuan Lu, John Corring, Dinei Florencio, Cha Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10787&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the De-duplication of LAION-2B&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryan Webster, Julien Rabin, Loic Simon, Frederic Jurie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12733&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Recipe for Watermarking Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, Min Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10137&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yunqing-me/WatermarkDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DIRE for Diffusion-Generated Image Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, Houqiang Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09295&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZhendongWang6/DIRE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Maham Tanveer, Yizhi Wang, Ali Mahdavi-Amiri, Hao Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09604&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ds-fusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionAD: Denoising Diffusion for Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hui Zhang, Zheng Wang, Zuxuan Wu, Yu-Gang Jiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08730&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayoutDM: Discrete Diffusion Model for Controllable Layout Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, Kota Yamaguchi&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08137&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cyberagentailab.github.io/layout-dm/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CyberAgentAILab/layout-dm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Parallel Vertex Diffusion for Unified Visual Grounding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zesen Cheng, Kehan Li, Peng Jin, Xiangyang Ji, Li Yuan, Chang Liu, Jie Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07216&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixiang Zhao, Haowen Bai, Yuanzhi Zhu, Jiangshe Zhang, Shuang Xu, Yulun Zhang, Kai Zhang, Deyu Meng, Radu Timofte, Luc Van Gool&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06840&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Detecting Images Generated by Diffusers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Davide Alessandro Coccomini, Andrea Esuli, Fabrizio Falchi, Claudio Gennaro, Giuseppe Amato&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05275&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Layout Generation with a Decoupled Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mude Hui, Zhizheng Zhang, Xiaoyi Zhang, Wenxuan Xie, Yuwang Wang, Yan Lu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05049&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elad Levi, Eli Brosh, Mykola Mykhailych, Meir Perez&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03755&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cindy M. Nguyen, Eric R. Chan, Alexander W. Bergman, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04291&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ccnguyen.github.io/diffusion-in-the-dark/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 7 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Word-As-Image for Semantic Typography&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel Cohen-Or, Ariel Shamir&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://wordasimage.github.io/Word-As-Image-Page/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Makeup Extraction of 3D Representation via Illumination-Aware Image Decomposition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchao Yang, Takafumi Taketomi, Yoshihiro Kanamori&lt;/em&gt; &lt;br&gt; Eurographics 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13279&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Monocular Depth Estimation using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, David J. Fleet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.14816&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://depth-gen.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spatial-temporal Transformer-guided Diffusion based Data Augmentation for Efficient Skeleton-based Action Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifan Jiang, Han Chen, Hanseok Ko&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13434&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LDFA: Latent Diffusion Face Anonymization for Self-driving Applications&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marvin Klemp, Kevin Rösch, Royden Wagner, Jannik Quehl, Martin Lauer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08931&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Road Redesign Technique Achieving Enhanced Road Safety by Inpainting with a Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sumit Mishra, Medhavi Mishra, Taeyoung Kim, Dongsoo Har&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07440&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Effective Data Augmentation With Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Brandon Trabucco, Kyle Doherty, Max Gurinas, Ruslan Salakhutdinov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07944&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://btrabuc.co/da-fusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning End-to-End Channel Coding with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muah Kim, Rick Fritschek, Rafael F. Schaefer&lt;/em&gt; &lt;br&gt; WSA/SCC 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01714&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Extracting Training Data from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13188&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for High-Resolution Solar Forecasts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yusuke Hatanaka, Yannik Glaser, Geoff Galgon, Giuseppe Torri, Peter Sadowski&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00170&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Denoising Diffusion Model for Fluid Field Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gefan Yang, Stefan Sommer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11661&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Victor Boutin, Thomas Fel, Lakshya Singhal, Rishav Mukherji, Akash Nagaraj, Julien Colin, Thomas Serre&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11722&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PLay: Parametrically Conditioned Layout Generation using Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chin-Yi Cheng, Forrest Huang, Gang Li, Yang Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11529&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LEGO-Net: Learning Regular Rearrangements of Objects in Rooms&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiuhong Anna Wei, Sijie Ding, Jeong Joon Park, Rahul Sajnani, Adrien Poulenard, Srinath Sridhar, Leonidas Guibas&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.09629&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ivl.cs.brown.edu/#/projects/lego-net&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 23 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jun Yue, Leyuan Fang, Shaobo Xia, Yue Deng, Jiayi Ma&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.08072&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Image Compression with a Diffusion-Based Decoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Noor Fathima Goose, Jens Petersen, Auke Wiggers, Tianlin Xu, Guillaume Sautière&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.05489&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models For Stronger Face Morphing Attacks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zander Blasingame, Chen Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.04218&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AI Art in Architecture&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joern Ploennigs, Markus Berger&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09399&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusing Surrogate Dreams of Video Scenes to Predict Video Memorability&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lorin Sweeney, Graham Healy, Alan F. Smeaton&lt;/em&gt; &lt;br&gt; MediaEval Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09308&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-Font: Diffusion Model for Robust One-Shot Font Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haibin He, Xinyuan Chen, Chaoyue Wang, Juhua Liu, Bo Du, Dacheng Tao, Yu Qiao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.05895&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to Backdoor Diffusion Models?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.05400&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.03860&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/somepago/DCR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ObjectStitch: Generative Object Compositing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, Daniel Aliaga&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00932&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Post-training Quantization on Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, Yan Yan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.15736&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/42Shawn/PTQ4DM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Model Made Slim&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingyi Yang, Daquan Zhou, Jiashi Feng, Xinchao Wang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.17106&#34;&gt;Paper&lt;/a&gt;]&lt;br&gt; 27 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;German Barquero, Sergio Escalera, Cristina Palmero&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.14304&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://barquerogerman.github.io/BeLFusion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/BarqueroGerman/BeLFusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JigsawPlan: Room Layout Jigsaw Puzzle Extreme Structure from Motion using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sepidehsadat Hosseini, Mohammad Amin Shabani, Saghar Irandoust, Yasutaka Furukawa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13785&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sepidsh.github.io/JigsawPlan/index.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammad Amin Shabani, Sepidehsadat Hosseini, Yasutaka Furukawa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13287&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://aminshabani.github.io/housediffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can denoising diffusion probabilistic models generate realistic astrophysical fields?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nayantara Mudur, Douglas P. Finkbeiner&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12444&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffDreamer: Consistent Single-view Perpetual View Generation with Conditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad Shahbazi, Anton Obukhov, Luc Van Gool, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12131&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://primecai.github.io/diffdreamer&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CaDM: Codec-aware Diffusion Modeling for Neural-enhanced Video Streaming&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qihua Zhou, Ruibin Li, Song Guo, Yi Liu, Jingcai Guo, Zhenda Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.08428&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Extreme Generative Image Compression by Learning Text Embedding from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhihong Pan, Xin Zhou, Hao Tian&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.07793&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Evaluating a Synthetic Image Dataset Generated with Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andreas Stöckl&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01777&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the detection of synthetic images generated by diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, Luisa Verdoliva&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00680&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/grip-unina/DMimageDetection&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DOLPH: Diffusion Models for Phase Retrieval&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shirin Shoushtari, Jiaming Liu, Ulugbek S. Kamilov&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00529&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards the Detection of Diffusion Model Deepfakes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonas Ricker, Simon Damm, Thorsten Holz, Asja Fischer&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.14571&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Data Augmentation for Weed Recognition Enhancement: A Diffusion Probabilistic Model and Transfer Learning Based Approach&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dong Chen, Xinda Qi, Yu Zheng, Yuzhen Lu, Zhaojian Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.09509&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DongChen06/DMWeeds&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeyang Sha, Zheng Li, Ning Yu, Yang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.06998&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Markup-to-Image Diffusion Models with Scheduled Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuntian Deng, Noriyuki Kojima, Alexander M. Rush&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.05147&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What the DAAM: Interpreting Stable Diffusion Using Cross Attention&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Raphael Tang, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Jimmy Lin, Ferhan Ture&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.04885&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/castorini/daam&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIP-Diffusion-LM: Apply Diffusion Model on Image Captioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.04559&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xu-shitong/diffusion-image-captioning&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models Beat GANs on Topology Optimization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;François Mazé, Faez Ahmed&lt;/em&gt; &lt;br&gt; AAAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09591&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://decode.mit.edu/projects/topodiff/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/francoismaze/topodiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vector Quantized Diffusion Model with CodeUnet for Text-to-Sign Pose Sequences Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pan Xie, Qipeng Zhang, Zexian Li, Hao Tang, Yao Du, Xiaohui Hu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09141&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Diffusion Models for Seismic Processing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ricard Durall, Ammar Ghanim, Mario Fernandez, Norman Ettrich, Janis Keuper&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.10451&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stochastic Trajectory Prediction via Motion Indeterminacy Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, Jiwen Lu&lt;/em&gt;&lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.13777&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gutianpei/MID&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Mar 2022&lt;/p&gt; &#xA;&lt;h2&gt;Audio&lt;/h2&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Heeseung Kim, Sungwon Kim, Jiheum Yeom, Sungroh Yoon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16083&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shivam Mehta, Siyang Wang, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09417&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ji-Sang Hwang, Sang-Hoon Lee, Seong-Whan Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.06814&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haogeng Liu, Tao Wang, Jie Cao, Ran He, Jianhua Tao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05708&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haobin Tang, Xulong Zhang, Jianzong Wang, Ning Cheng, Jing Xiao&lt;/em&gt; &lt;br&gt; InterSpeech 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00648&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Neural Music Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max W. Y. Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, Xuchen Song, Jitong Chen, Yuping Wang, Yuxuan Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15719&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://efficient-melody.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating symbolic music using diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lilac Atassi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08385&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseRoll: Multi-track multi-category music generation based on diffusion model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongfei Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07794&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Source Diffusion Models for Simultaneous Music Generation and Separation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, Emanuele Rodolà&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02257&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gladia-research-group.github.io/multi-source-diffusion-models/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, Baining Guo&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.09478&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/researchmm/MM-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDMuse: Stochastic Differential Music Editing and Generation via Hybrid Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Zhang, Yi Ren, Kejun Zhang, Shuicheng Yan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00222&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sdmuse.github.io/posts/sdmuse/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Full-band General Audio Synthesis with Score-based Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Santiago Pascual, Gautam Bhattacharya, Chunghsin Yeh, Jordi Pons, Joan Serrà&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.14661&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchical Diffusion Models for Singing Voice Neural Vocoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Naoya Takahashi, Mayank Kumar, Singh, Yuki Mitsufuji&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.07508&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mandarin Singing Voice Synthesis with Denoising Diffusion Probabilistic Wasserstein GAN&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yin-Ping Cho, Yu Tsao, Hsin-Min Wang, Yi-Wen Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.10446&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yinping-cho.github.io/diffwgansvs.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDSP-based Singing Vocoders: A New Subtractive-based Synthesizer and A Comprehensive Evaluation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Da-Yi Wu, Wen-Yi Hsiao, Fu-Rong Yang, Oscar Friedman, Warren Jackson, Scott Bruzenak, Yi-Wen Liu, Yi-Hsuan Yang&lt;/em&gt; &lt;br&gt; ISMIR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.04756&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YatingMusic/ddsp-singing-vocoders/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rongjie Huang, Zhou Zhao, Huadai Liu, Jinglin Liu, Chenye Cui, Yi Ren&lt;/em&gt; &lt;br&gt; ACM Multimedia 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.06389&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://prodiff.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 13 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CARD: Classification and Regression Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xizewen Han, Huangjie Zheng, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07275&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial Audio Synthesis with Complex-valued Polynomial Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yongtao Wu, Grigorios G Chrysos, Volkan Cevher&lt;/em&gt; &lt;br&gt; ICML workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.06811&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-instrument Music Synthesis with Spectrogram Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Curtis Hawthorne, Ian Simon, Adam Roberts, Neil Zeghidour, Josh Gardner, Ethan Manilow, Jesse Engel&lt;/em&gt; &lt;br&gt; ISMIR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.05408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for Binaural Audio Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yichong Leng, Zehua Chen, Junliang Guo, Haohe Liu, Jiawei Chen, Xu Tan, Danilo Mandic, Lei He, Xiang-Yang Li, Tao Qin, Sheng Zhao, Tie-Yan Liu&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.14807&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://speechresearch.github.io/binauralgrad/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu, Yi Ren, Zhou Zhao&lt;/em&gt; &lt;br&gt; IJCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.09934&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fastdiff.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Rongjiehuang/FastDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SpecGrad: Diffusion Probabilistic Model based Neural Vocoder with Adaptive Noise Spectral Shaping&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuma Koizumi, Heiga Zen, Kohei Yatabe, Nanxin Chen, Michiel Bacchiani&lt;/em&gt; &lt;br&gt; Interspeech 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.16749&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BDDM: Bilateral Denoising Diffusion Models for Fast and High-Quality Speech Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.13508&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tencent-ailab/bddm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ItôWave: Itô Stochastic Differential Equation Is All You Need For Wave Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shoule Wu, Ziqiang Shi&lt;/em&gt; &lt;br&gt; CoRR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.12519&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://wushoule.github.io/ItoAudio/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Itô-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hideyuki Tachibana, Mocho Go, Muneyoshi Inahara, Yotaro Katayama, Yotaro Watanabe&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.13339&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Gamma Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eliya Nachmani, Robin San Roman, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.05948&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Variational Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.00630&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/revsic/jax-variational-diffwave&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Simon Rouard, Gaëtan Hadjeres&lt;/em&gt; &lt;br&gt; ISMIR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.07431&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://crash-diffusion.github.io/crash/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Driven Adaptive Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang Liu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, Tie-Yan Liu&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2106.06406&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://speechresearch.github.io/priorgrad/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 11 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ItôTTS and ItôWave: Linear Stochastic Differential Equation Is All You Need For Audio Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shoule Wu, Ziqiang Shi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2105.07583&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://wushoule.github.io/ItoAudio/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, Peng Liu, Zhou Zhao&lt;/em&gt; &lt;br&gt; AAAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2105.02446&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffsinger.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/keonlee9420/DiffSinger&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Symbolic Music Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gautam Mittal, Jesse Engel, Curtis Hawthorne, Ian Simon&lt;/em&gt; &lt;br&gt; ISMIR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.16091&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/magenta/symbolic-music-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffWave: A Versatile Diffusion Model for Audio Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, Bryan Catanzaro&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2009.09761&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffwave-demo.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Sep 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WaveGrad: Estimating Gradients for Waveform Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, William Chan&lt;/em&gt;&lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2009.00713&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://wavegrad.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ivanvovk/WaveGrad&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Sep 2020&lt;/p&gt; &#xA;&lt;h3&gt;Conversion&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled Representation and Prior Mixup for Verified Robust Voice Conversion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ha-Yeong Choi, Sang-Hoon Lee, Seong-Whan Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15816&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://hayeong0.github.io/DDDM-VC-demo/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Duplex Diffusion Models Improve Speech-to-Speech Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xianchao Wu&lt;/em&gt; &lt;br&gt; ACL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12628&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSVC: A Diffusion Probabilistic Model for Singing Voice Conversion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Songxiang Liu, Yuewen Cao, Dan Su, Helen Meng&lt;/em&gt; &lt;br&gt; IEEE 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.13871&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/liusongxiang/diffsvc&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling Scheme&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov, Jiansheng Wei&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2109.13821&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffvc-fast-ml-solver.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Sep 2021&lt;/p&gt; &#xA;&lt;h3&gt;Enhancement&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Posterior Sampling for Informed Single-Channel Dereverberation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jean-Marie Lemercier, Simon Welker, Timo Gerkmann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12286&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Variance-Preserving-Based Interpolation Diffusion Models for Speech Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zilu Guo, Jun Du, Chin-Hui Lee, Yu Gao, Wenbin Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08527&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anastasiia Iashchenko, Pavel Andreev, Ivan Shchekotov, Nicholas Babaev, Dmitry Vetrov&lt;/em&gt; &lt;br&gt; Interspeech 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00721&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SE-Bridge: Speech Enhancement with Consistent Brownian Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhibin Qiu, Mengfan Fu, Fuchun Sun, Gulila Altenbek, Hao Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13796&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao Shi, Kazuki Shimada, Masato Hirano, Takashi Shibuya, Yuichiro Koyama, Zhi Zhong, Shusuke Takahashi, Tatsuya Kawahara, Yuki Mitsufuji&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10734&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Signal Improvement Using Causal Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julius Richter, Simon Welker, Jean-Marie Lemercier, Bunlong Lay, Tal Peer, Timo Gerkmann&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08674&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reducing the Prior Mismatch of Stochastic Differential Equations for Diffusion-based Speech Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bunlong Lay, Simon Welker, Julius Richter, Timo Gerkmann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.14748&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Metric-oriented Speech Enhancement using Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Chen, Yuchen Hu, Weiwei Weng, Eng Siong Chng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11989&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StoRM: A Diffusion-based Stochastic Regeneration Model for Speech Enhancement and Dereverberation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jean-Marie Lemercier, Julius Richter, Simon Welker, Timo Gerkmann&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.11851&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised vocal dereverberation with diffusion-based generative models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Koichi Saito, Naoki Murata, Toshimitsu Uesaka, Chieh-Hsin Lai, Yuhta Takida, Takao Fukui, Yuki Mitsufuji&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.04124&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffPhase: Generative Diffusion-based STFT Phase Retrieval&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tal Peer, Simon Welker, Timo Gerkmann&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.04332&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cold Diffusion for Speech Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao Yen, François G. Germain, Gordon Wichern, Jonathan Le Roux&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.02527&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analysing Diffusion-based Generative Approaches versus Discriminative Approaches for Speech Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jean-Marie Lemercier, Julius Richter, Simon Welker, Timo Gerkmann&lt;/em&gt; &lt;br&gt; Interspeech 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.02397&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.inf.uni-hamburg.de/en/inst/ab/sp/publications/sgmse-multitask.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sp-uhh/sgmse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SRTNet: Time Domain Speech Enhancement Via Stochastic Refinement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhibin Qiu, Mengfan Fu, Yinfeng Yu, LiLi Yin, Fuchun Sun, Hao Huang&lt;/em&gt; &lt;br&gt; ICASSP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.16805&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhibinQiu/SRTNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Versatile Diffusion-based Generative Refiner for Speech Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryosuke Sawata, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Takashi Shibuya, Shusuke Takahashi, Yuki Mitsufuji&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.17287&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditioning and Sampling in Variational Diffusion Models for Speech Super-resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chin-Yun Yu, Sung-Lin Yeh, György Fazekas, Hao Tang&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.15793&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yoyololicon.github.io/diffwave-sr/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yoyololicon/diffwave-sr&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Audio Inverse Problems with a Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eloi Moliner, Jaakko Lehtinen, Vesa Välimäki&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.15228&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Enhancement and Dereverberation with Diffusion-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julius Richter, Simon Welker, Jean-Marie Lemercier, Bunlong Lay, Timo Gerkmann&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.05830&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.inf.uni-hamburg.de/en/inst/ab/sp/publications/sgmse&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sp-uhh/sgmse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NU-Wave 2: A General Neural Audio Upsampling Model for Various Sampling Rates&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seungu Han, Junhyeok Lee&lt;/em&gt; &lt;br&gt; Interspeech 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.08545&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mindslab-ai.github.io/nuwave2/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Universal Speech Enhancement with Score-based Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joan Serrà, Santiago Pascual, Jordi Pons, R. Oguz Araz, Davide Scaini&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.03065&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Enhancement with Score-Based Generative Models in the Complex STFT Domain&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Simon Welker, Julius Richter, Timo Gerkmann&lt;/em&gt; &lt;br&gt; InterSpeech 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.17004&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sp-uhh/sgmse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Probabilistic Model for Speech Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yen-Ju Lu, Zhong-Qiu Wang, Shinji Watanabe, Alexander Richard, Cheng Yu, Yu Tsao&lt;/em&gt; &lt;br&gt; IEEE 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.05256&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/neillu23/cdiffuse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Study on Speech Enhancement Based on Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yen-Ju Lu, Yu Tsao, Shinji Watanabe&lt;/em&gt; &lt;br&gt; APSIPA 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.11876&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoring degraded speech via a modified diffusion model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianwei Zhang, Suren Jayasuriya, Visar Berisha&lt;/em&gt; &lt;br&gt; Interspeech 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.11347&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Apr 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NU-Wave: A Diffusion Probabilistic Model for Neural Audio Upsampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junhyeok Lee, Seungu Han&lt;/em&gt; &lt;br&gt; Interspeech 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.02321&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mindslab-ai.github.io/nuwave/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mindslab-ai/nuwave&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Apr 2021&lt;/p&gt; &#xA;&lt;h3&gt;Separation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Signal Refiner for Speech Separation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Masato Hirano, Kazuki Shimada, Yuichiro Koyama, Shusuke Takahashi, Yuki Mitsufuji&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05857&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Source Diffusion Models for Simultaneous Music Generation and Separation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giorgio Mariani, Irene Tallini, Emilian Postolache, Michele Mancusi, Luca Cosmo, Emanuele Rodolà&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02257&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gladia-research-group.github.io/multi-source-diffusion-models/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Separate And Diffuse: Using a Pretrained Diffusion Model for Improving Source Separation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shahar Lutati, Eliya Nachmani, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10752&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Generative Speech Source Separation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Scheibler, Youna Ji, Soo-Whan Chung, Jaeuk Byun, Soyeon Choe, Min-Seok Choi&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.17327&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instrument Separation of Symbolic Music by Explicitly Guided Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sangjun Han, Hyeongrae Ihm, DaeHan Ahn, Woohyung Lim&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.02696&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Sep 2022&lt;/p&gt; &#xA;&lt;h3&gt;Text-to-Speech&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Driven Foley Sound Generation With Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Yuan, Haohe Liu, Xubo Liu, Xiyuan Kang, Peipei Wu, Mark D. Plumbley, Wenwu Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10359&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao-Wen Dong, Xiaoyu Liu, Jordi Pons, Gautam Bhattacharya, Santiago Pascual, Joan Serrà, Taylor Berg-Kirkpatrick, Julian McAuley&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09635&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07691&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenpeng Du, Yiwei Guo, Feiyu Shen, Zhijun Liu, Zheng Liang, Xie Chen, Shuai Wang, Hui Zhang, Kai Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07547&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenhao Guan, Tao Li, Yishuang Li, Hukai Huang, Qingyang Hong, Lin Li&lt;/em&gt; &lt;br&gt; Interspeech 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04301&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin, Zejun Ma, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03509&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mega-tts.github.io/demo-page/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18474&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minki Kang, Wooseok Han, Sung Ju Hwang, Eunho Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13831&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;U-DiT TTS: U-Diffusion Vision Transformer for Text-to-Speech&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Jing, Yi Chang, Zijiang Yang, Jiangjian Xie, Andreas Triantafyllopoulos, Bjoern W. Schuller&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13195&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://eihw.github.io/u-dit-tts/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffAVA: Personalized Text-to-Audio Generation with Visual Alignment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shentong Mo, Jing Shi, Yapeng Tian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12903&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huadai Liu, Rongjie Huang, Xuan Lin, Wenqiang Xu, Maozong Zheng, Hong Chen, Jinzheng He, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12708&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vit-tts.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RMSSinger: Realistic-Music-Score based Singing Voice Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinzheng He, Jinglin Liu, Zhenhui Ye, Rongjie Huang, Chenye Cui, Huadai Liu, Zhou Zhao&lt;/em&gt; &lt;br&gt; ACL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10686&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://rmssinger.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhen Ye, Wei Xue, Xu Tan, Jie Chen, Qifeng Liu, Yike Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06908&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://comospeech.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Soujanya Poria&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13731&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tango-web.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/declare-lab/tango&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffVoice: Text-to-Speech with Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhijun Liu, Yiwei Guo, Kai Yu&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11750&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An investigation into the adaptability of a diffusion-based TTS model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haolin Chen, Philip N. Garner&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01849&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiyoung Lee, Joon Son Chung, Soo-Whan Chung&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13700&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pengfei Zhu, Chao Pang, Shuohuan Wang, Yekun Chai, Yu Sun, Hao Tian, Hua Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04456&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Noise2Music: Text-conditioned Music Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qingqing Huang, Daniel S. Park, Tao Wang, Timo I. Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, Jesse Engel, Quoc V. Le, William Chan, Wei Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03917&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://google-research.github.io/noise2music/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Flavio Schneider, Zhijing Jin, Bernhard Schölkopf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11757&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://anonymous0.notion.site/anonymous0/Mo-sai-Text-to-Audio-with-Long-Context-Latent-Diffusion-b43dbc71caf94b5898f9e8de714ab5dc&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/archinetai/audio-diffusion-pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongchao Yang, Songxiang Liu, Rongjie Huang, Guangzhi Lei, Chao Weng, Helen Meng, Dong Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13662&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://dongchaoyang.top/InstructTTS/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang Yin, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12661&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://text-to-audio.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AudioLDM: Text-to-Audio Generation with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, Mark D. Plumbley&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12503&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://audioldm.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haoheliu/AudioLDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zehua Chen, Yihan Wu, Yichong Leng, Jiawei Chen, Haohe Liu, Xu Tan, Yang Cui, Ke Wang, Lei He, Sheng Zhao, Jiang Bian, Danilo Mandic&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.14518&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://resgrad1.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yusuke Yasuda, Tomoki Toda&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.08329&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Any-speaker Adaptive Text-To-Speech Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minki Kang, Dongchan Min, Sung Ju Hwang&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.09383&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nardien.github.io/grad-stylespeech-demo/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiwei Guo, Chenpeng Du, Xie Chen, Kai Yu&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.09496&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cantabile-kwok.github.io/EmoDiff-intensity-ctrl/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongchao Yang, Songxiang Liu, Jianwei Yu, Helin Wang, Chao Weng, Yuexian Zou&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.02448&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WaveFit: An Iterative and Non-autoregressive Neural Vocoder based on Fixed-Point Iteration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuma Koizumi, Kohei Yatabe, Heiga Zen, Michiel Bacchiani&lt;/em&gt; &lt;br&gt; IEEE SLT 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.01029&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://google.github.io/df-conformer/wavefit/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 3 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffsound: Discrete Diffusion Model for Text-to-sound Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, Dong Yu&lt;/em&gt; &lt;br&gt; TASLP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.09983&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://dongchaoyang.top/text-to-sound-synthesis-demo/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Voice Conditioning for Denoising Diffusion TTS Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alon Levkovitch, Eliya Nachmani, Lior Wolf&lt;/em&gt; &lt;br&gt; Interspeech 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.02246&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://alonlevko.github.io/ilvr-tts-diff&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guided-TTS 2: A Diffusion Model for High-quality Adaptive Text-to-Speech with Untranscribed Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sungwon Kim, Heeseung Kim, Sungroh Yoon&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.15370&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ksw0306.github.io/guided-tts2-demo/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InferGrad: Improving Diffusion Models for Vocoder by Considering Inference in Training&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zehua Chen, Xu Tan, Ke Wang, Shifeng Pan, Danilo Mandic, Lei He, Sheng Zhao&lt;/em&gt; &lt;br&gt; ICASSP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.03751&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Songxiang Liu, Dan Su, Dong Yu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.11972&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/keonlee9420/DiffGAN-TTS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guided-TTS:Text-to-Speech with Untranscribed Speech&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Heeseung Kim, Sungwon Kim, Sungroh Yoon&lt;/em&gt; &lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.11755&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EdiTTS: Score-based Editing for Controllable Text-to-Speech&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaesung Tae, Hyeongju Kim, Taesu Kim&lt;/em&gt; &lt;br&gt; Interspeech 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.02584&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://editts.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/neosapience/EdiTTS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, Najim Dehak, William Chan&lt;/em&gt; &lt;br&gt; Interspeech 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.09660&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mindslab-ai.github.io/wavegrad2/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/keonlee9420/WaveGrad2&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mindslab-ai/wavegrad2&#34;&gt;Github2&lt;/a&gt;] &lt;br&gt; 17 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, Mikhail Kudinov&lt;/em&gt; &lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.06337&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://grad-tts.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, Peng Liu, Zhou Zhao&lt;/em&gt; &lt;br&gt; AAAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2105.02446&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffsinger.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/keonlee9420/DiffSinger&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-TTS: A Denoising Diffusion Model for Text-to-Speech&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, Nam Soo Kim&lt;/em&gt; &lt;br&gt; Interspeech 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.01409&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2021&lt;/p&gt; &#xA;&lt;h3&gt;Miscellany&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Blind Audio Bandwidth Extension&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eloi Moliner, Filip Elvander, Vesa Välimäki&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01433&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diverse and Expressive Speech Prosody Prediction with Denoising Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiang Li, Songxiang Liu, Max W. Y. Lam, Zhiyong Wu, Chao Weng, Helen Meng&lt;/em&gt; &lt;br&gt; Interspeech 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16749&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based Audio Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eloi Moliner, Vesa Välimäki&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15266&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FluentSpeech: Stutter-Oriented Automatic Speech Editing with Context-Aware Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyue Jiang, Qian Yang, Jialong Zuo, Zhenhui Ye, Rongjie Huang, Yi Ren, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13612&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Zain-Jiang/Speech-Editing-Toolkit&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Preliminary Study on Augmenting Speech Emotion Recognition using a Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ibrahim Malik, Siddique Latif, Raja Jurdak, Björn Schuller&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11413&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng Wu, Jiang Bian, Sheng Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.00830&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://audit-demo.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Augmentation for Environmental Sound Classification Using Diffusion Probabilistic Model with Top-k Selection Discriminator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunhao Chen, Yunjie Zhu, Zihui Yan, Jianlu Shen, Zhen Ren, Yifan Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15161&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JNAIC/DPMs-for-Audio-Data-Augmentation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhancing Unsupervised Speech Recognition with Diffusion GANs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xianchao Wu&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13559&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Defending against Adversarial Audio via Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shutong Wu, Jiongxiao Wang, Wei Ping, Weili Nie, Chaowei Xiao&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01507&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cychomatica/AudioPure&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TransFusion: Transcribing Speech with Multinomial Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthew Baas, Kevin Eloff, Herman Kamper&lt;/em&gt; &lt;br&gt; SACAIR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.07677&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RF5/transfusion-asr&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Oct 2022&lt;/p&gt; &#xA;&lt;h2&gt;Natural Language&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuDetox: A Mixed Diffusion Model for Text Detoxification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Griffin Floto, Mohammad Mahdi Abdollah Pour, Parsa Farinneya, Zhenwei Tang, Ali Pesaranghader, Manasa Bharadwaj, Scott Sanner&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08505&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyuan Hu, Chumin Liu, Yue Feng, Bryan Hooi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08456&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02531&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guanqun Bi, Lei Shen, Yanan Cao, Meng Chen, Yuqiang Xie, Zheng Lin, Xiaodong He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01657&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifei Liu, Rex Shen, Xiaotong Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Likelihood-Based Diffusion Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ishaan Gulrajani, Tatsunori B. Hashimoto&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18619&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/igul222/plaid&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning to Imagine: Visually-Augmented Natural Language Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen&lt;/em&gt; &lt;br&gt; ACL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16944&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xueliang Zhao, Wenda Li, Lingpeng Kong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16366&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dior-CVAE: Diffusion Priors in Variational Dialog Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianyu Yang, Thy Thy Tran, Iryna Gurevych&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15025&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SSD-2: Scaling and Inference-time Fusion of Diffusion Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov, Marjan Ghazvininejad&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14771&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionNER: Boundary Diffusion for Named Entity Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang&lt;/em&gt; &lt;br&gt; ACL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13298&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCap: Exploring Continuous Diffusion on Image Captioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yufeng He, Zefan Cai, Xu Gan, Baobao Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12144&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chao-Hong Tan, Jia-Chen Gu, Zhen-Hua Ling&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11517&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Democratized Diffusion Language Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikita Balagansky, Daniil Gavrilov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10818&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, Weizhu Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09515&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TESS: Text-to-Text Self-Conditioned Simplex Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rabeeh Karimi Mahabadi, Jaesung Tae, Hamish Ivison, James Henderson, Iz Beltagy, Matthew E. Peters, Arman Cohan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08379&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Large Language Models Need Holistically Thought in Medical Conversational QA&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yixuan Weng, Bin Li, Fei Xia, Minjun Zhu, Bin Sun, Shizhu He, Kang Liu, Jun Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05410&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/WENGSYX/HoT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyuan Zhang, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04547&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can Diffusion Model Achieve Better Performance in Text Generation? Bridging the Gap between Training and Inference!&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zecheng Tang, Pinzheng Wang, Keyan Zhou, Juntao Li, Ziqiang Cao, Min Zhang&lt;/em&gt; &lt;br&gt; ACL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04465&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CODINNLG/Bridge_Gap_Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kun Zhou, Yifan Li, Wayne Xin Zhao, Ji-Rong Wen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04044&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuSum: Generation Enhanced Extractive Summarization with Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haopeng Zhang, Xiao Liu, Jiawei Zhang&lt;/em&gt; &lt;br&gt; ACL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01735&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RenderDiffusion: Text Generation as Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyi Li, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12519&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuwei Yin, Jean Kaddour, Xiang Zhang, Yixin Nie, Zhenguang Liu, Lingpeng Kong, Qi Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08821&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Cheaper and Better Diffusion Language Model with Soft-Masked Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaao Chen, Aston Zhang, Mu Li, Alex Smola, Diyi Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04746&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/amazon-science/masked-diffusion-lm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, Mingxuan Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10025&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Reparameterized Discrete Diffusion Model for Text Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lin Zheng, Jianbo Yuan, Lei Yu, Lingpeng Kong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05737&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HKUNLP/reparam-discrete-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Long Horizon Temperature Scaling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andy Shih, Dorsa Sadigh, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03686&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AndyShih12/LongHorizonTemperatureScaling&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusER: Diffusion via Edit-based Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Machel Reid, Vincent Josua Hellendoorn, Graham Neubig&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://openreview.net/forum?id=nG9RF9z1yy3&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GENIE: Large Scale Pre-training for Text Generation with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Weizhu Chen, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11685&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-Glat: Diffusion Glancing Transformer for Parallel Sequence to Sequence Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lihua Qian, Mingxuan Wang, Yang Liu, Hao Zhou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10240&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SeqDiffuSeq: Text Diffusion with Encoder-Decoder Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang, Songfang Huang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10325&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Diffusion for Language Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Justin Lovelace, Varsha Kishore, Chao Wan, Eliot Shekhtman, Kilian Weinberger&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09462&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Difformer: Empowering Diffusion Model on Embedding Space for Text Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhujin Gao, Junliang Guo, Xu Tan, Yongxin Zhu, Fang Zhang, Jiang Bian, Linli Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09412&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, Xipeng Qiu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.15029&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Hzfinfdu/Diffusion-BERT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Continuous diffusion for categorical data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H. Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, Curtis Hawthorne, Rémi Leblond, Will Grathwohl, Jonas Adler&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.15089&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-conditioned Embedding Diffusion for Text Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Strudel, Corentin Tallec, Florent Altché, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, Rémi Leblond&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.04236&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusER: Discrete Diffusion via Edit-based Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Machel Reid, Vincent J. Hellendoorn, Graham Neubig&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.16886&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaochuang Han, Sachin Kumar, Yulia Tsvetkov&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.17432&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xhan77/ssd-lm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, LingPeng Kong&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.08933&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Composable Text Controls in Latent Space with ODEs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guangyi Liu, Zeyu Feng, Yuan Gao, Zichao Yang, Xiaodan Liang, Junwei Bao, Xiaodong He, Shuguang Cui, Zhen Li, Zhiting Hu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.00638&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/guangyliu/LatentOps&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Structured Denoising Diffusion Models in Discrete State-Spaces&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.03006&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Diffusion Energy-Based Model for Interpretable Text Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peiyu Yu, Sirui Xie, Xiaojian Ma, Baoxiong Jia, Bo Pang, Ruigi Gao, Yixin Zhu, Song-Chun Zhu, Ying Nian Wu&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.05895&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuPeiyu98/LDEBM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-LM Improves Controllable Text Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, Tatsunori B. Hashimoto&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.14217&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XiangLi1999/Diffusion-LM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Step-unrolled Denoising Autoencoders for Text Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, Aaron van den Oord&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.06749&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vvvm23/sundae&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Translation using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eliya Nachmani, Shaked Dovrat&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.01471&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, Max Welling&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.05379&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2021&lt;/p&gt; &#xA;&lt;h2&gt;Tabular and Time Series&lt;/h2&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffECG: A Generalized Probabilistic Diffusion Model for ECG Signals Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nour Neifar, Achraf Ben-Hamadou, Afef Mdhaffar, Mohamed Jmaiel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01875&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaejeong Lee, Jayoung Kim, Noseong Park&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12654&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Customized Load Profiles Synthesis for Electricity Customers Based on Conditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenyi Wang, Hongcai Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12076&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic Health-related Longitudinal Data with Mixed-type Variables Generated using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nicholas I-Hsien Kuo, Louisa Jorm, Sebastiano Barbieri&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12281&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusing Gaussian Mixtures for Generating Categorical Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Florence Regol, Mark Coates&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04635&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EHRDiff: Exploring Realistic EHR Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongyi Yuan, Songchi Zhou, Sheng Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05656&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sczzz3/ehrdiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing Mixed-type Electronic Health Records using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Taha Ceritli, Ghadeer O. Ghosheh, Vinod Kumar Chauhan, Tingting Zhu, Andrew P. Creagh, David A. Clifton&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.14679&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedDiff: Generating Electronic Health Records using Accelerated Denoising Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huan He, Shifan Zhao, Yuanzhe Xi, Joyce C Ho&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04355&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Conditional ECG Generation with Structured State Space Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Juan Miguel Lopez Alcaraz, Nils Strodthoff&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.08227&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TabDDPM: Modelling Tabular Data with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, Artem Babenko&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.15421&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rotot0/tab-ddpm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Sep 2022&lt;/p&gt; &#xA;&lt;h3&gt;Forecasting&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Augmentation for Seizure Prediction with Generative Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Shu, Yuchang Zhao, Le Wu, Aiping Liu, Ruobing Qian, Xun Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08256&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Non-autoregressive Conditional Diffusion Models for Time Series Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lifeng Shen, James Kwok&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05043&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DYffusion: A Dynamics-informed Diffusion Model for Spatiotemporal Forecasting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Salva Rühling Cachay, Bo Zhao, Hailey James, Rose Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01984&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhixian Wang, Qingsong Wen, Chaoli Zhang, Liang Sun, Yi Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01001&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TDSTF: Transformer-based Diffusion probabilistic model for Sparse Time series Forecasting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ping Chang, Huayu Li, Stuart F. Quan, Janet Roveda, Ao Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.06625&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PingChang818/TDSTF&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Time Series Forecasting with Diffusion, Denoise, and Disentanglement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yan Li, Xinjiang Lu, Yaqing Wang, Dejing Dou&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2301.03028&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpatial/tree/main/research/D3VAE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising diffusion probabilistic models for probabilistic energy forecasting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Esteban Hernandez, Jonathan Dumas&lt;/em&gt; &lt;br&gt; Powertech 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02977&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modeling Temporal Data as Continuous Functions with Process Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marin Biloš, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Stephan Günnemann&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.02590&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Juan Miguel Lopez Alcaraz, Nils Strodthoff&lt;/em&gt; &lt;br&gt; TMLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09399&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AI4HealthUOL/SSSD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CARD: Classification and Regression Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xizewen Han, Huangjie Zheng, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07275&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ScoreGrad: Multivariate Probabilistic Time Series Forecasting with Continuous Energy-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tijin Yan, Hongwei Zhang, Tong Zhou, Yufeng Zhan, Yuanqing Xia&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.10121&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yantijin/ScoreGradPred&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Autoregressive Denoising Diffusion Models for Multivariate Probabilistic Time Series Forecasting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kashif Rasul, Calvin Seward, Ingmar Schuster, Roland Vollgraf&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.12072&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zalandoresearch/pytorch-ts&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Feb 2021&lt;/p&gt; &#xA;&lt;h3&gt;Imputation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;PriSTI: A Conditional Diffusion Framework for Spatiotemporal Imputation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, Yanjie Fu&lt;/em&gt; &lt;br&gt; ICDE 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.09746&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LMZZML/PriSTI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modeling Temporal Data as Continuous Functions with Process Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marin Biloš, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Stephan Günnemann&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.02590&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models for missing value imputation in tabular data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuhan Zheng, Nontawat Charoenphakdee&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.17128&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Markov Controlled SDE: Stochastic Optimization for Continuous-Time Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sung Woo Park, Kyungjae Lee, Junseok Kwon&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://openreview.net/forum?id=7DI6op61AY&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Juan Miguel Lopez Alcaraz, Nils Strodthoff&lt;/em&gt; &lt;br&gt; TMLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09399&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AI4HealthUOL/SSSD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yusuke Tashiro, Jiaming Song, Yang Song, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.03502&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/csdi&#34;&gt;Github&lt;/a&gt;]&lt;br&gt; 7 Jul 2021&lt;/p&gt; &#xA;&lt;h3&gt;Miscellany&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;RecFusion: A Binomial Diffusion Process for 1D Data for Recommendation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gabriel Bénédict, Olivier Jeunen, Samuele Papa, Samarth Bhargav, Daan Odijk, Maarten de Rijke&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08947&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Statistical Feature-Guided Diffusion Model for Sensor-based Human Activity Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Si Zuo, Vitor Fortes Rey, Sungho Suh, Stephan Sigg, Paul Lukowicz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05285&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Domain-Specific Denoising Diffusion Probabilistic Models for Brain Dynamics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiqun Duan, Jinzhao Zhou, Zhen Wang, Yu-Cheng Chang, Yu-Kai Wang, Chin-Teng Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04200&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Denoising Diffusion for Sequential Recommendation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Wang, Zhiwei Liu, Liangwei Yang, Philip S. Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11433&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Recommender Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjie Wang, Yiyan Xu, Fuli Feng, Xinyu Lin, Xiangnan He, Tat-Seng Chua&lt;/em&gt; &lt;br&gt; SIGIR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04971&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sequential Recommendation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanwen Du, Huanhuan Yuan, Zhen Huang, Pengpeng Zhao, Xiaofang Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04541&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuRec: A Diffusion Model for Sequential Recommendation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zihao Li, Aixin Sun, Chenliang Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.00686&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EEG Synthetic Data Generation Using Probabilistic Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giulio Tosato, Cesare M. Dalbagno, Francesco Fumagalli&lt;/em&gt; &lt;br&gt; Synapsium 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06068&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeScoD-ECG: Deep Score-Based Diffusion Model for ECG Baseline Wander and Noise Removal&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huayu Li, Gregory Ditzler, Janet Roveda, Ao Li&lt;/em&gt; &lt;br&gt; IEEE JBHI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2208.00542&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Recommendation via Collaborative Diffusion Generative Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joojo Walker, Ting Zhong, Fengli Zhang, Qiang Gao, Fan Zhou&lt;/em&gt; &lt;br&gt; KSEM 2022. [&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-031-10989-8_47&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jul 2022&lt;/p&gt; &#xA;&lt;h2&gt;Graph&lt;/h2&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;SaGess: Sampling Graph Denoising Diffusion Model for Scalable Graph Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Stratis Limnios, Praveen Selvaraj, Mihai Cucuringu, Carsten Maple, Gesine Reinert, Andrew Elliott&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16827&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Complexity-aware Large Scale Origin-Destination Network Generation via Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Can Rong, Jingtao Ding, Zhicheng Liu, Yong Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04873&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Diffusion Model for Event Skeleton Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fangqi Zhu, Lin Zhang, Jun Gao, Bing Qin, Ruifeng Xu, Haiqin Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17458&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhufq00/EventSkeletonGeneration&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Confidence-Based Feature Imputation for Graphs with Partially Known Features&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daeho Um, Jiwoong Park, Seulki Park, Jin Young Choi&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16618&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/daehoum1/pcfi&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spatio-temporal Diffusion Point Processes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuan Yuan, Jingtao Ding, Chenyang Shao, Depeng Jin, Yong Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12403&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tsinghua-fib-lab/Spatio-temporal-Diffusion-Point-Processes&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaohui Chen, Jiaxing He, Xu Han, Li-Ping Liu&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04111&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A 2D Graph-Based Generative Approach For Exploring Transition States Using Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seonghwan Kim, Jeheon Woo, Woo Youn Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12233&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Two-stage Denoising Diffusion Model for Source Localization in Graph Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bosong Huang, Weihao Yu, Ruzhong Xie, Jing Xiao, Jin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08841&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Models for Graph-Structured Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyosoon Jang, Sangwoo Mo, Sungsoo Ahn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10506&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GraphGUIDE: interpretable and controllable conditional graph generation with discrete Bernoulli diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex M. Tseng, Nathaniel Diamant, Tommaso Biancalani, Gabriele Scalia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03790&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Graph Generation with Destination-Driven Diffusion Mixture&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaehyeong Jo, Dongki Kim, Sung Ju Hwang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03596&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSTG: Probabilistic Spatio-Temporal Graph Forecasting with Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Roger Zimmermann, Yuxuan Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13629&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, Junchi Yan&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.09474&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GraphGDP: Generative Diffusion Processes for Permutation Invariant Graph Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Han Huang, Leilei Sun, Bowen Du, Yanjie Fu, Weifeng Lv&lt;/em&gt; &lt;br&gt; IEEE ICDM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.01842&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/GRAPH-0/GraphGDP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NVDiff: Graph Generation through the Diffusion of Node Vectors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cristian Sbrolli, Paolo Cudrano, Matteo Frosi, Matteo Matteucci&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10794&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Graph Generative Model via Spectral Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianze Luo, Zhanfeng Mo, Sinno Jialin Pan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.08892&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Graphs Benefit From Discrete State Spaces&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kilian Konstantin Haefeli, Karolis Martinkus, Nathanaël Perraudin, Roger Wattenhofer&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.01549&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiGress: Discrete Denoising diffusion for graph generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, Pascal Frossard&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.14734&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Permutation Invariant Graph Generation via Score-Based Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, Stefano Ermon&lt;/em&gt; &lt;br&gt; AISTATS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2003.00638&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/GraphScoreMatching&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Mar 2020&lt;/p&gt; &#xA;&lt;h3&gt;Molecular and Material Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Graph Denoising Diffusion for Inverse Protein Folding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Yi, Bingxin Zhou, Yiqing Shen, Pietro Liò, Yu Guang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16819&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffDTM: A conditional structure-free framework for bioactive molecules generation targeted for dual proteins&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lei Huang, Zheng Yuan, Huihui Yan, Rong Sheng, Linjing Liu, Fuzhou Wang, Weidun Xie, Nanjun Chen, Fei Huang, Songfang Huang, Ka-Chun Wong, Yaoyun Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13957&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hyperbolic Graph Diffusion Model for Molecule Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lingfeng Wen, Xian Wei&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07618&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D molecule generation by denoising voxel grids&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pedro O. Pinheiro, Joshua Rackers, Joseph Kleinhenz, Michael Maser, Omar Mahmood, Andrew Martin Watkins, Stephen Ra, Vishnu Sresht, Saeed Saremi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07473&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yangtian Zhan, Zuobai Zhang, Bozitao Zhong, Sanchit Misra, Jian Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01794&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Protein Design with Guided Discrete Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nate Gruver, Samuel Stanton, Nathan C. Frey, Tim G. J. Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, Andrew Gordon Wilson&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20009&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RINGER: Rapid Conformer Generation for Macrocycles with Sequence-Conditioned Internal Coordinate Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Colin A. Grambow, Hayley Weir, Nathaniel L. Diamant, Alex M. Tseng, Tommaso Biancalani, Gabriele Scalia, Kangway V. Chuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19800&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Trans-Dimensional Generative Modeling via Jump Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrew Campbell, William Harvey, Christian Weilbach, Valentin De Bortoli, Tom Rainforth, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Joint 2D &amp;amp; 3D Diffusion Models for Complete Molecule Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Han Huang, Leilei Sun, Bowen Du, Weifeng Lv&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12347&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/GRAPH-0/JODO&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MolDiff: Addressing the Atom-Bond Inconsistency Problem in 3D Molecule Diffusion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingang Peng, Jiaqi Guan, Qiang Liu, Jianzhu Ma&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.07508&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Qiang, Yuxuan Song, Minkai Xu, Jingjing Gong, Bowen Gao, Hao Zhou, Weiying Ma, Yanyan Lan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13266&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Latent Diffusion Model for Protein Structure Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cong Fu, Keqiang Yan, Limei Wang, Wing Yee Au, Michael McThrow, Tao Komikado, Koji Maruhashi, Kanji Uchino, Xiaoning Qian, Shuiwang Ji&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04120&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Geometric Latent Diffusion Models for 3D Molecule Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minkai Xu, Alexander Powers, Ron Dror, Stefano Ermon, Jure Leskovec&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01140&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MUDiff: Unified Diffusion for Complete Molecule Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenqing Hua, Sitao Luan, Minkai Xu, Rex Ying, Jie Fu, Stefano Ermon, Doina Precup&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14621&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Controllable Diffusion Models via Reward-Guided Exploration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hengtong Zhang, Tingyang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07132&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenru Duan, Yuanqi Du, Haojun Jia, Heather J. Kulik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06174&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffDock-PP: Rigid Protein-Protein Docking with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohamed Amine Ketata, Cedrik Laue, Ruslan Mammadov, Hannes Stärk, Menghua Wu, Gabriele Corso, Céline Marquet, Regina Barzilay, Tommi S. Jaakkola&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03889&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, Jianzhu Ma&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03543&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EigenFold: Generative Protein Structure Prediction with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bowen Jing, Ezra Erives, Peter Pao-Huang, Gabriele Corso, Bonnie Berger, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; ICLR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02198&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bjing2016/EigenFold&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising diffusion algorithm for inverse design of microstructures with fine-tuned nonlinear material properties&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikolaos N. Vlassis, WaiChing Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12881&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Aligned Diffusion Schrödinger Bridges&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, Charlotte Bunne&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11419&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Clement Vignac, Nagham Osman, Laura Toni, Pascal Frossard&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.09048&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Geometry-Complete Diffusion for 3D Molecule Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Morehead, Jianlin Cheng&lt;/em&gt; &lt;br&gt; ICML Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04313&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/BioinfoMachineLearning/bio-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SE(3) diffusion model with application to protein backbone generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jason Yim, Brian L. Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02277&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data-Efficient Protein 3D Geometric Pretraining via Refinement of Diffused Protein Structure Decoy&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yufei Huang, Lirong Wu, Haitao Lin, Jiangbin Zheng, Ge Wang, Stan Z. Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10888&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Two for One: Diffusion Models and Force Fields for Coarse-Grained Molecular Dynamics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marloes Arts, Victor Garcia Satorras, Chin-Wei Huang, Daniel Zuegner, Marco Federici, Cecilia Clementi, Frank Noé, Robert Pinsler, Rianne van den Berg&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00600&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Novel, Designable, and Diverse Protein Structures by Equivariantly Diffusing Oriented Residue Clouds&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yeqing Lin, Mohammed AlQuraishi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12485&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Physics-Inspired Protein Encoder Pre-Training via Siamese Sequence-Structure Diffusion Trajectory Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zuobai Zhang, Minghao Xu, Aurélie Lozano, Vijil Chenthamarakshan, Payel Das, Jian Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12068&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSDS: A language diffusion model for protein backbone inpainting under geometric conditions and constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhangyang Gao, Cheng Tan, Stan Z. Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.09642&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffBP: Generative Diffusion of 3D Molecules for Target Protein Binding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haitao Lin, Yufei Huang, Meng Liu, Xuanjing Li, Shuiwang Ji, Stan Z. Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11214&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ParticleGrid: Enabling Deep Learning using 3D Representation of Materials&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shehtab Zaman, Ethan Ferguson, Cecile Pereira, Denis Akhiyarov, Mauricio Araya-Polo, Kenneth Chiu&lt;/em&gt; &lt;br&gt; IEEE eScience 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.08506&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Structure-based Drug Design with Equivariant Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro Lió, Carla Gomes, Max Welling, Michael Bronstein, Bruno Correia&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.13695&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Protein Sequence and Structure Co-Design with Equivariant Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chence Shi, Chuanrui Wang, Jiarui Lu, Bozitao Zhong, Jian Tang&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.08761&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Equivariant 3D-Conditional Diffusion Models for Molecular Linker Design&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ilia Igashov, Hannes Stärk, Clément Vignac, Victor Garcia Satorras, Pascal Frossard, Max Welling, Michael Bronstein, Bruno Correia&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.05274&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;State-specific protein-ligand complex structure prediction with a multi-scale deep generative model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuoran Qiao, Weili Nie, Arash Vahdat, Thomas F. Miller III, Anima Anandkumar&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.15171&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Equivariant Energy-Guided SDE for Inverse Molecular Design&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.15408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Protein structure generation via folding diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kevin E. Wu, Kevin K. Yang, Rianne van den Berg, James Y. Zou, Alex X. Lu, Ava P. Amini&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.15611&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MDM: Molecular Diffusion Model for 3D Molecule Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lei Huang, Hengtong Zhang, Tingyang Xu, Ka-Chun Wong&lt;/em&gt; &lt;br&gt; AAAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.05710&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Molecule Generation with Informative Prior Bridges&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, Qiang Liu&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.00865&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Antigen-Specific Antibody Design and Optimization with Diffusion-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, Jianzhu Ma&lt;/em&gt; &lt;br&gt; BioRXiv 2022. [&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2022.07.10.499510v1&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data-driven discovery of novel 2D materials by deep generative models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peder Lyngby, Kristian Sommer Thygesen&lt;/em&gt; &lt;br&gt; NPJ 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.12159&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Generative Models for Calorimeter Shower Simulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vinicius Mikuni, Benjamin Nachman&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.11898&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion probabilistic modeling of protein backbones in 3D for the motif-scaffolding problem&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Brian L. Trippe, Jason Yim, Doug Tischer, Tamara Broderick, David Baker, Regina Barzilay, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; CoRR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.04119&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Torsional Diffusion for Molecular Conformer Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bowen Jing, Gabriele Corso, Regina Barzilay, Tommi S. Jaakkola&lt;/em&gt; &lt;br&gt; ICLR Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.01729&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gcorso/torsional-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Protein Structure and Sequence Generation with Equivariant Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Namrata Anand, Tudor Achim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.15019&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nanand2.github.io/proteins/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/ddpm-ipa-protein-generation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Score-based Geometric Model for Molecular Dynamics Simulations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fang Wu, Qiang Zhang, Xurui Jin, Yinghui Jiang, Stan Z. Li&lt;/em&gt; &lt;br&gt; CoRR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.08672&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Equivariant Diffusion for Molecule Generation in 3D&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Victor Garcia Satorras, Clément Vignac, Max Welling&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.17003&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ehoogeboom/e3_diffusion_for_molecules&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, Jian Tang&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.02923&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MinkaiXu/GeoDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Crystal Diffusion Variational Autoencoder for Periodic Material Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, Tommi Jaakkola&lt;/em&gt;&lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.06197&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/txie-93/cdvae&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Predicting Molecular Conformation via Dynamic Graph Score Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Luo, Chence Shi, Minkai Xu, Jian Tang&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://proceedings.neurips.cc/paper/2021/hash/a45a1d12ee0fb7f1f872ab91da18f899-Abstract.html&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2021&lt;/p&gt; &#xA;&lt;h2&gt;Reinforcement Learning&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Trajectory Generation, Control, and Safety with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nicolò Botteghi, Federico Califano, Mannes Poel, Christoph Brune&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.15512&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;H. J. Terry Suh, Glen Chou, Hongkai Dai, Lujie Yang, Abhishek Gupta, Russ Tedrake&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14079&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaolin Fang, Caelan Reed Garrett, Clemens Eppner, Tomás Lozano-Pérez, Leslie Pack Kaelbling, Dieter Fox&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13196&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reward Shaping via Diffusion Process in Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peeyush Kumar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11885&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Value function estimation using conditional diffusion models for control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bogdan Mazoure, Walter Talbott, Miguel Angel Bautista, Devon Hjelm, Alexander Toshev, Josh Susskind&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07290&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instructed Diffuser with Temporal Condition Guidance for Offline Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jifeng Hu, Yanchao Sun, Sili Huang, SiYuan Guo, Hechang Chen, Li Shen, Lichao Sun, Yi Chang, Dacheng Tao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04875&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Professional Basketball Player Behavior Synthesis via Planning with Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiusi Chen, Wei-Yao Wang, Ziniu Hu, Curtis Chou, Lam Hoang, Kun Jin, Mingyan Liu, P. Jeffrey Brantingham, Wei Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04090&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chiyu Max Jiang, Andre Cornman, Cheolho Park, Ben Sapp, Yin Zhou, Dragomir Anguelov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03083&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Extracting Reward Functions from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Felipe Nuti, Tim Franzmeyer, João F. Henriques&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01804&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reconstructing Graph Diffusion History from a Single Snapshot&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruizhong Qiu, Dingsu Wang, Lei Ying, H. Vincent Poor, Yifang Zhang, Hanghang Tong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00488&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SafeDiffuser: Safe Planning with Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wei Xiao, Tsun-Hsuan Wang, Chuang Gan, Daniela Rus&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00148&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://safediffuser.github.io/safediffuser/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Diffusion Policies for Offline Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, Shuicheng Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20081&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sail-sg/edp&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan Zheng, Bin Wang, Zhixuan Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19923&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Behaviorally Diverse Policies with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shashank Hegde, Sumeet Batra, K. R. Zentner, Gaurav S. Sukhatme&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18738&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/policydiffusion/home&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, Xuelong Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18459&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MADiff: Offline Multi-agent Learning with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengbang Zhu, Minghuan Liu, Liyuan Mao, Bingyi Kang, Minkai Xu, Yong Yu, Stefano Ermon, Weinan Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17330&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Policy Representation via Diffusion Probability Model for Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13122&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/BellmanTimeHut/DIPO&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Co-Policy for Synergistic Human-Robot Collaborative Tasks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eley Ng, Ziang Liu, Monroe Kennedy III&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12171&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12824&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenDRAG/CEP-energy-guided-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Goal-Conditioned Imitation Learning using Score-based Diffusion Policies&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Moritz Reuss, Maximilian Li, Xiaogang Jia, Rudolf Lioutikov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02532&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PDPP:Projected Diffusion for Procedure Planning in Instructional Videos&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanlin Wang, Yilu Wu, Sheng Guo, Limin Wang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14676&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EDGI: Equivariant Diffusion for Planning with Embodied Agents&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Johann Brehmer, Joey Bose, Pim de Haan, Taco Cohen&lt;/em&gt; &lt;br&gt; ICLR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12410&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Agent Adversarial Training Using Diffusion Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ying Cao, Elsa Rizk, Stefan Vlaski, Ali H. Sayed&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01936&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model-Augmented Behavioral Cloning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hsiang-Chun Wang, Shang-Fu Chen, Shao-Hua Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13335&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;To the Noise and Back: Diffusion for Shared Autonomy&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Takuma Yoneda, Luzhe Sun, Bradly Stadie, Ge Yang, Matthew Walter&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12244&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusion-for-shared-autonomy.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, Ping Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01877&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imitating Human Behaviour with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, Sam Devlin&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10677&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is Conditional Generative Modeling all you need for Decision-Making?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, Pulkit Agrawal&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.15657&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LAD: Language Augmented Diffusion for Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Edwin Zhang, Yujie Lu, William Wang, Amy Zhang&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.15629&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhendong Wang, Jonathan J Hunt, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2208.06193&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhendong-wang/diffusion-policies-for-offline-rl&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.14548&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Planning with Diffusion for Flexible Behavior Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michael Janner, Yilun Du, Joshua B. Tenenbaum, Sergey Levine&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.09991&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jannerm/diffuser&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 May 2022&lt;/p&gt; &#xA;&lt;h2&gt;Theory&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gen Li, Yuting Wei, Yuxin Chen, Yuejie Chi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09251&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Black-Box Optimization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siddarth Krishnamoorthy, Satvik Mehul Mashkaria, Aditya Grover&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07180&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Variational Gaussian Process Diffusion Processes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Prakhar Verma, Vincent Adam, Arno Solin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02066&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring the Optimal Choice for Generative Processes in Diffusion Models: Ordinary vs Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Cao, Jingrun Chen, Yixin Luo, Xiang Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02063&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Equivalence of Consistency-Type Models: Consistency Models, Consistent Diffusion Models, and Fokker-Planck Regularization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Naoki Murata, Yuki Mitsufuji, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00367&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Stochastic Mechanics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elena Orlova, Aleksei Ustimenko, Ruoxi Jiang, Peter Y. Lu, Rebecca Willett&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19685&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional score-based diffusion models for Bayesian inference in infinite dimensions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lorenzo Baldassari, Ali Siahkoohi, Josselin Garnier, Knut Solna, Maarten V. de Hoop&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19147&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Error Bounds for Flow Matching Methods&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joe Benton, George Deligiannidis, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16860&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tree-Based Diffusion Schr&#34;odinger Bridge with Applications to Wasserstein Barycenters&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Maxence Noble, Valentin De Bortoli, Arnaud Doucet, Alain Durmus&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16557&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhong Yi Wan, Ricardo Baptista, Yi-fan Chen, John Anderson, Anudhyan Boral, Fei Sha, Leonardo Zepeda-Núñez&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15618&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimal Linear Subspace Search: Learning to Construct Fast and High-Quality Schedulers for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhongjie Duan, Chengyu Wang, Cen Chen, Jun Huang, Weining Qian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14677&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SEEDS: Exponential SDE Solvers for Fast High-Quality Sampling from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Martin Gonzalez, Nelson Fernandez, Thuy Tran, Elies Gherbi, Hatem Hajri, Nader Masmoudi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14267&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Convergence of Score-Based Diffusion Models via Prediction-Correction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Francesco Pedrotti, Jan Maas, Marco Mondelli&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14164&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Expressiveness Remarks for Denoising Diffusion Models and Samplers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Francisco Vargas, Teodora Reu, Anna Kerekes&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09605&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Score-Difference Flow for Implicit Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Romann M. Weber&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12906&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Constrained Domains&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nic Fishman, Leo Klarner, Valentin De Bortoli, Emile Mathieu, Michael Hutchinson&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05364&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Bridge Mixture Transports, Schrödinger Bridge Problems and Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Stefano Peluchetti&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.00917&#34;&gt;Paper&lt;/a&gt;] 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models&lt;/strong&gt; \ &lt;em&gt;Anant Raj, Umut Şimşekli, Alessandro Rudi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17109&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Schrödinger Bridge Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuyang Shi, Valentin De Bortoli, Andrew Campbell, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2023. [[Paper(https://arxiv.org/abs/2303.16852)] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sitan Chen, Giannis Daras, Alexandros G. Dimakis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03384&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models are Minimax Optimal Distribution Estimators&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kazusato Oko, Shunta Akiyama, Taiji Suzuki&lt;/em&gt; &lt;br&gt; ICLR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01861&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding the Diffusion Objective as a Weighted Integral of ELBOs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Diederik P. Kingma, Ruiqi Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.00848&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Continuous-Time Functional Diffusion Processes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giulio Franzese, Simone Rossi, Dario Rossi, Markus Heinonen, Maurizio Filippone, Pietro Michiardi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.00800&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Samplers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Francisco Vargas, Will Grathwohl, Arnaud Doucet&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13834&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Infinite-Dimensional Diffusion Models for Function Spaces&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jakiw Pidstrigach, Youssef Marzouk, Sebastian Reich, Sven Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10130&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Diffusion Models in Function Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jae Hyun Lim, Nikola B. Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, Christopher Pal, Arash Vahdat, Anima Anandkumar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07400&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score Approximation, Estimation and Distribution Recovery of Diffusion Models on Low-Dimensional Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minshuo Chen, Kaixuan Huang, Tuo Zhao, Mengdi Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07194&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stochastic Modified Flows, Mean-Field Limits and Dynamics of Stochastic Gradient Descent&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Benjamin Gess, Sebastian Kassing, Vitalii Konarovskyi&lt;/em&gt; &lt;br&gt; JMLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07125&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Monte Carlo Neural Operator for Learning PDEs via Probabilistic Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rui Zhang, Qi Meng, Rongchan Zhu, Yue Wang, Wenlei Shi, Shihua Zhang, Zhi-Ming Ma, Tie-Yan Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05104&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example-Based Sampling with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bastien Doignies, Nicolas Bonneel, David Coeurjolly, Julie Digne, Loïs Paulin, Jean-Claude Iehl, Victor Ostromoukhov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05116&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Information-Theoretic Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xianghao Kong, Rob Brekelmans, Greg Ver Steeg&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03792&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gregversteeg/InfoDiffusionSimple&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Flow Matching: Simulation-Free Dynamic Optimal Transport&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, Yoshua Bengio&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00482&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/atong01/conditional-flow-matching&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transport with Support: Data-Conditional Diffusion Bridges&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ella Tamir, Martin Trapp, Arno Solin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13636&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding and contextualising diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Stefano Scotta, Alberto Messina&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01394&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Mathematics of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David McAllester&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11108&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding the diffusion models by conditional expectations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yubin Lu, Zhongjian Wang, Guillaume Bal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.07882&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Thompson Sampling with Diffusion Generative Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Guan Hsieh, Shiva Prasad Kasiviswanathan, Branislav Kveton, Patrick Blöbaum&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.05182&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your diffusion model secretly knows the dimension of the data manifold&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.12611&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Generative Modeling Secretly Minimizes the Wasserstein Distance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dohyun Kwon, Ying Fan, Kangwook Lee&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06359&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UW-Madison-Lee-Lab/score-wasserstein&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nonlinear controllability and function representation by neural stochastic differential equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tanya Veeravalli, Maxim Raginsky&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00896&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Generative Models in Infinite Dimensions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gavin Kerrigan, Justin Ley, Padhraic Smyth&lt;/em&gt; &lt;br&gt; AISTATS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.00886&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Langevin Dynamics: towards interpretable Neural Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Simon M. Koop, Mark A. Peletier, Jacobus W. Portegies, Vlado Menkovski&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09537&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Analysis of Score-based Generative Modeling: User-Friendly Bounds under Minimal Smoothness Assumptions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongrui Chen, Holden Lee, Jianfeng Lu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Categorical SDEs with Simplex Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pierre H. Richemond, Sander Dieleman, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.14784&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Causal Discovery via Topological Ordering&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pedro Sanchez, Xiao Liu, Alison Q O&#39;Neil, Sotirios A. Tsaftaris&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.06201&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vios-s/DiffAN&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Regularizing Score-based Models with Score Fokker-Planck Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.04296&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Louis Sharrock, Jack Simons, Song Liu, Mark Beaumont&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.04872&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analyzing Diffusion as Serial Reproduction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Raja Marjieh, Ilia Sucholutsky, Thomas A. Langlois, Nori Jacoby, Thomas L. Griffiths&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14821&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Convergence of score-based generative modeling for general data distributions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Holden Lee, Jianfeng Lu, Yixin Tan&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.12381&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, Anru R. Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.11215&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Riemannian Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chin-Wei Huang, Milad Aghajohari, Avishek Joey Bose, Prakash Panangaden, Aaron Courville&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.07949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Convergence of denoising diffusion models under the manifold hypothesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Valentin De Bortoli&lt;/em&gt; &lt;br&gt; TMLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.05314&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Diffusion Processes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, Fergus Simpson&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.03992&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Theory and Algorithms for Diffusion Processes on Riemannian Manifolds&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.01729&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gcorso/torsional-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Riemannian Score-Based Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Valentin De Bortoli, Emile Mathieu, Michael Hutchinson, James Thornton, Yee Whye Teh, Arnaud Doucet&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.02763&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting diffusion score matching using normalizing flow&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenbo Gong, Yingzhen Li&lt;/em&gt; &lt;br&gt; ICML Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.10072&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Connection Between Score Matching and Denoising Autoencoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pascal Vincent&lt;/em&gt; &lt;br&gt; Neural Computation 2011. [&lt;a href=&#34;http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2011&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bayesian Learning via Stochastic Gradient Langevin Dynamics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max Welling, Yee Whye Teh&lt;/em&gt; &lt;br&gt; ICML 2011. [&lt;a href=&#34;https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JavierAntoran/Bayesian-Neural-Networks#stochastic-gradient-langevin-dynamics-sgld&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Apr 2011&lt;/p&gt; &#xA;&lt;h2&gt;Applications&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Source Separation with Applications to Digital Communication Signals&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tejas Jayashankar, Gary C. F. Lee, Alejandro Lancho, Amir Weiss, Yury Polyanskiy, Gregory W. Wornell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14411&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SEEDS: Emulation of Weather Forecast Ensembles with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lizao Li, Rob Carver, Ignacio Lopez-Gomez, Fei Sha, John Anderson&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14066&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A prior regularized full waveform inversion using generative diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fu Wang, Xinquan Huang, Tariq Alkhalifah&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12776&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanDiffusion: diffusion model using perceptual gradients&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yota Ueda, Shinnosuke Takamichi, Yuki Saito, Norihiro Takamune, Hiroshi Saruwatari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12169&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ambigram Generation by A Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Takahiro Shirakawa, Seiichi Uchida&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12049&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unbalanced Diffusion Schr&#34;odinger Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matteo Pariset, Ya-Ping Hsieh, Charlotte Bunne, Andreas Krause, Valentin De Bortoli&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09099&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;User-defined Event Sampling and Uncertainty Quantification in Diffusion Models for Physical Dynamical Systems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marc Finzi, Anudhyan Boral, Andrew Gordon Wilson, Fei Sha, Leonardo Zepeda-Núñez&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07526&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Dynamical Implicit Diffusion Processes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammad R. Rezaei&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07077&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Professional Basketball Player Behavior Synthesis via Planning with Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiusi Chen, Wei-Yao Wang, Ziniu Hu, Curtis Chou, Lam Hoang, Kun Jin, Mingyan Liu, P. Jeffrey Brantingham, Wei Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04090&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-dimensional and Permutation Invariant Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vinicius Mikuni, Benjamin Nachman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03933&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SwinRDM: Integrate SwinRNN with Diffusion Model towards High-Resolution and High-Quality Weather Forecasting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lei Chen, Fei Du, Yuan Hu, Fan Wang, Zhibin Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03110&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inverse-design of nonlinear mechanical metamaterials via video denoising diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jan-Hendrik Bastek, Dennis M. Kochmann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19836&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Score-Based Model for Learning Neural Wavefunctions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Zhang, Shenglong Xu, Shuiwang Ji&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16540&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dirichlet Diffusion Score Model for Biological Sequence Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pavel Avdeyev, Chenlai Shi, Yuhao Tan, Kseniia Dudnyk, Jian Zhou&lt;/em&gt; &lt;br&gt; ICML 2023 [&lt;a href=&#34;https://arxiv.org/abs/2305.10699&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jzhoulab/ddsm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ang Lv, Xu Tan, Peiling Lu, Wei Ye, Shikun Zhang, Jiang Bian, Rui Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10841&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/muzic&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;End-To-End Latent Variational Diffusion Models for Inverse Problems in High Energy Physics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexander Shmakov, Kevin Greif, Michael Fenton, Aishik Ghosh, Pierre Baldi, Daniel Whiteson&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10399&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discrete Diffusion Probabilistic Models for Symbolic Music Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthias Plasser, Silvan Peter, Gerhard Widmer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09489&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/plassma/symbolic-music-discrete-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AWFSD: Accelerated Wirtinger Flow with Score-based Diffusion Image Prior for Poisson-Gaussian Holographic Phase Retrieval&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zongyu Li, Jason Hu, Xiaojian Xu, Liyue Shen, Jeffrey A. Fessler&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.07712&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LatentPINNs: Generative physics-informed neural networks via a latent representation learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammad H. Taufik, Tariq Alkhalifah&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.07671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent diffusion models for generative precipitation nowcasting with accurate uncertainty quantification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jussi Leinonen, Ulrich Hamann, Daniele Nerini, Urs Germann, Gabriele Franch&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12891&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffESM: Conditional Emulation of Earth System Models with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seth Bassetti, Brian Hutchinson, Claudia Tebaldi, Ben Kravitz&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11699&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model for GPS Trajectory Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanshao Zhu, Yongchao Ye, Xiangyu Zhao, James J.Q. Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11582&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guoqiang Zhang, Niwa Kenta, W. Bastiaan Kleijn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11328&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;iPINNs: Incremental learning for Physics-informed neural networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aleksandr Dekhovich, Marcel H.F. Sluiter, David M.J. Tax, Miguel A. Bessa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04854&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models to Predict the Density of Molecular Clouds&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Duo Xu, Jonathan C. Tan, Chia-Jung Hsu, Ye Zhu&lt;/em&gt; &lt;br&gt; ApJ 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01670&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Generative Model and Its Applications in Efficient Wireless Network Management: A Tutorial and Case Study&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yinqiu Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim, Abbas Jamalipour&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17114&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AI-Generated 6G Internet Design: A Diffusion Model-based Learning Approach&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yudong Huang, Minrui Xu, Xinyuan Zhang, Dusit Niyato, Zehui Xiong, Shuo Wang, Tao Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13869&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative AI-aided Optimization for AI-Generated Content (AIGC) Services in Edge Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongyang Du, Zonghang Li, Dusit Niyato, Jiawen Kang, Zehui Xiong, Huawei Huang, Shiwen Mao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13052&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Lizonghang/AGOD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stable Bias: Analyzing Societal Representations in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, Yacine Jernite&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PC-JeDi: Diffusion for Particle Cloud Generation in High Energy Physics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthew Leigh, Debajyoti Sengupta, Guillaume Quétant, John Andrew Raine, Knut Zoch, Tobias Golling&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05376&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Initial Conditions for Ensemble Data Assimilation of Large-Eddy Simulations with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Rybchuk, Malik Hassanaly, Nicholas Hamilton, Paula Doubrawa, Mitchell J. Fulton, Luis A. Martínez-Tossas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.00836&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReorientDiff: Diffusion Model based Reorientation for Object Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Utkarsh A. Mishra, Yongxin Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12700&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://umishra.me/ReorientDiff/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interventional and Counterfactual Inference with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Patrick Chao, Patrick Blöbaum, Shiva Prasad Kasiviswanathan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00860&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion for Sampling SAT Solutions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Karlis Freivalds, Sergejs Kozlovics&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00121&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating astronomical spectra from photometry with conditional diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lars Doorenbos, Stefano Cavuoti, Giuseppe Longo, Massimo Brescia, Raphael Sznitman, Pablo Márquez-Neila&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.05556&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LarsDoorenbos/generate-spectra&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Graphically Structured Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Christian Weilbach, William Harvey, Frank Wood&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.11633&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Error Correction Codes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yoni Choukroun, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.13533&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Diffusion Models for Robust Channel Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marius Arvinte, Jonathan I Tamir&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.08177&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/utcsilab/diffusion-channels&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models for Handwriting Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Troy Luhman, Eric Luhman&lt;/em&gt; &lt;br&gt; arXiv 2020. [&lt;a href=&#34;https://arxiv.org/abs/2011.06704&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tcl9876/Diffusion-Handwriting-Generation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Nov 2020&lt;/p&gt;</summary>
  </entry>
</feed>