<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-15T01:41:21Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>rvaidun/befake</title>
    <updated>2023-06-15T01:41:21Z</updated>
    <id>tag:github.com,2023-06-15:/rvaidun/befake</id>
    <link href="https://github.com/rvaidun/befake" rel="alternate"></link>
    <summary type="html">&lt;p&gt;view bereals without posting your own :)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BeFake&lt;/h1&gt; &#xA;&lt;p&gt;Site will not be hosted anymore since I got a Cease &amp;amp; Desist from BeReal for unauthorized access to their API. Code will still be kept public for educational purposes and to point out the security vulnerabilities in the BeReal API&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;See friends&#39; posts without posting anything yourself&lt;/p&gt; &lt;p&gt;BeFake allows you to see your friends&#39; posts without posting anything yourself. You can also screenshot their posts without them getting notified. You can also see the location (if available), date and time, retakes, comments, and realmoji reactions on all posts. You can even write your own comments straight from BeFake. Support for realmoji reactions is coming soon.&lt;/p&gt; &#xA;  &lt;!-- add an image --&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rvaidun/befake/main/images/post.png&#34; alt=&#34;BeFake Posts&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Upload your own posts and realmoji reactions!&lt;/p&gt; &lt;p&gt;You can also upload your own posts with photos from your camera roll, which will be visible to your friends. GIFs are also supported, meaning your BeReal can be a GIF! The BeFake client also supports custom location with longitude and latitude.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rvaidun/befake/main/images/upload.png&#34; alt=&#34;BeFake Upload&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Interactive Map See where your friends are posting from on an interactive map.The map is interactive, so you can zoom in and out, and move around. You can also click on a marker to see the details of the post in a popup on the map! Once logged in see &lt;a href=&#34;https://befake.ong/map&#34;&gt;https://befake.ong/map&lt;/a&gt; to see the map. Note: The map is does not work well on mobile devices. &lt;img src=&#34;https://raw.githubusercontent.com/rvaidun/befake/main/images/map.png&#34; alt=&#34;BeFake Map&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Development Setup&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://vitejs.dev/config/&#34;&gt;Vite Configuration Reference&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Project Setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Compile and Hot-Reload for Development&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Lint with &lt;a href=&#34;https://eslint.org/&#34;&gt;ESLint&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm run lint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Thank you for considering contributing to Befake! We are grateful for your interest in helping to improve the project and make it even better.&lt;/p&gt; &#xA;&lt;p&gt;There are many ways that you can contribute to the project, including but not limited to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reporting bugs and submitting feature requests&lt;/li&gt; &#xA; &lt;li&gt;Submitting pull requests for bug fixes and new features&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Before you start contributing, please take a moment to read through the following guidelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Familiarize yourself with the project and its goals.&lt;/li&gt; &#xA; &lt;li&gt;Check the existing issues to see if your bug or feature request has already been reported. If it has, please add your thoughts to the existing issue.&lt;/li&gt; &#xA; &lt;li&gt;Follow the project&#39;s code style and conventions.&lt;/li&gt; &#xA; &lt;li&gt;Test your changes thoroughly before submitting a pull request.&lt;/li&gt; &#xA; &lt;li&gt;Make sure your pull request targets the correct branch (usually master).&lt;/li&gt; &#xA; &lt;li&gt;Thank you for your contributions! Every bug report, feature request, and code change helps make befake better for everyone.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Donations&lt;/h1&gt; &#xA;&lt;p&gt;This project is free and open source. If you would like to support the development of this project, you can donate to the project using the following link &lt;a href=&#34;https://ko-fi.com/rahulvaidun&#34;&gt;https://ko-fi.com/rahulvaidun&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alura-es-cursos/1868-java-servlet-1</title>
    <updated>2023-06-15T01:41:21Z</updated>
    <id>tag:github.com,2023-06-15:/alura-es-cursos/1868-java-servlet-1</id>
    <link href="https://github.com/alura-es-cursos/1868-java-servlet-1" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repositorio del curso Servlets 1 de Alura Latam&lt;/p&gt;&lt;hr&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA/warp</title>
    <updated>2023-06-15T01:41:21Z</updated>
    <id>tag:github.com,2023-06-15:/NVIDIA/warp</id>
    <link href="https://github.com/NVIDIA/warp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Python framework for high performance GPU simulation and graphics&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NVIDIA Warp (Preview)&lt;/h1&gt; &#xA;&lt;p&gt;Warp is a Python framework for writing high-performance simulation and graphics code. Kernels are defined in Python syntax and JIT converted to C++/CUDA and compiled at runtime.&lt;/p&gt; &#xA;&lt;p&gt;Warp is comes with a rich set of primitives that make it easy to write programs for physics simulation, geometry processing, and procedural animation. In addition, Warp kernels are differentiable, and can be used as part of machine-learning training pipelines with other frameworks such as PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the project &lt;a href=&#34;https://nvidia.github.io/warp/&#34;&gt;Documentation&lt;/a&gt; for API and language reference and &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/warp/main/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt; for release history.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/warp/main/docs/img/gifs/aldina.gif&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/warp/main/docs/img/gifs/nanovdb.gif&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/warp/main/docs/img/gifs/ocean.gif&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/warp/main/docs/img/gifs/particles.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;A selection of physical simulations computed with Warp&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;p&gt;Warp supports Python versions 3.7.x-3.11.x. The easiest way is to install from PyPi:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install warp-lang&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pre-built binary packages for Windows and Linux are also available on the &lt;a href=&#34;https://github.com/NVIDIA/warp/releases&#34;&gt;Releases&lt;/a&gt; page. To install in your local Python environment extract the archive and run the following command from the root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;An example first program that computes the lengths of random 3D vectors is given below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import warp as wp&#xA;import numpy as np&#xA;&#xA;wp.init()&#xA;&#xA;num_points = 1024&#xA;&#xA;@wp.kernel&#xA;def length(points: wp.array(dtype=wp.vec3),&#xA;           lengths: wp.array(dtype=float)):&#xA;&#xA;    # thread index&#xA;    tid = wp.tid()&#xA;    &#xA;    # compute distance of each point from origin&#xA;    lengths[tid] = wp.length(points[tid])&#xA;&#xA;&#xA;# allocate an array of 3d points&#xA;points = wp.array(np.random.rand(num_points, 3), dtype=wp.vec3)&#xA;lengths = wp.zeros(num_points, dtype=float)&#xA;&#xA;# launch kernel&#xA;wp.launch(kernel=length,&#xA;          dim=len(points),&#xA;          inputs=[points, lengths])&#xA;&#xA;print(lengths)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running Examples&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;examples&lt;/code&gt; directory contains a number of scripts that show how to implement different simulation methods using the Warp API. Most examples will generate USD files containing time-sampled animations in the &lt;code&gt;examples/outputs&lt;/code&gt; directory. Before running examples users should ensure that the &lt;code&gt;usd-core&lt;/code&gt; package is installed using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install usd-core&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;USD files can be viewed or rendered inside NVIDIA &lt;a href=&#34;https://developer.nvidia.com/omniverse&#34;&gt;Omniverse&lt;/a&gt;, Pixar&#39;s UsdView, and Blender. Note that Preview in macOS is not recommended as it has limited support for time-sampled animations.&lt;/p&gt; &#xA;&lt;p&gt;Built-in unit tests can be run from the command-line as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m warp.tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;For developers who want to build the library themselves the following tools are required:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Microsoft Visual Studio 2019 upwards (Windows)&lt;/li&gt; &#xA; &lt;li&gt;GCC 7.2 upwards (Linux)&lt;/li&gt; &#xA; &lt;li&gt;CUDA Toolkit 11.5 or higher&lt;/li&gt; &#xA; &lt;li&gt;Git LFS installed (&lt;a href=&#34;https://git-lfs.github.com/&#34;&gt;https://git-lfs.github.com/&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After cloning the repository, users should run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python build_lib.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate the &lt;code&gt;warp.dll&lt;/code&gt; / &lt;code&gt;warp.so&lt;/code&gt; core library respectively. When building manually users should ensure that their CUDA_PATH environment variable is set, otherwise Warp will be built without CUDA support. Alternatively, the path to the CUDA toolkit can be passed to the build command as &lt;code&gt;--cuda_path=&#34;...&#34;&lt;/code&gt;. After building the Warp package should be installed using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which ensures that subsequent modifications to the library will be reflected in the Python package.&lt;/p&gt; &#xA;&lt;p&gt;If you are cloning from Windows, please first ensure that you have enabled &#34;Developer Mode&#34; in Windows settings and symlinks in git:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git config --global core.symlinks true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will ensure symlinks inside &lt;code&gt;exts/omni.warp&lt;/code&gt; work upon cloning.&lt;/p&gt; &#xA;&lt;h2&gt;Omniverse&lt;/h2&gt; &#xA;&lt;p&gt;A Warp Omniverse extension is available in the extension registry inside Omniverse Kit or Create:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVIDIA/warp/main/docs/img/omniverse.png&#34; width=&#34;550px/&#34;&gt; &#xA;&lt;p&gt;Enabling the extension will automatically install and initialize the Warp Python module inside the Kit Python environment. Please see the &lt;a href=&#34;http://docs.omniverse.nvidia.com/extensions/warp.html&#34;&gt;Omniverse Warp Documentation&lt;/a&gt; for more details on how to use Warp in Omniverse.&lt;/p&gt; &#xA;&lt;h2&gt;Learn More&lt;/h2&gt; &#xA;&lt;p&gt;Please see the following resources for additional background on Warp:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41599&#34;&gt;GTC 2022 Presentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31838&#34;&gt;GTC 2021 Presentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.1145/3476117.3483433&#34;&gt;SIGGRAPH Asia 2021 Differentiable Simulation Course&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The underlying technology in Warp has been used in a number of research projects at NVIDIA including the following publications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accelerated Policy Learning with Parallel Differentiable Simulation - Xu, J., Makoviychuk, V., Narang, Y., Ramos, F., Matusik, W., Garg, A., &amp;amp; Macklin, M. &lt;a href=&#34;https://short-horizon-actor-critic.github.io&#34;&gt;(2022)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DiSECt: Differentiable Simulator for Robotic Cutting - Heiden, E., Macklin, M., Narang, Y., Fox, D., Garg, A., &amp;amp; Ramos, F &lt;a href=&#34;https://github.com/NVlabs/DiSECt&#34;&gt;(2021)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;gradSim: Differentiable Simulation for System Identification and Visuomotor Control - Murthy, J. Krishna, Miles Macklin, Florian Golemo, Vikram Voleti, Linda Petrini, Martin Weiss, Breandan Considine et al. &lt;a href=&#34;https://gradsim.github.io&#34;&gt;(2021)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;p&gt;If you use Warp in your research please use the following citation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{warp2022,&#xA;title= {Warp: A High-performance Python Framework for GPU Simulation and Graphics},&#xA;author = {Miles Macklin},&#xA;month = {March},&#xA;year = {2022},&#xA;note= {NVIDIA GPU Technology Conference (GTC)},&#xA;howpublished = {\url{https://github.com/nvidia/warp}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;How does Warp relate to other Python projects for GPU programming, e.g.: Numba, Taichi, cuPy, PyTorch, etc?&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Warp is inspired by many of these projects, and is closely related to Numba and Taichi which both expose kernel programming to Python. These frameworks map to traditional GPU programming models, so many of the high-level concepts are similar, however there are some functionality and implementation differences.&lt;/p&gt; &#xA;&lt;p&gt;Compared to Numba, Warp supports a smaller subset of Python, but offering auto-differentiation of kernel programs, which is useful for machine learning. Compared to Taichi Warp uses C++/CUDA as an intermediate representation, which makes it convenient to implement and expose low-level routines. In addition, we are building in data structures to support geometry processing (meshes, sparse volumes, point clouds, USD data) as first-class citizens that are not exposed in other runtimes.&lt;/p&gt; &#xA;&lt;p&gt;Warp does not offer a full tensor-based programming model like PyTorch and JAX, but is designed to work well with these frameworks through data sharing mechanisms like &lt;code&gt;__cuda_array_interface__&lt;/code&gt;. For computations that map well to tensors (e.g.: neural-network inference) it makes sense to use these existing tools. For problems with a lot of e.g.: sparsity, conditional logic, hetergenous workloads (like the ones we often find in simulation and graphics), then the kernel-based programming model like the one in Warp are often more convenient since users have control over individual threads.&lt;/p&gt; &#xA;&lt;h3&gt;Does Warp support all of the Python language?&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;No, Warp supports a subset of Python that maps well to the GPU. Our goal is to not have any performance cliffs so that users can expect consistently good behavior from kernels that is close to native code. Examples of unsupported concepts that don&#39;t map well to the GPU are dynamic types, list comprehensions, exceptions, garbage collection, etc.&lt;/p&gt; &#xA;&lt;h3&gt;When should I call &lt;code&gt;wp.synchronize()&lt;/code&gt;?&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;One of the common sources of confusion for new users is when calls to &lt;code&gt;wp.synchronize()&lt;/code&gt; are necessary. The answer is &#34;almost never&#34;! Synchronization is quite expensive, and should generally be avoided unless necessary. Warp naturally takes care of synchronization between operations (e.g.: kernel launches, device memory copies).&lt;/p&gt; &#xA;&lt;p&gt;For example, the following requires no manual synchronization, as the conversion to NumPy will automatically synchronize:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# run some kernels&#xA;wp.launch(kernel_1, dim, [array_x, array_y], device=&#34;cuda&#34;)&#xA;wp.launch(kernel_2, dim, [array_y, array_z], device=&#34;cuda&#34;)&#xA;&#xA;# bring data back to host (and implicitly synchronize)&#xA;x = array_z.numpy()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;em&gt;only&lt;/em&gt; case where manual synchronization is needed is when copies are being performed back to CPU asynchronously, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# copy data back to cpu from gpu, all copies will happen asynchronously to Python&#xA;wp.copy(cpu_array_1, gpu_array_1)&#xA;wp.copy(cpu_array_2, gpu_array_2)&#xA;wp.copy(cpu_array_3, gpu_array_3)&#xA;&#xA;# ensure that the copies have finished&#xA;wp.synchronize()&#xA;&#xA;# return a numpy wrapper around the cpu arrays, note there is no implicit synchronization here&#xA;a1 = cpu_array_1.numpy()&#xA;a2 = cpu_array_2.numpy()&#xA;a3 = cpu_array_3.numpy()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;What happens when you differentiate a function like &lt;code&gt;wp.abs(x)&lt;/code&gt;?&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Non-smooth functions such as &lt;code&gt;y=|x|&lt;/code&gt; do not have a single unique gradient at &lt;code&gt;x=0&lt;/code&gt;, rather they have what is known as a &lt;code&gt;subgradient&lt;/code&gt;, which is formally the convex hull of directional derivatives at that point. The way that Warp (and most auto-differentiation framworks) handles these points is to pick an arbitrary gradient from this set, e.g.: for &lt;code&gt;wp.abs()&lt;/code&gt;, it will arbitrarily choose the gradient to be 1.0 at the origin. You can find the implementation for these functions in &lt;code&gt;warp/native/builtin.h&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Most optimizers (particularly ones that exploit stochasticity), are not sensitive to the choice of which gradient to use from the subgradient, although there are exceptions.&lt;/p&gt; &#xA;&lt;h3&gt;Does Warp support multi-GPU programming?&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Yes! Since version &lt;code&gt;0.4.0&lt;/code&gt; we support allocating, launching, and copying between multiple GPUs in a single process. We follow the naming conventions of PyTorch and use aliases such as &lt;code&gt;cuda:0&lt;/code&gt;, &lt;code&gt;cuda:1&lt;/code&gt;, &lt;code&gt;cpu&lt;/code&gt; to identify individual devices.&lt;/p&gt; &#xA;&lt;h3&gt;Should I switch to Warp over IsaacGym / PhysX?&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Warp is not a replacement for IsaacGym, IsaacSim, or PhysX - while Warp does offer some physical simulation capabilities this is primarily aimed at developers who need differentiable physics, rather than a fully featured physics engine. Warp is also integrated with IsaacGym and is great for performing auxiliary tasks such as reward and observation computations for reinforcement learning.&lt;/p&gt; &#xA;&lt;h2&gt;Discord&lt;/h2&gt; &#xA;&lt;p&gt;We have a &lt;strong&gt;#warp&lt;/strong&gt; channel on the public &lt;a href=&#34;https://discord.com/invite/XWQNJDNuaC&#34;&gt;Omniverse Discord&lt;/a&gt; sever, come chat to us!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Warp is provided under the NVIDIA Source Code License (NVSCL), please see &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA/warp/main/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt; for full license text. Note that the license currently allows only non-commercial use of this code.&lt;/p&gt;</summary>
  </entry>
</feed>