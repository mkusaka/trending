<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-19T01:36:11Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>jerryjliu/create_llama_projects</title>
    <updated>2023-11-19T01:36:11Z</updated>
    <id>tag:github.com,2023-11-19:/jerryjliu/create_llama_projects</id>
    <link href="https://github.com/jerryjliu/create_llama_projects" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;create-llama Projects&lt;/h1&gt; &#xA;&lt;p&gt;Here&#39;s some fun projects created with the &lt;code&gt;create-llama&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jerryjliu/create_llama_projects/main/embedded-tables/README.md&#34;&gt;&lt;code&gt;embedded-tables&lt;/code&gt;&lt;/a&gt;: This project builds an agent that can analyze different data items wtihin the 2021+2020 Tesla 10Q document, including embedded tables.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jerryjliu/create_llama_projects/main/multi-document-agent/README.md&#34;&gt;&lt;code&gt;multi-document-agent&lt;/code&gt;&lt;/a&gt;: This project builds an agent that can do advanced analysis/comparisons across multiple documents. It streams intermediate results as they come in.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>emilwallner/Screenshot-to-code</title>
    <updated>2023-11-19T01:36:11Z</updated>
    <id>tag:github.com,2023-11-19:/emilwallner/Screenshot-to-code</id>
    <link href="https://github.com/emilwallner/Screenshot-to-code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A neural network that transforms a design mock-up into a static website.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/screenshot-to-code.svg?raw=true&#34; width=&#34;800px&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;A detailed tutorial covering the code in this repository:&lt;/strong&gt; &lt;a href=&#34;https://emilwallner.medium.com/how-you-can-train-an-ai-to-convert-your-design-mockups-into-html-and-css-cc7afd82fed4&#34;&gt;Turning design mockups into code with deep learning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plug:&lt;/strong&gt; ðŸ‘‰ Check out my 60-page guide, &lt;a href=&#34;https://twitter.com/EmilWallner/status/1528961488206979072&#34;&gt;No ML Degree&lt;/a&gt;, on how to land a machine learning job without a degree.&lt;/p&gt; &#xA;&lt;p&gt;The neural network is built in three iterations. Starting with a Hello World version, followed by the main neural network layers, and ending by training it to generalize.&lt;/p&gt; &#xA;&lt;p&gt;The models are based on Tony Beltramelli&#39;s &lt;a href=&#34;https://github.com/tonybeltramelli/pix2code&#34;&gt;pix2code&lt;/a&gt;, and inspired by Airbnb&#39;s &lt;a href=&#34;https://airbnb.design/sketching-interfaces/&#34;&gt;sketching interfaces&lt;/a&gt;, and Harvard&#39;s &lt;a href=&#34;https://github.com/harvardnlp/im2markup&#34;&gt;im2markup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; only the Bootstrap version can generalize on new design mock-ups. It uses 16 domain-specific tokens which are translated into HTML/CSS. It has a 97% accuracy. The best model uses a GRU instead of an LSTM. This version can be trained on a few GPUs. The raw HTML version has potential to generalize, but is still unproven and requires a significant amount of GPUs to train. The current model is also trained on a homogeneous and small dataset, thus it&#39;s hard to tell how well it behaves on more complex layouts.&lt;/p&gt; &#xA;&lt;p&gt;A quick overview of the process:&lt;/p&gt; &#xA;&lt;h3&gt;1) Give a design image to the trained neural network&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/LDmoLLV.png&#34; alt=&#34;Insert image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2) The neural network converts the image into HTML markup&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/html_display.gif?raw=true&#34; width=&#34;800px&#34;&gt; &#xA;&lt;h3&gt;3) Rendered output&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/tEAfyZ8.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;FloydHub&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://floydhub.com/run?template=https://github.com/floydhub/pix2code-template&#34;&gt;&lt;img src=&#34;https://static.floydhub.com/button/button.svg?sanitize=true&#34; alt=&#34;Run on FloydHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Click this button to open a &lt;a href=&#34;https://blog.floydhub.com/workspaces/&#34;&gt;Workspace&lt;/a&gt; on &lt;a href=&#34;https://www.floydhub.com/?utm_medium=readme&amp;amp;utm_source=pix2code&amp;amp;utm_campaign=aug_2018&#34;&gt;FloydHub&lt;/a&gt; where you will find the same environment and dataset used for the &lt;em&gt;Bootstrap version&lt;/em&gt;. You can also find the trained models for testing.&lt;/p&gt; &#xA;&lt;h3&gt;Local&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install keras tensorflow pillow h5py jupyter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/emilwallner/Screenshot-to-code.git&#xA;cd Screenshot-to-code/&#xA;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Go do the desired notebook, files that end with &#39;.ipynb&#39;. To run the model, go to the menu then click on Cell &amp;gt; Run all&lt;/p&gt; &#xA;&lt;p&gt;The final version, the Bootstrap version, is prepared with a small set to test run the model. If you want to try it with all the data, you need to download the data here: &lt;a href=&#34;https://www.floydhub.com/emilwallner/datasets/imagetocode&#34;&gt;https://www.floydhub.com/emilwallner/datasets/imagetocode&lt;/a&gt;, and specify the correct &lt;code&gt;dir_name&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Folder structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  |  |-Bootstrap                           #The Bootstrap version&#xA;  |  |  |-compiler                         #A compiler to turn the tokens to HTML/CSS (by pix2code)&#xA;  |  |  |-resources&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#xA;  |  |  |  |-eval_light                    #10 test images and markup&#xA;  |  |-Hello_world                         #The Hello World version&#xA;  |  |-HTML                                #The HTML version&#xA;  |  |  |-Resources_for_index_file         #CSS,images and scripts to test index.html file&#xA;  |  |  |-html                             #HTML files to train it on&#xA;  |  |  |-images                           #Screenshots for training&#xA;  |-readme_images                          #Images for the readme page&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Hello World&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/Hello_world_model.png?raw=true&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;HTML&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/HTML_model.png?raw=true&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Bootstrap&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/Bootstrap_model.png?raw=true&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model weights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.floydhub.com/emilwallner/datasets/imagetocode&#34;&gt;Bootstrap&lt;/a&gt; (The pre-trained model uses GRUs instead of LSTMs)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.floydhub.com/emilwallner/datasets/html_models&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to IBM for donating computing power through their PowerAI platform&lt;/li&gt; &#xA; &lt;li&gt;The code is largely influenced by Tony Beltramelli&#39;s pix2code paper. &lt;a href=&#34;https://github.com/tonybeltramelli/pix2code&#34;&gt;Code&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1705.07962&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The structure and some of the functions are from Jason Brownlee&#39;s &lt;a href=&#34;https://machinelearningmastery.com/develop-a-caption-generation-model-in-keras/&#34;&gt;excellent tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>