<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-11T01:36:45Z</updated>
  <subtitle>Daily Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>google/styleguide</title>
    <updated>2022-06-11T01:36:45Z</updated>
    <id>tag:github.com,2022-06-11:/google/styleguide</id>
    <link href="https://github.com/google/styleguide" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Style guides for Google-originated open-source projects&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Google Style Guides&lt;/h1&gt; &#xA;&lt;p&gt;Every major open-source project has its own style guide: a set of conventions (sometimes arbitrary) about how to write code for that project. It is much easier to understand a large codebase when all the code in it is in a consistent style.&lt;/p&gt; &#xA;&lt;p&gt;“Style” covers a lot of ground, from “use camelCase for variable names” to “never use global variables” to “never use exceptions.” This project (&lt;a href=&#34;https://github.com/google/styleguide&#34;&gt;google/styleguide&lt;/a&gt;) links to the style guidelines we use for Google code. If you are modifying a project that originated at Google, you may be pointed to this page to see the style guides that apply to that project.&lt;/p&gt; &#xA;&lt;p&gt;This project holds the &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html&#34;&gt;C++ Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/csharp-style.html&#34;&gt;C# Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/swift/&#34;&gt;Swift Style Guide&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/google/styleguide/gh-pages/objcguide.md&#34;&gt;Objective-C Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/javaguide.html&#34;&gt;Java Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/pyguide.html&#34;&gt;Python Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/Rguide.html&#34;&gt;R Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/shellguide.html&#34;&gt;Shell Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/htmlcssguide.html&#34;&gt;HTML/CSS Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/jsguide.html&#34;&gt;JavaScript Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/tsguide.html&#34;&gt;TypeScript Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/angularjs-google-style.html&#34;&gt;AngularJS Style Guide&lt;/a&gt;, &lt;a href=&#34;https://google.github.io/styleguide/lispguide.xml&#34;&gt;Common Lisp Style Guide&lt;/a&gt;, and &lt;a href=&#34;https://google.github.io/styleguide/vimscriptguide.xml&#34;&gt;Vimscript Style Guide&lt;/a&gt;. This project also contains &lt;a href=&#34;https://github.com/google/styleguide/tree/gh-pages/cpplint&#34;&gt;cpplint&lt;/a&gt;, a tool to assist with style guide compliance, and &lt;a href=&#34;https://raw.githubusercontent.com/google/styleguide/gh-pages/google-c-style.el&#34;&gt;google-c-style.el&lt;/a&gt;, an Emacs settings file for Google style.&lt;/p&gt; &#xA;&lt;p&gt;If your project requires that you create a new XML document format, the &lt;a href=&#34;https://google.github.io/styleguide/xmlstyle.html&#34;&gt;XML Document Format Style Guide&lt;/a&gt; may be helpful. In addition to actual style rules, it also contains advice on designing your own vs. adapting an existing format, on XML instance document formatting, and on elements vs. attributes.&lt;/p&gt; &#xA;&lt;p&gt;The style guides in this project are licensed under the CC-By 3.0 License, which encourages you to share these documents. See &lt;a href=&#34;https://creativecommons.org/licenses/by/3.0/&#34;&gt;https://creativecommons.org/licenses/by/3.0/&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;The following Google style guides live outside of this project: &lt;a href=&#34;https://golang.org/wiki/CodeReviewComments&#34;&gt;Go Code Review Comments&lt;/a&gt; and &lt;a href=&#34;https://www.dartlang.org/guides/language/effective-dart&#34;&gt;Effective Dart&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;With few exceptions, these style guides are copies of Google&#39;s internal style guides to assist developers working on Google owned and originated open source projects. Changes to the style guides are made to the internal style guides first and eventually copied into the versions found here. &lt;strong&gt;External contributions are not accepted.&lt;/strong&gt; Pull requests are regularly closed without comment. Issues that raise questions, justify changes on technical merits, or point out obvious mistakes may get some engagement and could in theory lead to changes, but we are primarily optimizing for Google&#39;s internal needs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a rel=&#34;license&#34; href=&#34;https://creativecommons.org/licenses/by/3.0/&#34;&gt;&lt;img alt=&#34;Creative Commons License&#34; style=&#34;border-width:0&#34; src=&#34;https://i.creativecommons.org/l/by/3.0/88x31.png&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vladmandic/human</title>
    <updated>2022-06-11T01:36:45Z</updated>
    <id>tag:github.com,2022-06-11:/vladmandic/human</id>
    <link href="https://github.com/vladmandic/human" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Human: AI-powered 3D Face Detection &amp; Rotation Tracking, Face Description &amp; Recognition, Body Pose Tracking, 3D Hand &amp; Finger Tracking, Iris Analysis, Age &amp; Gender &amp; Emotion Prediction, Gaze Tracking, Gesture Recognition&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/package-json/v/vladmandic/human?style=flat-square&amp;amp;svg=true&amp;amp;label=git&#34; alt=&#34;Git Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/v/@vladmandic/human.png?style=flat-square&#34; alt=&#34;NPM Version&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/vladmandic/human?style=flat-square&amp;amp;svg=true&#34; alt=&#34;Last Commit&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/vladmandic/human?style=flat-square&amp;amp;svg=true&#34; alt=&#34;License&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/checks-status/vladmandic/human/main?style=flat-square&amp;amp;svg=true&#34; alt=&#34;GitHub Status Checks&#34;&gt; &lt;img src=&#34;https://img.shields.io/snyk/vulnerabilities/github/vladmandic/human?style=flat-square&amp;amp;svg=true&#34; alt=&#34;Vulnerabilities&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Human Library&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;AI-powered 3D Face Detection &amp;amp; Rotation Tracking, Face Description &amp;amp; Recognition,&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Body Pose Tracking, 3D Hand &amp;amp; Finger Tracking, Iris Analysis,&lt;/strong&gt;&lt;br&gt; &lt;strong&gt;Age &amp;amp; Gender &amp;amp; Emotion Prediction, Gaze Tracking, Gesture Recognition, Body Segmentation&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;JavaScript module using TensorFlow/JS Machine Learning library&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Browser&lt;/strong&gt;:&lt;br&gt; Compatible with both desktop and mobile platforms&lt;br&gt; Compatible with &lt;em&gt;CPU&lt;/em&gt;, &lt;em&gt;WebGL&lt;/em&gt;, &lt;em&gt;WASM&lt;/em&gt; backends&lt;br&gt; Compatible with &lt;em&gt;WebWorker&lt;/em&gt; execution&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;NodeJS&lt;/strong&gt;:&lt;br&gt; Compatible with both software &lt;em&gt;tfjs-node&lt;/em&gt; and&lt;br&gt; GPU accelerated backends &lt;em&gt;tfjs-node-gpu&lt;/em&gt; using CUDA libraries&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;Check out &lt;a href=&#34;https://vladmandic.github.io/human/demo/typescript/index.html&#34;&gt;&lt;strong&gt;Simple Live Demo&lt;/strong&gt;&lt;/a&gt; fully annotated app as a good start starting point (&lt;a href=&#34;https://github.com/vladmandic/human/raw/main/demo/typescript/index.html&#34;&gt;html&lt;/a&gt;)(&lt;a href=&#34;https://github.com/vladmandic/human/raw/main/demo/typescript/index.ts&#34;&gt;code&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Check out &lt;a href=&#34;https://vladmandic.github.io/human/demo/index.html&#34;&gt;&lt;strong&gt;Main Live Demo&lt;/strong&gt;&lt;/a&gt; app for advanced processing of of webcam, video stream or images static images with all possible tunable options&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To start video detection, simply press &lt;em&gt;Play&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;To process images, simply drag &amp;amp; drop in your Browser window&lt;/li&gt; &#xA; &lt;li&gt;Note: For optimal performance, select only models you&#39;d like to use&lt;/li&gt; &#xA; &lt;li&gt;Note: If you have modern GPU, WebGL (default) backend is preferred, otherwise select WASM backend&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Releases&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/releases&#34;&gt;Release Notes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.npmjs.com/package/@vladmandic/human&#34;&gt;NPM Link&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Demos&#34;&gt;&lt;strong&gt;List of all Demo applications&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vladmandic.github.io/human/samples/index.html&#34;&gt;&lt;strong&gt;Live Examples galery&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Browser Demos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Full&lt;/strong&gt; &lt;a href=&#34;https://vladmandic.github.io/human/demo/index.html&#34;&gt;[&lt;em&gt;Live&lt;/em&gt;]&lt;/a&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Main browser demo app that showcases all Human capabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt; &lt;a href=&#34;https://vladmandic.github.io/human/demo/typescript/index.html&#34;&gt;[&lt;em&gt;Live&lt;/em&gt;]&lt;/a&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/typescript&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Simple demo in WebCam processing demo in TypeScript&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Match&lt;/strong&gt; &lt;a href=&#34;https://vladmandic.github.io/human/demo/facematch/index.html&#34;&gt;[&lt;em&gt;Live&lt;/em&gt;]&lt;/a&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/facematch&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Extract faces from images, calculates face descriptors and simmilarities and matches them to known database&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face ID&lt;/strong&gt; &lt;a href=&#34;https://vladmandic.github.io/human/demo/faceid/index.html&#34;&gt;[&lt;em&gt;Live&lt;/em&gt;]&lt;/a&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/faceid&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Runs multiple checks to validate webcam input before performing face match to faces in IndexDB&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-thread&lt;/strong&gt; &lt;a href=&#34;https://vladmandic.github.io/human/demo/multithread/index.html&#34;&gt;[&lt;em&gt;Live&lt;/em&gt;]&lt;/a&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/multithread&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Runs each Human module in a separate web worker for highest possible performance&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;NextJS&lt;/strong&gt; &lt;a href=&#34;https://vladmandic.github.io/human-next/out/index.html&#34;&gt;[&lt;em&gt;Live&lt;/em&gt;]&lt;/a&gt; &lt;a href=&#34;https://github.com/vladmandic/human-next&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Use Human with TypeScript, NextJS and ReactJS&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ElectronJS&lt;/strong&gt; &lt;a href=&#34;https://github.com/vladmandic/human-electron&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Use Human with TypeScript and ElectonJS to create standalone cross-platform apps&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;3D Analysis&lt;/strong&gt; &lt;a href=&#34;https://vladmandic.github.io/human-motion/src/index.html&#34;&gt;[&lt;em&gt;Live&lt;/em&gt;]&lt;/a&gt; &lt;a href=&#34;https://github.com/vladmandic/human-motion&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: 3D tracking and visualization of heead, face, eye, body and hand&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Avatar Bone Mapping&lt;/strong&gt; &lt;a href=&#34;https://vladmandic.github.io/human-vrm/src/human-avatar.html&#34;&gt;[&lt;em&gt;Live&lt;/em&gt;]&lt;/a&gt; &lt;a href=&#34;https://github.com/vladmandic/human-avatar&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Human skeleton with full bone mapping using look and inverse kinematics controllers&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Virtual Model Tracking&lt;/strong&gt; &lt;a href=&#34;https://vladmandic.github.io/human-vrm/src/human-vrm.html&#34;&gt;[&lt;em&gt;Live&lt;/em&gt;]&lt;/a&gt; &lt;a href=&#34;https://github.com/vladmandic/human-vrm&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: VR model with head, face, eye, body and hand tracking&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;NodeJS Demos&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Main&lt;/strong&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/nodejs&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Process images from files, folders or URLs using native methods&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Canvas&lt;/strong&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/nodejs&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Process image from file or URL and draw results to a new image file using &lt;code&gt;node-canvas&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Video&lt;/strong&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/nodejs&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Processing of video input using &lt;code&gt;ffmpeg&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;WebCam&lt;/strong&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/nodejs&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Processing of webcam screenshots using &lt;code&gt;fswebcam&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Events&lt;/strong&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/nodejs&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Showcases usage of &lt;code&gt;Human&lt;/code&gt; eventing to get notifications on processing&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Similarity&lt;/strong&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/nodejs&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Compares two input images for similarity of detected faces&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Match&lt;/strong&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/facematch&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Parallel processing of face &lt;strong&gt;match&lt;/strong&gt; in multiple child worker threads&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multiple Workers&lt;/strong&gt; &lt;a href=&#34;https://github.com/vladmandic/human/tree/main/demo/nodejs&#34;&gt;[&lt;em&gt;Details&lt;/em&gt;]&lt;/a&gt;: Runs multiple parallel &lt;code&gt;human&lt;/code&gt; by dispaching them to pool of pre-created worker processes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project pages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human&#34;&gt;&lt;strong&gt;Code Repository&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.npmjs.com/package/@vladmandic/human&#34;&gt;&lt;strong&gt;NPM Package&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/issues&#34;&gt;&lt;strong&gt;Issues Tracker&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vladmandic.github.io/human/typedoc/classes/Human.html&#34;&gt;&lt;strong&gt;TypeDoc API Specification&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/raw/main/CHANGELOG.md&#34;&gt;&lt;strong&gt;Change Log&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/raw/main/TODO.md&#34;&gt;&lt;strong&gt;Current To-do List&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Wiki pages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki&#34;&gt;&lt;strong&gt;Home&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Install&#34;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Usage&#34;&gt;&lt;strong&gt;Usage &amp;amp; Functions&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Config&#34;&gt;&lt;strong&gt;Configuration Details&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Result&#34;&gt;&lt;strong&gt;Result Details&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Caching&#34;&gt;&lt;strong&gt;Caching &amp;amp; Smoothing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Image&#34;&gt;&lt;strong&gt;Input Processing&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Embedding&#34;&gt;&lt;strong&gt;Face Recognition &amp;amp; Face Description&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Gesture&#34;&gt;&lt;strong&gt;Gesture Recognition&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Issues&#34;&gt;&lt;strong&gt;Common Issues&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Background&#34;&gt;&lt;strong&gt;Background and Benchmarks&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Additional notes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Backends&#34;&gt;&lt;strong&gt;Comparing Backends&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Development-Server&#34;&gt;&lt;strong&gt;Development Server&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Build-Process&#34;&gt;&lt;strong&gt;Build Process&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Module&#34;&gt;&lt;strong&gt;Adding Custom Modules&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Performance&#34;&gt;&lt;strong&gt;Performance Notes&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Profiling&#34;&gt;&lt;strong&gt;Performance Profiling&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Platforms&#34;&gt;&lt;strong&gt;Platform Support&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Diag&#34;&gt;&lt;strong&gt;Diagnostic and Performance trace information&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Docker&#34;&gt;&lt;strong&gt;Dockerize Human applications&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Models&#34;&gt;&lt;strong&gt;List of Models &amp;amp; Credits&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human-models&#34;&gt;&lt;strong&gt;Models Download Repository&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/raw/main/SECURITY.md&#34;&gt;&lt;strong&gt;Security &amp;amp; Privacy Policy&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/raw/main/LICENSE&#34;&gt;&lt;strong&gt;License &amp;amp; Usage Restrictions&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;em&gt;See &lt;a href=&#34;https://github.com/vladmandic/human/issues?q=&#34;&gt;&lt;strong&gt;issues&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/vladmandic/human/discussions&#34;&gt;&lt;strong&gt;discussions&lt;/strong&gt;&lt;/a&gt; for list of known limitations and planned enhancements&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Suggestions are welcome!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt;&#xA;&lt;br&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://vladmandic.github.io/human/samples/samples.html&#34;&gt;Examples galery&lt;/a&gt; for more examples&lt;br&gt; &lt;a href=&#34;https://vladmandic.github.io/human/samples/samples.html&#34;&gt;https://vladmandic.github.io/human/samples/samples.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/human/main/assets/samples.jpg&#34; alt=&#34;samples&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Options&lt;/h2&gt; &#xA;&lt;p&gt;All options as presented in the demo application...&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/human/main/demo/index.html&#34;&gt;demo/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/human/main/assets/screenshot-menu.png&#34; alt=&#34;Options visible in demo&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Results Browser:&lt;/strong&gt;&lt;br&gt; [ &lt;em&gt;Demo -&amp;gt; Display -&amp;gt; Show Results&lt;/em&gt; ]&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/human/main/assets/screenshot-results.png&#34; alt=&#34;Results&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Advanced Examples&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Similarity Matching:&lt;/strong&gt;&lt;br&gt; Extracts all faces from provided input images,&lt;br&gt; sorts them by similarity to selected face&lt;br&gt; and optionally matches detected face with database of known people to guess their names&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/vladmandic/human/main/demo/facematch/index.html&#34;&gt;demo/facematch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/human/main/assets/screenshot-facematch.jpg&#34; alt=&#34;Face Matching&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;3D Rendering:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/vladmandic/human-motion&#34;&gt;human-motion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vladmandic/human-motion/raw/main/assets/screenshot-face.jpg&#34; alt=&#34;Face3D&#34;&gt; &lt;img src=&#34;https://github.com/vladmandic/human-motion/raw/main/assets/screenshot-body.jpg&#34; alt=&#34;Body3D&#34;&gt; &lt;img src=&#34;https://github.com/vladmandic/human-motion/raw/main/assets/screenshot-hand.jpg&#34; alt=&#34;Hand3D&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Avatar Bone Mapping:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/vladmandic/human-avatar&#34;&gt;human-avatar&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vladmandic/human-avatar/raw/main/assets/screenshot.jpg&#34; alt=&#34;Avatar&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;VR Model Tracking:&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/vladmandic/human-vrm&#34;&gt;human-vrmmotion&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/vladmandic/human-vrm/raw/main/assets/human-vrm-screenshot.jpg&#34; alt=&#34;VRM&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;468-Point Face Mesh Defails:&lt;/strong&gt;&lt;br&gt; (view in full resolution to see keypoints)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vladmandic/human/main/assets/facemesh.png&#34; alt=&#34;FaceMesh&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;br&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Simply load &lt;code&gt;Human&lt;/code&gt; (&lt;em&gt;IIFE version&lt;/em&gt;) directly from a cloud CDN in your HTML file:&lt;br&gt; (pick one: &lt;code&gt;jsdelirv&lt;/code&gt;, &lt;code&gt;unpkg&lt;/code&gt; or &lt;code&gt;cdnjs&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;script src=&#34;https://cdn.jsdelivr.net/npm/@vladmandic/human/dist/human.js&#34;&amp;gt;&amp;lt;/script&amp;gt;&#xA;&amp;lt;script src=&#34;https://unpkg.dev/@vladmandic/human/dist/human.js&#34;&amp;gt;&amp;lt;/script&amp;gt;&#xA;&amp;lt;script src=&#34;https://cdnjs.cloudflare.com/ajax/libs/human/2.1.5/human.js&#34;&amp;gt;&amp;lt;/script&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For details, including how to use &lt;code&gt;Browser ESM&lt;/code&gt; version or &lt;code&gt;NodeJS&lt;/code&gt; version of &lt;code&gt;Human&lt;/code&gt;, see &lt;a href=&#34;https://github.com/vladmandic/human/wiki/Install&#34;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Inputs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;Human&lt;/code&gt; library can process all known input types:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Image&lt;/code&gt;, &lt;code&gt;ImageData&lt;/code&gt;, &lt;code&gt;ImageBitmap&lt;/code&gt;, &lt;code&gt;Canvas&lt;/code&gt;, &lt;code&gt;OffscreenCanvas&lt;/code&gt;, &lt;code&gt;Tensor&lt;/code&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;HTMLImageElement&lt;/code&gt;, &lt;code&gt;HTMLCanvasElement&lt;/code&gt;, &lt;code&gt;HTMLVideoElement&lt;/code&gt;, &lt;code&gt;HTMLMediaElement&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, &lt;code&gt;HTMLVideoElement&lt;/code&gt;, &lt;code&gt;HTMLMediaElement&lt;/code&gt; can be a standard &lt;code&gt;&amp;lt;video&amp;gt;&lt;/code&gt; tag that links to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;WebCam on user&#39;s system&lt;/li&gt; &#xA; &lt;li&gt;Any supported video type&lt;br&gt; For example: &lt;code&gt;.mp4&lt;/code&gt;, &lt;code&gt;.avi&lt;/code&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;Additional video types supported via &lt;em&gt;HTML5 Media Source Extensions&lt;/em&gt;&lt;br&gt; Live streaming examples: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;HLS&lt;/strong&gt; (&lt;em&gt;HTTP Live Streaming&lt;/em&gt;) using &lt;code&gt;hls.js&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;DASH&lt;/strong&gt; (Dynamic Adaptive Streaming over HTTP) using &lt;code&gt;dash.js&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;WebRTC&lt;/strong&gt; media track using built-in support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;Example simple app that uses Human to process video input and&lt;br&gt; draw output on screen using internal draw helper functions&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// create instance of human with simple configuration using default values&#xA;const config = { backend: &#39;webgl&#39; };&#xA;const human = new Human(config);&#xA;// select input HTMLVideoElement and output HTMLCanvasElement from page&#xA;const inputVideo = document.getElementById(&#39;video-id&#39;);&#xA;const outputCanvas = document.getElementById(&#39;canvas-id&#39;);&#xA;&#xA;function detectVideo() {&#xA;  // perform processing using default configuration&#xA;  human.detect(inputVideo).then((result) =&amp;gt; {&#xA;    // result object will contain detected details&#xA;    // as well as the processed canvas itself&#xA;    // so lets first draw processed frame on canvas&#xA;    human.draw.canvas(result.canvas, outputCanvas);&#xA;    // then draw results on the same canvas&#xA;    human.draw.face(outputCanvas, result.face);&#xA;    human.draw.body(outputCanvas, result.body);&#xA;    human.draw.hand(outputCanvas, result.hand);&#xA;    human.draw.gesture(outputCanvas, result.gesture);&#xA;    // and loop immediate to the next frame&#xA;    requestAnimationFrame(detectVideo);&#xA;  });&#xA;}&#xA;&#xA;detectVideo();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or using &lt;code&gt;async/await&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// create instance of human with simple configuration using default values&#xA;const config = { backend: &#39;webgl&#39; };&#xA;const human = new Human(config); // create instance of Human&#xA;const inputVideo = document.getElementById(&#39;video-id&#39;);&#xA;const outputCanvas = document.getElementById(&#39;canvas-id&#39;);&#xA;&#xA;async function detectVideo() {&#xA;  const result = await human.detect(inputVideo); // run detection&#xA;  human.draw.all(outputCanvas, result); // draw all results&#xA;  requestAnimationFrame(detectVideo); // run loop&#xA;}&#xA;&#xA;detectVideo(); // start loop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or using &lt;code&gt;Events&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// create instance of human with simple configuration using default values&#xA;const config = { backend: &#39;webgl&#39; };&#xA;const human = new Human(config); // create instance of Human&#xA;const inputVideo = document.getElementById(&#39;video-id&#39;);&#xA;const outputCanvas = document.getElementById(&#39;canvas-id&#39;);&#xA;&#xA;human.events.addEventListener(&#39;detect&#39;, () =&amp;gt; { // event gets triggered when detect is complete&#xA;  human.draw.all(outputCanvas, human.result); // draw all results&#xA;});&#xA;&#xA;function detectVideo() {&#xA;  human.detect(inputVideo) // run detection&#xA;  .then(() =&amp;gt; requestAnimationFrame(detectVideo)); // upon detect complete start processing of the next frame&#xA;}&#xA;&#xA;detectVideo(); // start loop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or using interpolated results for smooth video processing by separating detection and drawing loops:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;const human = new Human(); // create instance of Human&#xA;const inputVideo = document.getElementById(&#39;video-id&#39;);&#xA;const outputCanvas = document.getElementById(&#39;canvas-id&#39;);&#xA;let result;&#xA;&#xA;async function detectVideo() {&#xA;  result = await human.detect(inputVideo); // run detection&#xA;  requestAnimationFrame(detectVideo); // run detect loop&#xA;}&#xA;&#xA;async function drawVideo() {&#xA;  if (result) { // check if result is available&#xA;    const interpolated = human.next(result); // calculate next interpolated frame&#xA;    human.draw.all(outputCanvas, interpolated); // draw the frame&#xA;  }&#xA;  requestAnimationFrame(drawVideo); // run draw loop&#xA;}&#xA;&#xA;detectVideo(); // start detection loop&#xA;drawVideo(); // start draw loop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And for even better results, you can run detection in a separate web worker thread&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;br&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Default models&lt;/h2&gt; &#xA;&lt;p&gt;Default models in Human library are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Detection&lt;/strong&gt;: MediaPipe BlazeFace Back variation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Mesh&lt;/strong&gt;: MediaPipe FaceMesh&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Iris Analysis&lt;/strong&gt;: MediaPipe Iris&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Face Description&lt;/strong&gt;: HSE FaceRes&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Emotion Detection&lt;/strong&gt;: Oarriaga Emotion&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Body Analysis&lt;/strong&gt;: MoveNet Lightning variation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hand Analysis&lt;/strong&gt;: HandTrack &amp;amp; MediaPipe HandLandmarks&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Body Segmentation&lt;/strong&gt;: Google Selfie&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Object Detection&lt;/strong&gt;: CenterNet with MobileNet v3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that alternative models are provided and can be enabled via configuration&lt;br&gt; For example, &lt;code&gt;PoseNet&lt;/code&gt; model can be switched for &lt;code&gt;BlazePose&lt;/code&gt;, &lt;code&gt;EfficientPose&lt;/code&gt; or &lt;code&gt;MoveNet&lt;/code&gt; model depending on the use case&lt;/p&gt; &#xA;&lt;p&gt;For more info, see &lt;a href=&#34;https://github.com/vladmandic/human/wiki/Configuration&#34;&gt;&lt;strong&gt;Configuration Details&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/vladmandic/human/wiki/Models&#34;&gt;&lt;strong&gt;List of Models&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;br&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Diagnostics&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vladmandic/human/wiki/Diag&#34;&gt;How to get diagnostic information or performance trace information&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;br&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Human&lt;/code&gt; library is written in &lt;code&gt;TypeScript&lt;/code&gt; &lt;a href=&#34;https://www.typescriptlang.org/docs/handbook/intro.html&#34;&gt;4.6&lt;/a&gt;&lt;br&gt; Conforming to latest &lt;code&gt;JavaScript&lt;/code&gt; &lt;a href=&#34;https://262.ecma-international.org/&#34;&gt;ECMAScript version 2021&lt;/a&gt; standard&lt;br&gt; Build target is &lt;code&gt;JavaScript&lt;/code&gt; &lt;a href=&#34;https://262.ecma-international.org/11.0/&#34;&gt;EMCAScript version 2018&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;For details see &lt;a href=&#34;https://github.com/vladmandic/human/wiki&#34;&gt;&lt;strong&gt;Wiki Pages&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; and &lt;a href=&#34;https://vladmandic.github.io/human/typedoc/classes/Human.html&#34;&gt;&lt;strong&gt;API Specification&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/stars/vladmandic/human?style=flat-square&amp;amp;svg=true&#34; alt=&#34;Stars&#34;&gt; &lt;img src=&#34;https://badgen.net/github/forks/vladmandic/human&#34; alt=&#34;Forks&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/languages/code-size/vladmandic/human?style=flat-square&amp;amp;svg=true&#34; alt=&#34;Code Size&#34;&gt; &lt;img src=&#34;https://data.jsdelivr.com/v1/package/npm/@vladmandic/human/badge&#34; alt=&#34;CDN&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://img.shields.io/npm/dw/@vladmandic/human.png?style=flat-square&#34; alt=&#34;Downloads&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/dm/@vladmandic/human.png?style=flat-square&#34; alt=&#34;Downloads&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/dy/@vladmandic/human.png?style=flat-square&#34; alt=&#34;Downloads&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>clong/DetectionLab</title>
    <updated>2022-06-11T01:36:45Z</updated>
    <id>tag:github.com,2022-06-11:/clong/DetectionLab</id>
    <link href="https://github.com/clong/DetectionLab" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Automate the creation of a lab environment complete with security tooling and logging best practices&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Detection Lab&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/clong/DetectionLab/master/img/DetectionLab.png&#34; alt=&#34;DetectionLab&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;DetectionLab is tested weekly on Saturdays via a scheduled CircleCI workflow to ensure that builds are passing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://circleci.com/gh/clong/DetectionLab/tree/master&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/clong/DetectionLab/tree/master.svg?style=shield&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/clong/DetectionLab/workflows/Lint%20Code%20Base/badge.svg?sanitize=true&#34; alt=&#34;Lint Code Base&#34;&gt; &lt;a href=&#34;https://github.com/clong/DetectionLab/raw/master/license.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/clong/DetectionLab.svg?style=flat-square&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/maintenance/yes/2022.svg?style=flat-square&#34; alt=&#34;Maintenance&#34;&gt; &lt;a href=&#34;https://github.com/clong/DetectionLab/commit/master&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/clong/DetectionLab.svg?style=flat-square&#34; alt=&#34;GitHub last commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/DetectionLab&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/DetectionLab.svg?style=social&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/detectionlab/shared_invite/zt-mv1qnw9f-3qo2ZrB0IbIKhvinfsgYhg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-DetectionLab-blue&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Donate to the project:&lt;/h4&gt; &#xA;&lt;p&gt;All of the infrastructure, building, and testing of DetectionLab is currently funded by myself in my spare time. If you find this project useful, feel free to buy me a coffee using one of the buttons below!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sponsors/clong&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-Sponsor-red.svg?sanitize=true&#34; alt=&#34;GitHub Sponsor&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/sponsors/clong?frequency=one-time&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub-One--Time%20Sponsor-red&#34; alt=&#34;GitHub One-Time Payment&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/login?return_to=%2Fsponsors%2Fclong%2Fsponsorships%3Ftier_id%3D89561&#34;&gt;$5&lt;/a&gt; | &lt;a href=&#34;https://github.com/login?return_to=%2Fsponsors%2Fclong%2Fsponsorships%3Ftier_id%3D89562&#34;&gt;$20&lt;/a&gt; | &lt;a href=&#34;https://github.com/login?return_to=%2Fsponsors%2Fclong%2Fsponsorships%3Ftier_id%3D97537&#34;&gt;$100&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Purpose&lt;/h2&gt; &#xA;&lt;p&gt;This lab has been designed with defenders in mind. Its primary purpose is to allow the user to quickly build a Windows domain that comes pre-loaded with security tooling and some best practices when it comes to system logging configurations. It can easily be modified to fit most needs or expanded to include additional hosts.&lt;/p&gt; &#xA;&lt;p&gt;Read more about Detection Lab on Medium here: &lt;a href=&#34;https://medium.com/@clong/introducing-detection-lab-61db34bed6ae&#34;&gt;https://medium.com/@clong/introducing-detection-lab-61db34bed6ae&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;NOTE: This lab has not been hardened in any way and runs with default vagrant credentials. Please do not connect or bridge it to any networks you care about. This lab is deliberately designed to be insecure; the primary purpose of it is to provide visibility and introspection into each host.&lt;/p&gt; &#xA;&lt;h2&gt;Primary Lab Features:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Microsoft Advanced Threat Analytics (&lt;a href=&#34;https://www.microsoft.com/en-us/cloud-platform/advanced-threat-analytics&#34;&gt;https://www.microsoft.com/en-us/cloud-platform/advanced-threat-analytics&lt;/a&gt;) is installed on the WEF machine, with the lightweight ATA gateway installed on the DC&lt;/li&gt; &#xA; &lt;li&gt;A Splunk forwarder is pre-installed and all indexes are pre-created. Technology add-ons are also preconfigured.&lt;/li&gt; &#xA; &lt;li&gt;A custom Windows auditing configuration is set via GPO to include command line process auditing and additional OS-level logging&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://github.com/palantir/windows-event-forwarding&#34;&gt;Palantir&#39;s Windows Event Forwarding&lt;/a&gt; subscriptions and custom channels are implemented&lt;/li&gt; &#xA; &lt;li&gt;Powershell transcript logging is enabled. All logs are saved to &lt;code&gt;\\wef\pslogs&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;osquery comes installed on each host and is pre-configured to connect to a &lt;a href=&#34;https://fleetdm.com/&#34;&gt;Fleet&lt;/a&gt; server via TLS. Fleet is preconfigured with the configuration from &lt;a href=&#34;https://github.com/palantir/osquery-configuration&#34;&gt;Palantir&#39;s osquery Configuration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sysmon is installed and configured using &lt;a href=&#34;https://github.com/olafhartong/sysmon-modular&#34;&gt;Olaf Hartong&#39;s open-sourced Sysmon configuration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;All autostart items are logged to Windows Event Logs via &lt;a href=&#34;https://github.com/palantir/windows-event-forwarding/tree/master/AutorunsToWinEventLog&#34;&gt;AutorunsToWinEventLog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Zeek and Suricata are pre-configured to monitor and alert on network traffic&lt;/li&gt; &#xA; &lt;li&gt;Apache Guacamole is installed to easily access all hosts from your local browser&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Building Detection Lab&lt;/h2&gt; &#xA;&lt;p&gt;When preparing to build DetectionLab locally, be sure to use the &lt;code&gt;prepare.[sh|ps1]&lt;/code&gt; scripts inside of the Vagrant folder to ensure your system passes the prerequisite checks for building DetectionLab.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/introduction/prerequisites/&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/macosvm/&#34;&gt;MacOS - Virtualbox or VMware Fusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/windowsvm/&#34;&gt;Windows - Virtualbox or VMware Workstation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/linuxvm/&#34;&gt;Linux - Virtualbox or VMware Workstation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/aws/&#34;&gt;AWS via Terraform&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/azure/&#34;&gt;Azure via Terraform &amp;amp; Ansible&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/esxi/&#34;&gt;ESXi via Terraform &amp;amp; Ansible&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/hyperv/&#34;&gt;HyperV&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/libvirt/&#34;&gt;LibVirt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/proxmox/&#34;&gt;Proxmox&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;DetectionLab Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The primary documentation site is located at &lt;a href=&#34;https://detectionlab.network&#34;&gt;https://detectionlab.network&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/introduction/basicvagrant/&#34;&gt;Basic Vagrant Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/introduction/infoandcreds/&#34;&gt;Lab Information &amp;amp; Credentials&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.detectionlab.network/deployment/troubleshooting/&#34;&gt;Troubleshooting and Known Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please do all of your development in a feature branch on your own fork of DetectionLab. Contribution guidelines can be found here: &lt;a href=&#34;https://raw.githubusercontent.com/clong/DetectionLab/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;In the Media&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://securityweekly.com/2019/02/08/detectionlab-chris-long-pauls-security-weekly-593/&#34;&gt;DetectionLab, Chris Long – Paul’s Security Weekly #593&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://taosecurity.blogspot.com/2019/01/trying-detectionlab.html&#34;&gt;TaoSecurity - Trying DetectionLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.psattack.com/articles/20171218/setting-up-chris-longs-detectionlab/&#34;&gt;Setting up Chris Long&#39;s DetectionLab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://isc.sans.edu/forums/diary/Detection+Lab+Visibility+Introspection+for+Defenders/23135/&#34;&gt;Detection Lab: Visibility &amp;amp; Introspection for Defenders&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits/Resources&lt;/h2&gt; &#xA;&lt;p&gt;A sizable percentage of this code was borrowed and adapted from &lt;a href=&#34;https://twitter.com/stefscherer&#34;&gt;Stefan Scherer&lt;/a&gt;&#39;s &lt;a href=&#34;https://github.com/StefanScherer/packer-windows&#34;&gt;packer-windows&lt;/a&gt; and &lt;a href=&#34;https://github.com/StefanScherer/adfs2&#34;&gt;adfs2&lt;/a&gt; Github repos. A huge thanks to him for building the foundation that allowed me to design this lab environment.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/cloud-platform/advanced-threat-analytics&#34;&gt;Microsoft Advanced Threat Analytics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.splunk.com&#34;&gt;Splunk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://osquery.io&#34;&gt;osquery&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fleetdm/fleet&#34;&gt;Fleet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@palantir/windows-event-forwarding-for-network-defense-cb208d5ff86f&#34;&gt;Windows Event Forwarding for Network Defense&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://github.com/palantir/windows-event-forwarding&#34;&gt;palantir/windows-event-forwarding&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@palantir/osquery-across-the-enterprise-3c3c9d13ec55&#34;&gt;osquery Across the Enterprise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/palantir/osquery-configuration&#34;&gt;palantir/osquery-configuration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.petri.com/configure-event-log-forwarding-windows-server-2012-r2&#34;&gt;Configure Event Log Forwarding in Windows Server 2012 R2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blogs.technet.microsoft.com/jepayne/2015/11/23/monitoring-what-matters-windows-event-forwarding-for-everyone-even-if-you-already-have-a-siem/&#34;&gt;Monitoring what matters — Windows Event Forwarding for everyone&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://technet.microsoft.com/en-us/itpro/windows/keep-secure/use-windows-event-forwarding-to-assist-in-instrusion-detection&#34;&gt;Use Windows Event Forwarding to help with intrusion detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hackernoon.com/the-windows-event-forwarding-survival-guide-2010db7a68c4&#34;&gt;The Windows Event Forwarding Survival Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blogs.msdn.microsoft.com/powershell/2015/06/09/powershell-the-blue-team/&#34;&gt;PowerShell ♥ the Blue Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.microsoftpressstore.com/articles/article.aspx?p=2762082&#34;&gt;Autoruns&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/splunk/TA-microsoft-sysmon&#34;&gt;TA-microsoft-sysmon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SwiftOnSecurity/sysmon-config&#34;&gt;SwiftOnSecurity - Sysmon Config&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/olafhartong/ThreatHunting&#34;&gt;ThreatHunting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/olafhartong/sysmon-modular&#34;&gt;sysmon-modular&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/redcanaryco/atomic-red-team&#34;&gt;Atomic Red Team&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://findingbad.blogspot.com/2020/05/hunting-for-beacons-part-2.html&#34;&gt;Hunting for Beacons&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Velocidex/velociraptor&#34;&gt;Velociraptor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/davidprowe/BadBlood&#34;&gt;BadBlood&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mvelazc0/PurpleSharp&#34;&gt;PurpleSharp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sbousseaden/EVTX-ATTACK-SAMPLES&#34;&gt;EVTX-ATTACK-SAMPLES&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;DetectionLab Sponsors&lt;/h1&gt; &#xA;&lt;h4&gt;Last updated: 11/16/2021&lt;/h4&gt; &#xA;&lt;p&gt;I would like to extend thanks to the following sponsors for funding DetectionLab development. If you are interested in becoming a sponsor, please visit the &lt;a href=&#34;https://github.com/sponsors/clong&#34;&gt;sponsors page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Diamond Sponsors:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/veramine&#34;&gt;Veramine&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ThinkstAppliedResearch&#34;&gt;Thinkst&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kungskal&#34;&gt;kungskal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CyDefUnicorn&#34;&gt;CyDefUnicorn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/olliencc&#34;&gt;olliencc&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/snaplabsio&#34;&gt;snaplabsio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/0x0lolbin&#34;&gt;0x0lolbin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/materaj2&#34;&gt;materaj2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OutpostSecurity&#34;&gt;OutpostSecurity&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Premium Sponsors:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dlee35&#34;&gt;dlee35&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chrissanders&#34;&gt;chrissanders&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jaredhaight&#34;&gt;jaredhaight&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iamfuntime&#34;&gt;iamfuntime&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Luct0r&#34;&gt;Luct0r&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;+1 private sponsor&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Standard Sponsors:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/braimee&#34;&gt;braimee&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/defensivedepth&#34;&gt;defensivedepth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kafkaesqu3&#34;&gt;kafkaesqu3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mdtro&#34;&gt;mdtro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ealaney&#34;&gt;ealaney&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/elreydetoda&#34;&gt;elreydetoda&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/DevBits1702&#34;&gt;DevBits1702&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;+2 private sponsors&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>