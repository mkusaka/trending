<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-03T01:45:00Z</updated>
  <subtitle>Weekly Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bulianglin/psub</title>
    <updated>2024-03-03T01:45:00Z</updated>
    <id>tag:github.com,2024-03-03:/bulianglin/psub</id>
    <link href="https://github.com/bulianglin/psub" rel="alternate"></link>
    <summary type="html">&lt;p&gt;利用CF Worker搭建的反代订阅转换工具&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;psub&lt;/h1&gt; &#xA;&lt;p&gt;利用CF Worker搭建的反代订阅转换工具，通过随机化服务器地址和节点账号密码，解决用户转换订阅的隐私问题&lt;/p&gt; &#xA;&lt;h3&gt;演示网站&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://psub.888005.xyz&#34;&gt;https://psub.888005.xyz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;视频教程&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/X7CC5jrgazo&#34;&gt;https://youtu.be/X7CC5jrgazo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;环境变量名：&lt;code&gt;BACKEND&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;KV或R2变量名：&lt;code&gt;SUB_BUCKET&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;支持反代转换的协议&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;shadowsocks&lt;/li&gt; &#xA; &lt;li&gt;shadowsocksR&lt;/li&gt; &#xA; &lt;li&gt;vmess&lt;/li&gt; &#xA; &lt;li&gt;trojan&lt;/li&gt; &#xA; &lt;li&gt;vless(取决于后端)&lt;/li&gt; &#xA; &lt;li&gt;hysteria(取决于后端)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;打赏&lt;/h3&gt; &#xA;&lt;p&gt;请我喝矿泉水：&lt;a href=&#34;http://b.880805.xyz/&#34;&gt;全专线中专机场AFF链接&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;请我喝桶装水：&lt;a href=&#34;https://bwg.880805.xyz/&#34;&gt;搬瓦工美国CN2 GIA线路AFF链接&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tarpetra/welcome-to-darknet</title>
    <updated>2024-03-03T01:45:00Z</updated>
    <id>tag:github.com,2024-03-03:/tarpetra/welcome-to-darknet</id>
    <link href="https://github.com/tarpetra/welcome-to-darknet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A verified list of darknet markets and services with links for educational reference.&lt;/p&gt;&lt;hr&gt;&lt;center&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/tarpetra/welcome-to-darknet/main/logo.png&#34; width=&#34;200&#34;&gt; &#xA;&lt;/center&gt; &#xA;&lt;h1&gt;Welcome to Darknet&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;A curated list of verified &lt;code&gt;.onion&lt;/code&gt; links of various Onion Services on darknet&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;small&gt; &lt;p&gt;&lt;em&gt;All listed sites have been verified by WTD as legitimate. Beware of unverified links that can lead to you becoming a victim of fraud. WTD is one of the oldest and most reputable darknet link portals, used by thousands of people every day for secure access to darknet sites. This is a selection of the most popular darknet markets and services that are currently represented in our repository:&lt;/em&gt;&lt;/p&gt; &lt;/small&gt; &#xA;&lt;h2&gt;Crypto Exchanges&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://exchanger.dhme3vnfeleniirt5nxuhpmjsfq5srp44uyq2jyihhnrxus7ibfqhiqd.onion&#34;&gt;Infinity Exchanger&lt;/a&gt; - &lt;code&gt;http://exchanger.dhme3vnfeleniirt5nxuhpmjsfq5srp44uyq2jyihhnrxus7ibfqhiqd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://hszyoqnysrl7lpyfms2o5xonhelz2qrz36zrogi2jhnzvpxdzbvzimqd.onion&#34;&gt;Exch Exchange&lt;/a&gt; - &lt;code&gt;http://hszyoqnysrl7lpyfms2o5xonhelz2qrz36zrogi2jhnzvpxdzbvzimqd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://uicrmrtwpfy4y5kddhlwsbsxzykolx73ure5dhd5dst3pc77lfcmm7id.onion&#34;&gt;Swp.cx&lt;/a&gt; - &lt;code&gt;http://uicrmrtwpfy4y5kddhlwsbsxzykolx73ure5dhd5dst3pc77lfcmm7id.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://vyzjtg3peh3rspo67i55pd644o4vh5ygggqhz25c7w3qwfqwuacifoyd.onion&#34;&gt;Majestic Bank&lt;/a&gt; - &lt;code&gt;http://vyzjtg3peh3rspo67i55pd644o4vh5ygggqhz25c7w3qwfqwuacifoyd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Marketplaces&lt;/h2&gt; &#xA;&lt;h3&gt;Established&lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Established Darknet markets that have been operating for a while&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://incognirftkvasqxfh2iess6cr5a243y72z3vbz75nvj2fx62a5ukpad.onion&#34;&gt;Incognito Market&lt;/a&gt; - &lt;code&gt;http://incognirftkvasqxfh2iess6cr5a243y72z3vbz75nvj2fx62a5ukpad.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://zgcbvkvsbesmyji3naq6iuiajpgrcfcjmaue7jvtdtw3dqhrjk7phvqd.onion&#34;&gt;Nemesis Market&lt;/a&gt; - &lt;code&gt;http://zgcbvkvsbesmyji3naq6iuiajpgrcfcjmaue7jvtdtw3dqhrjk7phvqd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://abacusxqw5uv7amzqazdbxo2nd57vaioblew6m25pbzznaf4ph6nh6ad.onion&#34;&gt;Abacus Market&lt;/a&gt; - &lt;code&gt;http://abacusxqw5uv7amzqazdbxo2nd57vaioblew6m25pbzznaf4ph6nh6ad.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://arche442iyjshuhzxgm2crkhoswvshdx2bko5vkvvbnkoeo3fr6liuqd.onion&#34;&gt;Archetyp Market&lt;/a&gt; - &lt;code&gt;http://arche442iyjshuhzxgm2crkhoswvshdx2bko5vkvvbnkoeo3fr6liuqd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://kerberoispgarcfojevke3m6pcz62i5vq7ftjpyea6tnrpv7vu4anjid.onion&#34;&gt;Kerberos Market&lt;/a&gt; - &lt;code&gt;http://kerberoispgarcfojevke3m6pcz62i5vq7ftjpyea6tnrpv7vu4anjid.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://duysanj4hc67rd2hkkrmngj66vj25kxhvafnp3qwrcjcyfgwwd4eeiid.onion&#34;&gt;MGM Grand Market&lt;/a&gt; - &lt;code&gt;http://duysanj4hc67rd2hkkrmngj66vj25kxhvafnp3qwrcjcyfgwwd4eeiid.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://drughub3uuejhj5c7hqrd2fjh7k5y3m6ksg3s3u2upy4byzqmgr7glid.onion&#34;&gt;DrugHub Market&lt;/a&gt; - &lt;code&gt;http://drughub3uuejhj5c7hqrd2fjh7k5y3m6ksg3s3u2upy4byzqmgr7glid.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://superxcceybzhdr6x6xvjslakt53hfcyujnku4g4zy7q6zvvjux3i3id.onion&#34;&gt;Super Market&lt;/a&gt; - &lt;code&gt;http://superxcceybzhdr6x6xvjslakt53hfcyujnku4g4zy7q6zvvjux3i3id.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://rj25tf2tbwt4no22slufc56weql2em2v2zkorklebk3qj6rbbr6bmoid.onion&#34;&gt;Ares Market&lt;/a&gt; - &lt;code&gt;http://rj25tf2tbwt4no22slufc56weql2em2v2zkorklebk3qj6rbbr6bmoid.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://6c5qaapy6ejxcu55u3zsti2yqrkhpddo4fyapgtng2b3bpv6jny2btqd.onion&#34;&gt;Cypher Market&lt;/a&gt; - &lt;code&gt;http://6c5qaapy6ejxcu55u3zsti2yqrkhpddo4fyapgtng2b3bpv6jny2btqd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://drugulkauahb27ehfilah52rusw3n3vffe2big6enjkhphn2wv6qw6ad.onion&#34;&gt;Drugula Market&lt;/a&gt; - &lt;code&gt;http://drugulkauahb27ehfilah52rusw3n3vffe2big6enjkhphn2wv6qw6ad.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://darkmatu4oxdnzpm7j2lohty35nboxdvxz4stfburupf3f4teg6vetyd.onion&#34;&gt;Dark Matter Market&lt;/a&gt; - &lt;code&gt;http://darkmatu4oxdnzpm7j2lohty35nboxdvxz4stfburupf3f4teg6vetyd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://kingdom45dj7wfumjwej2ztogtladxhpwkpifofyeruwgvq4qaltfhad.onion&#34;&gt;Kingdom Market&lt;/a&gt; - &lt;code&gt;http://kingdom45dj7wfumjwej2ztogtladxhpwkpifofyeruwgvq4qaltfhad.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://gofish4dbb4iofcevmcykeox6jbdu43ee2laagspkyhumvvobtwf2aad.onion&#34;&gt;Go Fish Market&lt;/a&gt; - &lt;code&gt;http://gofish4dbb4iofcevmcykeox6jbdu43ee2laagspkyhumvvobtwf2aad.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://hn2paw7xhuadhegnbdbdmpap2g6vxjv7d22uhuzg4uz7thf7qbqqvkid.onion&#34;&gt;We The North Market (Canada)&lt;/a&gt; - &lt;code&gt;http://hn2paw7xhuadhegnbdbdmpap2g6vxjv7d22uhuzg4uz7thf7qbqqvkid.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://flugsvplodga6slsddquahf523xjst2an7umejwgrgbwuk7jd4mbotqd.onion&#34;&gt;Flugsvamp 4.0 (Sweden)&lt;/a&gt; - &lt;code&gt;http://flugsvplodga6slsddquahf523xjst2an7umejwgrgbwuk7jd4mbotqd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://btrhbfajwnzdfheibqqpnl2cp3lftwmde5dvlmeumbn6k5f36fxyi4ad.onion&#34;&gt;BlackSprut Market (Russia)&lt;/a&gt; - &lt;code&gt;http://btrhbfajwnzdfheibqqpnl2cp3lftwmde5dvlmeumbn6k5f36fxyi4ad.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://mega555smdaywpehj5kbrywjpzcrsiuzzkuya7p3sd2zmuc3xhv7i6yd.onion&#34;&gt;Mega Market (Russia)&lt;/a&gt; - &lt;code&gt;http://mega555smdaywpehj5kbrywjpzcrsiuzzkuya7p3sd2zmuc3xhv7i6yd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://rampclunuztejmvknyklq3qdophzzccmejhsle5nzrfrwfdkq5vbnqid.onion&#34;&gt;Ramp Market (Russia)&lt;/a&gt; - &lt;code&gt;http://rampclunuztejmvknyklq3qdophzzccmejhsle5nzrfrwfdkq5vbnqid.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Stores&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://ujhwc2qcu5r6rwerjvf4yostrmqq5epwlfvyfi25pp376jo6ktccnbid.onion&#34;&gt;GammaGoblin&lt;/a&gt; - &lt;code&gt;http://ujhwc2qcu5r6rwerjvf4yostrmqq5epwlfvyfi25pp376jo6ktccnbid.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://smokerbae4sv2bywadc7thltca6o63pqe7nonglextdkvm4yfrdp6oqd.onion&#34;&gt;SmokersCo&lt;/a&gt; - &lt;code&gt;http://smokerbae4sv2bywadc7thltca6o63pqe7nonglextdkvm4yfrdp6oqd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://pygmascvypnxch6a5mgqmurtzwxjxo3pqv2mqnei4yalrxikz4ip5tqd.onion&#34;&gt;Pygmalion Store&lt;/a&gt; - &lt;code&gt;http://pygmascvypnxch6a5mgqmurtzwxjxo3pqv2mqnei4yalrxikz4ip5tqd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://spitzgblkde6uskdj7bon7fevke7sompyoxb44mwfl57immd2r7dbaid.onion&#34;&gt;The Spitzenkörper&lt;/a&gt; - &lt;code&gt;http://spitzgblkde6uskdj7bon7fevke7sompyoxb44mwfl57immd2r7dbaid.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://eisrgs2gltxr3v6zwby3louej4ive5e2nj4e5weton5sdzvyut7vz3qd.onion&#34;&gt;Tribe Seuss&lt;/a&gt; - &lt;code&gt;http://eisrgs2gltxr3v6zwby3louej4ive5e2nj4e5weton5sdzvyut7vz3qd.onion&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! Don&#39;t forget to include links to &lt;code&gt;mirrors.txt&lt;/code&gt; and &lt;code&gt;pgp.txt&lt;/code&gt; of the service you want to list, so that I can verify them before listing.&lt;/p&gt; &#xA;&lt;!-- Contributions welcome! Read the [contribution guidelines](contributing.md) first. --&gt;</summary>
  </entry>
  <entry>
    <title>diff-usion/Awesome-Diffusion-Models</title>
    <updated>2024-03-03T01:45:00Z</updated>
    <id>tag:github.com,2024-03-03:/diff-usion/Awesome-Diffusion-Models</id>
    <link href="https://github.com/diff-usion/Awesome-Diffusion-Models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A collection of resources and papers on Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/hee9joon/Awesome-Diffusion-Models&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-green.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/chetanraj/awesome-github-badges&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Made%20With-Love-red.svg?sanitize=true&#34; alt=&#34;Made With Love&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains a collection of resources and papers on &lt;em&gt;&lt;strong&gt;Diffusion Models&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://diff-usion.github.io/Awesome-Diffusion-Models/&#34;&gt;this page&lt;/a&gt; as this page may not contain all the information due to page constraints.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#resources&#34;&gt;Resources&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#introductory-posts&#34;&gt;Introductory Posts&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#introductory-papers&#34;&gt;Introductory Papers&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#introductory-videos&#34;&gt;Introductory Videos&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#introductory-lectures&#34;&gt;Introductory Lectures&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#tutorial-and-jupyter-notebook&#34;&gt;Tutorial and Jupyter Notebook&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#papers&#34;&gt;Papers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#survey&#34;&gt;Survey&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#vision&#34;&gt;Vision&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#generation&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#classification&#34;&gt;Classification&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#segmentation&#34;&gt;Segmentation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#image-translation&#34;&gt;Image Translation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#inverse-problems&#34;&gt;Inverse Problems&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#medical-imaging&#34;&gt;Medical Imaging&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#multi-modal-learning&#34;&gt;Multi-modal Learning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#3d-vision&#34;&gt;3D Vision&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#adversarial-attack&#34;&gt;Adversarial Attack&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#miscellany&#34;&gt;Miscellany&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#audio&#34;&gt;Audio&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#generation-1&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#conversion&#34;&gt;Conversion&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#enhancement&#34;&gt;Enhancement&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#separation&#34;&gt;Separation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#text-to-speech&#34;&gt;Text-to-Speech&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#miscellany-1&#34;&gt;Miscellany&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#natural-language&#34;&gt;Natural Language&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#tabular-and-time-series&#34;&gt;Tabular and Time Series&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#generation-2&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#forecasting&#34;&gt;Forecasting&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#imputation&#34;&gt;Imputation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#miscellany-2&#34;&gt;Miscellany&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#graph&#34;&gt;Graph&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#generation-3&#34;&gt;Generation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#molecular-and-material-generation&#34;&gt;Molecular and Material Generation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#theory&#34;&gt;Theory&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/diff-usion/Awesome-Diffusion-Models/main/#applications&#34;&gt;Applications&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;h2&gt;Introductory Posts&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;⏩&lt;/span&gt; DiffusionFastForward: 01-Diffusion-Theory&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mikolaj Czerkawski (@mikonvergence)&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/mikonvergence/DiffusionFastForward/raw/master/notes/01-Diffusion-Theory.md&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 4 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How diffusion models work: the math from scratch&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sergios Karagiannakos,Nikolas Adaloglou&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://theaisummer.com/diffusion-models/?fbclid=IwAR1BIeNHqa3NtC8SL0sKXHATHklJYphNH-8IGNoO3xZhSKM_GYcvrrQgB0o&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 24 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Path to the Variational Diffusion Loss&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Alemi&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://blog.alexalemi.com/diffusion.html&#34;&gt;Website&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/github/google-research/vdm/blob/main/colab/SimpleDiffusionColab.ipynb&#34;&gt;Colab&lt;/a&gt;] &lt;br&gt; 15 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Annotated Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Niels Rogge, Kashif Rasul&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://huggingface.co/blog/annotated-diffusion&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 06 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The recent rise of diffusion-based models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Maciej Domagała&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://maciejdomagala.github.io/generative_models/2022/06/06/The-recent-rise-of-diffusion-based-models.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 06 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Introduction to Diffusion Models for Machine Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryan O&#39;Connor&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.assemblyai.com/blog/diffusion-models-for-machine-learning-introduction/&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 12 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Models as an Alternative To GANs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arash Vahdat and Karsten Kreis&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/&#34;&gt;Website-Part 1&lt;/a&gt;] [&lt;a href=&#34;https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-2/&#34;&gt;Website-Part 2&lt;/a&gt;] &lt;br&gt; 26 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An introduction to Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayan Das&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 04 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Introduction to deep generative modeling: Diffusion-based Deep Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jakub Tomczak&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://jmtomczak.github.io/blog/10/10_ddgms_lvm_p2.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 30 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What are Diffusion Models?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lilian Weng&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://lilianweng.github.io/lil-log/2021/07/11/diffusion-models.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 11 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models as a kind of VAE&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Angus Turner&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://angusturner.github.io/generative_models/2021/06/29/diffusion-probabilistic-models-I.html&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 29 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Modeling by Estimating Gradients of the Data Distribution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://yang-song.github.io/blog/2021/score/&#34;&gt;Website&lt;/a&gt;] &lt;br&gt; 5 May 2021&lt;/p&gt; &#xA;&lt;h2&gt;Introductory Papers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding Diffusion Models: A Unified Perspective&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Calvin Luo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.11970&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to Train Your Energy-Based Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Diederik P. Kingma&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2101.03288&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jan 2021&lt;/p&gt; &#xA;&lt;h2&gt;Introductory Videos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;⏩&lt;/span&gt; DiffusionFastForward&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mikolaj Czerkawski (@mikonvergence)&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/playlist?list=PL5RHjmn-MVHDMcqx-SI53mB7sFOqPK6gN&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 4 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models from scratch in PyTorch&lt;/strong&gt; &lt;br&gt; &lt;em&gt;DeepFindr&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=a4Yfz2FxXiY&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 18 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models | Paper Explanation | Math Explained&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Outlier&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=HoKDTa5jHvg&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 6 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What are Diffusion Models?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ari Seff&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=fbLgFrlTnGU&amp;amp;list=LL&amp;amp;index=2&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 20 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models explained&lt;/strong&gt; &lt;br&gt; &lt;em&gt;AI Coffee Break with Letitia&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=344w5h24-h8&amp;amp;ab_channel=AICoffeeBreakwithLetitia&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 23 Mar 2022&lt;/p&gt; &#xA;&lt;h2&gt;Introductory Lectures&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion-based Generative Modeling: Foundations and Applications&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Karsten Kreis, Ruiqi Gao, Arash Vahdat&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://cvpr2022-tutorial-diffusion-models.github.io/&#34;&gt;Page&lt;/a&gt;] &lt;br&gt; 19 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jascha Sohl-Dickstein, MIT 6.S192 - Lecture 22&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://www.youtube.com/watch?v=XCUlnHP1TNM&#34;&gt;Video&lt;/a&gt;] &lt;br&gt; 19 Apr 2022&lt;/p&gt; &#xA;&lt;h2&gt;Tutorial and Jupyter Notebook&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;⏩&lt;/span&gt; DiffusionFastForward: train from scratch in colab&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mikolaj Czerkawski (@mikonvergence)&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/mikonvergence/DiffusionFastForward&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mikonvergence/DiffusionFastForward#computer-code&#34;&gt;notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;diffusion-for-beginners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;ozanciga&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/ozanciga/diffusion-for-beginners&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Beyond Diffusion: What is Personalized Image Generation and How Can You Customize Image Synthesis?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;J. Rafid Siddiqui&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/azad-academy/personalized-diffusion&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://medium.com/mlearning-ai/beyond-diffusion-what-is-personalized-image-generation-and-how-can-you-customize-image-synthesis-26a89d5b335&#34;&gt;Medium&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion_models_tutorial&lt;/strong&gt; &lt;br&gt; &lt;em&gt;FilippoMB&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/FilippoMB/Diffusion_models_tutorial&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ScoreDiffusionModel&lt;/strong&gt; &lt;br&gt; &lt;em&gt;JeongJiHeon&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/JeongJiHeon/ScoreDiffusionModel&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Minimal implementation of diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;VSehwag&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/VSehwag/minimal-diffusion&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;diffusion_tutorial&lt;/strong&gt; &lt;br&gt; &lt;em&gt;sunlin-ai&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/sunlin-ai/diffusion_tutorial&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising diffusion probabilistic models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;acids-ircam&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://github.com/acids-ircam/diffusion_models&#34;&gt;Github&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Centipede Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zalring&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/Zalring/Centipede_Diffusion/blob/main/Centipede_Diffusion.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deforum Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;deforum&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/deforum/stable-diffusion/blob/main/Deforum_Stable_Diffusion.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stable Diffusion Interpolation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;None&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/drive/1EHZtFjQoRr-bns1It5mTcOVyZzZD9bBc?usp=sharing&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Keras Stable Diffusion: GPU starter example&lt;/strong&gt; &lt;br&gt; &lt;em&gt;None&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/drive/1zVTa4mLeM_w44WaFwl7utTaa6JcaH1zK&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Huemin Jax Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;huemin-art&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/huemin-art/jax-guided-diffusion/blob/v2.7/Huemin_Jax_Diffusion_2_7.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disco Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;alembics&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Simplified Disco Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;entmike&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/entmike/disco-diffusion-1/blob/main/Simplified_Disco_Diffusion.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WAS&#39;s Disco Diffusion - Portrait Generator Playground&lt;/strong&gt; &lt;br&gt; &lt;em&gt;WASasquatch&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/WASasquatch/disco-diffusion-portrait-playground/blob/main/WAS&#39;s_Disco_Diffusion_v5_6_9_%5BPortrait_Generator_Playground%5D.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusers - Hugging Face&lt;/strong&gt; &lt;br&gt; &lt;em&gt;huggingface&lt;/em&gt; &lt;br&gt; [&lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb&#34;&gt;Notebook&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;h1&gt;Papers&lt;/h1&gt; &#xA;&lt;h2&gt;Survey&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Survey on Video Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu and Yu-Gang Jiang&lt;/em&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/pdf/2310.10647.pdf&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;State of the Art on Diffusion Models for Visual Computing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T. Barron, Amit H. Bermano, Eric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, C. Karen Liu, Lingjie Liu, Ben Mildenhall, Matthias Nießner, Björn Ommer, Christian Theobalt, Peter Wonka, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07204&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Benjamin Hoover, Hendrik Strobelt, Dmitry Krotov, Judy Hoffman, Zsolt Kira, Duen Horng Chau&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.16750&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianyi Zhang, Zheng Wang, Jing Huang, Mohiuddin Muhammad Tasnim, Wei Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13142&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Image Restoration and Enhancement -- A Comprehensive Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Li, Yulin Ren, Xin Jin, Cuiling Lan, Xingrui Wang, Wenjun Zeng, Xinchao Wang, Zhibo Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09388&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Comprehensive Survey on Generative Diffusion Models for Structured Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Heejoon Koo, To Eun Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04139&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Design Fundamentals of Diffusion Models: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyi Chang, George A. Koulieris, Hubert P. H. Shum&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04542&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models in NLP: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao Zou, Zae Myung Kim, Dongyeop Kang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Time Series Applications: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, Junbin Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00624&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Comprehensive Survey on Knowledge Distillation of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weijian Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04262&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Survey on Graph Diffusion Models: Generative AI in Science for Molecule, Protein and Material&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, Chaoning Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01565&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenshuang Zhang, Chaoning Zhang, Sheng Zheng, Mengchun Zhang, Maryam Qamar, Sung-Ho Bae, In So Kweon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13336&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models in NLP: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuansong Zhu, Yu Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07576&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-image Diffusion Model in Generative AI: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, In So Kweon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07909&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Non-autoregressive Text Generation: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifan Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06574&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models in Bioinformatics: A New Wave of Deep Learning Revolution in Action&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiye Guo, Jian Liu, Yanli Wang, Mengrui Chen, Duolin Wang, Dong Xu, Jianlin Cheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10907&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Diffusion Models on Graphs: Methods and Applications&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenqi Fan, Chengyi Liu, Yunqing Liu, Jiatong Li, Hang Li, Hui Liu, Jiliang Tang, Qing Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02591&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Medical Image Analysis: A Comprehensive Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, Dorit Merhof&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.07804&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Diffusion Models for Vision: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anwaar Ulhaq, Naveed Akhtar, Ganna Pogrebna&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.09292&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models in Vision: A Survey&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.04747&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Survey on Generative Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanqun Cao, Cheng Tan, Zhangyang Gao, Guangyong Chen, Pheng-Ann Heng, Stan Z. Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.02646&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models: A Comprehensive Survey of Methods and Applications&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ling Yang, Zhilong Zhang, Shenda Hong, Wentao Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.00796&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2022&lt;/p&gt; &#xA;&lt;h2&gt;Vision&lt;/h2&gt; &#xA;&lt;h3&gt;Generation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffEnc: Variational Diffusion with a Learned Encoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Beatrix M. G. Nielsen, Anders Christensen, Andrea Dittadi, Ole Winther&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19789&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Z. Xiao, Johannes Zenn, Robert Bamler&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19653&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Successfully Applying Lottery Ticket Hypothesis to Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chao Jiang, Bo Hui, Bohan Liu, Da Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.18823&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Noise-Free Score Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Oren Katzir, Or Patashnik, Daniel Cohen-Or, Dani Lischinski&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17590&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The statistical thermodynamics of generative diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luca Ambrogioni&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17467&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenkai Zhang, Krista A. Ehinger, Tom Drummond&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17167&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Longlin Yu, Tianyu Xie, Yu Zhu, Tong Yang, Xiangyu Zhang, Cheng Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17153&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/longinyu/hsivi&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RePoseDM: Recurrent Pose Alignment and Gradient Guidance for Pose Guided Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anant Khandelwal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16074&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Techniques for Training Consistency Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Prafulla Dhariwal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.14189&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhongzhan Huang, Pan Zhou, Shuicheng Yan, Liang Lin&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13545&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sail-sg/ScaleLong&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Particle Guidance: non-I.I.D. Diverse Sampling with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gabriele Corso, Yilun Xu, Valentin de Bortoli, Regina Barzilay, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13102&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gcorso/particle-guidance&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Closed-Form Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Christopher Scarvelis, Haitz Sáez de Ocáriz Borde, Justin Solomon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12395&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Elucidating The Design Space of Classifier-Guided Diffusion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiajun Ma, Tianyang Hu, Wenjia Wang, Jiacheng Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11311&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alexmaols/elucd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siqi Kou, Lei Gan, Dequan Wang, Chongxuan Li, Zhijie Deng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11142&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Interpretable Directions in h-space of Pre-trained Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijian Zhang, Luping Liu. Zhijie Lin, Yichen Zhu, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09912&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengfei Xia, Yujun Shen, Changsong Lei, Yu Zhou, Ran Yi, Deli Zhao, Wenping Wang, Yong-jin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09469&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unseen Image Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Zhu, Yu Wu, Zhiwei Deng, Olga Russakovsky, Yan Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09213&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Debias the Training of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hu Yu, Li Shen, Jie Huang, Man Zhou, Hongsheng Li, Feng Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08442&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Grigory Bartosh, Dmitry Vetrov, Christian A. Naesseth&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08337&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Integrators for Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kushagra Pandey, Maja Rudolph, Stephan Mandt&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07894&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huangjie Zheng, Zhendong Wang, Jianbo Yuan, Guanghan Ning, Pengcheng He, Quanzeng You, Hongxia Yang, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06389&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05737&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/magvit2-pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Emergence of Reproducibility and Consistency in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Liyue Shen, Qing Qu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05264&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffNAS: Bootstrapping Diffusion Models by Prompting for Better Architectures&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenhao Li, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.04750&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Observation-Guided Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junoh Kang, Jinyoung Choi, Sungik Choi, Bohyung Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.04041&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, Hang Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.04378&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Step-aware Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuai Yang, Yukang Chen, Luozhou Wang, Shu Liu, Yingcong Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03337&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yefei He, Jing Liu, Weijia Wu, Hong Zhou, Bohan Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03270&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Energy-Based Prior Model with Diffusion-Amortized MCMC&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peiyu Yu, Yaxuan Zhu, Sirui Xie, Xiaojian Ma, Ruiqi Gao, Song-Chun Zhu, Ying Nian Wu&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03218&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuPeiyu98/Diffusion-Amortized-MCMC&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Memorization in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, Ye Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02664&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sail-sg/DiffMemorize&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sequential Data Generation with Groupwise Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sangyun Lee, Gayoung Lee, Hyunsu Kim, Junho Kim, Youngjung Uh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01400&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02279&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Completing Visual Objects via Bridging Generation and Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00808&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Decoding Realistic Images from Brain Activity with Contrastive Self-supervision and Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyuan Sun, Mingxiao Li, Marie-Francine Moens&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00318&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tasin Islam, Alina Miron, XiaoHui Liu, Yongmin Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00106&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Bridge Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Linqi Zhou, Aaron Lou, Samar Khanna, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.16948&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeeDiff: Dynamic Uncertainty-Aware Early Exiting for Accelerating Diffusion Model Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengkun Tang, Yaqing Wang, Caiwen Ding, Yi Liang, Yao Li, Dongkuan Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.17074&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Distilling ODE Solvers of Diffusion Models into Smaller Steps&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sanghwan Kim, Hao Tang, Fisher Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.16421&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Yuan, Michael Maire&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.15726&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Escher Meshes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Noam Aigerman, Thibault Groueix&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14564&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yangming Li, Boris van Breugel, Mihaela van der Schaar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14068&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLOBER: Coherent Non-autoregressive Video Generation via GLOBal Guided Video DecodER&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingzhen Sun, Weining Wang, Zihan Qin, Jiahui Sun, Sihan Chen, Jing Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.13274&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/iva-mzsun/glober&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score Mismatching for Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Senmao Ye, Fei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11043&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generalised Probabilistic Diffusion Scale-Spaces&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pascal Peter&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.08511&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Image Dynamics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengqi Li, Richard Tucker, Noah Snavely, Aleksander Holynski&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07906&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://generative-dynamics.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Beta Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingyuan Zhou, Tianqi Chen, Zhendong Wang, Huangjie Zheng&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07867&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adapt and Diffuse: Sample-adaptive Reconstruction via Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zalan Fabian, Berk Tinaz, Mahdi Soltanolkotabi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.06642&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Elucidating the solution space of extended reverse-time SDE for diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinpeng Cui, Xinyi Zhang, Zongqing Lu, Qingmin Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.06169&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaxuan Zhu, Jianwen Xie, Yingnian Wu, Ruiqi Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05153&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Relay Diffusion: Unifying diffusion process across resolutions for image synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, Jie Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03350&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gradient Domain Diffusion Models for Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanhao Gong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01875&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchical Masked 3D Diffusion Model for Video Outpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning Jiang, Chunjie Luo, Jianfeng Zhan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.02119&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fanfanda.github.io/M3DDM/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models with Deterministic Normalizing Flow Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohsen Zand, Ali Etemad, Michael Greenspan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01274&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MohsenZand/DiNof&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Inertial Poser: Human Motion Reconstruction from Arbitrary Sparse IMU Configurations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tom Van Wouwe, Seunghwan Lee, Antoine Falisse, Scott Delp, C. Karen Liu&lt;/em&gt; &lt;br&gt; AAAI 2024. [&lt;a href=&#34;https://arxiv.org/abs/2308.16682&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Davide Scassola, Sebastiano Saccani, Ginevra Carbone, Luca Bortolussi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16534&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Elucidating the Exposure Bias in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, Itir Onal Ertugrul&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15321&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Residual Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, Liangqiong Qu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13712&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nachifur/RDDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Transfer Learning in Diffusion Models via Adversarial Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiyu Wang, Baijiong Lin, Daochang Liu, Chang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.11948&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Diffusion Models with an Adaptive Momentum Sampler&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiyu Wang, Anh-Dung Dinh, Daochang Liu, Chang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.11941&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-It-4D: Synthesizing a Consistent Long-Term Dynamic Scene Video from a Single Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Liao Shen, Xingyi Li, Huiqiang Sun, Juewen Peng, Ke Xian, Zhiguo Cao, Guosheng Lin&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10257&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spiking-Diffusion: Vector Quantized Discrete Diffusion Model with Spiking Neural Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingxuan Liu, Rui Wen, Hong Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10187&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SciRE-Solver: Efficient Sampling of Diffusion Probabilistic Models by Score-integrand Solver with Recursive Derivative Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shigui Li, Wei Chen, Delu Zeng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07896&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Jiaming Song, Yongxin Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02157&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Patched Denoising Diffusion Models For High-Resolution Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheng Ding, Mengqi Zhang, Jiajun Wu, Zhuowen Tu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spatial-Frequency U-Net for Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Yuan, Linjie Li, Jianfeng Wang, Zhengyuan Yang, Kevin Lin, Zicheng Liu, Lijuan Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.14648&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesis of Batik Motifs using a Diffusion -- Generative Adversarial Network&lt;/strong&gt; &lt;br&gt; &lt;em&gt;One Octadion, Novanto Yudistira, Diva Kurnianingtyas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.12122&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DPM-OT: A New Diffusion Probabilistic Model Based on Optimal Transport&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zezeng Li, ShengHao Li, Zhanpeng Wang, Na Lei, Zhongxuan Luo, Xianfeng Gu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.11308&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cognaclee/DPM-OT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Sampling with Momentum for Mitigating Divergence Artifacts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suttisak Wizadwongsa, Worameth Chinchuthakun, Pramook Khungurn, Amit Raj, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.11118&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flow Matching in Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Quan Dao, Hao Phung, Binh Nguyen, Anh Tran&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08698&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vinairesearch.github.io/LFM/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Manifold-Guided Sampling in Diffusion Models for Unbiased Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingzhe Su, Wenwen Qiang, Zeen Song, Hang Gao, Fengge Wu, Changwen Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08199&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Complexity Matters: Rethinking the Latent Space for Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianyang Hu, Fei Chen, Haonan Wang, Jiawei Li, Wenjia Wang, Jiacheng Sun, Zhenguo Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08283&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collaborative Score Distillation for Consistent Visual Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, Jinwoo Shin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.04787&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://subin-kim-cv.github.io/CSD/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/subin-kim-cv/CSD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProtoDiffusion: Classifier-Free Diffusion Guidance with Prototype Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gulcin Baykal, Halil Faruk Karagoz, Taha Binhuraib, Gozde Unal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.01924&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.01952&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tserendorj Adiya, Sanghun Kim, Jung Eun Lee, Jae Shin Yoon, Hwasup Lim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.00574&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spiking Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiahang Cao, Ziqing Wang, Hanzhong Guo, Hao Cheng, Qiang Zhang, Renjing Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17046&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DomainStudio: Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14153&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Decoupled Diffusion Models with Explicit Transition Probability&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuhang Huang, Zheng Qin, Xinwang Liu, Kai Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13720&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Continuous Layout Editing of Single Images with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyuan Zhang, Zhitong Huang, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13078&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semi-Implicit Denoising Diffusion Models (SIDDMs)&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yanwu Xu, Mingming Gong, Shaoan Xie, Wei Wei, Matthias Grundmann, kayhan Batmanghelich, Tingbo Hou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12511&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Eliminating Lipschitz Singularities in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhantao Yang, Ruili Feng, Han Zhang, Yujun Shen, Kai Zhu, Lianghua Huang, Yifei Zhang, Yu Liu, Deli Zhao, Jingren Zhou, Fan Cheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11251&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GD-VDM: Generated Depth for better Diffusion-based Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ariel Lapid, Idan Achituve, Lior Bracha, Ethan Fetaya&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11173&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Harmonization with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiajie Li, Jian Wang, Chen Wang, Jinjun Xiong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10441&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training Diffusion Classifiers with Denoising Assistance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chandramouli Sastry, Sri Harsha Dumpala, Sageev Oore&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09192&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Human Sketch Synthesis with Explicit Abstraction Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dar-Yen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09274&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Training of Diffusion Models with Masked Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongkai Zheng, Weili Nie, Arash Vahdat, Anima Anandkumar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09305&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Anima-Lab/MaskDiT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Relation-Aware Diffusion Model for Controllable Poster Layout Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fengheng Li, An Liu, Wei Feng, Honghe Zhu, Yaoyu Li, Zheng Zhang, Jingjing Lv, Xin Zhu, Junjie Shen, Zhangang Lin, Jingping Shao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09086&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OMS-DPM: Optimizing the Model Schedule for Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Enshu Liu, Xuefei Ning, Zinan Lin, Huazhong Yang, Yu Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08860&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Allan Jabri, Sjoerd van Steenkiste, Emiel Hoogeboom, Mehdi S. M. Sajjadi, Thomas Kipf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08068&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zike Wu, Pan Zhou, Kenji Kawaguchi, Hanwang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.06991&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sail-sg/FDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changyao Tian, Chenxin Tao, Jifeng Dai, Hao Li, Ziheng Li, Lewei Lu, Xiaogang Wang, Hongsheng Li, Gao Huang, Xizhou Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05423&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Architecture Multi-Expert Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunsung Lee, Jin-Young Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, Seungtaek Choi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04990&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpreting and Improving Diffusion Models Using the Euclidean Distance Function&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Frank Permenter, Chenyang Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04848&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video Diffusion Models with Local-Global Context Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siyuan Yang, Lu Zhang, Yu Liu, Zhizhuo Jiang, You He&lt;/em&gt; &lt;br&gt; IJCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02562&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/exisas/LGC-VD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain Diffusion for Visual Exploration: Cortical Discovery using Large Scale Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrew F. Luo, Margaret M. Henderson, Leila Wehbe, Michael J. Tarr&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03089&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Faster Training of Diffusion Models and Improved Density Estimation via Parallel Score Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Etrit Haxholli, Marco Lorenzi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02658&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Temporal Dynamic Quantization for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, Eunhyeok Park&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Generation from Unconditional Diffusion Models using Denoiser Representations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Graikos, Srikar Yellapragada, Dimitris Samaras&lt;/em&gt; &lt;br&gt; BMVC 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01900&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cvlab-stonybrook/fewshot-conditional-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditioning Diffusion Models via Attributes and Semantic Masks for Face Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nico Giambi, Giuseppe Lisanti&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00914&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Differential Diffusion: Giving Each Pixel Its Strength&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eran Levin, Ohad Fried&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00950&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Addressing Discrepancies in Semantic and Visual Alignment in Neural Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Natalie Abreu, Nathan Vaska, Victoria Helus&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01148&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Addressing Negative Transfer in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyojun Go, JinYoung Kim, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, Hyeongdon Moon, Seungtaek Choi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00354&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Geometric Perspective on Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Defang Chen, Zhenyu Zhou, Jian-Ping Mei, Chunhua Shen, Chun Chen, Can Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19947&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spontaneous symmetry breaking in generative diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gabriel Raya, Luca Ambrogioni&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19693&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifei Liu, Rex Shen, Xiaotong Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One-Line-of-Code Data Mollification Improves Optimization of Likelihood-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ba-Hien Tran, Giulio Franzese, Pietro Michiardi, Maurizio Filippone&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18900&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ambient Diffusion: Learning Clean Distributions from Corrupted Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alexandros G. Dimakis, Adam Klivans&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19256&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Accurate Data-free Quantization for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18723&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BRIGHT: Bi-level Feature Representation of Image Collections using Groups of Hash Tables&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dingdong Yang, Yizhi Wang, Ali Mahdavi-Amiri, Hao Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18601&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bright-project01.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, Zhihua Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18455&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning to Jump: Thinning and Thickening Latent Counts for Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianqi Chen, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18375&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tqch/poisson-jump&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reconstructing the Mind&#39;s Eye: fMRI-to-Image with Contrastive Learning and Diffusion Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paul S. Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Ethan Cohen, Aidan J. Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth A. Norman, Tanishq Mathew Abraham&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18274&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://medarc-ai.github.io/mindeye/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyuan Sun, Mingxiao Li, Zijiao Chen, Yunhao Zhang, Shaonan Wang, Marie-Francine Moens&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17214&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Parallel Sampling of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, Nima Anari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16317&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AndyShih12/paradigms&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Trans-Dimensional Generative Modeling via Jump Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrew Campbell, William Harvey, Christian Weilbach, Valentin De Bortoli, Tom Rainforth, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UDPM: Upsampling Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shady Abu-Hussein, Raja Giryes&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16269&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying GANs and Score-Based Diffusion as Generative Particle Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bézenac, Mickaël Chen, Alain Rakotomamonjy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16150&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DuDGAN: Improving Class-Conditional GANs via Dual-Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Taesun Yeom, Minhyeok Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14849&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingxiao Li, Tingyu Qu, Wei Sun, Marie-Francine Moens&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15583&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Robust Classification via a Single Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15241&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Generalization of Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingyang Yi, Jiacheng Sun, Zhenguo Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14712&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VDT: An Empirical Study on Video Diffusion with Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, Mingyu Ding&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13311&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RERV/VDT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijiao Chen, Jiaxin Qing, Juan Helen Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11675&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mind-video.com/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PTQD: Accurate Post-Training Quantization for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, Bohan Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10657&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Javier E Santos, Zachary R. Fox, Nicholas Lubbers, Yen Ting Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11089&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Structural Pruning for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gongfan Fang, Xinyin Ma, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10924&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/VainF/Diff-Pruning&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Shao, Xu Dai, Shouyi Yin, Lujun Li, Huanran Chen, Yang Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10769&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Mind Visual Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bohan Zeng, Shanglin Li, Xuhui Liu, Sicheng Gao, Xiaolong Jiang, Xu Tang, Yao Hu, Jianzhuang Liu, Baochang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10135&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analyzing Bias in Diffusion-based Face Generation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Malsha V. Perera, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06402&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaiwen Zheng, Cheng Lu, Jianfei Chen, Jun Zhu&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03935&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LEO: Generative Latent Image Animator for Human Video Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaohui Wang, Xin Ma, Xinyuan Chen, Antitza Dantcheva, Bo Dai, Yu Qiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03989&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://wyhsirius.github.io/LEO-project/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wyhsirius/LEO&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Iterative α-(de)Blending: a Minimalist Deterministic Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eric Heitz, Laurent Belcour, Thomas Chambon&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03486&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reconstructing seen images from human brain activity via guided stochastic search&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Reese Kneeland, Jordyn Ojeda, Ghislain St-Yves, Thomas Naselaris&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00556&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Motion-Conditioned Diffusion Model for Controllable Video Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, Ming-Hsuan Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14404&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tsaishien-chen.github.io/MCDiff/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 27 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zihao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13224&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Compositional Visual Generation with Latent Classifier Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changhao Shi, Haomiao Ni, Kai Li, Shaobo Han, Mingfu Liang, Martin Renqiang Min&lt;/em&gt; &lt;br&gt; CVPR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12536&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao Wang, Pengcheng He, Zhangyang Wang, Weizhu Chen, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12526&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Variational Diffusion Auto-encoder: Deep Latent Variable Model with Unconditional Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12141&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LaMD: Latent Motion Diffusion for Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaosi Hu, Zhenzhong Chen, Chong Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11603&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lookahead Diffusion Probabilistic Models for Refining Mean Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guoqiang Zhang, Niwa Kenta, W. Bastiaan Kleijn&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11312&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/guoqiang-zhang-x/LA-DPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seung Wook Kim, Bradley Brown, Kangxue Yin, Karsten Kreis, Katja Schwarz, Daiqing Li, Robin Rombach, Antonio Torralba, Sanja Fidler&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09787&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attributing Image Generative Models using Latent Fingerprints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guangyu Nie, Changhoon Kim, Yezhou Yang, Yi Ren&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09752&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Identity Encoder for Personalized Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Chuan Su, Kelvin C.K. Chan, Yandong Li, Yang Zhao, Han Zhang, Boqing Gong, Huisheng Wang, Xuhui Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07429&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Memory Efficient Diffusion Probabilistic Models via Patch-based Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shinei Arakawa, Hideki Tsunashima, Daichi Horita, Keitaro Tanaka, Shigeo Morishima&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07087&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DCFace: Synthetic Face Generation with Dual Condition Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minchul Kim, Feng Liu, Anil Jain, Xiaoming Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07060&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mk-minchul/dcface&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li, Zhenguo Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06648&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, Tong Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06767&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Johanna Karras, Aleksander Holynski, Ting-Chun Wang, Ira Kemelmacher-Shlizerman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06025&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://grail.cs.washington.edu/projects/dreampose/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/johannakarras/DreamPose&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reflected Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aaron Lou, Stefano Ermon&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04740&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://aaronlou.com/blog/2023/reflected-diffusion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/louaaron/Reflected-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Binary Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ze Wang, Jiang Wang, Zicheng Liu, Qiang Qiu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04820&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models as Masked Autoencoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Wei, Karttikeya Mangalam, Po-Yao Huang, Yanghao Li, Haoqi Fan, Hu Xu, Huiyu Wang, Cihang Xie, Alan Yuille, Christoph Feichtenhofer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03283&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://weichen582.github.io/diffmae.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot Semantic Image Synthesis with Class Affinity Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marlène Careil, Jakob Verbeek, Stéphane Lathuilière&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02321&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EGC: Image Generation and Classification via a Diffusion Energy-Based Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiushan Guo, Chuofan Ma, Yi Jiang, Zehuan Yuan, Yizhou Yu, Ping Luo&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02012&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://guoqiushan.github.io/egc.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Token Merging for Fast Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daniel Bolya, Judy Hoffman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17604&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/dbolya/tomesd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Closer Look at Parameter-Efficient Tuning in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chendong Xiang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.18181&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;-Diff: Infinite Resolution Diffusion with Subsampled Mollified States&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sam Bond-Taylor, Chris G. Willcocks&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.18242&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-aware Image Generation using 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianfeng Xiang, Jiaolong Yang, Binbin Huang, Xin Tong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17905&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jeffreyxiang.github.io/ivid/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistent View Synthesis with Pose-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, Johannes Kopf&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17598&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCollage: Parallel Generation of Large Content with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, Ming-Yu Liu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17076&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/dir/diffcollage/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Masked Diffusion Transformer is a Strong Image Synthesizer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shanghua Gao, Pan Zhou, Ming-Ming Cheng, Shuicheng Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14389&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sail-sg/MDT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Image-to-Video Generation with Latent Flow Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haomiao Ni, Changhao Shi, Kai Li, Sharon X. Huang, Martin Renqiang Min&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13744&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nihaomiao/CVPR23_LFDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12346&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://msra-nuwa.azurewebsites.net/#/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Object-Centric Slot Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jindong Jiang, Fei Deng, Gautam Singh, Sungjin Ahn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10834&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LDMVFI: Video Frame Interpolation with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Duolikun Danier, Fan Zhang, David Bull&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09508&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Diffusion Training via Min-SNR Weighting Strategy&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, Baining Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09556&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VideoFusion: Decomposed Diffusion Models for High-Quality Video Generation&lt;/strong&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08320&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpretable ODE-style Generative Diffusion Model via Force Field Construction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weiyang Jin, Yongpei Zhu, Yuxi Peng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08063&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Regularized Vector Quantization for Tokenized Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiahui Zhang, Fangneng Zhan, Christian Theobalt, Shijian Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06424&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PARASOL: Parametric Style Control for Diffusion Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gemma Canet Tarrés, Dan Ruta, Tu Bui, John Collomosse&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06464&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain-Diffuser: Natural scene reconstruction from fMRI signals using generative latent diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Furkan Ozcelik, Rufin VanRullen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05334&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paul Hagemann, Lars Ruthotto, Gabriele Steidl, Nicole Tianjiao Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04772&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TRACT: Denoising Diffusion Models with Transitive Closure Time-Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, Eric Gu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04248&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Diffusions in Augmented Spaces: A Complete Recipe&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kushagra Pandey, Stephan Mandt&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01748&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistency Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Prafulla Dhariwal, Mark Chen, Ilya Sutskever&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01469&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peiye Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing, Joshua M. Susskind, Miguel Ángel Bautista&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.00165&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of Semantic Latent Directions in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yong-Hyun Park, Mingi Kwon, Junghyo Jo, Youngjung Uh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12469&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11552&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://energy-based-model.github.io/reduce-reuse-recycle/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning 3D Photography Videos via Self-supervised Diffusion on Single Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaodong Wang, Chenfei Wu, Shengming Yin, Minheng Ni, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Fan Yang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10781&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Calibrating Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianyu Pang, Cheng Lu, Chao Du, Min Lin, Shuicheng Yan, Zhijie Deng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10688&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/thudzj/Calibrated-DPMs&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zebin You, Yong Zhong, Fan Bao, Jiacheng Sun, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10586&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cross-domain Compositing with Pretrained Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roy Hachnochi, Mingrui Zhao, Nadav Orzech, Rinon Gal, Ali Mahdavi-Amiri, Daniel Cohen-Or, Amit Haim Bermano&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10167&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cross-domain-compositing/cross-domain-compositing&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoration based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaemoo Choi, Yesom Park, Myungjoo Kang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05456&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giannis Daras, Yuval Dagan, Alexandros G. Dimakis, Constantinos Daskalakis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.09057&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/giannisdaras/cdm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun Xiao, Mu Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08908&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video Probabilistic Diffusion Models in Projected Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sihyun Yu, Kihyuk Sohn, Subin Kim, Jinwoo Shin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07685&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sihyun.me/PVDM/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFaceSketch: High-Fidelity Face Image Synthesis with Sketch-Guided Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yichen Peng, Chunqi Zhao, Haoran Xie, Tsukasa Fukusato, Kazunori Miyata&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.06908&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Where to Diffuse, How to Diffuse, and How to Get Back: Automated Learning for Multivariate Diffusions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Raghav Singhal, Mark Goldstein, Rajesh Ranganath&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Preconditioned Score-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Li Zhang, Hengyuan Ma, Xiatian Zhu, Jianfeng Feng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.06504&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://github.com/fudan-zvg/PDS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Star-Shaped Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrey Okhotin, Dmitry Molchanov, Vladimir Arkhipkin, Grigory Bartosh, Aibek Alanov, Dmitry Vetrov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05259&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04867&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://unipc.ivg-research.xyz&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wl-zhao/UniPC&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Geometry of Score Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sandesh Ghimire, Jinyang Liu, Armand Comas, Davin Hill, Aria Masoomi, Octavia Camps, Jennifer Dy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04411&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q-Diffusion: Quantizing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, Kurt Keutzer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04304&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PFGM++: Unlocking the Potential of Physics-Inspired Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04265&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Newbeeer/pfgmpp&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Long Horizon Temperature Scaling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andy Shih, Dorsa Sadigh, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03686&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spatial Functa: Scaling Functa to ImageNet Classification and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthias Bauer, Emilien Dupont, Andy Brock, Dan Rosenbaum, Jonathan Schwarz, Hyunjik Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03130&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ShiftDDPMs: Exploring Conditional Diffusion Models by Shifting Diffusion Trajectories&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijian Zhang, Zhou Zhao, Jun Yu, Qi Tian&lt;/em&gt; &lt;br&gt; AAAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02373&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Divide and Compose with Score Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sandesh Ghimire, Armand Comas, Davin Hill, Aria Masoomi, Octavia Camps, Jennifer Dy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02272&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sandeshgh/Score-based-disentanglement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stable Target Field for Reduced Variance Score Estimation in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilun Xu, Shangyuan Tong, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00670&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Newbeeer/stf&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DisDiff: Unsupervised Disentanglement of Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Yang, Yuwang Wang, Yan Lv, Nanning Zheng&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13721&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing DDPM Sampling with Shortcut Fine-Tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ying Fan, Kangwook Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13362&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Data Representations with Joint Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kamil Deja, Tomasz Trzcinski, Jakub M. Tomczak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13622&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERA-Solver: Error-Robust Adams Solver for Fast Sampling of Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengmeng Li, Luping Liu, Zenghao Chai, Runnan Li, Xu Tan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12935&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Don&#39;t Play Favorites: Minority Guidance for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Soobin Um, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12334&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sangyun884/fast-ode&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Guided Diffusion Sampling with Splitting Numerical Methods&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suttisak Wizadwongsa, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11558&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Input Perturbation Reduces Exposure Bias in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, Rita Cucchiara&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11706&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/forever208/DDPM-IP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Minimizing Trajectory Curvature of ODE-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sangyun Lee, Beomsu Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12003&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Importance of Noise Scheduling for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ting Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10972&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;simple diffusion: End-to-end diffusion for high resolution images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Jonathan Heek, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11093&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Inference in Denoising Diffusion Models via MMD Finetuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emanuele Aiello, Diego Valsesia, Enrico Magli&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.07969&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/diegovalsesia/MMD-DDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Transformer Backbones for Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Princy Chahal&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.14678&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijian Zhang, Zhou Zhao, Zhijie Lin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.12990&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scalable Adaptive Computation for Iterative Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Allan Jabri, David Fleet, Ting Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11972&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchically branched diffusion models for efficient and interpretable multi-class conditional generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex M. Tseng, Tommaso Biancalani, Max Shen, Gabriele Scalia&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.10777&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, Baining Guo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09478&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/researchmm/MM-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scalable Diffusion Models with Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;William Peebles, Saining Xie&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09748&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.wpeebles.com/DiT&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyeongnyeon Kim, Wooseok Jang, Gyuseong Lee, Susung Hong, Junyoung Seo, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08861&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ku-cvlab.github.io/DAG/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Practical Plug-and-Play Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyojun Go, Yunsung Lee, Jin-Young Kim, Seunghyun Lee, Myeongho Jeong, Hyun Seung Lee, Seungtaek Choi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.05973&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Brain Decoding: from fMRI to conceptually similar image reconstruction of visual stimuli&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matteo Ferrante, Tommaso Boccato, Nicola Toschi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06726&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MAGVIT: Masked Generative Video Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, Lu Jiang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.05199&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://magvit.cs.cmu.edu/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 10 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyeongman Kim, Hajin Shim, Hyunsu Kim, Yunjey Choi, Junho Kim, Eunho Yang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02802&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Naoki Matsunaga, Masato Ishii, Akio Hayakawa, Kenji Suzuki, Takuya Narihira&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02024&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VIDM: Video Implicit Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kangfu Mei, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00235&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://kfmei.page/vidm/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MKFMIKU/VIDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Why Are Conditional Generative Models Better Than Unconditional Ones?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00362&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity Guided Image Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaskirat Singh, Stephen Gould, Liang Zheng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.17084&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://1jsingh.github.io/gradop&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Continuous-time Discrete Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, Hanjun Dai&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16750&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wavelet Diffusion Models are fast and scalable Image Generators&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao Phung, Quan Dao, Anh Tran&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16152&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dimensionality-Varying Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Han Zhang, Ruili Feng, Zhantao Yang, Lianghua Huang, Yu Liu, Yifei Zhang, Yujun Shen, Deli Zhao, Jingren Zhou, Fan Cheng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16032&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Refining Generative Process with Discriminator Guidance in Score-based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongjun Kim, Yeongmin Kim, Wanmo Kang, Il-Chul Moon&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.17091&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Model Made Slim&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingyi Yang, Daquan Zhou, Jiashi Feng, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.17106&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Sampling of Diffusion Models via Operator Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, Anima Anandkumar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13449&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Video Diffusion Models for High-Fidelity Video Generation with Arbitrary Lengths&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, Qifeng Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13221&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paint by Example: Exemplar-based Image Editing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, Fang Wen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13227&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinDiffusion: Learning a Diffusion Model from a Single Natural Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/WeilunWang/SinDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Sampling with Classifier-based Feature Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wujie Sun, Defang Chen, Can Wang, Deshi Ye, Yan Feng, Chun Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12039&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SceneComposer: Any-Level Semantic Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Zeng, Zhe Lin, Jianming Zhang, Qing Liu, John Collomosse, Jason Kuen, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11742&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zengyu.me/scenec/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based Scene Graph to Image Generation with Masked Contrastive Pre-Training&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ling Yang, Zhilin Huang, Yang Song, Shenda Hong, Guohao Li, Wentao Zhang, Bin Cui, Bernard Ghanem, Ming-Hsuan Yang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11138&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinFusion: Training Diffusion Models on a Single Image or Video&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaniv Nikankin, Niv Haim, Michal Irani&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11743&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MagicVideo: Efficient Video Generation With Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, Jiashi Feng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11018&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://magicvideo.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, Juan Helen Zhou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06956&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mind-vis.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zjc062/mind-vis&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot Image Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.03264&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Denoising Diffusions to Denoising Markov Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.03595&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuyang-shi/generalized-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, Jun-Yan Zhu&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.02048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lmxyy/sige&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An optimal control perspective on diffusion-based generative modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julius Berner, Lorenz Richter, Karen Ullrich&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01364&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Entropic Neural Optimal Transport via Diffusion Processes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry Vetrov, Evgeny Burnaev&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01156&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; NeurIPS 2022 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2211.01095&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LuChengTHU/dpm-solver&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Denoising Diffusion with Non-Isotropic Gaussian Noise Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vikram Voleti, Christopher Pal, Adam Oberman&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12254&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Equilibrium Approaches to Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ashwini Pokle, Zhengyang Geng, Zico Kolter&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12867&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/locuslab/deq-ddim&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Representation Learning with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jeremias Traub&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.11058&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jeremiastraub/diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vincent Tao Hu, David W Zhang, Yuki M. Asano, Gertjan J. Burghouts, Cees G. M. Snoek&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.06462&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://taohu.me/sgdm/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GENIE: Higher-Order Denoising Diffusion Solvers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Dockhorn, Arash Vahdat, Karsten Kreis&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.05475&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/GENIE/&#34;&gt;Project&lt;/a&gt; [&lt;a href=&#34;https://github.com/nv-tlabs/GENIE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Angel Bautista, Josh Susskind&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.04955&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://jiataogu.me/fdm/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 10 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Distillation of Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.03142&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Sample Quality of Diffusion Model Using Self-Attention Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Susung Hong, Gyuseong Lee, Wooseok Jang, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.00939&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ku-cvlab.github.io/Self-Attention-Guidance/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 3 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OCD: Learning to Overfit with Conditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shahar Shlomo Lutati, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.00471&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ShaharLutatiPersonal/OCD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Faces in the Wild: Quantitative Comparison of Stable Diffusion, Midjourney and DALL-E 2&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ali Borji&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.00586&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/aliborji/GFW&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising MCMC for Accelerating Diffusion-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Beomsu Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14593&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/1202kbs/DMCMC&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;All are Worth Words: a ViT Backbone for Score-based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Chongxuan Li, Yue Cao, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.12152&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Wavelet-domain Diffusion for 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ka-Hei Hui, Ruihui Li, Jingyu Hu, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08725&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can segmentation models be trained with fully synthetically generated data?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Petru-Daniel Tudosiu, Mark S Graham, Tom Vercauteren, M Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08256&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blurring Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.05557&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Soft Diffusion: Score Matching for General Corruptions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G. Dimakis, Peyman Milanfar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.05442&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Masked Image Generation with Token-Critic&lt;/strong&gt; &lt;br&gt; &lt;em&gt;José Lezama, Huiwen Chang, Lu Jiang, Irfan Essa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.04439&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let us Build Bridges: Understanding and Extending Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchao Liu, Lemeng Wu, Mao Ye, Qiang Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.14699&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu Cheng, Lu Yuan, Yu-Chiang Frank Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.13753&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adaptively-Realistic Image Generation from Stroke and Sketch with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shin-I Cheng, Yu-Jie Chen, Wei-Chen Chiu, Hsin-Ying Lee, Hung-Yu Tseng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.12675&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cyj407.github.io/DiSS/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09392&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arpitbansal297/Cold-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhancing Diffusion-Based Image Synthesis with Robust Classifier Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Roy Ganz, Michael Elad&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.08664&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your ViT is Secretly a Hybrid Discriminative-Generative Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiulong Yang, Sheng-Min Shih, Yinlin Fu, Xiaoting Zhao, Shihao Ji&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.07791&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sndnyang/Diffusion_ViT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Applying Regularized Schrödinger-Bridge-Based Stochastic Process in Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ki-Ung Song&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.07131&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KiUngSong/RSB&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ting Chen, Ruixiang Zhang, Geoffrey Hinton&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.04202&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pyramidal Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dohoon Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.01864&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Progressive Deblurring of Diffusion Models for Coarse-to-Fine Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sangyun Lee, Hyungjin Chung, Jaehyeon Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.11192&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sangyun884/blur-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Model Efficiency Through Patching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Troy Luhman, Eric Luhman&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.04316&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ericl122333/PatchDiffusion-Pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hengyuan Ma, Li Zhang, Xiatian Zhu, Jianfeng Feng&lt;/em&gt; &lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.02196&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPI-GAN: Distilling Score-based Generative Models with Straight-Path Interpolations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinsung Jeon, Noseong Park&lt;/em&gt; &lt;br&gt; arxiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.14464&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Entropy-driven Sampling and Training Scheme for Conditional Diffusion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengming Li, Guangcong Zheng, Hui Wang, Taiping Yao, Yang Chen, Shoudong Ding, Xi Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.11474&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Modelling With Inverse Heat Dissipation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Severi Rissanen, Markus Heinonen, Arno Solin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.13397&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://aaltoml.github.io/generative-inverse-heat-dissipation/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models as plug-and-play priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, Dimitris Samaras&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.09012&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alexgraikos/diffusion_priors&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Flexible Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weitao Du, Tao Yang, He Zhang, Yuanqi Du&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2206.10365&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lossy Compression with Gaussian Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lucas Theis, Tim Salimans, Matthew D. Hoffman, Fabian Mentzer&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.08889&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maximum Likelihood Training for Score-Based Diffusion ODEs by High-Order Denoising Score Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.08265&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LuChengTHU/mle_score_ode&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Estimating the Optimal Covariance with Imperfect Mean in Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, Bo Zhang&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07309&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/baofff/Extended-Analytic-DPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Video Prediction and Infilling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tobias Höppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, Andrea Dittadi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07696&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, Yan Yan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07771&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/L-YeZhu/CDCD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;gDDIM: Generalized denoising diffusion implicit models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Molei Tao, Yongxin Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.05564&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qsh-zh/gDDIM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How Much is Enough? A Study on Diffusion Times in Score-based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giulio Franzese, Simone Rossi, Lixuan Yang, Alessandro Finamore, Dario Rossi, Maurizio Filippone, Pietro Michiardi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.05173&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Generation with Multimodal Priors using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.05039&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Score-based Generative Models for High-Resolution Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hengyuan Ma, Li Zhang, Xiatian Zhu, Jingfeng Zhang, Jianfeng Feng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.04029&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-GAN: Training GANs with Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.02262&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; NeurrIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00927&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LuChengTHU/dpm-solver&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Elucidating the Design Space of Diffusion-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tero Karras, Miika Aittala, Timo Aila, Samuli Laine&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00364&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Analyzing Generative and Denoising Capabilities of Diffusion-based Deep Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kamil Deja, Anna Kuzina, Tomasz Trzciński, Jakub M. Tomczak&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00070&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-Shot Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giorgio Giannone, Didrik Nielsen, Ole Winther&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.15463&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Continuous Time Framework for Discrete Denoising Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrew Campbell, Joe Benton, Valentin De Bortoli, Tom Rainforth, George Deligiannidis, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.14987&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maximum Likelihood Training of Implicit Nonlinear Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, Il-Chul Moon&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.13699&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Models via Early Stop of the Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaoyang Lyu, Xudong XU, Ceyuan Yang, Dahua Lin, Bo Dai&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.12524&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flexible Diffusion Modeling of Long Videos&lt;/strong&gt; &lt;br&gt; &lt;em&gt;William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.11495&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/plai-group/flexible-video-diffusion-modeling&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vikram Voleti, Alexia Jolicoeur-Martineau, Christopher Pal&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.09853&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/voletiv/mcvd-pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Conditioning the Input Noise for Controlled Image Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vedant Singh, Surgan Jandial, Ayush Chopra, Siddharth Ramesh, Balaji Krishnamurthy, Vineeth N. Balasubramanian&lt;/em&gt; &lt;br&gt; CVPR Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.03859&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Subspace Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bowen Jing, Gabriele Corso, Renato Berlinghieri, Tommi Jaakkola&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.01490&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bjing2016/subspace-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Sampling of Diffusion Models with Exponential Integrator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Yongxin Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.13902&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semi-Parametric Neural Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas Müller, Björn Ommer&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.11824&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, David J. Fleet&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.03458&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Perception Prioritized Training of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, Sungroh Yoon&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.00227&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jychoi118/P2-weighting&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating High Fidelity Data from Low-density Regions using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vikash Sehwag, Caner Hazirbas, Albert Gordo, Firat Ozgenel, Cristian Canton Ferrer&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.17260&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Counterfactual Explanations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Jeanneret, Loïc Simon, Frédéric Jurie&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.15636&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Likelihood Score Matching for Conditional Score-based Data Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, Chia-Ping Chen, Chun-Yi Lee&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.14206&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Modeling for Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruihan Yang, Prakhar Srivastava, Stephan Mandt&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.09481&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/buggyyang/rvd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dynamic Dual-Output Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaniv Benny, Lior Wolf&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.04304&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Simulation Using Diffusion Schrödinger Bridges&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.13460&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Causal Models for Counterfactual Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pedro Sanchez, Sotirios A. Tsaftaris&lt;/em&gt; &lt;br&gt; PMLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.10166&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pseudo Numerical Methods for Diffusion Models on Manifolds&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luping Liu, Yi Ren, Zhijie Lin, Zhou Zhao&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/luping-liu/PNDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Truncated Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.09671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding DDPM Latent Codes Through Optimal Transport&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Valentin Khrulkov, Ivan Oseledets&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.07477&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daniel Watson, William Chan, Jonathan Ho, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.05830&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion bridges vector quantized Variational AutoEncoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max Cohen, Guillaume Quispe, Sylvain Le Corff, Charles Ollion, Eric Moulines&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.04895&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Progressive Distillation for Fast Sampling of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Salimans, Jonathan Ho&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.00512&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Chongxuan Li, Jun Zhu, Bo Zhang&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.06503&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kushagra Pandey, Avideep Mukherjee, Piyush Rai, Abhishek Kumar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.00308&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/kpandey008/DiffuseVAE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Itô-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hideyuki Tachibana, Mocho Go, Muneyoshi Inahara, Yotaro Katayama, Yotaro Watanabe&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.13339&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, Mark Chen&lt;/em&gt; &lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.10741&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/glide-text2im&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Heavy-tailed denoising score matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jacob Deasy, Nikola Simidjievski, Pietro Liò&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.09788&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High Fidelity Visualization of What Your Self-Supervised Representation Knows About&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Florian Bordes, Randall Balestriero, Pascal Vincent&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.09164&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tackling the Generative Learning Trilemma with Denoising Diffusion GANs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhisheng Xiao, Karsten Kreis, Arash Vahdat&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.07804&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/denoising-diffusion-gan&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 15 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Generative Modeling with Critically-Damped Langevin Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Dockhorn, Arash Vahdat, Karsten Kreis&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.07068&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/CLD-SGM/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;More Control for Free! Image Synthesis with Semantic Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.05744&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Global Context with Discrete Diffusion in Vector Quantised Modelling for Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minghui Hu, Yujie Wang, Tat-Jen Cham, Jianfei Yang, P.N.Suganthan&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.01799&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Autoencoders: Toward a Meaningful and Decodable Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.15640&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diff-ae.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/phizaz/diffae&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Image Generation with Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.13606&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sam Bond-Taylor, Peter Hessey, Hiroshi Sasaki, Toby P. Breckon, Chris G. Willcocks&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.12701&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/samb-t/unleashing-transformers&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Normalizing Flow&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Yongxin Chen&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.07579&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/qsh-zh/DiffFlow&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Gamma Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eliya Nachmani, Robin San Roman, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.05948&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Generative Neural Networks for Large-Scale Optimal Transport&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max Daniels, Tyler Maunu, Paul Hand&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.03237&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Generative Classifiers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roland S. Zimmermann, Lukas Schott, Yang Song, Benjamin A. Dunn, David A. Klindt&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.00473&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Classifier-Free Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Tim Salimans&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2207.12598&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Sep 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bilateral Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max W. Y. Lam, Jun Wang, Rongjie Huang, Dan Su, Dong Yu&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.11514&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bilateral-denoising-diffusion-model.github.io&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Patrick Esser, Robin Rombach, Andreas Blattmann, Björn Ommer&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2108.08827&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://compvis.github.io/imagebart/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 19 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon&lt;/em&gt; &lt;br&gt; ICCV 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2108.02938&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jychoi118/ilvr_adm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, Stefano Ermon&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2108.01073&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sde-image-editing.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/SDEdit&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Structured Denoising Diffusion Models in Discrete State-Spaces&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, Rianne van den Berg&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.03006&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Variational Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Diederik P. Kingma, Tim Salimans, Ben Poole, Jonathan Ho&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.00630&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google-research/vdm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Priors In Variational Autoencoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Antoine Wehenkel, Gilles Louppe&lt;/em&gt; &lt;br&gt; ICML Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.15671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Generative Learning via Schrödinger Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, Can Yang&lt;/em&gt; &lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.10410&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Non Gaussian Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eliya Nachmani, Robin San Roman, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.07582&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://enk100.github.io/Non-Gaussian-Denoising-Diffusion-Models/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;D2C: Diffusion-Denoising Models for Few-shot Conditional Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Abhishek Sinha, Jiaming Song, Chenlin Meng, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.06819&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://d2c-model.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/d2c-model/d2c-model.github.io&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based Generative Modeling in Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arash Vahdat, Karsten Kreis, Jan Kautz&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.05931&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning to Efficiently Sample from Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daniel Watson, Jonathan Ho, Mohammad Norouzi, William Chan&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.03802&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Variational Perspective on Diffusion-Based Generative Models and Score Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chin-Wei Huang, Jae Hyun Lim, Aaron Courville&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.02808&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CW-Huang/sdeflow-light&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Soft Truncation: A Universal Training Technique of Score-based Diffusion Model for High Precision Score Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, Il-Chul Moon&lt;/em&gt; &lt;br&gt; ICML 2022. [&lt;a href=&#34;https://arxiv.org/abs/2106.05527&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Schrödinger Bridge with Applications to Score-Based Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Valentin De Bortoli, James Thornton, Jeremy Heng, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.01357&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jtt94.github.io/papers/schrodinger_bridge&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JTT94/diffusion_schrodinger_bridge&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Fast Sampling of Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhifeng Kong, Wei Ping&lt;/em&gt; &lt;br&gt; ICML Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.00132&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/FengNiMa/FastDPM_pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cascaded Diffusion Models for High Fidelity Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, Tim Salimans&lt;/em&gt; &lt;br&gt; JMLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.15282&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cascaded-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gotta Go Fast When Generating Data with Score-Based Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexia Jolicoeur-Martineau, Ke Li, Rémi Piché-Taillefer, Tal Kachman, Ioannis Mitliagkas&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.14080&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AlexiaJM/score_sde_fast_sampling&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models Beat GANs on Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Prafulla Dhariwal, Alex Nichol&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2105.05233&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Super-Resolution via Iterative Refinement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.07636&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://iterative-refinement.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Apr 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Noise Estimation for Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin San-Roman, Eliya Nachmani, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.02600&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Nichol, Prafulla Dhariwal&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.09672&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/improved-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Feb 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Maximum Likelihood Training of Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Conor Durkan, Iain Murray, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.09258&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jan 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eric Luhman, Troy Luhman&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2101.02388&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tcl9876/Denoising_Student&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Jan 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Energy-Based Models by Diffusion Recovery Likelihood&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, Diederik P. Kingma&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2012.08125&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ruiqigao/recovery_likelihood&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Dec 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Generative Modeling through Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole&lt;/em&gt; &lt;br&gt; ICLR 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2011.13456&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yang-song/score_sde&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Nov 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Kun Xu, Chongxuan Li, Lanqing Hong, Jun Zhu, Bo Zhang&lt;/em&gt; &lt;br&gt; ICML 2021. [&lt;a href=&#34;https://arxiv.org/abs/2010.08258&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Implicit Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaming Song, Chenlin Meng, Stefano Ermon&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2010.02502&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/ddim&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Oct 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial score matching and improved sampling for image generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexia Jolicoeur-Martineau, Rémi Piché-Taillefer, Rémi Tachet des Combes, Ioannis Mitliagkas&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2009.05475&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AlexiaJM/AdversarialConsistentScoreMatching&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Sep 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Ajay Jain, Pieter Abbeel&lt;/em&gt; &lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.11239&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hojonathanho/diffusion&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/pesser/pytorch_diffusion&#34;&gt;Github2&lt;/a&gt;] &lt;br&gt; 19 Jun 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Techniques for Training Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS 2020. [&lt;a href=&#34;https://arxiv.org/abs/2006.09011&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/ncsnv2&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Jun 2020&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Modeling by Estimating Gradients of the Data Distribution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS 2019. [&lt;a href=&#34;https://arxiv.org/abs/1907.05600&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yang-song.github.io/blog/2021/score/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ermongroup/ncsn&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Jul 2019&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Belinda Tzen, Maxim Raginsky&lt;/em&gt; &lt;br&gt; arXiv 2019. [&lt;a href=&#34;https://arxiv.org/abs/1905.09883&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2019&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Unsupervised Learning using Nonequilibrium Thermodynamics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, Surya Ganguli&lt;/em&gt; &lt;br&gt; ICML 2015. [&lt;a href=&#34;https://arxiv.org/abs/1503.03585&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Sohl-Dickstein/Diffusion-Probabilistic-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Mar 2015&lt;/p&gt; &#xA;&lt;h3&gt;Classification&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Likelihood-based Out-of-Distribution Detection with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joseph Goodier, Neill D. F. Campbell&lt;/em&gt; &lt;br&gt; BMVC 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17432&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-scale Diffusion Denoised Smoothing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jongheon Jeong, Jinwoo Shin&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16779&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Se-Ho Kim, Inyong Koo, Inyoung Lee, Byeongjun Park, Changick Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16349&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Task Routing for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, Changick Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07138&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Leveraging Diffusion-Based Image Variations for Robust Training on Poisoned Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lukas Struppek, Martin B. Hentschel, Clifton Poth, Dominik Hintersdorf, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06372&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LukasStruppek/Robust_Training_on_Poisoned_Samples&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dream the Impossible: Outlier Imagination with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuefeng Du, Yiyou Sun, Xiaojin Zhu, Yixuan Li&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.13415&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/deeplearning-wisc/dream-ood&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Object Counting with Language-Vision Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyi Xu, Hieu Le, Dimitris Samaras&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.13097&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cvlab-stonybrook/zero-shot-counting&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chengyou Jia, Minnan Luo, Zhuohang Dang, Guang Dai, Xiaojun Chang, Jingdong Wang, Qinghua Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11125&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Laurent Itti, Vibhav Vineet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05956&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gyhandy/Text2Image-for-Detection&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Manlin Zhang, Jie Wu, Yuxi Ren, Ming Li, Jie Qin, Xuefeng Xiao, Wei Liu, Rui Wang, Min Zheng, Andy J. Ma&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03893&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mettyz.github.io/DiffusionEngine/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bytedance/DiffusionEngine&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based 3D Object Detection with Random Boxes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Zhou, Jinghua Hou, Tingting Yao, Dingkang Liang, Zhe Liu, Zhikang Zou, Xiaoqing Ye, Jianwei Cheng, Xiang Bai&lt;/em&gt; &lt;br&gt; PRCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.02049&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model as Representation Learner&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingyi Yang, Xinchao Wang&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionTrack: Diffusion Model For Multi-Object Tracking&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Run Luo, Zikai Song, Lintao Ma, Jinlin Wei, Wei Yang, Min Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09905&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-trained Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruiyuan Gao, Chenchen Zhao, Lanqing Hong, Qiang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07687&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fadi Boutros, Jonas Henry Grebe, Arjan Kuijper, Naser Damer&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.04995&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, Nayeong Kim, Suha Kwak, Tae-Hyun Oh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.00994&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model for Camouflaged Object Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhennan Chen, Rongrong Gao, Tian-Zhu Xiang, Fan Lin&lt;/em&gt; &lt;br&gt; ECAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.00303&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Runyang Feng, Yixing Gao, Tze Ho Elden Tse, Xueqing Ma, Hyung Jin Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.16687&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Baoquan Zhang, Demin Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.16424&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Prompt Model for Weakly Supervised Object Localization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuzhong Zhao, Qixiang Ye, Weijia Wu, Chunhua Shen, Fang Wan&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.09756&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/callsys/GenPromp&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models Beat GANs on Image Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Soumik Mukhopadhyay, Matthew Gwilliam, Vatsal Agarwal, Namitha Padmanabhan, Archana Swaminathan, Srinidhi Hegde, Tianyi Zhou, Abhinav Shrivastava&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08702&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion to Confusion: Naturalistic Adversarial Patch Generation Based on Diffusion Model for Object Detector&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuo-Yen Lin, Ernie Chu, Che-Hsien Lin, Jun-Cheng Chen, Jia-Ching Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08076&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTeacher: Pretraining Image Backbones with Deep Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Torralba, Sanja Fidler&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.07487&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/DreamTeacher/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProtoDiff: Learning to Learn Prototypical Networks by Task-Guided Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yingjun Du, Zehao Xiao, Shengcai Liao, Cees Snoek&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14770&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Masked Diffusion Models are Fast Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiachen Lei, Peng Cheng, Zhongjie Ba, Kui Ren&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11363&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Renderers are Good Zero-Shot Representation Learners: Exploring Diffusion Latents for Metric Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michael Tang, David Shustin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10721&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Big Data Myth: Using Diffusion Models for Dataset Generation to Train Deep Detection Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roy Voetman, Maya Aghaei, Klaas Dijkstra&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09762&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;When Hyperspectral Image Classification Meets Diffusion Models: An Unsupervised Feature Learning Framework&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyi Zhou, Jiamu Sheng, Jiayuan Fan, Peng Ye, Tong He, Bin Wang, Tao Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08964&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic Latent Particles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tal Daniel, Aviv Tamar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05957&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ADDP: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changyao Tian, Chenxin Tao, Jifeng Dai, Hao Li, Ziheng Li, Lewei Lu, Xiaogang Wang, Hongsheng Li, Gao Huang, Xizhou Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05423&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Generation from Unconditional Diffusion Models using Denoiser Representations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Graikos, Srikar Yellapragada, Dimitris Samaras&lt;/em&gt; &lt;br&gt; BMVC 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01900&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cvlab-stonybrook/fewshot-conditional-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, Xinxiao Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15957&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training on Thin Air: Improve Image Classification with Generated Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yongchao Zhou, Hshmat Sahak, Jimmy Ba&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15316&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/diffusion-inversion&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yongchao97/diffusion_inversion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheng Li, Yuxuan Li, Penghai Zhao, Renjie Song, Xiang Li, Jian Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12954&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhengli97/DM-KD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Human-Object Interaction Detection with Text-to-Image Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jie Yang, Bingliang Li, Fengyu Yang, Ailing Zeng, Lei Zhang, Ruimao Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12252&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Meta-DM: Applications of Diffusion Models on Few-Shot Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wentao Hu, Xiurong Jiang, Jiarun Liu, Yuqi Yang, Hui Tian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08092&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Class-Balancing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, Ya Zhang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00562&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic Data from Diffusion Models Improves ImageNet Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, David J. Fleet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08466&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OVTrack: Open-Vocabulary Multiple Object Tracking&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siyuan Li, Tobias Fischer, Lei Ke, Henghui Ding, Martin Danelljan, Fisher Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Your Diffusion Model is Secretly a Zero-Shot Classifier&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexander C. Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, Deepak Pathak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16203&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusion-classifier.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Image Diffusion Models are Zero-Shot Classifiers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kevin Clark, Priyank Jaini&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15233&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nicola Franco, Daniel Korth, Jeanette Miriam Lorenz, Karsten Roscher, Stephan Guennemann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14961&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jordan J. Bird, Ahmad Lotfi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14126&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Autoencoders are Unified Self-supervised Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weilai Xiang, Hongyu Yang, Di Huang, Yunhong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09769&#34;&gt;Paper&lt;/a&gt;] )] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Zero-shot Classification with Synthetic Data Diversity via Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, Clinton Fookes&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03298&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fake it till you make it: Learning(s) from a synthetic ImageNet clone&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, Yannis Kalantidis&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.08420&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://europe.naverlabs.com/research/computer-vision/imagenet-sd/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffAlign : Few-shot learning using diffusion based synthesis and alignment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aniket Roy, Anshul Shah, Ketul Shah, Anirban Roy, Rama Chellappa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.05404&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Denoising Process for Perceptron Bias in Out-of-distribution Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luping Liu, Yi Ren, Xize Cheng, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11255&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/luping-liu/DiffOOD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionDet: Diffusion Model for Object Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shoufa Chen, Peize Sun, Yibing Song, Ping Luo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09788&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ShoufaChen/DiffusionDet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Models for Out-of-Distribution Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mark S. Graham, Walter H.L. Pinaya, Petru-Daniel Tudosiu, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.07740&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/marksgraham/ddpm-ood&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A simple, efficient and scalable contrastive masked autoencoder for learning visual representations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shlok Mishra, Joshua Robinson, Huiwen Chang, David Jacobs, Aaron Sarna, Aaron Maschinot, Dilip Krishnan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.16870&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Points to Functions: Infinite-dimensional Representations in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sarthak Mittal, Guillaume Lajoie, Stefan Bauer, Arash Mehrjou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.13774&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sarthmit/traj_drl&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boomerang: Local sampling on image manifolds using diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lorenzo Luzi, Ali Siahkoohi, Paul M Mayer, Josue Casco-Rodriguez, Richard Baraniuk&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12100&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/drive/1PV5Z6b14HYZNx1lHCaEVhId-Y4baKXwt&#34;&gt;Colab&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Meta-Learning via Classifier(-free) Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elvis Nava, Seijin Kobayashi, Yifei Yin, Robert K. Katzschmann, Benjamin F. Grewe&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.08942&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2022&lt;/p&gt; &#xA;&lt;h3&gt;Segmentation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;One-shot Localization and Segmentation of Medical Images with Foundation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Deepa Anand, Gurunath Reddy M, Vanika Singhal, Dattesh D. Shanbhag, Shriram KS, Uday Patil, Chitresh Bhushan, Kavitha Manickam, Dawei Gui, Rakesh Mullick, Avinash Gopal, Parminder Bhatia, Taha Kass-Hout&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.18642&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic-preserving image coding based on Conditional Diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Francesco Pezone, Osman Musa, Giuseppe Caire, Sergio Barbarossa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.15737&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Data Augmentation for Nuclei Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinyi Yu, Guanbin Li, Wei Lou, Siqi Liu, Xiang Wan, Yan Chen, Haofeng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.14197&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EMIT-Diff: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheyuan Zhang, Lanhong Yao, Bin Wang, Debesh Jha, Elif Keles, Alpay Medetalibeyoglu, Ulas Bagci&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12868&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Training-free Open-world Segmentation via Image Prompting Foundation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lv Tang, Peng-Tao Jiang, Hao-Ke Xiao, Bo Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10912&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haonan Wang, Xiaomeng Li&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11320&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xmed-lab/GenericSSL&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Augmentation with Controlled Diffusion for Weakly-Supervised Semantic Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wangyu Wu, Tianhong Dai, Xiaowei Huang, Fei Ma, Jimin Xiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09760&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Anoop Cherian, Suhas Lohit, Ye Wang, Toshiaki Koike-Akino, Vishal M. Patel, Tim K. Marks&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00224&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Factorized Diffusion Architectures for Unsupervised Image Generation and Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Yuan, Michael Maire&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.15726&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dataset Diffusion: Diffusion-based Synthetic Dataset Generation for Pixel-Level Semantic Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Quang Nguyen, Truong Vu, Anh Tran, Khoi Nguyen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14303&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, Yew Soon Ong, Chen Change Loy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.13042&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Jiahao000/MosaicFusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunhao Ge, Jiashu Xu, Brian Nlong Zhao, Neel Joshi, Laurent Itti, Vibhav Vineet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05956&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gyhandy/Text2Image-for-Detection&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Introducing Shape Prior Module in Diffusion Model for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiqing Zhang, Guojia Fan, Tianyong Liu, Nan Li, Yuyang Liu, Ziyu Liu, Canwei Dong, Shoujun Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05929&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changming Xiao, Qi Yang, Feng Zhou, Changshui Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.04109&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SLiMe: Segment Like Me&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aliasghar Khani, Saeid Asgari Taghanaki, Aditya Sanghi, Ali Mahdavi Amiri, Ghassan Hamarneh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03179&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/aliasgharkhani/SLiMe&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinglong Wang, Xiawei Li, Jing Zhang, Qingyuan Xu, Qin Zhou, Qian Yu, Lu Sheng, Dong Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.02773&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GenSelfDiff-HIS: Generative Self-Supervision Using Diffusion for Histopathological Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vishnuvardhan Purma, Suhas Srinath, Seshan Srirangarajan, Aanchal Kakkar, Prathosh A. P&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01487&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PurmaVishnuVardhanReddy/GenSelfDiff-HIS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attention as Annotation: Generating Images and Pseudo-masks for Weakly Supervised Semantic Segmentation with Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryota Yoshihashi, Yuya Otsuka, Kenji Doi, Tomohiro Tanaka&lt;/em&gt; &lt;br&gt; AAAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2309.01369&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ArSDM: Colonoscopy Images Synthesis with Adaptive Refinement Semantic Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuhao Du, Yuncheng Jiang, Shuangyi Tan, Xusheng Wu, Qi Dou, Zhen Li, Guanbin Li, Xiang Wan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01111&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minheng Ni, Yabo Zhang, Kailai Feng, Xiaoming Li, Yiwen Guo, Wangmeng Zuo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16777&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyun Liang, Harry Anthony, Felix Wagner, Konstantinos Kamnitsas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16150&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Recycling Training Strategy for Medical Image Segmentation with Diffusion Denoising Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunguan Fu, Yiwen Li, Shaheer U Saeed, Matthew J Clarkson, Yipeng Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16355&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mathpluscode/ImgX-DiffSeg&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, Mar Gonzalez-Franco&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.12469&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Duo Peng, Ping Hu, Qiuhong Ke, Jun Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.12350&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoxiao He, Chaowei Tan, Ligong Han, Bo Liu, Leon Axel, Kang Li, Dimitris N. Metaxas&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09223&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hexiaoxiao-cs/DMCVR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Masked Diffusion as Self-supervised Representation Learner&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixuan Pan, Jianxu Chen, Yiyu Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.05695&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Afshin Bozorgpour, Yousef Sadegheih, Amirhossein Kazerouni, Reza Azad, Dorit Merhof&lt;/em&gt; &lt;br&gt; MICCAI Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02959&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mindflow-institue/dermosegdiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusePast: Diffusion-based Generative Replay for Class Incremental Semantic Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingfan Chen, Yuxi Wang, Pengfei Wang, Xiao Chen, Zhaoxiang Zhang, Zhen Lei, Qing Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01127&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, Chenliang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.00122&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pre-Training with Diffusion models for Dental Radiography segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jérémy Rousseau, Christian Alaka, Emma Covili, Hippolyte Mayard, Laura Misrachi, Willy Au&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.14066&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FEDD -- Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation and Malignancy Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Héctor Carrión, Narges Norouzi&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.11654&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hectorcarrion/fedd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTeacher: Pretraining Image Backbones with Deep Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Torralba, Sanja Fidler&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.07487&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/DreamTeacher/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompting Diffusion Representations for Cross-Domain Semantic Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rui Gong, Martin Danelljan, Han Sun, Julio Delgado Mangas, Luc Van Gool&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.02138&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DifFSS: Diffusion Model for Few-Shot Semantic Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weimin Tan, Siyuan Chen, Bo Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.00773&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Better Certified Segmentation via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Othmane Laousy, Alexandre Araujo, Guillaume Chassagnon, Marie-Pierre Revel, Siddharth Garg, Farshad Khorrami, Maria Vakalopoulou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Zero-Shot Open-Vocabulary Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Laurynas Karazija, Iro Laina, Andrea Vedaldi, Christian Rupprecht&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tomer Amit, Shmuel Shichrur, Tal Shaharabany, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09004&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Semantic Communication: Diffusion Models Beyond Bit Recovery&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eleonora Grassucci, Sergio Barbarossa, Danilo Comminiello&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04321&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ispamm/GESCO&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinrong Hu, Yu-Jen Chen, Tsung-Yi Ho, Yiyu Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03878&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DFormer: Diffusion-guided Transformer for Universal Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hefeng Wang, Jiale Cao, Rao Muhammad Anwer, Jin Xie, Fahad Shahbaz Khan, Yanwei Pang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03437&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cp3wan/DFormer&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Semantic Segmentation with Mask Prior Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeqiang Lai, Yuchen Duan, Jifeng Dai, Ziheng Li, Ying Fu, Hongsheng Li, Yu Qiao, Wenhai Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01721&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Level Global Context Cross Consistency Model for Semi-Supervised Ultrasound Image Segmentation with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fenghe Tang, Jianrui Ding, Lingtao Wang, Min Xian, Chunping Ning&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09447&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/FengheTan9/Multi-Level-Global-Context-Cross-Consistency&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Stojanovski, Uxio Hermida, Pablo Lamata, Arian Beqiri, Alberto Gomez&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05424&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Personalize Segment Anything Model with One Shot&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZrrSkywalker/Personalize-SAM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Personalize Segment Anything Model with One Shot&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZrrSkywalker/Personalize-SAM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nurislam Tursynbek, Marc Niethammer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00067&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseExpand: Expanding dataset for 2D medical image segmentation using diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Shao, Xiaohan Yuan, Zhen Huang, Ziming Qiu, Shuai Wang, Kevin Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13416&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://anonymous.4open.science/r/DiffuseExpand/README.md&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Realistic Data Enrichment for Robust Image Segmentation in Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sarah Cechnicka, James Ball, Callum Arthurs, Candice Roufosse, Bernhard Kainz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09534&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Medical Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pham Ngoc Huy, Tran Minh Quan&lt;/em&gt; &lt;br&gt; IEEE ISBI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09383&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ambiguous Medical Image Segmentation using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aimon Rahman, Jeya Maria Jose Valanarasu, Ilker Hacihaliloglu, Vishal M Patel&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04745&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/aimansnigdha/Ambiguous-Medical-Image-Segmentation-using-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Chen, Chenhui Wang, Hongming Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04429&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haipeng Zhou, Lei Zhu, Yuyin Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12313&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral Fracture Grading&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthias Keicher, Matan Atad, David Schinz, Alexandra S. Gersing, Sarah C. Foreman, Sophia S. Goller, Juergen Weissinger, Jon Rischewski, Anna-Sophia Dietrich, Benedikt Wiestler, Jan S. Kirschke, Nassir Navab&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12031&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LD-ZNet: A Latent Diffusion Approach for Text-Based Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Koutilya Pnvr, Bharat Singh, Pallabi Ghosh, Behjat Siddiquie, David Jacobs&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12343&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, Chunhua Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11681&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://weijiawu.github.io/DiffusionMask/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Object-Centric Slot Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jindong Jiang, Fei Deng, Gautam Singh, Sungjin Ahn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10834&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-UNet: A Diffusion Embedded Network for Volumetric Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaohu Xing, Liang Wan, Huazhu Fu, Guang Yang, Lei Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10326&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ge-xing/Diff-UNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionSeg: Adapting Diffusion Towards Unsupervised Object Discovery&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaofan Ma, Yuhuan Yang, Chen Ju, Fei Zhang, Jinxiang Liu, Yu Wang, Ya Zhang, Yanfeng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09813&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stochastic Segmentation with Conditional Categorical Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lukas Zbinden, Lars Doorenbos, Theodoros Pissas, Raphael Sznitman, Pablo Márquez-Neila&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08888&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LarsDoorenbos/ccdm-stochastic-segmentation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffBEV: Conditional Diffusion Model for Bird&#39;s Eye View Perception&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiayu Zou, Zheng Zhu, Yun Ye, Xingang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08333&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Importance of Aligning Training Strategy with Evaluation for Diffusion Models in 3D Multiclass Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunguan Fu, Yiwen Li, Shaheer U. Saeed, Matthew J. Clarkson, Yipeng Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06040&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mathpluscode/ImgX-DiffSeg&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model for Few-Shot Instance Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minh-Quan Le, Tam V. Nguyen, Trung-Nghia Le, Thanh-Toan Do, Minh N. Do, Minh-Triet Tran&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05105&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, Shalini De Mello&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04803&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jerryxu.net/ODISE/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, Yanwu Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11798&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionInst: Diffusion Model for Instance Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhangxuan Gu, Haoxing Chen, Zhuoer Xu, Jun Lan, Changhua Meng, Weiqiang Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02773&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chenhaoxing/DiffusionInst&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 DEc 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Class Segmentation from Aerial Views using Recursive Noise Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Benedikt Kolbeinsson, Krystian Mikolajczyk&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00787&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Peekaboo: Text to Image Diffusion Models are Zero-Shot Segmentors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryan Burgert, Kanchana Ranasinghe, Xiang Li, Michael S. Ryoo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13224&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved HER2 Tumor Segmentation with Subtype Balancing using Deep Generative Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mathias Öttl, Jana Mönius, Matthias Rübner, Carol I. Geppert, Jingna Qiu, Frauke Wilm, Arndt Hartmann, Matthias W. Beckmann, Peter A. Fasching, Andreas Maier, Ramona Erber, Katharina Breininger&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06150&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junde Wu, Huihui Fang, Yu Zhang, Yehui Yang, Yanwu Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00611&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Models via Pre-segmentation Diffusion Sampling for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xutao Guo, Yanwu Yang, Chenfei Ye, Shang Lu, Yang Xiang, Ting Ma&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.17408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anatomically constrained CT image translation for heterogeneous blood vessel segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giammarco La Barbera, Haithem Boussaid, Francesco Maso, Sabine Sarnacki, Laurence Rouet, Pietro Gori, Isabelle Bloch&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.01713&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boah Kim, Yujin Oh, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14566&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can segmentation models be trained with fully synthetically generated data?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Virginia Fernandez, Walter Hugo Lopez Pinaya, Pedro Borges, Petru-Daniel Tudosiu, Mark S Graham, Tom Vercauteren, M Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08256&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let us Build Bridges: Understanding and Extending Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchao Liu, Lemeng Wu, Mao Ye, Qiang Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.14699&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Image Synthesis via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.00050&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Remote Sensing Change Detection (Segmentation) using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wele Gedara Chaminda Bandara, Nithin Gopalakrishnan Nair, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.11892&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wgcban/ddpm-cd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models as plug-and-play priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, Dimitris Samaras&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.09012&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Walter H. L. Pinaya, Mark S. Graham, Robert Gray, Pedro F Da Costa, Petru-Daniel Tudosiu, Paul Wright, Yee H. Mah, Andrew D. MacKinnon, James T. Teo, Rolf Jager, David Werring, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardos&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.03461&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Decoder Denoising Pretraining for Semantic Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emmanuel Brempong Asiedu, Simon Kornblith, Ting Chen, Niki Parmar, Matthias Minderer, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.11423&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Implicit Image Segmentation Ensembles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe Valmaggia, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; MIDL 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.03145&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Label-Efficient Semantic Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, Artem Babenko&lt;/em&gt; &lt;br&gt; ICLR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.03126&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yandex-research/ddpm-segmentation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SegDiff: Image Segmentation with Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tomer Amit, Eliya Nachmani, Tal Shaharbany, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.00390&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, Max Welling&lt;/em&gt; &lt;br&gt; NeurIPS 2021. [&lt;a href=&#34;https://arxiv.org/abs/2102.05379&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2021&lt;/p&gt; &#xA;&lt;h3&gt;Image Translation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Diffusion Counterfactual Explanations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Karim Farid, Simon Schrodi, Max Argus, Thomas Brox&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06668&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Teng Hu, Jiangning Zhang, Liang Liu, Ran Yi, Siqi Kou, Haokun Zhu, Xu Chen, Yabiao Wang, Chengjie Wang, Lizhuang Ma&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03729&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Painter&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shih-Chieh Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16490&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhanbo Feng, Zenan Ling, Ci Gong, Feng Zhou, Jie Li, Robert C. Qiu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15854&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffI2I: Efficient Diffusion Model for Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, Radu Timotfe, Luc Van Gool&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13767&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form Layout-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chengyou Jia, Minnan Luo, Zhuohang Dang, Guang Dai, Xiaojun Chang, Mengmeng Wang, Jingdong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10156&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ernie Chu, Tzuhsuan Huang, Shuo-Yen Lin, Jun-Cheng Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10079&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://medm2023.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 19 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhizhong Wang, Lei Zhao, Wei Xing&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07863&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via Stochastic Differential Equations without Training&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ximing Xing, Chuang Wang, Haitao Zhou, Zhihao Hu, Chongxuan Li, Dong Xu, Qian Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07665&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming the Power of Diffusion Models for High-Quality Virtual Try-On with Appearance Flow&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, Liqing Zhang&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06101&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Head Rotation in Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrea Asperti, Gabriele Colasuonno, Antonio Guerra&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06057&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Photorealistic and Identity-Preserving Image-Based Emotion Manipulation with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ioannis Pikoulis, Panagiotis P. Filntisis, Petros Maragos&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.03183&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDDM: Score-Decomposed Diffusion Models on Manifolds for Unpaired Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shikun Sun, Longhui Wei, Junliang Xing, Jia Jia, Qi Tian&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02154&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpolating between Images with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Clinton J. Wang, Polina Golland&lt;/em&gt; &lt;br&gt; ICML Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.12560&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://clintonjwang.github.io/interpolation&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/clintonjwang/ControlNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shilin Lu, Yanzhu Liu, Adams Wai-Kin Kong&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.12493&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Shilin-LU/TF-ICON&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseGAE: Controllable and High-fidelity Image Manipulation from Disentangled Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yipeng Leng, Qiangjuan Huang, Zhiyuan Wang, Yangyang Liu, Haoyu Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.05899&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dan Ruta, Gemma Canet Tarrés, Andrew Gilbert, Eli Shechtman, Nicholas Kolkin, John Collomosse&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.04157&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Applying a Color Palette with Local Control using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vaibhav Vavilala, David Forsyth&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.02698&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.02421&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mc-e.github.io/project/DragonDiffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent Y. F. Tan, Song Bai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14435&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ArtFusion: Controllable Arbitrary Style Transfer using Dual Conditional Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dar-Yen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09330&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenDarYen/ArtFusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yingheng Wang, Yair Schiff, Aaron Gokaslan, Weishen Pan, Fei Wang, Christopher De Sa, Volodymyr Kuleshov&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08757&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TryOnDiffusion: A Tale of Two UNets&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, Ira Kemelmacher-Shlizerman&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08276&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04396&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSketching: Sketch Control Image Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiang Wang, Di Kong, Fengyin Lin, Yonggang Qi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18812&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real-World Image Variation by Aligning Diffusion Inversion Chain&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuechen Zhang, Jinbo Xing, Eric Lo, Jiaya Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18729&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Photoswap: Personalized Subject Swapping in Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, Xin Eric Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18286&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://photoswap.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16289&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lisadunlap/ALIA&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unpaired Image-to-Image Translation via Neural Schrödinger Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Beomsu Kim, Gihyun Kwon, Kwanyoung Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15086&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cyclomon/UNSB&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SAR-to-Optical Image Translation via Thermodynamics-inspired Network&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingjin Zhang, Jiamin Xu, Chengyu He, Wenteng Shang, Yunsong Li, Xinbo Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13839&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Huang, Wenjing Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06710&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nulltextforcartoon.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NullTextforCartoon/NullTextforCartoon&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yupei Lin, Sen Zhang, Xiaojun Yang, Xiao Wang, Yukai Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04651&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yupeilin2388.github.io/publication/ReDiffuser&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchical Diffusion Autoencoders and Disentangled Image Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeyu Lu, Chengyue Wu, Xinyuan Chen, Yaohui Wang, Yu Qiao, Xihui Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11829&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionRig: Learning Personalized Priors for Facial Appearance Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheng Ding, Xuaner Zhang, Zhihao Xia, Lars Jebe, Zhuowen Tu, Xiuming Zhang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06711&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusionrig.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/adobe-research/diffusion-rig&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Animation with an Attribute-Guided Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bohan Zeng, Xuhui Liu, Sicheng Gao, Boyu Liu, Hong Li, Jianzhuang Liu, Baochang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03199&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reference-based Image Composition with Sketch via Structure-aware Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kangyeol Kim, Sunghyun Park, Junsoo Lee, Jaegul Choo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09748&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-free Style Transfer Emerges from h-space in Diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaeseok Jeong, Mingi Kwon, Youngjung Uh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15403&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://curryjung.github.io/DiffStyle/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/curryjung/DiffStyle_official&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Target Sampler for Unsupervised Domain Adaptation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yulong Zhang, Shuhao Chen, Yu Zhang, Jiangang Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12724&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyO: Stylize Your Face in Only One-Shot&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bonan Li, Zicheng Zhang, Xuecheng Nie, Congying Han, Yinhan Hu, Tiande Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03231&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFashion: Reference-based Fashion Design with Structure-aware Transfer by Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shidong Cao, Wenhao Chai, Shengyu Hao, Yanting Zhang, Hangyue Chen, Gaoang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.06826&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I2SB: Image-to-Image Schrödinger Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos A. Theodorou, Weili Nie, Anima Anandkumar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05872&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://i2sb.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zihao Wang, Yingyu Yang, Maxime Sermesant, Hervé Delingette, Ona Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13743&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFace: Diffusion-based Face Swapping with Facial Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kihong Kim, Yunho Kim, Seokju Cho, Junyoung Seo, Jisu Nam, Kychul Lee, Seungryong Kim, KwangHee Lee&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.13344&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://hxngiee.github.io/DiffFace/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 27 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HS-Diffusion: Learning a Semantic-Guided Diffusion Model for Head Swapping&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinghe Wang, Lijie Liu, Miao Hua, Qian He, Pengfei Zhu, Bing Cao, Qinghua Hu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06458&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inversion-Based Creativity Transfer with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, Changsheng Xu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.13203&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zyxElsa/InST&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Person Image Synthesis via Denoising Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ankan Kumar Bhunia, Salman Khan, Hisham Cholakkal, Rao Muhammad Anwer, Jorma Laaksonen, Mubarak Shah, Fahad Shahbaz Khan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12500&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Diffusion Models&#39; Latent Space, with Applications to CycleDiffusion and Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Henry Wu, Fernando De la Torre&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.05559&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/cycle-diffusion&#34;&gt;Github-1&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/unified-generative-zoo&#34;&gt;Github-2&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anatomically constrained CT image translation for heterogeneous blood vessel segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giammarco La Barbera, Haithem Boussaid, Francesco Maso, Sabine Sarnacki, Laurence Rouet, Pietro Gori, Isabelle Bloch&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.01713&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Image Translation using Disentangled Style and Content Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.15264&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MIDMs: Matching Interleaved Diffusion Models for Exemplar-based Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyoung Seo, Gyuseong Lee, Seokju Cho, Jiyoung Lee, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.11047&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ku-cvlab.github.io/MIDMs/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ozan Özdenizci, Robert Legenstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.14626&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Non-Uniform Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.09786&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Medical Image Translation with Adversarial Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muzaffer Özbey, Salman UH Dar, Hasan A Bedel, Onat Dalmaz, Şaban Özturk, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08208&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EGSDE: Unpaired Image-to-Image Translation via Energy-Guided Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Min Zhao, Fan Bao, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.06635&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, Yan Yan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.07771&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/L-YeZhu/CDCD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pretraining is All You Need for Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong Chen, Qifeng Chen, Fang Wen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.12952&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tengfei-wang.github.io/PITI/index.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PITI-Synthesis/PITI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VQBB: Image-to-image Translation with Vector Quantized Brownian Bridge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Li, Kaitao Xue, Bin Liu, Yu-Kun Lai&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.07680&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.02641&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dual Diffusion Implicit Bridges for Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Su, Jiaming Song, Chenlin Meng, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.08382&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Restoration Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Michael Elad, Stefano Ermon, Jiaming Song&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.11793&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseMorph: Unsupervised Deformable Image Registration Along Continuous Trajectory Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boah Kim, Inhwa Han, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.05149&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Autoencoders: Toward a Meaningful and Decodable Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.15640&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diff-ae.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Image Generation with Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.13606&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon&lt;/em&gt; &lt;br&gt; ICCV 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2108.02938&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jychoi118/ilvr_adm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UNIT-DDPM: UNpaired Image Translation with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hiroshi Sasaki, Chris G. Willcocks, Toby P. Breckon&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.05358&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2021&lt;/p&gt; &#xA;&lt;h3&gt;Inverse Problems&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;EDiffSR: An Efficient Diffusion Probabilistic Model for Remote Sensing Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Xiao, Qiangqiang Yuan, Kui Jiang, Jiang He, Xianyu Jin, Liangpei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19288&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Global Structure-Aware Diffusion Process for Low-Light Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinhui Hou, Zhiyu Zhu, Junhui Hou, Hui Liu, Huanqiang Zeng, Hui Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17577&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Posterior Sampling to Meaningful Diversity in Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Noa Cohen, Hila Manor, Yuval Bahat, Tomer Michaeli&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16047&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Model-Assisted Supervised Learning of Generative Models for Density Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yanfang Liu, Minglei Yang, Zezhong Zhang, Feng Bao, Yanzhao Cao, Guannan Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.14458&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Quality 3D Face Reconstruction with Affine Convolutional Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiqian Lin, Jiangke Lin, Lincheng Li, Yi Yuan, Zhengxia Zou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.14237&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Super-resolution Via Latent Diffusion: A Sampling-space Mixture Of Experts And Frequency-augmented Decoder Approach&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Feng Luo, Jinxi Xiang, Jun Zhang, Xiao Han, Wei Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12004&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards image compression with perfect realism at ultra-low bitrates&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marlène Careil, Matthew J. Muckley, Jakob Verbeek, Stéphane Lathuilière&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10325&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yitong Jiang, Zhaoyang Zhang, Tianfan Xue, Jinwei Gu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10123&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring the Design Space of Diffusion Autoencoders for Face Morphing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zander Blasingame, Chen Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09484&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Prior Regularized Iterative Reconstruction for Low-dose CT&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Yongyi Shi, Chuang Niu, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Batu Ozturkler, Chao Liu, Benjamin Eckart, Morteza Mardani, Jiaming Song, Jan Kautz&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01799&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/SMRD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kangfu Mei, Mauricio Delbracio, Hossein Talebi, Zhengzhong Tu, Vishal M. Patel, Peyman Milanfar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01407&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CommIN: Semantic Image Communications as an Inverse Problem with INN-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiakang Chen, Di You, Deniz Gündüz, Pier Luigi Dragotti&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01130&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt-tuning latent diffusion models for inverse problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jong Chul Ye, Peyman Milanfar, Mauricio Delbracio&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01110&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Anoop Cherian, Suhas Lohit, Ye Wang, Toshiaki Koike-Akino, Vishal M. Patel, Tim K. Marks&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00224&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Visual Scenes from Touch&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fengyu Yang, Jiacheng Zhang, Andrew Owens&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.15117&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fredfyyang.github.io/vision-from-touch/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiancheng Huang, Yifan Liu, Shifeng Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14709&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tsiry Mayet, Simon Bernard, Clement Chatelain, Romain Herault&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14394&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Domain-Guided Conditional Diffusion Model for Unsupervised Domain Adaptation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yulong Zhang, Shuhao Chen, Weisen Jiang, Yu Zhang, Jiangang Lu, James T. Kwok&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14360&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;License Plate Super-Resolution Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sawsan AlHalawani, Bilel Benjdira, Adel Ammar, Anis Koubaa, Anas M. Ali&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.12506&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow removal&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiao Feng Zhang, Tian Yi Song, Jia Wei Yao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11715&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Aging via Diffusion-based Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangyi Chen, Stéphane Lathuilière&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11321&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PGDiff: Guiding Diffusion Models for Versatile Face Restoration via Partial Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peiqing Yang, Shangchen Zhou, Qingyi Tao, Chen Change Loy&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.10810&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/pq-yang/PGDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reconstruct-and-Generate Diffusion Model for Detail-Preserving Image Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yujin Wang, Lingen Li, Tianfan Xue, Jinwei Gu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.10714&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gradpaint: Gradient-Guided Inpainting with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Asya Grechka, Guillaume Couairon, Matthieu Cord&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.09614&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AdBooster: Personalized Ad Creative Generation using Stable Diffusion Outpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Veronika Shilova, Ludovic Dos Santos, Flavian Vasile, Gaëtan Racic, Ugo Tanielian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11507&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Underwater Image Enhancement by Transformer-based Diffusion Model with Non-uniform Sampling for Skip Strategy&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Tang, Takafumi Iwaguchi, Hiroshi Kawasaki&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/piggy2009/DM_underwater&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Bayesian Computational Imaging with a Surrogate Score-Based Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Berthy T. Feng, Katherine L. Bouman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Modeling with Domain-conditioned Prior Guidance for Accelerated MRI and qMRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wanyu Bian, Albert Jang, Fang Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00783&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Correlated and Multi-frequency Diffusion Modeling for Highly Under-sampled MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Guan, Chuanming Yu, Shiyu Lu, Zhuoxu Cui, Dong Liang, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00853&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yqx7150/CM-DM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Diffusion EM: a diffusion model for blind inverse problems with application to deconvolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Charles Laroche, Andrés Almansa, Eva Coupete&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00287&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://anonymous.4open.science/r/FastDiffusionEM-26BE/README.md&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised CT Metal Artifact Reduction by Plugging Diffusion Priors in Dual Domains&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16742&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stage-by-stage Wavelet Optimization Refinement Diffusion Model for Sparse-View CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Xu, Shiyu Lu, Bin Huang, Weiwen Wu, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15942&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Physics-Informed DeepMRI: Bridging the Gap from Heat Diffusion to k-Space Interpolation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Congcong Liu, Xiaohong Fan, Chentao Cao, Jing Cheng, Qingyong Zhu, Yuanyuan Liu, Sen Jia, Yihang Zhou, Haifeng Wang, Yanjie Zhu, Jianping Zhang, Qiegen Liu, Dong Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15918&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffBIR: Towards Blind Image Restoration with Generative Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, Chao Dong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15070&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/XPixelGroup/DiffBIR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and Personalized Stylization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Yang, Peiran Ren, Xuansong Xie, Lei Zhang&lt;/em&gt; &lt;br&gt; AAAI 2024. [&lt;a href=&#34;https://arxiv.org/abs/2308.14469&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data-iterative Optimization Score Model for Stable Ultra-Sparse-View CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weiwen Wu, Yanyang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14437&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Residual Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiawei Liu, Qiang Wang, Huijie Fan, Yinong Wang, Yandong Tang, Liangqiong Qu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13712&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nachifur/RDDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xunpeng Yi, Han Xu, Hao Zhang, Linfeng Tang, Jiayi Ma&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13164&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Full-dose PET Synthesis from Low-dose PET Using High-efficiency Diffusion Denoising Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoyan Pan, Elham Abouei, Junbo Peng, Joshua Qian, Jacob F Wynne, Tonghe Wang, Chih-Wei Chang, Justin Roper, Jonathon A Nye, Hui Mao, Xiaofeng Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13072&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jueqi Wang, Jacob Levman, Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, M. Jorge Cardoso, Razvan Marinescu&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.12465&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/BioMedAI-UCSC/InverseSR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-quality Image Dehazing with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hu Yu, Jie Huang, Kaiwen Zheng, Man Zhou, Feng Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.11949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Frequency Compensated Diffusion Model for Real-scene Dehazing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jing Wang, Songtao Wu, Kuanhong Xu, Zhiqiang Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10510&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeyu Han, Yuhan Wang, Luping Zhou, Peng Wang, Binyu Yan, Jiliu Zhou, Yan Wang, Dinggang Shen&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10157&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Show-han/PET-Reconstruction&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffLLE: Diffusion-guided Domain Calibration for Unsupervised Low-light Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuzhou Yang, Xuanyu Zhang, Yinhuai Wang, Jiwen Yu, Yuhan Wang, Jian Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09279&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning A Coarse-to-Fine Diffusion Transformer for Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Liyan Wang, Qinyu Yang, Cong Wang, Wei Wang, Jinshan Pan, Zhixun Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.08730&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Monte Carlo guided Diffusion for Bayesian linear inverse problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gabriel Cardoso, Yazid Janati El Idrissi, Sylvain Le Corff, Eric Moulines&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07983&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Geometry of the Visual Cortex with Applications to Image Inpainting and Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Francesco Ballerin, Erlend Grong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07652&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ballerin/v1diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;YODA: You Only Diffuse Areas. An Area-Masked Diffusion Approach For Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Brian B. Moser, Stanislav Frolov, Federico Raue, Sebastian Palacio, Andreas Dengel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07977&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TextDiff: Mask-Guided Residual Diffusion Models for Scene Text Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Baolin Liu, Zongyuan Yang, Pengfei Wang, Junjie Zhou, Ziqi Liu, Ziyi Song, Yan Liu, Yongping Xiong&lt;/em&gt; &lt;br&gt; AAAI 2024. [&lt;a href=&#34;https://arxiv.org/abs/2308.06743&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLE Diffusion: Controllable Light Enhancement Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuyang Yin, Dejia Xu, Chuangchuang Tan, Ping Liu, Yao Zhao, Yunchao Wei&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06725&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yuyangyin.github.io/CLEDiffusion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YuyangYin/CLEDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Augmented Depth Prediction with Sparse Annotations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaqi Li, Yiran Wang, Zihao Huang, Jinghong Zheng, Ke Xian, Zhiguo Cao, Jianming Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02283&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Painterly Image Harmonization using Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lingxiao Lu, Jiangtong Li, Junyan Cao, Li Niu, Liqing Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02228&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kyungryun Lee, Won-Ki Jeong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01594&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Fourier-Constrained Diffusion Bridges for MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muhammad U. Mirza, Onat Dalmaz, Hasan A. Bedel, Gokberk Elmas, Yilmaz Korkmaz, Alper Gungor, Salman UH Dar, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01096&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ultrasound Image Reconstruction with Denoising Diffusion Restoration Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus&lt;/em&gt; &lt;br&gt; MICCAI Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.15990&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Yuxin-Zhang-Jasmine/DRUS-v1&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLDiffusion: Learning Degradation Representations in Diffusion Models for Low-Light Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Wang, Kaihao Zhang, Ziqian Shao, Wenhan Luo, Bjorn Stenger, Tae-Kyun Kim, Wei Liu, Hongdong Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.14659&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Artifact Restoration in Histology Images with Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenqi He, Junjun He, Jin Ye, Yiqing Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.14262&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhenqi-he/ArtiFusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zongsheng Yue, Jianyi Wang, Chen Change Loy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.12348&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zsyOAOA/ResShift&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Iterative Reconstruction Based on Latent Diffusion Model for Sparse Data Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Linchao He, Hongyu Yan, Mengting Luo, Kunming Luo, Wang Wang, Wenchao Du, Hu Chen, Hongyu Yang, Yi Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.12070&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PartDiff: Image Super-resolution with Partial Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Zhao, Alex Ling Yu Hung, Kaifeng Pang, Haoxin Zheng, Kyunghyun Sung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.11926&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dejia Xu, Xingqian Xu, Wenyan Cong, Humphrey Shi, Zhangyang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.10584&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vita-group.github.io/RefPaint/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AnyDoor: Zero-shot Object-level Image Customization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, Hengshuang Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.09481&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://damo-vilab.github.io/AnyDoor-Page/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Zhao, Tingbo Hou, Yu-Chuan Su, Xuhui Jia. Yandong Li, Matthias Grundmann&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08996&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flow Matching in Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Quan Dao, Hao Phung, Binh Nguyen, Anh Tran&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08698&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vinairesearch.github.io/LFM/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Identity-Preserving Aging of Face Images via Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sudipta Banerjee, Govind Mittal, Ameya Joshi, Chinmay Hegde, Nasir Memon&lt;/em&gt; &lt;br&gt; IJCB 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08585&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, Liyue Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08123&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ExposureDiffusion: Learning to Expose for Low-light Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yufei Wang, Yi Yu, Wenhan Yang, Lanqing Guo, Lap-Pui Chau, Alex C. Kot, Bihan Wen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.07710&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kyle Luther, H. Sebastian Seung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.04946&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stimulating the Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tong Li, Hansen Feng, Lizhi Wang, Zhiwei Xiong, Hua Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.03992&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IPO-LDM: Depth-aided 360-degree Indoor RGB Panorama Outpainting via Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianhao Wu, Chuanxia Zheng, Tat-Jen Cham&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.03177&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sm0kywu.github.io/ipoldm/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Single Image LDR to HDR Conversion using Conditional Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dwip Dalal, Gautam Vashishtha, Prajwal Singh, Shanmuganathan Raman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.02814&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ACDMSR: Accelerated Conditional Diffusion Models for Single Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Axi Niu, Pham Xuan Trung, Kang Zhang, Jinqiu Sun, Yu Zhu, In So Kweon, Yanning Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.00781&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Linoy Tsaban, Apolinário Passos&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.00522&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Linear Inverse Problems Provably via Posterior Sampling with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alexandros G. Dimakis, Sanjay Shakkottai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.00619&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LituRout/PSLD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Content-Preserving Diffusion Model for Unsupervised AS-OCT image Despeckling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Li Sanqian, Higashita Risa, Fu Huazhu, Li Heng, Niu Jingxuan, Liu Jiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17717&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Supervised MRI Reconstruction with Unrolled Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilmaz Korkmaz, Tolga Cukur, Vishal Patel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16654&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SVNR: Spatially-variant Noise Removal with Denoising Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Naama Pearl, Yaron Brodsky, Dana Berman, Assaf Zomet, Alex Rav Acha, Daniel Cohen-Or, Dani Lischinski&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16052&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Easing Color Shifts in Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Katherine Deck, Tobias Bischoff&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.15832&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model Based Low-Light Image Enhancement for Space Satellite&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiman Zhu, Lu Wang, Jingyi Yuan, Yu Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14227&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marco Aversa, Gabriel Nobis, Miriam Hägele, Kai Standvoss, Mihaela Chirica, Roderick Murray-Smith, Ahmed Alaa, Lukas Ruff, Daniela Ivanova, Wojciech Samek, Frederick Klauschen, Bruno Sanguinetti, Luis Oala&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13384&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wind Noise Reduction with a Diffusion-based Stochastic Regeneration Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jean-Marie Lemercier, Joachim Thiemann, Raphael Koning, Timo Gerkmann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12867&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseIR:Diffusion Models For Isotropic Reconstruction of 3D Microscopic Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingjie Pan, Yulu Gan, Fangxu Zhou, Jiaming Liu, Aimin Wang, Shanghang Zhang, Dawei Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12109&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HSR-Diff:Hyperspectral Image Super-Resolution via Conditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chanyue Wu, Dong Wang, Hanyu Mao, Ying Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12085&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion with Forward Models: Solving Stochastic Inverse Problems Without Direct Supervision&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Frédo Durand, William T. Freeman, Vincent Sitzmann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11719&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Ultrasound Denoising Using Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hojat Asgariandehkordi, Sobhan Goudarzi, Adrian Basarab, Hassan Rivaz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07440&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Visual Foundational Models of Physical Scenes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chethan Parameshwara, Alessandro Achille, Matthew Trager, Xiaolong Li, Jiawei Mo, Matthew Trager, Ashwin Swaminathan, CJ Taylor, Dheera Venkatraman, Xiaohan Fei, Stefano Soatto&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03727&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;INDigo: An INN-Guided Probabilistic Diffusion Algorithm for Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Di You, Andreas Floros, Pier Luigi Dragotti&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular Depth Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, David J. Fleet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01923&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dissecting Arbitrary-scale Super-resolution Capability from Pre-trained Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruibin Li, Qihua Zhou, Song Guo, Jie Zhang, Jingcai Guo, Xinyang Jiang, Yifei Shen, Zhenhua Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00714&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Low-Light Image Enhancement with Wavelet-based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hai Jiang, Ao Luo, Songchen Han, Haoqiang Fan, Shuaicheng Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00306&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Unified Conditional Framework for Diffusion-based Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Zhang, Xiaoyu Shi, Dasong Li, Xiaogang Wang, Jian Wang, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20049&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Direct Diffusion Bridge using Data Consistency for Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jeongsol Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19809&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Models for Inverse Problems through Shortcut Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gongye Liu, Haoze Sun, Jiayi Li, Fei Yin, Yujiu Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16965&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthew Chang, Aditya Prakash, Saurabh Gupta&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16301&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://matthewchang.github.io/vidm/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Diffusion Probabilistic Prior for Low-Dose CT Image Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15887&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiyang Ma, Huan Yang, Wenhan Yang, Jianlong Fu, Jiaying Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15357&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WaveDM: Wavelet-Based Diffusion Models for Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Huang, Jiancheng Huang, Jianzhuang Liu, Yu Dong, Jiaxi Lv, Shifeng Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13819&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dual-Diffusion: Dual Conditional Denoising Diffusion Probabilistic Models for Blind Super-Resolution Reconstruction in RSIs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengze Xu, Jie Ma, Yuanyuan Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12170&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Lincoln20030413/DDSR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, Ran Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11147&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pyramid Diffusion Models For Low-light Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dewei Zhou, Zongxin Yang, Yi Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10028&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruoqi Wang, Zhuoyang Chen, Qiong Luo, Feng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09121&#34;&gt;Paper&lt;/a&gt;] 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Models for Plug-and-Play Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan Wen, Radu Timofte, Luc Van Gool&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08995&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuanzhi-zhu/DiffPIR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploiting Diffusion Prior for Real-World Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K. Chan, Chen Change Loy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.07015&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://iceclear.github.io/projects/stablesr/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IceClear/StableSR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Atmospheric Turbulence Correction via Variational Deep Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xijun Wang, Santiago López-Tapia, Aggelos K. Katsaggelos&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05077&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Light Diffusion for Portraits&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Futschik, Kelvin Ritland, James Vecore, Sean Fanello, Sergio Orts-Escolano, Brian Curless, Daniel Sýkora, Rohit Pandey&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04745&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffBFR: Bootstrapping Diffusion Model Towards Blind Face Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinmin Qiu, Congying Han, ZiCheng Zhang, Bonan Li, Tiande Guo, Xuecheng Nie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04517&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real-World Denoising via Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Yang, Lijing Liang, Zhixun Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04457&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Variational Perspective on Solving Inverse Problems with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Morteza Mardani, Jiaming Song, Jan Kautz, Arash Vahdat&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04391&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Taofeng Xie, Chentao Cao, Zhuoxu Cui, Yu Guo, Caiying Wu, Xuemei Wang, Qingneng Li, Zhanli Hu, Tao Sun, Ziru Sang, Yihang Zhou, Yanjie Zhu, Dong Liang, Qiyu Jin, Guoqing Chen, Haifeng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03901&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DocDiff: Document Enhancement via Residual Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zongyuan Yang, Baolin Liu, Yongping Xiong, Lan Yi, Guibin Wu, Xiaojun Tang, Ziqi Liu, Junjie Zhou, Xing Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03892&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Royalvice/DocDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Asad Aali, Marius Arvinte, Sidharth Kumar, Jonathan I. Tamir&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01166&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-similarity-based super-resolution of photoacoustic angiography from hand-drawn doodles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanzheng Ma, Wangting Zhou, Rui Ma, Sihua Yang, Yansong Tang, Xun Guan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01165&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Diffusion Models as Principled Priors for Inverse Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Berthy T. Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L. Bouman, William T. Freeman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11751&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Diffusion-based Image Colorization via Piggybacked Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanyuan Liu, Jinbo Xing, Minshan Xie, Chengze Li, Tien-Tsin Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.11105&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://piggyback-color.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiFaReli: Diffusion Face Relighting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Puntawat Ponglertnapakorn, Nontawat Tritrong, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09479&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusion-face-relighting.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inpaint Anything: Segment Anything Meets Image Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, Zhibo Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06790&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/geekyutao/Inpaint-Anything&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Refusion: Enabling Large-Size Realistic Image Restoration with Latent-Space Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08291&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Algolzw/image-restoration-sde&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPIRiT-Diffusion: Self-Consistency Driven Diffusion Model for Accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Chentao Cao, Jing Cheng, Sen Jia, Hairong Zheng, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05060&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot CT Field-of-view Completion with Unconditional Generative Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaiwen Xu, Aravind R. Krishnan, Thomas Z. Li, Yuankai Huo, Kim L. Sandler, Fabien Maldonado, Bennett A. Landman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03760&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SketchFFusion: Sketch-guided image editing with diffusion model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weihang Mao, Bo Han, Zihao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03174&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inst-Inpaint: Instructing to Remove Objects with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut Erdem, Aysegul Dundar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03246&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://instinpaint.abyildirim.com/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guanhua Zhang, Jiabao Ji, Yang Zhang, Mo Yu, Tommi Jaakkola, Shiyu Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03322&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UCSB-NLP-Chang/CoPaint/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Medical Image Translation via Frequency-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunxiang Li, Hua-Chieh Shao, Xiao Liang, Liyuan Chen, Ruiqi Li, Steve Jiang, Jing Wang, You Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02742&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Brian Moser, Stanislav Frolov, Federico Raue, Sebastian Palacio, Andreas Dengel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01994&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qi Gao, Zilong Li, Junping Zhang, Yi Zhang, Hongming Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01814&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Diffusion Prior for Unified Image Restoration and Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ben Fei, Zhaoyang Lyu, Liang Pan, Junzhe Zhang, Weidong Yang, Tianyue Luo, Bo Zhang, Bo Dai&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Implicit Diffusion Models for Continuous Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yanjing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, Baochang Zhang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16491&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiracDiffusion: Denoising and Incremental Reconstruction with Assured Data-Consistency&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zalan Fabian, Berk Tinaz, Mahdi Soltanolkotabi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14353&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yizhuo Lu, Changde Du, Dianpeng Wang, Huiguang He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14139&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DisC-Diff: Disentangled Conditional Diffusion Model for Multi-Contrast MRI Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Mao, Lan Jiang, Xi Chen, Chao Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13933&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sub-volume-based Denoising Diffusion Probabilistic Model for Cone-beam CT Reconstruction from Incomplete Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Chuang Niu, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12861&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Perceptual Quality Assessment Exploration for AIGC Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zicheng Zhang, Chunyi Li, Wei Sun, Xiaohong Liu, Xiongkuo Min, Guangtao Zhai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12618&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mauricio Delbracio, Peyman Milanfar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11435&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Neural Generation of 4K Masks for Homogeneous Diffusion Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Karl Schrader, Pascal Peter, Niklas Kämper, Joachim Weickert&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10096&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Post-Processing for Low-Light Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Savvas Panagiotou, Anna S. Bosman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09627&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SUD2: Supervision by Denoising Diffusion Models for Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthew A. Chan, Sean I. Young, Christopher A. Metzler&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09642&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffIR: Efficient Diffusion Model for Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, Luc Van Gool&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09472&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ResDiff: Combining CNN and Diffusion Model for Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuyao Shang, Zhengyang Shan, Guangxing Liu, Jinglin Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08714&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jan Oscar Cross-Zamirski, Praveen Anand, Guy Williams, Elizabeth Mouchet, Yinhai Wang, Carola-Bibiane Schönlieb&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08863&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/crosszamirski/guided-I2I&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Contrast Harmonization of Magnetic Resonance Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alicia Durrer, Julia Wolleb, Florentin Bieder, Tim Sinnecker, Matthias Weigel, Robin Sandkühler, Cristina Granziera, Özgür Yaldizli, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08189&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing Realistic Image Restoration Training Pairs: A Diffusion Approach&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Yang, Peiran Ren, Xuansong xie, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06994&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DR2: Diffusion-based Robust Degradation Remover for Blind Face Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhixin Wang, Xiaoyun Zhang, Ziying Zhang, Huangjie Zheng, Mingyuan Zhou, Ya Zhang, Yanfeng Wang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06885&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuchun Miao, Lefei Zhang, Liangpei Zhang, Dacheng Tao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06682&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Suhyeon Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05754&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generalized Diffusion MRI Denoising and Super-Resolution using Swin Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amir Sadikov, Jamie Wren-Jarvis, Xinlei Pan, Lanya T. Cai, Pratik Mukherjee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05686&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionDepth: Diffusion Denoising Approach for Monocular Depth Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiqun Duan, Zheng Zhu, Xianda Guo&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05021&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/duanyiqun/DiffusionDepth&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Enhancement From Degradation: A Diffusion Model For Fundus Image Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Puijin Cheng, Li Lin, Yijin Huang, Huaqing He, Wenhan Luo, Xiaoying Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04603&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/QtacierP/LED&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unlimited-Size Diffusion Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yinhuai Wang, Jiwen Yu, Runyi Yu, Jian Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.00354&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Out-of-Distribution Detection with Diffusion Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenzhen Liu, Jin Peng Zhou, Yufan Wang, Kilian Q. Weinberger&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10326&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoration based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaemoo Choi, Yesom Park, Myungjoo Kang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05456&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explicit Diffusion of Gaussian Mixture Model Based Image Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Martin Zach, Thomas Pock, Erich Kobler, Antonin Chambolle&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08411&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models for Robust Image Super-Resolution in the Wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hshmat Sahak, Daniel Watson, Chitwan Saharia, David Fleet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07864&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CDPMSR: Conditional Diffusion Probabilistic Models for Single Image Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Axi Niu, Kang Zhang, Trung X. Pham, Jinqiu Sun, Yu Zhu, In So Kweon, Yanning Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12831&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jacopo Teneggi, Matt Tivnan, J Webster Stayman, Jeremias Sulam&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03791&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tiange Xiang, Mahmut Yurt, Ali B Syed, Kawin Setsompop, Akshay Chaudhari&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03018&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/StanfordMIMI/DDM2&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model for Generative Image Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutong Xie, Minne Yuan, Bin Dong, Quanzheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02398&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Theoretical Justification for Image Inpainting using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Litu Rout, Advait Parulekar, Constantine Caramanis, Sanjay Shakkottai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01217&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GibbsDDRM: A Partially Collapsed Gibbs Sampler for Solving Blind Inverse Problems with Denoising Diffusion Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Naoki Murata, Koichi Saito, Chieh-Hsin Lai, Yuhta Takida, Toshimitsu Uesaka, Yuki Mitsufuji, Stefano Ermon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12686&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Guided Diffusion Sampling with Splitting Numerical Methods&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suttisak Wizadwongsa, Supasorn Suwajanakorn&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11558&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Denoising for Low-Dose-CT Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Runyi Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11482&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Screen Space Indirect Lighting with Visibility Bitmask&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Olivier Therrien, Yannick Levesque, Guillaume Gilet&lt;/em&gt; &lt;br&gt; Visual Computer 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11376&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dual Diffusion Architecture for Fisheye Image Rectification: Synthetic-to-Real Generalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shangrong Yang, Chunyu Lin, Kang Liao, Yao Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11785&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RainDiffusion:When Unsupervised Learning Meets Diffusion Models for Real-world Image Deraining&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingqiang Wei, Yiyang Shen, Yongzhen Wang, Haoran Xie, Fu Lee Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.09430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dif-Fusion: Towards High Color Fidelity in Infrared and Visible Image Fusion with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingqiang Wei, Yiyang Shen, Yongzhen Wang, Haoran Xie, Fu Lee Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.09430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Removing Structured Noise with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tristan S.W. Stevens, Jean-Luc Robert, Faik C. Meral Jason Yu, Jun Seob Shin, Ruud J.G. van Sloun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05290&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Restoration with Mean-Reverting Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziwei Luo, Fredrik K. Gustafsson, Zheng Zhao, Jens Sjölund, Thomas B. Schön&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11699&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Algolzw/image-restoration-sde&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionCT: Latent Diffusion Model for CT Image Standardization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Md Selim, Jie Zhang, Michael A. Brooks, Ge Wang, Jin Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.08815&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Targeted Image Reconstruction by Sampling Pre-trained Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiageng Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.07557&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyutaek Oh, Jeong Eun Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03027&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Vision Transformers as Diffusion Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;He Cao, Jianan Wang, Tianhe Ren, Xianbiao Qi, Yihao Chen, Yuan Yao, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.13771&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Blind Watermarking: Combining Invertible and Non-invertible Mechanisms&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rui Ma, Mengxi Guo, Yi Hou, Fan Yang, Yuan Li, Huizhu Jia, Xiaodong Xie&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.12678&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rmpku/CIN&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bi-Noising Diffusion: Towards Conditional Diffusion Models with Generative Restoration Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kangfu Mei, Nithin Gopalakrishnan Nair, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07352&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://kfmei.page/bi-noising/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPIRiT-Diffusion: SPIRiT-driven Score-Based Generative Modeling for Vessel Wall imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chentao Cao, Zhuo-Xu Cui, Jing Cheng, Sen Jia, Hairong Zheng, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11274&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Universal Generative Modeling in Dual-domain for Dynamic MR Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chuanming Yu, Yu Guan, Ziwen Ke, Dong Liang, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07599&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DifFace: Blind Face Restoration with Diffused Error Contraction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zongsheng Yue, Chen Change Loy&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06512&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zsyOAOA/DifFace&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ShadowDiffusion: When Degradation Prior Meets Diffusion Model for Shadow Removal&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang, Yufei Wang, Hanspeter Pfister, Bihan Wen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04711&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Sample Diffusion Model in Projection Domain for Low-Dose CT Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bin Huang, Liu Zhang, Shiyu Lu, Boyu Lin, Weiwen Wu, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03630&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDM: Spatial Diffusion Model for Large Hole Image Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenbo Li, Xin Yu, Kun Zhou, Yibing Song, Zhe Lin, Jiaya Jia&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02963&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ADIR: Adaptive Diffusion for Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shady Abu-Hussein, Tom Tirer, Raja Giryes&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03221&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://shadyabh.github.io/ADIR/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Deblurring with Domain Generalizable Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido Gerig, Peyman Milanfar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.01789&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yinhuai Wang, Jiwen Yu, Jian Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00490&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wyhuai/DDNM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FREDSR: Fourier Residual Efficient Diffusive GAN for Single Image Super Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kyoungwan Woo, Achyuta Rajaram&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16678&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shichong Peng, Alireza Moazeni, Ke Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.14286&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaming Liu, Rushil Anirudh, Jayaraman J. Thiagarajan, Stewart He, K. Aditya Mohan, Ulugbek S. Kamilov, Hyojin Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12340&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model Based Posterior Sampling for Noisy Linear Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangming Meng, Yoshiyuki Kabashima&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12343&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mengxiangming/dmps&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Parallel Diffusion Models of Operator and Image for Blind Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jeongsol Kim, Sehui Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10656&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Dohoon Ryu, Michael T. McCann, Marc L. Klasky, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10655&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Patch-Based Denoising Diffusion Probabilistic Model for Sparse-View CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10388&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Structure-Guided Diffusion Model for Large-Hole Diverse Image Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daichi Horita, Jiaolong Yang, Dong Chen, Yuki Koyama, Kiyoharu Aizawa&lt;/em&gt; &lt;br&gt; BMVC 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.10437&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conffusion: Confidence Intervals for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eliahu Horwitz, Yedid Hoshen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09795&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Superresolution Reconstruction of Single Image for Latent features&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Wang, Jing-Ke Yan, Jing-Ye Cai, Jian-Hua Deng, Qin Qin, Qin Wang, Heng Xiao, Yao Cheng, Peng-Fei Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12845&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning to Kindle the Starlight&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Yuan, Jiaqi Wu, Lindong Wang, Zhongliang Jing, Henry Leung, Shuyuan Zhu, Han Pan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09206&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ShadowDiffusion: Diffusion-based Shadow Removal using Classifier-driven Attention and Structure Preservation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yeying Jin, Wenhan Yang, Wei Ye, Yuan Yuan, Robby T. Tan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.08089&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DriftRec: Adapting diffusion models to blind image restoration tasks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Simon Welker, Henry N. Chapman, Timo Gerkmann&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06757&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Denoising Diffusions to Denoising Markov Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joe Benton, Yuyang Shi, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.03595&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yuyang-shi/generalized-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quantized Compressed Sensing with Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangming Meng, Yoshiyuki Kabashima&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13006&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mengxiangming/QCS-SGM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intelligent Painter: Picture Composition With Resampling Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wing-Fung Ku, Wan-Chi Siu, Xi Cheng, H. Anthony Chan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.17106&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pouria Rouzrokh, Bardia Khosravi, Shahriar Faghani, Mana Moassefi, Sanaz Vahdati, Bradley J. Erickson&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12113&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Mayo-Radiology-Informatics-Lab/MBTI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffGAR: Model-Agnostic Restoration from Generative Artifacts Using Image-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yueqin Yin, Lianghua Huang, Yu Liu, Kaiqi Huang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.08573&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Low-Dose CT Using Denoising Diffusion Probabilistic Model for 20× Speedup&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Qing Lyu, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.15136&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Posterior Sampling for General Noisy Inverse Problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14687&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DPS2022/diffusion-posterior-sampling&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Face Super-Resolution Using Stochastic Differential Equations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marcelo dos Santos, Rayson Laroca, Rafael O. Ribeiro, João Neves, Hugo Proença, David Menotti&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.12064&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/marcelowds/sr-sde&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JPEG Artifact Correction using Denoising Diffusion Restoration Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Jiaming Song, Stefano Ermon, Michael Elad&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.11888&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;T2V-DDPM: Thermal to Visible Face Translation using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08814&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Delving Globally into Texture and Structure for Image Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haipeng Liu, Yang Wang, Meng Wang, Yong Rui&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08217&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/htyjers/DGTS-Inpainting&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PET image denoising based on denoising diffusion probabilistic models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kuang Gong, Keith A. Johnson, Georges El Fakhri, Quanzheng Li, Tinsu Pan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.06167&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Score: Self-Supervised Learning on Score-Based Models for MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Chentao Cao, Shaonan Liu, Qingyong Zhu, Jing Cheng, Haifeng Wang, Yanjie Zhu, Dong Liang&lt;/em&gt; &lt;br&gt; IEEE TMI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.00835&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AT-DDPM: Restoring Faces degraded by Atmospheric Turbulence using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Kangfu Mei, Vishal M Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.11284&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S. Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09392&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arpitbansal297/Cold-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Frequency Space Diffusion Models for Accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chentao Cao, Zhuo-Xu Cui, Shaonan Liu, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.05481&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Restoring Vision in Adverse Weather Conditions with Patch-Based Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ozan Özdenizci, Robert Legenstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.14626&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/IGITUGraz/WeatherDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Non-Uniform Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.09786&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Medical Image Translation with Adversarial Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muzaffer Özbey, Salman UH Dar, Hasan A Bedel, Onat Dalmaz, Şaban Özturk, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08208&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adaptive Diffusion Priors for Accelerated MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Salman UH Dar, Şaban Öztürk, Yilmaz Korkmaz, Gokberk Elmas, Muzaffer Özbey, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.05876&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangxi Meng, Yuning Gu, Yongsheng Pan, Nizhuan Wang, Peng Xue, Mengkang Lu, Xuming He, Yiqiang Zhan, Dinggang Shen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.03430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SAR Despeckling using a Denoising Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Malsha V. Perera, Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.04514&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Models for Inverse Problems using Manifold Constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00941&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.02641&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Eun Sun Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.12621&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards performant and reliable undersampled MR reconstruction via diffusion model sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Peng, Pengfei Guo, S. Kevin Zhou, Vishal Patel, Rama Chellappa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.04292&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cpeng93/diffuserecon&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutong Xie, Quanzheng Li&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.03623&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Theodore-PKU/MC-DDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MRI Reconstruction via Data Driven Markov Chain with Joint Uncertainty Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guanxiong Luo, Martin Heide, Martin Uecker&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.01479&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mrirecon/spreco&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Denoising of Retinal OCT with Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dewei Hu, Yuankai K. Tao, Ipek Oguz&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.11760&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DeweiHu/OCT_DDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Restoration Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Michael Elad, Stefano Ermon, Jiaming Song&lt;/em&gt; &lt;br&gt; ICLR 2022 Workshop (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2201.11793&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RePaint: Inpainting using Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, Luc Van Gool&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.09865&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/andreas128/RePaint&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseVAE: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kushagra Pandey, Avideep Mukherjee, Piyush Rai, Abhishek Kumar&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.00308&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/kpandey008/DiffuseVAE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Byeongsu Sim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.05146&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deblurring via Stochastic Refinement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, Peyman Milanfar&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.02475&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Image Generation with Score-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Schönlieb, Christian Etmann&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.13606&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems in Medical Imaging with Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Liyue Shen, Lei Xing, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.08005&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yang-song/score_inverse_problems&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;S3RP: Self-Supervised Super-Resolution and Prediction for Advection-Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chulin Wang, Kyongmin Yeo, Xiao Jin, Andres Codas, Levente J. Klein, Bruce Elmegreen&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.04639&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based diffusion models for accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jong chul Ye&lt;/em&gt; &lt;br&gt; MIA 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.05243&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HJ-harry/score-MRI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Autoregressive Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, Tim Salimans&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.02037&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, Sungroh Yoon&lt;/em&gt; &lt;br&gt; ICCV 2021 (Oral). [&lt;a href=&#34;https://arxiv.org/abs/2108.02938&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jychoi118/ilvr_adm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Aug 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cascaded Diffusion Models for High Fidelity Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2106.15282&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://cascaded-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 May 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SRDiff: Single Image Super-Resolution with Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhihai Xu, Qi Li, Yueting Chen&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2104.14951&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Apr 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Super-Resolution via Iterative Refinement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; arXiv 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.07636&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://iterative-refinement.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Janspiry/Image-Super-Resolution-via-Iterative-Refinement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Apr 2021&lt;/p&gt; &#xA;&lt;h3&gt;Medical Imaging&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Data Augmentation for Nuclei Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinyi Yu, Guanbin Li, Wei Lou, Siqi Liu, Xiang Wan, Yan Chen, Haofeng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.14197&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EMIT-Diff: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheyuan Zhang, Lanhong Yao, Bin Wang, Debesh Jha, Elif Keles, Alpay Medetalibeyoglu, Ulas Bagci&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12868&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Generic Semi-Supervised Framework for Volumetric Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haonan Wang, Xiaomeng Li&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11320&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xmed-lab/GenericSSL&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-supervised Fetal MRI 3D Reconstruction Based on Radiation Diffusion Generation Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junpeng Tan, Xin Zhang, Yao Lv, Xiangmin Xu, Gang Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10209&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JSMoCo: Joint Coil Sensitivity and Motion Correction in Parallel MRI with a Self-Calibrating Score-Based Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lixuan Chen, Xuanyu Tian, Jiangjie Wu, Ruimin Feng, Guoyan Lao, Yuyao Zhang, Hongjiang Wei&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09625&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Histogram- and Diffusion-Based Medical Out-of-Distribution Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Evi M. C. Huijben, Sina Amirrajab, Josien P. W. Pluim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08654&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Echocardiography video synthesis from end diastolic semantic map via diffusion model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Phi Nguyen Van, Duc Tran Minh, Hieu Pham Huy, Long Tran Quoc&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07131&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Prior Regularized Iterative Reconstruction for Low-dose CT&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Yongyi Shi, Chuang Niu, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image Compression and Decompression Framework Based on Latent Diffusion Model for Breast Mammography&lt;/strong&gt; &lt;br&gt; &lt;em&gt;InChan Hwang, MinJae Woo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05299&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Diffusion Model for Medical Image Standardization and Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Md Selim, Jie Zhang, Faraneh Fathi, Michael A. Brooks, Ge Wang, Guoqiang Yu, Jin Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05237&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Characterizing the Features of Mitotic Figures Using a Conditional Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cagla Deniz Bahadir, Benjamin Liechty, David J. Pisapia, Mert R. Sabuncu&lt;/em&gt; &lt;br&gt; MICCAI Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03893&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yanwu Xu, Li Sun, Wei Peng, Shyam Visweswaran, Kayhan Batmanghelich&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03559&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blind CT Image Quality Assessment Using DDPM-derived Content and Transformer-based Evaluator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yongyi Shi, Wenjun Xia, Ge Wang, Xuanqin Mou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03118&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Batu Ozturkler, Chao Liu, Benjamin Eckart, Morteza Mardani, Jiaming Song, Jan Kautz&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01799&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NVlabs/SMRD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffGAN-F2S: Symmetric and Efficient Denoising Diffusion GANs for Structural Connectivity Prediction from Brain fMRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiankun Zuo, Ruiheng Li, Yi Di, Hao Tian, Changhong Jing, Xuhang Chen, Shuqiang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.16205&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhancing Knee Osteoarthritis severity level classification using diffusion augmented images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paleti Nikhil Chowdary, Gorantla V N S L Vishnu Vardhan, Menta Sai Akshay, Menta Sai Aashish, Vadlapudi Sai Aravind, Garapati Venkata Krishna Rayalu, Aswathy P&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.09328&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Introducing Shape Prior Module in Diffusion Model for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiqing Zhang, Guojia Fan, Tianyong Liu, Nan Li, Yuyang Liu, Ziyu Liu, Canwei Dong, Shoujun Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05929&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Treatment-aware Diffusion Probabilistic Model for Longitudinal MRI Generation and Diffuse Glioma Growth Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinghui Liu, Elies Fuster-Garcia, Ivar Thokle Hovden, Donatas Sederevicius, Karoline Skogen, Bradley J MacIntosh, Edvard Grødem, Till Schellhorn, Petter Brandal, Atle Bjørnerud, Kyrre Eeg Emblem&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05406&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Bayesian Computational Imaging with a Surrogate Score-Based Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Berthy T. Feng, Katherine L. Bouman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Segmentation of 3D pore space from CT images using curvilinear skeleton: application to numerical simulation of microbial decomposition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Olivier Monga, Zakaria Belghali, Mouad Klai, Lucie Druoton, Dominique Michelucci, Valerie Pot&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01611&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GenSelfDiff-HIS: Generative Self-Supervision Using Diffusion for Histopathological Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vishnuvardhan Purma, Suhas Srinath, Seshan Srirangarajan, Aanchal Kakkar, Prathosh A. P&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01487&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PurmaVishnuVardhanReddy/GenSelfDiff-HIS&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Correlated and Multi-frequency Diffusion Modeling for Highly Under-sampled MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Guan, Chuanming Yu, Shiyu Lu, Zhuoxu Cui, Dong Liang, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00853&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yqx7150/CM-DM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Modeling with Domain-conditioned Prior Guidance for Accelerated MRI and qMRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wanyu Bian, Albert Jang, Fang Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00783&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PathLDM: Text conditioned Latent Diffusion Model for Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Srikar Yellapragada, Alexandros Graikos, Prateek Prasanna, Tahsin Kurc, Joel Saltz, Dimitris Samaras&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00748&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised CT Metal Artifact Reduction by Plugging Diffusion Priors in Dual Domains&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16742&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Recycling Training Strategy for Medical Image Segmentation with Diffusion Denoising Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunguan Fu, Yiwen Li, Shaheer U Saeed, Matthew J Clarkson, Yipeng Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16355&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mathpluscode/ImgX-DiffSeg&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Physics-Informed DeepMRI: Bridging the Gap from Heat Diffusion to k-Space Interpolation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Congcong Liu, Xiaohong Fan, Chentao Cao, Jing Cheng, Qingyong Zhu, Yuanyuan Liu, Sen Jia, Yihang Zhou, Haifeng Wang, Yanjie Zhu, Jianping Zhang, Qiegen Liu, Dong Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15918&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stage-by-stage Wavelet Optimization Refinement Diffusion Model for Sparse-View CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Xu, Shiyu Lu, Bin Huang, Weiwen Wu, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15942&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyun Liang, Harry Anthony, Felix Wagner, Konstantinos Kamnitsas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16150&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data-iterative Optimization Score Model for Stable Ultra-Sparse-View CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weiwen Wu, Yanyang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14437&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Full-dose PET Synthesis from Low-dose PET Using High-efficiency Diffusion Denoising Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoyan Pan, Elham Abouei, Junbo Peng, Joshua Qian, Jacob F Wynne, Tonghe Wang, Chih-Wei Chang, Justin Roper, Jonathon A Nye, Hui Mao, Xiaofeng Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13072&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Augmenting medical image classifiers with synthetic data from latent diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luke W. Sagers, James A. Diao, Luke Melas-Kyriazi, Matthew Groh, Pranav Rajpurkar, Adewole S. Adamson, Veronica Rotemberg, Roxana Daneshjou, Arjun K. Manrai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.12453&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InverseSR: 3D Brain MRI Super-Resolution Using a Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jueqi Wang, Jacob Levman, Walter Hugo Lopez Pinaya, Petru-Daniel Tudosiu, M. Jorge Cardoso, Razvan Marinescu&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.12465&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/BioMedAI-UCSC/InverseSR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Texture Generation on 3D Meshes with Point-UV Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, Xiaojuan Qi&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10490&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contrastive Diffusion Model with Auxiliary Guidance for Coarse-to-Fine PET Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeyu Han, Yuhan Wang, Luping Zhou, Peng Wang, Binyu Yan, Jiliu Zhou, Yan Wang, Dinggang Shen&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10157&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Show-han/PET-Reconstruction&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising diffusion-based MR to CT image translation enables whole spine vertebral segmentation in 2D and 3D without manual annotations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robert Graf, Joachim Schmitt, Sarah Schlaeger, Hendrik Kristian Möller, Vasiliki Sideri-Lampretsa, Anjany Sekuboyina, Sandro Manuel Krieg, Benedikt Wiestler, Bjoern Menze, Daniel Rueckert, Jan Stefan Kirschke&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09345&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoxiao He, Chaowei Tan, Ligong Han, Bo Liu, Leon Axel, Kang Li, Dimitris N. Metaxas&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09223&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hexiaoxiao-cs/DMCVR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Model for Retinal Image Generation and Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alnur Alimanov, Md Baharul Islam&lt;/em&gt; &lt;br&gt; ICCP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.08339&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shape-guided Conditional Latent Diffusion Models for Synthesising Brain Vasculature&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yash Deo, Haoran Dou, Nishant Ravikumar, Alejandro F. Frangi, Toni Lassila&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06781&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Masked Diffusion as Self-supervised Representation Learner&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixuan Pan, Jianxu Chen, Yiyu Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.05695&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic Augmentation with Large-scale Unconditional Pre-training&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiarong Ye, Haomiao Ni, Peng Jin, Sharon X. Huang, Yuan Xue&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.04020&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/karenyyy/HistoDiffAug&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Energy-Guided Diffusion Model for CBCT-to-CT Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Linjie Fu, Xia Li, Xiuding Cai, Dong Miao, Yu Yao, Yali Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.03354&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DermoSegDiff: A Boundary-aware Segmentation Diffusion Model for Skin Lesion Delineation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Afshin Bozorgpour, Yousef Sadegheih, Amirhossein Kazerouni, Reza Azad, Dorit Merhof&lt;/em&gt; &lt;br&gt; MICCAI Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02959&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mindflow-institue/dermosegdiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesising Rare Cataract Surgery Samples with Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yannik Frisch, Moritz Fuchs, Antoine Sanner, Felix Anton Ucar, Marius Frenzel, Joana Wasielica-Poslednik, Adrian Gericke, Felix Mathias Wagner, Thomas Dratsch, Anirban Mukhopadhyay&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02587&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Counterfactual Generation and Anomaly Detection in Brain Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alessandro Fontanella, Grant Mair, Joanna Wardlaw, Emanuele Trucco, Amos Storkey&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02062&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kyungryun Lee, Won-Ki Jeong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01594&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bilel Guetarni, Feryal Windal, Halim Benhabiles, Marianne Petit, Romain Dubois, Emmanuelle Leteurtre, Dominique Collard&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01328&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Fourier-Constrained Diffusion Bridges for MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muhammad U. Mirza, Onat Dalmaz, Hasan A. Bedel, Gokberk Elmas, Yilmaz Korkmaz, Alper Gungor, Salman UH Dar, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01096&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;C-DARL: Contrastive diffusion adversarial representation learning for label-free blood vessel segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boah Kim, Yujin Oh, Bradford J. Wood, Ronald M. Summers, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.00193&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ultrasound Image Reconstruction with Denoising Diffusion Restoration Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Zhang, Clément Huneau, Jérôme Idier, Diana Mateus&lt;/em&gt; &lt;br&gt; MICCAI Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.15990&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Yuxin-Zhang-Jasmine/DRUS-v1&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pre-Training with Diffusion models for Dental Radiography segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jérémy Rousseau, Christian Alaka, Emma Covili, Hippolyte Mayard, Laura Misrachi, Willy Au&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.14066&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Iterative Reconstruction Based on Latent Diffusion Model for Sparse Data Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Linchao He, Hongyu Yan, Mengting Luo, Kunming Luo, Wang Wang, Wenchao Du, Hu Chen, Hongyu Yang, Yi Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.12070&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FSDiffReg: Feature-wise and Score-wise Diffusion-guided Unsupervised Deformable Image Registration for Cardiac Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Qin, Xiaomeng Li&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.12035&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xmed-lab/FSDiffReg&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FEDD -- Fair, Efficient, and Diverse Diffusion-based Lesion Segmentation and Malignancy Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Héctor Carrión, Narges Norouzi&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.11654&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hectorcarrion/fedd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PartDiff: Image Super-resolution with Partial Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Zhao, Alex Ling Yu Hung, Kaifeng Pang, Haoxin Zheng, Kyunghyun Sung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.11926&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lingting Zhu, Zeyue Xue, Zhenchao Jin, Xian Liu, Jingzhen He, Ziwei Liu, Lequan Yu&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.10094&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffDP: Radiotherapy Dose Prediction via a Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenghao Feng, Lu Wen, Peng Wang, Binyu Yan, Xi Wu, Jiliu Zhou, Yan Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.09794&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hasan Atakan Bedel, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.09547&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TractCloud: Registration-free tractography parcellation with a novel local-global streamline point cloud representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tengfei Xue, Yuqian Chen, Chaoyi Zhang, Alexandra J. Golby, Nikos Makris, Yogesh Rathi, Weidong Cai, Fan Zhang, Lauren J. O&#39;Donnell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.09000&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tractcloud.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SlicerDMRI/TractCloud&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, Liyue Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08123&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Adaptation with Bradley-Terry Preference Models in Text-To-Image Classification and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Victor Gallego&lt;/em&gt; &lt;br&gt; EYSM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07929&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Romain Hardy, Cornelia Ilin, Joe Klepich, Ryan Mitchell, Steve Hall, Jericho Villareal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.06507&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kyle Luther, H. Sebastian Seung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.04946&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLCaps: Learning to Illuminate Low-Light Capsule Endoscopy with Curved Wavelet Attention and Reverse Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Long Bai, Tong Chen, Yanan Wu, An Wang, Mobarakol Islam, Hongliang Ren&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.02452&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/longbai1006/LLCaps&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synchronous Image-Label Diffusion Probability Model with Application to Stroke Lesion Segmentation on Non-contrast CT&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianhai Zhang, Tonghua Wan, Ethan MacDonald, Bijoy Menon, Aravind Ganesh, Qiu Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.01740&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Investigating Data Memorization in 3D Latent Diffusion Models for Medical Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Salman Ul Hassan Dar, Arman Ghanaat, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliu, Stefan O. Schoenberg, Sandy Engelhardt&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.01148&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Content-Preserving Diffusion Model for Unsupervised AS-OCT image Despeckling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Li Sanqian, Higashita Risa, Fu Huazhu, Li Heng, Niu Jingxuan, Liu Jiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17717&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Supervised MRI Reconstruction with Unrolled Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilmaz Korkmaz, Tolga Cukur, Vishal Patel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16654&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DoseDiff: Distance-aware Diffusion Model for Dose Prediction in Radiotherapy&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiwen Zhang, Chuanpu Li, Liming Zhong, Zeli Chen, Wei Yang, Xuetao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16324&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffMix: Diffusion Model-based Data Synthesis for Nuclei Segmentation and Classification in Imbalanced Pathology Image Datasets&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyun-Jic Oh, Won-Ki Jeong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14132&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marco Aversa, Gabriel Nobis, Miriam Hägele, Kai Standvoss, Mihaela Chirica, Roderick Murray-Smith, Ahmed Alaa, Lukas Ruff, Daniela Ivanova, Wojciech Samek, Frederick Klauschen, Bruno Sanguinetti, Luis Oala&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13384&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseIR:Diffusion Models For Isotropic Reconstruction of 3D Microscopic Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingjie Pan, Yulu Gan, Fangxu Zhou, Jiaming Liu, Aimin Wang, Shanghang Zhang, Dawei Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12109&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TauPETGen: Text-Conditional Tau PET Image Synthesis Based on Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Se-In Jang, Cristina Lois, Emma Thibault, J. Alex Becker, Yafei Dong, Marc D. Normandin, Julie C. Price, Keith A. Johnson, Georges El Fakhri, Kuang Gong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11984&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SANO: Score-Based Diffusion Model for Anomaly Localization in Dermatology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alvaro Gonzalez-Jimenez, Simone Lionetti, Marc Pouly, Alexander A. Navarini&lt;/em&gt; &lt;br&gt; CVPR Workshop 2023. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023W/VAND/html/Gonzalez-Jimenez_SANO_Score-Based_Diffusion_Model_for_Anomaly_Localization_in_Dermatology_CVPRW_2023_paper.html&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Aligning Synthetic Medical Images with Clinical Knowledge using Human Feedback&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shenghuan Sun, Gregory M. Goldgof, Atul Butte, Ahmed M. Alaa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12438&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Annotator Consensus Prediction for Medical Image Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tomer Amit, Shmuel Shichrur, Tal Shaharabany, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09004&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deep Ultrasound Denoising Using Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hojat Asgariandehkordi, Sobhan Goudarzi, Adrian Basarab, Hassan Rivaz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07440&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Models for Weakly Supervised Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinrong Hu, Yu-Jen Chen, Tsung-Yi Ho, Yiyu Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03878&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpretable Alzheimer&#39;s Disease Classification Via a Contrastive Diffusion Autoencoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayodeji Ijishakin, Ahmed Abdulaal, Adamos Hadjivasiliou, Sophie Martin, James Cole&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03022&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing Sampling Patterns for Compressed Sensing MRI with Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sriram Ravula, Brett Levac, Ajil Jalal, Jonathan I. Tamir, Alexandros G. Dimakis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03284&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain tumor segmentation using synthetic MR images -- A comparison of GANs and diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muhammad Usman Akbar, Måns Larsson, Anders Eklund&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02986&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Anomaly Detection in Medical Images Using Masked Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hasan Iqbal, Umar Khalid, Jing Hua, Chen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19867&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cosmin I. Bercea, Michael Neumayr, Daniel Rueckert, Julia A. Schnabel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19643&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthetic CT Generation from MRI using 3D Transformer-based Denoising Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoyan Pan, Elham Abouei, Jacob Wynne, Tonghe Wang, Richard L. J. Qiu, Yuheng Li, Chih-Wei Chang, Junbo Peng, Justin Roper, Pretesh Patel, David S. Yu, Hui Mao, Xiaofeng Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19467&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Models for Semantic 3D Medical Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zolnamar Dorjsembe, Hsing-Kuo Pao, Sodtavilan Odonchimed, Furen Xiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18453&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GenerateCT: Text-Guided 3D Chest CT Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ibrahim Ethem Hamamci, Sezgin Er, Enis Simsar, Alperen Tezcan, Ayse Gulnihan Simsek, Furkan Almas, Sevval Nil Esirgun, Hadrien Reynaud, Sarthak Pati, Christian Bluethgen, Bjoern Menze&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16037&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ibrahimethemhamamci/GenerateCT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Diffusion Probabilistic Prior for Low-Dose CT Image Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15887&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Level Global Context Cross Consistency Model for Semi-Supervised Ultrasound Image Segmentation with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fenghe Tang, Jianrui Ding, Lingtao Wang, Min Xian, Chunping Ning&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09447&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/FengheTan9/Multi-Level-Global-Context-Cross-Consistency&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Beware of diffusion models for synthesizing medical images -- A comparison with GANs in terms of memorizing brain tumor images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muhammad Usman Akbar, Wuhao Wang, Anders Eklund&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.07644&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generation of Structurally Realistic Retinal Fundus Images with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sojung Go, Younghoon Ji, Sang Jun Park, Soochahn Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06813&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Stojanovski, Uxio Hermida, Pablo Lamata, Arian Beqiri, Alberto Gomez&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05424&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Taofeng Xie, Chentao Cao, Zhuoxu Cui, Yu Guo, Caiying Wu, Xuemei Wang, Qingneng Li, Zhanli Hu, Tao Sun, Ziru Sang, Yihang Zhou, Yanjie Zhu, Dong Liang, Qiyu Jin, Guoqing Chen, Haifeng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03901&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Asad Aali, Marius Arvinte, Sidharth Kumar, Jonathan I. Tamir&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01166&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-similarity-based super-resolution of photoacoustic angiography from hand-drawn doodles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanzheng Ma, Wangting Zhou, Rui Ma, Sihua Yang, Yansong Tang, Xun Guan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01165&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Fidelity Image Synthesis from Pulmonary Nodule Lesion Maps using Semantic Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Zhao, Benjamin Hou&lt;/em&gt; &lt;br&gt; MIDL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01138&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Discovery of 3D Hierarchical Structure with Generative Diffusion Features&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nurislam Tursynbek, Marc Niethammer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00067&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cycle-guided Denoising Diffusion Probability Model for 3D Cross-modality MRI Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoyan Pan, Chih-Wei Chang, Junbo Peng, Jiahan Zhang, Richard L.J. Qiu, Tonghe Wang, Justin Roper, Tian Liu, Hui Mao, Xiaofeng Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.00042&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseExpand: Expanding dataset for 2D medical image segmentation using diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Shao, Xiaohan Yuan, Zhen Huang, Ziming Qiu, Shuai Wang, Kevin Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13416&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://anonymous.4open.science/r/DiffuseExpand/README.md&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Realistic Data Enrichment for Robust Image Segmentation in Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sarah Cechnicka, James Ball, Callum Arthurs, Candice Roufosse, Bernhard Kainz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09534&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Medical Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pham Ngoc Huy, Tran Minh Quan&lt;/em&gt; &lt;br&gt; IEEE ISBI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.09383&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Multi-Institutional Open-Source Benchmark Dataset for Breast Cancer Clinical Decision Support using Synthetic Correlated Diffusion Imaging Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chi-en Amy Tai, Hayden Gunraj, Alexander Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05623&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cancer-Net BCa-S: Breast Cancer Grade Prediction using Volumetric Deep Radiomic Features from Synthetic Correlated Diffusion Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chi-en Amy Tai, Hayden Gunraj, Alexander Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05899&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPIRiT-Diffusion: Self-Consistency Driven Diffusion Model for Accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Chentao Cao, Jing Cheng, Sen Jia, Hairong Zheng, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05060&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mask-conditioned latent diffusion for generating gastrointestinal polyp images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roman Macháček, Leila Mozaffari, Zahra Sepasdar, Sravanthi Parasa, Pål Halvorsen, Michael A. Riegler, Vajira Thambawita&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05233&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tao Chen, Chenhui Wang, Hongming Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04429&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ambiguous Medical Image Segmentation using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aimon Rahman, Jeya Maria Jose Valanarasu, Ilker Hacihaliloglu, Vishal M Patel&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04745&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/aimansnigdha/Ambiguous-Medical-Image-Segmentation-using-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedGen3D: A Deep Generative Framework for Paired 3D Image and Mask Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kun Han, Yifeng Xiong, Chenyu You, Pooya Khosravi, Shanlin Sun, Xiangyi Yan, James Duncan, Xiaohui Xie&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04106&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://krishan999.github.io/MedGen3D/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Realistic Ultrasound Fetal Brain Imaging Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michelle Iskandar, Harvey Mannering, Zhanxiang Sun, Jacqueline Matthew, Hamideh Kerdegari, Laura Peralta, Miguel Xochicale&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03941&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/budai4medtech/midl2023&#34;&gt;Gitub&lt;/a&gt;] &lt;br&gt; 8 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot CT Field-of-view Completion with Unconditional Generative Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaiwen Xu, Aravind R. Krishnan, Thomas Z. Li, Yuankai Huo, Kim L. Sandler, Fabien Maldonado, Bennett A. Landman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03760&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Medical Image Translation via Frequency-Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunxiang Li, Hua-Chieh Shao, Xiao Liang, Liyuan Chen, Ruiqi Li, Steve Jiang, Jing Wang, You Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02742&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qi Gao, Zilong Li, Junping Zhang, Yi Zhang, Hongming Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01814&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology Image Analysis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Xu, Saarthak Kapse, Rajarsi Gupta, Prateek Prasanna&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01053&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pay Attention: Accuracy Versus Interpretability Trade-off in Fine-tuned Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mischa Dombrowski, Hadrien Reynaud, Johanna P. Müller, Matthew Baugh, Bernhard Kainz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17908&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDMM-Synth: A Denoising Diffusion Model for Cross-modal Medical Image Synthesis with Sparse-view Measurement Embedding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoyue Li, Kai Shang, Gaoang Wang, Mark D. Butala&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15770&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Memory-efficient Processing of 3D Medical Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Florentin Bieder, Julia Wolleb, Alicia Durrer, Robin Sandkühler, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; MIDL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15288&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-task Learning of Histology and Molecular Markers for Classifying Diffuse Glioma&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaofei Wang, Stephen Price, Chao Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14845&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoLa-Diff: Conditional Latent Diffusion Model for Multi-Modal MRI Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lan Jiang, Ye Mao, Xi Chen, Xiangfeng Wang, Chao Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14081&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DisC-Diff: Disentangled Conditional Diffusion Model for Multi-Contrast MRI Super-Resolution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Mao, Lan Jiang, Xi Chen, Chao Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13933&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Medical diffusion on a budget: textual inversion for medical image generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bram de Wilde, Anindo Saha, Richard P.G. ten Broek, Henkjan Huisman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sub-volume-based Denoising Diffusion Probabilistic Model for Cone-beam CT Reconstruction from Incomplete Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Chuang Niu, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12861&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hadrien Reynaud, Mengyun Qiao, Mischa Dombrowski, Thomas Day, Reza Razavi, Alberto Gomez, Paul Leeson, Bernhard Kainz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12644&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haipeng Zhou, Lei Zhu, Yuyin Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12313&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Latent Space Regression of Diffusion Autoencoders for Vertebral Fracture Grading&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matthias Keicher, Matan Atad, David Schinz, Alexandra S. Gersing, Sarah C. Foreman, Sophia S. Goller, Juergen Weissinger, Jon Rischewski, Anna-Sophia Dietrich, Benedikt Wiestler, Jan S. Kirschke, Nassir Navab&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12031&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NASDM: Nuclei-Aware Semantic Histopathology Image Generation Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aman Shrivastava, P. Thomas Fletcher&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11477&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cascaded Latent Diffusion Models for High-Resolution Chest X-ray Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tobias Weber, Michael Ingrisch, Bernd Bischl, David Rügamer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11224&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yijun Yang, Huazhu Fu, Angelica Aviles-Rivero, Carola-Bibiane Schönlieb, Lei Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10610&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-UNet: A Diffusion Embedded Network for Volumetric Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaohu Xing, Liang Wan, Huazhu Fu, Guang Yang, Lei Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10326&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ge-xing/Diff-UNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reversing the Abnormal: Pseudo-Healthy Generative Networks for Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cosmin I Bercea, Benedikt Wiestler, Daniel Rueckert, Julia A Schnabel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08452&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suhyeon Lee, Hyungjin Chung, Minyoung Park, Jonghyuk Park, Wi-Sun Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08440&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Class-Guided Image-to-Image Diffusion: Cell Painting from Brightfield Images with Class Labels&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jan Oscar Cross-Zamirski, Praveen Anand, Guy Williams, Elizabeth Mouchet, Yinhai Wang, Carola-Bibiane Schönlieb&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08863&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/crosszamirski/guided-I2I&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stochastic Segmentation with Conditional Categorical Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lukas Zbinden, Lars Doorenbos, Theodoros Pissas, Raphael Sznitman, Pablo Márquez-Neila&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08888&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LarsDoorenbos/ccdm-stochastic-segmentation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Contrast Harmonization of Magnetic Resonance Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alicia Durrer, Julia Wolleb, Florentin Bieder, Tim Sinnecker, Matthias Weigel, Robin Sandkühler, Cristina Granziera, Özgür Yaldizli, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08189&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficiently Training Vision Transformers on Structural MRI Scans for Alzheimer&#39;s Disease Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikhil J. Dhinagar, Sophia I. Thomopoulos, Emily Laltoo, Paul M. Thompson&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08216&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based Hierarchical Multi-Label Object Detection to Analyze Panoramic Dental X-rays&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ibrahim Ethem Hamamci, Sezgin Er, Enis Simsar, Anjany Sekuboyina, Mustafa Gundogar, Bernd Stadlinger, Albert Mehl, Bjoern Menze&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06500&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AugDiff: Diffusion based Feature Augmentation for Multiple Instance Learning in Whole Slide Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuchen Shao, Liuxi Dai, Yifeng Wang, Haoqian Wang, Yongbing Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06371&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuhang Chen, Baiying Lei, Chi-Man Pun, Shuqiang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06410&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Suhyeon Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05754&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generalized Diffusion MRI Denoising and Super-Resolution using Swin Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amir Sadikov, Jamie Wren-Jarvis, Xinlei Pan, Lanya T. Cai, Pratik Mukherjee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05686&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Importance of Aligning Training Strategy with Evaluation for Diffusion Models in 3D Multiclass Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunguan Fu, Yiwen Li, Shaheer U. Saeed, Matthew J. Clarkson, Yipeng Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06040&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mathpluscode/ImgX-DiffSeg&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Patched Diffusion Models for Unsupervised Anomaly Detection in Brain MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Finn Behrendt, Debayan Bhattacharya, Julia Krüger, Roland Opfer, Alexander Schlaefer&lt;/em&gt; &lt;br&gt; MIDL 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03758&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bi-parametric prostate MR image synthesis using pathology and sequence-conditioned stable diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaheer U. Saeed, Tom Syer, Wen Yan, Qianye Yang, Mark Emberton, Shonit Punwani, Matthew J. Clarkson, Dean C. Barratt, Yipeng Hu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.02094&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dissolving Is Amplifying: Towards Fine-Grained Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jian Shi, Pengyi Zhang, Ni Zhang, Hakim Ghazzai, Yehia Massoud&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.14696&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDM2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tiange Xiang, Mahmut Yurt, Ali B Syed, Kawin Setsompop, Akshay Chaudhari&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03018&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/StanfordMIMI/DDM2&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot-Learning Cross-Modality Data Translation Through Mutual Information Guided Stochastic Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zihao Wang, Yingyu Yang, Maxime Sermesant, Hervé Delingette, Ona Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13743&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Denoising for Low-Dose-CT Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Runyi Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11482&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionCT: Latent Diffusion Model for CT Image Standardization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Md Selim, Jie Zhang, Michael A. Brooks, Ge Wang, Jin Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.08815&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSegDiff-V2: Diffusion based Medical Image Segmentation with Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junde Wu, Rao Fu, Huihui Fang, Yu Zhang, Yanwu Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11798&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The role of noise in denoising models for anomaly detection in medical images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Antanas Kascenas, Pedro Sanchez, Patrick Schrempf, Chaoyang Wang, William Clackett, Shadia S. Mikhael, Jeremy P. Voisey, Keith Goatman, Alexander Weir, Nicolas Pugeault, Sotirios A. Tsaftaris, Alison Q. O&#39;Neil&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.08330&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AntanasKascenas/DenoisingAE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohamed Akrout, Bálint Gyepesi, Péter Holló, Adrienn Poór, Blága Kincső, Stephen Solis, Katrina Cirone, Jeremy Kawahara, Dekker Slade, Latif Abid, Máté Kovács, István Fazekas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.04802&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Annealed Score-Based Diffusion Model for MR Motion Artifact Reduction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyutaek Oh, Jeong Eun Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03027&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models for Generation of Realistic Fully-Annotated Microscopy Image Data Sets&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dennis Eschweiler, Johannes Stegmaier&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10227&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model based Semi-supervised Learning on Brain Hemorrhage Images for Efficient Midline Shift Quantification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shizhan Gong, Cheng Chen, Yuqi Gong, Nga Yan Chan, Wenao Ma, Calvin Hoi-Kwan Mak, Jill Abrigo, Qi Dou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.00409&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SADM: Sequence-Aware Diffusion Model for Longitudinal Medical Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jee Seok Yoon, Chenghao Zhang, Heung-Il Suk, Jia Guo, Xiaoxiao Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08228&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Universal Generative Modeling in Dual-domain for Dynamic MR Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chuanming Yu, Yu Guan, Ziwen Ke, Dong Liang, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07599&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Realistic 3D Brain MRIs Using a Conditional Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wei Peng, Ehsan Adeli, Qingyu Zhao, Kilian M. Pohl&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08034&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Project-MONAI/GenerativeModels/tree/260-add-cdpm-model&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SPIRiT-Diffusion: SPIRiT-driven Score-Based Generative Modeling for Vessel Wall imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chentao Cao, Zhuo-Xu Cui, Jing Cheng, Sen Jia, Hairong Zheng, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11274&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Models beat GANs on Medical Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gustav Müller-Franzes, Jan Moritz Niehues, Firas Khader, Soroosh Tayebi Arasteh, Christoph Haarburger, Christiane Kuhl, Tianci Wang, Tianyu Han, Sven Nebelung, Jakob Nikolas Kather, Daniel Truhn&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07501&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Sample Diffusion Model in Projection Domain for Low-Dose CT Imaging&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bin Huang, Liu Zhang, Shiyu Lu, Boyu Lin, Weiwen Wu, Qiegen Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03630&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Cell Video Synthesis via Optical-Flow Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Manuel Serna-Aguilera, Khoa Luu, Nathaniel Harris, Min Zou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03250&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving dermatology classifiers across populations using images generated by large diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luke W. Sagers, James A. Diao, Matthew Groh, Pranav Rajpurkar, Adewole S. Adamson, Arjun K. Manrai&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13352&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RoentGen: Vision-Language Foundation Model for Chest X-ray Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pierre Chambon, Christian Bluethgen, Jean-Benoit Delbrouck, Rogier Van der Sluijs, Małgorzata Połacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis P. Langlotz, Akshay Chaudhari&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12737&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DOLCE: A Model-Based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaming Liu, Rushil Anirudh, Jayaraman J. Thiagarajan, Stewart He, K. Aditya Mohan, Ulugbek S. Kamilov, Hyojin Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12340&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving 3D Inverse Problems using Pre-trained 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Dohoon Ryu, Michael T. McCann, Marc L. Klasky, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10655&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Patch-Based Denoising Diffusion Probabilistic Model for Sparse-View CT Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Wenxiang Cong, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10388&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain PET Synthesis from MRI Using Joint Probability Distribution of Diffusion Model at Ultrahigh Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xie Taofeng, Cao Chentao, Cui Zhuoxu, Li Fanshi, Wei Zidong, Zhu Yanjie, Li Ye, Liang Dong, Jin Qiyu, Chen Guoqing, Wang Haifeng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.08901&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved HER2 Tumor Segmentation with Subtype Balancing using Deep Generative Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mathias Öttl, Jana Mönius, Matthias Rübner, Carol I. Geppert, Jingna Qiu, Frauke Wilm, Arndt Hartmann, Matthias W. Beckmann, Peter A. Fasching, Andreas Maier, Ramona Erber, Katharina Breininger&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06150&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An unobtrusive quality supervision approach for medical image annotation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sonja Kunzmann, Mathias Öttl, Prathmesh Madhu, Felix Denzinger, Andreas Maier&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.06146&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Medical Diffusion -- Denoising Diffusion Probabilistic Models for 3D Medical Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Firas Khader, Gustav Mueller-Franzes, Soroosh Tayebi Arasteh, Tianyu Han, Christoph Haarburger, Maximilian Schulze-Hagen, Philipp Schad, Sandy Engelhardt, Bettina Baessler, Sebastian Foersch, Johannes Stegmaier, Christiane Kuhl, Sven Nebelung, Jakob Nikolas Kather, Daniel Truhn&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.03364&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generation of Anonymous Chest Radiographs Using Latent Diffusion Models for Training Thoracic Abnormality Classification Systems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Packhäuser, Lukas Folle, Florian Thamm, Andreas Maier&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01323&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spot the fake lungs: Generating Synthetic Medical Images using Neural Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hazrat Ali, Shafaq Murad, Zubair Shah&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00902&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.kaggle.com/datasets/hazrat/awesomelungs&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSegDiff: Medical Image Segmentation with Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junde Wu, Huihui Fang, Yu Zhang, Yehui Yang, Yanwu Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.00611&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion Models via Pre-segmentation Diffusion Sampling for Medical Image Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xutao Guo, Yanwu Yang, Chenfei Ye, Shang Lu, Yang Xiang, Ting Ma&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.17408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multitask Brain Tumor Inpainting with Diffusion Models: A Methodological Report&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pouria Rouzrokh, Bardia Khosravi, Shahriar Faghani, Mana Moassefi, Sanaz Vahdati, Bradley J. Erickson&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12113&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Mayo-Radiology-Informatics-Lab/MBTI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pierre Chambon, Christian Bluethgen, Curtis P. Langlotz, Akshay Chaudhari&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.04133&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anatomically constrained CT image translation for heterogeneous blood vessel segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giammarco La Barbera, Haithem Boussaid, Francesco Maso, Sabine Sarnacki, Laurence Rouet, Pietro Gori, Isabelle Bloch&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.01713&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Low-Dose CT Using Denoising Diffusion Probabilistic Model for 20× Speedup&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjun Xia, Qing Lyu, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.15136&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boah Kim, Yujin Oh, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14566&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conversion Between CT and MRI Images Using Diffusion and Score-Matching Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qing Lyu, Ge Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.12104&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain Imaging Generation with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Walter H. L. Pinaya, Petru-Daniel Tudosiu, Jessica Dafflon, Pedro F da Costa, Virginia Fernandez, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.07162&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PET image denoising based on denoising diffusion probabilistic models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kuang Gong, Keith A. Johnson, Georges El Fakhri, Quanzheng Li, Tinsu Pan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.06167&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Self-Score: Self-Supervised Learning on Score-Based Models for MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuo-Xu Cui, Chentao Cao, Shaonan Liu, Qingyong Zhu, Jing Cheng, Haifeng Wang, Yanjie Zhu, Dong Liang&lt;/em&gt; &lt;br&gt; IEEE TMI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.00835&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Frequency Space Diffusion Models for Accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chentao Cao, Zhuo-Xu Cui, Shaonan Liu, Dong Liang, Yanjie Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.05481&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What is Healthy? Generative Counterfactual Diffusion for Lesion Localization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pedro Sanchez, Antanas Kascenas, Xiao Liu, Alison Q. O&#39;Neil, Sotirios A. Tsaftaris&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.12268&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vios-s/Diff-SCM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Medical Image Translation with Adversarial Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muzaffer Özbey, Salman UH Dar, Hasan A Bedel, Onat Dalmaz, Şaban Özturk, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08208&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adaptive Diffusion Priors for Accelerated MRI Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Salman UH Dar, Şaban Öztürk, Yilmaz Korkmaz, Gokberk Elmas, Muzaffer Özbey, Alper Güngör, Tolga Çukur&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.05876&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Novel Unified Conditional Score-based Generative Framework for Multi-modal Medical Image Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangxi Meng, Yuning Gu, Yongsheng Pan, Nizhuan Wang, Peng Xue, Mengkang Lu, Xuming He, Yiqiang Zhan, Dinggang Shen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.03430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cross-Modal Transformer GAN: A Brain Structure-Function Deep Fusing Framework for Alzheimer&#39;s Disease&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junren Pan, Shuqiang Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.13393&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Deformable Model for 4D Temporal Medical Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boah Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.13295&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/torchddm/ddm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Walter H. L. Pinaya, Mark S. Graham, Robert Gray, Pedro F Da Costa, Petru-Daniel Tudosiu, Paul Wright, Yee H. Mah, Andrew D. MacKinnon, James T. Teo, Rolf Jager, David Werring, Geraint Rees, Parashkev Nachev, Sebastien Ourselin, M. Jorge Cardos&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.03461&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Models for Inverse Problems using Manifold Constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00941&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AnoDDPM: Anomaly Detection with Denoising Diffusion Probabilistic Models using Simplex Noise&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julian Wyatt, Adam Leach, Sebastian M. Schmon, Chris G. Willcocks&lt;/em&gt; &lt;br&gt; CVPR Workshop 2022. [&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Wyatt_AnoDDPM_Anomaly_Detection_With_Denoising_Diffusion_Probabilistic_Models_Using_Simplex_CVPRW_2022_paper.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Julian-Wyatt/AnoDDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Swiss Army Knife for Image-to-Image Translation: Multi-Task Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Robin Sandkühler, Florentin Bieder, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.02641&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MR Image Denoising and Super-Resolution Using Regularized Reverse Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Eun Sun Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.12621&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Medical Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julia Wolleb, Florentin Bieder, Robin Sandkühler, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.04306&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JuliaWolleb/diffusion-anomaly&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards performant and reliable undersampled MR reconstruction via diffusion model sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cheng Peng, Pengfei Guo, S. Kevin Zhou, Vishal Patel, Rama Chellappa&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.04292&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cpeng93/diffuserecon&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutong Xie, Quanzheng Li&lt;/em&gt; &lt;br&gt; MICCAI 2022. [&lt;a href=&#34;https://arxiv.org/abs/2203.03623&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Theodore-PKU/MC-DDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Mar 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MRI Reconstruction via Data Driven Markov Chain with Joint Uncertainty Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guanxiong Luo, Martin Heide, Martin Uecker&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2202.01479&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mrirecon/spreco&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Feb 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Denoising of Retinal OCT with Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dewei Hu, Yuankai K. Tao, Ipek Oguz&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2201.11760&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DeweiHu/OCT_DDPM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Jan 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Byeongsu Sim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.05146&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Solving Inverse Problems in Medical Imaging with Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yang Song, Liyue Shen, Lei Xing, Stefano Ermon&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2021. [&lt;a href=&#34;https://arxiv.org/abs/2111.08005&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yang-song/score_inverse_problems&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-based diffusion models for accelerated MRI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jong chul Ye&lt;/em&gt; &lt;br&gt; MIA 2021. [&lt;a href=&#34;https://arxiv.org/abs/2110.05243&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HJ-harry/score-MRI&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Oct 2021&lt;/p&gt; &#xA;&lt;h3&gt;Multi-modal Learning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;IterInv: Iterative Inversion for Pixel-Level T2I Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chuanming Tang, Kai Wang, Joost van de Weijer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19540&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VideoCrafter1: Open Diffusion Models for High-Quality Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, Ying Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19512&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPRESS: Evaluating the Resilience of Imperceptible Perturbations Against Unauthorized Data Usage in Diffusion-Based Generative AI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bochuan Cao, Changjiang Li, Ting Wang, Jinyuan Jia, Bo Li, Jinghui Chen&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19248&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi, Chun Yuan, Ying Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19784&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seeing Through the Conversation: Audio-Visual Speech Separation based on Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suyeon Lee, Chaeyoung Jung, Youngjoon Jang, Jaehun Kim, Joon Son Chung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19581&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-3D with Classifier Score Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, Xiaojuan Qi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19415&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Customizing 360-Degree Panoramas through Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hai Wang, Xiaoyu Xiang, Yuchen Fan, Jing-Hao Xue&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.18840&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinghui Li, Jingyi Lu, Kai Han, Victor Prisacariu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17569&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seyedmorteza Sadat, Jakob Buhmann, Derek Bradely, Otmar Hilliges, Romann M. Weber&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17347&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Iterative Refinement with Diffusion Models for Video Grounding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiao Liang, Tao Shi, Yaoyuan Liang, Te Tao, Shao-Lun Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17189&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, Yaniv Leviathan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16656&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir Patel, Jonathan Frankle, Cory Stephenson, Volodymyr Kuleshov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16825&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yixin Wu, Ning Yu, Michael Backes, Yun Shen, Yang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16613&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianyi Lu, Xing Zhang, Jiaxi Gu, Hang Xu, Renjing Pei, Songcen Xu, Zuxuan Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16400&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adapt Anything: Tailor Any Image Classifiers across Domains And Categories Using Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weijie Chen, Haoyu Wang, Shicai Yang, Lei Zhang, Wei Wei, Yanning Zhang, Luojun Lin, Di Xie, Yueting Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16573&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text Guided Video Editing Competition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, Rui He, Feng Hu, Junhua Hu, Hai Huang, Hanyu Zhu, Xu Cheng, Jie Tang, Mike Zheng Shou, Kurt Keutzer, Forrest Iandola&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16003&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language-driven Scene Synthesis using Multi-conditional Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;An Vuong, Minh Nhat Vu, Toan Tien Nguyen, Baoru Huang, Dzung Nguyen, Thieu Vo, Anh Nguyen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.15948&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.15169&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://haonanqiu.com/projects/FreeNoise.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 23 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SyncFusion: Multimodal Onset-synchronized Video-to-Audio Foley Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marco Comunità, Riccardo F. Gramaccioni, Emilian Postolache, Emanuele Rodolà, Danilo Comminiello, Joshua D. Reiss&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.15247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Matryoshka Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Josh Susskind, Navdeep Jaitly&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.15111&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Large Language Models can Share Images, Too!&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Young-Jun Lee, Jonghwan Hyeon, Ho-Jin Choi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.14804&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shawn Shan, Wenxin Ding, Josephine Passananti, Haitao Zheng, Ben Y. Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13828&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, Kangxue Yin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13772&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DPM-Solver-v3: Improved Diffusion ODE Solver with Empirical Model Statistics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaiwen Zheng, Cheng Lu, Jianfei Chen, Jun Zhu&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13268&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/dpmv3/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Localizing and Editing Knowledge in Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samyadeep Basu, Nanxuan Zhao, Vlad Morariu, Soheil Feizi, Varun Manjunatha&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13730&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TapMo: Shape-aware Motion Generation of Skeleton-free Characters&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaxu Zhang, Shaoli Huang, Zhigang Tu, Xin Chen, Xiaohang Zhan, Gang Yu, Ying Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12678&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sihan Xu, Ziqiao Ma, Yidong Huang, Honglak Lee, Joyce Chai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13165&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamSpace: Dreaming Your Room Space with Text-Driven Panoramic Texture Propagation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bangbang Yang, Wenqi Dong, Lin Ma, Wenbo Hu, Xiao Liu, Zhaopeng Cui, Yuewen Ma&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13119&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EMIT-Diff: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheyuan Zhang, Lanhong Yao, Bin Wang, Debesh Jha, Elif Keles, Alpay Medetalibeyoglu, Ulas Bagci&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12868&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diverse Diffusion: Enhancing Image Diversity in Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mariia Zameshina, Olivier Teytaud, Laurent Najman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12583&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, Ying Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12190&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei Zhang, Jian Zhang, Li Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11784&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language Agents for Detecting Implicit Stereotypes in Text-to-image Models at Scale&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qichao Wang, Tian Bian, Yian Yin, Tingyang Xu, Hong Cheng, Helen M. Meng, Zibin Zheng, Liang Chen, Bingzhe Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11778&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Elucidating The Design Space of Classifier-Guided Diffusion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiajun Ma, Tianyang Hu, Wenjia Wang, Jiacheng Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11311&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alexmaols/elucd&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siqi Kou, Lei Gan, Dequan Wang, Chongxuan Li, Zhijie Deng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11142&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dhruba Ghosh, Hanna Hajishirzi, Ludwig Schmidt&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11513&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Training-free Open-world Segmentation via Image Prompting Foundation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lv Tang, Peng-Tao Jiang, Hao-Ke Xiao, Bo Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10912&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, Xiangyu Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10769&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://rq-wu.github.io/projects/LAMP/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/RQ-Wu/LAMP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Scene Graph Conditioning in Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Frank Fundel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10338&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/FrankFundel/SGCond&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo Li, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10012&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, Sergey Levine&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10639&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ViPE: Visualise Pretty-much Everything&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hassan Shahmohammadi, Adhiraj Ghosh, Hendrik P. A. Lensch&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10543&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TOSS:High-quality Text-guided Novel View Synthesis from a Single Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao Qi, Tianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang, Heung-Yeung Shum&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10644&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, Peter Wonka&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10640&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LOVECon: Text-driven Training-Free Long Video Editing with ControlNet&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenyi Liao, Zhijie Deng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09711&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via Denoised Score Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, Wayne Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09458&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Maya Okawa, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka&lt;/em&gt; &lt;br&gt; ICML Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09336&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anton Baryshnikov, Max Ryabinin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Making Multimodal Generation Easier: When Diffusion Models Meet LLMs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiangyu Zhao, Bo Liu, Qijiong Liu, Guangyuan Shi, Xiao-Ming Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;R&amp;amp;B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiayu Xiao, Liang Li, Henglei Lv, Shuhui Wang, Qingming Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08872&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeltaSpace: A Semantic-aligned Feature Space for Flexible Text-guided Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yueming Lyu, Kang Zhao, Bo Peng, Yue Jiang, Yingya Zhang, Jing Dong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08785&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OmniControl: Control Any Joint at Any Time for Human Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, Huaizu Jiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08580&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://neu-vi.github.io/omnicontrol/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperHuman: Hyper-Realistic Human Generation with Latent Structural Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xian Liu, Jian Ren, Aliaksandr Siarohin, Ivan Skorokhodov, Yanyu Li, Dahua Lin, Xihui Liu, Ziwei Liu, Sergey Tulyakov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08579&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://snap-research.github.io/HyperHuman/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/snap-research/HyperHuman&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GaussianDreamer: Fast Generation from Text to 3D Gaussian Splatting with Point Cloud Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, Xinggang Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08529&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MotionDirector: Motion Customization of Text-to-Video Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, Mike Zheng Shou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08465&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interpretable Diffusion via Information Decomposition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xianghao Kong, Ollie Liu, Han Li, Dani Yogatama, Greg Ver Steeg&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07972&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaofan Li, Yifu Zhang, Xiaoqing Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07771&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://drivingdiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/shalfun/DrivingDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun, Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng Chen, Ying Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07702&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yingqinghe.github.io/scalecrafter/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YingqingHe/ScaleCrafter&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ConditionVideo: Training-Free Condition-Guided Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Peng, Xinyuan Chen, Yaohui Wang, Chaochao Lu, Yu Qiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07697&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeqiang Lai, Xizhou Zhu, Jifeng Dai, Yu Qiao, Wenhai Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07653&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Concept T2I-Zero: Tweaking Only The Text Embeddings and Nothing Else&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hazarapet Tunanyan, Dejia Xu, Shant Navasardyan, Zhangyang Wang, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07419&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Uni-paint: A Unified Framework for Multimodal Image Inpainting with Pretrained Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shiyuan Yang, Xiaodong Chen, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07222&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ObjectComposer: Consistent Generation of Multiple Objects Without Fine-tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alec Helbling, Evan Montoya, Duen Horng Chau&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06968&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyang Zhang, Shiwei Li, Yuanxun Lu, Tian Fang, David McKinnon, Yanghai Tsin, Long Quan, Yao Yao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06347&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Compositional Text-to-image Generation with Large Vision-Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Song Wen, Guian Fang, Renrui Zhang, Peng Gao, Hao Dong, Dimitris Metaxas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06311&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Geom-Erasing: Geometry-Driven Removal of Implicit Concept in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhili Liu, Kai Chen, Yifan Zhang, Jianhua Han, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James Kwok&lt;/em&gt; &lt;br&gt; arXiv 2023 [&lt;a href=&#34;https://arxiv.org/abs/2310.05873&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, Sen He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05922&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05737&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/magvit2-pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IPDreamer: Appearance-Controllable 3D Object Generation with Image Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang, Jianzhuang Liu, Baochang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05375&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models as Masked Audio-Video Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elvis Nunez, Yanzi Jin, Mohammad Rastegari, Sachin Mehta, Maxwell Horton&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03937&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Aligning Text-to-Image Diffusion Models with Reward Backpropagation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, Katerina Fragkiadaki&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03739&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chuan Fang, Xiaotao Hu, Kunming Luo, Ping Tan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03602&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MedSyn: Text-guided Anatomy-aware Synthesis of High-Fidelity 3D CT Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yanwu Xu, Li Sun, Wei Peng, Shyam Visweswaran, Kayhan Batmanghelich&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03559&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, Denis Dimitrov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03502&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Realistic Speech-to-Face Generation with Speech-Conditioned Latent Diffusion Model with Face Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinting Wang, Li Liu, Jun Wang, Hei Victor Cheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03363&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, Yong-Jin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02977&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://t3bench.com/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/THU-LYJ-Lab/T3Bench&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Dermatoscopic Lesion Segmentation via Diffusion Models with Visual and Textual Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shiyi Du, Xiaosong Wang, Yongyi Lu, Yuyin Zhou, Shaoting Zhang, Alan Yuille, Kang Li, Zongwei Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02906&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siyuan Yang, Lu Zhang, Liqian Ma, Yu Liu, JingJing Fu, You He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02848&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jangho Park, Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02712&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weiyu Li, Rui Chen, Xuelin Chen, Ping Tan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02596&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sweetdreamer3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EditVal: Benchmarking Diffusion Based Text-Guided Image Editing Methods&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samyadeep Basu, Mehrdad Saberi, Shweta Bhardwaj, Atoosa Malemir Chegini, Daniela Massiceti, Maziar Sanjabi, Shell Xu Hu, Soheil Feizi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02426&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://deep-ml-research.github.io/editval/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/deep-ml-research/editval_code&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yingqian Cui, Jie Ren, Yuping Lin, Han Xu, Pengfei He, Yue Xing, Wenqi Fan, Hui Liu, Jiliang Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02401&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Amazing Combinatorial Creation: Acceptable Swap-Sampling for Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jun Li, Zedong Zhang, Jian Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01819&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://asst2i.github.io/anon/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 3 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shivang Chopra, Suraj Kothawade, Houda Aynaou, Aman Chadha&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01701&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kangfu Mei, Mauricio Delbracio, Hossein Talebi, Zhengzhong Tu, Vishal M. Patel, Peyman Milanfar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01407&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, Qiang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01506&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt-tuning latent diffusion models for inverse problems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyungjin Chung, Jong Chul Ye, Peyman Milanfar, Mauricio Delbracio&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01110&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yongchan Kwon, Eric Wu, Kevin Wu, James Zou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00902&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyeonho Jeong, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01107&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Ground-A-Video/Ground-A-Video&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Music- and Lyrics-driven Dance Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjie Yin, Qingyuan Yao, Yi Yu, Hang Yin, Danica Kragic, Mårten Björkman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00455&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Gaetan Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, Yong-jin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00434&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://raineggplant.github.io/DiffPoseTalk/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PixArt-$\alpha$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00426&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pixart-alpha.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, Ahmed M. Alaa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00390&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Anoop Cherian, Suhas Lohit, Ye Wang, Toshiaki Koike-Akino, Vishal M. Patel, Tim K. Marks&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00224&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Directly Fine-Tuning Diffusion Models on Differentiable Rewards&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kevin Clark, Paul Vicol, Kevin Swersky, David J Fleet&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.17400&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-image Alignment for Diffusion-based Perception&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Neehar Kondapaneni, Markus Marks, Manuel Knott, Rogério Guimarães, Pietro Perona&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00031&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLM-grounded Video Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, Boyi Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.17444&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://llm-grounded-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TonyLianLong/LLM-groundedDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiancheng Huang, Yifan Liu, Jin Qin, Shifeng Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.16608&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CCEdit: Creative and Controllable Video Editing via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, Baining Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.16496&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, Mike Zheng Shou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.15818&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.15664&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamCom: Finetuning Text-guided Inpainting Model for Image Composition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lingxiao Lu, Bo Zhang, Li Niu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.15508&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Using Generated Privileged Information by Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rafael-Edy Menadil, Mariana-Iuliana Georgescu, Radu Tudor Ionescu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.15238&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.15103&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vchitect.github.io/LaVie-project/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Using Generated Privileged Information by Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rafael-Edy Menadil, Mariana-Iuliana Georgescu, Radu Tudor Ionescu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.15238&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FEC: Three Finetuning-free Methods to Enhance Consistency for Real Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Songyan Chen, Jiancheng Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14934&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shin-Ying Yeh, Yu-Guan Hsieh, Zhidong Gao, Bernard B W Yang, Giyeong Oh, Yanmin Gong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14859&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-image guided Diffusion Model for generating Deepfake celebrity interactions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunzhuo Chen, Nur Al Hasan Haldar, Naveed Akhtar, Ajmal Mian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14751&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, Sibei Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14494&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tiep Le, Vasudev Lal, Phillip Howard&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14356&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Object Counting with Language-Vision Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyi Xu, Hieu Le, Dimitris Samaras&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.13097&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cvlab-stonybrook/zero-shot-counting&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MosaicFusion: Diffusion Models as Data Augmenters for Large Vocabulary Instance Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiahao Xie, Wei Li, Xiangtai Li, Ziwei Liu, Yew Soon Ong, Chen Change Loy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.13042&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Jiahao000/MosaicFusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DurIAN-E: Duration Informed Attention Network For Expressive Text-to-Speech Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu Gu, Yianrao Bian, Guangzhi Lei, Chao Weng, Dan Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.12792&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FreeU: Free Lunch in Diffusion U-Net&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenyang Si, Ziqi Huang, Yuming Jiang, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11497&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Investigating Personalization Methods in Text to Music Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Manos Plitsis, Theodoros Kouzelis, Georgios Paraskevopoulos, Vassilis Katsouros, Yannis Panagakis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11140&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zelaki.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerating Diffusion-Based Text-to-Audio Generation with Consistency Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yatong Bai, Trung Dang, Dung Tran, Kazuhito Koishida, Somayeh Sojoudi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.10740&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Forgedit: Text Guided Image Editing via Learning and Forgetting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shiwen Zhang, Shuai Xiao, Weilin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.10556&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/witcherofresearch/Forgedit&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What is a Fair Diffusion Model? Designing Generative Text-To-Image Models to Incorporate Various Worldviews&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zoe De Simone, Angie Boggust, Arvind Satyanarayan, Ashia Wilson&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.09944&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Causal-Story: Local Causal Attention Utilizing Parameter-Efficient Tuning For Visual Story Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianyi Song, Jiuxin Cao, Kun Wang, Bo Liu, Xiaofeng Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.09553&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Progressive Text-to-Image Diffusion with Soft Latent Direction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;YuTeng Ye, Jiale Cai, Hang Zhou, Guanwen Li, Youjia Zhang, Zikai Song, Chenxing Gao, Junqing Yu, Wei Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.09466&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yihao Zhi, Xiaodong Cun, Xuelin Chen, Xi Shen, Wen Guo, Shaoli Huang, Shenghua Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.09294&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Reo Shimizu, Ryuichi Yamamoto, Masaya Kawamura, Yuma Shirahata, Hironori Doi, Tatsuya Komatsu, Kentaro Tachibana&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.08140&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ju-Chieh Chou, Chung-Ming Chien, Karen Livescu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.08030&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;James Burgess, Kuan-Chieh Wang, Serena Yeung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07986&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jmhb0/view_neti&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Image Models for Counterfactual Explanations: a Black-Box Approach&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Jeanneret, Loïc Simon, Frédéric Jurie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07944&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Large-Vocabulary 3D Diffusion Model with Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziang Cao, Fangzhou Hong, Tong Wu, Liang Pan, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07920&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ziangcao0312.github.io/difftf_pages/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ziangcao0312/DiffTF&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffTalker: Co-driven audio-image diffusion for talking faces via intermediate landmarks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zipeng Qi, Xulong Zhang, Ning Cheng, Jing Xiao, Jianzong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07509&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion models for audio semantic communication&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eleonora Grassucci, Christian Marinoni, Andrea Rodriguez, Danilo Comminiello&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07195&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Namhyuk Ahn, Junsoo Lee, Chunggi Lee, Kunhee Kim, Daesik Kim, Seung-Hun Nam, Kibeom Hong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.06933&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhichao Wu, Qiulin Li, Sixing Liu, Qun Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.06787&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, Qiang Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.06380&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gnobitab/InstaFlow&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yin Wang, Zhiying Leng, Frederick W. B. Li, Shun-Cheng Wu, Xiaohui Liang&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.06284&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, Wei-Chen Chiu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.06135&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, Min Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05793&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://photoverse2d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 11 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PAI-Diffusion: Constructing and Serving a Family of Open Chinese Diffusion Models for Text-to-image Synthesis on the Cloud&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chengyu Wang, Zhongjie Duan, Bingyan Liu, Xinyi Zou, Cen Chen, Kui Jia, Jun Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05534&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based Co-Speech Gesture Generation Using Joint Text and Audio Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anna Deichler, Shivam Mehta, Simon Alexanderson, Jonas Beskow&lt;/em&gt; &lt;br&gt; ICMI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05455&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Effective Real Image Editing with Accelerated Iterative Diffusion Inversion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhihong Pan, Riccardo Gherardi, Xiufeng Xie, Stephen Huang&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.04907&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prefix-diffusion: A Lightweight Diffusion Model for Diverse Image Captioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guisheng Liu, Yi Li, Zhengcong Fei, Haiyan Fu, Xiangyang Luo, Yanqing Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.04965&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-driven Editing of 3D Scenes without Retraining&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuangkang Fang, Yufeng Wang, Yi Yang, Yi-Hsuan Tsai, Wenrui Ding, Shuchang Zhou, Ming-Hsuan Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.04917&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yujin Jeong, Wonjeong Ryoo, Seunghyun Lee, Dabin Seo, Wonmin Byeon, Sangpil Kim, Jinkyu Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.04509&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Create Your World: Lifelong Text-to-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gan Sun, Wenqi Liang, Jiahua Dong, Jun Li, Zhengming Ding, Yang Cong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.04430&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yupeng Zhou, Daquan Zhou, Zuo-Liang Zhu, Yaxing Wang, Qibin Hou, Jiashi Feng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.04399&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoEController: Instruction-based Arbitrary Image Manipulation with Mixture-of-Expert Controllers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sijia Li, Chen Chen, Haonan Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.04372&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://oppo-mente-lab.github.io/moe_controller/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changming Xiao, Qi Yang, Feng Zhou, Changshui Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.04109&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructDiffusion: A Generalist Modeling Interface for Vision Tasks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, Baining Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03895&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gengzigang.github.io/instructdiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cientgu/InstructDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-feature diffusion for audio-visual few-shot learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Otniel-Bogdan Mercea, Thomas Hummel, A. Sophia Koepke, Zeynep Akata&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03869&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sungwon Hwang, Junha Hyung, Jaegul Choo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03550&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://text2control3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang Jiang, Hang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03549&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SyncDreamer: Generating Multiview-consistent Images from a Single-view Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, Wenping Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03453&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://liuyuan-pal.github.io/SyncDreamer/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/liuyuan-pal/SyncDreamer&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MCM: Multi-condition Motion Synthesis Framework for Multi-scenario&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeyu Ling, Bo Han, Yongkang Wong, Mohan Kangkanhalli, Weidong Geng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03031&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinglong Wang, Xiawei Li, Jing Zhang, Qingyuan Xu, Qin Zhou, Qian Yu, Lu Sheng, Dong Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.02773&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating Realistic Images from In-the-wild Sounds&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Taegyeong Lee, Jeonghun Kang, Hyeonyu Kim, Taehwan Kim&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.02405&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative-based Fusion Mechanism for Multi-Modal Tracking&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Xiao-Jun Wu, Josef Kittler&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01728&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuyang Liu, Siteng Huang, Yachen Kang, Honggang Chen, Donglin Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01141&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bridge Diffusion Model: bridge non-English language-native text-to-image diffusion model with English communities&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shanyuan Liu, Dawei Leng, Yuhui Yin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00952&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanshu Yan, Jun Hao Liew, Long Mai, Shanchuan Lin, Jiashi Feng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00908&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Iterative Multi-granular Image Editing using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;K J Joseph, Prateksha Udhayanan, Tripti Shukla, Aishwarya Agarwal, Srikrishna Karanam, Koustava Goswami, Balaji Vasan Srinivasan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00613&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuGen: Adaptable Approach for Generating Labeled Image Datasets using Stable Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michael Shenoda, Edward Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00248&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PathLDM: Text conditioned Latent Diffusion Model for Histopathology&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Srikar Yellapragada, Alexandros Graikos, Prateek Prasanna, Tahsin Kurc, Joel Saltz, Dimitris Samaras&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00748&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding, Jingdong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00398&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Detecting Out-of-Context Image-Caption Pairs in News: A Counter-Intuitive Method&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eivind Moholdt, Sohail Ahmed Khan, Duc-Tien Dang-Nguyen&lt;/em&gt; &lt;br&gt; CBMI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16611&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Any-Size-Diffusion: Toward Efficient Text-Driven Synthesis for Any-Size HD Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qingping Zheng, Yuanfan Guo, Jiankang Deng, Jianhua Han, Ying Li, Songcen Xu, Hang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16582&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MVDream: Multi-view Diffusion for 3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, Xiao Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16512&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intriguing Properties of Diffusion Models: A Large-Scale Dataset for Evaluating Natural Attack Capability in Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Takami Sato, Justin Yue, Nanze Chen, Ningfei Wang, Qi Alfred Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15692&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionVMR: Diffusion Model for Video Moment Retrieval&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Henghao Zhao, Kevin Qinghong Lin, Rui Yan, Zechao Li&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15109&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;C2G2: Controllable Co-speech Gesture Generation with Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Longbin Ji, Pengfei Wei, Yi Ren, Jinglin Liu, Chen Zhang, Xiang Yin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15016&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;360-Degree Panorama Generation from Few Unregistered NFoV Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jionghao Wang, Ziyu Chen, Jun Ling, Rong Xie, Li Song&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14686&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/shanemankiw/Panodiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Priority-Centric Human Motion Generation in Discrete Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14480&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SketchDreamer: Interactive Text-Augmented Creative Sketch Ideation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyu Qu, Tao Xiang, Yi-Zhe Song&lt;/em&gt; &lt;br&gt; BMVC 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14191&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/WinKawaks/SketchDreamer&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Tat-Seng Chua&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13812&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://haofei.vip/Dysen-VDM/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ORES: Open-vocabulary Responsible Visual Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minheng Ni, Chenfei Wu, Xiaodong Wang, Shengming Yin, Lijuan Wang, Zicheng Liu, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13785&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The DiffuseStyleGesture+ entry to the GENEA Challenge 2023&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sicheng Yang, Haiwei Xue, Zhensong Zhang, Minglei Li, Zhiyong Wu, Xiaofei Wu, Songcen Xu, Zonghong Dai&lt;/em&gt; &lt;br&gt; ICMI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13879&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YoungSeng/DiffuseStyleGesture/tree/DiffuseStyleGesturePlus/BEAT-TWH-main&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng Li, Zeng Zhao, Zhipeng Hu, Changjie Fan, Xin Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13223&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unified Concept Editing in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzyńska, David Bau&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14761&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://unified.baulab.info/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rohitgandikota/unified-concept-editing&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dense Text-to-Image Generation with Attention Modulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, Jun-Yan Zhu&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.12964&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/naver-ai/DenseDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yupu Yao, Shangqi Deng, Zihan Cao, Harry Zhang, Liang-Jian Deng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.12605&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Manipulating Embeddings of Stable Diffusion Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Niklas Deckers, Julia Peters, Martin Potthast&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.12059&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Se Jin Park, Joanna Hong, Minsu Kim, Yong Man Ro&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05934&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IT3D: Improved Text-to-3D Generation with Explicit View Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, Guosheng Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.11473&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/buaacyw/IT3D-text-to-3D&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCloth: Diffusion Based Garment Synthesis and Manipulation via Structural Cross-modal Semantic Alignment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xujie Zhang, Binbin Yang, Michael C. Kampffmeyer, Wenqing Zhang, Shiyue Zhang, Guansong Lu, Liang Lin, Hang Xu, Xiaodan Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.11206&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MusicJam: Visualizing Music Insights via Generated Narrative Illustrations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chuer Chen, Nan Cao, Jiani Hou, Yi Guo, Yulei Zhang, Yang Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.11329&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TADA! Text to Animatable Digital Avatars&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang, Yangyi Huang, Justus Thies, Michael J. Black&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10899&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EVE: Efficient zero-shot text-based Video Editing with Depth Map Guidance and Temporal Consistency Constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutao Chen, Xingning Dong, Tian Gan, Chunluan Zhou, Ming Yang, Qingpei Guo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10648&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Backdooring Textual Inversion for Concept Censorship&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutong Wu, Jie Zhang, Florian Kerschbaum, Tianwei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10718&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://concept-censorship.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/concept-censorship/concept-censorship.github.io/tree/main/code&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AltDiffusion: A Multilingual Text-to-Image Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fulong Ye, Guang Liu, Xinya Wu, Ledell Wu&lt;/em&gt; &lt;br&gt; AAAI 2024. [&lt;a href=&#34;https://arxiv.org/abs/2308.09991&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/superhero-7/AltDiffuson&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Runhui Huang, Jianhua Han, Guansong Lu, Xiaodan Liang, Yihan Zeng, Wei Zhang, Hang Xu&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09306&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xudong Xu, Zhaoyang Lyu, Xingang Pan, Bo Dai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09278&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sheldontsui.github.io/projects/Matlaber&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Soumik Mukhopadhyay, Saksham Suri, Ravi Teja Gadde, Abhinav Shrivastava&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09716&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://soumik-kanad.github.io/diff2lip/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/soumik-kanad/diff2lip&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guide3D: Create 3D Avatars from Text and Image Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09705&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language-Guided Diffusion Model for Visual Grounding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sijia Chen, Baochun Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09599&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SimDA: Simple Diffusion Adapter for Efficient Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, Yu-Gang Jiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09710&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://chenhsing.github.io/SimDA/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StableVideo: Text-driven Consistency-aware Diffusion Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenhao Chai, Xun Guo, Gaoang Wang, Yan Lu&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09592&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rese1f/StableVideo&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edit Temporal-Consistent Videos with Image Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanzhi Wang, Yong Li, Xin Liu, Anbo Dai, Antoni Chan, Zhen Cui&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09091&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Watch Your Steps: Local Image and Scene Editing by Text Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G. Derpanis, Igor Gilitschenski&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.08947&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ashmrz.github.io/WatchYourSteps/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minho Park, Jooyeol Yun, Seunghwan Choi, Jaegul Choo&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.08157&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pmh9960.github.io/research/GCDP/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/pmh9960/GCDP/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.08089&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.microsoft.com/en-us/research/project/dragnuwa/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dual-Stream Diffusion Net for Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Binhui Liu, Xin Liu, Anbo Dai, Zhiyong Zeng, Zhen Cui, Jian Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.08316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jeongsoo Choi, Joanna Hong, Yong Man Ro&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07787&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SGDiff: A Style Guided Diffusion Model for Fashion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengwentai Sun, Yanghong Zhou, Honghong He, P. Y. Mok&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07605&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bosheng Qin, Wentao Ye, Qifan Yu, Siliang Tang, Yueting Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07749&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Based Augmentation for Captioning and Retrieval in Cultural Heritage&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dario Cioni, Lorenzo Berlincioni, Federico Becattini, Alberto del Bimbo&lt;/em&gt; &lt;br&gt; ICCV Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07151&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Jurassic World Remake: Bringing Ancient Fossils Back to Life via Zero-Shot Long Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexander Martin, Haitian Zheng, Jie An, Jiebo Luo&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniBrain: Unify Image Reconstruction and Captioning All in One Diffusion Model from Human Brain Activity&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weijian Mai, Zhijun Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07428&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Free-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Junhao Zhang, Mutian Xu, Chuhui Xue, Wenqing Zhang, Xiaoguang Han, Song Bai, Mike Zheng Shou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06739&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, Wei Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06721&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ip-adapter.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Binbin Yang, Yi Luo, Ziliang Chen, Guangrun Wang, Xiaodan Liang, Liang Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06713&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ModelScope Text-to-Video Technical Report&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, Shiwei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06571&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weijia Wu, Yuzhong Zhao, Hao Chen, Yuchao Gu, Rui Zhao, Yefei He, Hong Zhou, Mike Zheng Shou, Chunhua Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06160&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://weijiawu.github.io/DatasetDM_page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/showlab/DatasetDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, Wangmeng Zuo&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06038&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/chunmeifeng/DiffTPT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Masked-Attention Diffusion Guidance for Spatially Controlling Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuki Endo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06027&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Zhang, Naye Ji, Fuxing Gao, Siyuan Zhao, Zhaohan Wang, Shunman Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.05995&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Text-driven Physically Interpretable Face Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yapeng Meng, Songru Yang, Xu Hu, Rui Zhao, Lincheng Li, Zhenwei Shi, Zhengxia Zou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.05976&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PromptPaint: Steering Text-to-Image Generation Through Paint Medium-like Interactions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;John Joon Young Chung, Eytan Adar&lt;/em&gt; &lt;br&gt; UIST 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.05184&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, Tat-Seng Chua&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.05095&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://layoutllm-t2i.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 9 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daiheng Gao, Xu Chen, Xindi Zhang, Qi Wang, Ke Sun, Bang Zhang, Liefeng Bo, Qixing Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.04288&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yizhuo Lu, Changde Du, Qiongyi zhou, Dianpeng Wang, Huiguang He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.04249&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FLIRT: Feedback Loop In-context Red Teaming&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, Rahul Gupta&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.04265&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSynth: Latent In-Iteration Deflickering for Realistic Video Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhongjie Duan, Lizhou You, Chengyu Wang, Cen Chen, Ziheng Wu, Weining Qian, Jun Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.03463&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://anonymous456852.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarVerse: High-quality &amp;amp; Stable 3D Avatar Creation from Text and Pose&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, Min Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.03610&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://avatarverse3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 7 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Scene-Text to Scene-Text Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Onkar Susladkar, Prajwal Gatti, Anand Mishra&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.03024&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijie Wu, Yaonan Wang, Mingtao Feng, He Xie, Ajmal Mian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02874&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ConceptLab: Creative Generation using Diffusion Prior Constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elad Richardson, Kfir Goldberg, Yuval Alaluf, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02669&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://kfirgoldberg.github.io/ConceptLab/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/kfirgoldberg/ConceptLab&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianxin Lin, Peng Xiao, Yijun Wang, Rongju Zhang, Xiangxiang Zeng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01655&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhao Yang, Bing Su, Ji-Rong Wen&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01850&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yangzhao1230/PCMDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reverse Stable Diffusion: What prompt was used to generate this image?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, Mubarak Shah&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01472&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Degeneration-Tuning: Using Scrambled Grid shield Unwanted Concepts from Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixuan Ni, Longhui Wei, Jiacheng Li, Siliang Tang, Yueting Zhuang, Qi Tian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02552&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yasheng Sun, Yifan Yang, Houwen Peng, Yifei Shen, Yuqing Yang, Han Hu, Lili Qiu, Hideki Koike&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.00906&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Bias Amplification Paradox in Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Preethi Seshadri, Sameer Singh, Yanai Elazar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.00755&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.16489&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JJ-Vice/BAGM&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://ieee-dataport.org/documents/marketable-foods-mf-dataset&#34;&gt;Dataset&lt;/a&gt;] &lt;br&gt; 31 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MobileVidFactory: Automatic Diffusion-Based Social Media Video Generation for Mobile Devices from Text&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junchen Zhu, Huan Yang, Wenjing Wang, Huiguo He, Zixi Tuo, Yongsheng Yu, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, Jianlong Fu, Jiebo Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.16371&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, Chenliang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.00122&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contrastive Conditional Latent Diffusion for Audio-visual Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Mao, Jing Zhang, Mochu Xiang, Yunqiu Lv, Yiran Zhong, Yuchao Dai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.16579&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen Zhao, Haocheng Feng, Jingtuo Liu, Errui Ding&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.16183&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Ting Lan, Kan Ren, Yansen Wang, Wei-Long Zheng, Dongsheng Li, Bao-Liang Lu, Lili Qiu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02510&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhihao Hu, Dong Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.14073&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vcg-aigc.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin Wang, Fan Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.13908&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual Instruction Inversion: Image Editing via Visual Prompting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Thao Nguyen, Yuheng Li, Utkarsh Ojha, Yong Jae Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.14331&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://thaoshibe.github.io/visii/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/thaoshibe/visii&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Composite Diffusion | whole &amp;gt;= \Sigma parts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vikram Jamwal, Ramaneswaran S&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.13720&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fashion Matrix: Editing Photos by Just Talking&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zheng Chong, Xujie Zhang, Fuwei Zhao, Zhenyu Xie, Xiaodan Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.13240&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zheng-chong.github.io/FashionMatrix/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Zheng-Chong/FashionMatrix&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, Youngjung Uh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.12868&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anant Khandelwal&lt;/em&gt; &lt;br&gt; ICCV Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.00135&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jian Ma, Junhao Liang, Chen Chen, Haonan Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.11410&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://oppo-mente-lab.github.io/subject_diffusion/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/OPPO-Mente-Lab/Subject-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Divide &amp;amp; Bind Your Attention for Improved Generative Semantic Nursing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yumeng Li, Margret Keuper, Dan Zhang, Anna Khoreva&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.10864&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/divide-and-bind&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiachun Pan, Jun Hao Liew, Vincent Y. F. Tan, Jiashi Feng, Hanshu Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.10711&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, Mike Zheng Shou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.10816&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Sierkinhane/BoxDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Layer: Layered Image Generation using Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinyang Zhang, Wentian Zhao, Xin Lu, Jeff Chien&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.09781&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FABRIC: Personalizing Diffusion Models with Iterative Feedback&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dimitri von Rütte, Elisabetta Fedele, Jonathan Thomm, Lukas Wolf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.10159&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TokenFlow: Consistent Diffusion Features for Consistent Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michal Geyer, Omer Bar-Tal, Shai Bagon, Tali Dekel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.10373&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diffusion-tokenflow.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omerbt/TokenFlow&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Diffusion Segmentation Model for Object Segmentation from Manipulation Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yui Iioka, Yu Yoshida, Yuiga Wada, Shumpei Hatanaka, Komei Sugiura&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08597&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luozhou Wang, Shuai Yang, Shu Liu, Ying-cong Chen&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.08448&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AndysonYs/Selective-Diffusion-Distillation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alessandro Flaborea, Luca Collorone, Guido D&#39;Amely, Stefano D&#39;Arrigo, Bardh Prenkaj, Fabio Galasso&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.07205&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, Kfir Aberman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.06949&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://hyperdreambooth.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JiauZhang/hyperdreambooth&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exact Diffusion Inversion via Bi-directional Integration Approximation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guoqiang Zhang, J. P. Lewis, W. Bastiaan Kleijn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.10829&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.04725&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://animatediff.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/guoyww/animatediff/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaskirat Singh, Liang Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.04749&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://1jsingh.github.io/divide-evaluate-and-refine&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/1jsingh/Divide-Evaluate-and-Refine&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt Augmentation and Text-To-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jie S. Li, Yow-Ting Shiue, Yong-Siang Shih, Jonas Geiping&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.05564&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Measuring the Success of Diffusion Models at Imitating Human Artists&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Stephen Casper, Zifan Guo, Shreya Mogulothu, Zachary Marinov, Chinmay Deshpande, Rui-Jie Yew, Zheng Dai, Dylan Hadfield-Menell&lt;/em&gt; &lt;br&gt; ICML Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.04028&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How to Detect Unauthorized Data Usages in Text-to-image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenting Wang, Chen Chen, Yuchen Liu, Lingjuan Lyu, Dimitris Metaxas, Shiqing Ma&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.03108&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collaborative Score Distillation for Consistent Visual Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, Jinwoo Shin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.04787&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://subin-kim-cv.github.io/CSD/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/subin-kim-cv/CSD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, Robin Rombach&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.01952&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Stability-AI/generative-models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, Yasutaka Furukawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.01097&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mvdiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 3 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Counting Guidance for High Fidelity Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wonjun Kang, Kevin Galim, Hyung Il Koo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17567&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, Shenghua Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17115&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generate Anything Anywhere in Any Scene&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuheng Li, Haotian Liu, Yangming Wen, Yong Jae Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17154&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yuheng-li.github.io/PACGen/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Simian Luo, Chuanhao Yan, Chenxu Hu, Hang Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17203&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/luosiallen/Diff-Foley&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjing Huang, Shikui Tu, Lei Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16894&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSketcher: Text Guided Vector Sketch Synthesis through Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ximing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian Yu, Dong Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14685&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A-STAR: Test-time Attention Segregation and Retention for Text-to-image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aishwarya Agarwal, Srikrishna Karanam, K J Joseph, Apoorv Saxena, Koustava Goswami, Balaji Vasan Srinivasan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14544&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Decompose and Realign: Tackling Condition Misalignment in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luozhou Wang, Guibao Shen, Yijun Li, Ying-cong Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.14408&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot spatial layout conditioning for text-to-image diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Couairon, Marlène Careil, Matthieu Cord, Stéphane Lathuilière, Jakob Verbeek&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13754&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12422&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Align, Adapt and Inject: Sound-guided Unified Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yue Yang, Kaipeng Zhang, Yuying Ge, Wenqi Shao, Zeyue Xue, Yu Qiao, Ping Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11504&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EMoG: Synthesizing Emotive Co-speech 3D Gesture with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lianying Yin, Yijun Wang, Tianyu He, Jinming Liu, Wei Zhao, Bohan Li, Xin Jin, Jianxin Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11496&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11300&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuqi Sun, Reian He, Weimin Tan, Bo Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10813&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Text Image Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuanzhi Zhu, Zhaohai Li, Tianwei Wang, Mengchao He, Cong Yao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10804&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Point-Cloud Completion with Pretrained Text-to-image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yoni Kasten, Ohad Rahamim, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10533&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Geon Yeong Park, Jeongsol Kim, Beomsu Kim, Sang Wan Lee, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09869&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hongcheng Gao, Hao Zhang, Yinpeng Dong, Zhijie Deng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.13103&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao-Wen Dong, Xiaoyu Liu, Jordi Pons, Gautam Bhattacharya, Santiago Pascual, Joan Serrà, Taylor Berg-Kirkpatrick, Julian McAuley&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09635&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming Diffusion Models for Music-driven Conducting Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhuoran Zhao, Jinbin Bai, Delong Chen, Debang Wang, Yubo Pan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10065&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shivam Mehta, Siyang Wang, Simon Alexanderson, Jonas Beskow, Éva Székely, Gustav Eje Henter&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09417&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models for Zero-Shot Open-Vocabulary Segmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Laurynas Karazija, Iro Laina, Andrea Vedaldi, Christian Rupprecht&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09316&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08877&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training Multimedia Event Extraction With Generated Images and Captions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zilin Du, Yunxin Li, Xu Guo, Yidan Sun, Boyang Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08966&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VidEdit: Zero-Shot and Spatially Aware Text-Driven Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paul Couairon, Clément Rambour, Jean-Emmanuel Haugeard, Nicolas Thome&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08707&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Norm-guided latent space exploration for text-to-image generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08687&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-free Diffusion Model Adaptation for Variable-Sized Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyu Jin, Xuli Shen, Bin Li, Xiangyang Xue&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08645&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GBSD: Generative Bokeh with Stage Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jieren Deng, Xin Zhou, Hao Tian, Zhihong Pan, Derek Aguiar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08251&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yongqi Yang, Ruoyu Wang, Zhihao Qian, Ye Zhu, Yu Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07954&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Zhang, Jiaxian Guo, Paul Yoo, Yutaka Matsuo, Yusuke Iwasawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07596&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controlling Text-to-Image Diffusion by Orthogonal Finetuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, Bernhard Schölkopf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07280&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi Tuo, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, Jianlong Fu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07257&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying Shan, Shenghua Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07154&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language-Guided Traffic Simulation via Scene-Level Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, Baishakhi Ray&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.06344&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BOOT: Data-free Distillation of Denoising Diffusion Models with Bootstrapping&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, Josh Susskind&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05544&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Grounded Text-to-Image Synthesis with Attention Refocusing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Quynh Phung, Songwei Ge, Jia-Bin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05427&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SyncDiffusion: Coherent Montage via Synchronized Joint Diffusions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuseung Lee, Kunho Kim, Hyunjin Kim, Minhyuk Sung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05178&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://syncdiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KAIST-Geometric-AI-Group/SyncDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Tuning-Free Real Image Editing with Proximal Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu, Qilong Zhangli, Anastasis Stathopoulos, Jindong Jiang, Zhaoyang Xia, Akash Srivastava, Dimitris Metaxas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05414&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changhoon Kim, Kyle Min, Maitreya Patel, Sheng Cheng, Yezhou Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04744&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Maitreya Patel, Tejas Gokhale, Chitta Baral, Yezhou Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04695&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Designing a Better Asymmetric VQGAN for StableDiffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, Gang Hua&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04632&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/buxiangzhiren/Asymmetric_VQGAN&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-modal Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mustapha Bounoua, Giulio Franzese, Pietro Michiardi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04445&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04607&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion-based Image Translation using Asymmetric Gradient Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04396&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stable Diffusion is Unstable&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chengbin Du, Yanxi Li, Zhongwei Qiu, Chang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02583&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yochai Yemini, Aviv Shamsian, Lior Bracha, Sharon Gannot, Ethan Fetaya&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03258&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://lipvoicer.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HeadSculpt: Crafting 3D Head Avatars with Text&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.03038&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://brandonhan.uk/HeadSculpt/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoxu Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02903&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Unified Text-based Person Retrieval: A Large-scale Multi-Attribute and Language Search Benchmark&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuyu Yang, Yinan Zhou, Yaxiong Wang, Yujiao Wu, Li Zhu, Zhedong Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02898&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;User-friendly Image Editing with Minimal Text Input: Leveraging Captioning and Injection Techniques&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sunwoo Kim, Wooseok Jang, Hyunsu Kim, Junho Kim, Yunjey Choi, Seungryong Kim, Gayeong Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02717&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Detector Guidance for Multi-Object Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luping Liu, Zijian Zhang, Yi Ren, Rongjie Huang, Xiang Yin, Zhou Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02236&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VideoComposer: Compositional Video Synthesis with Motion Controllability&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, Jingren Zhou&lt;/em&gt; &lt;br&gt; NeruIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02018&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://videocomposer.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/damo-vilab/videocomposer&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Word-Level Explanations for Analyzing Bias in Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexander Lin, Lucas Monteiro Paes, Sree Harsha Tanneru, Suraj Srinivas, Himabindu Lakkaraju&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05500&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Text-Guided 3D-Aware Portrait Generation with Score Distillation Sampling on Distribution&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiji Cheng, Fei Yin, Xiaoke Huang, Xintong Yu, Jiaxiang Liu, Shikun Feng, Yujiu Yang, Yansong Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.02083&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Probabilistic Adaptation of Text-to-Video Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengjiao Yang, Yilun Du, Bo Dai, Dale Schuurmans, Joshua B. Tenenbaum, Pieter Abbeel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01872&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://video-adapter.github.io/video-adapter/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video Colorization with Pre-trained Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanyuan Liu, Minshan Xie, Jinbo Xing, Chengze Li, Tien-Tsin Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01732&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Audio-Visual Speech Enhancement with Score-Based Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julius Richter, Simone Frintrop, Timo Gerkmann&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01432&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Privacy Distillation: Reducing Re-identification Risk of Multimodal Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Virginia Fernandez, Pedro Sanchez, Walter Hugo Lopez Pinaya, Grzegorz Jacenków, Sotirios A. Tsaftaris, Jorge Cardoso&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.01322&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StableRep: Synthetic Images from Text-to-Image Models Make Strong Visual Representation Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, Dilip Krishnan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00984&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Self-Guidance for Controllable Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, Aleksander Holynski&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00986&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dave.ml/selfguidance/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleDrop: Text-to-Image Generation in Any Style&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, Dilip Krishnan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00983&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://styledrop.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intriguing Properties of Text-guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, Alan Yuille&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00974&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intelligent Grimm -- Open-ended Visual Storytelling via Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Weidi Xie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00973&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://haoningwu3639.github.io/StoryGen_Webpage/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaozhe Hao, Kai Han, Shihao Zhao, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00971&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haoosz/ViCo&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Hidden Language of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Michal Irani, Inbar Mosseri, Lior Wolf&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00966&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://hila-chefer.github.io/Conceptor/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minghui Hu, Jianbin Zheng, Daqing Liu, Chuanxia Zheng, Chaoyue Wang, Dacheng Tao, Tat-Jen Cham&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00964&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mhh0318.github.io/cocktail/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mhh0318/Cocktail&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-Your-Video: Customized Video Generation Using Textual and Structural Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, Ying Shan, Tien-Tsin Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00943&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://doubiiu.github.io/projects/Make-Your-Video/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inserting Anybody in Diffusion Models via Celeb Basis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li, Chenyang Qi, Xintao Wang, Ying Shan, Huicheng Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00926&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://celeb-basis.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wuerstchen: Efficient Pretraining of Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pablo Pernias, Dominic Rampas, Marc Aubreville&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00637&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniDiff: Advancing Vision-Language Models with Generative and Discriminative Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiao Dong, Runhui Huang, Xiaoyong Wei, Zequn Jie, Jianxing Yu, Jian Yin, Xiaodan Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00813&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FigGen: Text to Scientific Figure Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Juan A. Rodriguez, David Vazquez, Issam Laradji, Marco Pedersoli, Pau Rodriguez&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00800&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peyman Gholami, Robert Xiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00219&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding and Mitigating Copying in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20086&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/somepago/DCR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, Yebin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20082&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://control4darxiv.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guian Fang, Zutao Jiang, Jianhua Han, Guansong Lu, Hang Xu, Xiaodan Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19599&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Enderfga/FineRewards&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Perturbation-Assisted Sample Synthesis: A Novel Approach for Uncertainty Quantification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifei Liu, Rex Shen, Xiaotong Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18671&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jialu Li, Mohit Bansal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19195&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pano-gen.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/jialuli-luka/PanoGen&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video ControlNet: Towards Temporally Consistent Synthetic-to-Real Video Translation Using Conditional Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ernie Chu, Shuo-Yen Lin, Jun-Cheng Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19193&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nested Diffusion Processes for Anytime Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Noam Elata, Bahjat Kawar, Tomer Michaeli, Michael Elad&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19066&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19012&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junzhe Zhu, Peiye Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18766&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayerDiffusion: Layered Controlled Image Editing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pengzhi Li, QInxuan Huang, Yikang Ding, Zhiheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18676&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Text-to-Image Generation with GPT-4&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, Xin Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18583&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cognitively Inspired Cross-Modal Data Generation Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zizhao Hu, Mohammad Rostami&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18433&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, Ping Luo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18295&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, Yixiao Ge, Ying Shan, Mike Zheng Shou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18292&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://showlab.github.io/Mix-of-Show/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18264&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/G-U-N/Gen-L-Video&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Only Image Captioning with Multi-Context Data Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Feipeng Ma, Yizhou Zhou, Fengyun Rao, Yueyi Zhang, Xiaoyan Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18072&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qian Wang, Biao Zhang, Michael Birsak, Peter Wonka&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18047&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Score Guidance for Text-Driven Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyunsoo Lee, Minsoo Kang, Bohyung Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18007&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-image Editing by Image Information Removal&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhongping Zhang, Jian Zheng, Jacob Zhiyuan Fang, Bryan A. Plummer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17489&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Consistent Video Editing with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, Luoqi Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17431&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FISEdit: Accelerating Text-to-image Editing via Cache-enabled Sparse Diffusion Inference&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zihao Yu, Haoyang Li, Fangcheng Fu, Xupeng Miao, Bin Cui&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17423&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.17098&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/controlvideo/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Visual Story Generation with Adaptive Context Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhangyin Feng, Yuchen Ren, Xinmiao Yu, Xiaocheng Feng, Duyu Tang, Shuming Shi, Bing Qin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16811&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daiki Miyake, Akihiro Iohara, Yu Saito, Toshiyuki Tanaka&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16807&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Are Diffusion Models Vision-And-Language Reasoners?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Benno Krojer, Elinor Poole-Dayan, Vikram Voleti, Christopher Pal, Siva Reddy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16397&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/McGill-NLP/diffusion-itm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, Kimin Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16381&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16322&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://shihaozhaozsh.github.io/unicontrolnet/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ShihaoZhaoZSH/Uni-ControlNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Parallel Sampling of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, Nima Anari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16317&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AndyShih12/paradigms&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Break-A-Scene: Extracting Multiple Concepts from a Single Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, Dani Lischinski&lt;/em&gt; &lt;br&gt; SIGGRAPH Asia 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16311&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/break-a-scene/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/google/break-a-scene&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E. Gonzalez, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16289&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lisadunlap/ALIA&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt-Free Diffusion: Taking &#34;Text&#34; out of Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Irfan Essa, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16223&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SHI-Labs/Prompt-Free-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProSpect: Expanded Conditioning for the Personalization of Attribute-aware Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, Changsheng Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16225&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16213&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/prolificdreamer/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On Architectural Compression of Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, Shinkook Choi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15798&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jooyoung Choi, Yunjey Choi, Yunji Kim, Junho Kim, Sungroh Yoon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15779&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marco Bellagente, Manuel Brack, Hannah Teufel, Felix Friedrich, Björn Deiseroth, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Koen Oostermeijer, Andres Felipe Cruz-Salinas, Patrick Schramowski, Kristian Kersting, Samuel Weinbach&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15296&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongxu Yue, Qin Guo, Munan Ning, Jiaxi Cui, Yuesheng Zhu, Li Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14742&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffBlender: Scalable and Composable Multimodal Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sungnyun Kim, Junsoo Lee, Kibeom Hong, Daesik Kim, Namhyuk Ahn&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15194&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/sungnyun/diffblender&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tuhin Chakrabarty, Arkadiy Saakyan, Olivia Winn, Artemis Panagopoulou, Yue Yang, Marianna Apidianaki, Smaranda Muresan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14724&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dongxu Li, Junnan Li, Steven C. H. Hoi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14720&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alicia Parrish, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max Bartolo, Oana Inel, Juan Ciro, Rafael Mosquera, Addison Howard, Will Cukierski, D. Sculley, Vijay Janapa Reddi, Lora Aroyo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.14384&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional Text-to-Image Synthesis with Attention Map Control of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, Xiaodong Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13921&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, Yang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13873&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Control-A-Video: Controllable Text-to-Video Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, Liang Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13840&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding Text-driven Motion Synthesis with Keyframe Collaboration via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dong Wei, Xiaoning Sun, Huaijiang Sun, Bin Li, Shengxiang Hu, Weiqing Li, Jianfeng Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13773&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Long Lian, Boyi Li, Adam Yala, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13655&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, Rita Cucchiara&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13501&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FACTIFY3M: A Benchmark for Multimodal Fact Verification with Explainability through 5W Question-Answering&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Megha Chakraborty, Khusbu Pahwa, Anku Rani, Adarsh Mahor, Aditya Pakala, Arghya Sarkar, Harshit Dave, Ishan Paul, Janvita Reddy, Preethi Gurumurthy, Ritvik G, Samahriti Mukherjee, Shreyas Chatterjee, Kinjal Sensharma, Dwip Dalal, Suryavardan S, Shreyash Mishra, Parth Patwa, Aman Chadha, Amit Sheth, Amitava Das&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05523&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training Diffusion Models with Reinforcement Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13301&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;If at First You Don&#39;t Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, Zeynep Akata&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13308&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://rl-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ControlVideo: Training-free Controllable Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, Qi Tian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13077&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YBYBZhang/ControlVideo&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guy Yariv, Itai Gat, Lior Wolf, Yossi Adi, Idan Schwartz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13050&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The CLIP Model is Secretly an Image-to-Prompt Converter&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxuan Ding, Chunna Tian, Haoxuan Ding, Lingqiao Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12716&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructVid2Vid: Controllable Video Editing with Natural Language Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, Yueting Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12328&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models&#39; Safety Filters&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, Yinzhi Cao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12082&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Late-Constraint Diffusion Guidance for Controllable Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chang Liu, Dong Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11520&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://alonzoleeeooo.github.io/LCDG/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AlonzoLeeeooo/LCDG&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Any-to-Any Generation via Composable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, Mohit Bansal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11846&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://codi-gen.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/i-Code/tree/main/i-Code-V3&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11588&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Brain Captioning: Decoding human brain activity into images and text&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Matteo Ferrante, Furkan Ozcelik, Tommaso Boccato, Rufin VanRullen, Nicola Toschi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11560&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinyi Hu, Xu Han, Xiaoyuan Yi, Yutong Chen, Wenhao Li, Zhiyuan Liu, Maosong Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11540&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discriminative Diffusion Models as Few-shot Vision and Language Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuehai He, Weixi Feng, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, William Yang Wang, Xin Eric Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10722&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yihao Huang, Qing Guo, Felix Juefei-Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10701&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AIwriting: Relations Between Image Generation and Digital Writing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Scott Rettberg, Talan Memmott, Jill Walker Rettberg, Jason Nelson, Patrick Lichty&lt;/em&gt; &lt;br&gt; ISEA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10834&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TextDiffuser: Diffusion Models as Text Painters&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, Furu Wei&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10855&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, Jiaying Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10874&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LDM3D: Latent Diffusion Model for 3D&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, Vasudev Lal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10853&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yixiong Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10843&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inspecting the Geographical Representativeness of Images from Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Abhipsa Basu, R. Venkatesh Babu, Danish Pruthi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11080&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, Yogesh Balaji&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10474&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/dir/pyoco/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AMD: Autoregressive Motion Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Han, Hao Peng, Minjing Dong, Chang Xu, Yi Ren, Yixuan Shen, Yuheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09381&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generating coherent comic with rich story using ChatGPT and Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ze Jin, Zorina Song&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11067&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh, Sonal Gupta&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09662&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://azadis.github.io/make-an-animation/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li, Gim Hee Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08850&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://make-a-protagonist.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Make-A-Protagonist/Make-A-Protagonist&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Common Diffusion Noise Schedules and Sample Steps are Flawed&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shanchuan Lin, Bingchen Liu, Jiashi Li, Xiao Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.08891&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interactive Fashion Content Generation Using LLMs and Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Krishna Sri Ipsit Mantri, Nevasini Sasikumar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05182&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Huang, Wenjing Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06710&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nulltextforcartoon.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/NullTextforCartoon/NullTextforCartoon&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 11 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;iEdit: Localised Text-guided Image Editing with Weak Supervision&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rumeysa Bodur, Erhan Gundogdu, Binod Bhattarai, Tae-Kyun Kim, Michael Donoser, Loris Bazzani&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05947&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05189&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Qrange-group/SUR-adapter&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nisha Huang, Yuxin Zhang, Weiming Dong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05464&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuseStyleGesture: Stylized Audio-Driven Co-Speech Gesture Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sicheng Yang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Lei Hao, Weihong Bao, Ming Cheng, Long Xiao&lt;/em&gt; &lt;br&gt; IJCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04919&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YoungSeng/DiffuseStyleGesture&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IIITD-20K: Dense captioning for Text-Image ReID&lt;/strong&gt; &lt;br&gt; &lt;em&gt;A V Subramanyam, Niranjan Sundararajan, Vibhu Dubey, Brejesh Lall&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04497&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReGeneration Learning of Diffusion Models with Rich Prompts for Zero-Shot Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yupei Lin, Sen Zhang, Xiaojun Yang, Xiao Wang, Yukai Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04651&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yupeilin2388.github.io/publication/ReDiffuser&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenkai Dong, Song Xue, Xiaoyue Duan, Shumin Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04441&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-Image Diffusion Models can be Easily Backdoored through Multimodal Data Poisoning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengfang Zhai, Yinpeng Dong, Qingni Shen, Shi Pu, Yuejian Fang, Hang Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04175&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seungwoo Lee, Chaerin Kong, Donghyeon Jeon, Nojun Kwak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04001&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data Curation for Image Captioning with Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenyan Li, Jonas F. Lotz, Chen Qiu, Desmond Elliott&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03610&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DisenBooth: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, Wenwu Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://disenbooth.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guided Image Synthesis via Initial Image Editing in Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiafeng Mao, Xueting Wang, Kiyoharu Aizawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03382&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seongmin Lee, Benjamin Hoover, Hendrik Strobelt, Zijie J. Wang, ShengYun Peng, Austin Wright, Kevin Li, Haekyu Park, Haoyang Yang, Duen Horng Chau&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.03509&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://poloclub.github.io/diffusion-explainer/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal-driven Talking Face Generation, Face Swapping, Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chao Xu, Shaoting Zhu, Junwei Zhu, Tianxin Huang, Jiangning Zhang, Ying Tai, Yong Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.02594&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Data Augmentation for Image Captioning using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Changrong Xiao, Sean Xin Xu, Kunpeng Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01855&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;In-Context Learning Unlocked for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01115&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zhendong-wang.github.io/prompt-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Zhendong-Wang/Prompt-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SceneGenie: Scene Graph Guided Diffusion Models for Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Azade Farshad, Yousef Yeganeh, Yu Chi, Chengzhi Shen, Björn Ommer, Nassir Navab&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14573&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;It is all about where you start: Text-to-image generation with seed selection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14530&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edit Everything: A Text-Guided Generative System for Images Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Defeng Xie, Ruichen Wang, Jian Ma, Chen Chen, Haonan Lu, Dong Yang, Fobo Shi, Xiaodong Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14006&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DefengXie/Edit_Everything&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-Free Location-Aware Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiafeng Mao, Xueting Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.13427&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TextMesh: Generation of Realistic 3D Meshes From Text Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, Federico Tombari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12439&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using Text-to-Image Generation for Architectural Design Ideation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ville Paananen, Jonas Oppenlaender, Aku Visuri&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10182&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anything-3D: Towards Single-view Anything Reconstruction in the Wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiuhong Shen, Xingyi Yang, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10261&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Anything-of-anything/Anything-3D&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Soon Yau Cheong, Armin Mustafa, Andrew Gilbert&lt;/em&gt; &lt;br&gt; ICCV Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08870&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/soon-yau/upgpt&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuwei Yin, Jean Kaddour, Xiang Zhang, Yixin Nie, Zhenguang Liu, Lingpeng Kong, Qi Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08821&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, Karsten Kreis&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/VideoLDM/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Performer: Text-Driven Human Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuming Jiang, Shuai Yang, Tong Liang Koh, Wayne Wu, Chen Change Loy, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08483&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yumingj.github.io/projects/Text2Performer.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, Xi Yin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08477&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://latent-shift.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, Yinqiang Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08465&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TencentARC/MasaCtrl&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Conditional Contextualized Avatars For Zero-Shot Personalization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samaneh Azadi, Thomas Hayes, Akbar Shah, Guan Pang, Devi Parikh, Sonal Gupta&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07410&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Delta Denoising Score&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amir Hertz, Kfir Aberman, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07090&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://delta-denoising-score.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Expressive Text-to-Image Generation with Rich Text&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Songwei Ge, Taesung Park, Jun-Yan Zhu, Jia-Bin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06720&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://rich-text-to-image.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SongweiGe/rich-text-to-image&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Soundini: Sound-Guided Diffusion for Natural Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seung Hyun Lee, Sieun Kim, Innfarn Yoo, Feng Yang, Donghyeon Cho, Youngseo Kim, Huiwen Chang, Jinkyu Kim, Sangpil Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://kuai-lab.github.io/soundini-gallery/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Diffusion Models for Scene Text Editing with Dual Encoders&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, Shiyu Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05568&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UCSB-NLP-Chang/DiffSTE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;An Edit Friendly DDPM Noise Space: Inversion and Manipulations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Inbar Huberman-Spiegelglas, Vladimir Kulikov, Tomer Michaeli&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06140&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Continual Diffusion: Continual Customization of Text-to-Image Diffusion with C-LoRA&lt;/strong&gt; &lt;br&gt; &lt;em&gt;James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, Hongxia Jin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06027&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jamessealesmith.github.io/continual-diffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eslam Mohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan Farooq Khan, Li Erran Li, Mohamed Elhoseiny&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05390&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://eslambakr.github.io/hrsbench.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammadreza Armandpour, Huangjie Zheng, Ali Sadeghian, Amir Sadeghian, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04968&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Real-time Text-driven Image Manipulation with Unconditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikita Starodubcev, Dmitry Baranchuk, Valentin Khrulkov, Artem Babenko&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04344&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, Qiang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04269&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://idea-research.github.io/HumanSD/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiucheng Wu, Yujian Liu, Handong Zhao, Trung Bui, Zhe Lin, Yang Zhang, Shiyu Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03869&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Generative Model Adaptation via Image-specific Prompt Learning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiayi Guo, Chaofei Wang, You Wu, Eric Zhang, Kai Wang, Xingqian Xu, Shiji Song, Humphrey Shi, Gao Huang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03119&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-Free Layout Control with Cross-Attention Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minghao Chen, Iro Laina, Andrea Vedaldi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03373&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://silent-chen.github.io/layout-guidance/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/silent-chen/layout-guidance&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Benchmarking Robustness to Text-Guided Corruptions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammadreza Mofayezi, Yasamin Medghalchi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02963&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02827&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://janeyeon.github.io/ditto-nerf/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xuhui Jia, Yang Zhao, Kelvin C.K. Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, Yu-Chuan Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02642&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Diffusion-based Method for Multi-turn Compositional Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chao Wang, Xiaoyu Yang, Jinmiao Huang, Kevin Ferreira&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02192&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;viz2viz: Prompt-driven stylized visualization generation using a diffusion model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaqi Wu, John Joon Young Chung, Eytan Adar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01919&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alberto Baldrati, Davide Morelli, Giuseppe Cartella, Marcella Cornia, Marco Bertini, Rita Cucchiara&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02051&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Ji Ha Jang, Se Young Chun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01900&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gwang-kim.github.io/podia_3d/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jaewoong Lee, Sangwon Jang, Jaehyeong Jo, Jaehong Yoon, Yunji Kim, Jin-Hwa Kim, Jung-Woo Ha, Sung Ju Hwang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01515&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01116&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mingyuan-zhang/ReMoDiffuse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.00916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, Jingyi Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03117&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/dreamface&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, Xiaodong Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17870&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://1073521013.github.io/glyph-draw.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17606&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://avatar-craft.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/songrise/avatarcraft&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu Sebe, Trevor Darrell, Zhangyang Wang, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17546&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Picsart-AI-Research/PAIR-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Social Biases through the Text-to-Image Generation Lens&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ranjita Naik, Besmira Nushi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06034&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17591&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SHI-Labs/Forget-Me-Not&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCollage: Parallel Generation of Large Content with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, Ming-Yu Liu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17076&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/dir/diffcollage/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, Chunhua Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17599&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discriminative Class Tokens for Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Idan Schwartz, Vésteinn Snæbjarnarson, Sagie Benaim, Hila Chefer, Ryan Cotterell, Lior Wolf, Serge Belongie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17155&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenpng Du, Qi Chen, Tianyu He, Xu Tan, Xie Chen, Kai Yu, Sheng Zhao, Jiang Bian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17550&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, Xi Li&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17189&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZGCTroy/LayoutDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4D Facial Expression Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaifeng Zou, Sylvain Faisan, Boyang Yu, Sébastien Valette, Hyewon Seo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16611&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZOUKaifeng/4DFM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MDP: A Generalized Framework for Text-Guided Image Editing by Manipulating the Diffusion Path&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qian Wang, Biao Zhang, Michael Birsak, Peter Wonka&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16765&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/QianWangX/MDP-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Ishii, Takuya Narihira&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15780&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sony.github.io/Instruct3Dto3D-doc/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, Jian Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15649&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Seer: Language Instructed Video Prediction with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xianfan Gu, Chuan Wen, Jiaming Song, Yang Gao&lt;/em&gt; &lt;br&gt; CVPR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14897&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Susung Hong, Donghoon Ahn, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15413&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anti-DreamBooth: Protecting users from personalized text-to-image synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Thanh Van Le, Hao Phung, Thuan Hoang Nguyen, Quan Dao, Ngoc Tran, Anh Tran&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15433&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/VinAIResearch/Anti-DreamBooth&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tenglong Ao, Zeyi Zhang, Libin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14613&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Better Aligning Text-to-Image Models with Human Preference&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14420&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tgxs002.github.io/align_sd_web/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ISS++: Image as Stepping Stone for Text-Guided 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15181&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuScene: Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiapeng Tang, Yinyu Nie, Lev Markhasin, Angela Dai, Justus Thies, Matthias Nießner&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14207&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tangjiapeng.github.io/projects/DiffuScene/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13843&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fantasia3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rui Chen, Yongwei Chen, Ningxin Jiao, Kui Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13873&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReVersion: Diffusion-Based Relation Inversion from Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13495&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ziqihuangg.github.io/projects/reversion.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ziqihuangg/ReVersion&#34;&gt;Github&lt;/a&gt;] 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ablating Concepts in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, Jun-Yan Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13516&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~concept-ablation/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nupurkmr9/concept-ablation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13439&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Picsart-AI-Research/Text2Video-Zero&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wenjing Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13126&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://magicfusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MagicFusion/MagicFusion.github.io&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pix2Video: Video Editing using Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Duygu Ceylan, Chun-Hao Paul Huang, Niloy J. Mitra&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12688&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://duyguceylan.github.io/pix2video.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12789&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instruct-nerf2nerf.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, Minhyuk Sung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12236&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://salad3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vox-E: Text-guided Voxel Editing of 3D Objects&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Etai Sella, Gal Fiebelman, Peter Hedman, Hadar Averbuch-Elor&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tau-vailab.github.io/Vox-E/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, Sangdoo Yun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Jhe Li, Kris Kitani&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11938&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Tex: Text-driven Texture Synthesis via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, Matthias Nießner&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11396&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://daveredrum.github.io/Text2Tex/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Localizing Object-level Shape Variations with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11306&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://orpatashnik.github.io/local-prompt-mixing/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SVDiff: Compact Parameter Space for Diffusion Fine-Tuning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, Feng Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11305&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discovering Interpretable Directions in the Semantic Latent Space of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;René Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff, Tomer Michaeli&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11073&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SKED: Sketch-guided Text-based 3D Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aryan Mikaeili, Or Perel, Daniel Cohen-Or, Ali Mahdavi-Amiri&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10735&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DialogPaint: A Dialog-based Image Editing Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingxuan Wei, Shiyu Wu, Xin Jiang, Yequan Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10073&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Can Qin, Ning Yu, Chen Xing, Shu Zhang, Zeyuan Chen, Stefano Ermon, Yun Fu, Caiming Xiong, Ran Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10056&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionRet: Generative Text-Video Retrieval with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji, Chang Liu, Li Yuan, Jie Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09867&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, Jian Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09833&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vvictoryuki/FreeDoM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, Jiaying Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09319&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FateZero: Fusing Attentions for Zero-shot Text-based Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, Qifeng Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09535&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fate-zero-edit.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenyangQiQi/FateZero&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HIVE: Harnessing Human Feedback for Instructional Visual Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09618&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;P+: Extended Textual Conditioning in Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, Kfir Aberman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09522&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://prompt-plus.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Inhwa Han, Serin Yang, Taesung Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08767&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Aerial Diffusion: Text Guided Ground-to-Aerial View Translation from a Single Image using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Divya Kothandaraman, Tianyi Zhou, Ming Lin, Dinesh Manocha&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11444&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/divyakraman/AerialDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Serin Yang, Hyunmin Hwang, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08622&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edit-A-Video: Single Video Editing with Object-Aware Consistency&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, Sungroh Yoon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07945&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://edit-a-video.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Editing Implicit Assumptions in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hadas Orgad, Bahjat Kawar, Yonatan Belinkov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08084&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://time-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/bahjat-kawar/time-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07937&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04671&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/visual-chatgpt&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Video-P2P: Video Editing with Cross-attention Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, Jiaya Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04761&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://video-p2p.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Erasing Concepts from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, David Bau&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07345&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://erasing.baulab.info/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/rohitgandikota/erasing&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.06555&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/thu-ml/unidiffuser&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cones: Concept Neurons in Diffusion Models for Customized Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, Yang Cao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05125&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Prompt Log Analysis of Text-to-Image Generation Systems&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yutong Xie, Zhaoying Pan, Jinge Ma, Jie Luo, Qiaozhu Mei&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.04587&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiwei Tang, Dmitry Rybin, Tsung-Hui Chang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.03751&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TZW1998/Taming-Stable-Diffusion-with-Human-Ranking-Feedback&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unleashing Text-to-Image Diffusion Models for Visual Perception&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.02153&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/wl-zhao/VPD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Collage Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vishnu Sarukkai, Linden Li, Arden Ma, Christopher Ré, Kayvon Fatahalian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.00262&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Enhanced Controllability of Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wonwoong Cho, Hareesh Ravi, Midhun Harikumar, Vinh Khuc, Krishna Kumar Singh, Jingwan Lu, David I. Inouye, Ajinkya Kale&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.14368&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Directed Diffusion: Direct Control of Object Placement through Attention Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wan-Duo Kurt Ma, J.P. Lewis, W. Bastiaan Kleijn, Thomas Leung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.13153&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modulating Pretrained Diffusion Models for Multimodal Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cusuh Ham, James Hays, Jingwan Lu, Krishna Kumar Singh, Zhifei Zhang, Tobias Hinz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12764&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Region-Aware Diffusion for Zero-shot Text-driven Image Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nisha Huang, Fan Tang, Weiming Dong, Tong-Yee Lee, Changsheng Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11797&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haha-lisa/RDM-Region-Aware-Diffusion-Model&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controlled and Conditional Text to Image Generation with Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pranav Aggarwal, Hareesh Ravi, Naveen Marri, Sachin Kelkar, Fengbin Chen, Vinh Khuc, Midhun Harikumar, Ritiz Tambi, Sudharshan Reddy Kakumanu, Purvak Lapsiya, Alvin Ghouas, Sarah Saber, Malavika Ramprasad, Baldo Faieta, Ajinkya Kale&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11710&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.11552&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://energy-based-model.github.io/reduce-reuse-recycle/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning 3D Photography Videos via Self-supervised Diffusion on Single Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaodong Wang, Chenfei Wu, Shengming Yin, Minheng Ni, Jianfeng Wang, Linjie Li, Zhengyuan Yang, Fan Yang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10781&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring the Representation Manifolds of Stable Diffusion Through the Lens of Intrinsic Dimension&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Henry Kvinge, Davis Brown, Charles Godfrey&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.09301&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-driven Visual Synthesis with Latent Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ting-Hsuan Liao, Songwei Ge, Yiran Xu, Yao-Chih Lee, Badour AlBahar, Jia-Bin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08510&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://latent-diffusion-prior.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, Xiaohu Qie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08453&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TencentARC/T2I-Adapter&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omer Bar-Tal, Lior Yariv, Yaron Lipman, Tali Dekel&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08113&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://multidiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omerbt/MultiDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boundary Guided Mixing Trajectory for Semantic Control with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Zhu, Yu Wu, Zhiwei Deng, Olga Russakovsky, Yan Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.08357&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dataset Interfaces: Diagnosing Model Failures Using Controllable Counterfactual Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Joshua Vendrow, Saachi Jain, Logan Engstrom, Aleksander Madry&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07865&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MadryLab/dataset-interfaces&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PRedItOR: Text Guided Image Editing with Diffusion Prior&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Hareesh Ravi, Sachin Kelkar, Midhun Harikumar, Ajinkya Kale&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07979&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Guided Scene Sketch-to-Photo Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;AprilPyone MaungMaung, Makoto Shing, Kentaro Mitsui, Kei Sawada, Fumio Okura&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.06883&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Universal Guidance for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.07121&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/arpitbansal297/Universal-Guided-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adding Conditional Control to Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lvmin Zhang, Maneesh Agrawala&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05543&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Analyzing Multimodal Objectives Through the Lens of Generative Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaerin Kong, Nojun Kwak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10305&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is This Loss Informative? Speeding Up Textual Inversion with Deterministic Objective Evaluation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anton Voronov, Mikhail Khoroshikh, Artem Babenko, Max Ryabinin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04841&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q-Diffusion: Quantizing Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, Kurt Keutzer&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04304&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Xiuyu-Li/q-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GLAZE: Protecting Artists from Style Mimicry by Text-to-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, Ben Y. Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.04222&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyeonho Jeong, Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03900&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Felix Friedrich, Patrick Schramowski, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Sasha Luccioni, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10893&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, Tom Goldstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03668&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YuxinWenRick/hard-prompts-made-easy&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-shot Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, Jun-Yan Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03027&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Structure and Content-Guided Video Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, Anastasis Germanidis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03011&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.runwayml.com/gen1&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mixture of Diffusers for scene composition and high resolution image generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Álvaro Barbero Jiménez&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02412&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/albarji/mixture-of-diffusers&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReDi: Efficient Learning-Free Diffusion Inference via Trajectory Retrieval&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kexun Zhang, Xianjun Yang, William Yang Wang, Lei Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02285&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Eliminating Prior Bias for Semantic Image Editing via Dual-Cycle Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zuopeng Yang, Tianshu Chu, Xin Lin, Erdun Gao, Daqing Liu, Jie Yang, Chaoyue Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02394&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic-Guided Image Augmentation with Pre-trained Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bohan Li, Xinghao Wang, Xiao Xu, Yutai Hou, Yunlong Feng, Feng Wang, Wanxiang Che&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.02070&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://texturepaper.github.io/TEXTurePaper/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TEXTure: Text-Guided Texturing of 3D Shapes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01721&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://texturepaper.github.io/TEXTurePaper/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TEXTurePaper/TEXTurePaper&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dreamix: Video Diffusion Models are General Video Editors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, Yedid Hoshen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01329&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreamix-video-editing.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Trash to Treasure: Using text-to-image models to inform the design of physical artefacts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Amy Smith, Hope Schroeder, Ziv Epstein, Michael Cook, Simon Colton, Andrew Lippman&lt;/em&gt; &lt;br&gt; AAAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00561&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13826&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://attendandexcite.github.io/Attend-and-Excite/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/AttendAndExcite/Attend-and-Excite&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero3D: Semantic-Driven Multi-Category 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Han, Yitong Liu, Yixuan Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13591&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shape-aware Text-driven Layered Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth Qiu, Jia-Bin Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13173&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://text-video-edit.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PromptMix: Text-to-image diffusion models enhance the performance of lightweight networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Arian Bakhtiarnia, Qi Zhang, Alexandros Iosifidis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12914&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gitlab.au.dk/maleci/promptmix&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ming Tao, Bing-Kun Bao, Hao Tang, Changsheng Xu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12959&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tobran/GALIP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SEGA: Instructing Diffusion using Semantic Dimensions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Equitable Representation in Text-to-Image Synthesis Models with the Cross-Cultural Understanding Benchmark (CCUB) Dataset&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhixuan Liu, Youeun Shin, Beverley-Claire Okogwu, Youngsik Yun, Lia Coleman, Peter Schaldenbrand, Jihie Kim, Jean Oh&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.12073&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-To-4D Dynamic Scene Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, Yaniv Taigman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11280&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guiding Text-to-Image Diffusion Model Towards Grounded Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.05221&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://lipurple.github.io/Grounded_Diffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Driven Video Editing via an Audio-Conditioned Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dan Bigioi, Shubhajit Basak, Hugh Jordan, Rachel McDonnell, Peter Corcoran&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.04474&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://danbigioi.github.io/DiffusionVideoEditing/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DanBigioi/DiffusionVideoEditing&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 10 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual Story Generation Based on Emotion and Keywords&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuetian Chen, Ruohua Li, Bowen Shi, Peiru Liu, Mei Si&lt;/em&gt; &lt;br&gt; AIIDE INT 2022. [&lt;a href=&#34;https://arxiv.org/abs/2301.02777&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffTalk: Crafting Diffusion Models for Generalized Talking Head Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03786&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speech Driven Video Editing via an Audio-Conditioned Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dan Bigioi, Shubhajit Basak, Hugh Jordan, Rachel McDonnell, Peter Corcoran&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.04474&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michał Stypułkowski, Konstantinos Vougioukas, Sen He, Maciej Zięba, Stavros Petridis, Maja Pantic&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03396&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mstypulkowski.github.io/diffusedheads/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Muse: Text-To-Image Generation via Masked Generative Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, Dilip Krishnan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.00704&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://muse-model.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Shenghua Gao&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.14704&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bluestyle97.github.io/dream3d/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Vision Transformers as Diffusion Learners&lt;/strong&gt; &lt;br&gt; &lt;em&gt;He Cao, Jianan Wang, Tianhe Ren, Xianbiao Qi, Yihao Chen, Yuan Yao, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.13771&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, Mike Zheng Shou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11565&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tuneavideo.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robert Wolfe, Yiwei Yang, Bill Howe, Aylin Caliskan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.11261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimizing Prompts for Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaru Hao, Zewen Chi, Li Dong, Furu Wei&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09611&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/microsoft/Promptist&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/LMOps/tree/main/promptist&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, Shiyu Chang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08698&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TeTIm-Eval: a novel curated evaluation data set for comparing text-to-image models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Federico A. Galatolo, Mario G. C. A. Cimino, Edoardo Cogotti&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.07839&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Infinite Index: Information Retrieval on Generative Text-To-Image Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Niklas Deckers, Maik Fröbe, Johannes Kiesel, Gianluca Pandolfo, Christopher Schröder, Benno Stein, Martin Potthast&lt;/em&gt; &lt;br&gt; CHIIR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.07476&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LidarCLIP or: How I Learned to Talk to Point Clouds&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Georg Hess, Adam Tonderski, Christoffer Petersson, Lennart Svensson, Kalle Åström&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06858&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/atonderski/lidarclip&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, William Chan&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.06909&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Stable Artist: Steering Semantics in Diffusion Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Manuel Brack, Patrick Schramowski, Felix Friedrich, Dominik Hintersdorf, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06013&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, Kun Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.05034&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, William Yang Wang&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.05032&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/weixi-feng/Structured-Diffusion-Guidance&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, Christian Theobalt&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04495&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/MoFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, Liangyan Gui&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04493&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yccyenchicheng.github.io/SDFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SINE: SINgle Image Editing with Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, Jian Ren&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04489&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zhang-zx.github.io/SINE/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhang-zx/SINE&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-Concept Customization of Text-to-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, Jun-Yan Zhu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04488&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cs.cmu.edu/~custom-diffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Guided Domain Adaptation of Image Generators&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris Metaxas, Ahmed Elgammal&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04473&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://styleganfusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Executing your Commands via Motion Diffusion in Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, Gang Yu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://chenxin.tech/mld/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn Wong, Baoyuan Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.04248&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zxyin.github.io/TH-PAD/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magic: Multi Art Genre Intelligent Choreography Dataset and Network for 3D Dance Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Xiu Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03741&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Judge, Localize, and Edit: Ensuring Visual Commonsense Morality for Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Seongbeom Park, Suhong Moon, Jinkyu Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03507&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Congyue Deng, Chiyu &#34;Max&#39;&#39; Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03267&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic-Conditional Diffusion Networks for Image Captioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng, Hongyang Chao, Tao Mei&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.03099&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YehLi/xmodaler/tree/master/configs/image_caption/scdnet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-SDF: Text-to-Shape via Voxelized Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muheng Li, Yueqi Duan, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.03293&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ttlmh.github.io/DiffusionSDF/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ttlmh/Diffusion-SDF&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ADIR: Adaptive Diffusion for Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shady Abu-Hussein, Tom Tirer, Raja Giryes&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03221&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://shadyabh.github.io/ADIR/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;M-VADER: A Model for Diffusion with Multimodal Context&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samuel Weinbach, Marco Bellagente, Constantin Eichenberg, Andrew Dai, Robert Baldock, Souradeep Nanda, Björn Deiseroth, Koen Oostermeijer, Hannah Teufel, Andres Felipe Cruz-Salinas&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02936&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Video Autoencoders: Toward Temporally Consistent Face Video Editing via Disentangled Video Encoding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyeongman Kim, Hajin Shim, Hyunsu Kim, Yunjey Choi, Junho Kim, Eunho Yang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.02802&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://diff-video-ae.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/man805/Diffusion-Video-Autoencoders&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unite and Conquer: Cross Dataset Multimodal Synthesis using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, Vishal M. Patel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00793&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nithin-gk.github.io/projectpages/Multidiff/index.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shape-Guided Diffusion with Inside-Outside Attention&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dong Huk Park, Grace Luo, Clayton Toste, Samaneh Azadi, Xihui Liu, Maka Karalashvili, Anna Rohrbach, Trevor Darrell&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00210&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://shape-guided-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinDDM: A Single Image Denoising Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vladimir Kulikov, Shahar Yadin, Matan Kleiner, Tomer Michaeli&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16582&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://matankleiner.github.io/sinddm/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Se Young Chun&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.16374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://datid-3d.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Refined Semantic Enhancement towards Frequency Diffusion for Video Captioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xian Zhong, Zipeng Li, Shuqin Chen, Kui Jiang, Chen Chen, Mang Ye&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.15076&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lzp870/RSFD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unified Discrete Diffusion for Simultaneous Vision-Language Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minghui Hu, Chuanxia Zheng, Heliang Zheng, Tat-Jen Cham, Chaoyue Wang, Zuopeng Yang, Dacheng Tao, Ponnuthurai N. Suganthan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.14842&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, Dacheng Tao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.14108&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SpaText: Spatio-Textual Representation for Controllable Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, Xi Yin&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.14305&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/spatext/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sketch-Guided Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andrey Voynov, Kfir Aberman, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sketch-guided-diffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shifted Diffusion for Text-to-image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yufan Zhou, Bingchen Liu, Yizhe Zhu, Xiao Yang, Changyou Chen, Jinhui Xu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.15388&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-A-Story: Visual Memory Conditioned Consistent Story Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, Leonid Sigal&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.13319&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Schrödinger&#39;s Bat: Diffusion Models Sometimes Generate Polysemous Words in Superposition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jennifer C. White, Ryan Cotterell&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13095&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EDICT: Exact Diffusion Inversion via Coupled Transformations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bram Wallace, Akash Gokul, Nikhil Naik&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12446&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/salesforce/EDICT&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Narek Tumanyan, Michal Geyer, Shai Bagon, Tali Dekel&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.12572&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/MichalGeyer/plug-and-play&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Human Evaluation of Text-to-Image Models on a Multi-Task Benchmark&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vitali Petsiuk, Alexander E. Siemenn, Saisamrit Surbehera, Zad Chin, Keith Tyser, Gregory Hunter, Arvind Raghavan, Yann Hicke, Bryan A. Plummer, Ori Kerret, Tonio Buonassisi, Kate Saenko, Armando Solar-Lezama, Iddo Drori&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12112&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinDiffusion: Learning a Diffusion Model from a Single Natural Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, Houqiang Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.12445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/WeilunWang/SinDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinFusion: Training Diffusion Models on a Single Image or Video&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yaniv Nikankin, Niv Haim, Michal Irani&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11743&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yanivnik.github.io/sinfusion/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exploring Discrete Diffusion Models for Image Captioning&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixin Zhu, Yixuan Wei, Jianfeng Wang, Zhe Gan, Zheng Zhang, Le Wang, Gang Hua, Lijuan Wang, Zicheng Liu, Han Hu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11694&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/buxiangzhiren/DDCap&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Investigating Prompt Engineering in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sam Witteveen, Martin Andrews&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.15462&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;VectorFusion: Text-to-SVG by Abstracting Pixel-Based Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ajay Jain, Amber Xie, Pieter Abbeel&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.11319&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ajayj.com/vectorfusion&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xichen Pan, Pengda Qin, Yuhong Li, Hui Xue, Wenhu Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10950&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/xichenpan/ARLDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffStyler: Controllable Dual Diffusion for Text-Driven Image Stylization&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nisha Huang, Yuxin Zhang, Fan Tang, Chongyang Ma, Haibin Huang, Yong Zhang, Weiming Dong, Changsheng Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10682&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magic3D: High-Resolution Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.10440&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://deepimagination.cc/Magic3D/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Invariant Learning via Diffusion Dreamed Distribution Shifts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Priyatham Kattakinda, Alexander Levine, Soheil Feizi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10370&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Null-text Inversion for Editing Real Images using Guided Diffusion Models&lt;/strong&gt;&lt;br&gt; &lt;em&gt;Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09794&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructPix2Pix: Learning to Follow Image Editing Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tim Brooks, Aleksander Holynski, Alexei A. Efros&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.09800&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.timothybrooks.com/instruct-pix2pix&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Versatile Diffusion: Text, Images and Variations All in One Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, Humphrey Shi&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.08332&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SHI-Labs/Versatile-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Direct Inversion: Optimization-Free Text-Driven Real Image Editing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Adham Elarabawy, Harish Kamath, Samuel Denton&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.07825&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Arbitrary Style Guidance for Enhanced Diffusion-Based Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhihong Pan, Xin Zhou, Hao Tian&lt;/em&gt; &lt;br&gt; WACV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.07751&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Patrick Schramowski, Manuel Brack, Björn Deiseroth, Kristian Kersting&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.05105&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ml-research/safe-latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lukas Struppek, Dominik Hintersdorf, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.02408&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LukasStruppek/Rickrolling-the-Artist&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.01324&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://deepimagination.cc/eDiffi/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MagicMix: Semantic Mixing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jun Hao Liew, Hanshu Yan, Daquan Zhou, Jiashi Feng&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.16056&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://magicmix.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UPainting: Unified Text-to-Image Diffusion Generation with Cross-modal Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wei Li, Xue Xu, Xinyan Xiao, Jiachen Liu, Hu Yang, Guohao Li, Zhanpeng Wang, Zhifan Feng, Qiaoqiao She, Yajuan Lyu, Hua Wu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.16031&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;How well can Text-to-Image Generative Models understand Ethical Natural Language Interventions?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hritik Bansal, Da Yin, Masoud Monajatipoor, Kai-Wei Chang&lt;/em&gt; &lt;br&gt; EMNLP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.15230&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Hritikbansal/entigen_emnlp&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, Yu Sun, Hao Tian, Hua Wu, Haifeng Wang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.15257&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, Duen Horng Chau&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.14896&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://poloclub.github.io/diffusiondb/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/poloclub/diffusiondb&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Lafite2: Few-shot Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yufan Zhou, Chunyuan Li, Changyou Chen, Jianfeng Gao, Jinhui Xu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.14124&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Editing via Multi-Stage Blended Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Johannes Ackermann, Minjun Li&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12965&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/pfnet-research/multi-stage-blended-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion with Less Explicit Guidance via Model Predictive Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Max W. Shen, Ehsan Hajiramezanali, Gabriele Scalia, Alex Tseng, Nathaniel Diamant, Tommaso Biancalani, Andreas Loukas&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12192&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Visual Tour Of Current Challenges In Multimodal Language Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shashank Sonkar, Naiming Liu, Richard G. Baraniuk&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.12565&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffEdit: Diffusion-based semantic image editing with mask guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Couairon, Jakob Verbeek, Holger Schwenk, Matthieu Cord&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.11427&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Models already have a Semantic Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingi Kwon, Jaeseok Jeong, Youngjung Uh&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.10960&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://kwonminki.github.io/Asyrp/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dani Valevski, Matan Kalman, Yossi Matias, Yaniv Leviathan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.09477&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruijun Li, Weihua Li, Yi Yang, Hanyu Wei, Jianhua Jiang, Quan Bai&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.09549&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imagic: Text-Based Real Image Editing with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, Michal Irani&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.09276&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://imagic-editing.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Leveraging Off-the-shelf Diffusion Model for Multi-attribute Fashion Image Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaerin Kong, DongHyeon Jeon, Ohjoon Kwon, Nojun Kwak&lt;/em&gt; &lt;br&gt; WACV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.05872&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Diffusion Models&#39; Latent Space, with Applications to CycleDiffusion and Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Henry Wu, Fernando De la Torre&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.05559&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/cycle-diffusion&#34;&gt;Github-1&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/unified-generative-zoo&#34;&gt;Github-2&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Imagen Video: High Definition Video Generation with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.02303&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ivan Kapelyukh, Vitalis Vosylius, Edward Johns&lt;/em&gt; &lt;br&gt; IEEE RA-L 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.02438&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LDEdit: Towards Generalized Text Guided Image Manipulation via Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paramanand Chandramouli, Kanchana Vaishnavi Gandikota&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.02249&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;clip2latent: Text driven sampling of a pre-trained StyleGAN using denoising diffusion and CLIP&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Justin N. M. Pinkney, Chuan Li&lt;/em&gt; &lt;br&gt; BMVC 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.02347&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/justinpinkney/clip2latent&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Membership Inference Attacks Against Text-to-image Generation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yixin Wu, Ning Yu, Zheng Li, Michael Backes, Yang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.00968&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-A-Video: Text-to-Video Generation without Text-Video Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14792&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14988&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Re-Imagen: Retrieval-Augmented Text-to-Image Generator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenhu Chen, Hexiang Hu, Chitwan Saharia, William W. Cohen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14491&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Creative Painting with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xianchao Wu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14697&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Draw Your Art Dream: Diverse Digital Art Synthesis with Multimodal Guided Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nisha Huang, Fan Tang, Weiming Dong, Changsheng Xu&lt;/em&gt; &lt;br&gt; ACM MM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.13360&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/haha-lisa/MGAD-multimodal-guided-artwork-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 27 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Personalizing Text-to-Image Generation via Aesthetic Gradients&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Victor Gallego&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.12330&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/vicgalle/stable-diffusion-aesthetic-gradients&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Best Prompts for Text-to-Image Models and How to Find Them&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikita Pavlichenko, Dmitry Ustalov&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.11711&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The Biased Artist: Exploiting Cultural Biases via Homoglyphs in Text-Guided Image Generation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lukas Struppek, Dominik Hintersdorf, Kristian Kersting&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.08891&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LukasStruppek/The-Biased-Artist&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chen Henry Wu, Saman Motamed, Shaunak Srivastava, Fernando De la Torre&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.06970&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenWu98/Generative-Visual-Prompt&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ISS: Image as Stepping Stone for Text-Guided 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.04145&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/liuzhengzhe/ISS-Image-as-Stepping-Stone-for-Text-Guided-3D-Shape-Generation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, Kfir Aberman&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2208.12242&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Victarry/stable-dreambooth&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-Guided Synthesis of Artistic Images with Retrieval-Augmented Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Björn Ommer&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.13038&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Discrete Contrastive Diffusion for Cross-Modal and Conditional Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, Yan Yan&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2206.07771&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/L-YeZhu/CDCD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blended Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omri Avrahami, Ohad Fried, Dani Lischinski&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.02779&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/blended-latent-diffusion-page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omriav/blended-latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional Visual Generation with Composable Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, Joshua B. Tenenbaum&lt;/em&gt; &lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.01714&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiVAE: Photorealistic Images Synthesis with Denoising Diffusion Decoder&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jie Shi, Chenfei Wu, Jian Liang, Xiang Liu, Nan Duan&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2206.00386&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improved Vector Quantized Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhicong Tang, Shuyang Gu, Jianmin Bao, Dong Chen, Fang Wen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.16007&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/VQ-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Human: Text-Driven Controllable Human Image Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu, Chen Change Loy, Ziwei Liu&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.15996&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yumingj/Text2Human&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/imagen-pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 May 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Retrieval-Augmented Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Andreas Blattmann, Robin Rombach, Kaan Oktay, Björn Ommer&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.11824&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/retrieval-augmented-ddpm&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 25 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2204.06125&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lucidrains/DALLE2-pytorch&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 13 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;KNN-Diffusion: Image Generation via Large-Scale Retrieval&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Oron Ashual, Shelly Sheynin, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, Yaniv Taigman&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2204.02849&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Apr 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;More Control for Free! Image Synthesis with Semantic Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, Trevor Darrell&lt;/em&gt; &lt;br&gt; WACV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2112.05744&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://xh-liu.github.io/sdg/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 10 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vector Quantized Diffusion Model for Text-to-Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, Baining Guo&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.14822&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/microsoft/VQ-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Blended Diffusion for Text-driven Editing of Natural Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Omri Avrahami, Dani Lischinski, Ohad Fried&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2111.14818&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://omriavrahami.com/blended-diffusion-page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/omriav/blended-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tackling the Generative Learning Trilemma with Denoising Diffusion GANs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhisheng Xiao, Karsten Kreis, Arash Vahdat&lt;/em&gt; &lt;br&gt; ICLR 2022 (Spotlight). [&lt;a href=&#34;https://arxiv.org/abs/2112.07804&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/denoising-diffusion-gan&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 15 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionCLIP: Text-guided Image Manipulation Using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Jong Chul Ye&lt;/em&gt; &lt;br&gt; CVPR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2110.02711&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/gwang-kim/DiffusionCLIP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Oct 2021&lt;/p&gt; &#xA;&lt;h3&gt;3D Vision&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-to-3D with Classifier Score Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, Xiaojuan Qi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19415&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Group Choreography using Contrastive Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nhat Le, Tuong Do, Khoa Do, Hien Nguyen, Erman Tjiputra, Quang D. Tran, Anh Nguyen&lt;/em&gt; &lt;br&gt; ACM ToG 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.18986&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D Object Pose Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haobo Jiang, Mathieu Salzmann, Zheng Dang, Jin Xie, Jian Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17359&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;6-DoF Stability Field via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Takuma Yoneda, Tianchong Jiang, Gregory Shakhnarovich, Matthew R. Walter&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.17649&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, Yebin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16818&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Se-Ho Kim, Inyong Koo, Inyoung Lee, Byeongjun Park, Changick Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16349&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yash Kant, Aliaksandr Siarohin, Michael Vasilkovsky, Riza Alp Guler, Jian Ren, Sergey Tulyakov, Igor Gilitschenski&lt;/em&gt; &lt;br&gt; SIGGRAPH ASIA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.16167&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yashkant.github.io/invs/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Wonder3D: Single Image to 3D using Cross-Domain Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, Wenping Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.15008&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Roy Kapon, Guy Tevet, Daniel Cohen-Or, Amit H. Bermano&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.14729&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;High-Quality 3D Face Reconstruction with Affine Convolutional Networks&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiqian Lin, Jiangke Lin, Lincheng Li, Yi Yuan, Zhengxia Zou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.14237&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp, Kangxue Yin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13772&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Generative Modeling for Images, 3D Animations, and Video&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Vikram Voleti&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.13157&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TapMo: Shape-aware Motion Generation of Skeleton-free Characters&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaxu Zhang, Shaoli Huang, Zhigang Tu, Xin Chen, Xiaohang Zhan, Gang Yu, Ying Shan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12678&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijie Pan, Jiachen Lu, Xiatian Zhu, Li Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.12474&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Progressive3D: Progressively Local Editing for Text-to-3D Content Creation with Complex Semantic Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinhua Cheng, Tianyu Yang, Jianan Wang, Yu Li, Lei Zhang, Jian Zhang, Li Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11784&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Structure-guided Network for Tooth Alignment in 2D Photograph&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yulong Dou, Lanzhuju Mei, Dinggang Shen, Zhiming Cui&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.11106&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jia-Wei Liu, Yan-Pei Cao, Jay Zhangjie Wu, Weijia Mao, Yuchao Gu, Rui Zhao, Jussi Keppo, Ying Shan, Mike Zheng Shou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10624&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, Hongdong Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10343&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PaintHuman: Towards High-fidelity Text-to-3D Human Texturing via Denoised Score Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, Wayne Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.09458&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OmniControl: Control Any Joint at Any Time for Human Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, Huaizu Jiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08580&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://neu-vi.github.io/omnicontrol/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistent123: Improve Consistency for One Image to 3D Object Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, C. L. Philip Chen, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.08092&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://consistent-123.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What Does Stable Diffusion Know about the 3D Scene?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guanqi Zhan, Chuanxia Zheng, Weidi Xie, Andrew Zisserman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06836&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HiFi-123: Towards High-fidelity One Image to 3D Content Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wangbo Yu, Li Yuan, Yan-Pei Cao, Xiangjun Gao, Xiaoyu Li, Long Quan, Ying Shan, Yonghong Tian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.06744&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IPDreamer: Appearance-Controllable 3D Object Generation with Image Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang, Jianzhuang Liu, Baochang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05375&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DragD3D: Vertex-based Editing for Realistic Mesh Deformations using 2D Diffusion Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tianhao Xie, Eugene Belilovsky, Sudhir Mudur, Tiberiu Popa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.04561&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ctrl-Room: Controllable Text-to-3D Room Meshes Generation with Layout Constraints&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chuan Fang, Xiaotao Hu, Kunming Luo, Ping Tan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03602&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FreeReg: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haiping Wang, Yuan Liu, Bing Wang, Yujing Sun, Zhen Dong, Wenping Wang, Bisheng Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03420&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistent-1-to-3: Consistent Image to 3D View Synthesis via Geometry-aware Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, Heng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03020&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jianglongye.com/consistent123/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifan Jiang, Hao Tang, Jen-Hao Rick Chang, Liangchen Song, Zhangyang Wang, Liangliang Cao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.03015&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;T$^3$Bench: Benchmarking Current Progress in Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuze He, Yushi Bai, Matthieu Lin, Wang Zhao, Yubin Hu, Jenny Sheng, Ran Yi, Juanzi Li, Yong-Jin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02977&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://t3bench.com/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/THU-LYJ-Lab/T3Bench&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jangho Park, Gihyun Kwon, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02712&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MagicDrive: Street View Generation with Diverse 3D Geometry Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, Qiang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02601&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gaoruiyuan.com/magicdrive/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weiyu Li, Rui Chen, Xuelin Chen, Ping Tan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02596&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sweetdreamer3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hierarchical Generation of Human-Object Interactions with Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, Hujun Bao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.02242&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zju3dv.github.io/hghoi/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zju3dv/hghoi&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying Feng, Yebin Liu, Qing Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.01406&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://humannorm.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Posterior Illumination for Ambiguity-aware Inverse Rendering&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Linjie Lyu, Ayush Tewari, Marc Habermann, Shunsuke Saito, Michael Zollhöfer, Thomas Leimkühler, Christian Theobalt&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00362&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EPiC-ly Fast Particle Cloud Generation with Flow-Matching and Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Erik Buhmann, Cedric Ewen, Darius A. Faroughy, Tobias Golling, Gregor Kasieczka, Matthew Leigh, Guillaume Quétant, John Andrew Raine, Debajyoti Sengupta, David Shih&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.00049&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Consistent123: One Image to Highly Consistent 3D Asset Using Case-Aware Diffusion Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu, Yachao Zhang, Xiu Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.17261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Object Motion Guided Human Motion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaman Li, Jiajun Wu, C. Karen Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.16237&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ITEM3D: Illumination-Aware Directional Texture Editing for 3D Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shengqi Liu, Zhuo Chen, Jingnan Gao, Yichao Yan, Wenhan Zhu, Xiaobo Li, Ke Gao, Jiangjing Lyu, Xiaokang Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.14872&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Light Field Diffusion for Single-View Novel View Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifeng Xiong, Haoyu Ma, Shanlin Sun, Kun Han, Xiaohui Xie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11525&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent Diffusion Models for Structural Component Design&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ethan Herron, Jaydeep Rade, Anushrut Jignasu, Baskar Ganapathysubramanian, Aditya Balu, Soumik Sarkar, Adarsh Krishnamurthy&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11601&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Stefan Stan, Kazi Injamamul Haque, Zerrin Yumak&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11306&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TwinTex: Geometry-aware Texture Generation for Abstracted 3D Architectural Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weidan Xiong, Hongqian Zhang, Botao Peng, Ziyu Hu, Yongli Wu, Jianwei Guo, Hui Huang&lt;/em&gt; &lt;br&gt; SIGGRAPH ASIA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.11258&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Language-Conditioned Affordance-Pose Detection in 3D Point Clouds&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Toan Nguyen, Minh Nhat Vu, Baoru Huang, Tuan Van Vo, Vy Truong, Ngan Le, Thieu Vo, Bac Le, Anh Nguyen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.10911&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaouther Mouheb, Mobina Ghojogh Nejad, Lavsen Dahal, Ehsan Samei, W. Paul Segars, Joseph Y. Lo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.08289&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised Disentangling of Facial Representations with 3D-aware Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruian He, Zhen Xing, Weimin Tan, Bo Yan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.08273&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;M3Dsynth: A dataset of medical 3D images with AI-generated local manipulations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Giada Zingarini, Davide Cozzolino, Riccardo Corvi, Giovanni Poggi, Luisa Verdoliva&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07973&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Large-Vocabulary 3D Diffusion Model with Transformer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziang Cao, Fangzhou Hong, Tong Wu, Liang Pan, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07920&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ziangcao0312.github.io/difftf_pages/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ziangcao0312/DiffTF&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UnifiedGesture: A Unified Gesture Synthesis Model for Multiple Skeletons&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sicheng Yang, Zilin Wang, Zhiyong Wu, Minglei Li, Zhensong Zhang, Qiaochu Huang, Lei Hao, Songcen Xu, Xiaofei Wu, changpeng yang, Zonghong Dai&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07051&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yin Wang, Zhiying Leng, Frederick W. B. Li, Shun-Cheng Wu, Xiaohui Liang&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.06284&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SyncDreamer: Generating Multiview-consistent Images from a Single-view Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, Wenping Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03453&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://liuyuan-pal.github.io/SyncDreamer/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/liuyuan-pal/SyncDreamer&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SADIR: Shape-Aware Diffusion Models for 3D Image Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nivetha Jayakumar, Tonmoy Hossain, Miaomiao Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03335&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MCM: Multi-condition Motion Synthesis Framework for Multi-scenario&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zeyu Ling, Bo Han, Yongkang Wong, Mohan Kangkanhalli, Weidong Geng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03031&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang, Yi Yang&lt;/em&gt; &lt;br&gt; AAAI 2024. [&lt;a href=&#34;https://arxiv.org/abs/2309.01372&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BuilDiff: 3D Building Shape Generation using Single-Image Conditional Point Cloud Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yao Wei, George Vosselman, Michael Ying Yang&lt;/em&gt; &lt;br&gt; ICCV Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.00158&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MVDream: Multi-view Diffusion for 3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, Xiao Yang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16512&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Inertial Poser: Human Motion Reconstruction from Arbitrary Sparse IMU Configurations&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tom Van Wouwe, Seunghwan Lee, Antoine Falisse, Scott Delp, C. Karen Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16682&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, Liang-Yan Gui&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.16905&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sirui-xu.github.io/InterDiff/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Sirui-Xu/InterDiff&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 31 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Priority-Centric Human Motion Generation in Discrete Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14480&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HoloFusion: Towards Photo-realistic 3D Generative Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Animesh Karnewar, Niloy J. Mitra, Andrea Vedaldi, David Novotny&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14244&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://holodiffusion.github.io/holofusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Abril Corona-Figueroa, Sam Bond-Taylor, Neelanjan Bhowmik, Yona Falinie A. Gaus, Toby P. Breckon, Hubert P. H. Shum, Chris G. Willcocks&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14152&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sparse3D: Distilling Multiview-Consistent Diffusion for Object Reconstruction from Sparse Views&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zi-Xin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang, Ying Shan, Song-Hai Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14078&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Multi-plane denoising diffusion-based dimensionality expansion for 2D-to-3D reconstruction of microstructures with harmonized sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kang-Hyun Lee, Gun Jin Yun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14035&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The DiffuseStyleGesture+ entry to the GENEA Challenge 2023&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sicheng Yang, Haiwei Xue, Zhensong Zhang, Minglei Li, Zhiyong Wu, Xiaofei Wu, Songcen Xu, Zonghong Dai&lt;/em&gt; &lt;br&gt; ICMI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13879&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/YoungSeng/DiffuseStyleGesture/tree/DiffuseStyleGesturePlus/BEAT-TWH-main&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Distribution-Aligned Diffusion for Human Mesh Recovery&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lin Geng Foo, Jia Gong, Hossein Rahmani, Jun Liu&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13369&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gongjia0208.github.io/HMDiff/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minda Zhao, Chaoyi Zhao, Xinyue Liang, Lincheng Li, Zeng Zhao, Zhipeng Hu, Changjie Fan, Xin Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.13223&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Se Jin Park, Joanna Hong, Minsu Kim, Yong Man Ro&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.05934&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LongDanceDiff: Long-term Dance Generation with Conditional Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siqi Yang, Zejun Yang, Zhisheng Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.11945&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IT3D: Improved Text-to-3D Generation with Explicit View Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, Guosheng Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.11473&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/buaacyw/IT3D-text-to-3D&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 22 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Texture Generation on 3D Meshes with Point-UV Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, Xiaojuan Qi&lt;/em&gt; &lt;br&gt; ICCV 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10490&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Physics-Guided Human Motion Capture with Pose Probability Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyi Ju, Buzhen Huang, Chen Zhu, Zhihao Li, Yangang Wang&lt;/em&gt; &lt;br&gt; IJCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09910&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Me-Ditto/Physics-Guided-Mocap&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 19 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haorui Ji, Hui Deng, Yuchao Dai, Hongdong Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.10705&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MATLABER: Material-Aware Text-to-3D via LAtent BRDF auto-EncodeR&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xudong Xu, Zhaoyang Lyu, Xingang Pan, Bo Dai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09278&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sheldontsui.github.io/projects/Matlaber&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;O^2-Recon: Completing 3D Reconstruction of Occluded Objects in the Scene with a Pre-trained 2D Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yubin Hu, Sheng Ye, Wang Zhao, Matthieu Lin, Yuze He, Yu-Hui Wen, Ying He, Yong-Jin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09591&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion for 3D Hand Pose Estimation from Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Maksym Ivashechkin, Oscar Mendez, Richard Bowden&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09523&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PoSynDA: Multi-Hypothesis Pose Synthesis Domain Adaptation for Robust 3D Human Pose Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanbing Liu, Jun-Yan He, Zhi-Qi Cheng, Wangmeng Xiang, Qize Yang, Wenhao Chai, Gaoang Wang, Xu Bao, Bin Luo, Yifeng Geng, Xuansong Xie&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09678&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hbing-l/PoSynDA&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Guide3D: Create 3D Avatars from Text and Image Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09705&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DMCVR: Morphology-Guided Diffusion Model for 3D Cardiac Volume Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoxiao He, Chaowei Tan, Ligong Han, Bo Liu, Leon Axel, Kang Li, Dimitris N. Metaxas&lt;/em&gt; &lt;br&gt; MICCAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09223&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hexiaoxiao-cs/DMCVR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanLiff: Layer-wise 3D Human Generation with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shoukang Hu, Fangzhou Hong, Tao Hu, Liang Pan, Haiyi Mei, Weiye Xiao, Lei Yang, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.09712&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://skhu101.github.io/HumanLiff/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 18 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Watch Your Steps: Local Image and Scene Editing by Text Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A. Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G. Derpanis, Igor Gilitschenski&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.08947&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ashmrz.github.io/WatchYourSteps/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 17 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TeCH: Text-guided Reconstruction of Lifelike Clothed Humans&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, Justus Thies&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.08545&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://huangyangyi.github.io/TeCH/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/huangyangyi/TeCH&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CCD-3DR: Consistent Conditioning in Diffusion for Single-Image 3D Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yan Di, Chenyangguang Zhang, Pengyuan Wang, Guangyao Zhai, Ruida Zhang, Fabian Manhardt, Benjamin Busam, Xiangyang Ji, Federico Tombari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07837&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bosheng Qin, Wentao Ye, Qifan Yu, Siliang Tang, Yueting Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.07749&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Scene Diffusion Guidance using Scene Graphs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammad Naanaa, Katharina Schmid, Yinyu Nie&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.04468&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Cloth2Tex: A Customized Cloth Texture Generation Pipeline for 3D Virtual Try-On&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daiheng Gao, Xu Chen, Xindi Zhang, Qi Wang, Ke Sun, Bang Zhang, Liefeng Bo, Qixing Huang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.04288&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarVerse: High-quality &amp;amp; Stable 3D Avatar Creation from Text and Pose&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu Wang, Li Chen, Chao Long, Feida Zhu, Kang Du, Min Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.03610&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://avatarverse3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 7 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Approach for Probabilistic Human Mesh Recovery using Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hanbyel Cho, Junmo Kim&lt;/em&gt; &lt;br&gt; ICCV Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02963&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/hanbyel0105/Diff-HMR&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 5 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiaosong Qi, Le Zhuo, Aixi Zhang, Yue Liao, Fei Fang, Si Liu, Shuicheng Yan&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02915&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zijie Wu, Yaonan Wang, Mingtao Feng, He Xie, Ajmal Mian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02874&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;On the Transition from Neural Representation to Symbolic Knowledge&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyan Cheng, Peter Chin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.02000&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhao Yang, Bing Su, Ji-Rong Wen&lt;/em&gt; &lt;br&gt; ACM MM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.01850&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/yangzhao1230/PCMDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HD-Fusion: Detailed Text-to-3D Generation Leveraging Multiple Noise Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jinbo Wu, Xiaobo Gao, Xing Liu, Zhengyang Shen, Chen Zhao, Haocheng Feng, Jingtuo Liu, Errui Ding&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.16183&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TransFusion: A Practical and Effective Transformer-based Diffusion Model for 3D Human Motion Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sibo Tian, Minghui Zheng, Xiao Liang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.16106&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TEDi: Temporally-Entangled Diffusion for Long-Term Motion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zihan Zhang, Richard Liu, Kfir Aberman, Rana Hanocka&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.15042&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin Wang, Fan Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.13908&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 26 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fake It Without Making It: Conditioned Face Generation for Accurate 3D Face Shape Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Will Rowan, Patrik Huber, Nick Pears, Andrew Keeling&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.13639&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, Leonidas Guibas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.07511&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nileshkulkarni.github.io/nifty/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarFusion: Zero-shot Generation of Clothing-Decoupled 3D Avatars Using 2D Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shuo Huang, Zongxin Yang, Liangting Li, Yi Yang, Jia Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.06526&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexander W. Bergman, Wang Yifan, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.04859&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhongyu Jiang, Zhuoran Zhou, Lei Li, Wenhao Chai, Cheng-Yen Yang, Jenq-Neng Hwang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.03833&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AutoDecoding Latent 3D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Van Gool, Sergey Tulyakov&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.05445&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SVDM: Single-View Diffusion Model for Pseudo-Stereo 3D Object Detection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuguang Shi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.02270&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 5 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shentong Mo, Enze Xie, Ruihang Chu, Lewei Yao, Lanqing Hong, Matthias Nießner, Zhenguo Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.01831&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Jul 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, Bernard Ghanem&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17843&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://guochengqian.github.io/project/magic123/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, Shenghua Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.17115&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffComplete: Diffusion-based Generative 3D Shape Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruihang Chu, Enze Xie, Shentong Mo, Zhenguo Li, Matthias Nießner, Chi-Wing Fu, Jiaya Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.16329&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, Lei Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.12422&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;EMoG: Synthesizing Emotive Co-speech 3D Gesture with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lianying Yin, Yijun Wang, Tianyu He, Jinming Liu, Wei Zhao, Bohan Li, Xin Jin, Jianxin Lin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.11496&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Point-Cloud Completion with Pretrained Text-to-image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yoni Kasten, Ohad Rahamim, Gal Chechik&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.10533&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarBooth: High-Quality and Customizable 3D Human Avatar Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, Xun Cao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09864&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Edit-DiffNeRF: Editing 3D Neural Radiance Fields using 2D Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lu Yu, Wei Xiang, Kang Han&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.09551&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adding 3D Geometry Control to Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wufei Ma, Qihao Liu, Jiahao Wang, Angtian Wang, Yaoyao Liu, Adam Kortylewski, Alan Yuille&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.08103&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Stanislaw Szymanowicz, Christian Rupprecht, Andrea Vedaldi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07881&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D molecule generation by denoising voxel grids&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Pedro O. Pinheiro, Joshua Rackers, Joseph Kleinhenz, Michael Maser, Omar Mahmood, Andrew Martin Watkins, Stephen Ra, Vishnu Sresht, Saeed Saremi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07473&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 13 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying Shan, Shenghua Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.07154&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchen Zhou, Ying He, F. Richard Yu, Jianqiang Li, You Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05668&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stochastic Multi-Person 3D Motion Forecasting&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sirui Xu, Yu-Xiong Wang, Liang-Yan Gui&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.05421&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chun-Han Yao, Amit Raj, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, Varun Jampani&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04619&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Synthesizing realistic sand assemblies with denoising diffusion in latent space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikolaos N. Vlassis, WaiChing Sun, Khalid A. Alshibli, Richard A. Regueiro&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.04411&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarStudio: Text-driven Editing of 3D Dynamic Human Head Avatars&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohit Mendiratta, Xingang Pan, Mohamed Elgharib, Kartik Teotia, Mallikarjun B R, Ayush Tewari, Vladislav Golyanik, Adam Kortylewski, Christian Theobalt&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00547&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffRoom: Diffusion-based High-Quality 3D Room Reconstruction and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaoliang Ju, Zhaoyang Huang, Yijin Li, Guofeng Zhang, Yu Qiao, Hongsheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00519&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yi Shi, Jingbo Wang, Xuekun Jiang, Bo Dai&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00416&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://controllablemdm.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FDNeRF: Semantics-Driven Face Reconstruction, Prompt Editing and Relighting with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hao Zhang, Yanbo Xu, Tianyuan Dai, Yu-Wing, Tai Chi-Keung Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2306.00783&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Jun 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Explicit Contact for Implicit Reconstruction of Hand-held Objects from Monocular Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junxing Hu, Hongwen Zhang, Zerui Chen, Mengcheng Li, Yunlong Wang, Yebin Liu, Zhenan Sun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.20089&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://junxinghu.github.io/projects/hoi.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chi Zhang, Yiwen Chen, Yijun Fu, Zhenglin Zhou, Gang YU, Billzb Wang, Bin Fu, Tao Chen, Guosheng Lin, Chunhua Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.19012&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junzhe Zhu, Peiye Zhuang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18766&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conditional Diffusion Models for Semantic 3D Medical Image Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zolnamar Dorjsembe, Hsing-Kuo Pao, Sodtavilan Odonchimed, Furen Xiao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.18453&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ZeroAvatar: Zero-shot 3D Avatar Generation from a Single Image&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenzhen Weng, Zeyu Wang, Serena Yeung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16411&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NAP: Neural 3D Articulation Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiahui Lei, Congyue Deng, Bokui Shen, Leonidas Guibas, Kostas Daniilidis&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16315&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://www.cis.upenn.edu/~leijh/projects/nap/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guangyao Zhai, Evin Pınar Örnek, Shun-Cheng Wu, Yan Di, Federico Tombari, Nassir Navab, Benjamin Busam&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16283&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, Jun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.16213&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ml.cs.tsinghua.edu.cn/prolificdreamer/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sitian Shen, Zilin Zhu, Linqian Fan, Harry Zhang, Xinxiao Wu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15957&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tsu-Ching Hsiao, Hao-Wei Chen, Hsuan-Kung Yang, Chun-Yi Lee&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15873&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Deceptive-NeRF: Enhancing NeRF Reconstruction using Pseudo-Observations from Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xinhang Liu, Shiu-hong Kao, Jiaben Chen, Yu-Wing Tai, Chi-Keung Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15171&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Manifold Diffusion Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ahmed A. Elhag, Joshua M. Susskind, Miguel Angel Bautista&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15586&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rundi Wu, Ruoshi Liu, Carl Vondrick, Changxi Zheng&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.15399&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sin3dm.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Sin3DM/Sin3DM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding Text-driven Motion Synthesis with Keyframe Collaboration via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dong Wei, Xiaoning Sun, Huaijiang Sun, Bin Li, Shengxiang Hu, Weiqing Li, Jianfeng Lu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13773&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffHand: End-to-End Hand Mesh Reconstruction via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lijun Li, Li&#39;an Zhuo, Bang Zhang, Liefeng Bo, Chen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.13705&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GMD: Controllable Human Motion Synthesis via Guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, Siyu Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12577&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://korrawe.github.io/gmd-project/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Globally Consistent Stochastic Human Motion Prediction via Motion Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiarui Sun, Girish Chowdhary&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.12554&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Few-shot 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyuan Zhu, Huimin Ma, Jiansheng Chen, Jian Yuan&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11664&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Chupa: Carving 3D Clothed Humans from Skinned Shape Priors using 2D Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Byungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi Lee, Sookwan Han, Daesik Kim, Hanbyul Joo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11870&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://snuvclab.github.io/chupa/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11588&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Liangchen Song, Liangliang Cao, Hongyu Xu, Kai Kang, Feng Tang, Junsong Yuan, Yang Zhao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.11337&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LDM3D: Latent Diffusion Model for 3D&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, Vasudev Lal&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.10853&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 18 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh, Sonal Gupta&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09662&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://azadis.github.io/make-an-animation/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FitMe: Deep Photorealistic 3D Morphable Model Avatars&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Baris Gecer, Jiankang Deng, Stefanos Zafeiriou&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09641&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://alexlattas.com/fitme&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AMD: Autoregressive Motion Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Han, Hao Peng, Minjing Dong, Chang Xu, Yi Ren, Yixuan Shen, Yuheng Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.09381&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text-guided High-definition Consistency Texture Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhibin Tang, Tiantong He&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.05901&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Relightify: Relightable 3D Faces from a Single Image via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, Stefanos Zafeiriou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.06077&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://foivospar.github.io/Relightify/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 10 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CaloClouds: Fast Geometry-Independent Highly-Granular Calorimeter Simulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Erik Buhmann, Sascha Diefenbacher, Engin Eren, Frank Gaede, Gregor Kasieczka, Anatolii Korol, William Korcari, Katja Krüger, Peter McKeown&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04847&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Locally Attentional SDF Diffusion for Controllable 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, Heung-Yeung Shum&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.04461&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffFacto: Controllable Part-Based 3D Point Cloud Generation with Cross Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui Huang, Shi-Min Hu, Ke Li, Leonidas J Guibas&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01921&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://difffacto.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Shap-E: Generating Conditional 3D Implicit Functions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Heewoo Jun, Alex Nichol&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.02463&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/shap-e&#34;&gt;Github&lt;/a&gt;] 3 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zehao Zhu, Jiashun Wang, Yuzhe Qin, Deqing Sun, Varun Jampani, Xiaolong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01618&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zehaozhu.github.io/ContactArt/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On without 3D Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mehmet Saygin Seyfioglu, Karim Bouyarmane, Suren Kumar, Amir Tavanaei, Ismail B. Tutar&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2305.01257&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 May 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning a Diffusion Prior for NeRFs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guandao Yang, Abhijit Kundu, Leonidas J. Guibas, Jonathan T. Barron, Ben Poole&lt;/em&gt; &lt;br&gt; ICLR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.14473&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TextMesh: Generation of Realistic 3D Meshes From Text Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, Federico Tombari&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.12439&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nerfbusters: Removing Ghostly Artifacts from Casually Captured NeRFs&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Holynski, Angjoo Kanazawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10532&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ethanweber.me/nerfbusters/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ethanweber/nerfbusters&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Farm3D: Learning Articulated 3D Animals by Distilling 2D Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10535&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://farm3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Anything-3D: Towards Single-view Anything Reconstruction in the Wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Qiuhong Shen, Xingyi Yang, Xinchao Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.10261&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke, Ali Thabet, Artsiom Sanakoyeu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.08577&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://dulucas.github.io/agrol/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Towards Controllable Diffusion Models via Reward-Guided Exploration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hengtong Zhang, Tingyang Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.07132&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning Controllable 3D Diffusion Models from Single-view Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiatao Gu, Qingzhe Gao, Shuangfei Zhai, Baoquan Chen, Lingjie Liu, Josh Susskind&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06700&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jiataogu.me/control3diff/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, Hao Su&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06714&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://lakonik.github.io/ssdnerf/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 13 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, Siyu Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.06024&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sanweiliti.github.io/egohmr/egohmr.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, Lan Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.05684&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/tr3e/InterGen&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siwei Zhang, Qianli Ma, Yan Zhang, Sadegh Aliakbarian, Darren Cosker, Siyu Tang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sanweiliti.github.io/egohmr/egohmr.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mohammadreza Armandpour, Huangjie Zheng, Ali Sadeghian, Amir Sadeghian, Mingyuan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04968&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://perp-neg.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 11 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRF applied to satellite imagery for surface reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Federico Semeraro, Yi Zhang, Wenying Wu, Patrick Carroll&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04133&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/fsemerar/satnerf&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hoigi Seo, Hayeon Kim, Gwanghyun Kim, Se Young Chun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02827&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://janeyeon.github.io/ditto-nerf/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Novel View Synthesis with 3D-Aware Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini De Mello, Tero Karras, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.02602&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/genvs/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, Or Litany&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01893&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://research.nvidia.com/labs/toronto-ai/trace-pace/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Ji Ha Jang, Se Young Chun&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01900&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://gwang-kim.github.io/podia_3d/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 4 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.01116&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/mingyuan-zhang/ReMoDiffuse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Motion Synthesis and Reconstruction with Autoregressive Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjie Yin, Ruibo Tu, Hang Yin, Danica Kragic, Hedvig Kjellström, Mårten Björkman&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.04681&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, Kwan-Yee K. Wong&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.00916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 3 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, Jingyi Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2304.03117&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/dreamface&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Apr 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, Jing Liao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.17606&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://avatar-craft.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/songrise/avatarcraft&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HOLODIFFUSION: Training a 3D Diffusion Model using 2D Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Animesh Karnewar, Andrea Vedaldi, David Novotny, Niloy Mitra&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16509&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://holodiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;4D Facial Expression Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kaifeng Zou, Sylvain Faisan, Boyang Yu, Sébastien Valette, Hyewon Seo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.16611&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZOUKaifeng/4DFM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Ishii, Takuya Narihira&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15780&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sony.github.io/Instruct3Dto3D-doc/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://sony.github.io/Instruct3Dto3D-doc/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Novel View Synthesis of Humans using Differentiable Rendering&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guillaume Rochette, Chris Russell, Richard Bowden&lt;/em&gt; &lt;br&gt; IEEE T-BIOM 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15880&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/GuillaumeRochette/HumanViewSynthesis&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Susung Hong, Donghoon Ahn, Seungryong Kim&lt;/em&gt; &lt;br&gt; CVPR Workshop 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15413&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 27 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, Dong Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.14184&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://make-it-3d.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://make-it-3d.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ISS++: Image as Stepping Stone for Text-Guided 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.15181&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin, Hui Xiong, Lin Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13843&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fantasia3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rui Chen, Yongwei Chen, Ningxin Jiao, Kui Jia&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13873&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://fantasia3d.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Gorilla-Lab-SCUT/Fantasia3D&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ce Zheng, Guo-Jun Qi, Chen Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.13397&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, Angjoo Kanazawa&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12789&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://instruct-nerf2nerf.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianglong Ye, Naiyan Wang, Xiaolong Wang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12786&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jianglongye.com/featurenerf/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 22 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vox-E: Text-guided Voxel Editing of 3D Objects&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Etai Sella, Gal Fiebelman, Peter Hedman, Hadar Averbuch-Elor&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://tau-vailab.github.io/Vox-E/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Compositional 3D Scene Generation using Locally Conditioned Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ryan Po, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12218&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ryanpo.com/comp3d/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, Wen Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11579&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/paTRICK-swk/D3DP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yu-Jhe Li, Kris Kitani&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11938&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Affordance Diffusion: Synthesizing Hand-Object Interactions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello, Stan Birchfield, Jiaming Song, Shubham Tulsiani, Sifei Liu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12538&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://judyye.github.io/affordiffusion-www/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, Minhyuk Sung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.12236&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://salad3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learning a 3D Morphable Face Reflectance Model from Low-cost Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuxuan Han, Zhibo Wang, Feng Xu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11686&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yxuhan.github.io/ReflectanceMM/index.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 21 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text2Tex: Text-driven Texture Synthesis via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, Matthias Nießner&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11396&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://daveredrum.github.io/Text2Tex/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero-1-to-3: Zero-shot One Image to 3D Object&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, Carl Vondrick&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.11328&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zero123.cs.columbia.edu/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SKED: Sketch-guided Text-based 3D Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Aryan Mikaeili, Or Perel, Daniel Cohen-Or, Ali Mahdavi-Amiri&lt;/em&gt; &lt;br&gt; arxiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10735&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yuhan Li, Yishun Dou, Xuanhong Chen, Bingbing Ni, Yilin Sun, Yutian Liu, Fuzhen Wang&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.10406&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/colorful-liyu/3DQD&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 18 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, Lequan Yu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09119&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Advocate99/DiffGesture&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-HPC: Generating Synthetic Images with Realistic Humans&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhenzhen Weng, Laura Bravo-Sánchez, Serena Yeung&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09541&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ZZWENG/Diffusion_HPC&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars&lt;/strong&gt; &lt;br&gt; &lt;em&gt;David Svitov, Dmitrii Gudkov, Renat Bashirov, Victor Lempitsky&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.09375&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Suhyeon Lee, Hyungjin Chung, Minyoung Park, Jonghyuk Park, Wi-Sun Ryu, Jong Chul Ye&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08440&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 15 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Controllable Mesh Generation Through Sparse Latent Point Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua Lin, Bo Dai&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07938&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://slide-3d.github.io/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MeshDiffusion: Score-based Generative 3D Mesh Modeling&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, Weiyang Liu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08133&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://meshdiffusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lzzcd001/MeshDiffusion/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Point Cloud Diffusion Models for Automatic Implant Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Paul Friedrich, Julia Wolleb, Florentin Bieder, Florian M. Thieringer, Philippe C. Cattin&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.08061&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.07937&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/KU-CVLAB/3DFuse&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GECCO: Geometrically-Conditioned Point Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Michał J. Tyszkiewicz, Pascal Fua, Eduard Trulls&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05916&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DGen: Triplane Latent Diffusion for Textured Mesh Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, Barlas Oğuz&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.05371&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Human Motion Diffusion as a Generative Prior&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yonatan Shafir, Guy Tevet, Roy Kapon, Amit H. Bermano&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2303.01418&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Mar 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can We Use Diffusion Probabilistic Models for 3D Motion Prediction?&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hyemin Ahn, Esteve Valls Mascaro, Dongheui Lee&lt;/em&gt; &lt;br&gt; ICRA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.14503&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sites.google.com/view/diffusion-motion-prediction&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/cotton-ahn/diffusion-motion-prediction&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 28 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jamie Wynn, Daniyar Turmukhambetov&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.12231&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/nianticlabs/diffusionerf&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/lukemelas/projection-conditioned-point-cloud-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PC2: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Luke Melas-Kyriazi, Christian Rupprecht, Andrea Vedaldi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10668&#34;&gt;Paper&lt;/a&gt;] &lt;a href=&#34;https://lukemelas.github.io/projection-conditioned-point-cloud-diffusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 23 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NerfDiff: Single-image View Synthesis with NeRF-guided Distillation from 3D-aware Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, Ravi Ramamoorthi&lt;/em&gt; &lt;br&gt; ICML 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.10109&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jiataogu.me/nerfdiff/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 20 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SinMDM: Single Motion Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar, Amit H. Bermano, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05905&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sinmdm.github.io/SinMDM-page/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/SinMDM/SinMDM&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Colored Shape Reconstruction from a Single RGB Image through Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Li, Xiaolin Wei, Fengwei Chen, Bin Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.05573&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;HumanMAC: Masked Motion Completion for Human Motion Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, Tongliang Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.03665&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://lhchen.top/Human-MAC/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/LinghaoChan/HumanMAC&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TEXTure: Text-Guided Texturing of 3D Shapes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.01721&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://texturepaper.github.io/TEXTurePaper/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TEXTurePaper/TEXTurePaper&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 3 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Zero3D: Semantic-Driven Multi-Category 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Bo Han, Yitong Liu, Yixuan Shen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.13591&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Wavelet-domain Diffusion for 3D Shape Generation, Inversion, and Manipulation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Ruihui Li, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; SIGGRAPH ASIA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2302.00190&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/edward1997104/Wavelet-Generation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Feb 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Biao Zhang, Jiapeng Tang, Matthias Niessner, Peter Wonka&lt;/em&gt; &lt;br&gt; SIGGRAPH 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.11445&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://1zb.github.io/3DShape2VecSet/&#34;&gt;Github&lt;/a&gt;] [&lt;a href=&#34;https://github.com/1zb/3DShape2VecSet&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 26 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fan Zhang, Naye Ji, Fuxing Gao, Yongping Li&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10047&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bipartite Graph Diffusion Model for Human Interaction Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Baptiste Chopin, Hao Tang, Mohamed Daoudi&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.10134&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 24 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-based Generation, Optimization, and Planning in 3D Scenes&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, Song-Chun Zhu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.06015&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://scenediffuser.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/scenediffuser/Scene-Diffuser&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 15 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Modiff: Action-Conditioned 3D Motion Generation with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai, Nicu Sebe&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.03949&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jumin Lee, Woobin Im, Sebin Lee, Sung-Eui Yoon&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2301.00527&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zoomin-lee/scene-scale-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Jan 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, Shenghua Gao&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.14704&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://bluestyle97.github.io/dream3d/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 28 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Point-E: A System for Generating 3D Point Clouds from Complex Prompts&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, Mark Chen&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08751&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/openai/point-e&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Real-Time Rendering of Arbitrary Surface Geometries using Learnt Transfer&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sirikonda Dhawal, Aakash KT, P.J. Narayanan&lt;/em&gt; &lt;br&gt; ICVGIP 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.09315&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 19 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Unifying Human Motion Synthesis and Style Transfer with Denoising Diffusion Probabilistic Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ziyi Chang, Edmund J. C. Findlay, Haozheng Zhang, Hubert P. H. Shum&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.08526&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rodin: A Generative Model for Sculpting 3D Digital Avatars Using Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, Baining Guo&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.06135&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://3d-avatar-diffusion.microsoft.com/#/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative Scene Synthesis via Incremental View Inpainting using RGBD Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiabao Lei, Jiapeng Tang, Kui Jia&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.05993&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jblei.site/project-pages/rgbd-diffusion.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Karbo123/RGBD-Diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 12 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ego-Body Pose Estimation via Ego-Head Pose Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiaman Li, C. Karen Liu, Jiajun Wu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.04636&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, Christian Theobalt&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.04495&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/MoFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, Liangyan Gui&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.04493&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://yccyenchicheng.github.io/SDFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Executing your Commands via Motion Diffusion in Latent Space&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, Gang Yu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.04048&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://chenxin.tech/mld/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ChenFengYe/motion-latent-diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Magic: Multi Art Genre Intelligent Choreography Dataset and Network for 3D Dance Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Xiu Li&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03741&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Congyue Deng, Chiyu &#34;Max&#39;&#39; Jiang, Charles R. Qi, Xinchen Yan, Yin Zhou, Leonidas Guibas, Dragomir Anguelov&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.03267&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion-SDF: Text-to-Shape via Voxelized Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Muheng Li, Yueqi Duan, Jie Zhou, Jiwen Lu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.03293&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/ttlmh/Diffusion-SDF&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pretrained Diffusion Models for Unified Human Motion Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jianxin Ma, Shuai Bai, Chang Zhou&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02837&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://ofa-sys.github.io/MoFusion/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuPose: Monocular 3D Human Pose Estimation via Denoising Diffusion Probabilistic Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jeongjun Choi, Dongseok Shim, H. Jin Kim&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02796&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PhysDiff: Physics-Guided Human Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, Jan Kautz&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.02500&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nvlabs.github.io/PhysDiff/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 5 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast Point Cloud Generation with Straight Flows&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Lemeng Wu, Dilin Wang, Chengyue Gong, Xingchao Liu, Yunyang Xiong, Rakesh Ranjan, Raghuraman Krishnamoorthi, Vikas Chandra, Qiang Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.01747&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffRF: Rendering-Guided 3D Radiance Field Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Norman Müller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulò, Peter Kontschieder, Matthias Nießner&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.01206&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sirwyver.github.io/DiffRF/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 2 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, Paul Guerrero&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2212.00842&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, Greg Shakhnarovich&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.00774&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://pals.ttic.edu/p/score-jacobian-chaining&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhizhuo Zhou, Shubham Tulsiani&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2212.00792&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://sparsefusion.github.io/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://sparsefusion.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 1 Dec 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Neural Field Generation using Triplane Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, Gordon Wetzstein&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16677&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://jryanshue.com/nfd/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffPose: Toward More Reliable 3D Pose Estimation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hossein Rahmani, Jun Liu&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.16940&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/GONGJIA0208/Diffpose&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 30 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Karl Holmquist, Bastian Wandt&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16487&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/paTRICK-swk/D3DP&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gwanghyun Kim, Se Young Chun&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.16374&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://datid-3d.github.io/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360° Views&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, Zhangyang Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16431&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://vita-group.github.io/NeuralLift-360/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/VITA-Group/NeuralLift-360&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Ada3Diff: Defending against 3D Adversarial Point Clouds via Adaptive Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Kui Zhang, Hang Zhou, Jie Zhang, Qidong Huang, Weiming Zhang, Nenghai Yu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16247&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;UDE: A Unified Driving Engine for Human Motion Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zixiang Zhou, Baoyuan Wang&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.16016&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://zixiangzhou916.github.io/UDE/&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zixiangzhou916/UDE/&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 29 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, Dacheng Tao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.14108&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 25 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionSDF: Conditional Generative Modeling of Signed Distance Functions&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gene Chou, Yuval Bahat, Felix Heide&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13757&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/princeton-computational-imaging/Diffusion-SDF&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 24 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Tetrahedral Diffusion Models for 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Nikolai Kalischek, Torben Peters, Jan D. Wegner, Konrad Schindler&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.13220&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IC3D: Image-Conditioned 3D Diffusion for Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Cristian Sbrolli, Paolo Cudrano, Matteo Frosi, Matteo Matteucci&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.10865&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 20 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Listen, denoise, action! Audio-driven motion synthesis with diffusion models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Simon Alexanderson, Rajmund Nagy, Jonas Beskow, Gustav Eje Henter&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.09707&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Titas Anciukevičius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy J. Mitra, Paul Guerrero&lt;/em&gt; &lt;br&gt; CVPR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.09869&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/Anciukevicius/RenderDiffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 17 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.07600&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/eladrich/latent-nerf&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 14 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ReFu: Refine and Fuse the Unobserved View for Detail-Preserving Single-Image 3D Human Reconstruction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Gyumin Shim, Minsoo Lee, Jaegul Choo&lt;/em&gt; &lt;br&gt; ACM 2022. [&lt;a href=&#34;https://arxiv.org/abs/2211.04753&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 9 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;StructDiffusion: Object-Centric Diffusion for Semantic Rearrangement of Novel Objects&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weiyu Liu, Tucker Hermans, Sonia Chernova, Chris Paxton&lt;/em&gt; &lt;br&gt; RSS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2211.04604&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 8 Nov 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhiyuan Ren, Zhihong Pan, Xin Zhou, Le Kang&lt;/em&gt; &lt;br&gt; ICASSP 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.12315&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 22 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LION: Latent Point Diffusion Models for 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, Karsten Kreis&lt;/em&gt; &lt;br&gt; NeurIPS 2022. [&lt;a href=&#34;https://arxiv.org/pdf/2210.06978.pdf&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://nv-tlabs.github.io/LION/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Human Joint Kinematics Diffusion-Refinement for Stochastic Motion Prediction&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dong Wei, Huaijiang Sun, Bin Li, Jianfeng Lu, Weiqing Li, Xiaoning Sun, Shengxiang Hu&lt;/em&gt; &lt;br&gt; AAAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.05976&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A generic diffusion-based approach for 3D human pose prediction in the wild&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Saeed Saadatnejad, Ali Rasekh, Mohammadreza Mofayezi, Yasamin Medghalchi, Sara Rajabzadeh, Taylor Mordan, Alexandre Alahi&lt;/em&gt; &lt;br&gt; ICRA 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.05669&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Novel View Synthesis with Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, Mohammad Norouzi&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2210.04628&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Neural Volumetric Mesh Generator&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yan Zheng, Lemeng Wu, Xingchao Liu, Zhen Chen, Qiang Liu, Qixing Huang&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2210.03158&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Oct 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Denoising Diffusion Probabilistic Models for Styled Walking Synthesis&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Edmund J. C. Findlay, Haozheng Zhang, Ziyi Chang, Hubert P. H. Shum&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.14828&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Human Motion Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H. Bermano, Daniel Cohen-Or&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.14916&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://guytevet.github.io/mdm-page/&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 29 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ISS: Image as Stepping Stone for Text-Guided 3D Shape Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, Chi-Wing Fu&lt;/em&gt; &lt;br&gt; ICLR 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.04145&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/liuzhengzhe/ISS-Image-as-Stepping-Stone-for-Text-Guided-3D-Shape-Generation&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 9 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;SE(3)-DiffusionFields: Learning cost functions for joint grasp and motion optimization through diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Julen Urain, Niklas Funk, Georgia Chalvatzaki, Jan Peters&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.03855&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/TheCamusean/grasp_diffusion&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 8 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;First Hitting Diffusion Models for Generating Manifold, Graph and Categorical Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mao Ye, Lemeng Wu, Qiang Liu&lt;/em&gt; &lt;br&gt; NeruIPS 2022. [&lt;a href=&#34;https://arxiv.org/abs/2209.01170&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 2 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FLAME: Free-form Language-based Motion Synthesis &amp;amp; Editing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jihoon Kim, Jiseob Kim, Sungjoon Choi&lt;/em&gt; &lt;br&gt; AAAI 2023. [&lt;a href=&#34;https://arxiv.org/abs/2209.00349&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 1 Sep 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Let us Build Bridges: Understanding and Extending Diffusion Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xingchao Liu, Lemeng Wu, Mao Ye, Qiang Liu&lt;/em&gt; &lt;br&gt; NeurIPS Workshop 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.14699&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 31 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, Ziwei Liu&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.15001&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://mingyuan-zhang.github.io/projects/MotionDiffuse.html&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 31 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Diffusion Model Predicts 3D Shapes from 2D Microscopy Images&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Dominik J. E. Waibel, Ernst Röell, Bastian Rieck, Raja Giryes, Carsten Marr&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.14125&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiachen Sun, Weili Nie, Zhiding Yu, Z. Morley Mao, Chaowei Xiao&lt;/em&gt; &lt;br&gt; arXiv 2022. [&lt;a href=&#34;https://arxiv.org/abs/2208.09801&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 21 Aug 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, Dahua Lin&lt;/em&gt; &lt;br&gt; ICLR 2022. [&lt;a href=&#34;https://arxiv.org/abs/2112.03530&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/zhaoyanglyu/point_diffusion_refinement&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Dec 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Score-Based Point Cloud Denoising&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Luo, Wei Hu&lt;/em&gt;&lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2107.10981&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/luost26/score-denoise&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 23 Jul 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffuStereo: High Quality Human Reconstruction via Diffusion-based Stereo Using Sparse Cameras&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang Sun, Yebin Liu&lt;/em&gt; &lt;br&gt; ECCV 2022. [&lt;a href=&#34;https://arxiv.org/abs/2207.08000&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;http://liuyebin.com/diffustereo/diffustereo.html&#34;&gt;Project&lt;/a&gt;] [&lt;a href=&#34;https://github.com/DSaurus/DiffuStereo&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 16 Jul 2022&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3D Shape Generation and Completion through Point-Voxel Diffusion&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Linqi Zhou, Yilun Du, Jiajun Wu&lt;/em&gt; &lt;br&gt; ICCV 2021. [&lt;a href=&#34;https://arxiv.org/abs/2104.03670&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://alexzhou907.github.io/pvd&#34;&gt;Project&lt;/a&gt;] &lt;br&gt; 8 Apr 2021&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Probabilistic Models for 3D Point Cloud Generation&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Shitong Luo, Wei Hu&lt;/em&gt; &lt;br&gt; CVPR 2021. [&lt;a href=&#34;https://arxiv.org/abs/2103.01458&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/luost26/diffusion-point-cloud&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 2 Mar 2021&lt;/p&gt; &#xA;&lt;h3&gt;Adversarial Attack&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generated Distributions Are All You Need for Membership Inference Attacks Against Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Minxing Zhang, Ning Yu, Rui Wen, Michael Backes, Yang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.19410&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Adversarial Examples Are Not Real Features&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Ang Li, Yifei Wang, Yiwen Guo, Yisen Wang&lt;/em&gt; &lt;br&gt; NeurIPS 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.18936&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 29 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Purify++: Improving Diffusion-Purification with Advanced Diffusion Models and Control of Randomness&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boya Zhang, Weijian Luo, Zhihua Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.18762&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Energy-Based Models for Anomaly Detection: A Manifold Diffusion Recovery Approach&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Sangwoong Yoon, Young-Uk Jin, Yung-Kyun Noh, Frank C. Park&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.18677&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Model Selection of Anomaly Detectors in the Absence of Labeled Validation Data&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Clement Fung, Chen Qiu, Aodong Li, Maja Rudolph&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.10461&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 16 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Boosting Black-box Attack to Deep Neural Networks with Conditional Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Renyang Liu, Wei Zhou, Tianwei Zhang, Kangjie Chen, Jun Zhao, Kwok-Yan Lam&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07492&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Investigating the Adversarial Robustness of Density Estimation Using the Probability Flow ODE&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Marius Arvinte, Cory Cornelius, Jason Martin, Nageen Himayat&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.07084&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 10 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Understanding and Improving Adversarial Attacks on Latent Diffusion Model&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Boyang Zheng, Chumeng Liang, Xiaoyu Wu, Yan Liu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2310.04687&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 7 Oct 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Semantic Adversarial Attacks via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Chenan Wang, Jinhao Duan, Chaowei Xiao, Edward Kim, Matthew Stamm, Kaidi Xu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.07398&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 14 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Catch You Everything Everywhere: Guarding Textual Inversion via Concept Watermarking&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Weitao Feng, Jiyan He, Jie Zhang, Tianwei Zhang, Wenbo Zhou, Weiming Zhang, Nenghai Yu&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05940&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 12 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diff-Privacy: Diffusion-based Face Privacy Protection&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Xiao He, Mingrui Zhu, Dongxin Chen, Nannan Wang, Xinbo Gao&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.05330&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffDefense: Defending against Adversarial Attacks via Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Hondamunige Prasanna Silva, Lorenzo Seidenari, Alberto Del Bimbo&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03702&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/HondamunigePrasannaSilva/DiffDefence&#34;&gt;Github&lt;/a&gt;] &lt;br&gt; 7 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;My Art My Choice: Adversarial Protection Against Unruly AI&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Anthony Rhodes, Ram Bhagat, Umur Aybars Ciftci, Ilke Demir&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.03198&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 6 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improving Visual Quality and Transferability of Adversarial Attacks on Face Recognition Simultaneously with Adversarial Restoration&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Fengfan Zhou&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2309.01582&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 4 Sep 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Intriguing Properties of Diffusion Models: A Large-Scale Dataset for Evaluating Natural Attack Capability in Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Takami Sato, Justin Yue, Nanze Chen, Ningfei Wang, Qi Alfred Chen&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.15692&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 30 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffSmooth: Certifiably Robust Learning via Diffusion Models and Local Smoothing&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jiawei Zhang, Zhongzhu Chen, Huan Zhang, Chaowei Xiao, Bo Li&lt;/em&gt; &lt;br&gt; USENIX Security 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.14333&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 28 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;A Probabilistic Fluctuation based Membership Inference Attack for Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.12143&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 23 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;White-box Membership Inference Attacks against Diffusion Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Yan Pang, Tianhao Wang, Xuhui Kang, Mengdi Huai, Yang Zhang&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2308.06405&#34;&gt;Paper&lt;/a&gt;] &lt;br&gt; 11 Aug 2023&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models&lt;/strong&gt; &lt;br&gt; &lt;em&gt;Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian&lt;/em&gt; &lt;br&gt; arXiv 2023. [&lt;a href=&#34;https://arxiv.org/abs/2307.16489&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://github.com/JJ-Vice/BAGM&#34;&gt;Github&lt;/a&gt;] [[Dataset](https://ieee-datapo&lt;/p&gt;</summary>
  </entry>
</feed>