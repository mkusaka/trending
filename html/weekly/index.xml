<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-15T01:46:43Z</updated>
  <subtitle>Weekly Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AliceWonderland/hacktoberfest</title>
    <updated>2023-10-15T01:46:43Z</updated>
    <id>tag:github.com,2023-10-15:/AliceWonderland/hacktoberfest</id>
    <link href="https://github.com/AliceWonderland/hacktoberfest" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Participate in Hacktoberfest by contributing to any Open Source project on GitHub! Here is a starter project for first time contributors. #hacktoberfest&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ğŸƒ HacktoberFest Starter Project ğŸƒ&lt;/h1&gt; &#xA;&lt;p&gt;** &lt;strong&gt;Oct 24th, 2017 Update:&lt;/strong&gt; THIS REPO IS TEMPORARILY &lt;strong&gt;NOT MERGING NEW PRs&lt;/strong&gt; until the CONTRIBUTORS.md file is sorted! Thanks for your patience! **&lt;/p&gt; &#xA;&lt;p&gt;Use this project to make your first contribution to an open source project on GitHub. Practice making your first pull request to a public repository before doing the real thing!&lt;/p&gt; &#xA;&lt;p&gt;Celebrate &lt;a href=&#34;https://hacktoberfest.digitalocean.com/&#34;&gt;Hacktoberfest&lt;/a&gt; by getting involved in the open source community by completing some simple tasks in this project.&lt;/p&gt; &#xA;&lt;p&gt;This repository is open to all members of the GitHub community. Any member may contribute to this project without being a collaborator.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://alicewonderland.github.io/hacktoberfest/&#34;&gt;https://alicewonderland.github.io/hacktoberfest/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Hacktoberfest?&lt;/h2&gt; &#xA;&lt;p&gt;A month-long celebration from October 1st - 31st sponsored by &lt;a href=&#34;https://hacktoberfest.digitalocean.com/&#34;&gt;Digital Ocean&lt;/a&gt; and &lt;a href=&#34;https://github.com/blog/2433-celebrate-open-source-this-october-with-hacktoberfest&#34;&gt;GitHub&lt;/a&gt; to get people involved in &lt;a href=&#34;https://github.com/open-source&#34;&gt;Open Source&lt;/a&gt;. Create your very first pull request to any public repository on GitHub and contribute to the open source developer community.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hacktoberfest.digitalocean.com/&#34;&gt;https://hacktoberfest.digitalocean.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to contribute to this project&lt;/h2&gt; &#xA;&lt;p&gt;Here are 3 quick and painless ways to contribute to this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add your name to the &lt;code&gt;CONTRIBUTORS.md&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;li&gt;Add a profile page to the &lt;code&gt;profiles&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;Create a simple &#34;Hello, World&#34; script in a language of your choice&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Choose one or all 3, make a pull request for your work and wait for it to be merged!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork this repository (Click the Fork button in the top right of this page, click your Profile Image)&lt;/li&gt; &#xA; &lt;li&gt;Clone your fork down to your local machine&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;git clone https://github.com/your-username/hacktoberfest.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a branch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;git checkout -b branch-name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make your changes (choose from any task below)&lt;/li&gt; &#xA; &lt;li&gt;Commit and push&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;git add .&#xA;git commit -m &#39;Commit message&#39;&#xA;git push origin branch-name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new pull request from your forked repository (Click the &lt;code&gt;New Pull Request&lt;/code&gt; button located at the top of your repo)&lt;/li&gt; &#xA; &lt;li&gt;Wait for your PR review and merge approval!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Star this repository&lt;/strong&gt; if you had fun!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Choose from these tasks&lt;/h2&gt; &#xA;&lt;h3&gt;1. Add your name&lt;/h3&gt; &#xA;&lt;p&gt;Add your name to the &lt;code&gt;CONTRIBUTORS.md&lt;/code&gt; file using the below convention:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;#### Name: [YOUR NAME](GitHub link)&#xA;- Place: City, State, Country&#xA;- Bio: Who are you?&#xA;- GitHub: [GitHub account name](GitHub link)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Add a profile page&lt;/h3&gt; &#xA;&lt;p&gt;Add a &lt;code&gt;Your_Name.md&lt;/code&gt; file to the &lt;code&gt;profiles&lt;/code&gt; directory. Use any combination of content and Markdown you&#39;d like. Here is an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;# Your Name&#xA;&#xA;### Location&#xA;&#xA;Your City/Country&#xA;&#xA;### Academics&#xA;&#xA;Your School&#xA;&#xA;### Interests&#xA;&#xA;- Some Things You Like&#xA;&#xA;### Development&#xA;&#xA;- Inventor of the My Pillow&#xA;&#xA;### Projects&#xA;&#xA;- [My Project](GitHub Link) Short Description&#xA;&#xA;### Profile Link&#xA;&#xA;[Your Name](GitHub Link)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Create a &lt;code&gt;Hello, World!&lt;/code&gt; Script&lt;/h3&gt; &#xA;&lt;p&gt;Add a &lt;code&gt;hello_world_yourusername.xx&lt;/code&gt; script to the &lt;code&gt;scripts&lt;/code&gt; directory in any language of your choice! Here is an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Javascript&#34;&gt;// LANGUAGE: Javascript&#xA;// ENV: Node.js&#xA;// AUTHOR: Alice Chuang&#xA;// GITHUB: https://github.com/AliceWonderland&#xA;&#xA;console.log(&#39;Hello, World!&#39;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Name the file &lt;code&gt;hello_world_yourusername.xx&lt;/code&gt;. e.g., &lt;code&gt;hello_world_alicewonderland.js&lt;/code&gt; or &lt;code&gt;hello_world_alicewonderland.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t forget to include the comments as seen above. Feel free to include additional information about the language you choose in your comments too! Like a link to a helpful introduction or tutorial.&lt;/p&gt; &#xA;&lt;p&gt;Here is my &lt;code&gt;hello_world&lt;/code&gt; example: &lt;a href=&#34;https://github.com/AliceWonderland/hacktoberfest/raw/master/scripts/hello_world_alicewonderland.js&#34;&gt;hello_world_alicewonderland.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;BONUS!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See profiles submitted by fellow coders from around the globe ... from Kathmandu to Copenhagen.&lt;/li&gt; &#xA; &lt;li&gt;Discover some obscure to new and trending languages ... from BrainFuck to Groovy.&lt;/li&gt; &#xA; &lt;li&gt;Check out some very creative ways to print out a &#34;Hello, World!&#34; string.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference links&lt;/h2&gt; &#xA;&lt;p&gt;Here is a great tutorial for creating your first pull request by &lt;a href=&#34;https://github.com/Roshanjossey&#34;&gt;Roshan Jossey&lt;/a&gt;: &lt;a href=&#34;https://github.com/Roshanjossey/first-contributions&#34;&gt;https://github.com/Roshanjossey/first-contributions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Managing your Forked Repo: &lt;a href=&#34;https://help.github.com/articles/fork-a-repo/&#34;&gt;https://help.github.com/articles/fork-a-repo/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Syncing a Fork: &lt;a href=&#34;https://help.github.com/articles/syncing-a-fork/&#34;&gt;https://help.github.com/articles/syncing-a-fork/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Keep Your Fork Synced: &lt;a href=&#34;https://gist.github.com/CristinaSolana/1885435&#34;&gt;https://gist.github.com/CristinaSolana/1885435&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checkout this list for README examples - Awesome README &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Github-Flavored Markdown &lt;a href=&#34;https://guides.github.com/features/mastering-markdown/&#34;&gt;https://guides.github.com/features/mastering-markdown/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Additional references added by contributors&lt;/h2&gt; &#xA;&lt;p&gt;GitHub license explained &lt;a href=&#34;https://choosealicense.com&#34;&gt;https://choosealicense.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DSXiangLi/DecryptPrompt</title>
    <updated>2023-10-15T01:46:43Z</updated>
    <id>tag:github.com,2023-10-15:/DSXiangLi/DecryptPrompt</id>
    <link href="https://github.com/DSXiangLi/DecryptPrompt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;æ€»ç»“Prompt&amp;LLMè®ºæ–‡ï¼Œå¼€æºæ•°æ®&amp;æ¨¡å‹ï¼ŒAIGCåº”ç”¨&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DecryptPrompt&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;å¦‚æœLLMçš„çªç„¶åˆ°æ¥è®©ä½ æ„Ÿåˆ°æ²®ä¸§ï¼Œä¸å¦¨è¯»ä¸‹ä¸»ç›®å½•çš„Choose Your Weapon Survival Strategies for Depressed AI Academics æŒç»­æ›´æ–°ä»¥ä¸‹å†…å®¹ï¼ŒStar to keep updated~&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;ç›®å½•é¡ºåºå¦‚ä¸‹&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;å›½å†…å¤–ï¼Œå‚ç›´é¢†åŸŸå¤§æ¨¡å‹&lt;/li&gt; &#xA; &lt;li&gt;Agentå’ŒæŒ‡ä»¤å¾®è°ƒç­‰è®­ç»ƒæ¡†æ¶&lt;/li&gt; &#xA; &lt;li&gt;å¼€æºæŒ‡ä»¤ï¼Œé¢„è®­ç»ƒï¼Œrlhfï¼Œå¯¹è¯ï¼Œagentè®­ç»ƒæ•°æ®æ¢³ç†&lt;/li&gt; &#xA; &lt;li&gt;AIGCç›¸å…³åº”ç”¨&lt;/li&gt; &#xA; &lt;li&gt;promptå†™ä½œæŒ‡å—å’Œ5æ˜Ÿåšå®¢ç­‰èµ„æºæ¢³ç†&lt;/li&gt; &#xA; &lt;li&gt;Promptå’ŒLLMè®ºæ–‡ç»†åˆ†æ–¹å‘æ¢³ç†&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;My blogs &amp;amp; ChatGPTåº”ç”¨&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2215545?areaSource=&amp;amp;traceId=&#34;&gt;è§£å¯†Promptç³»åˆ—1. Tunning-Free Promptï¼šGPT2 &amp;amp; GPT3 &amp;amp; LAMA &amp;amp; AutoPrompt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2223355?areaSource=&amp;amp;traceId=&#34;&gt;è§£å¯†Promptç³»åˆ—2. å†»ç»“Promptå¾®è°ƒLMï¼š T5 &amp;amp; PET &amp;amp; LM-BFF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2237259?areaSource=&amp;amp;traceId=&#34;&gt;è§£å¯†Promptç³»åˆ—3. å†»ç»“LMå¾®è°ƒPrompt: Prefix-tuning &amp;amp; Prompt-tuning &amp;amp; P-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2245094?areaSource=&amp;amp;traceId=&#34;&gt;è§£å¯†Promptç³»åˆ—4. å‡çº§Instruction Tuningï¼šFlan/T0/InstructGPT/TKInstruct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2260697?areaSource=&amp;amp;traceId=&#34;&gt;è§£å¯†promptç³»åˆ—5. APE+SELF=è‡ªåŠ¨åŒ–æŒ‡ä»¤é›†æ„å»ºä»£ç å®ç°&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2276508&#34;&gt;è§£å¯†Promptç³»åˆ—6. loraæŒ‡ä»¤å¾®è°ƒæ‰£ç»†èŠ‚-è¯·å†·é™,1ä¸ªå°æ—¶çœŸä¸å¤Ÿ~&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2289566?areaSource=&amp;amp;traceId=&#34;&gt;è§£å¯†Promptç³»åˆ—7. åå¥½å¯¹é½RLHF-OpenAIÂ·DeepMindÂ·Anthropicå¯¹æ¯”åˆ†æ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2295783?areaSource=&amp;amp;traceId=&#34;&gt;è§£å¯†Promptç³»åˆ—8. æ— éœ€è®­ç»ƒè®©LLMæ”¯æŒè¶…é•¿è¾“å…¥:çŸ¥è¯†åº“ &amp;amp; Unlimiformer &amp;amp; PCW &amp;amp; NBCE &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2296079?areaSource=&amp;amp;traceId=&#34;&gt;è§£å¯†Promptç³»åˆ—9. æ¨¡å‹å¤æ‚æ¨ç†-æ€ç»´é“¾åŸºç¡€å’Œè¿›é˜¶ç©æ³•&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2298660&#34;&gt;è§£å¯†Promptç³»åˆ—10. æ€ç»´é“¾COTåŸç†æ¢ç©¶&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2301999&#34;&gt;è§£å¯†Promptç³»åˆ—11. å°æ¨¡å‹ä¹Ÿèƒ½COTï¼Œå…ˆå¤©ä¸è¶³åå¤©è¡¥&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2305421&#34;&gt;è§£å¯†Promptç³»åˆ—12. LLM Agenté›¶å¾®è°ƒèŒƒå¼ ReAct &amp;amp; Self Ask&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2312674&#34;&gt;è§£å¯†Promptç³»åˆ—13. LLM AgentæŒ‡ä»¤å¾®è°ƒæ–¹æ¡ˆ: Toolformer &amp;amp; Gorilla&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2319879&#34;&gt;è§£å¯†Promptç³»åˆ—14. LLM Agentä¹‹æœç´¢åº”ç”¨è®¾è®¡ï¼šWebGPT &amp;amp; WebGLM &amp;amp; WebCPM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2328749&#34;&gt;è§£å¯†Promptç³»åˆ—15. LLM Agentä¹‹æ•°æ®åº“åº”ç”¨è®¾è®¡ï¼šDIN &amp;amp; C3 &amp;amp; SQL-Palm &amp;amp; BIRD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2333495&#34;&gt;è§£å¯†Promptç³»åˆ—16. LLMå¯¹é½ç»éªŒä¹‹æ•°æ®è¶Šå°‘è¶Šå¥½ï¼ŸLTD &amp;amp; LIMA &amp;amp; AlpaGasus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2338592&#34;&gt;è§£å¯†Promptç³»åˆ—17. LLMå¯¹é½æ–¹æ¡ˆå†å‡çº§ WizardLM &amp;amp; BackTranslation &amp;amp; SELF-ALIGN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLMS&lt;/h2&gt; &#xA;&lt;h3&gt;æ¨¡å‹è¯„æµ‹&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;å¤§æ¨¡å‹è¯„ä¼°å°šæœªå‡ºç°åŒ—ææ˜ŸæŒ‡æ ‡ï¼Œæ¦œå•æ’åå¾€å¾€å’Œå®é™…ä½¿ç”¨èƒ½åŠ›å­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œå‡ å¤©æ²¡çœ‹æ„Ÿè§‰æœ‰çš„æ¦œå•å¿«è¢«ç©åäº†......&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ¦œå•&lt;/th&gt; &#xA;   &lt;th&gt;ç»“æœ&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;AlpacaEvalï¼šLLM-based automatic evaluation &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å¼€æºæ¨¡å‹ç‹è€…vicuna,openchat, wizardlm&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;Huggingface Open LLM Leaderboard&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MMLUåªè¯„ä¼°å¼€æºæ¨¡å‹ï¼ŒFalconå¤ºå† ï¼Œåœ¨Eleuther AI4ä¸ªè¯„ä¼°é›†ä¸Šè¯„ä¼°çš„LLMæ¨¡å‹æ¦œå•,vicunaå¤ºå† &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lmsys.org/blog/2023-05-03-arena/&#34;&gt;Berkleyå‡ºå“å¤§æ¨¡å‹æ’ä½èµ›æ¦œæœ‰å‡†ä¸­æ–‡æ¦œå•&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Eloè¯„åˆ†æœºåˆ¶ï¼ŒGPT4è‡ªç„¶æ˜¯ç¨³å±…ç¬¬ä¸€ï¼ŒGPT4&amp;gt;Claude&amp;gt;GPT3.5&amp;gt;Vicuna&amp;gt;others&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zeno-ml/zeno-build&#34;&gt;CMUå¼€æºèŠå¤©æœºå™¨äººè¯„æµ‹åº”ç”¨&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ChatGPT&amp;gt;Vicuna&amp;gt;othersï¼›åœ¨å¯¹è¯åœºæ™¯ä¸­è®­ç»ƒå¯èƒ½å¾ˆé‡è¦&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zhenbench/z-bench&#34;&gt;Z-Benchä¸­æ–‡çœŸæ ¼åŸºé‡‘è¯„æµ‹&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å›½äº§ä¸­æ–‡æ¨¡å‹çš„ç¼–ç¨‹å¯ç”¨æ€§è¿˜ç›¸å¯¹è¾ƒä½ï¼Œå¤§å®¶æ°´å¹³å·®ä¸å¤ªå¤šï¼Œä¸¤ç‰ˆChatGLMæå‡æ˜æ˜¾&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FranxYao/chain-of-thought-hub&#34;&gt;Chain-of-thoughtè¯„ä¼°&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GSM8k, MATHç­‰å¤æ‚é—®é¢˜æ’è¡Œæ¦œ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;amp;mid=2651170429&amp;amp;idx=1&amp;amp;sn=b98af3bd14c9f97f1aa07f0f839bb3ec&amp;amp;scene=21#wechat_redirect&#34;&gt;InfoQ å¤§æ¨¡å‹ç»¼åˆèƒ½åŠ›è¯„ä¼°&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é¢å‘ä¸­æ–‡ï¼ŒChatGPT&amp;gt;æ–‡å¿ƒä¸€è¨€&amp;gt; Claude&amp;gt;æ˜Ÿç«&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/ToolBench&#34;&gt;ToolBench: å·¥å…·è°ƒç”¨è¯„ä¼°æ¦œå•&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å·¥å…·å¾®è°ƒæ¨¡å‹å’ŒChatGPTè¿›è¡Œå¯¹æ¯”ï¼Œæä¾›è¯„æµ‹è„šæœ¬&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/AgentBench&#34;&gt;AgentBench: æ¨ç†å†³ç­–è¯„ä¼°æ¦œå•&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¸…åè”åˆå¤šé«˜æ ¡æ¨å‡ºä¸åŒä»»åŠ¡ç¯å¢ƒï¼Œä¾‹å¦‚è´­ç‰©ï¼Œå®¶å±…ï¼Œæ“ä½œç³»ç»Ÿç­‰åœºæ™¯ä¸‹æ¨¡å‹æ¨ç†å†³ç­–èƒ½åŠ›&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://flageval.baai.ac.cn/#/home&#34;&gt;FlagEval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ™ºæºå‡ºå“ä¸»è§‚+å®¢è§‚LLMè¯„åˆ†æ¦œå•&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bird-bench.github.io/&#34;&gt;Bird-Bench&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ›´è´´åˆçœŸå®ä¸–ç•Œåº”ç”¨çš„è¶…å¤§æ•°æ®åº“ï¼Œéœ€è¦é¢†åŸŸçŸ¥è¯†çš„NL2SQLæ¦œå•ï¼Œæ¨¡å‹è¿½èµ¶äººç±»å°šæœ‰æ—¶æ—¥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://103.238.162.37:31622/LeaderBoard&#34;&gt;kola&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä»¥ä¸–ç•ŒçŸ¥è¯†ä¸ºæ ¸å¿ƒçš„è¯„ä»·åŸºå‡†ï¼ŒåŒ…æ‹¬å·²çŸ¥çš„ç™¾ç§‘çŸ¥è¯†å’ŒæœªçŸ¥çš„è¿‘90å¤©ç½‘ç»œå‘å¸ƒå†…å®¹ï¼Œè¯„ä»·çŸ¥è¯†è®°å¿†ï¼Œç†è§£ï¼Œåº”ç”¨å’Œåˆ›é€ èƒ½åŠ›&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cevalbenchmark.com/index.html#home&#34;&gt;CEVAL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä¸­æ–‡çŸ¥è¯†è¯„ä¼°ï¼Œè¦†ç›–52ä¸ªå­¦ç§‘ï¼Œæœºå™¨è¯„ä»·ä¸»è¦ä¸ºå¤šé¡¹é€‰æ‹©&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/haonan-li/CMMLU&#34;&gt;CMMLU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;67ä¸ªä¸»é¢˜ä¸­æ–‡çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›è¯„ä¼°ï¼Œå¤šé¡¹é€‰æ‹©æœºå™¨è¯„ä¼°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;å›½å¤–å¼€æºæ¨¡å‹&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ¨¡å‹é“¾æ¥&lt;/th&gt; &#xA;   &lt;th&gt;æ¨¡å‹æè¿°&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/llama/&#34;&gt;LLama2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Open Metaå¸¦ç€å¯å•†ç”¨å¼€æºçš„ç¾Šé©¼2æ¨¡å‹æ¥äº†~&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpacaå‰æˆå‘˜ç­‰å¼€æºä»¥LLama13Bä¸ºåŸºç¡€ä½¿ç”¨ShareGPTæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹ï¼Œæå‡ºäº†ç”¨GPT4æ¥è¯„æµ‹æ¨¡å‹æ•ˆæœ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nlpxucan/WizardLM&#34;&gt;WizardLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å¾®è½¯æ–°å‘å¸ƒ13Bï¼Œç™»é¡¶AlpacaEvalå¼€æºæ¨¡å‹Top3ï¼Œä½¿ç”¨ChatGPTå¯¹æŒ‡ä»¤è¿›è¡Œå¤æ‚åº¦è¿›åŒ–å¾®è°ƒLLama2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/imoneoi/openchat&#34;&gt;OpenChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;80k ShareGPTå¯¹è¯å¾®è°ƒLLama-2 13Bå¼€æºæ¨¡å‹ä¸­çš„æˆ˜æ–—æœº&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KBlueLeaf/guanaco-7B-leh&#34;&gt;Guanaco&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama 7BåŸºåº§ï¼Œåœ¨alpaca52Kæ•°æ®ä¸ŠåŠ å…¥534Kå¤šè¯­è¨€æŒ‡ä»¤æ•°æ®å¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Metaå¼€æºæŒ‡ä»¤å¾®è°ƒLLMï¼Œè§„æ¨¡70 äº¿åˆ° 650 äº¿ä¸ç­‰&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b-chat&#34;&gt;MPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MosaicMLå¼€æºçš„é¢„è®­ç»ƒ+æŒ‡ä»¤å¾®è°ƒçš„æ–°æ¨¡å‹ï¼Œå¯å•†ç”¨ï¼Œæ”¯æŒ84k tokensè¶…é•¿è¾“å…¥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b&#34;&gt;Falcon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Falconç”±é˜¿è”é…‹æŠ€æœ¯ç ”ç©¶æ‰€åœ¨è¶…é«˜è´¨é‡1ä¸‡äº¿Tokenä¸Šè®­ç»ƒå¾—åˆ°1Bï¼Œ7Bï¼Œ40Bå¼€æºï¼Œå…è´¹å•†ç”¨ï¼åœŸè±ªä»¬è¡¨ç¤ºé’±ä»€ä¹ˆçš„æ ¼å±€å°äº†&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1&#34;&gt;RedPajama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RedPajamaé¡¹ç›®æ—¢å¼€æºé¢„è®­ç»ƒæ•°æ®åå¼€æº3Bï¼Œ7Bçš„é¢„è®­ç»ƒ+æŒ‡ä»¤å¾®è°ƒæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bair.berkeley.edu/blog/2023/04/03/koala/&#34;&gt;koala&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä½¿ç”¨alpacaï¼ŒHC3ç­‰å¼€æºæŒ‡ä»¤é›†+ ShareGPTç­‰ChatGPTæ•°æ®å¾®è°ƒllamaï¼Œåœ¨æ¦œå•ä¸Šæ’åè¾ƒé«˜&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama&#34;&gt;ChatLLaMA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŸºäºRLHFå¾®è°ƒäº†LLaMA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ–¯å¦ç¦å¼€æºçš„ä½¿ç”¨52kæ•°æ®åœ¨7Bçš„LLaMAä¸Šå¾®è°ƒå¾—åˆ°ï¼Œ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca-lora&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LORAå¾®è°ƒçš„LLaMA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IBM/Dromedary&#34;&gt;Dromedary&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IBM self-aligned model with the LLaMA base&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;ColossalChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HPC-AI Techå¼€æºçš„Llama+RLHFå¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna+BLIP2 æ–‡æœ¬è§†è§‰èåˆ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/trl-lib/llama-7b-se-rl-peft&#34;&gt;StackLLama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLamaä½¿ç”¨Stackexchangeæ•°æ®+SFT+RL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/cerebras/Cerebras-GPT-13B&#34;&gt;Cerebras&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Cerebraså¼€æºäº†1äº¿åˆ°130äº¿çš„7ä¸ªæ¨¡å‹ï¼Œä»é¢„è®­ç»ƒæ•°æ®åˆ°å‚æ•°å…¨å¼€æº&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://palm-e.github.io&#34;&gt;PaLM-E&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;è°·æ­Œå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œ540Bçš„PaLMè¯­è¨€æ¨¡å‹å’Œ22Bçš„ViTè§†è§‰æ¨¡å‹ç›¸ç»“åˆï¼Œå¾—åˆ°562Bçš„PaLM-Eæ¨¡å‹ï¼Œåœ¨æœºå™¨äººåº”ç”¨åœºæ™¯æœ‰äº†æ–°çš„çªç ´&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-7b&#34;&gt;Dolly-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å¯å•†ç”¨ 7bæŒ‡ä»¤å¾®è°ƒå¼€æºæ¨¡å‹åœ¨GPT-J-6Bä¸Šå¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/togethercomputer/OpenChatKit&#34;&gt;OpenChatKit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;openaiç ”ç©¶å‘˜æ‰“é€ GPT-NoX-20Bå¾®è°ƒ+6Bå®¡æ ¸æ¨¡å‹è¿‡æ»¤&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/unilm&#34;&gt;MetaLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å¾®è½¯å¼€æºçš„å¤§è§„æ¨¡è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/cn/bedrock/titan/&#34;&gt;Amazon Titan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;äºšé©¬é€Šåœ¨awsä¸Šå¢åŠ è‡ªå®¶å¤§æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/metaseq/tree/main/projects/OPT&#34;&gt;OPT-IML&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Metaå¤åˆ»GPT3ï¼Œup to 175B, ä¸è¿‡æ•ˆæœå¹¶ä¸åŠGPT3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;Bloom&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BigScienceå‡ºå“ï¼Œè§„æ¨¡æœ€å¤§176B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloomz&#34;&gt;BloomZ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BigScienceå‡ºå“, åŸºäºBloomå¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/paperswithcode/galai&#34;&gt;Galacia&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å’ŒBloomç›¸ä¼¼ï¼Œæ›´é’ˆå¯¹ç§‘ç ”é¢†åŸŸè®­ç»ƒçš„æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bigscience-workshop/t-zero&#34;&gt;T0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BigScienceå‡ºå“ï¼Œ3B~11Bçš„åœ¨T5è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/turboderp/exllama&#34;&gt;EXLLama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weight&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/lmsys/longchat-13b-16k&#34;&gt;LongChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;llama-13bä½¿ç”¨condensing rotary embedding techniqueå¾®è°ƒçš„é•¿æ–‡æœ¬æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-30b&#34;&gt;MPT-30B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MosaicMLå¼€æºçš„åœ¨8Ktokenä¸Šè®­ç»ƒçš„å¤§æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;å›½å†…å¼€æºæ¨¡å‹&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ¨¡å‹é“¾æ¥&lt;/th&gt; &#xA;   &lt;th&gt;æ¨¡å‹æè¿°&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thudm/chatglm2-6b&#34;&gt;ChatGLM2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;32Ké•¿æ–‡æœ¬ï¼ŒFlashAttention+Multi-Query Attenionçš„æ˜¾å­˜ä¼˜åŒ–ï¼Œæ›´å¼ºæ¨ç†èƒ½åŠ›ï¼Œå“ˆå“ˆä¸è¿‡å¾ˆå¤šç®€å•é—®é¢˜ä¹Ÿç¡¬è¦COTï¼Œä¸­è‹±å¹³è¡Œèƒ½åŠ›ä¼¼ä¹ç•¥æœ‰ä¸‹é™çš„ChatGLM2ï¼Œä½†æ˜¯å…è´¹å•†ç”¨ï¼&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¸…åå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨äº†ä»£ç è®­ç»ƒï¼ŒæŒ‡ä»¤å¾®è°ƒå’ŒRLHFã€‚chatglm2æ”¯æŒè¶…é•¿æ–‡æœ¬ï¼Œå¯å…è´¹å•†ç”¨å•¦ï¼&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FlagAlpha/Llama2-Chinese&#34;&gt;LLama2-chinese&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ²¡ç­‰å¤ªä¹…ä¸­æ–‡é¢„è®­ç»ƒå¾®è°ƒåçš„llama2å®ƒæ¥äº†~&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/RUC-GSAI/YuLan-Chat&#34;&gt;YuLan-chat2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é«˜ç“´äººå·¥æ™ºèƒ½åŸºäºLlama-2ä¸­è‹±åŒè¯­ç»§ç»­é¢„è®­ç»ƒ+æŒ‡ä»¤å¾®è°ƒ/å¯¹è¯å¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward&#34;&gt;ziya&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IDEAç ”ç©¶é™¢åœ¨7B/13B llamaä¸Šç»§ç»­é¢„è®­ç»ƒ+SFT+RM+PPO+HFTT+COHFT+RBRS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/baichuan-inc/baichuan-7B&#34;&gt;Baichuan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ç™¾å·æ™ºèƒ½å¼€æº7Bå¤§æ¨¡å‹å¯å•†ç”¨å…è´¹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan2&#34;&gt;Baichuan2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ç™¾å·ç¬¬äºŒä»£ï¼Œæä¾›äº†7B/13B Baseå’Œchatçš„ç‰ˆæœ¬&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Xwin-LM/Xwin-LM&#34;&gt;XWin-LM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;llama2 + SFT + RLHF&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å“ˆå·¥å¤§ä¸­æ–‡æŒ‡ä»¤å¾®è°ƒçš„LLaMA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenLMLab/MOSS&#34;&gt;Moss&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä¸ºå¤æ—¦æ­£åï¼å¼€æºäº†é¢„è®­ç»ƒï¼ŒæŒ‡ä»¤å¾®è°ƒçš„å…¨éƒ¨æ•°æ®å’Œæ¨¡å‹ã€‚å¯å•†ç”¨&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen-7B&#34;&gt;Qwen-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é˜¿é‡Œå¼€æºï¼Œå¯å•†ç”¨ï¼Œé€šä¹‰åƒæ–‡7Bæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM&#34;&gt;IntenrLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä¹¦ç”Ÿæµ¦è¯­åœ¨è¿‡ä¸‡äº¿ token æ•°æ®ä¸Šè®­ç»ƒçš„å¤šè¯­åƒäº¿å‚æ•°åŸºåº§æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FlagAI-Open/Aquila2/raw/main/README_CN.md&#34;&gt;Aquila2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ™ºæºæ›´æ–°Aquila2æ¨¡å‹ç³»åˆ—åŒ…æ‹¬å…¨æ–°34B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila&#34;&gt;Aquila&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ™ºæºå¼€æº7Bå¤§æ¨¡å‹å¯å•†ç”¨å…è´¹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.moonshot.cn/?ref=aihub.cn&#34;&gt;kimi Chat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moonshotä¸Šçº¿è¶…é•¿æ–‡æœ¬LLM å¯è¾“å…¥20Wä¸Šæ–‡éœ€è¦ç”³è¯·è¯•ç”¨&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dandelionsllm/pandallm&#34;&gt;PandaLLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLAMA2ä¸Šä¸­æ–‡wikiç»§ç»­é¢„è®­ç»ƒ+COIGæŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xverse-ai/XVERSE-13B&#34;&gt;XVERSE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ®è¯´ä¸­æ–‡è¶…è¶Šllama2çš„å…ƒè±¡å¼€æºæ¨¡å‹13Bæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Neutralzz/BiLLa&#34;&gt;BiLLa&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLamaè¯è¡¨æ‰©å……é¢„è®­ç»ƒ+é¢„è®­ç»ƒå’Œä»»åŠ¡1æ¯”1æ··åˆSFT+æŒ‡ä»¤æ ·æœ¬SFTä¸‰é˜¶æ®µè®­ç»ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FreedomIntelligence/LLMZoo&#34;&gt;Phoenix&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¸¯ä¸­æ–‡å¼€æºå‡¤å‡°å’Œå¥‡ç¾æ‹‰LLMï¼ŒBloomåŸºåº§ï¼Œ40+è¯­è¨€æ”¯æŒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/GanjinZero/wombat-7b-delta&#34;&gt;Wombat-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;è¾¾æ‘©é™¢å¼€æºæ— éœ€å¼ºåŒ–å­¦ä¹ ä½¿ç”¨RRHFå¯¹é½çš„è¯­è¨€æ¨¡å‹, alpacaåŸºåº§&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/TigerResearch/TigerBot&#34;&gt;TigerBot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;è™åšå¼€æºäº†7B 180Bçš„æ¨¡å‹ä»¥åŠé¢„è®­ç»ƒå’Œå¾®è°ƒè¯­æ–™&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM&#34;&gt;Luotuo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä¸­æ–‡æŒ‡ä»¤å¾®è°ƒçš„LLaMAï¼Œå’ŒChatGLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBuddy/OpenBuddy&#34;&gt;OpenBuddy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Llama å¤šè¯­è¨€å¯¹è¯å¾®è°ƒæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Facico/Chinese-Vicuna&#34;&gt;Chinese Vincuna&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama 7BåŸºåº§ï¼Œä½¿ç”¨Belle+Guanacoæ•°æ®è®­ç»ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CVI-SZU/Linly&#34;&gt;Linly&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Llama 7BåŸºåº§ï¼Œä½¿ç”¨belle+guanaco+pclue+firefly+CSL+newscommentaryç­‰7ä¸ªæŒ‡ä»¤å¾®è°ƒæ•°æ®é›†è®­ç»ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yangjianxin1/Firefly&#34;&gt;Firefly&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä¸­æ–‡2.6Bæ¨¡å‹ï¼Œæå‡æ¨¡å‹ä¸­æ–‡å†™ä½œï¼Œå¤æ–‡èƒ½åŠ›ï¼Œå¾…å¼€æºå…¨éƒ¨è®­ç»ƒä»£ç ï¼Œå½“å‰åªæœ‰æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/project-baize/baize-chatbot&#34;&gt;Baize&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä½¿ç”¨100k self-chatå¯¹è¯æ•°æ®å¾®è°ƒçš„LLama&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä½¿ç”¨ChatGPTç”Ÿæˆæ•°æ®å¯¹å¼€æºæ¨¡å‹è¿›è¡Œä¸­æ–‡ä¼˜åŒ–&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/search?q=chatyuan&amp;amp;type=repositories&#34;&gt;Chatyuan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;chatgptå‡ºæ¥åæœ€æ—©çš„å›½å†…å¼€æºå¯¹è¯æ¨¡å‹ï¼ŒT5æ¶æ„æ˜¯ä¸‹é¢PromptCLUEçš„è¡ç”Ÿæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/clue-ai/PromptCLUE&#34;&gt;PromptCLUE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å¤šä»»åŠ¡Promptè¯­è¨€æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.alice-mind.com/portal#/&#34;&gt;PLUG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é˜¿é‡Œè¾¾æ‘©é™¢å‘å¸ƒçš„å¤§æ¨¡å‹ï¼Œæäº¤ç”³è¯·ä¼šç»™ä¸‹è½½é“¾æ¥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://baai.ac.cn/&#34;&gt;CPM2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ™ºæºå‘å¸ƒCPM2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/GLM-130B&#34;&gt;GLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¸…åå‘å¸ƒçš„ä¸­è‹±åŒè¯­130Bé¢„è®­ç»ƒæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ictnlp/BayLing&#34;&gt;BayLing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŸºäºLLama7B/13Bï¼Œå¢å¼ºçš„è¯­è¨€å¯¹é½çš„è‹±è¯­/ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;å‚ç›´é¢†åŸŸæ¨¡å‹&amp;amp;è¿›å±•&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;é¢†åŸŸ&lt;/th&gt; &#xA;   &lt;th&gt;æ¨¡å‹é“¾æ¥&lt;/th&gt; &#xA;   &lt;th&gt;æ¨¡å‹æè¿°&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://medgpt.co/home/zh&#34;&gt;MedGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŒ»è”å‘å¸ƒçš„&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;MedPalm&lt;/td&gt; &#xA;   &lt;td&gt;Googleåœ¨Faln-PaLMçš„åŸºç¡€ä¸Šé€šè¿‡å¤šç§ç±»å‹çš„åŒ»ç–—QAæ•°æ®è¿›è¡Œprompt-tuningæŒ‡ä»¤å¾®è°ƒå¾—åˆ°ï¼ŒåŒæ—¶æ„å»ºäº†MultiMedQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kent0n-Li/ChatDoctor&#34;&gt;ChatDoctor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;110KçœŸå®åŒ»æ‚£å¯¹è¯æ ·æœ¬+5KChatGPTç”Ÿæˆæ•°æ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese&#34;&gt;Huatuo&lt;/a&gt; &lt;a href=&#34;https://github.com/SCIR-HI/Med-ChatGLM&#34;&gt;Med-ChatGLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŒ»å­¦çŸ¥è¯†å›¾è°±å’Œchatgptæ„å»ºä¸­æ–‡åŒ»å­¦æŒ‡ä»¤æ•°æ®é›†+åŒ»å­¦æ–‡çŒ®å’Œchatgptæ„å»ºå¤šè½®é—®ç­”æ•°æ®&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Facico/Chinese-Vicuna/raw/master/docs/performance-medical.md&#34;&gt;Chinese-vicuna-med&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese-vicunaåœ¨cMedQA2æ•°æ®ä¸Šå¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BioFM/OpenBioMed&#34;&gt;OpenBioMed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ¸…åAIRå¼€æºè½»é‡ç‰ˆBioMedGPT, çŸ¥è¯†å›¾è°±&amp;amp;20+ç”Ÿç‰©ç ”ç©¶é¢†åŸŸå¤šæ¨¡æ€é¢„è®­ç»ƒæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xionghonglin/DoctorGLM&#34;&gt;DoctorGLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ChatDoctor+MedDialog+CMD å¤šè½®å¯¹è¯+å•è½®æŒ‡ä»¤æ ·æœ¬å¾®è°ƒGLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MediaBrain-SJTU/MedicalGPT-zh&#34;&gt;MedicalGPT-zh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;è‡ªå»ºçš„åŒ»å­¦æ•°æ®åº“ChatGPTç”ŸæˆQA+16ä¸ªæƒ…å¢ƒä¸‹SELFæ„å»ºæƒ…æ™¯å¯¹è¯&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/chaoyi-wu/PMC-LLaMA&#34;&gt;PMC-LLaMA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŒ»ç–—è®ºæ–‡å¾®è°ƒLlama&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/openmedlab/PULSE&#34;&gt;PULSE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Bloomå¾®è°ƒ+ç»§ç»­é¢„è®­ç»ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CogStack/OpenGPT/tree/main&#34;&gt;NHS-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chatgptç”Ÿæˆçš„åŒ»ç–—é—®ç­”ï¼Œå¯¹è¯ï¼Œå¾®è°ƒæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/michael-wzhu/ShenNong-TCM-LLM&#34;&gt;ç¥å†œåŒ»ç–—å¤§æ¨¡å‹&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä»¥ä¸­åŒ»çŸ¥è¯†å›¾è°±çš„å®ä½“ä¸ºä¸­å¿ƒç”Ÿæˆçš„ä¸­åŒ»çŸ¥è¯†æŒ‡ä»¤æ•°æ®é›†11w+ï¼Œå¾®è°ƒLLama-7B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;å²é»„é—®é“å¤§æ¨¡å‹&lt;/td&gt; &#xA;   &lt;td&gt;3ä¸ªå­æ¨¡å‹æ„æˆï¼Œå·²ç¡®è¯Šç–¾ç—…çš„ä¸´åºŠæ²»ç–—æ¨¡å‹+åŸºäºç—‡çŠ¶çš„ä¸´åºŠè¯Šç–—æ¨¡å‹+ä¸­åŒ»å…»ç”Ÿæ¡ç†æ¨¡å‹ï¼Œçœ‹èµ·æ¥æ˜¯è¦ToBè½åœ°&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SupritYoung/Zhongjing&#34;&gt;Zhongjing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŸºäºZiya-LLama+åŒ»ç–—é¢„è®­ç»ƒ+SFT+RLHFçš„ä¸­æ–‡åŒ»å­¦å¤§æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/qiuhuachuan/smile&#34;&gt;MeChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å¿ƒç†å’¨è¯¢é¢†åŸŸï¼Œé€šè¿‡chatgptæ”¹å†™å¤šè½®å¯¹è¯56k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/scutcyr/SoulChat&#34;&gt;SoulChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å¿ƒç†å’¨è¯¢é¢†åŸŸä¸­æ–‡é•¿æ–‡æœ¬æŒ‡ä»¤ä¸å¤šè½®å…±æƒ…å¯¹è¯æ•°æ®è”åˆæŒ‡ä»¤å¾®è°ƒ ChatGLM-6B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/X-D-Lab/MindChat&#34;&gt;MindChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MindChat-Baichuan-13B,Qwen-7B,MindChat-InternLM-7Bä½¿ç”¨ä¸åŒåŸºåº§åœ¨æ¨¡å‹å®‰å…¨ï¼Œå…±æƒ…ï¼Œäººç±»ä»·å€¼è§‚å¯¹å…¶ä¸Šè¿›è¡Œäº†å¼ºåŒ–&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ»ç–—&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FudanDISC/DISC-MedLLM&#34;&gt;DISC-MedLLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ç–¾ç—…çŸ¥è¯†å›¾è°±æ„å»ºQAå¯¹+QAå¯¹è½¬åŒ–æˆå•è®ºå¯¹è¯+çœŸå®ä¸–ç•Œæ•°æ®é‡æ„+äººç±»åå¥½æ•°æ®ç­›é€‰ï¼ŒSFTå¾®è°ƒbaichuan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ³•å¾‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LiuHC0428/LAW-GPT&#34;&gt;LawGPT-zh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åˆ©ç”¨ChatGPTæ¸…æ´—CrimeKgAssitantæ•°æ®é›†å¾—åˆ°52kå•è½®é—®ç­”+æˆ‘ä»¬æ ¹æ®ä¸­åäººæ°‘å…±å’Œå›½æ³•å¾‹æ‰‹å†Œä¸Šæœ€æ ¸å¿ƒçš„9kæ³•å¾‹æ¡æ–‡ï¼Œåˆ©ç”¨ChatGPTè”æƒ³ç”Ÿæˆå…·ä½“çš„æƒ…æ™¯é—®ç­”+çŸ¥è¯†é—®ç­”ä½¿ç”¨ChatGPTåŸºäºæ–‡æœ¬æ„å»ºQAå¯¹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ³•å¾‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT&#34;&gt;LawGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŸºäºllama+æ‰©å……è¯è¡¨äºŒæ¬¡é¢„è®­ç»ƒ+åŸºäºæ³•å¾‹æ¡æ¬¾æ„å»ºQAæŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ³•å¾‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AndrewZhe/lawyer-llama&#34;&gt;Lawyer Llama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ³•å¾‹æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼šå’¨è¯¢+æ³•å¾‹è€ƒè¯•+å¯¹è¯è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ³•å¾‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CSHaitao/LexiLaw&#34;&gt;LexiLaw&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ³•å¾‹æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼šé—®ç­”+ä¹¦ç±æ¦‚å¿µè§£é‡Šï¼Œæ³•æ¡å†…å®¹è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ³•å¾‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chatlaw.cloud/&#34;&gt;ChatLaw&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŒ—å¤§æ¨å‡ºçš„æ³•å¾‹å¤§æ¨¡å‹ï¼Œåº”ç”¨å½¢å¼å¾ˆæ–°é¢–ç±»ä¼¼é¢‘é“å†…æµä¸€åˆ‡åŠŸèƒ½çš†èåˆåœ¨å¯¹è¯å½¢å¼å†…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ³•å¾‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zhihaiLLM/wisdomInterrogatory&#34;&gt;å½•é—®æ¨¡å‹&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åœ¨baichuanåŸºç¡€ä¸Š40GäºŒæ¬¡é¢„è®­ç»ƒ+100KæŒ‡ä»¤å¾®è°ƒï¼Œåœ¨çŸ¥è¯†åº“æ„å»ºä¸Šé‡‡ç”¨äº†Emb+æ„å›¾+å…³é”®è¯è”æƒ³ç»“åˆçš„æ–¹æ¡ˆ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://finchat.io/&#34;&gt;FinChat.io&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä½¿ç”¨æœ€æ–°çš„è´¢åŠ¡æ•°æ®ï¼Œç”µè¯ä¼šè®®è®°å½•ï¼Œå­£åº¦å’Œå¹´åº¦æŠ¥å‘Šï¼ŒæŠ•èµ„ä¹¦ç±ç­‰è¿›è¡Œè®­ç»ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CogStack/OpenGPT&#34;&gt;OpenGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é¢†åŸŸLLMæŒ‡ä»¤æ ·æœ¬ç”Ÿæˆ+å¾®è°ƒæ¡†æ¶&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ssymmetry/BBT-FinCUGE-Applications/tree/main&#34;&gt;ä¹¾å…ƒBigBangé‡‘è2äº¿æ¨¡å‹&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é‡‘èé¢†åŸŸé¢„è®­ç»ƒ+ä»»åŠ¡å¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/xyz-nlp/XuanYuan2.0&#34;&gt;åº¦å°æ»¡åƒäº¿é‡‘èå¤§æ¨¡å‹&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åœ¨Bloom-176Bçš„åŸºç¡€ä¸Šè¿›è¡Œé‡‘è+ä¸­æ–‡é¢„è®­ç»ƒå’Œå¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ltxtrading.com/bondgpt&#34;&gt;bondGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT4åœ¨ç»†åˆ†å€ºåˆ¸å¸‚åœºçš„åº”ç”¨å¼€æ”¾ç”³è¯·ä¸­&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cnbc.com/2023/05/25/jpmorgan-develops-ai-investment-advisor.html&#34;&gt;IndexGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JPMorganåœ¨ç ”çš„ç”Ÿæˆå¼æŠ•èµ„é¡¾é—®&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/vLvxvi2nOywkjt7ppiFC2g&#34;&gt;æ’ç”ŸLightGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é‡‘èé¢†åŸŸç»§ç»­é¢„è®­ç»ƒ+æ’ä»¶åŒ–è®¾è®¡&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://finance.sina.com.cn/jjxw/2023-07-03/doc-imyzmaut2132017.shtml&#34;&gt;çŸ¥å½¼é˜¿å°”æ³•&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ä¼æŸ¥æŸ¥å•†æŸ¥å¤§æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.alphabox.top&#34;&gt;AlphaBox&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ç†µç®€ç§‘æŠ€å‘å¸ƒå¤§æ¨¡å‹é‡‘èåº”ç”¨ï¼Œå¤šæ–‡æ¡£é—®ç­”+ä¼šè®®è½¬å½•+æ–‡æ¡£ç¼–è¾‘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.datagrand.com/products/aigc/&#34;&gt;æ›¹æ¤&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;è¾¾è§‚å‘å¸ƒé‡‘èå¤§æ¨¡å‹èåˆdata2textç­‰é‡‘èä»»åŠ¡ï¼Œèµ‹èƒ½æŠ¥å‘Šå†™ä½œ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese&#34;&gt;èšå®ç›†&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŸºäº LLaMA ç³»åŸºæ¨¡å‹ç»è¿‡ä¸­æ–‡é‡‘èçŸ¥è¯†æŒ‡ä»¤ç²¾è°ƒ/æŒ‡ä»¤å¾®è°ƒ(Instruct-tuning) çš„å¾®è°ƒæ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/chancefocus/PIXIU&#34;&gt;PIXIU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ•´ç†äº†å¤šä¸ªé‡‘èä»»åŠ¡æ•°æ®é›†åŠ å…¥äº†æ—¶é—´åºåˆ—æ•°æ®è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chat.funddb.cn/&#34;&gt;ChatFund&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;éŸ­åœˆå„¿å‘å¸ƒçš„ç¬¬ä¸€ä¸ªåŸºé‡‘å¤§æ¨¡å‹ï¼Œçœ‹èµ·æ¥æ˜¯åšäº†å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒï¼Œå’ŒAPPå·²æœ‰çš„æ•°æ®åŠŸèƒ½è¿›è¡Œäº†å…¨æ–¹ä½çš„æ‰“é€šï¼Œä»é€‰åŸºï¼Œåˆ°æŒä»“åˆ†æç­‰ç­‰&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinGPT&#34;&gt;FinGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é‡‘èä¼ ç»Ÿä»»åŠ¡å¾®è°ƒ or chatgptç”Ÿæˆé‡‘èå·¥å…·è°ƒç”¨&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é‡‘è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/TongjiFinLab/CFGPT&#34;&gt;CFGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é‡‘èé¢„è®­ç»ƒ+æŒ‡ä»¤å¾®è°ƒ+RAGç­‰æ£€ç´¢ä»»åŠ¡å¢å¼º&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç¼–ç¨‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bigcode-project/starcoder&#34;&gt;Starcoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;80ç§ç¼–ç¨‹è¯­è¨€+Issue+Commitè®­ç»ƒå¾—åˆ°çš„ç¼–ç¨‹å¤§æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç¼–ç¨‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cubenlp/ChatSQL&#34;&gt;ChatSQL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åŸºäºChatGLMå®ç°NL2sql&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç¼–ç¨‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://keg.cs.tsinghua.edu.cn/codegeex/index_zh.html&#34;&gt;codegeex&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;13Bé¢„è®­ç»ƒ+å¾®è°ƒå¤šè¯­è¨€å˜æˆå¤§æ¨¡å‹&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç¼–ç¨‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/CodeGeeX2&#34;&gt;codegeex2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chatglm2çš„åŸºç¡€ä¸ŠCodeGeeX2-6B è¿›ä¸€æ­¥ç»è¿‡äº† 600B ä»£ç æ•°æ®é¢„è®­ç»ƒ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç¼–ç¨‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://stability.ai/blog/stablecode-llm-generative-ai-coding&#34;&gt;stabelcode&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;560B tokenå¤šè¯­è¨€é¢„è®­ç»ƒ+ 120,000 ä¸ª AlpacaæŒ‡ä»¤å¯¹é½&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç¼–ç¨‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/defog-ai/sqlcoder&#34;&gt;SQLCoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;åœ¨StarCoderçš„åŸºç¡€ä¸Šå¾®è°ƒ15Bè¶…è¶Šgpt3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ•°å­¦&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.mathgpt.com/&#34;&gt;MathGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;æ˜¯å¥½æœªæ¥è‡ªä¸»ç ”å‘çš„ï¼Œé¢å‘å…¨çƒæ•°å­¦çˆ±å¥½è€…å’Œç§‘ç ”æœºæ„ï¼Œä»¥è§£é¢˜å’Œè®²é¢˜ç®—æ³•ä¸ºæ ¸å¿ƒçš„å¤§æ¨¡å‹ã€‚&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ•°å­¦&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tiger-ai-lab.github.io/MAmmoTH/&#34;&gt;MammoTH&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;é€šè¿‡COT+POTæ„å»ºäº†MathInstructæ•°æ®é›†å¾®è°ƒllamaåœ¨OODæ•°æ®é›†ä¸Šè¶…è¶Šäº†WizardLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;äº¤é€š&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/DUOMO/TransGPT&#34;&gt;TransGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama-7B+34.6ä¸‡é¢†åŸŸé¢„è®­ç»ƒ+5.8ä¸‡æ¡é¢†åŸŸæŒ‡ä»¤å¯¹è¯å¾®è°ƒï¼ˆæ¥è‡ªæ–‡æ¡£é—®ç­”ï¼‰&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;äº¤é€š&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lijlansg/TrafficGPT/tree/main&#34;&gt;TrafficGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ChatGPT+Promptå®ç°è§„åˆ’ï¼Œè°ƒç”¨äº¤é€šæµé‡é¢†åŸŸä¸“ä¸šTFMæ¨¡å‹ï¼ŒTFMè´Ÿè´£æ•°æ®åˆ†æï¼Œä»»åŠ¡æ‰§è¡Œï¼Œå¯è§†åŒ–ç­‰æ“ä½œ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç§‘æŠ€&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/gmftbyGMFTBY/science-llm&#34;&gt;Mozi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;çº¢ç¡è¡£é¢„è®­ç»ƒ+è®ºæ–‡QAæ•°æ®é›† + ChatGPTæ‰©å……ç§‘ç ”å¯¹è¯æ•°æ®&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¤©æ–‡&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Yu-Yang-Li/StarGLM&#34;&gt;StarGLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;å¤©æ–‡çŸ¥è¯†æŒ‡ä»¤å¾®è°ƒï¼Œé¡¹ç›®è¿›è¡Œä¸­åæœŸè€ƒè™‘å¤©æ–‡äºŒæ¬¡é¢„è®­ç»ƒ+KG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å†™ä½œ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.zhihu.com/question/613058630&#34;&gt;é˜…æ–‡-ç½‘æ–‡å¤§æ¨¡å‹ä»‹ç»&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ç­¾çº¦ä½œè€…å†…æµ‹ä¸­ï¼Œä¸»æ‰“çš„å†…å®¹ä¸ºæ‰“æ–—åœºæ™¯ï¼Œå‰§æƒ…åˆ‡æ¢ï¼Œç¯å¢ƒæå†™ï¼Œäººè®¾ï¼Œä¸–ç•Œè§‚ç­‰è¾…åŠ©ç‰‡æ®µçš„ç”Ÿæˆ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å†™ä½œ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/search?q=MediaGPT&amp;amp;type=repositories&#34;&gt;MediaGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama-7Bæ‰©å……è¯è¡¨+æŒ‡ä»¤å¾®è°ƒï¼ŒæŒ‡ä»¤æ¥è‡ªå›½å†…åª’ä½“ä¸“å®¶ç»™å‡ºçš„åœ¨æ–°é—»åˆ›ä½œä¸Šçš„80ä¸ªå­ä»»åŠ¡&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ç”µå•†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Alibaba-NLP/EcomGPT&#34;&gt;EcomGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ç”µå•†é¢†åŸŸä»»åŠ¡æŒ‡ä»¤å¾®è°ƒå¤§æ¨¡å‹ï¼ŒæŒ‡ä»¤æ ·æœ¬250ä¸‡ï¼ŒåŸºåº§æ¨¡å‹æ˜¯Bloomz&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tool and Library&lt;/h2&gt; &#xA;&lt;h3&gt;æ¨ç†æ¡†æ¶&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;å·¥å…·æè¿°&lt;/th&gt; &#xA;   &lt;th&gt;é“¾æ¥&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FlexFlowï¼šæ¨¡å‹éƒ¨ç½²æ¨ç†æ¡†æ¶&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/flexflow/FlexFlow&#34;&gt;https://github.com/flexflow/FlexFlow&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Medusaï¼šé’ˆå¯¹é‡‡æ ·è§£ç çš„æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œå¯ä»¥å’Œå…¶ä»–ç­–ç•¥ç»“åˆ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FasterDecoding/Medusa&#34;&gt;https://github.com/FasterDecoding/Medusa&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FlexGen: LLMæ¨ç† CPU Offloadè®¡ç®—æ¶æ„&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FMInference/FlexGen&#34;&gt;https://github.com/FMInference/FlexGen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VLLMï¼šè¶…é«˜é€Ÿæ¨ç†æ¡†æ¶Vicunaï¼ŒArenaèƒŒåçš„æ— åè‹±é›„ï¼Œæ¯”HFå¿«24å€ï¼Œæ”¯æŒå¾ˆå¤šåŸºåº§æ¨¡å‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;https://github.com/vllm-project/vllm&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Streamingllm: æ–°æ³¨æ„åŠ›æ± Attentionæ–¹æ¡ˆï¼Œæ— éœ€å¾®è°ƒæ‹“å±•æ¨¡å‹æ¨ç†é•¿åº¦ï¼ŒåŒæ—¶ä¸ºæ¨ç†æé€Ÿ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm&#34;&gt;https://github.com/mit-han-lab/streaming-llm&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;æŒ‡ä»¤å¾®è°ƒï¼Œé¢„è®­ç»ƒï¼Œrlhfæ¡†æ¶&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;å·¥å…·æè¿°&lt;/th&gt; &#xA;   &lt;th&gt;é“¾æ¥&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRAï¼šLow-RankæŒ‡ä»¤å¾®è°ƒæ–¹æ¡ˆ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;peftï¼šparameter-efficient prompt tunngingå·¥å…·é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;https://github.com/huggingface/peft&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RL4LMsï¼šAllenAIçš„RLå·¥å…·&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/RL4LMs&#34;&gt;https://github.com/allenai/RL4LMs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLLTEï¼šæ¸¯å¤§ï¼Œå¤§ç–†ç­‰è”åˆå¼€æºRLLTEå¼€æºå­¦ä¹ æ¡†æ¶&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/RLE-Foundation/rllte&#34;&gt;https://github.com/RLE-Foundation/rllte&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;trlï¼šåŸºäºTransformerçš„å¼ºåŒ–è®­ç»ƒæ¡†æ¶&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lvwerra/trl&#34;&gt;https://github.com/lvwerra/trl&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;trlxï¼šåˆ†å¸ƒå¼è®­ç»ƒtrl&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CarperAI/trlx&#34;&gt;https://github.com/CarperAI/trlx&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;åŒ—å¤§å¼€æºæ²³ç‹¸é¡¹ç›®å¯å¤ç°RLHFï¼Œæ”¯æŒå¤šæ•°LLMï¼Œæä¾›RLHFæ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PKU-Alignment/safe-rlhf&#34;&gt;https://github.com/PKU-Alignment/safe-rlhf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RL4LMsï¼šAllenAIçš„RLå·¥å…·&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/RL4LMs&#34;&gt;https://github.com/allenai/RL4LMs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LMFlowï¼šæ¸¯ç§‘å¤§å®éªŒå®¤å¼€æºçš„å¤§æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œæ”¯æŒä»¥ä¸Šå¤šæ•°å¼€æºæ¨¡å‹çš„æŒ‡ä»¤å¾®è°ƒå’ŒRLHF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OptimalScale/LMFlow&#34;&gt;https://github.com/OptimalScale/LMFlow&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hugNLP:åŸºäºHuggingfaceå¼€å‘ç»§æ‰¿PromptæŠ€æœ¯ï¼Œé¢„è®­ç»ƒå’Œæ˜¯æŒ‡è¾“å…¥ç­‰å¤šç§æ–¹æ¡ˆ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/wjn1996/HugNLP&#34;&gt;https://github.com/wjn1996/HugNLP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deepspeedï¼šé’ˆå¯¹RLè®­ç»ƒå’Œæ¨ç†çš„æ•´åˆä¼˜åŒ–&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;https://github.com/microsoft/DeepSpeed&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Uerpy:é¢„è®­ç»ƒæ¡†æ¶æ”¯æŒlm,mlm,unilmç­‰&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dbiir/UER-py&#34;&gt;https://github.com/dbiir/UER-py&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TecentPretrain: Uerpyçš„é‡æ„ç‰ˆæœ¬æ”¯æŒllamaé¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Tencent/TencentPretrain/tree/main&#34;&gt;https://github.com/Tencent/TencentPretrain/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lamini: æ•´åˆæŒ‡ä»¤æ•°æ®ç”Ÿæˆï¼ŒSFTï¼ŒRLHFçš„å·¥å…·åº“&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lamini-ai/lamini/&#34;&gt;https://github.com/lamini-ai/lamini/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chain-of-thought-hubï¼šæ¨¡å‹æ¨ç†èƒ½åŠ›è¯„ä¼°å¹³å°&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FranxYao/chain-of-thought-hub&#34;&gt;https://github.com/FranxYao/chain-of-thought-hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EasyEditï¼šæµ™å¤§å¼€æºæ”¯æŒå¤šç§æ¨¡å‹ï¼Œå¤šç§æ–¹æ¡ˆçš„æ¨¡å‹çŸ¥è¯†ç²¾å‡†ç¼–è¾‘å™¨&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zjunlp/EasyEdit&#34;&gt;https://github.com/zjunlp/EasyEdit&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenDeltaï¼šé›†æˆäº†å„ç§å¢é‡å¾®è°ƒæ–¹æ¡ˆçš„å¼€æºå®ç°&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/OpenDelta&#34;&gt;https://github.com/thunlp/OpenDelta&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;LLM Agentå·¥å…·&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;å·¥å…·æè¿°&lt;/th&gt; &#xA;   &lt;th&gt;é“¾æ¥&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;langchainï¼šLLM Agentæ¡†æ¶&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;https://github.com/hwchase17/langchain&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama indexï¼šLLM Agentæ¡†æ¶&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;https://github.com/jerryjliu/llama_index&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;semantic-kernelï¼šæ•´åˆå¤§æ¨¡å‹å’Œç¼–ç¨‹è¯­è¨€çš„SDK&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/semantic-kernel&#34;&gt;https://github.com/microsoft/semantic-kernel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BMTTools: æ¸…åå‡ºå“å¤šå·¥å…·è°ƒç”¨å¼€æºåº“ï¼Œæä¾›å¾®è°ƒæ•°æ®å’Œè¯„ä¼°ToolBench&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/BMTools&#34;&gt;https://github.com/OpenBMB/BMTools&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BabyAGIï¼šè‡ªæ‰§è¡ŒLLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yoheinakajima/babyagi&#34;&gt;https://github.com/yoheinakajima/babyagi&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AutoGPTï¼šè‡ªæ‰§è¡ŒLLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Torantulino/Auto-GPT&#34;&gt;https://github.com/Torantulino/Auto-GPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MetaGPT: è¦†ç›–è½¯ä»¶å…¬å¸å…¨ç”Ÿå‘½æµç¨‹ï¼Œä¾‹å¦‚äº§å“ç»ç†ç­‰å„ä¸ªèŒä¸šçš„AutoGPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/geekan/MetaGPT&#34;&gt;https://github.com/geekan/MetaGPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResearchGPT: è®ºæ–‡å†™ä½œé¢†åŸŸçš„AutoGPTï¼Œèåˆè®ºæ–‡æ‹†è§£+ç½‘ç»œçˆ¬è™«&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/assafelovic/gpt-researcher&#34;&gt;https://github.com/assafelovic/gpt-researcher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MiniAGIï¼šè‡ªæ‰§è¡ŒLLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/muellerberndt/mini-agi&#34;&gt;https://github.com/muellerberndt/mini-agi&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AL Legionï¼š è‡ªæ‰§è¡ŒLLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/eumemic/ai-legion&#34;&gt;https://github.com/eumemic/ai-legion&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AgentVerseï¼šå¤šæ¨¡å‹äº¤äº’ç¯å¢ƒ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/AgentVerse&#34;&gt;https://github.com/OpenBMB/AgentVerse&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AgentSims: ç»™å®šä¸€ä¸ªç¤¾ä¼šç¯å¢ƒï¼Œè¯„ä¼°LLMä½œä¸ºæ™ºèƒ½ä½“çš„é¢„å®šä»»åŠ¡ç›®æ ‡å®Œæˆèƒ½åŠ›çš„æ²™ç›’ç¯å¢ƒ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/py499372727/AgentSims/&#34;&gt;https://github.com/py499372727/AgentSims/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPTRPGï¼šRPGç¯å¢ƒ AI Agentæ¸¸æˆåŒ–&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dzoba/gptrpg&#34;&gt;https://github.com/dzoba/gptrpg&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Generative Agents:æ–¯å¦ç¦AIå°é•‡çš„å¼€æºä»£ç &lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/joonspk-research/generative_agents&#34;&gt;https://github.com/joonspk-research/generative_agents&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPTeamï¼šå¤šæ™ºèƒ½ä½“äº¤äº’&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/101dotxyz/GPTeam&#34;&gt;https://github.com/101dotxyz/GPTeam&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPTEngineerï¼šè‡ªåŠ¨å·¥å…·æ„å»ºå’Œä»£ç ç”Ÿæˆ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer&#34;&gt;https://github.com/AntonOsika/gpt-engineer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Jarvis: å¤§æ¨¡å‹è°ƒç”¨å°æ¨¡å‹æ¡†æ¶ï¼Œç»™å°æ¨¡å‹ä¸€ä¸ªæœªæ¥ï¼&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/search?q=jarvis&#34;&gt;https://github.com/search?q=jarvis&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLM-ToolMaker:è®©LLMè‡ªå·±åˆ¶é€ Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FMInference/FlexGen&#34;&gt;https://github.com/FMInference/FlexGen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gorilla: LLMè°ƒç”¨å¤§é‡API&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ShishirPatil/gorilla&#34;&gt;https://github.com/ShishirPatil/gorilla&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IncarnaMindï¼šå¤šæ–‡æ¡£RAGæ–¹æ¡ˆï¼ŒåŠ¨æ€chunkingçš„æ–¹æ¡ˆå¯ä»¥å€Ÿé‰´&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/junruxiong/IncarnaMind&#34;&gt;https://github.com/junruxiong/IncarnaMind&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wenda:é—»è¾¾å°æ¨¡å‹æ•´åˆæœç´¢ç”¨äºçŸ¥è¯†èå…¥&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/l15y/wenda&#34;&gt;https://github.com/l15y/wenda&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WorkGPTï¼šç±»ä¼¼AutoGPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/team-openpm/workgpt&#34;&gt;https://github.com/team-openpm/workgpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deep-KEï¼šåŸºäºLLMå¯¹æ•°æ®è¿›è¡Œæ™ºèƒ½è§£æå®ç°çŸ¥è¯†æŠ½å–&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zjunlp/DeepKE&#34;&gt;https://github.com/zjunlp/DeepKE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vectraï¼šå¹³å°åŒ–çš„LLM Agentæ­å»ºæ–¹æ¡ˆï¼Œä»ç´¢å¼•æ„å»ºï¼Œå†…å®¹å¬å›æ’åºï¼Œåˆ°äº‹å®æ£€æŸ¥çš„LLMç”Ÿæˆ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://vectara.com/tour-vectara/&#34;&gt;https://vectara.com/tour-vectara/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alexandria: ä»Arixè®ºæ–‡å¼€å§‹æŠŠæ•´ä¸ªäº’è”ç½‘å˜æˆå‘é‡ç´¢å¼•ï¼Œå¯ä»¥å…è´¹ä¸‹è½½&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://alex.macrocosm.so/download&#34;&gt;https://alex.macrocosm.so/download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RapidAPI: ç»Ÿä¸€è¿™ä¸ªä¸–ç•Œçš„æ‰€æœ‰APIï¼Œæœ€å¤§API Hubï¼Œæœ‰è°ƒç”¨æˆåŠŸç‡ï¼Œlatencyç­‰ï¼Œæ˜¯çœŸçˆ±ï¼&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rapidapi.com/hub&#34;&gt;https://rapidapi.com/hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Data-Copilotï¼šæ—¶é—´åºåˆ—ç­‰ç»“æ„åŒ–æ•°æ®åˆ†æé¢†åŸŸçš„Agentè§£å†³æ–¹æ¡ˆ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zwq2018/Data-Copilot&#34;&gt;https://github.com/zwq2018/Data-Copilot&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DB-GPT: ä»¥æ•°æ®åº“ä¸ºåŸºç¡€çš„GPTå®éªŒé¡¹ç›®ï¼Œä½¿ç”¨æœ¬åœ°åŒ–çš„GPTå¤§æ¨¡å‹ä¸æ‚¨çš„æ•°æ®å’Œç¯å¢ƒè¿›è¡Œäº¤äº’&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-cn/zh_CN/latest/index.html&#34;&gt;https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-cn/zh_CN/latest/index.html&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;guardrailsï¼šé™ä½æ¨¡å‹å¹»è§‰çš„pythonæ¡†æ¶ï¼Œprompæ¨¡æ¿+validation+ä¿®æ­£&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/shreyar/guardrails&#34;&gt;https://github.com/shreyar/guardrails&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;guidanceï¼šå¾®è½¯æ–°å¼€æºæ¡†æ¶ï¼ŒåŒæ ·æ˜¯é™ä½æ¨¡å‹å¹»è§‰çš„æ¡†æ¶ï¼Œprompt+chainçš„å‡çº§ç‰ˆåŠ å…¥é€æ­¥ç”Ÿæˆå’Œæ€ç»´é“¾è·¯&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/guidance-ai/guidance&#34;&gt;https://github.com/guidance-ai/guidance&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ragas: è¯„ä¼°æ£€ç´¢å¢å¼ºLLMæ•ˆæœçš„æ¡†æ¶ï¼ŒåŸºäºå¤§æ¨¡å‹promptè¯„ä¼°äº‹å®æ€§ï¼Œå¬å›ç›¸å…³æ€§ï¼Œå¬å›å†…å®¹è´¨é‡ï¼Œå›ç­”ç›¸å…³æ€§ç­‰&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/explodinggradients/ragas#fire-quickstart&#34;&gt;https://github.com/explodinggradients/ragas#fire-quickstart&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;langflowï¼šæŠŠlangchainç­‰agentç»„ä»¶åšæˆäº†å¯æ‹–æ‹½å¼çš„UI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/logspace-ai/langflow&#34;&gt;https://github.com/logspace-ai/langflow&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Haystack: LLM Agent æ¡†æ¶ï¼Œpipelineçš„è®¾è®¡æ¨¡å¼ä¸ªäººæ„Ÿè§‰æ¯”langchainæ›´çµæ´»æ›´ç®€æ´&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/deepset-ai/haystack&#34;&gt;https://github.com/deepset-ai/haystack&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EdgeChain: é€šè¿‡Jsonneté…ç½®æ–‡ä»¶å®ç°LLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/arakoodev/EdgeChains/tree/main&#34;&gt;https://github.com/arakoodev/EdgeChains/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YiVal:è°ƒæ•´å’Œè¯„ä¼°promptsã€é…ç½®å’Œæ¨¡å‹å‚æ•°çš„å¼€æºGenAI-Opså·¥å…·&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/YiVal/YiVal&#34;&gt;https://github.com/YiVal/YiVal&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Training Data&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;æ•°æ®ç±»å‹&lt;/th&gt; &#xA;   &lt;th&gt;æ•°æ®æè¿°&lt;/th&gt; &#xA;   &lt;th&gt;æ•°æ®é“¾æ¥&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;self-instructï¼ŒGPT3è‡ªåŠ¨ç”Ÿæˆ&amp;amp;è¿‡æ»¤å¾—åˆ°æŒ‡ä»¤é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;https://github.com/yizhongw/self-instruct&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;Standford Alpacaï¼š52K text-davinci-003ç”Ÿæˆçš„self-instructæŒ‡ä»¤æ•°æ®é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;GPT4-for-LLM ä¸­æ–‡+è‹±æ–‡+å¯¹æ¯”æŒ‡ä»¤&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;GPTTeacheræ›´å¤šæ ·çš„é€šç”¨æŒ‡ä»¤ï¼Œè§’è‰²æ‰®æ¼”å’Œä»£ç æŒ‡ä»¤&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/teknium1/GPTeacher/tree/main&#34;&gt;https://github.com/teknium1/GPTeacher/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;ä¸­æ–‡ç¿»è¯‘Alpacaè¿˜æœ‰ä¸€äº›å…¶ä»–æŒ‡ä»¤æ•°æ®é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hikariming/alpaca_chinese_dataset&#34;&gt;https://github.com/hikariming/alpaca_chinese_dataset&lt;/a&gt; &lt;a href=&#34;https://github.com/carbonz0/alpaca-chinese-dataset&#34;&gt;https://github.com/carbonz0/alpaca-chinese-dataset&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;alpacaæŒ‡ä»¤GPT4ç”Ÿæˆï¼Œå’Œä»¥ä¸Šå‡ ç‰ˆå¯¹æ¯”æ˜¾è‘—è´¨é‡æ›´é«˜ï¼Œå›å¤æ›´é•¿&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/tree/main&#34;&gt;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;Guanacoæ•°æ®ï¼šå¯¹AlphcaæŒ‡ä»¤é‡å†™åä»¥ä¸åŒè¯­è¨€ç”Ÿæˆæ€»å…±534Kï¼Œæœ‰å¯¹è¯å’Œéå¯¹è¯ç±»å‹ï¼Œè¿˜æœ‰è¡¥å……çš„QAç”Ÿæˆæ ·æœ¬&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;OIGä¸­æ–‡æŒ‡ä»¤åŒ…æ‹¬ç¿»è¯‘alpaca+natural+unnaturalï¼Œå¤šè½®å¯¹è¯ï¼Œè€ƒè¯•ï¼ŒleetcodeæŒ‡ä»¤&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BAAI-Zlab/COIG&#34;&gt;https://github.com/BAAI-Zlab/COIG&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;Vicunaè®­ç»ƒä½¿ç”¨çš„æ ·æœ¬ï¼Œç”¨APIè·å–äº†sharegptä¸Šç”¨æˆ·å’Œchatgptå¯¹è¯å†å²ï¼Œéƒ¨åˆ†ç½‘å‹æ•´ç†åˆ°äº†HF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/domeccleston/sharegpt&#34;&gt;https://github.com/domeccleston/sharegpt&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main&#34;&gt;https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;HC3æŒ‡ä»¤æ•°æ®ä¸­è‹±æ–‡ï¼ŒåŒ…æ‹¬é‡‘èï¼Œå¼€æ”¾QAï¼Œç™¾ç§‘ï¼ŒDBQAï¼ŒåŒ»å­¦ç­‰åŒ…å«äººå·¥å›å¤&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main&#34;&gt;https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;MOSSå¼€æºçš„SFTæ•°æ®åŒ…å«ä½¿ç”¨pluginçš„å¯¹è¯æ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main&#34;&gt;https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;InstructWildæ•°æ®ï¼šç”¨å››å¤„çˆ¬å–çš„chatgptæŒ‡ä»¤ä½œä¸ºç§å­self-instructæ‰©å……ç”Ÿæˆï¼Œä¸­è‹±åŒè¯­&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/XueFuzhao/InstructionWild/tree/main/data&#34;&gt;https://github.com/XueFuzhao/InstructionWild/tree/main/data&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;BELLE100ä¸‡æŒ‡ä»¤æ•°æ®ï¼Œå‚è€ƒAlpacaç”¨ChatGPTç”Ÿæˆï¼Œæœ‰æ•°å­¦ï¼Œå¤šè½®å¯¹è¯ï¼Œæ ¡è‰²å¯¹è¯ç­‰ç­‰&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;https://github.com/LianjiaTech/BELLE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;PromptCLUEå¤šä»»åŠ¡æç¤ºæ•°æ®é›†ï¼šæ¨¡æ¿æ„å»ºï¼ŒåªåŒ…å«æ ‡å‡†NLPä»»åŠ¡&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CLUEbenchmark/pCLUE&#34;&gt;https://github.com/CLUEbenchmark/pCLUE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;TK-Instructå¾®è°ƒç”¨çš„æŒ‡ä»¤æ•°æ®é›†, å…¨äººå·¥æ ‡æ³¨1600+NLPä»»åŠ¡&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://instructions.apps.allenai.org/&#34;&gt;https://instructions.apps.allenai.org/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;T0å¾®è°ƒç”¨çš„æŒ‡ä»¤æ•°æ®é›†ï¼ˆP3ï¼‰&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigscience/P3&#34;&gt;https://huggingface.co/datasets/bigscience/P3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;p3è¡ç”Ÿçš„46ç§å¤šè¯­è¨€æ•°æ®é›†ï¼ˆxmtfï¼‰&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bigscience-workshop/xmtf&#34;&gt;https://github.com/bigscience-workshop/xmtf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;Unnatural Instructionä½¿ç”¨GPT3ç”Ÿæˆåæ”¹å†™å¾—åˆ°240k&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/orhonovich/unnatural-instructions&#34;&gt;https://github.com/orhonovich/unnatural-instructions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;alpaca COTå¯¹å¤šä¸ªæ•°æ®æºè¿›è¡Œäº†æ¸…ç†å¹¶ç»Ÿä¸€æ ¼å¼æ”¾åˆ°çš„äº†HF, é‡ç‚¹æ˜¯äººå·¥æ•´ç†çš„COTæ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PhoebusSi/Alpaca-CoT&#34;&gt;https://github.com/PhoebusSi/Alpaca-CoT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;äººå·¥ç¼–å†™åŒ…å«23ç§å¸¸è§çš„ä¸­æ–‡NLPä»»åŠ¡çš„æŒ‡ä»¤æ•°æ®ï¼Œä¸­æ–‡å†™ä½œæ–¹å‘&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yangjianxin1/Firefly&#34;&gt;https://github.com/yangjianxin1/Firefly&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;Amazon COTæŒ‡ä»¤æ ·æœ¬åŒ…æ‹¬å„ç±»QAï¼Œbigbenchï¼Œmathç­‰&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/amazon-science/auto-cot&#34;&gt;https://github.com/amazon-science/auto-cot&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;CSLåŒ…å« 396,209 ç¯‡ä¸­æ–‡æ ¸å¿ƒæœŸåˆŠè®ºæ–‡å…ƒä¿¡æ¯ ï¼ˆæ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯ã€å­¦ç§‘ã€é—¨ç±»ï¼‰å¯åšé¢„è®­ç»ƒå¯æ„å»ºNLPæŒ‡ä»¤ä»»åŠ¡&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ydli-ai/CSL&#34;&gt;https://github.com/ydli-ai/CSL&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;alpaca code 20Kä»£ç æŒ‡ä»¤æ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sahil280114/codealpaca#data-release&#34;&gt;https://github.com/sahil280114/codealpaca#data-release&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;GPT4Tools 71K GPT4æŒ‡ä»¤æ ·æœ¬&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/StevenGrove/GPT4Tools&#34;&gt;https://github.com/StevenGrove/GPT4Tools&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;GPT4æŒ‡ä»¤+è§’è‰²æ‰®æ¼”+ä»£ç æŒ‡ä»¤&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/teknium1/GPTeacher&#34;&gt;https://github.com/teknium1/GPTeacher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;Mol-Instructions 2043K åˆ†å­+è›‹ç™½è´¨+ç”Ÿç‰©åˆ†å­æ–‡æœ¬æŒ‡ä»¤ï¼Œè¦†ç›–åˆ†å­è®¾è®¡ã€è›‹ç™½è´¨åŠŸèƒ½é¢„æµ‹ã€è›‹ç™½è´¨è®¾è®¡ç­‰ä»»åŠ¡&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zjunlp/Mol-Instructions&#34;&gt;https://github.com/zjunlp/Mol-Instructions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ•°å­¦&lt;/td&gt; &#xA;   &lt;td&gt;è…¾è®¯äººå·¥æ™ºèƒ½å®éªŒå®¤å‘å¸ƒç½‘ä¸Šçˆ¬å–çš„æ•°å­¦é—®é¢˜APE210k&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Chenny0808/ape210k&#34;&gt;https://github.com/Chenny0808/ape210k&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ•°å­¦&lt;/td&gt; &#xA;   &lt;td&gt;çŒ¿è¾…å¯¼ AI Labå¼€æºå°å­¦åº”ç”¨é¢˜Math23K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SCNU203/Math23k/tree/main&#34;&gt;https://github.com/SCNU203/Math23k/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ•°å­¦&lt;/td&gt; &#xA;   &lt;td&gt;grade school mathæŠŠOpenAIçš„é«˜ä¸­æ•°å­¦é¢˜æœ‰æ”¹é€ æˆæŒ‡ä»¤æ ·æœ¬æœ‰2-8æ­¥æ¨ç†è¿‡ç¨‹&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions&#34;&gt;https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ•°å­¦&lt;/td&gt; &#xA;   &lt;td&gt;æ•°å­¦é—®ç­”æ•°æ®é›†æœ‰æ¨ç†è¿‡ç¨‹å’Œå¤šé¡¹é€‰æ‹©&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/math_qa/viewer/default/test?row=2&#34;&gt;https://huggingface.co/datasets/math_qa/viewer/default/test?row=2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ•°å­¦&lt;/td&gt; &#xA;   &lt;td&gt;AMCç«èµ›æ•°å­¦é¢˜&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/competition_math&#34;&gt;https://huggingface.co/datasets/competition_math&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;æ•°å­¦&lt;/td&gt; &#xA;   &lt;td&gt;çº¿æ€§ä»£æ•°ç­‰çº¯æ•°å­¦è®¡ç®—é¢˜&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/math_dataset&#34;&gt;https://huggingface.co/datasets/math_dataset&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä»£ç &lt;/td&gt; &#xA;   &lt;td&gt;APPSä»ä¸åŒçš„å¼€æ”¾è®¿é—®ç¼–ç ç½‘ç«™Codeforcesã€Kattis ç­‰æ”¶é›†çš„é—®é¢˜&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/APPS&#34;&gt;https://opendatalab.org.cn/APPS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä»£ç &lt;/td&gt; &#xA;   &lt;td&gt;Lyraä»£ç ç”±å¸¦æœ‰åµŒå…¥å¼ SQL çš„ Python ä»£ç ç»„æˆï¼Œç»è¿‡ä»”ç»†æ³¨é‡Šçš„æ•°æ®åº“æ“ä½œç¨‹åºï¼Œé…æœ‰ä¸­æ–‡è¯„è®ºå’Œè‹±æ–‡è¯„è®ºã€‚&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/Lyra&#34;&gt;https://opendatalab.org.cn/Lyra&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä»£ç &lt;/td&gt; &#xA;   &lt;td&gt;Conalaæ¥è‡ªStackOverflowé—®é¢˜,æ‰‹åŠ¨æ³¨é‡Š3kï¼Œè‹±æ–‡&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/CoNaLa/download&#34;&gt;https://opendatalab.org.cn/CoNaLa/download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä»£ç &lt;/td&gt; &#xA;   &lt;td&gt;code-alpaca ChatGPTç”Ÿæˆ20Kä»£ç æŒ‡ä»¤æ ·æœ¬&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sahil280114/codealpaca.git&#34;&gt;https://github.com/sahil280114/codealpaca.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä»£ç &lt;/td&gt; &#xA;   &lt;td&gt;32K, å››ç§ä¸åŒç±»å‹ã€ä¸åŒéš¾åº¦çš„ä»£ç ç›¸å…³ä¸­æ–‡å¯¹è¯æ•°æ®ï¼Œæœ‰å¤§æ¨¡å‹ç”Ÿæˆï¼Œ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zxx000728/CodeGPT&#34;&gt;https://github.com/zxx000728/CodeGPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¯¹è¯&lt;/td&gt; &#xA;   &lt;td&gt;LAION ç­–åˆ’çš„å¼€æ”¾æŒ‡ä»¤é€šç”¨æ•°æ®é›†ä¸­æ‰‹åŠ¨é€‰æ‹©çš„ç»„ä»¶å­é›† å·²å¼€æº40M 3ä¸‡ä¸ª,100Måœ¨è·¯ä¸Š&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LAION-AI/Open-Instruction-Generalist&#34;&gt;https://github.com/LAION-AI/Open-Instruction-Generalist&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¯¹è¯&lt;/td&gt; &#xA;   &lt;td&gt;BaizeåŸºäºChat GPTæ„å»ºçš„self-chatæ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/project-baize/baize-chatbot/tree/main/data&#34;&gt;https://github.com/project-baize/baize-chatbot/tree/main/data&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¯¹è¯&lt;/td&gt; &#xA;   &lt;td&gt;FaceBookå¼€æºBlenderBotè®­ç»ƒå¯¹è¯æ•°æ®~6K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/blended_skill_talk&#34;&gt;https://huggingface.co/datasets/blended_skill_talk&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¯¹è¯&lt;/td&gt; &#xA;   &lt;td&gt;AllenAIå¼€æº38.5ä¸‡ä¸ªå¯¹è¯é«˜è´¨é‡æ•°æ®é›†SODA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://realtoxicityprompts.apps.allenai.org/&#34;&gt;https://realtoxicityprompts.apps.allenai.org/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¯¹è¯&lt;/td&gt; &#xA;   &lt;td&gt;InstructDialåœ¨å•ä¸€å¯¹è¯ä»»åŠ¡ç±»å‹ä¸Šè¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/prakharguptaz/Instructdial&#34;&gt;https://github.com/prakharguptaz/Instructdial&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¯¹è¯&lt;/td&gt; &#xA;   &lt;td&gt;Ultra Chat ä¸¤ä¸ªç‹¬ç«‹çš„ ChatGPT Turbo API è¿›è¡Œå¯¹è¯ï¼Œä»è€Œç”Ÿæˆå¤šè½®å¯¹è¯æ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;https://github.com/thunlp/UltraChat&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¯¹è¯&lt;/td&gt; &#xA;   &lt;td&gt;Awesome Open-domain Dialogue Modelsæä¾›å¤šä¸ªå¼€æ”¾åŸŸå¯¹è¯æ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cingtiye/Awesome-Open-domain-Dialogue-Models#%E4%B8%AD%E6%96%87%E5%BC%80%E6%94%BE%E5%9F%9F%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86&#34;&gt;https://github.com/cingtiye/Awesome-Open-domain-Dialogue-Models#%E4%B8%AD%E6%96%87%E5%BC%80%E6%94%BE%E5%9F%9F%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¯¹è¯&lt;/td&gt; &#xA;   &lt;td&gt;Salesforceå¼€æºè¶…å…¨DialogStudio&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/salesforce/DialogStudio&#34;&gt;https://github.com/salesforce/DialogStudio&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¯¹è¯&lt;/td&gt; &#xA;   &lt;td&gt;åŸºäºäº‹å®Referenceçš„å¤šè½®é—®ç­”ä¸­æ–‡æ•°æ®ï¼Œå·²å¼€æº5ä¸‡ï¼Œä¹‹åä¼šå¼€æºæ›´å¤š&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sufengniu/RefGPT&#34;&gt;https://github.com/sufengniu/RefGPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLFH&lt;/td&gt; &#xA;   &lt;td&gt;åŒ—å¤§æ²³ç‹¸å¼€æºRLHFæ•°æ®é›†10Kï¼Œ1Méœ€è¦ç”³è¯·&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K&#34;&gt;https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;Anthropic hh-rlhfæ•°æ®é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf&#34;&gt;https://huggingface.co/datasets/Anthropic/hh-rlhf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;Stack-exchangeä¸Šé—®é¢˜å¯¹åº”å¤šä¸ªç­”æ¡ˆï¼Œæ¯ä¸ªç­”æ¡ˆæœ‰æ‰“åˆ†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences/tree/main&#34;&gt;https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;Facebook Bot Adversarial Dialoguesæ•°æ®é›†5K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI&#34;&gt;https://github.com/facebookresearch/ParlAI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;AllenAI Real Toxicity prompts&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI&#34;&gt;https://github.com/facebookresearch/ParlAI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;OpenAssistant Conversations 160Kæ¶ˆæ¯ï¼Œ13500äººå·¥ç”Ÿæˆ, è‹±æ–‡ä¸ºä¸»&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;https://huggingface.co/datasets/OpenAssistant/oasst1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;çŸ¥ä¹é—®ç­”åå¥½æ•°æ®é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k&#34;&gt;https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;hh-rlhfä¸­æ–‡ç¿»è¯‘åå¥½æ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liswei/rm-static-zhTW&#34;&gt;https://huggingface.co/datasets/liswei/rm-static-zhTW&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è¯„ä¼°é›†&lt;/td&gt; &#xA;   &lt;td&gt;BigBench(Beyond the Imitation Game Benchmark)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google/BIG-bench&#34;&gt;https://github.com/google/BIG-bench&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è¯„ä¼°é›†&lt;/td&gt; &#xA;   &lt;td&gt;Complex QAï¼šç”¨äºChatGPTçš„è¯„æµ‹æŒ‡ä»¤é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT&#34;&gt;https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è¯„ä¼°é›†&lt;/td&gt; &#xA;   &lt;td&gt;Langchainå¼€æºè¯„ä¼°æ•°æ®é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LangChainDatasets&#34;&gt;https://huggingface.co/LangChainDatasets&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è¯„ä¼°é›†&lt;/td&gt; &#xA;   &lt;td&gt;2010-2022å¹´å…¨å›½é«˜è€ƒå·çš„é¢˜ç›®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenLMLab/GAOKAO-Bench&#34;&gt;https://github.com/OpenLMLab/GAOKAO-Bench&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è¯„ä¼°é›†&lt;/td&gt; &#xA;   &lt;td&gt;ä¸­æ–‡é€šç”¨å¤§æ¨¡å‹ç»¼åˆæ€§è¯„æµ‹åŸºå‡†SuperCLUE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CLUEbenchmark/SuperCLUE&#34;&gt;https://github.com/CLUEbenchmark/SuperCLUE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è‹±æ–‡é¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;RedPajamaå¼€æºçš„å¤åˆ»llamaçš„é¢„è®­ç»ƒæ•°æ®é›†ï¼Œ1.21ä¸‡äº¿Token&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/togethercomputer/RedPajama-Data&#34;&gt;https://github.com/togethercomputer/RedPajama-Data&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è‹±æ–‡é¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;CerebrasåŸºäºRedPajamaè¿›è¡Œæ¸…æ´—å»é‡åå¾—åˆ°çš„é«˜è´¨é‡æ•°æ®é›†, 6270äº¿Token&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/cerebras/SlimPajama-627B/tree/main/train&#34;&gt;https://huggingface.co/datasets/cerebras/SlimPajama-627B/tree/main/train&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;è‹±æ–‡é¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;Pile 22ä¸ªé«˜è´¨é‡æ•°æ®é›†æ··åˆçš„é¢„è®­ç»ƒæ•°æ®é›†800G,å…¨é‡å¼€æ”¾ä¸‹è½½&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;https://pile.eleuther.ai/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é€šç”¨é¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;UERæ•´ç†CLUECorpusSmall+News Commentaryä¸­è‹±&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dbiir/UER-py/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE&#34;&gt;https://github.com/dbiir/UER-py/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä¸­æ–‡é¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;æ™ºæºäººå·¥æ™ºèƒ½å¼€æºçš„wudao 200Gé¢„è®­ç»ƒæ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openi.pcl.ac.cn/BAAI/WuDao-Data&#34;&gt;https://github.com/BAAI-WuDao/WuDaoMM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä¸­æ–‡é¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;é‡Œå±‹ç¤¾åŒºå‘èµ·å¼€æºåŠ›é‡æ”¶é›†ä¸­æ–‡äº’è”ç½‘è¯­æ–™é›†MNBVCç›®æ ‡æ˜¯å¯¹æ ‡ChatGPTçš„40T&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/esbatmop/MNBVC&#34;&gt;https://github.com/esbatmop/MNBVC&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä¸­æ–‡é¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;å¤æ—¦å¼€æº15ä¸‡ä¸­æ–‡å›¾ä¹¦ä¸‹è½½å’ŒæŠ½å–æ–¹æ¡ˆ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FudanNLPLAB/CBook-150K&#34;&gt;https://github.com/FudanNLPLAB/CBook-150K&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ä¸­æ–‡é¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;ä¹¦ç”Ÿä¸‡å·æ•°æ®é›†æ¥è‡ªå…¬å¼€ç½‘é¡µå¤šæ¨¡æ€æ•°æ®é›†ï¼ŒåŒ…æ‹¬æ–‡æœ¬ï¼Œå›¾æ–‡å’Œè§†é¢‘ï¼Œå…¶ä¸­æ–‡æœ¬1Tï¼Œå›¾æ–‡150G&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/OpenDataLab/WanJuan1_dot_0&#34;&gt;https://opendatalab.org.cn/OpenDataLab/WanJuan1_dot_0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;é¢†åŸŸé¢„è®­ç»ƒ&lt;/td&gt; &#xA;   &lt;td&gt;é¦–ä¸ªä¸­æ–‡ç§‘å­¦æ–‡çŒ®æ•°æ®é›†CSL,ä¹Ÿæœ‰å¤šç§NLPä»»åŠ¡æ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ydli-ai/CSL&#34;&gt;https://github.com/ydli-ai/CSL&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¹³è¡Œè¯­æ–™&lt;/td&gt; &#xA;   &lt;td&gt;news-commentaryä¸­è‹±å¹³è¡Œè¯­æ–™ï¼Œç”¨äºä¸­è‹±é—´çŸ¥è¯†è¿ç§»&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://data.statmt.org/news-commentary/v15/training/&#34;&gt;https://data.statmt.org/news-commentary/v15/training/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;å¤šæºæ•°æ®é›†æ•´åˆ&lt;/td&gt; &#xA;   &lt;td&gt;opendatalabæ•´åˆäº†é¢„è®­ç»ƒé˜¶æ®µçš„å¤šä¸ªæ•°æ®æº&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/?industry=9821&amp;amp;source=JUU3JTlGJUE1JUU0JUI5JThF&#34;&gt;https://opendatalab.org.cn/?industry=9821&amp;amp;source=JUU3JTlGJUE1JUU0JUI5JThF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tool-æœç´¢å¢å¼º&lt;/td&gt; &#xA;   &lt;td&gt;webCPMå¼€æºçš„å’Œæœç´¢å·¥å…·è¿›è¡Œäº¤äº’é—®ç­”çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç½‘é¡µæŠ½å–å¼æ‘˜è¦ï¼Œå¤šäº‹å®å†…å®¹å›ç­”ç­‰äººå·¥æ ‡æ³¨æ•°æ®&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/WebCPM&#34;&gt;https://github.com/thunlp/WebCPM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tool-å¤šå·¥å…·&lt;/td&gt; &#xA;   &lt;td&gt;BmToolså¼€æºçš„å¤šå·¥å…·è°ƒç”¨æŒ‡ä»¤æ•°æ®é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/BMTools&#34;&gt;https://github.com/OpenBMB/BMTools&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NL2SQL&lt;/td&gt; &#xA;   &lt;td&gt;DB-GPT-Hubæ¢³ç†äº†å¤šæºtext-to-sqlæ•°æ®é›†&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/eosphoros-ai/DB-GPT-Hub&#34;&gt;https://github.com/eosphoros-ai/DB-GPT-Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;AIGC&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nexus.snikpic.io/&#34;&gt;NexusGPT&lt;/a&gt;: &lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt;: AutoGPTå¯ä»¥å‡ºæ¥å·¥ä½œäº†ï¼Œç¬¬ä¸€ä¸ªå…¨AI Freelanceå¹³å°&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cognosys.ai/create&#34;&gt;cognosys&lt;/a&gt;: å…¨ç½‘æœ€ç«çš„webç«¯AutoGPTï¼Œä¸è¿‡å’‹è¯´å‘¢è¯•ç”¨äº†ä¸‹æ„Ÿè§‰ä¸‹å·´è¦ç¬‘æ‰äº†ï¼Œä¸å‰§é€å»è¯•è¯•ä½ å°±çŸ¥é“ &lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://godmode.space/&#34;&gt;godmode&lt;/a&gt;ï¼šå¯ä»¥è¿›è¡Œäººä¸ºæ¯ä¸€æ­¥äº¤äº’çš„çš„AutoGPT&lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://agentgpt.reworkd.ai/&#34;&gt;agentgpt&lt;/a&gt;: åŸºç¡€ç‰ˆAutoGPT&lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.doanythingmachine.com/&#34;&gt;do Anything&lt;/a&gt;: AutoGPT Likeçš„to Do Listç”Ÿæˆå™¨ &lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chatmind.tech/&#34;&gt;ChatMind&lt;/a&gt;: chatgptç”Ÿæˆæ€ç»´å¯¼å›¾ï¼Œæ¨¡æ¿å¾ˆä¸°å¯Œï¼Œæ³›åŒ–æ€§ä¹Ÿä¸é”™ï¼Œå·²ç»è¢«XMindæ”¶è´­äº†~ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bing.com/&#34;&gt;New Bing&lt;/a&gt;ï¼šéœ€è¦è¿å¤–ç½‘å¦åˆ™ä¼šé‡å®šå‘åˆ°bingä¸­å›½ï¼Œéœ€è¦ç”³è¯·waitlist &lt;img src=&#34;https://img.shields.io/badge/AIGC-Search-yellow&#34; alt=&#34;&#34;&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.perplexity.ai/&#34;&gt;Perplexity.ai&lt;/a&gt;: åŒæ ·éœ€è¦ç§‘å­¦ä¸Šç½‘ï¼Œæ„Ÿè§‰æ¯”Bingåšçš„æ›´å¥½çš„æ¥å…¥ChatGPTçš„ç¥å¥‡æœç´¢å¼•æ“ï¼Œåœ¨Bingä¹‹å¤–è¿˜åŠ å…¥äº†ç›¸å…³æ¨èå’Œè¿½é—® &lt;img src=&#34;https://img.shields.io/badge/AIGC-Search-yellow&#34; alt=&#34;&#34;&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT&#34;&gt;BingGPT&lt;/a&gt;: NewBingå¼€æºæ¡Œé¢å®¢æˆ·ç«¯ï¼Œå¯ä»¥å°†èŠå¤©è®°å½•å¯¼å‡º &lt;img src=&#34;https://img.shields.io/badge/AIGC-Search-yellow&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/refuel-ai/autolabel&#34;&gt;AutoLabel&lt;/a&gt;: AutoLabelæ ‡æ³¨æ–¹æ¡ˆ &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/arc53/DocsGPT&#34;&gt;DocsGPT&lt;/a&gt;: æŠŠChatGPTå¼€æ”¾åŸŸé—®ç­”è½¬åŒ–æˆå°é—­åŸŸé—®ç­”çš„é€šç”¨æ–¹æ¡ˆï¼Œè¯•ç”¨å‚ç±»é¢†åŸŸé—®ç­”åœºæ™¯,å¯ä»¥è¯•ç”¨å®šåˆ¶çš„ChatBot &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/imClumsyPanda/langchain-ChatGLM&#34;&gt;langchain-ChatGLM&lt;/a&gt;: åŸºäºChatGLMçš„æœ¬åœ°çŸ¥è¯†é—®ç­”ï¼Œå’Œä¸Šé¢çš„DocsGPTç›¸ä¼¼ï¼Œä¸è¿‡å¯ä»¥æœ¬åœ°éƒ¨ç½²&lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chat2doc.cn/&#34;&gt;ChatPDF&lt;/a&gt;: å›½å†…çš„ChatPDF, ä¸Šä¼ pdfåï¼Œä¼šç»™å‡ºæ–‡ç« çš„Top5å¯èƒ½é—®é¢˜ï¼Œç„¶åå¯¹è¯å¼ä»æ–‡æ¡£ä¸­è¿›è¡Œé—®ç­”å’Œæ£€ç´¢ï¼Œ10sè¯»3ä¸‡å­— &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chatdoc.com/?viaurl=ainavpro.com&#34;&gt;ChatDoc&lt;/a&gt;:ChatPDFå‡çº§ç‰ˆï¼Œå¢åŠ äº†è¡¨æ ¼ç±»è§£æï¼Œå’Œå®Œå–„çš„ç´¢å¼•å¼•ç”¨åŠ è·³è½¬åŠ å¯¹åº”æ–‡ç« å†…å®¹é«˜äº®ï¼Œå“ˆå“ˆæˆ‘å‡†å¤‡è‡ªå·±æ•´ä¸€ä¸ª &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kaixindelele/ChatPaper&#34;&gt;ChatPaper&lt;/a&gt;: æ ¹æ®è¾“å…¥å…³é”®è¯ï¼Œè‡ªåŠ¨åœ¨arxivä¸Šä¸‹è½½æœ€æ–°çš„è®ºæ–‡ï¼Œå¹¶å¯¹è®ºæ–‡è¿›è¡Œæ‘˜è¦æ€»ç»“ï¼Œå¯ä»¥åœ¨huggingfaceä¸Šè¯•ç”¨ï¼ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.openread.academy/home&#34;&gt;OpenRead&lt;/a&gt;: é¢å‘è®ºæ–‡å†™ä½œï¼Œé˜…è¯»åœºæ™¯ï¼Œå¯ä»¥å¸®åŠ©ç”Ÿæˆæ–‡çŒ®ç»¼è¿°ï¼Œä»¥åŠæä¾›å’ŒNotionAIç›¸ä¼¼çš„æ™ºèƒ½Markdownç”¨äºå†™ä½œ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mukulpatnaik/researchgpt&#34;&gt;researchgpt&lt;/a&gt;: å’ŒChatPDFç±»ä¼¼ï¼Œæ”¯æŒarivxè®ºæ–‡ä¸‹è½½ï¼ŒåŠ è½½åå¯¹è¯å¼è·å–è®ºæ–‡é‡ç‚¹ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://briefgpt.xyz/?viaurl=ainavpro.com&#34;&gt;BriefGPT&lt;/a&gt;: æ—¥æ›´Arxivè®ºæ–‡ï¼Œå¹¶å¯¹è®ºæ–‡è¿›è¡Œæ‘˜è¦ï¼Œå…³é”®è¯æŠ½å–ï¼Œå¸®åŠ©ç ”ç©¶è€…äº†è§£æœ€æ–°åŠ¨æ€, UIä¸é”™å“Ÿ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/binary-husky/chatgpt_academic&#34;&gt;ChatGPT-academic&lt;/a&gt;: åˆæ˜¯ä¸€ä¸ªåŸºäºgradioå®ç°çš„paperæ¶¦è‰²ï¼Œæ‘˜è¦ç­‰åŠŸèƒ½æ‰“åŒ…çš„å®ç° &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Leizhenpeng/feishu-chatgpt&#34;&gt;feishu-chatgpt&lt;/a&gt;: é£ä¹¦chatgptï¼Œå’Œ365copilotç›¸ä¼¼ä¹Ÿæ˜¯å¤šç»„ä»¶é›†æˆ, æœ‰ç‚¹å…¨ï¼ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ai-topia.com/&#34;&gt;AI Topiah&lt;/a&gt;: è†å¿ƒæ™ºèƒ½AIè§’è‰²èŠå¤©ï¼Œå’Œè·¯é£å” äº†ä¸¤å¥ï¼Œå¤šå°‘æœ‰ç‚¹ä¸­äºŒä¹‹é­‚åœ¨ç‡ƒçƒ§ &lt;img src=&#34;https://img.shields.io/badge/AIGC-Chatbot-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chatbase.co/&#34;&gt;chatbase&lt;/a&gt;: æƒ…æ„Ÿè§’è‰²èŠå¤©ï¼Œè¿˜æ²¡å°è¯• &lt;img src=&#34;https://img.shields.io/badge/AIGC-Chatbot-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gptme.vana.com/login&#34;&gt;Vana&lt;/a&gt;: virtual DNA, é€šè¿‡èŠå¤©åˆ›å»ºè™šæ‹Ÿè‡ªå·±ï¼æ¦‚å¿µå¾ˆç‚« &lt;img src=&#34;https://img.shields.io/badge/AIGC-Chatbot-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.writesonic.com/&#34;&gt;WriteSonic&lt;/a&gt;ï¼šAIå†™ä½œï¼Œæ”¯æŒå¯¹è¯å’Œå®šå‘åˆ›ä½œå¦‚å¹¿å‘Šæ–‡æ¡ˆï¼Œå•†å“æè¿°, æ”¯æŒWebæ£€ç´¢æ˜¯äº®ç‚¹ï¼Œæ”¯æŒä¸­æ–‡ &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.copy.ai/&#34;&gt;copy.ai&lt;/a&gt;: WriteSonicç«å“ï¼Œäº®ç‚¹æ˜¯åƒè®ºæ–‡å¼•ç”¨ä¸€æ ·æ¯å¥è¯éƒ½æœ‰å¯¹åº”ç½‘ç«™é“¾æ¥ï¼Œå¯ä»¥ä¸€é”®å¤åˆ¶åˆ°å³è¾¹çš„åˆ›ä½œMarkdownï¼Œè¶…çº§å¥½ç”¨ï¼ &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.notion.so/product?fredir=1&#34;&gt;NotionAI&lt;/a&gt;ï¼šæ™ºèƒ½Markdownï¼Œé€‚ç”¨çœŸç›¸ï¼åœ¨åˆ›ä½œä¸­ç”¨commandè°ƒç”¨AIè¾…åŠ©æ¶¦è‰²ï¼Œæ‰©å†™ï¼Œæ£€ç´¢å†…å®¹ï¼Œç»™åˆ›æ„idea &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jasper.ai/&#34;&gt;Jasper&lt;/a&gt;: åŒä¸Šï¼Œå…¨æ˜¯ç«å“å“ˆå“ˆ &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://copyai.cn/&#34;&gt;copy.down&lt;/a&gt;: ä¸­æ–‡çš„è¥é”€æ–‡æ¡ˆç”Ÿæˆï¼Œåªèƒ½å®šå‘åˆ›ä½œï¼Œæ”¯æŒå…³é”®è¯åˆ°æ–‡æ¡ˆçš„ç”Ÿæˆ &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chatexcel.com/convert&#34;&gt;ChatExcel&lt;/a&gt;: æŒ‡ä»¤æ§åˆ¶excelè®¡ç®—ï¼Œå¯¹ç†Ÿæ‚‰excelçš„æœ‰äº›é¸¡è‚‹ï¼Œå¯¹ä¸ç†Ÿæ‚‰çš„æœ‰ç‚¹ç”¨ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/williamfzc/chat-gpt-ppt&#34;&gt;ChatPPT&lt;/a&gt;: ä½¿ç”¨ChatGPTè¿›è¡ŒPPTåˆ¶ä½œ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JimmyLv/BibiGPT&#34;&gt;BibiGPT&lt;/a&gt;: Bilibliè§†é¢‘å†…å®¹ä¸€é”®æ€»ç»“ï¼Œå¤šæ¨¡æ€æ–‡æ¡£ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/features/copilot&#34;&gt;Copilot&lt;/a&gt;: è¦ä»˜è´¹å“Ÿ &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fauxpilot/fauxpilot&#34;&gt;Fauxpilot&lt;/a&gt;: copilotæœ¬åœ°å¼€æºæ›¿ä»£ &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://codegeex.cn/zh-CN&#34;&gt;CodeGex&lt;/a&gt;: å›½å†…æ›¿ä»£å“ï¼Œè¿˜æ²¡è¯•è¿‡ &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codeium.com/&#34;&gt;Codeium&lt;/a&gt;: Copilotæ›¿ä»£å“ï¼Œæœ‰å…è´¹ç‰ˆæœ¬æ”¯æŒå„ç§plugin &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sqltranslate.app/&#34;&gt;sql translate&lt;/a&gt;: text2sqlï¼Œåˆ©ç”¨ OpenAI çš„ API å®ç°çš„ä¸€ä¸ªå¾ˆç®€å•çš„å·¥å…·ï¼Œsqlåˆ°æ–‡å­—ï¼Œæ–‡å­—åˆ°sql &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ai2sql.io/&#34;&gt;ai2sql&lt;/a&gt;: text2sqlè€ç‰Œå…¬å¸ï¼Œç›¸æ¯”sqltranslateåŠŸèƒ½æ›´å…¨é¢ï¼Œæ”¯æŒSQL è¯­æ³•æ£€æŸ¥ã€æ ¼å¼åŒ–å’Œç”Ÿæˆå…¬å¼ &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pingcap.com/chat2query-an-innovative-ai-powered-sql-generator-for-faster-insights/&#34;&gt;chat2query&lt;/a&gt;: text2sql ç›¸æ¯”ä»¥ä¸Šä¸¤ä½æ”¯æŒæ›´è‡ªç„¶çš„æ–‡æœ¬æŒ‡ä»¤ï¼Œä»¥åŠæ›´å¤æ‚çš„æ•°æ®åˆ†æç±»çš„sqlç”Ÿæˆ &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://outerbase.com/&#34;&gt;OuterBase&lt;/a&gt;: text2sql è®¾è®¡é£æ ¼å¾ˆå¸ç›ï¼ç”µå­è¡¨æ ¼ç»“åˆmysqlå’Œdashboardï¼Œæ›´é€‚åˆæ•°æ®åˆ†æå®å® &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/biobootloader/wolverine&#34;&gt;Wolverine&lt;/a&gt;: ä»£ç è‡ªæˆ‘debugçš„pythonè„šæœ¬ &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://beta.dreamstudio.ai/dream&#34;&gt;dreamstudio.ai&lt;/a&gt;: å¼€åˆ›è€…ï¼ŒStable Difussionï¼Œ æœ‰è¯•ç”¨quota &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F&#34;&gt;midjourney&lt;/a&gt;: å¼€åˆ›è€…ï¼Œè‰ºæœ¯é£æ ¼ä¸ºä¸» &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/product/dall-e-2&#34;&gt;Dall.E&lt;/a&gt;: ä¸‰å·¨å¤´è¿™å°±å‡‘é½äº† &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/hysts/ControlNet&#34;&gt;ControlNet&lt;/a&gt;: ä¸ºç»˜ç”»åˆ›ä½œåŠ æŒå¯æ§æ€§ &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Nutlope/restorePhotos&#34;&gt;GFPGAN&lt;/a&gt;: ç…§ç‰‡ä¿®å¤ &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/microsoft/visual_chatgpt&#34;&gt;Visual ChatGPT&lt;/a&gt;: å¾®è½¯å‘å¸ƒå›¾åƒChatGPTï¼Œå¯¹è¯æ–¹å¼è¿›è¡Œå›¾åƒç”Ÿæˆç¼–è¾‘ï¼Œé—®ç­” &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.genmo.ai/&#34;&gt;gemo.ai&lt;/a&gt;: å¤šæ¨¡æ€èŠå¤©æœºå™¨äººï¼ŒåŒ…æ‹¬æ–‡æœ¬ï¼Œå›¾åƒï¼Œè§†é¢‘ç”Ÿæˆ&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://storybird.com/&#34;&gt;storybird&lt;/a&gt;: æ ¹æ®æç¤ºè¯ç”Ÿæˆæ•…äº‹ç»˜æœ¬ï¼Œè¿˜å¯ä»¥å”®å–&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;h3&gt;æ•™ç¨‹ç±»&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook&#34;&gt;OpenAI Cookbook&lt;/a&gt;: æä¾›OpenAIæ¨¡å‹ä½¿ç”¨ç¤ºä¾‹ &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/riba2534/openai-scf-goproxy&#34;&gt;OpenAI æ¥å£è¢«å¢™è§£å†³åŠæ³•&lt;/a&gt;: ä½¿ç”¨è…¾è®¯äº‘æ­å»ºä»£ç†ï¼Œäº²æµ‹éå¸¸å¥½ç”¨ä¸”æ‰‹æ®‹å…šä¹Ÿå¯ä»¥è½»æ¾ä¸Šæ‰‹&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://promptperfect.jinaai.cn/&#34;&gt;PromptPerfect&lt;/a&gt;:ç”¨é­”æ³•æ‰“è´¥é­”æ³•ï¼Œè¾“å…¥åŸå§‹æç¤ºè¯ï¼Œæ¨¡å‹è¿›è¡Œå®šå‘ä¼˜åŒ–ï¼Œè¯•ç”¨åæˆ‘æœ‰ç‚¹æ²‰é»˜äº†ï¼Œå¯ä»¥å®šå‘æ”¯æŒä¸åŒä½¿ç”¨promptçš„æ¨¡å‹å¦‚Difussionï¼ŒChatGPTï¼Œ Dalleç­‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.clickprompt.org/zh-CN/&#34;&gt;ClickPrompt&lt;/a&gt;: ä¸ºå„ç§promptåŠ æŒçš„å·¥å…·ç”ŸæˆæŒ‡ä»¤åŒ…æ‹¬Difussionï¼Œchatgptdeng, éœ€è¦OpenAI Key&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newzone.top/chatgpt/&#34;&gt;ChatGPT ShortCut&lt;/a&gt;ï¼šæä¾›å„å¼åœºæ™¯ä¸‹çš„PromptèŒƒä¾‹ï¼ŒèŒƒä¾‹å¾ˆå…¨ï¼Œä½¿ç”¨åå¯ä»¥ç‚¹èµï¼ &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://enchanting-trader-463.notion.site/Full-ChatGPT-Prompts-Resources-8aa78bb226b7467ab59b70d2b27042e9&#34;&gt;Full ChatGPT Prompts + Resources&lt;/a&gt;: å„ç§å°å°½çš„promptèŒƒä¾‹ï¼Œå’Œä»¥ä¸Šåœºæ™¯æœ‰æ‰€ä¸åŒ&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learnprompting.org/&#34;&gt;learning Prompt&lt;/a&gt;: prompt engineeringè¶…å…¨æ•™ç¨‹ï¼Œå’Œè½åœ°åº”ç”¨æ”¶è—ï¼ŒåŒ…æ‹¬å¾ˆå¤šLLMè°ƒç”¨Agentçš„é«˜çº§åœºæ™¯ &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ORDINAND/The-Art-of-Asking-ChatGPT-for-High-Quality-Answers-A-complete-Guide-to-Prompt-Engineering-Technique&#34;&gt;The art of asking chatgpt for high quality answers&lt;/a&gt;: å¦‚ä½•å†™PromptæŒ‡ä»¤å‡ºä¹¦äº†ï¼Œé“¾æ¥æ˜¯ä¸­æ–‡ç¿»è¯‘çš„ç‰ˆæœ¬ï¼Œæ¯”è¾ƒååŸºç¡€ä½¿ç”¨&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dair-ai/Prompt-Engineering-Guide&#34;&gt;Prompt-Engineer-Guide&lt;/a&gt;: åŒlearnig promptç±»çš„é›†æˆæ•™ç¨‹ï¼Œäº’ç›¸å¼•ç”¨å¯è¿˜è¡Œï¼Ÿï¼åˆ†ç±»ç´¢å¼•åšçš„æ›´å¥½äº› &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mojidoc.com/05z7y-dd5pa7hu3zfmhnbngoeztyqcnq-00b&#34;&gt;OpenAI åº”ç”¨æ±‡æ€»æŒ‡å—&lt;/a&gt;: çº¯åº”ç”¨ç±»çš„æ±‡æ€»æŒ‡å—&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ainavpro.com/#term-209&#34;&gt;AI å¯¼èˆª&lt;/a&gt;: åŒ…æ‹¬ä½†ä¸é™äºChatGPTçš„åº”ç”¨æ±‡æ€»ç½‘ç«™ï¼Œæ›´æ–°å¾ˆå¿«ï¼Œå‘ç°äº†ä¸€äº›æ–°å¤§é™†&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/&#34;&gt;AI Alignment Forum&lt;/a&gt;: RLHFç­‰å¯¹é½ç›¸å…³æœ€æ–°è®ºæ–‡å’Œè§‚ç‚¹çš„è®¨è®ºè®ºå›&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/&#34;&gt;Langchain: Chat with your data&lt;/a&gt;:å´æ©è¾¾LLMå®è·µè¯¾ç¨‹&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/phodal/aigc&#34;&gt;æ„ç­‘å¤§è¯­è¨€æ¨¡å‹åº”ç”¨ï¼šåº”ç”¨å¼€å‘ä¸æ¶æ„è®¾è®¡&lt;/a&gt;: ä¸€æœ¬å…³äº LLM åœ¨çœŸå®ä¸–ç•Œåº”ç”¨çš„å¼€æºç”µå­ä¹¦&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks-academy/large-language-models&#34;&gt;Large Language Models: Application through Production&lt;/a&gt;: å¤§æ¨¡å‹åº”ç”¨Edxå‡ºå“çš„è¯¾ç¨‹&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ä¹¦ç±åšå®¢ç±»&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/chatgpt/&#34;&gt;OpenAI ChatGPT Intro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/instruction-following/&#34;&gt;OpenAI InstructGPT intro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AllenAI ChatGPTèƒ½åŠ›è§£è¯»ï¼š&lt;a href=&#34;https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1&#34;&gt;How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources&lt;/a&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Huggingface ChatGPTèƒ½åŠ›è§£è¯»ï¼š&lt;a href=&#34;https://huggingface.co/blog/dialog-agents&#34;&gt;The techniques behind ChatGPT: RLHF, IFT, CoT, Red teaming, and more&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stephen Wolfram ChatGPTèƒ½åŠ›è§£è¯»: &lt;a href=&#34;https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/&#34;&gt;What Is ChatGPT Doing and Why Does It Work?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chenweiphd/ChatGPT-Hub&#34;&gt;Chatgptç›¸å…³è§£è¯»æ±‡æ€»&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/&#34;&gt;éº»çœç†å·¥ç§‘æŠ€é‡‡è®¿OpenAIå·¥ç¨‹å¸ˆ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jiqizhixin.com/articles/2018-11-15-6?from=timeline&#34;&gt;AGIå†å²ä¸ç°çŠ¶&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/597586623&#34;&gt;å¼ ä¿Šæ— é€šå‘AGIä¹‹è·¯ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æŠ€æœ¯ç²¾è¦&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/589639535/answer/2936696161&#34;&gt;çŸ¥ä¹å›ç­” OpenAI å‘å¸ƒ GPT-4ï¼Œæœ‰å“ªäº›æŠ€æœ¯ä¸Šçš„ä¼˜åŒ–æˆ–çªç ´?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/609877277&#34;&gt;è¿½èµ¶ChatGPTçš„éš¾ç‚¹ä¸å¹³æ›¿&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/615554635&#34;&gt;å‹ç¼©å³æ³›åŒ–ï¼Œæ³›åŒ–å³æ™ºèƒ½&lt;/a&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://new.qq.com/rain/a/20230423A08J7400&#34;&gt;é™†å¥‡æœ€æ–°æ¼”è®²å®å½•ï¼šæˆ‘çš„å¤§æ¨¡å‹ä¸–ç•Œè§‚ï½œç¬¬åå››æœŸ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2023-06-23-agent/&#34;&gt;LLM Powered Autonomous Agents&lt;/a&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac&#34;&gt;All You Need to Know to Build Your First LLM App&lt;/a&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.semianalysis.com/p/gpt-4-architecture-infrastructure&#34;&gt;GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://weread.qq.com/web/bookDetail/93832630811e7e827g0173ca&#34;&gt;ä¸ºä»€ä¹ˆä¼Ÿå¤§ä¸èƒ½è¢«è®¡åˆ’&lt;/a&gt;: OpenAIç ”ç©¶å‘˜å‡ºä¹¦&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MjM5ODY2OTQyNg==&amp;amp;mid=2649769138&amp;amp;idx=1&amp;amp;sn=2c408b73f66a52e43ea991b957729519&amp;amp;chksm=bec3d9af89b450b95e6432dc33f4f32ae7a29cc8e2916369aad6156c5817927d1f73a0c84e82&amp;amp;scene=21#wechat_redirect&#34;&gt;æ‹¾è±¡æŠ•ç ”æœºæ„å¯¹LLMçš„è°ƒç ”æŠ¥å‘Šï¼ˆæ–‡ä¸­æœ‰ä¸¤æ¬¡PPTçš„ç”³è¯·é“¾æ¥ï¼‰&lt;/a&gt;: å¯¹å¤§æ¨¡å‹åº”ç”¨ç»™å‡ºäº†å¾ˆå…¨é¢çš„æ€»ç»“æ¢³ç†&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.guotaixia.com/post/5336.html&#34;&gt;å¯æ˜åˆ›æŠ•State of Generative AI 2023&lt;/a&gt;: æœ€è¿‘å‘ç°åº”ç”¨è½åœ°æ‰æ˜¯LLMçœŸæ­£äº§ç”Ÿä»·å€¼çš„æ ¸å¿ƒï¼Œå¼€å§‹æ›´å¤šå…³æ³¨ä¸€äº›æŠ•ç ”çš„åˆ†ææŠ¥å‘Š&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.oneusefulthing.org/p/how-to-use-ai-to-do-stuff-an-opinionated&#34;&gt;How to Use AI to Do Stuff: An Opinionated Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.interconnects.ai/p/llama-2-from-meta&#34;&gt;Llama 2: an incredible open LLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://book.douban.com/subject/36449803/?icn=index-latestbook-subject&#34;&gt;Wolframè¯­è¨€ä¹‹çˆ¶æ–°ä¹¦ï¼šè¿™å°±æ˜¯ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pair.withgoogle.com/explorables/grokking/&#34;&gt;è°·æ­Œå‡ºå“ï¼šå¯¹å¤§æ¨¡å‹é¢†æ‚Ÿèƒ½åŠ›çš„ä¸€äº›æ¢ç´¢å¾ˆæœ‰æ„æ€ Do Machine Learning Models Memorize or Generalize?&lt;/a&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://simons.berkeley.edu/talks/ilya-sutskever-openai-2023-08-14&#34;&gt;OpenAIé¦–å¸­ç§‘å­¦å®¶æœ€æ–°è®²åº§è§£è¯»LMæ— ç›‘ç£é¢„è®­ç»ƒå­¦äº†å•¥ An observation on Generalization&lt;/a&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mattprd.com/p/the-complete-beginners-guide-to-autonomous-agents&#34;&gt;The Complete Beginners Guide To Autonomous Agents&lt;/a&gt;: Octane AIåˆ›å§‹äºº Matt Schlichtå‘è¡¨çš„å…³äºäººå·¥æ™ºèƒ½ä»£ç†çš„ä¸€äº›æ€è€ƒ&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yaofu.notion.site/An-Initial-Exploration-of-Theoretical-Support-for-Language-Model-Data-Engineering-Part-1-Pretraini-dc480d9bf7ff4659afd8c9fb738086eb&#34;&gt;An Initial Exploration of Theoretical Support for Language Model Data Engineering. Part 1: Pretraining&lt;/a&gt;: ç¬¦å°§å¤§ä½¬ç³»åˆ—æ–°ä½œï¼Œé€šè¿‡äº†è§£å¤§æ¨¡å‹èƒŒåçš„æ•°æ®å·¥ç¨‹æ¥äº†è§£æ¨¡å‹æœ¬è´¨ï¼Œç¬¬ä¸€ç¯‡é¢„è®­ç»ƒæ•°æ®&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;paper List&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dongguanting/In-Context-Learning_PaperList&#34;&gt;https://github.com/dongguanting/In-Context-Learning_PaperList&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/PromptPapers&#34;&gt;https://github.com/thunlp/PromptPapers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Timothyxxx/Chain-of-ThoughtsPapers&#34;&gt;https://github.com/Timothyxxx/Chain-of-ThoughtsPapers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/ToolLearningPapers&#34;&gt;https://github.com/thunlp/ToolLearningPapers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MLGroupJLU/LLM-eval-survey&#34;&gt;https://github.com/MLGroupJLU/LLM-eval-survey&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ç»¼è¿°&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A Survey of Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Paradigm Shift in Natural Language Processing&lt;/li&gt; &#xA; &lt;li&gt;Pre-Trained Models: Past, Present and Future&lt;/li&gt; &#xA; &lt;li&gt;What Language Model Architecture and Pretraining objects work best for zero shot generalization &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Towards Reasoning in Large Language Models: A Survey&lt;/li&gt; &#xA; &lt;li&gt;Reasoning with Language Model Prompting: A Survey &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;An Overview on Language Models: Recent Developments and Outlook &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;A Survey of Large Language Models[6.29æ›´æ–°ç‰ˆ]&lt;/li&gt; &#xA; &lt;li&gt;Unifying Large Language Models and Knowledge Graphs: A Roadmap&lt;/li&gt; &#xA; &lt;li&gt;Augmented Language Models: a Survey &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey&lt;/li&gt; &#xA; &lt;li&gt;Challenges and Applications of Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;The Rise and Potential of Large Language Model Based Agents: A Survey&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;å¤§æ¨¡å‹èƒ½åŠ›æ¢ç©¶&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In Context Learning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LARGER LANGUAGE MODELS DO IN-CONTEXT LEARNING DIFFERENTLY&lt;/li&gt; &#xA;   &lt;li&gt;How does in-context learning work? A framework for understanding the differences from traditional supervised learning&lt;/li&gt; &#xA;   &lt;li&gt;Why can GPT learn in-context? Language Model Secretly Perform Gradient Descent as Meta-Optimizers &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Rethinking the Role of Demonstrations What Makes incontext learning work? &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Trained Transformers Learn Linear Models In-Context&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;æ¶Œç°èƒ½åŠ› &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sparks of Artificial General Intelligence: Early experiments with GPT-4&lt;/li&gt; &#xA;   &lt;li&gt;Emerging Ability of Large Language Models &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;LANGUAGE MODELS REPRESENT SPACE AND TIME&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;èƒ½åŠ›è¯„ä¼° &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;IS CHATGPT A GENERAL-PURPOSE NATURAL LANGUAGE PROCESSING TASK SOLVER?&lt;/li&gt; &#xA;   &lt;li&gt;Can Large Language Models Infer Causation from Correlation?&lt;/li&gt; &#xA;   &lt;li&gt;Holistic Evaluation of Language Model&lt;/li&gt; &#xA;   &lt;li&gt;Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond&lt;/li&gt; &#xA;   &lt;li&gt;Theory of Mind May Have Spontaneously Emerged in Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Beyond The Imitation Game: Quantifying And Extrapolating The Capabilities Of Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations&lt;/li&gt; &#xA;   &lt;li&gt;Demystifying GPT Self-Repair for Code Generation&lt;/li&gt; &#xA;   &lt;li&gt;Evidence of Meaning in Language Models Trained on Programs&lt;/li&gt; &#xA;   &lt;li&gt;Can Explanations Be Useful for Calibrating Black Box Models&lt;/li&gt; &#xA;   &lt;li&gt;On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective&lt;/li&gt; &#xA;   &lt;li&gt;Language acquisition: do children and language models follow similar learning stages?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prompt TunningèŒƒå¼&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tunning Free Prompt &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GPT2: Language Models are Unsupervised Multitask Learners&lt;/li&gt; &#xA;   &lt;li&gt;GPT3: Language Models are Few-Shot Learners &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;LAMA: Language Models as Knowledge Bases?&lt;/li&gt; &#xA;   &lt;li&gt;AutoPrompt: Eliciting Knowledge from Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix-Prompt LM Tunning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/li&gt; &#xA;   &lt;li&gt;PET-TC(a): Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;PET-TC(b): PETSGLUE Itâ€™s Not Just Size That Matters Small Language Models are also few-shot learners&lt;/li&gt; &#xA;   &lt;li&gt;GenPET: Few-Shot Text Generation with Natural Language Instructions&lt;/li&gt; &#xA;   &lt;li&gt;LM-BFF: Making Pre-trained Language Models Better Few-shot Learners &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ADEPT: Improving and Simplifying Pattern Exploiting Training&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix-LM Prompt Tunning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Prefix-tuning: Optimizing continuous prompts for generation&lt;/li&gt; &#xA;   &lt;li&gt;Prompt-tunning: The power of scale for parameter-efficient prompt tuning &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;P-tunning: GPT Understands Too &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;WARP: Word-level Adversarial ReProgramming&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LM + Prompt Tunning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;P-tunning v2: Prompt Tuning Can Be Comparable to Fine-tunning Universally Across Scales and Tasks&lt;/li&gt; &#xA;   &lt;li&gt;PTR: Prompt Tuning with Rules for Text Classification&lt;/li&gt; &#xA;   &lt;li&gt;PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix-LM Adapter Tunning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning&lt;/li&gt; &#xA;   &lt;li&gt;Parameter-Efficient Transfer Learning for NLP&lt;/li&gt; &#xA;   &lt;li&gt;INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ä¸»æµLLMS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL&lt;/li&gt; &#xA; &lt;li&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/li&gt; &#xA; &lt;li&gt;PaLM: Scaling Language Modeling with Pathways&lt;/li&gt; &#xA; &lt;li&gt;PaLM 2 Technical Report&lt;/li&gt; &#xA; &lt;li&gt;GPT-4 Technical Report&lt;/li&gt; &#xA; &lt;li&gt;Backpack Language Models&lt;/li&gt; &#xA; &lt;li&gt;Llama 2: Open Foundation and Fine-Tuned Chat Models&lt;/li&gt; &#xA; &lt;li&gt;OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch&lt;/li&gt; &#xA; &lt;li&gt;Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;æŒ‡ä»¤å¾®è°ƒ&amp;amp;å¯¹é½ (instruction_tunning)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç»å…¸æ–¹æ¡ˆ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Flan: FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Flan-T5: Scaling Instruction-Finetuned Language Models&lt;/li&gt; &#xA;   &lt;li&gt;ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning&lt;/li&gt; &#xA;   &lt;li&gt;Instruct-GPT: Training language models to follow instructions with human feedback &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;T0: MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION&lt;/li&gt; &#xA;   &lt;li&gt;Natural Instructions: Cross-Task Generalization via Natural Language Crowdsourcing Instructions&lt;/li&gt; &#xA;   &lt;li&gt;Tk-INSTRUCT: SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks&lt;/li&gt; &#xA;   &lt;li&gt;ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-shot Generalization&lt;/li&gt; &#xA;   &lt;li&gt;Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor&lt;/li&gt; &#xA;   &lt;li&gt;INSTRUCTEVAL Towards Holistic Evaluation of Instrucion-Tuned Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;æ›´å°‘ï¼Œè´¨é‡æ›´é«˜ã€æ›´å¤šæ ·çš„æŒ‡ä»¤æ•°æ®å¸¦æ¥è´¨å˜ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LIMA: Less Is More for Alignment &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning&lt;/li&gt; &#xA;   &lt;li&gt;Textbooks Are All You Need &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;AlpaGasus: Training A Better Alpaca with Fewer Data&lt;/li&gt; &#xA;   &lt;li&gt;InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4&lt;/li&gt; &#xA;   &lt;li&gt;Instruction Mining: High-Quality Instruction Data Selection for Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Visual Instruction Tuning with Polite Flamingo&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;æ–°å¯¹é½/å¾®è°ƒæ–¹æ¡ˆ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WizardLM: Empowering Large Language Models to Follow Complex Instructions&lt;/li&gt; &#xA;   &lt;li&gt;Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning&lt;/li&gt; &#xA;   &lt;li&gt;Self-Alignment with Instruction Backtranslation &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks&lt;/li&gt; &#xA;   &lt;li&gt;PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions&lt;/li&gt; &#xA;   &lt;li&gt;OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs&lt;/li&gt; &#xA;   &lt;li&gt;Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision&lt;/li&gt; &#xA;   &lt;li&gt;Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;å¾®è°ƒç»éªŒ/å®éªŒæŠ¥å‘Š &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;BELLE: Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases&lt;/li&gt; &#xA;   &lt;li&gt;Baize: Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data&lt;/li&gt; &#xA;   &lt;li&gt;A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Large LM&lt;/li&gt; &#xA;   &lt;li&gt;Exploring ChatGPTâ€™s Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences&lt;/li&gt; &#xA;   &lt;li&gt;Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;å¯¹è¯æ¨¡å‹&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LaMDA: Language Models for Dialog Applications&lt;/li&gt; &#xA; &lt;li&gt;Sparrow: Improving alignment of dialogue agents via targeted human judgements &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage&lt;/li&gt; &#xA; &lt;li&gt;How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation&lt;/li&gt; &#xA; &lt;li&gt;DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI&lt;/li&gt; &#xA; &lt;li&gt;Enhancing Chat Language Models by Scaling High-quality Instructional Conversations&lt;/li&gt; &#xA; &lt;li&gt;DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;æ€ç»´é“¾ (prompt_chain_of_thought)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åŸºç¡€&amp;amp;è¿›é˜¶ç”¨æ³• &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[zero-shot-COT] Large Language Models are Zero-Shot Reasoners &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;[few-shot COT] Chain of Thought Prompting Elicits Reasoning in Large Language Models &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS&lt;/li&gt; &#xA;   &lt;li&gt;LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Tree of Thoughts: Deliberate Problem Solving with Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Decomposed Prompting A MODULAR APPROACH FOR Solving Complex Tasks&lt;/li&gt; &#xA;   &lt;li&gt;Successive Prompting for Decomposing Complex Questions&lt;/li&gt; &#xA;   &lt;li&gt;Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework&lt;/li&gt; &#xA;   &lt;li&gt;Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning&lt;/li&gt; &#xA;   &lt;li&gt;LAMBADA: Backward Chaining for Automated Reasoning in Natural Language&lt;/li&gt; &#xA;   &lt;li&gt;Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Graph of Thoughts: Solving Elaborate Problems with Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;åˆ†é¢†åŸŸCOT [Math, Code, Tabular, QA] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Solving Quantitative Reasoning Problems with Language Models&lt;/li&gt; &#xA;   &lt;li&gt;SHOW YOUR WORK: SCRATCHPADS FOR INTERMEDIATE COMPUTATION WITH LANGUAGE MODELS&lt;/li&gt; &#xA;   &lt;li&gt;Solving math word problems with processand outcome-based feedback&lt;/li&gt; &#xA;   &lt;li&gt;CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning&lt;/li&gt; &#xA;   &lt;li&gt;T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering&lt;/li&gt; &#xA;   &lt;li&gt;LEARNING PERFORMANCE-IMPROVING CODE EDITS&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning&lt;/li&gt; &#xA;   &lt;li&gt;Tab-CoT: Zero-shot Tabular Chain of Thought&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;åŸç†åˆ†æ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;TEXT AND PATTERNS: FOR EFFECTIVE CHAIN OF THOUGHT IT TAKES TWO TO TANGO&lt;/li&gt; &#xA;   &lt;li&gt;Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models Can Be Easily Distracted by Irrelevant Context&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;å°æ¨¡å‹COTè’¸é¦ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Specializing Smaller Language Models towards Multi-Step Reasoning &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Teaching Small Language Models to Reason&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models are Reasoning Teachers&lt;/li&gt; &#xA;   &lt;li&gt;Distilling Reasoning Capabilities into Smaller Language Models&lt;/li&gt; &#xA;   &lt;li&gt;The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;COTæ ·æœ¬è‡ªåŠ¨æ„å»º/é€‰æ‹© &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;STaR: Self-Taught Reasoner Bootstrapping ReasoningWith Reasoning&lt;/li&gt; &#xA;   &lt;li&gt;AutoCOTï¼šAUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models Can Self-Improve&lt;/li&gt; &#xA;   &lt;li&gt;Active Prompting with Chain-of-Thought for Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;others &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OlaGPT Empowering LLMs With Human-like Problem-Solving abilities&lt;/li&gt; &#xA;   &lt;li&gt;Challenging BIG-Bench tasks and whether chain-of-thought can solve them&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models are Better Reasoners with Self-Verification&lt;/li&gt; &#xA;   &lt;li&gt;ThoughtSource A central hub for large language model reasoning data&lt;/li&gt; &#xA;   &lt;li&gt;Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RLHF&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deepmind &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Teaching language models to support answers with verified quotes&lt;/li&gt; &#xA;   &lt;li&gt;sparrow, Improving alignment of dialogue agents via targetd human judgements &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;openai &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PPO: Proximal Policy Optimization Algorithms &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Deep Reinforcement Learning for Human Preference&lt;/li&gt; &#xA;   &lt;li&gt;Fine-Tuning Language Models from Human Preferences&lt;/li&gt; &#xA;   &lt;li&gt;learning to summarize from human feedback&lt;/li&gt; &#xA;   &lt;li&gt;InstructGPT: Training language models to follow instructions with human feedback &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Scaling Laws for Reward Model Over optimization &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Anthropic &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A General Language Assistant as a Laboratory for Alignmen&lt;/li&gt; &#xA;   &lt;li&gt;Red Teaming Language Models to Reduce Harms Methods,Scaling Behaviors and Lessons Learned&lt;/li&gt; &#xA;   &lt;li&gt;Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Constitutional AI Harmlessness from AI Feedback &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Pretraining Language Models with Human Preferences&lt;/li&gt; &#xA;   &lt;li&gt;The Capacity for Moral Self-Correction in Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;AllenAI, RL4LMï¼šIS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING BENCHMARKS&lt;/li&gt; &#xA; &lt;li&gt;æ”¹è‰¯æ–¹æ¡ˆ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RRHF: Rank Responses to Align Language Models with Human Feedback without tears&lt;/li&gt; &#xA;   &lt;li&gt;PRMï¼šLet&#39;s verify step by step&lt;/li&gt; &#xA;   &lt;li&gt;Chain of Hindsight Aligns Language Models with Feedback&lt;/li&gt; &#xA;   &lt;li&gt;AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback&lt;/li&gt; &#xA;   &lt;li&gt;Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback&lt;/li&gt; &#xA;   &lt;li&gt;RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Training Socially Aligned Language Models in Simulated Human Society&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM Agent è®©æ¨¡å‹ä½¿ç”¨å·¥å…· (llm_agent)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;åŸºäºprompté€šç”¨æ–¹æ¡ˆ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ReAct: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Self-ask: MEASURING AND NARROWING THE COMPOSITIONALITY GAP IN LANGUAGE MODELS &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;MRKL SystemsA modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning&lt;/li&gt; &#xA;   &lt;li&gt;PAL: Program-aided Language Models&lt;/li&gt; &#xA;   &lt;li&gt;ART: Automatic multi-step reasoning and tool-use for large language models&lt;/li&gt; &#xA;   &lt;li&gt;ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions&lt;/li&gt; &#xA;   &lt;li&gt;Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Faithful Chain-of-Thought Reasoning&lt;/li&gt; &#xA;   &lt;li&gt;Reflexion: Language Agents with Verbal Reinforcement Learning &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks&lt;/li&gt; &#xA;   &lt;li&gt;Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework&lt;/li&gt; &#xA;   &lt;li&gt;RestGPT: Connecting Large Language Models with Real-World RESTful APIs&lt;/li&gt; &#xA;   &lt;li&gt;ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;åŸºäºå¾®è°ƒé€šç”¨æ–¹æ¡ˆ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TALM: Tool Augmented Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Toolformer: Language Models Can Teach Themselves to Use Tools &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Tool Learning with Foundation Models&lt;/li&gt; &#xA;   &lt;li&gt;Tool Makerï¼šLarge Language Models as Tool Maker&lt;/li&gt; &#xA;   &lt;li&gt;TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;æ£€ç´¢å¢å¼ºæ–¹æ¡ˆ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WebGPTï¼šBrowser-assisted question-answering with human feedback&lt;/li&gt; &#xA;   &lt;li&gt;WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences&lt;/li&gt; &#xA;   &lt;li&gt;WebCPM: Interactive Web Search for Chinese Long-form Question Answering &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;REPLUG: Retrieval-Augmented Black-Box Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Query Rewriting for Retrieval-Augmented Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit&lt;/li&gt; &#xA;   &lt;li&gt;Atlas: Few-shot Learning with Retrieval Augmented Language Models&lt;/li&gt; &#xA;   &lt;li&gt;RRAML: Reinforced Retrieval Augmented Machine Learning&lt;/li&gt; &#xA;   &lt;li&gt;Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation&lt;/li&gt; &#xA;   &lt;li&gt;PDFTriage: Question Answering over Long, Structured Documents&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;è°ƒç”¨æ¨¡å‹æ–¹æ¡ˆ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace&lt;/li&gt; &#xA;   &lt;li&gt;Gorillaï¼šLarge Language Model Connected with Massive APIs &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;OpenAGI: When LLM Meets Domain Experts&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;å‚ç›´é¢†åŸŸ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents&lt;/li&gt; &#xA;   &lt;li&gt;ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings&lt;/li&gt; &#xA;   &lt;li&gt;ChemCrow Augmenting large language models with chemistry tools&lt;/li&gt; &#xA;   &lt;li&gt;Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow&lt;/li&gt; &#xA;   &lt;li&gt;GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information&lt;/li&gt; &#xA;   &lt;li&gt;PointLLM: Empowering Large Language Models to Understand Point Clouds&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;è¯„ä¼° &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Evaluating Verifiability in Generative Search Engines&lt;/li&gt; &#xA;   &lt;li&gt;Mind2Web: Towards a Generalist Agent for the Web&lt;/li&gt; &#xA;   &lt;li&gt;Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions&lt;/li&gt; &#xA;   &lt;li&gt;API-Bank: A Benchmark for Tool-Augmented LLMs&lt;/li&gt; &#xA;   &lt;li&gt;ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MultiAgent &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/li&gt; &#xA;   &lt;li&gt;AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents&lt;/li&gt; &#xA;   &lt;li&gt;CAMEL: Communicative Agents for &#34;Mind&#34; Exploration of Large Scale Language Model Society&lt;/li&gt; &#xA;   &lt;li&gt;Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;å…¶ä»– &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LLM+P: Empowering Large Language Models with Optimal Planning Proficiency&lt;/li&gt; &#xA;   &lt;li&gt;Inference with Reference: Lossless Acceleration of Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;RecallM: An Architecture for Temporal Context Understanding and Question Answering&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;æŒ‡ä»¤æ•°æ®ç”Ÿæˆ (instruction_data_gen)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;APE: LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;iPrompt: Explaining Data Patterns in Natural Language via Interpretable Autoprompting&lt;/li&gt; &#xA; &lt;li&gt;Flipped Learning: Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners&lt;/li&gt; &#xA; &lt;li&gt;Fairness-guided Few-shot Prompting for Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;Instruction induction: From few examples to natural language task descriptions.&lt;/li&gt; &#xA; &lt;li&gt;Baize An Open-Source Chat Model with Parameter-Efficient Tuning on self-Chat Data&lt;/li&gt; &#xA; &lt;li&gt;SELF-QA Unsupervised Knowledge Guided alignment.&lt;/li&gt; &#xA; &lt;li&gt;GPT Self-Supervision for a Better Data Annotator&lt;/li&gt; &#xA; &lt;li&gt;The Flan Collection Designing Data and Methods&lt;/li&gt; &#xA; &lt;li&gt;Self-Consuming Generative Models Go MAD&lt;/li&gt; &#xA; &lt;li&gt;InstructEval: Systematic Evaluation of Instruction Selection Methods&lt;/li&gt; &#xA; &lt;li&gt;Overwriting Pretrained Bias with Finetuning Data&lt;/li&gt; &#xA; &lt;li&gt;WizardLM: Empowering Large Language Models to Follow Complex Instructions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;é¢„è®­ç»ƒæ•°æ®(pretrain_data)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining&lt;/li&gt; &#xA; &lt;li&gt;The Pile: An 800GB Dataset of Diverse Text for Language Modeling&lt;/li&gt; &#xA; &lt;li&gt;CCNet: Extracting High Quality Monolingual Datasets fromWeb Crawl Data&lt;/li&gt; &#xA; &lt;li&gt;WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models&lt;/li&gt; &#xA; &lt;li&gt;CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;é¢†åŸŸæ¨¡å‹ (domain_llms)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MedGPT: Medical Concept Prediction from Clinical Narratives&lt;/li&gt; &#xA; &lt;li&gt;BioGPTï¼šGenerative Pre-trained Transformer for Biomedical Text Generation and Mining&lt;/li&gt; &#xA; &lt;li&gt;Galactiaï¼šA Large Language Model for Science&lt;/li&gt; &#xA; &lt;li&gt;PubMed GPT: A Domain-specific large language model for biomedical text &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;BloombergGPTï¼š A Large Language Model for Finance&lt;/li&gt; &#xA; &lt;li&gt;ChatDoctorï¼šMedical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge&lt;/li&gt; &#xA; &lt;li&gt;Med-PaLMï¼šLarge Language Models Encode Clinical Knowledge[V1,V2] &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Augmented Large Language Models with Parametric Knowledge Guiding&lt;/li&gt; &#xA; &lt;li&gt;XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters&lt;/li&gt; &#xA; &lt;li&gt;ChatLaw Open-Source Legal Large Language Model &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;MediaGPT : A Large Language Model For Chinese Media&lt;/li&gt; &#xA; &lt;li&gt;SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support&lt;/li&gt; &#xA; &lt;li&gt;KITLM: Domain-Specific Knowledge InTegration into Language Models for Question Answering&lt;/li&gt; &#xA; &lt;li&gt;FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis&lt;/li&gt; &#xA; &lt;li&gt;EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce&lt;/li&gt; &#xA; &lt;li&gt;FinGPT: Open-Source Financial Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT&lt;/li&gt; &#xA; &lt;li&gt;CFGPT: Chinese Financial Assistant with Large Language Model&lt;/li&gt; &#xA; &lt;li&gt;Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLMè¶…é•¿æ–‡æœ¬å¤„ç† (long_input)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Parallel Context Windows for Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;Structured Prompting: Scaling In-Context Learning to 1,000 Examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spaces.ac.cn/archives/9617&#34;&gt;è‹å‰‘æ—, NBCEï¼šä½¿ç”¨æœ´ç´ è´å¶æ–¯æ‰©å±•LLMçš„Contextå¤„ç†é•¿åº¦&lt;/a&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens&lt;/li&gt; &#xA; &lt;li&gt;Unlimiformer: Long-Range Transformers with Unlimited Length Input&lt;/li&gt; &#xA; &lt;li&gt;Scaling Transformer to 1M tokens and beyond with RMT&lt;/li&gt; &#xA; &lt;li&gt;RECURRENTGPT: Interactive Generation of (Arbitrarily) Long Text&lt;/li&gt; &#xA; &lt;li&gt;TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/li&gt; &#xA; &lt;li&gt;Extending Context Window of Large Language Models via Positional Interpolation&lt;/li&gt; &#xA; &lt;li&gt;LongNet: Scaling Transformers to 1,000,000,000 Tokens&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kaiokendev.github.io/til#extending-context-to-8k&#34;&gt;https://kaiokendev.github.io/til#extending-context-to-8k&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spaces.ac.cn/archives/9675&#34;&gt;è‹å‰‘æ—,Transformerå‡çº§ä¹‹è·¯ï¼š10ã€RoPEæ˜¯ä¸€ç§Î²è¿›åˆ¶ç¼–ç &lt;/a&gt; &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spaces.ac.cn/archives/9706&#34;&gt;è‹å‰‘æ—,Transformerå‡çº§ä¹‹è·¯ï¼š11ã€å°†Î²è¿›åˆ¶ä½ç½®è¿›è¡Œåˆ°åº•&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spaces.ac.cn/archives/9708&#34;&gt;è‹å‰‘æ—,Transformerå‡çº§ä¹‹è·¯ï¼š12ã€æ— é™å¤–æ¨çš„ReRoPEï¼Ÿ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Focused Transformer: Contrastive Training for Context Scaling&lt;/li&gt; &#xA; &lt;li&gt;Lost in the Middle: How Language Models Use Long Contexts &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS&lt;/li&gt; &#xA; &lt;li&gt;Ring Attention with Blockwise Transformers for Near-Infinite Context&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;NL2SQL&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å¤§æ¨¡å‹æ–¹æ¡ˆ &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;C3: Zero-shot Text-to-SQL with ChatGPT &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;SQL-PALM: IMPROVED LARGE LANGUAGE MODEL ADAPTATION FOR TEXT-TO-SQL&lt;/li&gt; &#xA;   &lt;li&gt;BIRD Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQL &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL&lt;/li&gt; &#xA;   &lt;li&gt;ChatDB: AUGMENTING LLMS WITH DATABASES AS THEIR SYMBOLIC MEMORY&lt;/li&gt; &#xA;   &lt;li&gt;A comprehensive evaluation of ChatGPTâ€™s zero-shot Text-to-SQL capability&lt;/li&gt; &#xA;   &lt;li&gt;Few-shot Text-to-SQL Translation using Structure and Content Prompt Learning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Domain Knowledge Intensive &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge&lt;/li&gt; &#xA;   &lt;li&gt;Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion&lt;/li&gt; &#xA;   &lt;li&gt;Towards Robustness of Text-to-SQL Models against Synonym Substitution&lt;/li&gt; &#xA;   &lt;li&gt;FinQA: A Dataset of Numerical Reasoning over Financial Data&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;others &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL&lt;/li&gt; &#xA;   &lt;li&gt;MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;é™ä½æ¨¡å‹å¹»è§‰ (reliability)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Survey of Hallucination in Natural Language Generation&lt;/li&gt; &#xA; &lt;li&gt;Trusting Your Evidence: Hallucinate Less with Context-aware Decoding &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;SELF-REFINE:ITERATIVE REFINEMENT WITH SELF-FEEDBACK &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;PROMPTING GPT-3 TO BE RELIABLE&lt;/li&gt; &#xA; &lt;li&gt;Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference&lt;/li&gt; &#xA; &lt;li&gt;On the Advance of Making Language Models Better Reasoners&lt;/li&gt; &#xA; &lt;li&gt;Progressive-Hint Prompting Improves Reasoning in Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;ASK ME ANYTHING: A SIMPLE STRATEGY FOR PROMPTING LANGUAGE MODELS &lt;span&gt;â­&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inference-Time Intervention: Eliciting Truthful Answers from a Language Model&lt;/li&gt; &#xA; &lt;li&gt;Reflexion: an autonomous agent with dynamic memory and self-reflection&lt;/li&gt; &#xA; &lt;li&gt;Self-consistency for open-ended generations&lt;/li&gt; &#xA; &lt;li&gt;Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback&lt;/li&gt; &#xA; &lt;li&gt;Factuality Enhanced Language Models for Open-Ended Text Generation&lt;/li&gt; &#xA; &lt;li&gt;Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes&lt;/li&gt; &#xA; &lt;li&gt;Rethinking with Retrieval: Faithful Large Language Model Inference&lt;/li&gt; &#xA; &lt;li&gt;RefGPT: Reference â†’ Truthful &amp;amp; Customized Dialogues Generation by GPTs and for GPTs&lt;/li&gt; &#xA; &lt;li&gt;Enabling Large Language Models to Generate Text with Citations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;å¤§æ¨¡å‹è¯„ä¼°ï¼ˆevaluationï¼‰&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;äº‹å®æ€§è¯„ä¼° &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TRUSTWORTHY LLMS: A SURVEY AND GUIDELINE FOR EVALUATING LARGE LANGUAGE MODELSâ€™ ALIGNMENT&lt;/li&gt; &#xA;   &lt;li&gt;TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;TRUE: Re-evaluating Factual Consistency Evaluation&lt;/li&gt; &#xA;   &lt;li&gt;SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation&lt;/li&gt; &#xA;   &lt;li&gt;KoLA: Carefully Benchmarking World Knowledge of Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;æ¨ç†ä¼˜åŒ–(inference)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast Transformer Decoding: One Write-Head is All You Need&lt;/li&gt; &#xA; &lt;li&gt;Fast Inference from Transformers via Speculative Decoding&lt;/li&gt; &#xA; &lt;li&gt;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/li&gt; &#xA; &lt;li&gt;Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding&lt;/li&gt; &#xA; &lt;li&gt;SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference&lt;/li&gt; &#xA; &lt;li&gt;BatchPrompt: Accomplish more with less&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;æ¨¡å‹çŸ¥è¯†ç¼–è¾‘é»‘ç§‘æŠ€(model_edit)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ROMEï¼šLocating and Editing Factual Associations in GPT&lt;/li&gt; &#xA; &lt;li&gt;Transformer Feed-Forward Layers Are Key-Value Memories&lt;/li&gt; &#xA; &lt;li&gt;MEMIT: Mass-Editing Memory in a Transformer&lt;/li&gt; &#xA; &lt;li&gt;MENDï¼šFast Model Editing at Scale&lt;/li&gt; &#xA; &lt;li&gt;Editing Large Language Models: Problems, Methods, and Opportunities&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other Prompt Engineer(prompt_engineer)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Calibrate Before Use: Improving Few-Shot Performance of Language Models&lt;/li&gt; &#xA; &lt;li&gt;In-Context Instruction Learning&lt;/li&gt; &#xA; &lt;li&gt;LEARNING PERFORMANCE-IMPROVING CODE EDITS&lt;/li&gt; &#xA; &lt;li&gt;Boosting Theory-of-Mind Performance in Large Language Models via Prompting&lt;/li&gt; &#xA; &lt;li&gt;Generated Knowledge Prompting for Commonsense Reasoning&lt;/li&gt; &#xA; &lt;li&gt;RECITATION-AUGMENTED LANGUAGE MODELS&lt;/li&gt; &#xA; &lt;li&gt;kNN PROMPTING: BEYOND-CONTEXT LEARNING WITH CALIBRATION-FREE NEAREST NEIGHBOR INFERENCE&lt;/li&gt; &#xA; &lt;li&gt;EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus&lt;/li&gt; &#xA; &lt;li&gt;Causality-aware Concept Extraction based on Knowledge-guided Prompting&lt;/li&gt; &#xA; &lt;li&gt;LARGE LANGUAGE MODELS AS OPTIMIZERS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Multimodal&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning&lt;/li&gt; &#xA; &lt;li&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/li&gt; &#xA; &lt;li&gt;PaLM-E: An Embodied Multimodal Language Model&lt;/li&gt; &#xA; &lt;li&gt;LLava Visual Instruction Tuning&lt;/li&gt; &#xA; &lt;li&gt;MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;TabLLM: Few-shot Classification of Tabular Data with Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions&lt;/li&gt; &#xA; &lt;li&gt;mPLUG-Owl : Modularization Empowers Large Language Models with Multimodality&lt;/li&gt; &#xA; &lt;li&gt;LVLM eHub: A Comprehensive Evaluation Benchmark for Large VisionLanguage Models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Others&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pretraining on the Test Set Is All You Need å“ˆå“ˆä½œè€…ä½ æ˜¯æ‡‚è®½åˆºæ–‡å­¦çš„&lt;/li&gt; &#xA; &lt;li&gt;Learnware: Small Models Do Big&lt;/li&gt; &#xA; &lt;li&gt;The economic potential of generative AI&lt;/li&gt; &#xA; &lt;li&gt;A PhD Studentâ€™s Perspective on Research in NLP in the Era of Very Large Language Models&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>CodeWithHarry/Sigma-Web-Dev-Course</title>
    <updated>2023-10-15T01:46:43Z</updated>
    <id>tag:github.com,2023-10-15:/CodeWithHarry/Sigma-Web-Dev-Course</id>
    <link href="https://github.com/CodeWithHarry/Sigma-Web-Dev-Course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Source Code for Sigma Web Development Course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sigma-Web-Dev-Course&lt;/h1&gt; &#xA;&lt;p&gt;Source Code for Sigma Web Development Course&lt;/p&gt;</summary>
  </entry>
</feed>