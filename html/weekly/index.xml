<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-26T01:44:07Z</updated>
  <subtitle>Weekly Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>valiantlynx/ollama-docker</title>
    <updated>2025-01-26T01:44:07Z</updated>
    <id>tag:github.com,2025-01-26:/valiantlynx/ollama-docker</id>
    <link href="https://github.com/valiantlynx/ollama-docker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Welcome to the Ollama Docker Compose Setup! This project simplifies the deployment of Ollama using Docker Compose, making it easy to run Ollama with all its dependencies in a containerized environment&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Ollama Docker Compose Setup&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the Ollama Docker Compose Setup! This project simplifies the deployment of Ollama using Docker Compose, making it easy to run Ollama with all its dependencies in a containerized environment. &lt;a href=&#34;https://star-history.com/#valiantlynx/ollama-docker&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=valiantlynx/ollama-docker&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you have the following prerequisites installed on your machine:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA; &lt;li&gt;Docker Compose&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;GPU Support (Optional)&lt;/h4&gt; &#xA;&lt;p&gt;If you have a GPU and want to leverage its power within a Docker container, follow these steps to install the NVIDIA Container Toolkit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \&#xA;  &amp;amp;&amp;amp; curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \&#xA;    sed &#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39; | \&#xA;    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list&#xA;sudo apt-get update&#xA;sudo apt-get install -y nvidia-container-toolkit&#xA;&#xA;# Configure NVIDIA Container Toolkit&#xA;sudo nvidia-ctk runtime configure --runtime=docker&#xA;sudo systemctl restart docker&#xA;&#xA;# Test GPU integration&#xA;docker run --gpus all nvidia/cuda:11.5.2-base-ubuntu20.04 nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the Docker Compose repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/valiantlynx/ollama-docker.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Change to the project directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ollama-docker&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Start Ollama and its dependencies using Docker Compose:&lt;/p&gt; &#xA;&lt;p&gt;if gpu is configured&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose -f docker-compose-ollama-gpu.yaml up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;else&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt; in your browser to access Ollama-webui.&lt;/p&gt; &#xA;&lt;h3&gt;Model Installation&lt;/h3&gt; &#xA;&lt;p&gt;Navigate to settings -&amp;gt; model and install a model (e.g., llava-phi3). This may take a couple of minutes, but afterward, you can use it just like ChatGPT.&lt;/p&gt; &#xA;&lt;h3&gt;Explore Langchain and Ollama&lt;/h3&gt; &#xA;&lt;p&gt;You can explore Langchain and Ollama within the project. A third container named &lt;strong&gt;app&lt;/strong&gt; has been created for this purpose. Inside, you&#39;ll find some examples.&lt;/p&gt; &#xA;&lt;h3&gt;Devcontainer and Virtual Environment&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;strong&gt;app&lt;/strong&gt; container serves as a devcontainer, allowing you to boot into it for experimentation. Additionally, the run.sh file contains code to set up a virtual environment if you prefer not to use Docker for your development environment. if you have vs code and the `Remote Development¬¥ extension simply opening this project from the root will make vscode ask you to reopen in container&lt;/p&gt; &#xA;&lt;h2&gt;Stop and Cleanup&lt;/h2&gt; &#xA;&lt;p&gt;To stop the containers and remove the network:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose down&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! If you&#39;d like to contribute to the Ollama Docker Compose Setup, please follow our &lt;a href=&#34;https://raw.githubusercontent.com/valiantlynx/ollama-docker/main/CONTRIBUTING.md&#34;&gt;Contribution Guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/valiantlynx/ollama-docker/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;. Feel free to use, modify, and distribute it according to the terms of the license. Just give me a mention and some credit&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or concerns, please contact us at &lt;a href=&#34;mailto:vantlynxz@gmail.com&#34;&gt;vantlynxz@gmail.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Enjoy using Ollama with Docker Compose! üê≥üöÄ&lt;/p&gt;</summary>
  </entry>
</feed>