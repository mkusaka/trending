<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-19T01:39:36Z</updated>
  <subtitle>Weekly Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>unclecode/crawl4ai</title>
    <updated>2024-05-19T01:39:36Z</updated>
    <id>tag:github.com,2024-05-19:/unclecode/crawl4ai</id>
    <link href="https://github.com/unclecode/crawl4ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🔥🕷️ Crawl4AI: Open-source LLM Friendly Web Crawler &amp; Scrapper&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Crawl4AI v0.2.0 🕷️🤖&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/unclecode/crawl4ai/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/unclecode/crawl4ai?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/unclecode/crawl4ai/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/unclecode/crawl4ai?style=social&#34; alt=&#34;GitHub Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/unclecode/crawl4ai/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/unclecode/crawl4ai&#34; alt=&#34;GitHub Issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/unclecode/crawl4ai/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/unclecode/crawl4ai&#34; alt=&#34;GitHub Pull Requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/unclecode/crawl4ai/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/unclecode/crawl4ai&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Crawl4AI has one clear task: to simplify crawling and extract useful information from web pages, making it accessible for large language models (LLMs) and AI applications. 🆓🌐&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1wz8u30rvbq6Scodye9AGCw8Qg_Z8QGsk&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Recent Changes v0.2.0&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🚀 10x faster!!&lt;/li&gt; &#xA; &lt;li&gt;📜 Execute custom JavaScript before crawling!&lt;/li&gt; &#xA; &lt;li&gt;🤝 Colab friendly!&lt;/li&gt; &#xA; &lt;li&gt;📚 Chunking strategies: topic-based, regex, sentence, and more!&lt;/li&gt; &#xA; &lt;li&gt;🧠 Extraction strategies: cosine clustering, LLM, and more!&lt;/li&gt; &#xA; &lt;li&gt;🎯 CSS selector support&lt;/li&gt; &#xA; &lt;li&gt;📝 Pass instructions/keywords to refine extraction&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Power and Simplicity of Crawl4AI 🚀&lt;/h2&gt; &#xA;&lt;p&gt;To show the simplicity take a look at the first example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from crawl4ai import WebCrawler&#xA;&#xA;# Create the WebCrawler instance &#xA;crawler = WebCrawler() &#xA;&#xA;&#xA;&#xA;# Run the crawler with keyword filtering and CSS selector&#xA;result = crawler.run(url=&#34;https://www.nbcnews.com/business&#34;)&#xA;print(result) # {url, html, markdown, extracted_content, metadata}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now let&#39;s try a complex task. Below is an example of how you can execute JavaScript, filter data using keywords, and use a CSS selector to extract specific content—all in one go!&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Instantiate a WebCrawler object.&lt;/li&gt; &#xA; &lt;li&gt;Execute custom JavaScript to click a &#34;Load More&#34; button.&lt;/li&gt; &#xA; &lt;li&gt;Extract semantical chunks of content and filter the data to include only content related to technology.&lt;/li&gt; &#xA; &lt;li&gt;Use a CSS selector to extract only paragraphs (&lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt; tags).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Import necessary modules&#xA;from crawl4ai import WebCrawler&#xA;from crawl4ai.chunking_strategy import *&#xA;from crawl4ai.extraction_strategy import *&#xA;from crawl4ai.crawler_strategy import *&#xA;&#xA;# Define the JavaScript code to click the &#34;Load More&#34; button&#xA;js_code = &#34;&#34;&#34;&#xA;const loadMoreButton = Array.from(document.querySelectorAll(&#39;button&#39;)).find(button =&amp;gt; button.textContent.includes(&#39;Load More&#39;));&#xA;loadMoreButton &amp;amp;&amp;amp; loadMoreButton.click();&#xA;&#34;&#34;&#34;&#xA;&#xA;# Define the crawling strategy&#xA;crawler_strategy = LocalSeleniumCrawlerStrategy(js_code=js_code)&#xA;&#xA;# Create the WebCrawler instance with the defined strategy&#xA;crawler = WebCrawler(crawler_strategy=crawler_strategy)&#xA;&#xA;# Run the crawler with keyword filtering and CSS selector&#xA;result = crawler.run(&#xA;    url=&#34;https://www.nbcnews.com/business&#34;,&#xA;    extraction_strategy=CosineStrategy(&#xA;        semantic_filter=&#34;technology&#34;,&#xA;    ),&#xA;)&#xA;&#xA;# Run the crawler with LLM extraction strategy&#xA;result = crawler.run(&#xA;    url=&#34;https://www.nbcnews.com/business&#34;,&#xA;    extraction_strategy=LLMExtractionStrategy(&#xA;        provider=&#34;openai/gpt-4o&#34;,&#xA;        api_token=os.getenv(&#39;OPENAI_API_KEY&#39;),&#xA;        instruction=&#34;Extract only content related to technology&#34;&#xA;    ),&#xA;    css_selector=&#34;p&#34;&#xA;)&#xA;&#xA;# Display the extracted result&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With Crawl4AI, you can perform advanced web crawling and data extraction tasks with just a few lines of code. This example demonstrates how you can harness the power of Crawl4AI to simplify your workflow and get the data you need efficiently.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;Continue reading to learn more about the features, installation process, usage, and more.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#features-&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#installation-&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#using-the-local-server-ot-rest-api-&#34;&gt;REST API/Local Server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#python-library-usage-&#34;&gt;Python Library Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#parameters-&#34;&gt;Parameters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#chunking-strategies-&#34;&gt;Chunking Strategies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#extraction-strategies-&#34;&gt;Extraction Strategies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#contributing-&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#license-&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#contact-&#34;&gt;Contact&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Features ✨&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🕷️ Efficient web crawling to extract valuable data from websites&lt;/li&gt; &#xA; &lt;li&gt;🤖 LLM-friendly output formats (JSON, cleaned HTML, markdown)&lt;/li&gt; &#xA; &lt;li&gt;🌍 Supports crawling multiple URLs simultaneously&lt;/li&gt; &#xA; &lt;li&gt;🌃 Replace media tags with ALT.&lt;/li&gt; &#xA; &lt;li&gt;🆓 Completely free to use and open-source&lt;/li&gt; &#xA; &lt;li&gt;📜 Execute custom JavaScript before crawling&lt;/li&gt; &#xA; &lt;li&gt;📚 Chunking strategies: topic-based, regex, sentence, and more&lt;/li&gt; &#xA; &lt;li&gt;🧠 Extraction strategies: cosine clustering, LLM, and more&lt;/li&gt; &#xA; &lt;li&gt;🎯 CSS selector support&lt;/li&gt; &#xA; &lt;li&gt;📝 Pass instructions/keywords to refine extraction&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation 💻&lt;/h2&gt; &#xA;&lt;p&gt;There are three ways to use Crawl4AI:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;As a library (Recommended)&lt;/li&gt; &#xA; &lt;li&gt;As a local server (Docker) or using the REST API&lt;/li&gt; &#xA; &lt;li&gt;As a Google Colab notebook. &lt;a href=&#34;https://colab.research.google.com/drive/1wz8u30rvbq6Scodye9AGCw8Qg_Z8QGsk&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To install Crawl4AI as a library, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the package from GitHub:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;virtualenv venv&#xA;source venv/bin/activate&#xA;pip install &#34;crawl4ai[all] @ git+https://github.com/unclecode/crawl4ai.git&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;💡 Better to run the following CLI-command to load the required models. This is optional, but it will boost the performance and speed of the crawler. You need to do this only once.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;crawl4ai-download-models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Alternatively, you can clone the repository and install the package locally:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;virtualenv venv&#xA;source venv/bin/activate&#xA;git clone https://github.com/unclecode/crawl4ai.git&#xA;cd crawl4ai&#xA;pip install -e .[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Use docker to run the local server:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Mac users&#xA;# docker build --platform linux/amd64 -t crawl4ai .&#xA;# For other users&#xA;# docker build -t crawl4ai .&#xA;docker run -d -p 8000:80 crawl4ai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using the Local server ot REST API 🌐&lt;/h2&gt; &#xA;&lt;p&gt;You can also use Crawl4AI through the REST API. This method allows you to send HTTP requests to the Crawl4AI server and receive structured data in response. The base URL for the API is &lt;code&gt;https://crawl4ai.com/crawl&lt;/code&gt; [Available now, on a CPU server, of course will be faster on GPU]. If you run the local server, you can use &lt;code&gt;http://localhost:8000/crawl&lt;/code&gt;. (Port is dependent on your docker configuration)&lt;/p&gt; &#xA;&lt;h3&gt;Example Usage&lt;/h3&gt; &#xA;&lt;p&gt;To use the REST API, send a POST request to &lt;code&gt;http://localhost:8000/crawl&lt;/code&gt; with the following parameters in the request body.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example Request:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;urls&#34;: [&#34;https://www.nbcnews.com/business&#34;],&#xA;    &#34;include_raw_html&#34;: false,&#xA;    &#34;bypass_cache&#34;: true,&#xA;    &#34;word_count_threshold&#34;: 5,&#xA;    &#34;extraction_strategy&#34;: &#34;CosineStrategy&#34;,&#xA;    &#34;chunking_strategy&#34;: &#34;RegexChunking&#34;,&#xA;    &#34;css_selector&#34;: &#34;p&#34;,&#xA;    &#34;verbose&#34;: true,&#xA;    &#34;extraction_strategy_args&#34;: {&#xA;        &#34;semantic_filter&#34;: &#34;finance economy and stock market&#34;,&#xA;        &#34;word_count_threshold&#34;: 20,&#xA;        &#34;max_dist&#34;: 0.2,&#xA;        &#34;linkage_method&#34;: &#34;ward&#34;,&#xA;        &#34;top_k&#34;: 3&#xA;    },&#xA;    &#34;chunking_strategy_args&#34;: {&#xA;        &#34;patterns&#34;: [&#34;\n\n&#34;]&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example Response:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;status&#34;: &#34;success&#34;,&#xA;    &#34;data&#34;: [&#xA;        {&#xA;            &#34;url&#34;: &#34;https://www.nbcnews.com/business&#34;,&#xA;            &#34;extracted_content&#34;: &#34;...&#34;,&#xA;            &#34;html&#34;: &#34;...&#34;,&#xA;            &#34;markdown&#34;: &#34;...&#34;,&#xA;            &#34;metadata&#34;: {...}&#xA;        }&#xA;    ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information about the available parameters and their descriptions, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/unclecode/crawl4ai/main/#parameters&#34;&gt;Parameters&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;Python Library Usage 🚀&lt;/h2&gt; &#xA;&lt;p&gt;🔥 A great way to try out Crawl4AI is to run &lt;code&gt;quickstart.py&lt;/code&gt; in the &lt;code&gt;docs/examples&lt;/code&gt; directory. This script demonstrates how to use Crawl4AI to crawl a website and extract content from it.&lt;/p&gt; &#xA;&lt;h3&gt;Quickstart Guide&lt;/h3&gt; &#xA;&lt;p&gt;Create an instance of WebCrawler and call the &lt;code&gt;warmup()&lt;/code&gt; function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;crawler = WebCrawler()&#xA;crawler.warmup()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Understanding &#39;bypass_cache&#39; and &#39;include_raw_html&#39; parameters&lt;/h3&gt; &#xA;&lt;p&gt;First crawl (caches the result):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(url=&#34;https://www.nbcnews.com/business&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Second crawl (Force to crawl again):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(url=&#34;https://www.nbcnews.com/business&#34;, bypass_cache=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;💡 Don&#39;t forget to set `bypass_cache` to True if you want to try different strategies for the same URL. Otherwise, the cached result will be returned. You can also set `always_by_pass_cache` in constructor to True to always bypass the cache.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Crawl result without raw HTML content:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(url=&#34;https://www.nbcnews.com/business&#34;, include_raw_html=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Adding a chunking strategy: RegexChunking&lt;/h3&gt; &#xA;&lt;p&gt;Using RegexChunking:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(&#xA;    url=&#34;https://www.nbcnews.com/business&#34;,&#xA;    chunking_strategy=RegexChunking(patterns=[&#34;\n\n&#34;])&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using NlpSentenceChunking:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(&#xA;    url=&#34;https://www.nbcnews.com/business&#34;,&#xA;    chunking_strategy=NlpSentenceChunking()&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Extraction strategy: CosineStrategy&lt;/h3&gt; &#xA;&lt;p&gt;So far, the extracted content is just the result of chunking. To extract meaningful content, you can use extraction strategies. These strategies cluster consecutive chunks into meaningful blocks, keeping the same order as the text in the HTML. This approach is perfect for use in RAG applications and semantical search queries.&lt;/p&gt; &#xA;&lt;p&gt;Using CosineStrategy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(&#xA;    url=&#34;https://www.nbcnews.com/business&#34;,&#xA;    extraction_strategy=CosineStrategy(&#xA;        semantic_filter=&#34;&#34;,&#xA;        word_count_threshold=10, &#xA;        max_dist=0.2, &#xA;        linkage_method=&#34;ward&#34;, &#xA;        top_k=3&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can set &lt;code&gt;semantic_filter&lt;/code&gt; to filter relevant documents before clustering. Documents are filtered based on their cosine similarity to the keyword filter embedding.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(&#xA;    url=&#34;https://www.nbcnews.com/business&#34;,&#xA;    extraction_strategy=CosineStrategy(&#xA;        semantic_filter=&#34;finance economy and stock market&#34;,&#xA;        word_count_threshold=10, &#xA;        max_dist=0.2, &#xA;        linkage_method=&#34;ward&#34;, &#xA;        top_k=3&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using LLMExtractionStrategy&lt;/h3&gt; &#xA;&lt;p&gt;Without instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(&#xA;    url=&#34;https://www.nbcnews.com/business&#34;,&#xA;    extraction_strategy=LLMExtractionStrategy(&#xA;        provider=&#34;openai/gpt-4o&#34;, &#xA;        api_token=os.getenv(&#39;OPENAI_API_KEY&#39;)&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With instructions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(&#xA;    url=&#34;https://www.nbcnews.com/business&#34;,&#xA;    extraction_strategy=LLMExtractionStrategy(&#xA;        provider=&#34;openai/gpt-4o&#34;,&#xA;        api_token=os.getenv(&#39;OPENAI_API_KEY&#39;),&#xA;        instruction=&#34;I am interested in only financial news&#34;&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Targeted extraction using CSS selector&lt;/h3&gt; &#xA;&lt;p&gt;Extract only H2 tags:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = crawler.run(&#xA;    url=&#34;https://www.nbcnews.com/business&#34;,&#xA;    css_selector=&#34;h2&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Passing JavaScript code to click &#39;Load More&#39; button&lt;/h3&gt; &#xA;&lt;p&gt;Using JavaScript to click &#39;Load More&#39; button:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;js_code = &#34;&#34;&#34;&#xA;const loadMoreButton = Array.from(document.querySelectorAll(&#39;button&#39;)).find(button =&amp;gt; button.textContent.includes(&#39;Load More&#39;));&#xA;loadMoreButton &amp;amp;&amp;amp; loadMoreButton.click();&#xA;&#34;&#34;&#34;&#xA;crawler_strategy = LocalSeleniumCrawlerStrategy(js_code=js_code)&#xA;crawler = WebCrawler(crawler_strategy=crawler_strategy, always_by_pass_cache=True)&#xA;result = crawler.run(url=&#34;https://www.nbcnews.com/business&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Parameters 📖&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Required&lt;/th&gt; &#xA;   &lt;th&gt;Default Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;urls&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A list of URLs to crawl and extract data from.&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;include_raw_html&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Whether to include the raw HTML content in the response.&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;bypass_cache&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Whether to force a fresh crawl even if the URL has been previously crawled.&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;false&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;word_count_threshold&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The minimum number of words a block must contain to be considered meaningful (minimum value is 5).&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;extraction_strategy&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The strategy to use for extracting content from the HTML (e.g., &#34;CosineStrategy&#34;).&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;NoExtractionStrategy&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;chunking_strategy&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The strategy to use for chunking the text before processing (e.g., &#34;RegexChunking&#34;).&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RegexChunking&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;css_selector&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The CSS selector to target specific parts of the HTML for extraction.&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;None&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;verbose&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Whether to enable verbose logging.&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Chunking Strategies 📚&lt;/h2&gt; &#xA;&lt;h3&gt;RegexChunking&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;RegexChunking&lt;/code&gt; is a text chunking strategy that splits a given text into smaller parts using regular expressions. This is useful for preparing large texts for processing by language models, ensuring they are divided into manageable segments.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructor Parameters:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;patterns&lt;/code&gt; (list, optional): A list of regular expression patterns used to split the text. Default is to split by double newlines (&lt;code&gt;[&#39;\n\n&#39;]&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chunker = RegexChunking(patterns=[r&#39;\n\n&#39;, r&#39;\. &#39;])&#xA;chunks = chunker.chunk(&#34;This is a sample text. It will be split into chunks.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;NlpSentenceChunking&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;NlpSentenceChunking&lt;/code&gt; uses a natural language processing model to chunk a given text into sentences. This approach leverages SpaCy to accurately split text based on sentence boundaries.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructor Parameters:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;None.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chunker = NlpSentenceChunking()&#xA;chunks = chunker.chunk(&#34;This is a sample text. It will be split into sentences.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;TopicSegmentationChunking&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;TopicSegmentationChunking&lt;/code&gt; uses the TextTiling algorithm to segment a given text into topic-based chunks. This method identifies thematic boundaries in the text.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructor Parameters:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;num_keywords&lt;/code&gt; (int, optional): The number of keywords to extract for each topic segment. Default is &lt;code&gt;3&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chunker = TopicSegmentationChunking(num_keywords=3)&#xA;chunks = chunker.chunk(&#34;This is a sample text. It will be split into topic-based segments.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;FixedLengthWordChunking&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;FixedLengthWordChunking&lt;/code&gt; splits a given text into chunks of fixed length, based on the number of words.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructor Parameters:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;chunk_size&lt;/code&gt; (int, optional): The number of words in each chunk. Default is &lt;code&gt;100&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chunker = FixedLengthWordChunking(chunk_size=100)&#xA;chunks = chunker.chunk(&#34;This is a sample text. It will be split into fixed-length word chunks.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SlidingWindowChunking&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;SlidingWindowChunking&lt;/code&gt; uses a sliding window approach to chunk a given text. Each chunk has a fixed length, and the window slides by a specified step size.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructor Parameters:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;window_size&lt;/code&gt; (int, optional): The number of words in each chunk. Default is &lt;code&gt;100&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;step&lt;/code&gt; (int, optional): The number of words to slide the window. Default is &lt;code&gt;50&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;chunker = SlidingWindowChunking(window_size=100, step=50)&#xA;chunks = chunker.chunk(&#34;This is a sample text. It will be split using a sliding window approach.&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Extraction Strategies 🧠&lt;/h2&gt; &#xA;&lt;h3&gt;NoExtractionStrategy&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;NoExtractionStrategy&lt;/code&gt; is a basic extraction strategy that returns the entire HTML content without any modification. It is useful for cases where no specific extraction is required.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructor Parameters:&lt;/strong&gt; None.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;extractor = NoExtractionStrategy()&#xA;extracted_content = extractor.extract(url, html)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLMExtractionStrategy&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;LLMExtractionStrategy&lt;/code&gt; uses a Language Model (LLM) to extract meaningful blocks or chunks from the given HTML content. This strategy leverages an external provider for language model completions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructor Parameters:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;provider&lt;/code&gt; (str, optional): The provider to use for the language model completions. Default is &lt;code&gt;DEFAULT_PROVIDER&lt;/code&gt; (e.g., openai/gpt-4).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;api_token&lt;/code&gt; (str, optional): The API token for the provider. If not provided, it will try to load from the environment variable &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;instruction&lt;/code&gt; (str, optional): An instruction to guide the LLM on how to perform the extraction. This allows users to specify the type of data they are interested in or set the tone of the response. Default is &lt;code&gt;None&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;extractor = LLMExtractionStrategy(provider=&#39;openai&#39;, api_token=&#39;your_api_token&#39;, instruction=&#39;Extract only news about AI.&#39;)&#xA;extracted_content = extractor.extract(url, html)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CosineStrategy&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;CosineStrategy&lt;/code&gt; uses hierarchical clustering based on cosine similarity to extract clusters of text from the given HTML content. This strategy is suitable for identifying related content sections.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructor Parameters:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;semantic_filter&lt;/code&gt; (str, optional): A string containing keywords for filtering relevant documents before clustering. If provided, documents are filtered based on their cosine similarity to the keyword filter embedding. Default is &lt;code&gt;None&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;word_count_threshold&lt;/code&gt; (int, optional): Minimum number of words per cluster. Default is &lt;code&gt;20&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_dist&lt;/code&gt; (float, optional): The maximum cophenetic distance on the dendrogram to form clusters. Default is &lt;code&gt;0.2&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;linkage_method&lt;/code&gt; (str, optional): The linkage method for hierarchical clustering. Default is &lt;code&gt;&#39;ward&#39;&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;top_k&lt;/code&gt; (int, optional): Number of top categories to extract. Default is &lt;code&gt;3&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model_name&lt;/code&gt; (str, optional): The model name for embedding generation. Default is &lt;code&gt;&#39;BAAI/bge-small-en-v1.5&#39;&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;extractor = CosineStrategy(semantic_filter=&#39;finance rental prices&#39;, word_count_threshold=10, max_dist=0.2, linkage_method=&#39;ward&#39;, top_k=3, model_name=&#39;BAAI/bge-small-en-v1.5&#39;)&#xA;extracted_content = extractor.extract(url, html)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;TopicExtractionStrategy&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;TopicExtractionStrategy&lt;/code&gt; uses the TextTiling algorithm to segment the HTML content into topics and extracts keywords for each segment. This strategy is useful for identifying and summarizing thematic content.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Constructor Parameters:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;num_keywords&lt;/code&gt; (int, optional): Number of keywords to represent each topic segment. Default is &lt;code&gt;3&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Example usage:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;extractor = TopicExtractionStrategy(num_keywords=3)&#xA;extracted_content = extractor.extract(url, html)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing 🤝&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the open-source community to help improve Crawl4AI and make it even more valuable for AI enthusiasts and developers. To contribute, please follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repository.&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch for your feature or bug fix.&lt;/li&gt; &#xA; &lt;li&gt;Make your changes and commit them with descriptive messages.&lt;/li&gt; &#xA; &lt;li&gt;Push your changes to your forked repository.&lt;/li&gt; &#xA; &lt;li&gt;Submit a pull request to the main repository.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more information on contributing, please see our &lt;a href=&#34;https://github.com/unclecode/crawl4ai/raw/main/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License 📄&lt;/h2&gt; &#xA;&lt;p&gt;Crawl4AI is released under the &lt;a href=&#34;https://github.com/unclecode/crawl4ai/raw/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact 📧&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, suggestions, or feedback, please feel free to reach out to us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/unclecode&#34;&gt;unclecode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Twitter: &lt;a href=&#34;https://twitter.com/unclecode&#34;&gt;@unclecode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://crawl4ai.com&#34;&gt;crawl4ai.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Let&#39;s work together to make the web more accessible and useful for AI applications! 💪🌐🤖&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>vidio-boy/Eaglercraft1.5.2</title>
    <updated>2024-05-19T01:39:36Z</updated>
    <id>tag:github.com,2024-05-19:/vidio-boy/Eaglercraft1.5.2</id>
    <link href="https://github.com/vidio-boy/Eaglercraft1.5.2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Eaglercraft javascript runtime. Multiplayer, singleplayer, and LAN worlds available. Customizable profile, skins, capes, and controls. Game modes including creative, survival, peaceful, and more! Villages, mine shafts, dungeons, caves, ravines, many different biomes, and other things can spawn. The Nether and End are also available.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Eaglercraft 1.5.2 Browser&lt;/h1&gt;  &#xA;&lt;p&gt; &lt;b&gt;Eaglercraft 1.5.2 javascript runtime.&lt;/b&gt; Multiplayer, singleplayer, and LAN worlds available. Customizable profile, skins, capes, and controls. Game modes including creative, survival, peaceful, and more! Villages, mine shafts, dungeons, caves, ravines, many different biomes, and other things can spawn. The Nether and End are also available. Commands are the same as the real Minecraft. &lt;/p&gt;   &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h1&gt;How to launch game in browser:&lt;/h1&gt; &#xA;&lt;p&gt;1. Download the zip file.&lt;/p&gt; &#xA;&lt;p&gt;2. Open the zip in your files.&lt;/p&gt; &#xA;&lt;p&gt;3. Find the eaglercraft.1.5.2.html file.&lt;/p&gt; &#xA;&lt;p&gt;4. Open the file and let it load.&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h1&gt;Servers&lt;/h1&gt; Go to &#xA;&lt;a href=&#34;https://web.archive.org/web/20230205110931/https://docs.google.com/document/d/1PhUJSb0ojMyhv1Fs8bmVqwANBkySOgdyfRinJto3xnE/edit&#34; &lt;a&gt;this doc&lt;/a&gt; (had to use the wayback machine) to get &#xA;&lt;b&gt;a list of Multiplayer Server Adresses.&lt;/b&gt; I do not own it or edit it in any way. &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h1&gt;IMPORTANT!&lt;/h1&gt; &#xA;&lt;p&gt; When you create a new world, it will be a black screen, or you may have spawned underground. &lt;b&gt;ALL YOU HAVE TO DO IS WAIT 1–5 MINUTES!&lt;/b&gt; It will fix on its own. It will also be laggy at first, but it will smooth out after 5–10 minutes. &lt;b&gt;Keep in mind that this is an old version of Minecraft and that not everything in the real current Minecraft will be avialable.&lt;/b&gt; &lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;footer&gt;&#xA;  &lt;small&gt;&lt;b&gt;Disclaimer:&lt;/b&gt; I do not own Eaglercraft, and I am not associated with it in any way, this is just a way for people to access it fullscreen on browser.&lt;small&gt;&#xA;    &lt;footer&gt;  &#xA;    &lt;/footer&gt;&lt;/small&gt;&lt;/small&gt;&#xA; &lt;/footer&gt;&#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>wandb/openui</title>
    <updated>2024-05-19T01:39:36Z</updated>
    <id>tag:github.com,2024-05-19:/wandb/openui</id>
    <link href="https://github.com/wandb/openui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenUI let&#39;s you describe UI using your imagination, then see it rendered live.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenUI&lt;/h1&gt; &#xA;&lt;p&gt;Building UI components can be a slog. OpenUI aims to make the process fun, fast, and flexible. It&#39;s also a tool we&#39;re using at &lt;a href=&#34;https://wandb.com&#34;&gt;W&amp;amp;B&lt;/a&gt; to test and prototype our next generation tooling for building powerful applications on top of LLM&#39;s.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/demo.gif&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenUI let&#39;s you describe UI using your imagination, then see it rendered live. You can ask for changes and convert HTML to React, Svelte, Web Components, etc. It&#39;s like &lt;a href=&#34;https://v0.dev&#34;&gt;v0&lt;/a&gt; but open source and not as polished &lt;span&gt;😝&lt;/span&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Live Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openui.fly.dev&#34;&gt;Try the demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;You can also run OpenUI locally and use models available to &lt;a href=&#34;https://ollama.com&#34;&gt;Ollama&lt;/a&gt;. &lt;a href=&#34;https://ollama.com/download&#34;&gt;Install Ollama&lt;/a&gt; and pull a model like &lt;a href=&#34;https://ollama.com/library/codellama&#34;&gt;CodeLlama&lt;/a&gt;, then assuming you have git and python installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/wandb/openui&#xA;cd openui/backend&#xA;# You probably want to do this from a virtual environment&#xA;pip install .&#xA;# This must be set to use OpenAI models, find your api key here: https://platform.openai.com/api-keys&#xA;export OPENAI_API_KEY=xxx&#xA;# You may change the base url to use an OpenAI-compatible api by setting the OPENAI_BASE_URL environment variable&#xA;# export OPENAI_BASE_URL=https://api.myopenai.com/v1&#xA;python -m openui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Groq&lt;/h2&gt; &#xA;&lt;p&gt;To use the super fast &lt;a href=&#34;https://groq.com&#34;&gt;Groq&lt;/a&gt; models, set &lt;code&gt;GROQ_API_KEY&lt;/code&gt; to your Groq api key which you can &lt;a href=&#34;https://console.groq.com/keys&#34;&gt;find here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can also change the default base url used for Groq (if necessary), i.e.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export GROQ_BASE_URL=https://api.groq.com/openai/v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Compose&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;DISCLAIMER: This is likely going to be very slow. If you have a GPU you may need to change the tag of the &lt;code&gt;ollama&lt;/code&gt; container to one that supports it. If you&#39;re running on a Mac, follow the instructions above and run Ollama natively to take advantage of the M1/M2.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;From the root directory you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker-compose up -d&#xA;docker exec -it openui-ollama-1 ollama pull llava&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have your OPENAI_API_KEY set in the environment already, just remove &lt;code&gt;=xxx&lt;/code&gt; from the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; line. You can also replace &lt;code&gt;llava&lt;/code&gt; in the command above with your open source model of choice &lt;em&gt;(&lt;a href=&#34;https://ollama.com/library/llava&#34;&gt;llava&lt;/a&gt; is one of the only Ollama models that support images currently)&lt;/em&gt;. You should now be able to access OpenUI at &lt;a href=&#34;http://localhost:7878&#34;&gt;http://localhost:7878&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you make changes to the frontend or backend, you&#39;ll need to run &lt;code&gt;docker-compose build&lt;/code&gt; to have them reflected in the service.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can build and run the docker file manually from the &lt;code&gt;/backend&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build . -t wandb/openui --load&#xA;docker run -p 7878:7878 -e OPENAI_API_KEY -e GROQ_API_KEY wandb/openui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can goto &lt;a href=&#34;http://localhost:7878&#34;&gt;http://localhost:7878&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://github.com/wandb/openui/raw/main/.devcontainer/devcontainer.json&#34;&gt;dev container&lt;/a&gt; is configured in this repository which is the quickest way to get started.&lt;/p&gt; &#xA;&lt;h3&gt;Codespace&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/codespace.png&#34; alt=&#34;New with options...&#34; width=&#34;500&#34;&gt; &#xA;&lt;p&gt;Choose more options when creating a Codespace, then select &lt;strong&gt;New with options...&lt;/strong&gt;. Select the US West region if you want a really fast boot time. You&#39;ll also want to configure your OPENAI_API_KEY secret or just set it to &lt;code&gt;xxx&lt;/code&gt; if you want to try Ollama &lt;em&gt;(you&#39;ll want at least 16GB of Ram)&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once inside the code space you can run the server in one terminal: &lt;code&gt;python -m openui --dev&lt;/code&gt;. Then in a new terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /workspaces/openui/frontend&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should open another service on port 5173, that&#39;s the service you&#39;ll want to visit. All changes to both the frontend and backend will automatically be reloaded and reflected in your browser.&lt;/p&gt; &#xA;&lt;h3&gt;Ollama&lt;/h3&gt; &#xA;&lt;p&gt;The codespace installs ollama automaticaly and downloads the &lt;code&gt;llava&lt;/code&gt; model. You can verify Ollama is running with &lt;code&gt;ollama list&lt;/code&gt; if that fails, open a new terminal and run &lt;code&gt;ollama serve&lt;/code&gt;. In Codespaces we pull llava on boot so you should see it in the list. You can select Ollama models from the settings gear icon in the upper left corner of the application. Any models you pull i.e. &lt;code&gt;ollama pull llama&lt;/code&gt; will show up in the settings modal.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/ollama.png&#34; width=&#34;500&#34; alt=&#34;Select Ollama models&#34;&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;p&gt;See the readmes in the &lt;a href=&#34;https://raw.githubusercontent.com/wandb/openui/main/frontend/README.md&#34;&gt;frontend&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/wandb/openui/main/backend/README.md&#34;&gt;backend&lt;/a&gt; directories.&lt;/p&gt;</summary>
  </entry>
</feed>