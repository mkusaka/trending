<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub HTML Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-15T01:46:43Z</updated>
  <subtitle>Weekly Trending of HTML in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AliceWonderland/hacktoberfest</title>
    <updated>2023-10-15T01:46:43Z</updated>
    <id>tag:github.com,2023-10-15:/AliceWonderland/hacktoberfest</id>
    <link href="https://github.com/AliceWonderland/hacktoberfest" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Participate in Hacktoberfest by contributing to any Open Source project on GitHub! Here is a starter project for first time contributors. #hacktoberfest&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🎃 HacktoberFest Starter Project 🎃&lt;/h1&gt; &#xA;&lt;p&gt;** &lt;strong&gt;Oct 24th, 2017 Update:&lt;/strong&gt; THIS REPO IS TEMPORARILY &lt;strong&gt;NOT MERGING NEW PRs&lt;/strong&gt; until the CONTRIBUTORS.md file is sorted! Thanks for your patience! **&lt;/p&gt; &#xA;&lt;p&gt;Use this project to make your first contribution to an open source project on GitHub. Practice making your first pull request to a public repository before doing the real thing!&lt;/p&gt; &#xA;&lt;p&gt;Celebrate &lt;a href=&#34;https://hacktoberfest.digitalocean.com/&#34;&gt;Hacktoberfest&lt;/a&gt; by getting involved in the open source community by completing some simple tasks in this project.&lt;/p&gt; &#xA;&lt;p&gt;This repository is open to all members of the GitHub community. Any member may contribute to this project without being a collaborator.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://alicewonderland.github.io/hacktoberfest/&#34;&gt;https://alicewonderland.github.io/hacktoberfest/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is Hacktoberfest?&lt;/h2&gt; &#xA;&lt;p&gt;A month-long celebration from October 1st - 31st sponsored by &lt;a href=&#34;https://hacktoberfest.digitalocean.com/&#34;&gt;Digital Ocean&lt;/a&gt; and &lt;a href=&#34;https://github.com/blog/2433-celebrate-open-source-this-october-with-hacktoberfest&#34;&gt;GitHub&lt;/a&gt; to get people involved in &lt;a href=&#34;https://github.com/open-source&#34;&gt;Open Source&lt;/a&gt;. Create your very first pull request to any public repository on GitHub and contribute to the open source developer community.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hacktoberfest.digitalocean.com/&#34;&gt;https://hacktoberfest.digitalocean.com/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How to contribute to this project&lt;/h2&gt; &#xA;&lt;p&gt;Here are 3 quick and painless ways to contribute to this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Add your name to the &lt;code&gt;CONTRIBUTORS.md&lt;/code&gt; file&lt;/li&gt; &#xA; &lt;li&gt;Add a profile page to the &lt;code&gt;profiles&lt;/code&gt; directory&lt;/li&gt; &#xA; &lt;li&gt;Create a simple &#34;Hello, World&#34; script in a language of your choice&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Choose one or all 3, make a pull request for your work and wait for it to be merged!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork this repository (Click the Fork button in the top right of this page, click your Profile Image)&lt;/li&gt; &#xA; &lt;li&gt;Clone your fork down to your local machine&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;git clone https://github.com/your-username/hacktoberfest.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a branch&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;git checkout -b branch-name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make your changes (choose from any task below)&lt;/li&gt; &#xA; &lt;li&gt;Commit and push&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;git add .&#xA;git commit -m &#39;Commit message&#39;&#xA;git push origin branch-name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a new pull request from your forked repository (Click the &lt;code&gt;New Pull Request&lt;/code&gt; button located at the top of your repo)&lt;/li&gt; &#xA; &lt;li&gt;Wait for your PR review and merge approval!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Star this repository&lt;/strong&gt; if you had fun!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Choose from these tasks&lt;/h2&gt; &#xA;&lt;h3&gt;1. Add your name&lt;/h3&gt; &#xA;&lt;p&gt;Add your name to the &lt;code&gt;CONTRIBUTORS.md&lt;/code&gt; file using the below convention:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;#### Name: [YOUR NAME](GitHub link)&#xA;- Place: City, State, Country&#xA;- Bio: Who are you?&#xA;- GitHub: [GitHub account name](GitHub link)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Add a profile page&lt;/h3&gt; &#xA;&lt;p&gt;Add a &lt;code&gt;Your_Name.md&lt;/code&gt; file to the &lt;code&gt;profiles&lt;/code&gt; directory. Use any combination of content and Markdown you&#39;d like. Here is an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;# Your Name&#xA;&#xA;### Location&#xA;&#xA;Your City/Country&#xA;&#xA;### Academics&#xA;&#xA;Your School&#xA;&#xA;### Interests&#xA;&#xA;- Some Things You Like&#xA;&#xA;### Development&#xA;&#xA;- Inventor of the My Pillow&#xA;&#xA;### Projects&#xA;&#xA;- [My Project](GitHub Link) Short Description&#xA;&#xA;### Profile Link&#xA;&#xA;[Your Name](GitHub Link)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Create a &lt;code&gt;Hello, World!&lt;/code&gt; Script&lt;/h3&gt; &#xA;&lt;p&gt;Add a &lt;code&gt;hello_world_yourusername.xx&lt;/code&gt; script to the &lt;code&gt;scripts&lt;/code&gt; directory in any language of your choice! Here is an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Javascript&#34;&gt;// LANGUAGE: Javascript&#xA;// ENV: Node.js&#xA;// AUTHOR: Alice Chuang&#xA;// GITHUB: https://github.com/AliceWonderland&#xA;&#xA;console.log(&#39;Hello, World!&#39;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Name the file &lt;code&gt;hello_world_yourusername.xx&lt;/code&gt;. e.g., &lt;code&gt;hello_world_alicewonderland.js&lt;/code&gt; or &lt;code&gt;hello_world_alicewonderland.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Don&#39;t forget to include the comments as seen above. Feel free to include additional information about the language you choose in your comments too! Like a link to a helpful introduction or tutorial.&lt;/p&gt; &#xA;&lt;p&gt;Here is my &lt;code&gt;hello_world&lt;/code&gt; example: &lt;a href=&#34;https://github.com/AliceWonderland/hacktoberfest/raw/master/scripts/hello_world_alicewonderland.js&#34;&gt;hello_world_alicewonderland.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;BONUS!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See profiles submitted by fellow coders from around the globe ... from Kathmandu to Copenhagen.&lt;/li&gt; &#xA; &lt;li&gt;Discover some obscure to new and trending languages ... from BrainFuck to Groovy.&lt;/li&gt; &#xA; &lt;li&gt;Check out some very creative ways to print out a &#34;Hello, World!&#34; string.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reference links&lt;/h2&gt; &#xA;&lt;p&gt;Here is a great tutorial for creating your first pull request by &lt;a href=&#34;https://github.com/Roshanjossey&#34;&gt;Roshan Jossey&lt;/a&gt;: &lt;a href=&#34;https://github.com/Roshanjossey/first-contributions&#34;&gt;https://github.com/Roshanjossey/first-contributions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Managing your Forked Repo: &lt;a href=&#34;https://help.github.com/articles/fork-a-repo/&#34;&gt;https://help.github.com/articles/fork-a-repo/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Syncing a Fork: &lt;a href=&#34;https://help.github.com/articles/syncing-a-fork/&#34;&gt;https://help.github.com/articles/syncing-a-fork/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Keep Your Fork Synced: &lt;a href=&#34;https://gist.github.com/CristinaSolana/1885435&#34;&gt;https://gist.github.com/CristinaSolana/1885435&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checkout this list for README examples - Awesome README &lt;a href=&#34;https://github.com/sindresorhus/awesome&#34;&gt;&lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Github-Flavored Markdown &lt;a href=&#34;https://guides.github.com/features/mastering-markdown/&#34;&gt;https://guides.github.com/features/mastering-markdown/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Additional references added by contributors&lt;/h2&gt; &#xA;&lt;p&gt;GitHub license explained &lt;a href=&#34;https://choosealicense.com&#34;&gt;https://choosealicense.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>DSXiangLi/DecryptPrompt</title>
    <updated>2023-10-15T01:46:43Z</updated>
    <id>tag:github.com,2023-10-15:/DSXiangLi/DecryptPrompt</id>
    <link href="https://github.com/DSXiangLi/DecryptPrompt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;总结Prompt&amp;LLM论文，开源数据&amp;模型，AIGC应用&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DecryptPrompt&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;如果LLM的突然到来让你感到沮丧，不妨读下主目录的Choose Your Weapon Survival Strategies for Depressed AI Academics 持续更新以下内容，Star to keep updated~&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;目录顺序如下&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;国内外，垂直领域大模型&lt;/li&gt; &#xA; &lt;li&gt;Agent和指令微调等训练框架&lt;/li&gt; &#xA; &lt;li&gt;开源指令，预训练，rlhf，对话，agent训练数据梳理&lt;/li&gt; &#xA; &lt;li&gt;AIGC相关应用&lt;/li&gt; &#xA; &lt;li&gt;prompt写作指南和5星博客等资源梳理&lt;/li&gt; &#xA; &lt;li&gt;Prompt和LLM论文细分方向梳理&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;My blogs &amp;amp; ChatGPT应用&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2215545?areaSource=&amp;amp;traceId=&#34;&gt;解密Prompt系列1. Tunning-Free Prompt：GPT2 &amp;amp; GPT3 &amp;amp; LAMA &amp;amp; AutoPrompt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2223355?areaSource=&amp;amp;traceId=&#34;&gt;解密Prompt系列2. 冻结Prompt微调LM： T5 &amp;amp; PET &amp;amp; LM-BFF&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2237259?areaSource=&amp;amp;traceId=&#34;&gt;解密Prompt系列3. 冻结LM微调Prompt: Prefix-tuning &amp;amp; Prompt-tuning &amp;amp; P-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2245094?areaSource=&amp;amp;traceId=&#34;&gt;解密Prompt系列4. 升级Instruction Tuning：Flan/T0/InstructGPT/TKInstruct&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2260697?areaSource=&amp;amp;traceId=&#34;&gt;解密prompt系列5. APE+SELF=自动化指令集构建代码实现&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2276508&#34;&gt;解密Prompt系列6. lora指令微调扣细节-请冷静,1个小时真不够~&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2289566?areaSource=&amp;amp;traceId=&#34;&gt;解密Prompt系列7. 偏好对齐RLHF-OpenAI·DeepMind·Anthropic对比分析&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2295783?areaSource=&amp;amp;traceId=&#34;&gt;解密Prompt系列8. 无需训练让LLM支持超长输入:知识库 &amp;amp; Unlimiformer &amp;amp; PCW &amp;amp; NBCE &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2296079?areaSource=&amp;amp;traceId=&#34;&gt;解密Prompt系列9. 模型复杂推理-思维链基础和进阶玩法&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2298660&#34;&gt;解密Prompt系列10. 思维链COT原理探究&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/old/2301999&#34;&gt;解密Prompt系列11. 小模型也能COT，先天不足后天补&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2305421&#34;&gt;解密Prompt系列12. LLM Agent零微调范式 ReAct &amp;amp; Self Ask&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2312674&#34;&gt;解密Prompt系列13. LLM Agent指令微调方案: Toolformer &amp;amp; Gorilla&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2319879&#34;&gt;解密Prompt系列14. LLM Agent之搜索应用设计：WebGPT &amp;amp; WebGLM &amp;amp; WebCPM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2328749&#34;&gt;解密Prompt系列15. LLM Agent之数据库应用设计：DIN &amp;amp; C3 &amp;amp; SQL-Palm &amp;amp; BIRD&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2333495&#34;&gt;解密Prompt系列16. LLM对齐经验之数据越少越好？LTD &amp;amp; LIMA &amp;amp; AlpaGasus&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/2338592&#34;&gt;解密Prompt系列17. LLM对齐方案再升级 WizardLM &amp;amp; BackTranslation &amp;amp; SELF-ALIGN&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LLMS&lt;/h2&gt; &#xA;&lt;h3&gt;模型评测&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;大模型评估尚未出现北极星指标，榜单排名往往和实际使用能力存在较大差异，几天没看感觉有的榜单快被玩坏了......&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;榜单&lt;/th&gt; &#xA;   &lt;th&gt;结果&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tatsu-lab.github.io/alpaca_eval/&#34;&gt;AlpacaEval：LLM-based automatic evaluation &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;开源模型王者vicuna,openchat, wizardlm&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;Huggingface Open LLM Leaderboard&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MMLU只评估开源模型，Falcon夺冠，在Eleuther AI4个评估集上评估的LLM模型榜单,vicuna夺冠&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lmsys.org/blog/2023-05-03-arena/&#34;&gt;Berkley出品大模型排位赛榜有准中文榜单&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Elo评分机制，GPT4自然是稳居第一，GPT4&amp;gt;Claude&amp;gt;GPT3.5&amp;gt;Vicuna&amp;gt;others&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zeno-ml/zeno-build&#34;&gt;CMU开源聊天机器人评测应用&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ChatGPT&amp;gt;Vicuna&amp;gt;others；在对话场景中训练可能很重要&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zhenbench/z-bench&#34;&gt;Z-Bench中文真格基金评测&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;国产中文模型的编程可用性还相对较低，大家水平差不太多，两版ChatGLM提升明显&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FranxYao/chain-of-thought-hub&#34;&gt;Chain-of-thought评估&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GSM8k, MATH等复杂问题排行榜&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;amp;mid=2651170429&amp;amp;idx=1&amp;amp;sn=b98af3bd14c9f97f1aa07f0f839bb3ec&amp;amp;scene=21#wechat_redirect&#34;&gt;InfoQ 大模型综合能力评估&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;面向中文，ChatGPT&amp;gt;文心一言&amp;gt; Claude&amp;gt;星火&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/ToolBench&#34;&gt;ToolBench: 工具调用评估榜单&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;工具微调模型和ChatGPT进行对比，提供评测脚本&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/AgentBench&#34;&gt;AgentBench: 推理决策评估榜单&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;清华联合多高校推出不同任务环境，例如购物，家居，操作系统等场景下模型推理决策能力&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://flageval.baai.ac.cn/#/home&#34;&gt;FlagEval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;智源出品主观+客观LLM评分榜单&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bird-bench.github.io/&#34;&gt;Bird-Bench&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;更贴合真实世界应用的超大数据库，需要领域知识的NL2SQL榜单，模型追赶人类尚有时日&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://103.238.162.37:31622/LeaderBoard&#34;&gt;kola&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;以世界知识为核心的评价基准，包括已知的百科知识和未知的近90天网络发布内容，评价知识记忆，理解，应用和创造能力&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://cevalbenchmark.com/index.html#home&#34;&gt;CEVAL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;中文知识评估，覆盖52个学科，机器评价主要为多项选择&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/haonan-li/CMMLU&#34;&gt;CMMLU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;67个主题中文知识和推理能力评估，多项选择机器评估&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;国外开源模型&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;模型链接&lt;/th&gt; &#xA;   &lt;th&gt;模型描述&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/llama/&#34;&gt;LLama2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Open Meta带着可商用开源的羊驼2模型来了~&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Alpaca前成员等开源以LLama13B为基础使用ShareGPT指令微调的模型，提出了用GPT4来评测模型效果&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nlpxucan/WizardLM&#34;&gt;WizardLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;微软新发布13B，登顶AlpacaEval开源模型Top3，使用ChatGPT对指令进行复杂度进化微调LLama2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/imoneoi/openchat&#34;&gt;OpenChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;80k ShareGPT对话微调LLama-2 13B开源模型中的战斗机&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/KBlueLeaf/guanaco-7B-leh&#34;&gt;Guanaco&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama 7B基座，在alpaca52K数据上加入534K多语言指令数据微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Meta开源指令微调LLM，规模70 亿到 650 亿不等&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b-chat&#34;&gt;MPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MosaicML开源的预训练+指令微调的新模型，可商用，支持84k tokens超长输入&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-40b&#34;&gt;Falcon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Falcon由阿联酋技术研究所在超高质量1万亿Token上训练得到1B，7B，40B开源，免费商用！土豪们表示钱什么的格局小了&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1&#34;&gt;RedPajama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RedPajama项目既开源预训练数据后开源3B，7B的预训练+指令微调模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bair.berkeley.edu/blog/2023/04/03/koala/&#34;&gt;koala&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;使用alpaca，HC3等开源指令集+ ShareGPT等ChatGPT数据微调llama，在榜单上排名较高&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllama&#34;&gt;ChatLLaMA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;基于RLHF微调了LLaMA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;斯坦福开源的使用52k数据在7B的LLaMA上微调得到，&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;Alpaca-lora&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LORA微调的LLaMA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IBM/Dromedary&#34;&gt;Dromedary&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IBM self-aligned model with the LLaMA base&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;ColossalChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HPC-AI Tech开源的Llama+RLHF微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna+BLIP2 文本视觉融合&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/trl-lib/llama-7b-se-rl-peft&#34;&gt;StackLLama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama使用Stackexchange数据+SFT+RL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/cerebras/Cerebras-GPT-13B&#34;&gt;Cerebras&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Cerebras开源了1亿到130亿的7个模型，从预训练数据到参数全开源&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://palm-e.github.io&#34;&gt;PaLM-E&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;谷歌多模态大模型，540B的PaLM语言模型和22B的ViT视觉模型相结合，得到562B的PaLM-E模型，在机器人应用场景有了新的突破&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-7b&#34;&gt;Dolly-v2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;可商用 7b指令微调开源模型在GPT-J-6B上微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/togethercomputer/OpenChatKit&#34;&gt;OpenChatKit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;openai研究员打造GPT-NoX-20B微调+6B审核模型过滤&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/unilm&#34;&gt;MetaLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;微软开源的大规模自监督预训练模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://aws.amazon.com/cn/bedrock/titan/&#34;&gt;Amazon Titan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;亚马逊在aws上增加自家大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/metaseq/tree/main/projects/OPT&#34;&gt;OPT-IML&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Meta复刻GPT3，up to 175B, 不过效果并不及GPT3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;Bloom&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BigScience出品，规模最大176B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloomz&#34;&gt;BloomZ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BigScience出品, 基于Bloom微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/paperswithcode/galai&#34;&gt;Galacia&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;和Bloom相似，更针对科研领域训练的模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bigscience-workshop/t-zero&#34;&gt;T0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;BigScience出品，3B~11B的在T5进行指令微调的模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/turboderp/exllama&#34;&gt;EXLLama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weight&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/lmsys/longchat-13b-16k&#34;&gt;LongChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;llama-13b使用condensing rotary embedding technique微调的长文本模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-30b&#34;&gt;MPT-30B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MosaicML开源的在8Ktoken上训练的大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;国内开源模型&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;模型链接&lt;/th&gt; &#xA;   &lt;th&gt;模型描述&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thudm/chatglm2-6b&#34;&gt;ChatGLM2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;32K长文本，FlashAttention+Multi-Query Attenion的显存优化，更强推理能力，哈哈不过很多简单问题也硬要COT，中英平行能力似乎略有下降的ChatGLM2，但是免费商用！&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM-6B&#34;&gt;ChatGLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;清华开源的、支持中英双语的对话语言模型，使用了代码训练，指令微调和RLHF。chatglm2支持超长文本，可免费商用啦！&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FlagAlpha/Llama2-Chinese&#34;&gt;LLama2-chinese&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;没等太久中文预训练微调后的llama2它来了~&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/RUC-GSAI/YuLan-Chat&#34;&gt;YuLan-chat2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;高瓴人工智能基于Llama-2中英双语继续预训练+指令微调/对话微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-7B-Reward&#34;&gt;ziya&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;IDEA研究院在7B/13B llama上继续预训练+SFT+RM+PPO+HFTT+COHFT+RBRS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/baichuan-inc/baichuan-7B&#34;&gt;Baichuan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;百川智能开源7B大模型可商用免费&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan2&#34;&gt;Baichuan2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;百川第二代，提供了7B/13B Base和chat的版本&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Xwin-LM/Xwin-LM&#34;&gt;XWin-LM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;llama2 + SFT + RLHF&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese-LLaMA-Alpaca&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;哈工大中文指令微调的LLaMA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenLMLab/MOSS&#34;&gt;Moss&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;为复旦正名！开源了预训练，指令微调的全部数据和模型。可商用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen-7B&#34;&gt;Qwen-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;阿里开源，可商用，通义千文7B模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM&#34;&gt;IntenrLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;书生浦语在过万亿 token 数据上训练的多语千亿参数基座模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FlagAI-Open/Aquila2/raw/main/README_CN.md&#34;&gt;Aquila2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;智源更新Aquila2模型系列包括全新34B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila&#34;&gt;Aquila&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;智源开源7B大模型可商用免费&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.moonshot.cn/?ref=aihub.cn&#34;&gt;kimi Chat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moonshot上线超长文本LLM 可输入20W上文需要申请试用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dandelionsllm/pandallm&#34;&gt;PandaLLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLAMA2上中文wiki继续预训练+COIG指令微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xverse-ai/XVERSE-13B&#34;&gt;XVERSE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;据说中文超越llama2的元象开源模型13B模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Neutralzz/BiLLa&#34;&gt;BiLLa&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama词表扩充预训练+预训练和任务1比1混合SFT+指令样本SFT三阶段训练&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FreedomIntelligence/LLMZoo&#34;&gt;Phoenix&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;港中文开源凤凰和奇美拉LLM，Bloom基座，40+语言支持&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/GanjinZero/wombat-7b-delta&#34;&gt;Wombat-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;达摩院开源无需强化学习使用RRHF对齐的语言模型, alpaca基座&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/TigerResearch/TigerBot&#34;&gt;TigerBot&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;虎博开源了7B 180B的模型以及预训练和微调语料&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LC1332/Luotuo-Chinese-LLM&#34;&gt;Luotuo&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;中文指令微调的LLaMA，和ChatGLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBuddy/OpenBuddy&#34;&gt;OpenBuddy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Llama 多语言对话微调模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Facico/Chinese-Vicuna&#34;&gt;Chinese Vincuna&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama 7B基座，使用Belle+Guanaco数据训练&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CVI-SZU/Linly&#34;&gt;Linly&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Llama 7B基座，使用belle+guanaco+pclue+firefly+CSL+newscommentary等7个指令微调数据集训练&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yangjianxin1/Firefly&#34;&gt;Firefly&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;中文2.6B模型，提升模型中文写作，古文能力，待开源全部训练代码，当前只有模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/project-baize/baize-chatbot&#34;&gt;Baize&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;使用100k self-chat对话数据微调的LLama&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;BELLE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;使用ChatGPT生成数据对开源模型进行中文优化&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/search?q=chatyuan&amp;amp;type=repositories&#34;&gt;Chatyuan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;chatgpt出来后最早的国内开源对话模型，T5架构是下面PromptCLUE的衍生模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/clue-ai/PromptCLUE&#34;&gt;PromptCLUE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;多任务Prompt语言模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.alice-mind.com/portal#/&#34;&gt;PLUG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;阿里达摩院发布的大模型，提交申请会给下载链接&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://baai.ac.cn/&#34;&gt;CPM2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;智源发布CPM2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/GLM-130B&#34;&gt;GLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;清华发布的中英双语130B预训练模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ictnlp/BayLing&#34;&gt;BayLing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;基于LLama7B/13B，增强的语言对齐的英语/中文大语言模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;垂直领域模型&amp;amp;进展&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;领域&lt;/th&gt; &#xA;   &lt;th&gt;模型链接&lt;/th&gt; &#xA;   &lt;th&gt;模型描述&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://medgpt.co/home/zh&#34;&gt;MedGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;医联发布的&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;MedPalm&lt;/td&gt; &#xA;   &lt;td&gt;Google在Faln-PaLM的基础上通过多种类型的医疗QA数据进行prompt-tuning指令微调得到，同时构建了MultiMedQA&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Kent0n-Li/ChatDoctor&#34;&gt;ChatDoctor&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;110K真实医患对话样本+5KChatGPT生成数据进行指令微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese&#34;&gt;Huatuo&lt;/a&gt; &lt;a href=&#34;https://github.com/SCIR-HI/Med-ChatGLM&#34;&gt;Med-ChatGLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;医学知识图谱和chatgpt构建中文医学指令数据集+医学文献和chatgpt构建多轮问答数据&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Facico/Chinese-Vicuna/raw/master/docs/performance-medical.md&#34;&gt;Chinese-vicuna-med&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chinese-vicuna在cMedQA2数据上微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BioFM/OpenBioMed&#34;&gt;OpenBioMed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;清华AIR开源轻量版BioMedGPT, 知识图谱&amp;amp;20+生物研究领域多模态预训练模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/xionghonglin/DoctorGLM&#34;&gt;DoctorGLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ChatDoctor+MedDialog+CMD 多轮对话+单轮指令样本微调GLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/MediaBrain-SJTU/MedicalGPT-zh&#34;&gt;MedicalGPT-zh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;自建的医学数据库ChatGPT生成QA+16个情境下SELF构建情景对话&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/chaoyi-wu/PMC-LLaMA&#34;&gt;PMC-LLaMA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;医疗论文微调Llama&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/openmedlab/PULSE&#34;&gt;PULSE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Bloom微调+继续预训练&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CogStack/OpenGPT/tree/main&#34;&gt;NHS-LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chatgpt生成的医疗问答，对话，微调模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/michael-wzhu/ShenNong-TCM-LLM&#34;&gt;神农医疗大模型&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;以中医知识图谱的实体为中心生成的中医知识指令数据集11w+，微调LLama-7B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;岐黄问道大模型&lt;/td&gt; &#xA;   &lt;td&gt;3个子模型构成，已确诊疾病的临床治疗模型+基于症状的临床诊疗模型+中医养生条理模型，看起来是要ToB落地&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SupritYoung/Zhongjing&#34;&gt;Zhongjing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;基于Ziya-LLama+医疗预训练+SFT+RLHF的中文医学大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/qiuhuachuan/smile&#34;&gt;MeChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;心理咨询领域，通过chatgpt改写多轮对话56k&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/scutcyr/SoulChat&#34;&gt;SoulChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;心理咨询领域中文长文本指令与多轮共情对话数据联合指令微调 ChatGLM-6B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/X-D-Lab/MindChat&#34;&gt;MindChat&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MindChat-Baichuan-13B,Qwen-7B,MindChat-InternLM-7B使用不同基座在模型安全，共情，人类价值观对其上进行了强化&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;医疗&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FudanDISC/DISC-MedLLM&#34;&gt;DISC-MedLLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;疾病知识图谱构建QA对+QA对转化成单论对话+真实世界数据重构+人类偏好数据筛选，SFT微调baichuan&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;法律&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LiuHC0428/LAW-GPT&#34;&gt;LawGPT-zh&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;利用ChatGPT清洗CrimeKgAssitant数据集得到52k单轮问答+我们根据中华人民共和国法律手册上最核心的9k法律条文，利用ChatGPT联想生成具体的情景问答+知识问答使用ChatGPT基于文本构建QA对&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;法律&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/pengxiao-song/LaWGPT&#34;&gt;LawGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;基于llama+扩充词表二次预训练+基于法律条款构建QA指令微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;法律&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AndrewZhe/lawyer-llama&#34;&gt;Lawyer Llama&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;法律指令微调数据集：咨询+法律考试+对话进行指令微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;法律&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CSHaitao/LexiLaw&#34;&gt;LexiLaw&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;法律指令微调数据集：问答+书籍概念解释，法条内容进行指令微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;法律&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chatlaw.cloud/&#34;&gt;ChatLaw&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;北大推出的法律大模型，应用形式很新颖类似频道内流一切功能皆融合在对话形式内&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;法律&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zhihaiLLM/wisdomInterrogatory&#34;&gt;录问模型&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;在baichuan基础上40G二次预训练+100K指令微调，在知识库构建上采用了Emb+意图+关键词联想结合的方案&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://finchat.io/&#34;&gt;FinChat.io&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;使用最新的财务数据，电话会议记录，季度和年度报告，投资书籍等进行训练&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CogStack/OpenGPT&#34;&gt;OpenGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;领域LLM指令样本生成+微调框架&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ssymmetry/BBT-FinCUGE-Applications/tree/main&#34;&gt;乾元BigBang金融2亿模型&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;金融领域预训练+任务微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/xyz-nlp/XuanYuan2.0&#34;&gt;度小满千亿金融大模型&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;在Bloom-176B的基础上进行金融+中文预训练和微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ltxtrading.com/bondgpt&#34;&gt;bondGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPT4在细分债券市场的应用开放申请中&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cnbc.com/2023/05/25/jpmorgan-develops-ai-investment-advisor.html&#34;&gt;IndexGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;JPMorgan在研的生成式投资顾问&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/vLvxvi2nOywkjt7ppiFC2g&#34;&gt;恒生LightGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;金融领域继续预训练+插件化设计&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://finance.sina.com.cn/jjxw/2023-07-03/doc-imyzmaut2132017.shtml&#34;&gt;知彼阿尔法&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;企查查商查大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.alphabox.top&#34;&gt;AlphaBox&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;熵简科技发布大模型金融应用，多文档问答+会议转录+文档编辑&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.datagrand.com/products/aigc/&#34;&gt;曹植&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;达观发布金融大模型融合data2text等金融任务，赋能报告写作&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese&#34;&gt;聚宝盆&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;基于 LLaMA 系基模型经过中文金融知识指令精调/指令微调(Instruct-tuning) 的微调模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/chancefocus/PIXIU&#34;&gt;PIXIU&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;整理了多个金融任务数据集加入了时间序列数据进行指令微调&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://chat.funddb.cn/&#34;&gt;ChatFund&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;韭圈儿发布的第一个基金大模型，看起来是做了多任务指令微调，和APP已有的数据功能进行了全方位的打通，从选基，到持仓分析等等&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AI4Finance-Foundation/FinGPT&#34;&gt;FinGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;金融传统任务微调 or chatgpt生成金融工具调用&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;金融&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/TongjiFinLab/CFGPT&#34;&gt;CFGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;金融预训练+指令微调+RAG等检索任务增强&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;编程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bigcode-project/starcoder&#34;&gt;Starcoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;80种编程语言+Issue+Commit训练得到的编程大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;编程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cubenlp/ChatSQL&#34;&gt;ChatSQL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;基于ChatGLM实现NL2sql&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;编程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://keg.cs.tsinghua.edu.cn/codegeex/index_zh.html&#34;&gt;codegeex&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;13B预训练+微调多语言变成大模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;编程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/CodeGeeX2&#34;&gt;codegeex2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Chatglm2的基础上CodeGeeX2-6B 进一步经过了 600B 代码数据预训练&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;编程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://stability.ai/blog/stablecode-llm-generative-ai-coding&#34;&gt;stabelcode&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;560B token多语言预训练+ 120,000 个 Alpaca指令对齐&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;编程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/defog-ai/sqlcoder&#34;&gt;SQLCoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;在StarCoder的基础上微调15B超越gpt3.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;数学&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.mathgpt.com/&#34;&gt;MathGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;是好未来自主研发的，面向全球数学爱好者和科研机构，以解题和讲题算法为核心的大模型。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;数学&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://tiger-ai-lab.github.io/MAmmoTH/&#34;&gt;MammoTH&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;通过COT+POT构建了MathInstruct数据集微调llama在OOD数据集上超越了WizardLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;交通&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/DUOMO/TransGPT&#34;&gt;TransGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama-7B+34.6万领域预训练+5.8万条领域指令对话微调（来自文档问答）&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;交通&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lijlansg/TrafficGPT/tree/main&#34;&gt;TrafficGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ChatGPT+Prompt实现规划，调用交通流量领域专业TFM模型，TFM负责数据分析，任务执行，可视化等操作&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;科技&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/gmftbyGMFTBY/science-llm&#34;&gt;Mozi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;红睡衣预训练+论文QA数据集 + ChatGPT扩充科研对话数据&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;天文&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Yu-Yang-Li/StarGLM&#34;&gt;StarGLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;天文知识指令微调，项目进行中后期考虑天文二次预训练+KG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;写作&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.zhihu.com/question/613058630&#34;&gt;阅文-网文大模型介绍&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;签约作者内测中，主打的内容为打斗场景，剧情切换，环境描写，人设，世界观等辅助片段的生成&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;写作&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/search?q=MediaGPT&amp;amp;type=repositories&#34;&gt;MediaGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLama-7B扩充词表+指令微调，指令来自国内媒体专家给出的在新闻创作上的80个子任务&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;电商&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Alibaba-NLP/EcomGPT&#34;&gt;EcomGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;电商领域任务指令微调大模型，指令样本250万，基座模型是Bloomz&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Tool and Library&lt;/h2&gt; &#xA;&lt;h3&gt;推理框架&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;工具描述&lt;/th&gt; &#xA;   &lt;th&gt;链接&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FlexFlow：模型部署推理框架&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/flexflow/FlexFlow&#34;&gt;https://github.com/flexflow/FlexFlow&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Medusa：针对采样解码的推理加速框架，可以和其他策略结合&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FasterDecoding/Medusa&#34;&gt;https://github.com/FasterDecoding/Medusa&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FlexGen: LLM推理 CPU Offload计算架构&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FMInference/FlexGen&#34;&gt;https://github.com/FMInference/FlexGen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VLLM：超高速推理框架Vicuna，Arena背后的无名英雄，比HF快24倍，支持很多基座模型&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;https://github.com/vllm-project/vllm&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Streamingllm: 新注意力池Attention方案，无需微调拓展模型推理长度，同时为推理提速&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm&#34;&gt;https://github.com/mit-han-lab/streaming-llm&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;指令微调，预训练，rlhf框架&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;工具描述&lt;/th&gt; &#xA;   &lt;th&gt;链接&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA：Low-Rank指令微调方案&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;peft：parameter-efficient prompt tunnging工具集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;https://github.com/huggingface/peft&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RL4LMs：AllenAI的RL工具&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/RL4LMs&#34;&gt;https://github.com/allenai/RL4LMs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLLTE：港大，大疆等联合开源RLLTE开源学习框架&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/RLE-Foundation/rllte&#34;&gt;https://github.com/RLE-Foundation/rllte&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;trl：基于Transformer的强化训练框架&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lvwerra/trl&#34;&gt;https://github.com/lvwerra/trl&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;trlx：分布式训练trl&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CarperAI/trlx&#34;&gt;https://github.com/CarperAI/trlx&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;北大开源河狸项目可复现RLHF，支持多数LLM，提供RLHF数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PKU-Alignment/safe-rlhf&#34;&gt;https://github.com/PKU-Alignment/safe-rlhf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RL4LMs：AllenAI的RL工具&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/RL4LMs&#34;&gt;https://github.com/allenai/RL4LMs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LMFlow：港科大实验室开源的大模型微调框架，支持以上多数开源模型的指令微调和RLHF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OptimalScale/LMFlow&#34;&gt;https://github.com/OptimalScale/LMFlow&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hugNLP:基于Huggingface开发继承Prompt技术，预训练和是指输入等多种方案&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/wjn1996/HugNLP&#34;&gt;https://github.com/wjn1996/HugNLP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deepspeed：针对RL训练和推理的整合优化&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;https://github.com/microsoft/DeepSpeed&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Uerpy:预训练框架支持lm,mlm,unilm等&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dbiir/UER-py&#34;&gt;https://github.com/dbiir/UER-py&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TecentPretrain: Uerpy的重构版本支持llama预训练&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Tencent/TencentPretrain/tree/main&#34;&gt;https://github.com/Tencent/TencentPretrain/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lamini: 整合指令数据生成，SFT，RLHF的工具库&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/lamini-ai/lamini/&#34;&gt;https://github.com/lamini-ai/lamini/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chain-of-thought-hub：模型推理能力评估平台&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FranxYao/chain-of-thought-hub&#34;&gt;https://github.com/FranxYao/chain-of-thought-hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EasyEdit：浙大开源支持多种模型，多种方案的模型知识精准编辑器&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zjunlp/EasyEdit&#34;&gt;https://github.com/zjunlp/EasyEdit&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenDelta：集成了各种增量微调方案的开源实现&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/OpenDelta&#34;&gt;https://github.com/thunlp/OpenDelta&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;LLM Agent工具&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;工具描述&lt;/th&gt; &#xA;   &lt;th&gt;链接&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;langchain：LLM Agent框架&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;https://github.com/hwchase17/langchain&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;llama index：LLM Agent框架&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jerryjliu/llama_index&#34;&gt;https://github.com/jerryjliu/llama_index&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;semantic-kernel：整合大模型和编程语言的SDK&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/semantic-kernel&#34;&gt;https://github.com/microsoft/semantic-kernel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BMTTools: 清华出品多工具调用开源库，提供微调数据和评估ToolBench&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/BMTools&#34;&gt;https://github.com/OpenBMB/BMTools&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BabyAGI：自执行LLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yoheinakajima/babyagi&#34;&gt;https://github.com/yoheinakajima/babyagi&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AutoGPT：自执行LLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Torantulino/Auto-GPT&#34;&gt;https://github.com/Torantulino/Auto-GPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MetaGPT: 覆盖软件公司全生命流程，例如产品经理等各个职业的AutoGPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/geekan/MetaGPT&#34;&gt;https://github.com/geekan/MetaGPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ResearchGPT: 论文写作领域的AutoGPT，融合论文拆解+网络爬虫&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/assafelovic/gpt-researcher&#34;&gt;https://github.com/assafelovic/gpt-researcher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MiniAGI：自执行LLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/muellerberndt/mini-agi&#34;&gt;https://github.com/muellerberndt/mini-agi&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AL Legion： 自执行LLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/eumemic/ai-legion&#34;&gt;https://github.com/eumemic/ai-legion&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AgentVerse：多模型交互环境&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/AgentVerse&#34;&gt;https://github.com/OpenBMB/AgentVerse&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AgentSims: 给定一个社会环境，评估LLM作为智能体的预定任务目标完成能力的沙盒环境&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/py499372727/AgentSims/&#34;&gt;https://github.com/py499372727/AgentSims/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPTRPG：RPG环境 AI Agent游戏化&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dzoba/gptrpg&#34;&gt;https://github.com/dzoba/gptrpg&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Generative Agents:斯坦福AI小镇的开源代码&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/joonspk-research/generative_agents&#34;&gt;https://github.com/joonspk-research/generative_agents&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPTeam：多智能体交互&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/101dotxyz/GPTeam&#34;&gt;https://github.com/101dotxyz/GPTeam&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPTEngineer：自动工具构建和代码生成&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/AntonOsika/gpt-engineer&#34;&gt;https://github.com/AntonOsika/gpt-engineer&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Jarvis: 大模型调用小模型框架，给小模型一个未来！&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/search?q=jarvis&#34;&gt;https://github.com/search?q=jarvis&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLM-ToolMaker:让LLM自己制造Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FMInference/FlexGen&#34;&gt;https://github.com/FMInference/FlexGen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gorilla: LLM调用大量API&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ShishirPatil/gorilla&#34;&gt;https://github.com/ShishirPatil/gorilla&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IncarnaMind：多文档RAG方案，动态chunking的方案可以借鉴&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/junruxiong/IncarnaMind&#34;&gt;https://github.com/junruxiong/IncarnaMind&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wenda:闻达小模型整合搜索用于知识融入&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/l15y/wenda&#34;&gt;https://github.com/l15y/wenda&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WorkGPT：类似AutoGPT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/team-openpm/workgpt&#34;&gt;https://github.com/team-openpm/workgpt&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deep-KE：基于LLM对数据进行智能解析实现知识抽取&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zjunlp/DeepKE&#34;&gt;https://github.com/zjunlp/DeepKE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vectra：平台化的LLM Agent搭建方案，从索引构建，内容召回排序，到事实检查的LLM生成&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://vectara.com/tour-vectara/&#34;&gt;https://vectara.com/tour-vectara/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alexandria: 从Arix论文开始把整个互联网变成向量索引，可以免费下载&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://alex.macrocosm.so/download&#34;&gt;https://alex.macrocosm.so/download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RapidAPI: 统一这个世界的所有API，最大API Hub，有调用成功率，latency等，是真爱！&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://rapidapi.com/hub&#34;&gt;https://rapidapi.com/hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Data-Copilot：时间序列等结构化数据分析领域的Agent解决方案&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zwq2018/Data-Copilot&#34;&gt;https://github.com/zwq2018/Data-Copilot&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DB-GPT: 以数据库为基础的GPT实验项目，使用本地化的GPT大模型与您的数据和环境进行交互&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-cn/zh_CN/latest/index.html&#34;&gt;https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-cn/zh_CN/latest/index.html&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;guardrails：降低模型幻觉的python框架，promp模板+validation+修正&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/shreyar/guardrails&#34;&gt;https://github.com/shreyar/guardrails&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;guidance：微软新开源框架，同样是降低模型幻觉的框架，prompt+chain的升级版加入逐步生成和思维链路&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/guidance-ai/guidance&#34;&gt;https://github.com/guidance-ai/guidance&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ragas: 评估检索增强LLM效果的框架，基于大模型prompt评估事实性，召回相关性，召回内容质量，回答相关性等&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/explodinggradients/ragas#fire-quickstart&#34;&gt;https://github.com/explodinggradients/ragas#fire-quickstart&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;langflow：把langchain等agent组件做成了可拖拽式的UI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/logspace-ai/langflow&#34;&gt;https://github.com/logspace-ai/langflow&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Haystack: LLM Agent 框架，pipeline的设计模式个人感觉比langchain更灵活更简洁&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/deepset-ai/haystack&#34;&gt;https://github.com/deepset-ai/haystack&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;EdgeChain: 通过Jsonnet配置文件实现LLM Agent&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/arakoodev/EdgeChains/tree/main&#34;&gt;https://github.com/arakoodev/EdgeChains/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YiVal:调整和评估prompts、配置和模型参数的开源GenAI-Ops工具&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/YiVal/YiVal&#34;&gt;https://github.com/YiVal/YiVal&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Training Data&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;数据类型&lt;/th&gt; &#xA;   &lt;th&gt;数据描述&lt;/th&gt; &#xA;   &lt;th&gt;数据链接&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;self-instruct，GPT3自动生成&amp;amp;过滤得到指令集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yizhongw/self-instruct&#34;&gt;https://github.com/yizhongw/self-instruct&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;Standford Alpaca：52K text-davinci-003生成的self-instruct指令数据集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;GPT4-for-LLM 中文+英文+对比指令&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;GPTTeacher更多样的通用指令，角色扮演和代码指令&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/teknium1/GPTeacher/tree/main&#34;&gt;https://github.com/teknium1/GPTeacher/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;中文翻译Alpaca还有一些其他指令数据集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hikariming/alpaca_chinese_dataset&#34;&gt;https://github.com/hikariming/alpaca_chinese_dataset&lt;/a&gt; &lt;a href=&#34;https://github.com/carbonz0/alpaca-chinese-dataset&#34;&gt;https://github.com/carbonz0/alpaca-chinese-dataset&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;alpaca指令GPT4生成，和以上几版对比显著质量更高，回复更长&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/tree/main&#34;&gt;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;Guanaco数据：对Alphca指令重写后以不同语言生成总共534K，有对话和非对话类型，还有补充的QA生成样本&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&#34;&gt;https://huggingface.co/datasets/JosephusCheung/GuanacoDataset&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;OIG中文指令包括翻译alpaca+natural+unnatural，多轮对话，考试，leetcode指令&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BAAI-Zlab/COIG&#34;&gt;https://github.com/BAAI-Zlab/COIG&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;Vicuna训练使用的样本，用API获取了sharegpt上用户和chatgpt对话历史，部分网友整理到了HF&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/domeccleston/sharegpt&#34;&gt;https://github.com/domeccleston/sharegpt&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main&#34;&gt;https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;HC3指令数据中英文，包括金融，开放QA，百科，DBQA，医学等包含人工回复&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main&#34;&gt;https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;MOSS开源的SFT数据包含使用plugin的对话数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main&#34;&gt;https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;InstructWild数据：用四处爬取的chatgpt指令作为种子self-instruct扩充生成，中英双语&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/XueFuzhao/InstructionWild/tree/main/data&#34;&gt;https://github.com/XueFuzhao/InstructionWild/tree/main/data&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;BELLE100万指令数据，参考Alpaca用ChatGPT生成，有数学，多轮对话，校色对话等等&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LianjiaTech/BELLE&#34;&gt;https://github.com/LianjiaTech/BELLE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;PromptCLUE多任务提示数据集：模板构建，只包含标准NLP任务&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CLUEbenchmark/pCLUE&#34;&gt;https://github.com/CLUEbenchmark/pCLUE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;TK-Instruct微调用的指令数据集, 全人工标注1600+NLP任务&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://instructions.apps.allenai.org/&#34;&gt;https://instructions.apps.allenai.org/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;T0微调用的指令数据集（P3）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigscience/P3&#34;&gt;https://huggingface.co/datasets/bigscience/P3&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;p3衍生的46种多语言数据集（xmtf）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/bigscience-workshop/xmtf&#34;&gt;https://github.com/bigscience-workshop/xmtf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;Unnatural Instruction使用GPT3生成后改写得到240k&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/orhonovich/unnatural-instructions&#34;&gt;https://github.com/orhonovich/unnatural-instructions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;alpaca COT对多个数据源进行了清理并统一格式放到的了HF, 重点是人工整理的COT数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PhoebusSi/Alpaca-CoT&#34;&gt;https://github.com/PhoebusSi/Alpaca-CoT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;人工编写包含23种常见的中文NLP任务的指令数据，中文写作方向&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/yangjianxin1/Firefly&#34;&gt;https://github.com/yangjianxin1/Firefly&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;Amazon COT指令样本包括各类QA，bigbench，math等&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/amazon-science/auto-cot&#34;&gt;https://github.com/amazon-science/auto-cot&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;CSL包含 396,209 篇中文核心期刊论文元信息 （标题、摘要、关键词、学科、门类）可做预训练可构建NLP指令任务&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ydli-ai/CSL&#34;&gt;https://github.com/ydli-ai/CSL&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;alpaca code 20K代码指令数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sahil280114/codealpaca#data-release&#34;&gt;https://github.com/sahil280114/codealpaca#data-release&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;GPT4Tools 71K GPT4指令样本&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/StevenGrove/GPT4Tools&#34;&gt;https://github.com/StevenGrove/GPT4Tools&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;GPT4指令+角色扮演+代码指令&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/teknium1/GPTeacher&#34;&gt;https://github.com/teknium1/GPTeacher&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;指令微调&lt;/td&gt; &#xA;   &lt;td&gt;Mol-Instructions 2043K 分子+蛋白质+生物分子文本指令，覆盖分子设计、蛋白质功能预测、蛋白质设计等任务&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zjunlp/Mol-Instructions&#34;&gt;https://github.com/zjunlp/Mol-Instructions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;数学&lt;/td&gt; &#xA;   &lt;td&gt;腾讯人工智能实验室发布网上爬取的数学问题APE210k&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Chenny0808/ape210k&#34;&gt;https://github.com/Chenny0808/ape210k&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;数学&lt;/td&gt; &#xA;   &lt;td&gt;猿辅导 AI Lab开源小学应用题Math23K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/SCNU203/Math23k/tree/main&#34;&gt;https://github.com/SCNU203/Math23k/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;数学&lt;/td&gt; &#xA;   &lt;td&gt;grade school math把OpenAI的高中数学题有改造成指令样本有2-8步推理过程&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions&#34;&gt;https://huggingface.co/datasets/qwedsacf/grade-school-math-instructions&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;数学&lt;/td&gt; &#xA;   &lt;td&gt;数学问答数据集有推理过程和多项选择&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/math_qa/viewer/default/test?row=2&#34;&gt;https://huggingface.co/datasets/math_qa/viewer/default/test?row=2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;数学&lt;/td&gt; &#xA;   &lt;td&gt;AMC竞赛数学题&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/competition_math&#34;&gt;https://huggingface.co/datasets/competition_math&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;数学&lt;/td&gt; &#xA;   &lt;td&gt;线性代数等纯数学计算题&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/math_dataset&#34;&gt;https://huggingface.co/datasets/math_dataset&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;代码&lt;/td&gt; &#xA;   &lt;td&gt;APPS从不同的开放访问编码网站Codeforces、Kattis 等收集的问题&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/APPS&#34;&gt;https://opendatalab.org.cn/APPS&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;代码&lt;/td&gt; &#xA;   &lt;td&gt;Lyra代码由带有嵌入式 SQL 的 Python 代码组成，经过仔细注释的数据库操作程序，配有中文评论和英文评论。&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/Lyra&#34;&gt;https://opendatalab.org.cn/Lyra&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;代码&lt;/td&gt; &#xA;   &lt;td&gt;Conala来自StackOverflow问题,手动注释3k，英文&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/CoNaLa/download&#34;&gt;https://opendatalab.org.cn/CoNaLa/download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;代码&lt;/td&gt; &#xA;   &lt;td&gt;code-alpaca ChatGPT生成20K代码指令样本&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sahil280114/codealpaca.git&#34;&gt;https://github.com/sahil280114/codealpaca.git&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;代码&lt;/td&gt; &#xA;   &lt;td&gt;32K, 四种不同类型、不同难度的代码相关中文对话数据，有大模型生成，&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/zxx000728/CodeGPT&#34;&gt;https://github.com/zxx000728/CodeGPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话&lt;/td&gt; &#xA;   &lt;td&gt;LAION 策划的开放指令通用数据集中手动选择的组件子集 已开源40M 3万个,100M在路上&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/LAION-AI/Open-Instruction-Generalist&#34;&gt;https://github.com/LAION-AI/Open-Instruction-Generalist&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话&lt;/td&gt; &#xA;   &lt;td&gt;Baize基于Chat GPT构建的self-chat数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/project-baize/baize-chatbot/tree/main/data&#34;&gt;https://github.com/project-baize/baize-chatbot/tree/main/data&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话&lt;/td&gt; &#xA;   &lt;td&gt;FaceBook开源BlenderBot训练对话数据~6K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/blended_skill_talk&#34;&gt;https://huggingface.co/datasets/blended_skill_talk&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话&lt;/td&gt; &#xA;   &lt;td&gt;AllenAI开源38.5万个对话高质量数据集SODA&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://realtoxicityprompts.apps.allenai.org/&#34;&gt;https://realtoxicityprompts.apps.allenai.org/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话&lt;/td&gt; &#xA;   &lt;td&gt;InstructDial在单一对话任务类型上进行指令微调&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/prakharguptaz/Instructdial&#34;&gt;https://github.com/prakharguptaz/Instructdial&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话&lt;/td&gt; &#xA;   &lt;td&gt;Ultra Chat 两个独立的 ChatGPT Turbo API 进行对话，从而生成多轮对话数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/UltraChat&#34;&gt;https://github.com/thunlp/UltraChat&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话&lt;/td&gt; &#xA;   &lt;td&gt;Awesome Open-domain Dialogue Models提供多个开放域对话数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cingtiye/Awesome-Open-domain-Dialogue-Models#%E4%B8%AD%E6%96%87%E5%BC%80%E6%94%BE%E5%9F%9F%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86&#34;&gt;https://github.com/cingtiye/Awesome-Open-domain-Dialogue-Models#%E4%B8%AD%E6%96%87%E5%BC%80%E6%94%BE%E5%9F%9F%E5%AF%B9%E8%AF%9D%E6%95%B0%E6%8D%AE%E9%9B%86&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话&lt;/td&gt; &#xA;   &lt;td&gt;Salesforce开源超全DialogStudio&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/salesforce/DialogStudio&#34;&gt;https://github.com/salesforce/DialogStudio&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话&lt;/td&gt; &#xA;   &lt;td&gt;基于事实Reference的多轮问答中文数据，已开源5万，之后会开源更多&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/sufengniu/RefGPT&#34;&gt;https://github.com/sufengniu/RefGPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLFH&lt;/td&gt; &#xA;   &lt;td&gt;北大河狸开源RLHF数据集10K，1M需要申请&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K&#34;&gt;https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-10K&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;Anthropic hh-rlhf数据集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/Anthropic/hh-rlhf&#34;&gt;https://huggingface.co/datasets/Anthropic/hh-rlhf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;Stack-exchange上问题对应多个答案，每个答案有打分&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences/tree/main&#34;&gt;https://huggingface.co/datasets/HuggingFaceH4/stack-exchange-preferences/tree/main&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;Facebook Bot Adversarial Dialogues数据集5K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI&#34;&gt;https://github.com/facebookresearch/ParlAI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;AllenAI Real Toxicity prompts&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebookresearch/ParlAI&#34;&gt;https://github.com/facebookresearch/ParlAI&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;OpenAssistant Conversations 160K消息，13500人工生成, 英文为主&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;https://huggingface.co/datasets/OpenAssistant/oasst1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;知乎问答偏好数据集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k&#34;&gt;https://huggingface.co/datasets/liyucheng/zhihu_rlhf_3k&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;hh-rlhf中文翻译偏好数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liswei/rm-static-zhTW&#34;&gt;https://huggingface.co/datasets/liswei/rm-static-zhTW&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;评估集&lt;/td&gt; &#xA;   &lt;td&gt;BigBench(Beyond the Imitation Game Benchmark)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google/BIG-bench&#34;&gt;https://github.com/google/BIG-bench&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;评估集&lt;/td&gt; &#xA;   &lt;td&gt;Complex QA：用于ChatGPT的评测指令集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT&#34;&gt;https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-ChatGPT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;评估集&lt;/td&gt; &#xA;   &lt;td&gt;Langchain开源评估数据集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/LangChainDatasets&#34;&gt;https://huggingface.co/LangChainDatasets&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;评估集&lt;/td&gt; &#xA;   &lt;td&gt;2010-2022年全国高考卷的题目&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenLMLab/GAOKAO-Bench&#34;&gt;https://github.com/OpenLMLab/GAOKAO-Bench&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;评估集&lt;/td&gt; &#xA;   &lt;td&gt;中文通用大模型综合性评测基准SuperCLUE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CLUEbenchmark/SuperCLUE&#34;&gt;https://github.com/CLUEbenchmark/SuperCLUE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;英文预训练&lt;/td&gt; &#xA;   &lt;td&gt;RedPajama开源的复刻llama的预训练数据集，1.21万亿Token&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/togethercomputer/RedPajama-Data&#34;&gt;https://github.com/togethercomputer/RedPajama-Data&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;英文预训练&lt;/td&gt; &#xA;   &lt;td&gt;Cerebras基于RedPajama进行清洗去重后得到的高质量数据集, 6270亿Token&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/cerebras/SlimPajama-627B/tree/main/train&#34;&gt;https://huggingface.co/datasets/cerebras/SlimPajama-627B/tree/main/train&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;英文预训练&lt;/td&gt; &#xA;   &lt;td&gt;Pile 22个高质量数据集混合的预训练数据集800G,全量开放下载&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;https://pile.eleuther.ai/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;通用预训练&lt;/td&gt; &#xA;   &lt;td&gt;UER整理CLUECorpusSmall+News Commentary中英&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dbiir/UER-py/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE&#34;&gt;https://github.com/dbiir/UER-py/wiki/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;中文预训练&lt;/td&gt; &#xA;   &lt;td&gt;智源人工智能开源的wudao 200G预训练数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openi.pcl.ac.cn/BAAI/WuDao-Data&#34;&gt;https://github.com/BAAI-WuDao/WuDaoMM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;中文预训练&lt;/td&gt; &#xA;   &lt;td&gt;里屋社区发起开源力量收集中文互联网语料集MNBVC目标是对标ChatGPT的40T&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/esbatmop/MNBVC&#34;&gt;https://github.com/esbatmop/MNBVC&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;中文预训练&lt;/td&gt; &#xA;   &lt;td&gt;复旦开源15万中文图书下载和抽取方案&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/FudanNLPLAB/CBook-150K&#34;&gt;https://github.com/FudanNLPLAB/CBook-150K&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;中文预训练&lt;/td&gt; &#xA;   &lt;td&gt;书生万卷数据集来自公开网页多模态数据集，包括文本，图文和视频，其中文本1T，图文150G&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/OpenDataLab/WanJuan1_dot_0&#34;&gt;https://opendatalab.org.cn/OpenDataLab/WanJuan1_dot_0&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;领域预训练&lt;/td&gt; &#xA;   &lt;td&gt;首个中文科学文献数据集CSL,也有多种NLP任务数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ydli-ai/CSL&#34;&gt;https://github.com/ydli-ai/CSL&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;平行语料&lt;/td&gt; &#xA;   &lt;td&gt;news-commentary中英平行语料，用于中英间知识迁移&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://data.statmt.org/news-commentary/v15/training/&#34;&gt;https://data.statmt.org/news-commentary/v15/training/&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;多源数据集整合&lt;/td&gt; &#xA;   &lt;td&gt;opendatalab整合了预训练阶段的多个数据源&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opendatalab.org.cn/?industry=9821&amp;amp;source=JUU3JTlGJUE1JUU0JUI5JThF&#34;&gt;https://opendatalab.org.cn/?industry=9821&amp;amp;source=JUU3JTlGJUE1JUU0JUI5JThF&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tool-搜索增强&lt;/td&gt; &#xA;   &lt;td&gt;webCPM开源的和搜索工具进行交互问答的数据集，包括网页抽取式摘要，多事实内容回答等人工标注数据&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/thunlp/WebCPM&#34;&gt;https://github.com/thunlp/WebCPM&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tool-多工具&lt;/td&gt; &#xA;   &lt;td&gt;BmTools开源的多工具调用指令数据集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/OpenBMB/BMTools&#34;&gt;https://github.com/OpenBMB/BMTools&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NL2SQL&lt;/td&gt; &#xA;   &lt;td&gt;DB-GPT-Hub梳理了多源text-to-sql数据集&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/eosphoros-ai/DB-GPT-Hub&#34;&gt;https://github.com/eosphoros-ai/DB-GPT-Hub&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;AIGC&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nexus.snikpic.io/&#34;&gt;NexusGPT&lt;/a&gt;: &lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt;: AutoGPT可以出来工作了，第一个全AI Freelance平台&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cognosys.ai/create&#34;&gt;cognosys&lt;/a&gt;: 全网最火的web端AutoGPT，不过咋说呢试用了下感觉下巴要笑掉了，不剧透去试试你就知道 &lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://godmode.space/&#34;&gt;godmode&lt;/a&gt;：可以进行人为每一步交互的的AutoGPT&lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://agentgpt.reworkd.ai/&#34;&gt;agentgpt&lt;/a&gt;: 基础版AutoGPT&lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.doanythingmachine.com/&#34;&gt;do Anything&lt;/a&gt;: AutoGPT Like的to Do List生成器 &lt;img src=&#34;https://img.shields.io/badge/Auto-Agent-white&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chatmind.tech/&#34;&gt;ChatMind&lt;/a&gt;: chatgpt生成思维导图，模板很丰富，泛化性也不错，已经被XMind收购了~ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bing.com/&#34;&gt;New Bing&lt;/a&gt;：需要连外网否则会重定向到bing中国，需要申请waitlist &lt;img src=&#34;https://img.shields.io/badge/AIGC-Search-yellow&#34; alt=&#34;&#34;&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.perplexity.ai/&#34;&gt;Perplexity.ai&lt;/a&gt;: 同样需要科学上网，感觉比Bing做的更好的接入ChatGPT的神奇搜索引擎，在Bing之外还加入了相关推荐和追问 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Search-yellow&#34; alt=&#34;&#34;&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dice2o/BingGPT&#34;&gt;BingGPT&lt;/a&gt;: NewBing开源桌面客户端，可以将聊天记录导出 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Search-yellow&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/refuel-ai/autolabel&#34;&gt;AutoLabel&lt;/a&gt;: AutoLabel标注方案 &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/arc53/DocsGPT&#34;&gt;DocsGPT&lt;/a&gt;: 把ChatGPT开放域问答转化成封闭域问答的通用方案，试用垂类领域问答场景,可以试用定制的ChatBot &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/imClumsyPanda/langchain-ChatGLM&#34;&gt;langchain-ChatGLM&lt;/a&gt;: 基于ChatGLM的本地知识问答，和上面的DocsGPT相似，不过可以本地部署&lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chat2doc.cn/&#34;&gt;ChatPDF&lt;/a&gt;: 国内的ChatPDF, 上传pdf后，会给出文章的Top5可能问题，然后对话式从文档中进行问答和检索，10s读3万字 &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chatdoc.com/?viaurl=ainavpro.com&#34;&gt;ChatDoc&lt;/a&gt;:ChatPDF升级版，增加了表格类解析，和完善的索引引用加跳转加对应文章内容高亮，哈哈我准备自己整一个 &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kaixindelele/ChatPaper&#34;&gt;ChatPaper&lt;/a&gt;: 根据输入关键词，自动在arxiv上下载最新的论文，并对论文进行摘要总结，可以在huggingface上试用！ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.openread.academy/home&#34;&gt;OpenRead&lt;/a&gt;: 面向论文写作，阅读场景，可以帮助生成文献综述，以及提供和NotionAI相似的智能Markdown用于写作 &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mukulpatnaik/researchgpt&#34;&gt;researchgpt&lt;/a&gt;: 和ChatPDF类似，支持arivx论文下载，加载后对话式获取论文重点 &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://briefgpt.xyz/?viaurl=ainavpro.com&#34;&gt;BriefGPT&lt;/a&gt;: 日更Arxiv论文，并对论文进行摘要，关键词抽取，帮助研究者了解最新动态, UI不错哟 &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/binary-husky/chatgpt_academic&#34;&gt;ChatGPT-academic&lt;/a&gt;: 又是一个基于gradio实现的paper润色，摘要等功能打包的实现 &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Leizhenpeng/feishu-chatgpt&#34;&gt;feishu-chatgpt&lt;/a&gt;: 飞书chatgpt，和365copilot相似也是多组件集成, 有点全！ &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ai-topia.com/&#34;&gt;AI Topiah&lt;/a&gt;: 聆心智能AI角色聊天，和路飞唠了两句，多少有点中二之魂在燃烧 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Chatbot-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.chatbase.co/&#34;&gt;chatbase&lt;/a&gt;: 情感角色聊天，还没尝试 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Chatbot-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gptme.vana.com/login&#34;&gt;Vana&lt;/a&gt;: virtual DNA, 通过聊天创建虚拟自己！概念很炫 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Chatbot-blue&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.writesonic.com/&#34;&gt;WriteSonic&lt;/a&gt;：AI写作，支持对话和定向创作如广告文案，商品描述, 支持Web检索是亮点，支持中文 &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.copy.ai/&#34;&gt;copy.ai&lt;/a&gt;: WriteSonic竞品，亮点是像论文引用一样每句话都有对应网站链接，可以一键复制到右边的创作Markdown，超级好用！ &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.notion.so/product?fredir=1&#34;&gt;NotionAI&lt;/a&gt;：智能Markdown，适用真相！在创作中用command调用AI辅助润色，扩写，检索内容，给创意idea &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jasper.ai/&#34;&gt;Jasper&lt;/a&gt;: 同上，全是竞品哈哈 &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://copyai.cn/&#34;&gt;copy.down&lt;/a&gt;: 中文的营销文案生成，只能定向创作，支持关键词到文案的生成 &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20wirter%20tools-brightgreen&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chatexcel.com/convert&#34;&gt;ChatExcel&lt;/a&gt;: 指令控制excel计算，对熟悉excel的有些鸡肋，对不熟悉的有点用 &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/williamfzc/chat-gpt-ppt&#34;&gt;ChatPPT&lt;/a&gt;: 使用ChatGPT进行PPT制作 &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JimmyLv/BibiGPT&#34;&gt;BibiGPT&lt;/a&gt;: Bilibli视频内容一键总结，多模态文档 &lt;img src=&#34;https://img.shields.io/badge/Tool-Business-red&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/features/copilot&#34;&gt;Copilot&lt;/a&gt;: 要付费哟 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fauxpilot/fauxpilot&#34;&gt;Fauxpilot&lt;/a&gt;: copilot本地开源替代 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://codegeex.cn/zh-CN&#34;&gt;CodeGex&lt;/a&gt;: 国内替代品，还没试过 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codeium.com/&#34;&gt;Codeium&lt;/a&gt;: Copilot替代品，有免费版本支持各种plugin &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sqltranslate.app/&#34;&gt;sql translate&lt;/a&gt;: text2sql，利用 OpenAI 的 API 实现的一个很简单的工具，sql到文字，文字到sql &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ai2sql.io/&#34;&gt;ai2sql&lt;/a&gt;: text2sql老牌公司，相比sqltranslate功能更全面，支持SQL 语法检查、格式化和生成公式 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pingcap.com/chat2query-an-innovative-ai-powered-sql-generator-for-faster-insights/&#34;&gt;chat2query&lt;/a&gt;: text2sql 相比以上两位支持更自然的文本指令，以及更复杂的数据分析类的sql生成 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://outerbase.com/&#34;&gt;OuterBase&lt;/a&gt;: text2sql 设计风格很吸睛！电子表格结合mysql和dashboard，更适合数据分析宝宝 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/biobootloader/wolverine&#34;&gt;Wolverine&lt;/a&gt;: 代码自我debug的python脚本 &lt;img src=&#34;https://img.shields.io/badge/AIGC-Coder-blueviolet&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://beta.dreamstudio.ai/dream&#34;&gt;dreamstudio.ai&lt;/a&gt;: 开创者，Stable Difussion， 有试用quota &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F&#34;&gt;midjourney&lt;/a&gt;: 开创者，艺术风格为主 &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/product/dall-e-2&#34;&gt;Dall.E&lt;/a&gt;: 三巨头这就凑齐了 &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/hysts/ControlNet&#34;&gt;ControlNet&lt;/a&gt;: 为绘画创作加持可控性 &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Nutlope/restorePhotos&#34;&gt;GFPGAN&lt;/a&gt;: 照片修复 &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/microsoft/visual_chatgpt&#34;&gt;Visual ChatGPT&lt;/a&gt;: 微软发布图像ChatGPT，对话方式进行图像生成编辑，问答 &lt;img src=&#34;https://img.shields.io/badge/AIGC-AI%20Artist-orange&#34; alt=&#34;&#34;&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.genmo.ai/&#34;&gt;gemo.ai&lt;/a&gt;: 多模态聊天机器人，包括文本，图像，视频生成&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://storybird.com/&#34;&gt;storybird&lt;/a&gt;: 根据提示词生成故事绘本，还可以售卖&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;h3&gt;教程类&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook&#34;&gt;OpenAI Cookbook&lt;/a&gt;: 提供OpenAI模型使用示例 &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/riba2534/openai-scf-goproxy&#34;&gt;OpenAI 接口被墙解决办法&lt;/a&gt;: 使用腾讯云搭建代理，亲测非常好用且手残党也可以轻松上手&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://promptperfect.jinaai.cn/&#34;&gt;PromptPerfect&lt;/a&gt;:用魔法打败魔法，输入原始提示词，模型进行定向优化，试用后我有点沉默了，可以定向支持不同使用prompt的模型如Difussion，ChatGPT， Dalle等&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.clickprompt.org/zh-CN/&#34;&gt;ClickPrompt&lt;/a&gt;: 为各种prompt加持的工具生成指令包括Difussion，chatgptdeng, 需要OpenAI Key&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://newzone.top/chatgpt/&#34;&gt;ChatGPT ShortCut&lt;/a&gt;：提供各式场景下的Prompt范例，范例很全，使用后可以点赞！ &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://enchanting-trader-463.notion.site/Full-ChatGPT-Prompts-Resources-8aa78bb226b7467ab59b70d2b27042e9&#34;&gt;Full ChatGPT Prompts + Resources&lt;/a&gt;: 各种尝尽的prompt范例，和以上场景有所不同&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learnprompting.org/&#34;&gt;learning Prompt&lt;/a&gt;: prompt engineering超全教程，和落地应用收藏，包括很多LLM调用Agent的高级场景 &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ORDINAND/The-Art-of-Asking-ChatGPT-for-High-Quality-Answers-A-complete-Guide-to-Prompt-Engineering-Technique&#34;&gt;The art of asking chatgpt for high quality answers&lt;/a&gt;: 如何写Prompt指令出书了，链接是中文翻译的版本，比较偏基础使用&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dair-ai/Prompt-Engineering-Guide&#34;&gt;Prompt-Engineer-Guide&lt;/a&gt;: 同learnig prompt类的集成教程，互相引用可还行？！分类索引做的更好些 &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mojidoc.com/05z7y-dd5pa7hu3zfmhnbngoeztyqcnq-00b&#34;&gt;OpenAI 应用汇总指南&lt;/a&gt;: 纯应用类的汇总指南&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.ainavpro.com/#term-209&#34;&gt;AI 导航&lt;/a&gt;: 包括但不限于ChatGPT的应用汇总网站，更新很快，发现了一些新大陆&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/&#34;&gt;AI Alignment Forum&lt;/a&gt;: RLHF等对齐相关最新论文和观点的讨论论坛&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data/&#34;&gt;Langchain: Chat with your data&lt;/a&gt;:吴恩达LLM实践课程&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/phodal/aigc&#34;&gt;构筑大语言模型应用：应用开发与架构设计&lt;/a&gt;: 一本关于 LLM 在真实世界应用的开源电子书&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/databricks-academy/large-language-models&#34;&gt;Large Language Models: Application through Production&lt;/a&gt;: 大模型应用Edx出品的课程&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;书籍博客类&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/chatgpt/&#34;&gt;OpenAI ChatGPT Intro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/instruction-following/&#34;&gt;OpenAI InstructGPT intro&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AllenAI ChatGPT能力解读：&lt;a href=&#34;https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1&#34;&gt;How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Huggingface ChatGPT能力解读：&lt;a href=&#34;https://huggingface.co/blog/dialog-agents&#34;&gt;The techniques behind ChatGPT: RLHF, IFT, CoT, Red teaming, and more&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stephen Wolfram ChatGPT能力解读: &lt;a href=&#34;https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/&#34;&gt;What Is ChatGPT Doing and Why Does It Work?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chenweiphd/ChatGPT-Hub&#34;&gt;Chatgpt相关解读汇总&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.technologyreview.com/2023/03/03/1069311/inside-story-oral-history-how-chatgpt-built-openai/&#34;&gt;麻省理工科技采访OpenAI工程师&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jiqizhixin.com/articles/2018-11-15-6?from=timeline&#34;&gt;AGI历史与现状&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/597586623&#34;&gt;张俊林 通向AGI之路：大型语言模型（LLM）技术精要&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/question/589639535/answer/2936696161&#34;&gt;知乎回答 OpenAI 发布 GPT-4，有哪些技术上的优化或突破?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/609877277&#34;&gt;追赶ChatGPT的难点与平替&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/615554635&#34;&gt;压缩即泛化，泛化即智能&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://new.qq.com/rain/a/20230423A08J7400&#34;&gt;陆奇最新演讲实录：我的大模型世界观｜第十四期&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2023-06-23-agent/&#34;&gt;LLM Powered Autonomous Agents&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac&#34;&gt;All You Need to Know to Build Your First LLM App&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.semianalysis.com/p/gpt-4-architecture-infrastructure&#34;&gt;GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://weread.qq.com/web/bookDetail/93832630811e7e827g0173ca&#34;&gt;为什么伟大不能被计划&lt;/a&gt;: OpenAI研究员出书&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MjM5ODY2OTQyNg==&amp;amp;mid=2649769138&amp;amp;idx=1&amp;amp;sn=2c408b73f66a52e43ea991b957729519&amp;amp;chksm=bec3d9af89b450b95e6432dc33f4f32ae7a29cc8e2916369aad6156c5817927d1f73a0c84e82&amp;amp;scene=21#wechat_redirect&#34;&gt;拾象投研机构对LLM的调研报告（文中有两次PPT的申请链接）&lt;/a&gt;: 对大模型应用给出了很全面的总结梳理&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.guotaixia.com/post/5336.html&#34;&gt;启明创投State of Generative AI 2023&lt;/a&gt;: 最近发现应用落地才是LLM真正产生价值的核心，开始更多关注一些投研的分析报告&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.oneusefulthing.org/p/how-to-use-ai-to-do-stuff-an-opinionated&#34;&gt;How to Use AI to Do Stuff: An Opinionated Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.interconnects.ai/p/llama-2-from-meta&#34;&gt;Llama 2: an incredible open LLM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://book.douban.com/subject/36449803/?icn=index-latestbook-subject&#34;&gt;Wolfram语言之父新书：这就是ChatGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pair.withgoogle.com/explorables/grokking/&#34;&gt;谷歌出品：对大模型领悟能力的一些探索很有意思 Do Machine Learning Models Memorize or Generalize?&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://simons.berkeley.edu/talks/ilya-sutskever-openai-2023-08-14&#34;&gt;OpenAI首席科学家最新讲座解读LM无监督预训练学了啥 An observation on Generalization&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mattprd.com/p/the-complete-beginners-guide-to-autonomous-agents&#34;&gt;The Complete Beginners Guide To Autonomous Agents&lt;/a&gt;: Octane AI创始人 Matt Schlicht发表的关于人工智能代理的一些思考&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yaofu.notion.site/An-Initial-Exploration-of-Theoretical-Support-for-Language-Model-Data-Engineering-Part-1-Pretraini-dc480d9bf7ff4659afd8c9fb738086eb&#34;&gt;An Initial Exploration of Theoretical Support for Language Model Data Engineering. Part 1: Pretraining&lt;/a&gt;: 符尧大佬系列新作，通过了解大模型背后的数据工程来了解模型本质，第一篇预训练数据&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Papers&lt;/h2&gt; &#xA;&lt;h3&gt;paper List&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/dongguanting/In-Context-Learning_PaperList&#34;&gt;https://github.com/dongguanting/In-Context-Learning_PaperList&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/PromptPapers&#34;&gt;https://github.com/thunlp/PromptPapers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Timothyxxx/Chain-of-ThoughtsPapers&#34;&gt;https://github.com/Timothyxxx/Chain-of-ThoughtsPapers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/thunlp/ToolLearningPapers&#34;&gt;https://github.com/thunlp/ToolLearningPapers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MLGroupJLU/LLM-eval-survey&#34;&gt;https://github.com/MLGroupJLU/LLM-eval-survey&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;综述&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A Survey of Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Paradigm Shift in Natural Language Processing&lt;/li&gt; &#xA; &lt;li&gt;Pre-Trained Models: Past, Present and Future&lt;/li&gt; &#xA; &lt;li&gt;What Language Model Architecture and Pretraining objects work best for zero shot generalization &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Towards Reasoning in Large Language Models: A Survey&lt;/li&gt; &#xA; &lt;li&gt;Reasoning with Language Model Prompting: A Survey &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;An Overview on Language Models: Recent Developments and Outlook &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;A Survey of Large Language Models[6.29更新版]&lt;/li&gt; &#xA; &lt;li&gt;Unifying Large Language Models and Knowledge Graphs: A Roadmap&lt;/li&gt; &#xA; &lt;li&gt;Augmented Language Models: a Survey &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey&lt;/li&gt; &#xA; &lt;li&gt;Challenges and Applications of Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;The Rise and Potential of Large Language Model Based Agents: A Survey&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;大模型能力探究&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In Context Learning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LARGER LANGUAGE MODELS DO IN-CONTEXT LEARNING DIFFERENTLY&lt;/li&gt; &#xA;   &lt;li&gt;How does in-context learning work? A framework for understanding the differences from traditional supervised learning&lt;/li&gt; &#xA;   &lt;li&gt;Why can GPT learn in-context? Language Model Secretly Perform Gradient Descent as Meta-Optimizers &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Rethinking the Role of Demonstrations What Makes incontext learning work? &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Trained Transformers Learn Linear Models In-Context&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;涌现能力 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Sparks of Artificial General Intelligence: Early experiments with GPT-4&lt;/li&gt; &#xA;   &lt;li&gt;Emerging Ability of Large Language Models &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;LANGUAGE MODELS REPRESENT SPACE AND TIME&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;能力评估 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;IS CHATGPT A GENERAL-PURPOSE NATURAL LANGUAGE PROCESSING TASK SOLVER?&lt;/li&gt; &#xA;   &lt;li&gt;Can Large Language Models Infer Causation from Correlation?&lt;/li&gt; &#xA;   &lt;li&gt;Holistic Evaluation of Language Model&lt;/li&gt; &#xA;   &lt;li&gt;Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond&lt;/li&gt; &#xA;   &lt;li&gt;Theory of Mind May Have Spontaneously Emerged in Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Beyond The Imitation Game: Quantifying And Extrapolating The Capabilities Of Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations&lt;/li&gt; &#xA;   &lt;li&gt;Demystifying GPT Self-Repair for Code Generation&lt;/li&gt; &#xA;   &lt;li&gt;Evidence of Meaning in Language Models Trained on Programs&lt;/li&gt; &#xA;   &lt;li&gt;Can Explanations Be Useful for Calibrating Black Box Models&lt;/li&gt; &#xA;   &lt;li&gt;On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective&lt;/li&gt; &#xA;   &lt;li&gt;Language acquisition: do children and language models follow similar learning stages?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Prompt Tunning范式&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tunning Free Prompt &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GPT2: Language Models are Unsupervised Multitask Learners&lt;/li&gt; &#xA;   &lt;li&gt;GPT3: Language Models are Few-Shot Learners &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;LAMA: Language Models as Knowledge Bases?&lt;/li&gt; &#xA;   &lt;li&gt;AutoPrompt: Eliciting Knowledge from Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix-Prompt LM Tunning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/li&gt; &#xA;   &lt;li&gt;PET-TC(a): Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;PET-TC(b): PETSGLUE It’s Not Just Size That Matters Small Language Models are also few-shot learners&lt;/li&gt; &#xA;   &lt;li&gt;GenPET: Few-Shot Text Generation with Natural Language Instructions&lt;/li&gt; &#xA;   &lt;li&gt;LM-BFF: Making Pre-trained Language Models Better Few-shot Learners &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;ADEPT: Improving and Simplifying Pattern Exploiting Training&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix-LM Prompt Tunning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Prefix-tuning: Optimizing continuous prompts for generation&lt;/li&gt; &#xA;   &lt;li&gt;Prompt-tunning: The power of scale for parameter-efficient prompt tuning &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;P-tunning: GPT Understands Too &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;WARP: Word-level Adversarial ReProgramming&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LM + Prompt Tunning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;P-tunning v2: Prompt Tuning Can Be Comparable to Fine-tunning Universally Across Scales and Tasks&lt;/li&gt; &#xA;   &lt;li&gt;PTR: Prompt Tuning with Rules for Text Classification&lt;/li&gt; &#xA;   &lt;li&gt;PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Fix-LM Adapter Tunning &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning&lt;/li&gt; &#xA;   &lt;li&gt;Parameter-Efficient Transfer Learning for NLP&lt;/li&gt; &#xA;   &lt;li&gt;INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;主流LLMS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GLM-130B: AN OPEN BILINGUAL PRE-TRAINED MODEL&lt;/li&gt; &#xA; &lt;li&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/li&gt; &#xA; &lt;li&gt;PaLM: Scaling Language Modeling with Pathways&lt;/li&gt; &#xA; &lt;li&gt;PaLM 2 Technical Report&lt;/li&gt; &#xA; &lt;li&gt;GPT-4 Technical Report&lt;/li&gt; &#xA; &lt;li&gt;Backpack Language Models&lt;/li&gt; &#xA; &lt;li&gt;Llama 2: Open Foundation and Fine-Tuned Chat Models&lt;/li&gt; &#xA; &lt;li&gt;OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch&lt;/li&gt; &#xA; &lt;li&gt;Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;指令微调&amp;amp;对齐 (instruction_tunning)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;经典方案 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Flan: FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Flan-T5: Scaling Instruction-Finetuned Language Models&lt;/li&gt; &#xA;   &lt;li&gt;ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning&lt;/li&gt; &#xA;   &lt;li&gt;Instruct-GPT: Training language models to follow instructions with human feedback &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;T0: MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION&lt;/li&gt; &#xA;   &lt;li&gt;Natural Instructions: Cross-Task Generalization via Natural Language Crowdsourcing Instructions&lt;/li&gt; &#xA;   &lt;li&gt;Tk-INSTRUCT: SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks&lt;/li&gt; &#xA;   &lt;li&gt;ZeroPrompt: Scaling Prompt-Based Pretraining to 1,000 Tasks Improves Zero-shot Generalization&lt;/li&gt; &#xA;   &lt;li&gt;Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor&lt;/li&gt; &#xA;   &lt;li&gt;INSTRUCTEVAL Towards Holistic Evaluation of Instrucion-Tuned Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;更少，质量更高、更多样的指令数据带来质变 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LIMA: Less Is More for Alignment &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning&lt;/li&gt; &#xA;   &lt;li&gt;Textbooks Are All You Need &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;AlpaGasus: Training A Better Alpaca with Fewer Data&lt;/li&gt; &#xA;   &lt;li&gt;InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4&lt;/li&gt; &#xA;   &lt;li&gt;Instruction Mining: High-Quality Instruction Data Selection for Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Visual Instruction Tuning with Polite Flamingo&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;新对齐/微调方案 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WizardLM: Empowering Large Language Models to Follow Complex Instructions&lt;/li&gt; &#xA;   &lt;li&gt;Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning&lt;/li&gt; &#xA;   &lt;li&gt;Self-Alignment with Instruction Backtranslation &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks&lt;/li&gt; &#xA;   &lt;li&gt;PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions&lt;/li&gt; &#xA;   &lt;li&gt;OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs&lt;/li&gt; &#xA;   &lt;li&gt;Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision&lt;/li&gt; &#xA;   &lt;li&gt;Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;微调经验/实验报告 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;BELLE: Exploring the Impact of Instruction Data Scaling on Large Language Models: An Empirical Study on Real-World Use Cases&lt;/li&gt; &#xA;   &lt;li&gt;Baize: Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data&lt;/li&gt; &#xA;   &lt;li&gt;A Comparative Study between Full-Parameter and LoRA-based Fine-Tuning on Chinese Instruction Data for Large LM&lt;/li&gt; &#xA;   &lt;li&gt;Exploring ChatGPT’s Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences&lt;/li&gt; &#xA;   &lt;li&gt;Towards Better Instruction Following Language Models for Chinese: Investigating the Impact of Training Data and Evaluation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;对话模型&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LaMDA: Language Models for Dialog Applications&lt;/li&gt; &#xA; &lt;li&gt;Sparrow: Improving alignment of dialogue agents via targeted human judgements &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage&lt;/li&gt; &#xA; &lt;li&gt;How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation&lt;/li&gt; &#xA; &lt;li&gt;DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI&lt;/li&gt; &#xA; &lt;li&gt;Enhancing Chat Language Models by Scaling High-quality Instructional Conversations&lt;/li&gt; &#xA; &lt;li&gt;DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;思维链 (prompt_chain_of_thought)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;基础&amp;amp;进阶用法 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;[zero-shot-COT] Large Language Models are Zero-Shot Reasoners &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;[few-shot COT] Chain of Thought Prompting Elicits Reasoning in Large Language Models &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT REASONING IN LANGUAGE MODELS&lt;/li&gt; &#xA;   &lt;li&gt;LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Tree of Thoughts: Deliberate Problem Solving with Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Decomposed Prompting A MODULAR APPROACH FOR Solving Complex Tasks&lt;/li&gt; &#xA;   &lt;li&gt;Successive Prompting for Decomposing Complex Questions&lt;/li&gt; &#xA;   &lt;li&gt;Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework&lt;/li&gt; &#xA;   &lt;li&gt;Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning&lt;/li&gt; &#xA;   &lt;li&gt;LAMBADA: Backward Chaining for Automated Reasoning in Natural Language&lt;/li&gt; &#xA;   &lt;li&gt;Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Graph of Thoughts: Solving Elaborate Problems with Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;分领域COT [Math, Code, Tabular, QA] &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Solving Quantitative Reasoning Problems with Language Models&lt;/li&gt; &#xA;   &lt;li&gt;SHOW YOUR WORK: SCRATCHPADS FOR INTERMEDIATE COMPUTATION WITH LANGUAGE MODELS&lt;/li&gt; &#xA;   &lt;li&gt;Solving math word problems with processand outcome-based feedback&lt;/li&gt; &#xA;   &lt;li&gt;CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning&lt;/li&gt; &#xA;   &lt;li&gt;T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering&lt;/li&gt; &#xA;   &lt;li&gt;LEARNING PERFORMANCE-IMPROVING CODE EDITS&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning&lt;/li&gt; &#xA;   &lt;li&gt;Tab-CoT: Zero-shot Tabular Chain of Thought&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;原理分析 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;TEXT AND PATTERNS: FOR EFFECTIVE CHAIN OF THOUGHT IT TAKES TWO TO TANGO&lt;/li&gt; &#xA;   &lt;li&gt;Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models Can Be Easily Distracted by Irrelevant Context&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;小模型COT蒸馏 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Specializing Smaller Language Models towards Multi-Step Reasoning &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Teaching Small Language Models to Reason&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models are Reasoning Teachers&lt;/li&gt; &#xA;   &lt;li&gt;Distilling Reasoning Capabilities into Smaller Language Models&lt;/li&gt; &#xA;   &lt;li&gt;The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;COT样本自动构建/选择 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;STaR: Self-Taught Reasoner Bootstrapping ReasoningWith Reasoning&lt;/li&gt; &#xA;   &lt;li&gt;AutoCOT：AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models Can Self-Improve&lt;/li&gt; &#xA;   &lt;li&gt;Active Prompting with Chain-of-Thought for Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;COMPLEXITY-BASED PROMPTING FOR MULTI-STEP REASONING&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;others &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OlaGPT Empowering LLMs With Human-like Problem-Solving abilities&lt;/li&gt; &#xA;   &lt;li&gt;Challenging BIG-Bench tasks and whether chain-of-thought can solve them&lt;/li&gt; &#xA;   &lt;li&gt;Large Language Models are Better Reasoners with Self-Verification&lt;/li&gt; &#xA;   &lt;li&gt;ThoughtSource A central hub for large language model reasoning data&lt;/li&gt; &#xA;   &lt;li&gt;Two Failures of Self-Consistency in the Multi-Step Reasoning of LLMs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;RLHF&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deepmind &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Teaching language models to support answers with verified quotes&lt;/li&gt; &#xA;   &lt;li&gt;sparrow, Improving alignment of dialogue agents via targetd human judgements &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;openai &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;PPO: Proximal Policy Optimization Algorithms &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Deep Reinforcement Learning for Human Preference&lt;/li&gt; &#xA;   &lt;li&gt;Fine-Tuning Language Models from Human Preferences&lt;/li&gt; &#xA;   &lt;li&gt;learning to summarize from human feedback&lt;/li&gt; &#xA;   &lt;li&gt;InstructGPT: Training language models to follow instructions with human feedback &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Scaling Laws for Reward Model Over optimization &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Anthropic &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A General Language Assistant as a Laboratory for Alignmen&lt;/li&gt; &#xA;   &lt;li&gt;Red Teaming Language Models to Reduce Harms Methods,Scaling Behaviors and Lessons Learned&lt;/li&gt; &#xA;   &lt;li&gt;Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Constitutional AI Harmlessness from AI Feedback &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Pretraining Language Models with Human Preferences&lt;/li&gt; &#xA;   &lt;li&gt;The Capacity for Moral Self-Correction in Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;AllenAI, RL4LM：IS REINFORCEMENT LEARNING (NOT) FOR NATURAL LANGUAGE PROCESSING BENCHMARKS&lt;/li&gt; &#xA; &lt;li&gt;改良方案 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RRHF: Rank Responses to Align Language Models with Human Feedback without tears&lt;/li&gt; &#xA;   &lt;li&gt;PRM：Let&#39;s verify step by step&lt;/li&gt; &#xA;   &lt;li&gt;Chain of Hindsight Aligns Language Models with Feedback&lt;/li&gt; &#xA;   &lt;li&gt;AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback&lt;/li&gt; &#xA;   &lt;li&gt;Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback&lt;/li&gt; &#xA;   &lt;li&gt;RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Training Socially Aligned Language Models in Simulated Human Society&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM Agent 让模型使用工具 (llm_agent)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;基于prompt通用方案 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ReAct: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Self-ask: MEASURING AND NARROWING THE COMPOSITIONALITY GAP IN LANGUAGE MODELS &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;MRKL SystemsA modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning&lt;/li&gt; &#xA;   &lt;li&gt;PAL: Program-aided Language Models&lt;/li&gt; &#xA;   &lt;li&gt;ART: Automatic multi-step reasoning and tool-use for large language models&lt;/li&gt; &#xA;   &lt;li&gt;ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions&lt;/li&gt; &#xA;   &lt;li&gt;Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Faithful Chain-of-Thought Reasoning&lt;/li&gt; &#xA;   &lt;li&gt;Reflexion: Language Agents with Verbal Reinforcement Learning &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks&lt;/li&gt; &#xA;   &lt;li&gt;Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework&lt;/li&gt; &#xA;   &lt;li&gt;RestGPT: Connecting Large Language Models with Real-World RESTful APIs&lt;/li&gt; &#xA;   &lt;li&gt;ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;基于微调通用方案 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TALM: Tool Augmented Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Toolformer: Language Models Can Teach Themselves to Use Tools &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Tool Learning with Foundation Models&lt;/li&gt; &#xA;   &lt;li&gt;Tool Maker：Large Language Models as Tool Maker&lt;/li&gt; &#xA;   &lt;li&gt;TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;检索增强方案 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WebGPT：Browser-assisted question-answering with human feedback&lt;/li&gt; &#xA;   &lt;li&gt;WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences&lt;/li&gt; &#xA;   &lt;li&gt;WebCPM: Interactive Web Search for Chinese Long-form Question Answering &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;REPLUG: Retrieval-Augmented Black-Box Language Models&lt;/li&gt; &#xA;   &lt;li&gt;Query Rewriting for Retrieval-Augmented Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit&lt;/li&gt; &#xA;   &lt;li&gt;Atlas: Few-shot Learning with Retrieval Augmented Language Models&lt;/li&gt; &#xA;   &lt;li&gt;RRAML: Reinforced Retrieval Augmented Machine Learning&lt;/li&gt; &#xA;   &lt;li&gt;Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation&lt;/li&gt; &#xA;   &lt;li&gt;PDFTriage: Question Answering over Long, Structured Documents&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;调用模型方案 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace&lt;/li&gt; &#xA;   &lt;li&gt;Gorilla：Large Language Model Connected with Massive APIs &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;OpenAGI: When LLM Meets Domain Experts&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;垂直领域 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents&lt;/li&gt; &#xA;   &lt;li&gt;ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings&lt;/li&gt; &#xA;   &lt;li&gt;ChemCrow Augmenting large language models with chemistry tools&lt;/li&gt; &#xA;   &lt;li&gt;Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow&lt;/li&gt; &#xA;   &lt;li&gt;GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information&lt;/li&gt; &#xA;   &lt;li&gt;PointLLM: Empowering Large Language Models to Understand Point Clouds&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;评估 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Evaluating Verifiability in Generative Search Engines&lt;/li&gt; &#xA;   &lt;li&gt;Mind2Web: Towards a Generalist Agent for the Web&lt;/li&gt; &#xA;   &lt;li&gt;Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions&lt;/li&gt; &#xA;   &lt;li&gt;API-Bank: A Benchmark for Tool-Augmented LLMs&lt;/li&gt; &#xA;   &lt;li&gt;ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;MultiAgent &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/li&gt; &#xA;   &lt;li&gt;AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents&lt;/li&gt; &#xA;   &lt;li&gt;CAMEL: Communicative Agents for &#34;Mind&#34; Exploration of Large Scale Language Model Society&lt;/li&gt; &#xA;   &lt;li&gt;Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;其他 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;LLM+P: Empowering Large Language Models with Optimal Planning Proficiency&lt;/li&gt; &#xA;   &lt;li&gt;Inference with Reference: Lossless Acceleration of Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;RecallM: An Architecture for Temporal Context Understanding and Question Answering&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;指令数据生成 (instruction_data_gen)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;APE: LARGE LANGUAGE MODELS ARE HUMAN-LEVEL PROMPT ENGINEERS &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;iPrompt: Explaining Data Patterns in Natural Language via Interpretable Autoprompting&lt;/li&gt; &#xA; &lt;li&gt;Flipped Learning: Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners&lt;/li&gt; &#xA; &lt;li&gt;Fairness-guided Few-shot Prompting for Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;Instruction induction: From few examples to natural language task descriptions.&lt;/li&gt; &#xA; &lt;li&gt;Baize An Open-Source Chat Model with Parameter-Efficient Tuning on self-Chat Data&lt;/li&gt; &#xA; &lt;li&gt;SELF-QA Unsupervised Knowledge Guided alignment.&lt;/li&gt; &#xA; &lt;li&gt;GPT Self-Supervision for a Better Data Annotator&lt;/li&gt; &#xA; &lt;li&gt;The Flan Collection Designing Data and Methods&lt;/li&gt; &#xA; &lt;li&gt;Self-Consuming Generative Models Go MAD&lt;/li&gt; &#xA; &lt;li&gt;InstructEval: Systematic Evaluation of Instruction Selection Methods&lt;/li&gt; &#xA; &lt;li&gt;Overwriting Pretrained Bias with Finetuning Data&lt;/li&gt; &#xA; &lt;li&gt;WizardLM: Empowering Large Language Models to Follow Complex Instructions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;预训练数据(pretrain_data)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining&lt;/li&gt; &#xA; &lt;li&gt;The Pile: An 800GB Dataset of Diverse Text for Language Modeling&lt;/li&gt; &#xA; &lt;li&gt;CCNet: Extracting High Quality Monolingual Datasets fromWeb Crawl Data&lt;/li&gt; &#xA; &lt;li&gt;WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models&lt;/li&gt; &#xA; &lt;li&gt;CLUECorpus2020: A Large-scale Chinese Corpus for Pre-training Language Model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;领域模型 (domain_llms)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MedGPT: Medical Concept Prediction from Clinical Narratives&lt;/li&gt; &#xA; &lt;li&gt;BioGPT：Generative Pre-trained Transformer for Biomedical Text Generation and Mining&lt;/li&gt; &#xA; &lt;li&gt;Galactia：A Large Language Model for Science&lt;/li&gt; &#xA; &lt;li&gt;PubMed GPT: A Domain-specific large language model for biomedical text &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;BloombergGPT： A Large Language Model for Finance&lt;/li&gt; &#xA; &lt;li&gt;ChatDoctor：Medical Chat Model Fine-tuned on LLaMA Model using Medical Domain Knowledge&lt;/li&gt; &#xA; &lt;li&gt;Med-PaLM：Large Language Models Encode Clinical Knowledge[V1,V2] &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Augmented Large Language Models with Parametric Knowledge Guiding&lt;/li&gt; &#xA; &lt;li&gt;XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters&lt;/li&gt; &#xA; &lt;li&gt;ChatLaw Open-Source Legal Large Language Model &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;MediaGPT : A Large Language Model For Chinese Media&lt;/li&gt; &#xA; &lt;li&gt;SMILE: Single-turn to Multi-turn Inclusive Language Expansion via ChatGPT for Mental Health Support&lt;/li&gt; &#xA; &lt;li&gt;KITLM: Domain-Specific Knowledge InTegration into Language Models for Question Answering&lt;/li&gt; &#xA; &lt;li&gt;FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis&lt;/li&gt; &#xA; &lt;li&gt;EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce&lt;/li&gt; &#xA; &lt;li&gt;FinGPT: Open-Source Financial Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT&lt;/li&gt; &#xA; &lt;li&gt;CFGPT: Chinese Financial Assistant with Large Language Model&lt;/li&gt; &#xA; &lt;li&gt;Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM超长文本处理 (long_input)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Parallel Context Windows for Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;Structured Prompting: Scaling In-Context Learning to 1,000 Examples&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spaces.ac.cn/archives/9617&#34;&gt;苏剑林, NBCE：使用朴素贝叶斯扩展LLM的Context处理长度&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens&lt;/li&gt; &#xA; &lt;li&gt;Unlimiformer: Long-Range Transformers with Unlimited Length Input&lt;/li&gt; &#xA; &lt;li&gt;Scaling Transformer to 1M tokens and beyond with RMT&lt;/li&gt; &#xA; &lt;li&gt;RECURRENTGPT: Interactive Generation of (Arbitrarily) Long Text&lt;/li&gt; &#xA; &lt;li&gt;TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;/li&gt; &#xA; &lt;li&gt;Extending Context Window of Large Language Models via Positional Interpolation&lt;/li&gt; &#xA; &lt;li&gt;LongNet: Scaling Transformers to 1,000,000,000 Tokens&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kaiokendev.github.io/til#extending-context-to-8k&#34;&gt;https://kaiokendev.github.io/til#extending-context-to-8k&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spaces.ac.cn/archives/9675&#34;&gt;苏剑林,Transformer升级之路：10、RoPE是一种β进制编码&lt;/a&gt; &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spaces.ac.cn/archives/9706&#34;&gt;苏剑林,Transformer升级之路：11、将β进制位置进行到底&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://spaces.ac.cn/archives/9708&#34;&gt;苏剑林,Transformer升级之路：12、无限外推的ReRoPE？&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Focused Transformer: Contrastive Training for Context Scaling&lt;/li&gt; &#xA; &lt;li&gt;Lost in the Middle: How Language Models Use Long Contexts &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SINKS&lt;/li&gt; &#xA; &lt;li&gt;Ring Attention with Blockwise Transformers for Near-Infinite Context&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;NL2SQL&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;大模型方案 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;C3: Zero-shot Text-to-SQL with ChatGPT &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;SQL-PALM: IMPROVED LARGE LANGUAGE MODEL ADAPTATION FOR TEXT-TO-SQL&lt;/li&gt; &#xA;   &lt;li&gt;BIRD Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQL &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA;   &lt;li&gt;A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL&lt;/li&gt; &#xA;   &lt;li&gt;ChatDB: AUGMENTING LLMS WITH DATABASES AS THEIR SYMBOLIC MEMORY&lt;/li&gt; &#xA;   &lt;li&gt;A comprehensive evaluation of ChatGPT’s zero-shot Text-to-SQL capability&lt;/li&gt; &#xA;   &lt;li&gt;Few-shot Text-to-SQL Translation using Structure and Content Prompt Learning&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Domain Knowledge Intensive &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge&lt;/li&gt; &#xA;   &lt;li&gt;Bridging the Generalization Gap in Text-to-SQL Parsing with Schema Expansion&lt;/li&gt; &#xA;   &lt;li&gt;Towards Robustness of Text-to-SQL Models against Synonym Substitution&lt;/li&gt; &#xA;   &lt;li&gt;FinQA: A Dataset of Numerical Reasoning over Financial Data&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;others &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;RESDSQL: Decoupling Schema Linking and Skeleton Parsing for Text-to-SQL&lt;/li&gt; &#xA;   &lt;li&gt;MIGA: A Unified Multi-task Generation Framework for Conversational Text-to-SQL&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;降低模型幻觉 (reliability)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Survey of Hallucination in Natural Language Generation&lt;/li&gt; &#xA; &lt;li&gt;Trusting Your Evidence: Hallucinate Less with Context-aware Decoding &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;SELF-REFINE:ITERATIVE REFINEMENT WITH SELF-FEEDBACK &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;PROMPTING GPT-3 TO BE RELIABLE&lt;/li&gt; &#xA; &lt;li&gt;Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference&lt;/li&gt; &#xA; &lt;li&gt;On the Advance of Making Language Models Better Reasoners&lt;/li&gt; &#xA; &lt;li&gt;Progressive-Hint Prompting Improves Reasoning in Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;ASK ME ANYTHING: A SIMPLE STRATEGY FOR PROMPTING LANGUAGE MODELS &lt;span&gt;⭐&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inference-Time Intervention: Eliciting Truthful Answers from a Language Model&lt;/li&gt; &#xA; &lt;li&gt;Reflexion: an autonomous agent with dynamic memory and self-reflection&lt;/li&gt; &#xA; &lt;li&gt;Self-consistency for open-ended generations&lt;/li&gt; &#xA; &lt;li&gt;Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback&lt;/li&gt; &#xA; &lt;li&gt;Factuality Enhanced Language Models for Open-Ended Text Generation&lt;/li&gt; &#xA; &lt;li&gt;Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes&lt;/li&gt; &#xA; &lt;li&gt;Rethinking with Retrieval: Faithful Large Language Model Inference&lt;/li&gt; &#xA; &lt;li&gt;RefGPT: Reference → Truthful &amp;amp; Customized Dialogues Generation by GPTs and for GPTs&lt;/li&gt; &#xA; &lt;li&gt;Enabling Large Language Models to Generate Text with Citations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;大模型评估（evaluation）&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;事实性评估 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;TRUSTWORTHY LLMS: A SURVEY AND GUIDELINE FOR EVALUATING LARGE LANGUAGE MODELS’ ALIGNMENT&lt;/li&gt; &#xA;   &lt;li&gt;TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;TRUE: Re-evaluating Factual Consistency Evaluation&lt;/li&gt; &#xA;   &lt;li&gt;SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models&lt;/li&gt; &#xA;   &lt;li&gt;FACTSCORE: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation&lt;/li&gt; &#xA;   &lt;li&gt;KoLA: Carefully Benchmarking World Knowledge of Large Language Models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;推理优化(inference)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast Transformer Decoding: One Write-Head is All You Need&lt;/li&gt; &#xA; &lt;li&gt;Fast Inference from Transformers via Speculative Decoding&lt;/li&gt; &#xA; &lt;li&gt;GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints&lt;/li&gt; &#xA; &lt;li&gt;Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding&lt;/li&gt; &#xA; &lt;li&gt;SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference&lt;/li&gt; &#xA; &lt;li&gt;BatchPrompt: Accomplish more with less&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;模型知识编辑黑科技(model_edit)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ROME：Locating and Editing Factual Associations in GPT&lt;/li&gt; &#xA; &lt;li&gt;Transformer Feed-Forward Layers Are Key-Value Memories&lt;/li&gt; &#xA; &lt;li&gt;MEMIT: Mass-Editing Memory in a Transformer&lt;/li&gt; &#xA; &lt;li&gt;MEND：Fast Model Editing at Scale&lt;/li&gt; &#xA; &lt;li&gt;Editing Large Language Models: Problems, Methods, and Opportunities&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Other Prompt Engineer(prompt_engineer)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Calibrate Before Use: Improving Few-Shot Performance of Language Models&lt;/li&gt; &#xA; &lt;li&gt;In-Context Instruction Learning&lt;/li&gt; &#xA; &lt;li&gt;LEARNING PERFORMANCE-IMPROVING CODE EDITS&lt;/li&gt; &#xA; &lt;li&gt;Boosting Theory-of-Mind Performance in Large Language Models via Prompting&lt;/li&gt; &#xA; &lt;li&gt;Generated Knowledge Prompting for Commonsense Reasoning&lt;/li&gt; &#xA; &lt;li&gt;RECITATION-AUGMENTED LANGUAGE MODELS&lt;/li&gt; &#xA; &lt;li&gt;kNN PROMPTING: BEYOND-CONTEXT LEARNING WITH CALIBRATION-FREE NEAREST NEIGHBOR INFERENCE&lt;/li&gt; &#xA; &lt;li&gt;EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus&lt;/li&gt; &#xA; &lt;li&gt;Causality-aware Concept Extraction based on Knowledge-guided Prompting&lt;/li&gt; &#xA; &lt;li&gt;LARGE LANGUAGE MODELS AS OPTIMIZERS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Multimodal&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning&lt;/li&gt; &#xA; &lt;li&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/li&gt; &#xA; &lt;li&gt;PaLM-E: An Embodied Multimodal Language Model&lt;/li&gt; &#xA; &lt;li&gt;LLava Visual Instruction Tuning&lt;/li&gt; &#xA; &lt;li&gt;MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;TabLLM: Few-shot Classification of Tabular Data with Large Language Models&lt;/li&gt; &#xA; &lt;li&gt;BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions&lt;/li&gt; &#xA; &lt;li&gt;mPLUG-Owl : Modularization Empowers Large Language Models with Multimodality&lt;/li&gt; &#xA; &lt;li&gt;LVLM eHub: A Comprehensive Evaluation Benchmark for Large VisionLanguage Models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Others&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pretraining on the Test Set Is All You Need 哈哈作者你是懂讽刺文学的&lt;/li&gt; &#xA; &lt;li&gt;Learnware: Small Models Do Big&lt;/li&gt; &#xA; &lt;li&gt;The economic potential of generative AI&lt;/li&gt; &#xA; &lt;li&gt;A PhD Student’s Perspective on Research in NLP in the Era of Very Large Language Models&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>CodeWithHarry/Sigma-Web-Dev-Course</title>
    <updated>2023-10-15T01:46:43Z</updated>
    <id>tag:github.com,2023-10-15:/CodeWithHarry/Sigma-Web-Dev-Course</id>
    <link href="https://github.com/CodeWithHarry/Sigma-Web-Dev-Course" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Source Code for Sigma Web Development Course&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sigma-Web-Dev-Course&lt;/h1&gt; &#xA;&lt;p&gt;Source Code for Sigma Web Development Course&lt;/p&gt;</summary>
  </entry>
</feed>