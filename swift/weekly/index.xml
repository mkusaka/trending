<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-06T01:48:01Z</updated>
  <subtitle>Weekly Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/swift-transformers</title>
    <updated>2024-10-06T01:48:01Z</updated>
    <id>tag:github.com,2024-10-06:/huggingface/swift-transformers</id>
    <link href="https://github.com/huggingface/swift-transformers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Swift Package to implement a transformers-like API in Swift&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;code&gt;swift-transformers&lt;/code&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/swift-transformers/actions/workflows/unit-tests.yml&#34;&gt;&lt;img src=&#34;https://github.com/huggingface/swift-transformers/actions/workflows/unit-tests.yml/badge.svg?sanitize=true&#34; alt=&#34;Unit Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://swiftpackageindex.com/huggingface/swift-transformers&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fhuggingface%2Fswift-transformers%2Fbadge%3Ftype%3Dswift-versions&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://swiftpackageindex.com/huggingface/swift-transformers&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fhuggingface%2Fswift-transformers%2Fbadge%3Ftype%3Dplatforms&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a collection of utilities to help adopt language models in Swift apps. It tries to follow the Python &lt;code&gt;transformers&lt;/code&gt; API and abstractions whenever possible, but it also aims to provide an idiomatic Swift interface and does not assume prior familiarity with &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;&lt;code&gt;transformers&lt;/code&gt;&lt;/a&gt; or &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;&lt;code&gt;tokenizers&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Rationale and Overview&lt;/h2&gt; &#xA;&lt;p&gt;Please, check &lt;a href=&#34;https://huggingface.co/blog/swift-coreml-llm&#34;&gt;our post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Modules&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Tokenizers&lt;/code&gt;. Utilities to convert text to tokens and back. Follows the abstractions in &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;&lt;code&gt;tokenizers&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/xenova/transformers.js&#34;&gt;&lt;code&gt;transformers.js&lt;/code&gt;&lt;/a&gt;. Usage example:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import Tokenizers&#xA;&#xA;func testTokenizer() async throws {&#xA;    let tokenizer = try await AutoTokenizer.from(pretrained: &#34;pcuenq/Llama-2-7b-chat-coreml&#34;)&#xA;    let inputIds = tokenizer(&#34;Today she took a train to the West&#34;)&#xA;    assert(inputIds == [1, 20628, 1183, 3614, 263, 7945, 304, 278, 3122])&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, you don&#39;t usually need to tokenize the input text yourself - the &lt;a href=&#34;https://github.com/huggingface/swift-transformers/raw/17d4bfae3598482fc7ecf1a621aa77ab586d379a/Sources/Generation/Generation.swift#L82&#34;&gt;&lt;code&gt;Generation&lt;/code&gt; code&lt;/a&gt; will take care of it.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Hub&lt;/code&gt;. Utilities to download configuration files from the Hub, used to instantiate tokenizers and learn about language model characteristics.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Generation&lt;/code&gt;. Algorithms for text generation. Currently supported ones are greedy search and top-k sampling.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;Models&lt;/code&gt;. Language model abstraction over a Core ML package.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;This package has been tested with autoregressive language models such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT, GPT-Neox, GPT-J.&lt;/li&gt; &#xA; &lt;li&gt;SantaCoder.&lt;/li&gt; &#xA; &lt;li&gt;StarCoder.&lt;/li&gt; &#xA; &lt;li&gt;Falcon.&lt;/li&gt; &#xA; &lt;li&gt;Llama 2.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Encoder-decoder models such as T5 and Flan are currently &lt;em&gt;not supported&lt;/em&gt;. They are high up in our &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/swift-transformers/main/#roadmap&#34;&gt;priority list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Other Tools&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/swift-chat&#34;&gt;&lt;code&gt;swift-chat&lt;/code&gt;&lt;/a&gt;, a simple app demonstrating how to use this package.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/exporters&#34;&gt;&lt;code&gt;exporters&lt;/code&gt;&lt;/a&gt;, a Core ML conversion package for transformers models, based on Apple&#39;s &lt;a href=&#34;https://github.com/apple/coremltools&#34;&gt;&lt;code&gt;coremltools&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/coreml-projects/transformers-to-coreml&#34;&gt;&lt;code&gt;transformers-to-coreml&lt;/code&gt;&lt;/a&gt;, a no-code Core ML conversion tool built on &lt;code&gt;exporters&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;SwiftPM&lt;/h2&gt; &#xA;&lt;p&gt;To use &lt;code&gt;swift-transformers&lt;/code&gt; with SwiftPM, you can add this to your &lt;code&gt;Package.swift&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;dependencies: [&#xA;    .package(url: &#34;https://github.com/huggingface/swift-transformers&#34;, from: &#34;0.1.5&#34;)&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then, add the Transformers library as a dependency to your target:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;targets: [&#xA;    .target(&#xA;        name: &#34;YourTargetName&#34;,&#xA;        dependencies: [&#xA;            .product(name: &#34;Transformers&#34;, package: &#34;swift-transformers&#34;)&#xA;        ]&#xA;    )&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;roadmap&#34;&gt;&lt;/a&gt; Roadmap / To Do&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Tokenizers: download from the Hub, port from &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;&lt;code&gt;tokenizers&lt;/code&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; BPE family&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fix Falcon, broken while porting BPE&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Improve tests, add edge cases, see &lt;a href=&#34;https://github.com/xenova/transformers.js/raw/27920d84831e323275b38f0b5186644b7936e1a2/tests/generate_tests.py#L24&#34;&gt;https://github.com/xenova/transformers.js/blob/27920d84831e323275b38f0b5186644b7936e1a2/tests/generate_tests.py#L24&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Include fallback &lt;code&gt;tokenizer_config.json&lt;/code&gt; for known architectures whose models don&#39;t have a configuration in the Hub (GPT2)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Port other tokenizer types: Unigram, WordPiece&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/huggingface/exporters&#34;&gt;&lt;code&gt;exporters&lt;/code&gt;&lt;/a&gt; â€“ Core ML conversion tool. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Allow max sequence length to be specified.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Allow discrete shapes&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Return &lt;code&gt;logits&lt;/code&gt; from converted Core ML model&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Use &lt;code&gt;coremltools&lt;/code&gt; @ &lt;code&gt;main&lt;/code&gt; for latest fixes. In particular, &lt;a href=&#34;https://github.com/apple/coremltools/pull/1915&#34;&gt;this merged PR&lt;/a&gt; makes it easier to use recent versions of transformers.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generation &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Nucleus sampling (we currently have greedy and top-k sampling)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Use &lt;a href=&#34;https://developer.apple.com/documentation/accelerate/bnns#4164142&#34;&gt;new &lt;code&gt;top-k&lt;/code&gt; implementation in &lt;code&gt;Accelerate&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support discrete shapes in the underlying Core ML model by selecting the smallest sequence length larger than the input.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimization: cache past key-values.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Encoder-decoder models (T5)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;a href=&#34;https://github.com/huggingface/swift-chat&#34;&gt;Demo app&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Allow system prompt to be specified.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; How to define a system prompt template?&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Test a code model (to stretch system prompt definition)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/swift-transformers/main/LICENSE&#34;&gt;Apache 2&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ml-explore/mlx-swift-examples</title>
    <updated>2024-10-06T01:48:01Z</updated>
    <id>tag:github.com,2024-10-06:/ml-explore/mlx-swift-examples</id>
    <link href="https://github.com/ml-explore/mlx-swift-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Examples using MLX Swift&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX Swift Examples&lt;/h1&gt; &#xA;&lt;p&gt;Example &lt;a href=&#34;https://github.com/ml-explore/mlx-swift&#34;&gt;MLX Swift&lt;/a&gt; programs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-swift-examples/main/Applications/MNISTTrainer/README.md&#34;&gt;MNISTTrainer&lt;/a&gt;: An example that runs on both iOS and macOS that downloads MNIST training data and trains a &lt;a href=&#34;https://en.wikipedia.org/wiki/LeNet&#34;&gt;LeNet&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-swift-examples/main/Applications/LLMEval/README.md&#34;&gt;LLMEval&lt;/a&gt;: An example that runs on both iOS and macOS that downloads an LLM and tokenizer from Hugging Face and generates text from a given prompt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-swift-examples/main/Tools/LinearModelTraining/README.md&#34;&gt;LinearModelTraining&lt;/a&gt;: An example that trains a simple linear model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-swift-examples/main/Applications/StableDiffusionExample/README.md&#34;&gt;StableDiffusionExample&lt;/a&gt;: An example that runs on both iOS and macOS that downloads a stable diffusion model from Hugging Face and and generates an image from a given prompt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-swift-examples/main/Tools/llm-tool/README.md&#34;&gt;llm-tool&lt;/a&gt;: A command line tool for generating text using a variety of LLMs available on the Hugging Face hub.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-swift-examples/main/Tools/image-tool/README.md&#34;&gt;image-tool&lt;/a&gt;: A command line tool for generating images using a stable diffusion model from Hugging Face.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-swift-examples/main/Tools/mnist-tool/README.md&#34;&gt;mnist-tool&lt;/a&gt;: A command line tool for training a a LeNet on MNIST.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;The application and command line tool examples can be run from Xcode or from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./mlx-run llm-tool --prompt &#34;swift programming language&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See also:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ml-explore.github.io/mlx-swift/MLX/documentation/mlx/troubleshooting&#34;&gt;MLX troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation of MLXLLM and MLXMNIST libraries&lt;/h2&gt; &#xA;&lt;p&gt;The MLXLLM, MLXMNIST and StableDiffusion libraries in the example repo are available as Swift Packages.&lt;/p&gt; &#xA;&lt;p&gt;Add the following dependency to your Package.swift&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;.package(url: &#34;https://github.com/ml-explore/mlx-swift-examples/&#34;, branch: &#34;main&#34;),&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then add one library or both libraries to the target as a dependency.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;.target(&#xA;    name: &#34;YourTargetName&#34;,&#xA;    dependencies: [&#xA;        .product(name: &#34;LLM&#34;, package: &#34;mlx-swift-examples&#34;)&#xA;    ]),&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, add &lt;code&gt;https://github.com/ml-explore/mlx-swift-examples/&lt;/code&gt; to the &lt;code&gt;Project Dependencies&lt;/code&gt; and set the &lt;code&gt;Dependency Rule&lt;/code&gt; to &lt;code&gt;Branch&lt;/code&gt; and &lt;code&gt;main&lt;/code&gt; in Xcode.&lt;/p&gt;</summary>
  </entry>
</feed>