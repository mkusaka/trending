<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-23T02:04:05Z</updated>
  <subtitle>Weekly Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>grishka/NearDrop</title>
    <updated>2023-04-23T02:04:05Z</updated>
    <id>tag:github.com,2023-04-23:/grishka/NearDrop</id>
    <link href="https://github.com/grishka/NearDrop" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An unofficial Google Nearby Share app for macOS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;NearDrop&lt;/strong&gt; is a partial implementation of &lt;a href=&#34;https://blog.google/products/android/nearby-share/&#34;&gt;Google&#39;s Nearby Share&lt;/a&gt; for macOS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grishka/NearDrop/master/PROTOCOL.md&#34;&gt;Protocol documentation&lt;/a&gt; is available separately.&lt;/p&gt; &#xA;&lt;p&gt;The app lives in your menu bar and saves files to your downloads folder. It&#39;s that simple, really.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Receive only&lt;/strong&gt;. For now. I haven&#39;t yet figured out how to make Android turn on the MDNS service and/or show the &#34;a device nearby is sharing&#34; notification.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wi-Fi LAN only&lt;/strong&gt;. Your Android device and your Mac need to be on the same network for this app to work. Google&#39;s implementation supports multiple mediums, including Wi-Fi Direct, Wi-Fi hotspot, Bluetooth, some kind of 5G peer-to-peer connection, and even a WebRTC-based protocol that goes over the internet through Google servers. Wi-Fi direct isn&#39;t supported on macOS (Apple has their own, incompatible, AWDL thing, used in AirDrop). Bluetooth needs further reverse engineering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Visible to everyone on your network at all times&lt;/strong&gt; while the app is running. Limited visibility (contacts etc) requires talking to Google servers, and becoming temporarily visible requires listening for whatever triggers the &#34;device nearby is sharing&#34; notification.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download the latest build from the releases section, unzip, move to your applications folder. When running for the first time, right-click the app and select &#34;Open&#34;, then confirm running an app from unidentified developer.&lt;/p&gt; &#xA;&lt;p&gt;If you want the app to start on boot, add it manually to login objects in System Preferences.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests that change the readme will not be accepted.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h4&gt;Why is the app not notarized?&lt;/h4&gt; &#xA;&lt;p&gt;Because I don&#39;t want to pay Apple $99 a year for the privilege of developing macOS apps and oppose their idea of security.&lt;/p&gt; &#xA;&lt;h4&gt;Why is this not on the app store?&lt;/h4&gt; &#xA;&lt;p&gt;Because I don&#39;t want to pay Apple $99 a year for the privilege of developing macOS apps. I also don&#39;t want to have to go through the review process.&lt;/p&gt; &#xA;&lt;h4&gt;Why not the other way around, i.e. AirDrop on Android?&lt;/h4&gt; &#xA;&lt;p&gt;While I am an Android developer, and I have looked into this, this is nigh-impossible. AirDrop uses &lt;a href=&#34;https://stackoverflow.com/questions/19587701/what-is-awdl-apple-wireless-direct-link-and-how-does-it-work&#34;&gt;AWDL&lt;/a&gt;, Apple&#39;s own proprietary take on peer-to-peer Wi-Fi. This works on top of 802.11 itself, the low-level Wi-Fi protocol, and thus can not be implemented without messing around with the Wi-Fi adapter drivers and raw packets and all that. It might be possible on Android, but it would at the very least require root and possibly a custom kernel. There is &lt;a href=&#34;https://owlink.org/code/&#34;&gt;an open-source implementation of AWDL and AirDrop for Linux&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>alexrozanski/LlamaChat</title>
    <updated>2023-04-23T02:04:05Z</updated>
    <id>tag:github.com,2023-04-23:/alexrozanski/LlamaChat</id>
    <link href="https://github.com/alexrozanski/LlamaChat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with your favourite LLaMA models in a native macOS app&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alexrozanski/LlamaChat/main/Resources/banner-a5248619.png&#34; alt=&#34;LlamaChat banner&#34;&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;Chat with your favourite LLaMA models, right on your Mac&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;LlamaChat&lt;/strong&gt; is a macOS app that allows you to chat with &lt;a href=&#34;http://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;GPT4All&lt;/a&gt; models all running locally on your Mac.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/alexrozanski/LlamaChat/raw/main/Resources/screenshot.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h2&gt;🚀 Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;LlamaChat requires macOS 13 Ventura, and either an Intel or Apple Silicon processor.&lt;/p&gt; &#xA;&lt;h3&gt;Direct Download&lt;/h3&gt; &#xA;&lt;p&gt;Download a &lt;code&gt;.dmg&lt;/code&gt; containing the latest version &lt;a href=&#34;https://llamachat.app/api/download&#34;&gt;👉 here 👈&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Building from Source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/alexrozanski/LlamaChat.git&#xA;cd LlamaChat&#xA;open LlamaChat.xcodeproj&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; LlamaChat includes &lt;a href=&#34;https://github.com/sparkle-project/Sparkle&#34;&gt;Sparkle&lt;/a&gt; for autoupdates, which will fail to load if LlamaChat is not signed. Ensure that you use a valid signing certificate when building and running LlamaChat.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; model inference runs really slowly in Debug builds, so if building from source make sure that the &lt;code&gt;Build Configuration&lt;/code&gt; in &lt;code&gt;LlamaChat &amp;gt; Edit Scheme... &amp;gt; Run&lt;/code&gt; is set to &lt;code&gt;Release&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;✨ Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supported Models:&lt;/strong&gt; LlamaChat supports LLaMA, Alpaca and GPT4All models out of the box. Support for other models including &lt;a href=&#34;https://vicuna.lmsys.org/&#34;&gt;Vicuna&lt;/a&gt; and &lt;a href=&#34;https://bair.berkeley.edu/blog/2023/04/03/koala/&#34;&gt;Koala&lt;/a&gt; is coming soon. We are also looking for Chinese and French speakers to add support for &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese LLaMA/Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/bofenghuang/vigogne&#34;&gt;Vigogne&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible Model Formats:&lt;/strong&gt; LLamaChat is built on top of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/alexrozanski/llama.swift&#34;&gt;llama.swift&lt;/a&gt;. The app supports adding LLaMA models in either their raw &lt;code&gt;.pth&lt;/code&gt; PyTorch checkpoints form or the &lt;code&gt;.ggml&lt;/code&gt; format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Conversion:&lt;/strong&gt; If raw PyTorch checkpoints are added these can be converted to &lt;code&gt;.ggml&lt;/code&gt; files compatible with LlamaChat and llama.cpp within the app.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat History:&lt;/strong&gt; Chat history is persisted within the app. Both chat history and model context can be cleared at any time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Funky Avatars:&lt;/strong&gt; LlamaChat ships with &lt;a href=&#34;https://github.com/alexrozanski/LlamaChat/tree/main/LlamaChat/Assets.xcassets/avatars&#34;&gt;7 funky avatars&lt;/a&gt; that can be used with your chat sources.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced Source Naming:&lt;/strong&gt; LlamaChat uses Special Magic™ to generate playful names for your chat sources.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Context Debugging:&lt;/strong&gt; For the keen ML enthusiasts, the current model context can be viewed for a chat in the info popover.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔮 Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; LlamaChat doesn&#39;t ship with any model files and requires that you obtain these from the respective sources in accordance with their respective terms and conditions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model formats:&lt;/strong&gt; LlamaChat allows you to use the LLaMA family of models in either their raw Python checkpoint form (&lt;code&gt;.pth&lt;/code&gt;) or pre-converted &lt;code&gt;.ggml&lt;/code&gt; file (the format used by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, which powers LlamaChat).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Using LLaMA models:&lt;/strong&gt; When importing LLaMA models in the &lt;code&gt;.pth&lt;/code&gt; format: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You should select the appropriate parameter size directory (e.g. &lt;code&gt;7B&lt;/code&gt;, &lt;code&gt;13B&lt;/code&gt; etc) in the conversion flow, which includes the &lt;code&gt;consolidated.NN.pth&lt;/code&gt; and &lt;code&gt;params.json&lt;/code&gt; files.&lt;/li&gt; &#xA;   &lt;li&gt;As per the LLaMA model release, the parent directory should contain &lt;code&gt;tokenizer.model&lt;/code&gt;. E.g. to use the LLaMA-13B model, your model directory should look something like the below, and you should select the &lt;code&gt;13B&lt;/code&gt; directory:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.&#xA;│   ...&#xA;├── 13B&#xA;│&amp;nbsp;&amp;nbsp; ├── checklist.chk.txt&#xA;│&amp;nbsp;&amp;nbsp; ├── consolidated.00.pth&#xA;│&amp;nbsp;&amp;nbsp; ├── consolidated.01.pth&#xA;│&amp;nbsp;&amp;nbsp; └── params.json&#xA;│   ...&#xA;└── tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Troubleshooting:&lt;/strong&gt; If using &lt;code&gt;.ggml&lt;/code&gt; files, make sure these are up-to-date. If you run into problems, you may need to use the conversion scripts from &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For the GPT4All model, you may need to use &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/master/convert-gpt4all-to-ggml.py&#34;&gt;convert-gpt4all-to-ggml.py&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;For the Alpaca model, you may need to use &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/master/convert-unversioned-ggml-to-ggml.py&#34;&gt;convert-unversioned-ggml-to-ggml.py&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You may also need to use &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/master/migrate-ggml-2023-03-30-pr613.py&#34;&gt;migrate-ggml-2023-03-30-pr613.py&lt;/a&gt; as well. For more information check out the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; repo.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;👩‍💻 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull Requests and Issues are welcome and much appreciated. Please make sure to adhere to the &lt;a href=&#34;https://raw.githubusercontent.com/alexrozanski/LlamaChat/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt; at all times.&lt;/p&gt; &#xA;&lt;p&gt;LlamaChat is fully built using Swift and SwiftUI, and makes use of &lt;a href=&#34;https://github.com/alexrozanski/llama.swift&#34;&gt;llama.swift&lt;/a&gt; under the hood to run inference and perform model operations.&lt;/p&gt; &#xA;&lt;p&gt;The project is mostly built using MVVM and makes heavy use of Combine and Swift Concurrency.&lt;/p&gt; &#xA;&lt;h2&gt;⚖️ License&lt;/h2&gt; &#xA;&lt;p&gt;LlamaChat is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/alexrozanski/LlamaChat/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Sileo/Sileo</title>
    <updated>2023-04-23T02:04:05Z</updated>
    <id>tag:github.com,2023-04-23:/Sileo/Sileo</id>
    <link href="https://github.com/Sileo/Sileo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modern package manager for iOS 12 and higher.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Sileo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Sileo/Sileo/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/Sileo/Sileo/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A modern APT package manager frontend&lt;/p&gt; &#xA;&lt;h1&gt;Info&lt;/h1&gt; &#xA;&lt;p&gt;Sileo focuses on speed, features, and a modern feel. It is made with love by people from all over the world!&lt;/p&gt; &#xA;&lt;p&gt;Our official Twitter is &lt;a href=&#34;https://twitter.com/getsileo&#34;&gt;@GetSileo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Support&lt;/h1&gt; &#xA;&lt;p&gt;For support, ask in the &lt;a href=&#34;https://discord.com/invite/Udn4kQg&#34;&gt;Sileo Discord server&lt;/a&gt; or contact &lt;a href=&#34;https://twitter.com/sileosupport&#34;&gt;@SileoSupport on Twitter&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Support the project&lt;/h1&gt; &#xA;&lt;p&gt;If you would like to help support the development of Sileo, consider donating at the following links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amy (Sileo Developer): &lt;a href=&#34;https://www.patreon.com/elihwyma&#34;&gt;Patreon&lt;/a&gt;, &lt;a href=&#34;https://paypal.me/anamy1024&#34;&gt;Paypal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Aarnav (Canister Developer/Maintainer): &lt;a href=&#34;https://github.com/sponsors/tale&#34;&gt;Github Sponsors&lt;/a&gt;, &lt;a href=&#34;https://www.patreon.com/aarnavtale&#34;&gt;Patreon&lt;/a&gt;, &lt;a href=&#34;https://paypal.me/aatale&#34;&gt;Paypal&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contribute&lt;/h1&gt; &#xA;&lt;p&gt;For localization, &lt;a href=&#34;https://crowdin.com/project/sileo&#34;&gt;join our Crowdin project&lt;/a&gt; and submit your translations there.&lt;/p&gt; &#xA;&lt;p&gt;For software, make a Pull Request with your changes and our team will review it.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone this repository&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone --recursive https://github.com/Sileo/Sileo&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the &lt;code&gt;DEVELOPMENT_TEAM&lt;/code&gt; Build Setting&lt;/p&gt; &lt;p&gt;There are multiple ways to do this, for example:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Using Xcode Custom Paths &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Go to Xcode &amp;gt; Preferences &amp;gt; Locations &amp;gt; Custom Paths&lt;/li&gt; &#xA;     &lt;li&gt;Add an entry with &lt;code&gt;Name&lt;/code&gt; as &lt;code&gt;DEVELOPMENT_TEAM&lt;/code&gt;, &lt;code&gt;Display Name&lt;/code&gt; as &lt;code&gt;Development Team&lt;/code&gt;, and &lt;code&gt;Path&lt;/code&gt; as your Apple Developer Team ID&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Using Xcode Build Settings &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Set the &lt;code&gt;Development Team&lt;/code&gt; Build Setting&lt;/li&gt; &#xA;     &lt;li&gt;Remember to never commit this change&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Apply our git hooks by running: &lt;code&gt;git config core.hooksPath .githooks&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open &lt;code&gt;Sileo.xcodeproj&lt;/code&gt; and have at it!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you have questions, ask in the Sileo Discord server.&lt;/p&gt; &#xA;&lt;h1&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Sileo Team 2018 - 2023&lt;/p&gt;</summary>
  </entry>
</feed>