<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-19T01:57:11Z</updated>
  <subtitle>Daily Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Sigil-Wen/Dream-with-Vision-Pro</title>
    <updated>2023-07-19T01:57:11Z</updated>
    <id>tag:github.com,2023-07-19:/Sigil-Wen/Dream-with-Vision-Pro</id>
    <link href="https://github.com/Sigil-Wen/Dream-with-Vision-Pro" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Text to 3D generation in Apple Vision Pro built with the VisionOS SDK. 3D Scribblenauts in AR for the Scale Generative AI Hackathon. Won Scale AI Prize&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sigil-Wen/Dream-with-Vision-Pro/main/image-4.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Dream with Vision Pro&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/C6ukDBEbFY&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1126234207044247622&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to Dream with Vision Pro, a lucid text-to-3D tool built with the Apple VisionOS SDK. Powered by Scale AI&#39;s Spellbook, OpenAI&#39;s GPT-4 and Shap-E, Modal, Replicate, and the Meta Quest 2, we empower you to transform your imagination into stunning immersive experiences.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sigil-Wen/Dream-with-Vision-Pro/main/image.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Enter Your Vision:&lt;/h2&gt; &#xA;&lt;p&gt;Type in the text description of the object you envision. This could be anything from an elephant to a sword. Unleash your imagination. Once you&#39;ve described it, your object will appear before you.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sigil-Wen/Dream-with-Vision-Pro/main/image-3.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Using Scale AI&#39;s Spellbound to infer the size of the objects to render accurately.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Sigil-Wen/Dream-with-Vision-Pro/main/image-1.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;How it Works&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a step-by-step breakdown of what Dream with Vision Pro does:&lt;/p&gt; &#xA;&lt;p&gt;First, the user specifies the object they want to visualize. This input triggers the &lt;a href=&#34;https://github.com/openai/shap-e&#34;&gt;Shap-E&lt;/a&gt; model via &lt;a href=&#34;https://mcantillon21--dream-fastapi-app.modal.run/&#34;&gt;Modal&lt;/a&gt; and &lt;a href=&#34;https://replicate.com/&#34;&gt;Replicate&lt;/a&gt;, producing a .obj file - a standard 3D model format.&lt;/p&gt; &#xA;&lt;p&gt;Next, we employ &lt;a href=&#34;https://dashboard.scale.com/spellbook/api/v2/deploy/9f33d7g&#34;&gt;Spellbook&lt;/a&gt; and GPT-4 to estimate the object&#39;s height, ensuring the 3D representation is accurately scaled.&lt;/p&gt; &#xA;&lt;p&gt;The final phase employs &lt;a href=&#34;https://3dviewer.net&#34;&gt;3D Viewer&lt;/a&gt; to convert your .obj file into a realistic 3D model that you can interact with. This 3D model can be directly accessed from Apple&#39;s VisionOS, which we stream directly to your Meta Quest 2, offering a fully immersive experience of your original concept.&lt;/p&gt; &#xA;&lt;h2&gt;Spellbook Prompts&lt;/h2&gt; &#xA;&lt;h3&gt;System:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;As an AI system, you are extremely skilled at extracting objects and estimating their realistic height in meters from a given text prompt. Your task is to identify the object(s) mentioned in the prompt and their estimated height in meters. Once identified, the information must be formatted according to the provided format for a text-to-3D model application.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;User:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Could you extract the object and realistic object height in meters from the following text prompts?&#xA;&#xA;Begin:&#xA;&#xA;Input: a red apple&#xA;Output: 0.075&#xA;&#xA;Input: a large elephant&#xA;Output: 3.000&#xA;&#xA;&#xA;Input: {{ input }}&#xA;Output:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Next Steps&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve started to integrate OpenAI&#39;s Whisper model, expanding our capability beyond text-to-3D transformations. Users will be able to engage in a more intuitive way, interacting with their 3D creations through the power of voice.&lt;/p&gt; &#xA;&lt;p&gt;Once we have the .obj file, we are working on using &lt;a href=&#34;https://developer.apple.com/augmented-reality/tools/&#34;&gt;USZD Tools&lt;/a&gt; which lets us convert to the .usdz format - a requisite for VisionOS. Following this conversion, we can seamlessly render the objects.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank the Scale AI Spellbook team for the credits and ease of use, Ben Firshman of Replicate for the dedicated A100 GPU we run Shap-E on, Erik Bernhardsson of Modal for dedicated Whisper and hosted endpoints, and especially Mehran Jalali for letting us borrow the Meta Quest 2 for testing.&lt;/p&gt;</summary>
  </entry>
</feed>