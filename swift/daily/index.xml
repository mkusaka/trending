<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-15T02:59:49Z</updated>
  <subtitle>Daily Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>alexrozanski/LlamaChat</title>
    <updated>2023-04-15T02:59:49Z</updated>
    <id>tag:github.com,2023-04-15:/alexrozanski/LlamaChat</id>
    <link href="https://github.com/alexrozanski/LlamaChat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with your favourite LLaMA models in a native macOS app&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alexrozanski/LlamaChat/main/Resources/banner-a5248619.png&#34; alt=&#34;LlamaChat banner&#34;&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;Chat with your favourite LLaMA models, right on your Mac&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;LlamaChat&lt;/strong&gt; is a macOS app that allows you to chat with &lt;a href=&#34;http://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;, &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;GPT4All&lt;/a&gt; models all running locally on your Mac.&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/alexrozanski/LlamaChat/raw/main/Resources/screenshot.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;LlamaChat requires macOS 13 Ventura, and either an Intel or Apple Silicon processor.&lt;/p&gt; &#xA;&lt;h3&gt;Direct Download&lt;/h3&gt; &#xA;&lt;p&gt;Download a &lt;code&gt;.dmg&lt;/code&gt; containing the latest version &lt;a href=&#34;https://llamachat.app/api/download&#34;&gt;üëâ here üëà&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Building from Source&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/alexrozanski/LlamaChat.git&#xA;cd LlamaChat&#xA;open LlamaChat.xcodeproj&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; model inference runs really slowly in Debug builds, so if building from source make sure that the &lt;code&gt;Build Configuration&lt;/code&gt; in &lt;code&gt;LlamaChat &amp;gt; Edit Scheme... &amp;gt; Run&lt;/code&gt; is set to &lt;code&gt;Release&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supported Models:&lt;/strong&gt; LlamaChat supports LLaMA, Alpaca and GPT4All models out of the box. Support for other models including &lt;a href=&#34;https://vicuna.lmsys.org/&#34;&gt;Vicuna&lt;/a&gt; and &lt;a href=&#34;https://bair.berkeley.edu/blog/2023/04/03/koala/&#34;&gt;Koala&lt;/a&gt; is coming soon. We are also looking for Chinese and French speakers to add support for &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese LLaMA/Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/bofenghuang/vigogne&#34;&gt;Vigogne&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible Model Formats:&lt;/strong&gt; LLamaChat is built on top of &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/alexrozanski/llama.swift&#34;&gt;llama.swift&lt;/a&gt;. The app supports adding LLaMA models in either their raw &lt;code&gt;.pth&lt;/code&gt; PyTorch checkpoints form or the &lt;code&gt;.ggml&lt;/code&gt; format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Conversion:&lt;/strong&gt; If raw PyTorch checkpoints are added these can be converted to &lt;code&gt;.ggml&lt;/code&gt; files compatible with LlamaChat and llama.cpp within the app.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chat History:&lt;/strong&gt; Chat history is persisted within the app. Both chat history and model context can be cleared at any time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Funky Avatars:&lt;/strong&gt; LlamaChat ships with &lt;a href=&#34;https://github.com/alexrozanski/LlamaChat/tree/main/LlamaChat/Assets.xcassets/avatars&#34;&gt;7 funky avatars&lt;/a&gt; that can be used with your chat sources.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced Source Naming:&lt;/strong&gt; LlamaChat uses Special Magic‚Ñ¢ to generate playful names for your chat sources.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Context Debugging:&lt;/strong&gt; For the keen ML enthusiasts, the current model context can be viewed for a chat in the info popover.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîÆ Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; LlamaChat doesn&#39;t ship with any model files and requires that you obtain these from the respective sources in accordance with their respective terms and conditions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model formats:&lt;/strong&gt; LlamaChat allows you to use the LLaMA family of models in either their raw Python checkpoint form (&lt;code&gt;.pth&lt;/code&gt;) or pre-converted &lt;code&gt;.ggml&lt;/code&gt; file (the format used by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;, which powers LlamaChat).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Using LLaMA models:&lt;/strong&gt; When importing LLaMA models in the &lt;code&gt;.pth&lt;/code&gt; format: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You should select the appropriate parameter size directory (e.g. &lt;code&gt;7B&lt;/code&gt;, &lt;code&gt;13B&lt;/code&gt; etc) in the conversion flow, which includes the &lt;code&gt;consolidated.NN.pth&lt;/code&gt; and &lt;code&gt;params.json&lt;/code&gt; files.&lt;/li&gt; &#xA;   &lt;li&gt;As per the LLaMA model release, the parent directory should contain &lt;code&gt;tokenizer.model&lt;/code&gt;. E.g. to use the LLaMA-13B model, your model directory should look something like the below, and you should select the &lt;code&gt;13B&lt;/code&gt; directory:&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.&#xA;‚îÇ   ...&#xA;‚îú‚îÄ‚îÄ 13B&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ checklist.chk.txt&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ consolidated.00.pth&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îú‚îÄ‚îÄ consolidated.01.pth&#xA;‚îÇ&amp;nbsp;&amp;nbsp; ‚îî‚îÄ‚îÄ params.json&#xA;‚îÇ   ...&#xA;‚îî‚îÄ‚îÄ tokenizer.model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Troubleshooting:&lt;/strong&gt; If using &lt;code&gt;.ggml&lt;/code&gt; files, make sure these are up-to-date. If you run into problems, you may need to use the conversion scripts from &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For the GPT4All model, you may need to use &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/master/convert-gpt4all-to-ggml.py&#34;&gt;convert-gpt4all-to-ggml.py&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;For the Alpaca model, you may need to use &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/master/convert-unversioned-ggml-to-ggml.py&#34;&gt;convert-unversioned-ggml-to-ggml.py&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;You may also need to use &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/raw/master/migrate-ggml-2023-03-30-pr613.py&#34;&gt;migrate-ggml-2023-03-30-pr613.py&lt;/a&gt; as well. For more information check out the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; repo.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üë©‚Äçüíª Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull Requests and Issues are welcome and much appreciated. Please make sure to adhere to the &lt;a href=&#34;https://raw.githubusercontent.com/alexrozanski/LlamaChat/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt; at all times.&lt;/p&gt; &#xA;&lt;p&gt;LlamaChat is fully built using Swift and SwiftUI, and makes use of &lt;a href=&#34;https://github.com/alexrozanski/llama.swift&#34;&gt;llama.swift&lt;/a&gt; under the hood to run inference and perform model operations.&lt;/p&gt; &#xA;&lt;p&gt;The project is mostly built using MVVM and makes heavy use of Combine and Swift Concurrency.&lt;/p&gt; &#xA;&lt;h2&gt;‚öñÔ∏è License&lt;/h2&gt; &#xA;&lt;p&gt;LlamaChat is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/alexrozanski/LlamaChat/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>