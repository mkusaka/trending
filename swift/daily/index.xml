<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-17T01:47:37Z</updated>
  <subtitle>Daily Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>guinmoon/LLMFarm</title>
    <updated>2023-12-17T01:47:37Z</updated>
    <id>tag:github.com,2023-12-17:/guinmoon/LLMFarm</id>
    <link href="https://github.com/guinmoon/LLMFarm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;llama and other large language models on iOS and MacOS offline using GGML library.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;LLMFarm&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;If you are experiencing the error &lt;code&gt;extras.otherInstrOffset != 0 &amp;amp;&amp;amp; &#34;Kind::arm64_adrp_ldr missing extra info&#34;&lt;/code&gt; see the solution &lt;a href=&#34;https://github.com/guinmoon/LLMFarm/issues/19&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://apps.apple.com/ru/app/llm-farm/id6461209867?l=en-GB&amp;amp;platform=iphone&#34;&gt;&lt;img width=&#34;178px&#34; alt=&#34;Icon&#34; src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/LLMFarm0.1.2_256.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://testflight.apple.com/join/6SpPLIVM&#34;&gt;&lt;img width=&#34;178px&#34; alt=&#34;Icon&#34; src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/testflight.png&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://apps.apple.com/ru/app/llm-farm/id6461209867?l=en-GB&amp;amp;platform=iphone&#34;&gt;&lt;strong&gt;Install Stable&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/strong&gt;&lt;/a&gt; &lt;a href=&#34;https://testflight.apple.com/join/6SpPLIVM&#34;&gt;&lt;strong&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Install Latest&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Icon&#34; height=&#34;400px&#34; src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/screen1.png&#34;&gt; &lt;img alt=&#34;Icon&#34; width=&#34;525px&#34; src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/screen2.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;LLMFarm is an iOS and MacOS app to work with large language models (LLM). It allows you to load different LLMs with certain parameters.With LLMFarm, you can test the performance of different LLMs on iOS and macOS and find the most suitable model for your project.&lt;br&gt; Based on &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;ggml&lt;/a&gt; and &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; by &lt;a href=&#34;https://github.com/ggerganov&#34;&gt;Georgi Gerganov&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Also used sources from:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp&#34;&gt;rwkv.cpp&lt;/a&gt; by &lt;a href=&#34;https://github.com/saharNooby&#34;&gt;saharNooby&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/byroneverson/Mia&#34;&gt;Mia&lt;/a&gt; by &lt;a href=&#34;https://github.com/byroneverson&#34;&gt;byroneverson&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alexrozanski/LlamaChat&#34;&gt;LlamaChat&lt;/a&gt; by &lt;a href=&#34;https://github.com/alexrozanski&#34;&gt;alexrozanski&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; MacOS (13+)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; iOS (16+)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Various inferences&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Various sampling methods&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Metal (&lt;a href=&#34;https://github.com/ggerganov/llama.cpp/issues/2407#issuecomment-1699544808&#34;&gt;dont work&lt;/a&gt; on intel Mac)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Model setting templates&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LoRA adapters support (&lt;a href=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/lora.md&#34;&gt;read more&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LoRA finetune support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LoRA export as model support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Restore context state (now only chat history)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Inferences&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/metal-96x96_2x.png&#34; width=&#34;16px&#34; heigth=&#34;16px&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt_neox&#34;&gt;GPTNeoX&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/replit/replit-code-v1-3b&#34;&gt;Replit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/gpt2&#34;&gt;GPT2&lt;/a&gt; + &lt;a href=&#34;https://arxiv.org/abs/2304.03208&#34;&gt;Cerebras&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/metal-96x96_2x.png&#34; width=&#34;16px&#34; heigth=&#34;16px&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/bigcode/santacoder&#34;&gt;Starcoder(Santacoder)&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/metal-96x96_2x.png&#34; width=&#34;16px&#34; heigth=&#34;16px&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/rwkv&#34;&gt;RWKV&lt;/a&gt; (20B tokenizer)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/cmp-nct/ggllm.cpp&#34;&gt;Falcon&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/metal-96x96_2x.png&#34; width=&#34;16px&#34; heigth=&#34;16px&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/guinmoon/mpt-7b-storywriter-GGUF&#34;&gt;MPT&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/metal-96x96_2x.png&#34; width=&#34;16px&#34; heigth=&#34;16px&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/guinmoon/bloomz-1b7-gguf&#34;&gt;Bloom&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/metal-96x96_2x.png&#34; width=&#34;16px&#34; heigth=&#34;16px&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-3b-4e1t&#34;&gt;StableLM-3b-4e1t&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/metal-96x96_2x.png&#34; width=&#34;16px&#34; heigth=&#34;16px&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/Qwen/Qwen-7B&#34;&gt;Qwen&lt;/a&gt; &lt;img src=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/dist/metal-96x96_2x.png&#34; width=&#34;16px&#34; heigth=&#34;16px&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note: For &lt;em&gt;Falcon, Alpaca, GPT4All, Chinese LLaMA / Alpaca and Chinese LLaMA-2 / Alpaca-2, Vigogne (French), Vicuna, Koala, OpenBuddy (Multilingual), Pygmalion/Metharme, WizardLM, Baichuan 1 &amp;amp; 2 + derivations, Aquila 1 &amp;amp; 2, Mistral AI v0.1, Refact, Persimmon 8B, MPT, Bloom&lt;/em&gt; select &lt;code&gt;llama inferece&lt;/code&gt; in model settings.&lt;/p&gt; &#xA;&lt;h1&gt;Sampling methods&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Temperature (temp, tok-k, top-p)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://www.trentonbricken.com/Tail-Free-Sampling/&#34;&gt;Tail Free Sampling (TFS)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://arxiv.org/abs/2202.00666&#34;&gt;Locally Typical Sampling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://arxiv.org/abs/2007.14966&#34;&gt;Mirostat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Greedy&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Grammar (dont work for GPTNeoX, GPT-2, RWKV)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Classifier-Free Guidance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;You can find answers to some questions in the &lt;a href=&#34;https://github.com/guinmoon/LLMFarm/wiki/FAQ&#34;&gt;FAQ section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Inference options&lt;/h2&gt; &#xA;&lt;p&gt;When creating a chat, a JSON file is generated in which you can specify additional inference options. The chat files are located in the &#34;chats&#34; directory. You can see all inference options &lt;a href=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/inference_options.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;You can download some of the supported &lt;a href=&#34;https://raw.githubusercontent.com/guinmoon/LLMFarm/main/models.md&#34;&gt;models here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;llmfarm_core&lt;/code&gt; has been moved to a &lt;a href=&#34;https://github.com/guinmoon/llmfarm_core.swift&#34;&gt;separate repository&lt;/a&gt;. To build llmfarm, you need to clone this repository recursively:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/guinmoon/LLMFarm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;P.S.&lt;/h1&gt; &#xA;&lt;p&gt;The number of open source models is constantly growing. One of the advantages of using such models is the possibility of preserving their original content without censorship. However, a disadvantage may be the irrelevance of the information they contain. It is also possible to get answers to questions from different industries, for example, there are models that specialize in medical terms or programming. In addition, you can use these models to create stories, songs, music, play quests, and so on...&lt;/p&gt;</summary>
  </entry>
</feed>