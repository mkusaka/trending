<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-06T01:36:40Z</updated>
  <subtitle>Daily Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PreternaturalAI/mlx-swift-chat</title>
    <updated>2024-03-06T01:36:40Z</updated>
    <id>tag:github.com,2024-03-06:/PreternaturalAI/mlx-swift-chat</id>
    <link href="https://github.com/PreternaturalAI/mlx-swift-chat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A multi-platform SwiftUI frontend for running local LLMs with Apple&#39;s MLX framework.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;mlx-swift-chat&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/PreternaturalAI/mlx-swift-chat/assets/8635253/f20862f3-8cab-4803-ba6e-44108b075c9b&#34;&gt;https://github.com/PreternaturalAI/mlx-swift-chat/assets/8635253/f20862f3-8cab-4803-ba6e-44108b075c9b&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run LLM models locally with MLX!&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;MLX is an efficient machine learning framework specifically designed for Apple silicon (i.e. your laptop!)&lt;/p&gt; &#xA; &lt;p&gt;– &lt;a href=&#34;https://twitter.com/awnihannun/status/1732184443451019431&#34;&gt;@awnihannun&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This project is a fully native SwiftUI app that allows you to run local LLMs (e.g. Llama, Mistral) on Apple silicon in real-time using &lt;a href=&#34;https://github.com/ml-explore/mlx&#34;&gt;MLX&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open the Xcode project.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;strong&gt;Signing &amp;amp; Capabilities&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Change the &lt;strong&gt;Team&lt;/strong&gt; to your own team.&lt;/li&gt; &#xA; &lt;li&gt;Set the destination to &lt;strong&gt;My Mac&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;Run&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Support for iOS is coming next week.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Click on &lt;strong&gt;Manage Models&lt;/strong&gt; in the inspector view.&lt;/li&gt; &#xA; &lt;li&gt;Download and install a model (we recommend starting with &lt;code&gt;Nous-Hermes-2-Mistral-7B-DPO-4bit-MLX&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Go back to the inspector and select the downloaded model from the model picker.&lt;/li&gt; &#xA; &lt;li&gt;Wait for the model to load, the status bar will flash &#34;Ready&#34; once it is loaded.&lt;/li&gt; &#xA; &lt;li&gt;Click the run button.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img width=&#34;300&#34; alt=&#34;Screenshot 2024-03-02 at 6 44 24 PM&#34; src=&#34;https://github.com/PreternaturalAI/mlx-swift-chat/assets/8635253/37dead8a-f943-4411-b50e-ab1731b46bfc&#34;&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fix iOS builds&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement support for StableLM&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement basic support automatically adding model-specific chat templates to the prompt&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add support for stop sequences&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add more model suggestions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ... (many, &lt;em&gt;many&lt;/em&gt; more items to be added soon pending sleep)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;h3&gt;What models are currently supported?&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral&lt;/td&gt; &#xA;   &lt;td&gt;Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi&lt;/td&gt; &#xA;   &lt;td&gt;Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;Supported (May have issues)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;How do I add new models?&lt;/h3&gt; &#xA;&lt;p&gt;Models are downloaded from Hugging Face. To add a new model, visit the &lt;a href=&#34;https://huggingface.co/mlx-community&#34;&gt;MLX Community on HuggingFace&lt;/a&gt; and search for the model you want, then add it via &lt;strong&gt;Manage Models&lt;/strong&gt; → &lt;strong&gt;Add Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Note that this project is still under active development and some models may require additional implementation to run correctly.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Is this suitable for production?&lt;/h3&gt; &#xA;&lt;p&gt;No. This is not intended for deploying into production.&lt;/p&gt; &#xA;&lt;h3&gt;What are the minimum hardware and software requirements?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple Silicon Mac (M1/M2/M3) with macOS 14.0 or newer.&lt;/li&gt; &#xA; &lt;li&gt;Any A-Series chip (iPad, iPhone) with iOS 17.2 or newer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Does this collect any data?&lt;/h3&gt; &#xA;&lt;p&gt;No. Everything is run locally on device.&lt;/p&gt; &#xA;&lt;h3&gt;What are the parameters?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Temperature&lt;/strong&gt;: Controls randomness. Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Top K&lt;/strong&gt;: Sort predicted tokens by probability and discards those below the k-th one. A top-k value of 1 is equivalent to greedy search (select the most probable token).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Maximum length&lt;/strong&gt;: The maximum number of tokens to generate. Requests can use up to 2,048 tokens shared between prompt and completion. The exact limit varies by model. (One token is roughly 4 characters for normal English text)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/buh/CompactSlider&#34;&gt;buh/CompactSlider&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ml-explore/mlx-swift&#34;&gt;ml-explore/mlx-swift&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/swift-chat&#34;&gt;huggingface/swift-chat&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://github.com/awni&#34;&gt;Awni Hannun&lt;/a&gt; and &lt;a href=&#34;https://github.com/davidkoski&#34;&gt;David Koski&lt;/a&gt; for early testing and feedback&lt;/p&gt; &#xA;&lt;p&gt;Much ❤️ to all the folks who made MLX (especially mlx-swift) possible!&lt;/p&gt;</summary>
  </entry>
</feed>