<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-29T01:36:30Z</updated>
  <subtitle>Daily Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lzell/AIProxySwift</title>
    <updated>2025-04-29T01:36:30Z</updated>
    <id>tag:github.com,2025-04-29:/lzell/AIProxySwift</id>
    <link href="https://github.com/lzell/AIProxySwift" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Swift client for AI providers. Can make requests straight to the provider or proxied through our API key protection backend&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;About&lt;/h1&gt; &#xA;&lt;p&gt;Use this library to adopt AI APIs in your app. Swift clients for the following providers are included:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;OpenAI&lt;/li&gt; &#xA; &lt;li&gt;Gemini&lt;/li&gt; &#xA; &lt;li&gt;Anthropic&lt;/li&gt; &#xA; &lt;li&gt;Stability AI&lt;/li&gt; &#xA; &lt;li&gt;DeepL&lt;/li&gt; &#xA; &lt;li&gt;Together AI&lt;/li&gt; &#xA; &lt;li&gt;Replicate&lt;/li&gt; &#xA; &lt;li&gt;ElevenLabs&lt;/li&gt; &#xA; &lt;li&gt;Fal&lt;/li&gt; &#xA; &lt;li&gt;Groq&lt;/li&gt; &#xA; &lt;li&gt;Perplexity&lt;/li&gt; &#xA; &lt;li&gt;Mistral&lt;/li&gt; &#xA; &lt;li&gt;EachAI&lt;/li&gt; &#xA; &lt;li&gt;OpenRouter&lt;/li&gt; &#xA; &lt;li&gt;DeepSeek&lt;/li&gt; &#xA; &lt;li&gt;Fireworks AI&lt;/li&gt; &#xA; &lt;li&gt;Brave&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Your initialization code determines whether requests go straight to the provider or are protected through the &lt;a href=&#34;https://www.aiproxy.pro&#34;&gt;AIProxy&lt;/a&gt; backend.&lt;/p&gt; &#xA;&lt;p&gt;We only recommend making requests straight to the provider during prototyping and for BYOK use-cases.&lt;/p&gt; &#xA;&lt;p&gt;Requests that are protected through AIProxy have five levels of security applied to keep your API key secure and your AI bill predictable:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Certificate pinning&lt;/li&gt; &#xA; &lt;li&gt;DeviceCheck verification&lt;/li&gt; &#xA; &lt;li&gt;Split key encryption&lt;/li&gt; &#xA; &lt;li&gt;Per user rate limits&lt;/li&gt; &#xA; &lt;li&gt;Per IP rate limits&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;How to add this package as a dependency to your Xcode project&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;From within your Xcode project, select &lt;code&gt;File &amp;gt; Add Package Dependencies&lt;/code&gt;&lt;/p&gt; &lt;img src=&#34;https://github.com/lzell/AIProxySwift/assets/35940/d44698a0-34e6-434b-b501-390254a14439&#34; alt=&#34;Add package dependencies&#34; width=&#34;420&#34;&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Punch &lt;code&gt;github.com/lzell/aiproxyswift&lt;/code&gt; into the package URL bar, and select the &#39;main&#39; branch as the dependency rule. Alternatively, you can choose specific releases if you&#39;d like to have finer control of when your dependency gets updated.&lt;/p&gt; &lt;img src=&#34;https://github.com/lzell/AIProxySwift/assets/35940/fd76b588-5e19-4d4d-9748-8db3fd64df8e&#34; alt=&#34;Set package rule&#34; width=&#34;720&#34;&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Call &lt;code&gt;AIProxy.configure&lt;/code&gt; during app launch. In a SwiftUI app, you can add an &lt;code&gt;init&lt;/code&gt; to your &lt;code&gt;MyApp.swift&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import AIProxy&#xA;&#xA;@main&#xA;struct MyApp: App {&#xA;    init() {&#xA;        AIProxy.configure(&#xA;            logLevel: .debug,&#xA;            printRequestBodies: false,  // Flip to true for library development&#xA;            printResponseBodies: false, // Flip to true for library development&#xA;            resolveDNSOverTLS: true,&#xA;            useStableID: true&#xA;        )&#xA;    }&#xA;    // ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In a UIKit app, add &lt;code&gt;configure&lt;/code&gt; to applicationDidFinishLaunching:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import AIProxy&#xA;&#xA;@UIApplicationMain&#xA;class AppDelegate: UIResponder, UIApplicationDelegate {&#xA;&#xA;    var window: UIWindow?&#xA;&#xA;    func application(_ application: UIApplication,&#xA;                     didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?) -&amp;gt; Bool {&#xA;        AIProxy.configure(&#xA;            logLevel: .debug,&#xA;            printRequestBodies: false,  // Flip to true for library development&#xA;            printResponseBodies: false, // Flip to true for library development&#xA;            resolveDNSOverTLS: true,&#xA;            useStableID: true&#xA;        )&#xA;        // ...&#xA;        return true&#xA;    }&#xA;    // ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;How to configure the package for use with AIProxy&lt;/h3&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://www.aiproxy.pro/docs/integration-guide.html&#34;&gt;AIProxy integration video&lt;/a&gt;. Note that this is not required if you are shipping an app where the customers provide their own API keys (known as BYOK for &#34;bring your own key&#34;).&lt;/p&gt; &#xA;&lt;p&gt;If you are shipping an app using a personal or company API key, we highly recommend setting up AIProxy as an alternative to building, monitoring, and maintaining your own backend.&lt;/p&gt; &#xA;&lt;h2&gt;How to update the package&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you set the dependency rule to &lt;code&gt;main&lt;/code&gt; in step 2 above, then you can ensure the package is up to date by right clicking on the package and selecting &#39;Update Package&#39;&lt;/p&gt; &lt;img src=&#34;https://github.com/lzell/AIProxySwift/assets/35940/aeee0ab2-362b-4995-b9ca-ff4e1dd04f47&#34; alt=&#34;Update package version&#34; width=&#34;720&#34;&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you selected a version-based rule, inspect the rule in the &#39;Package Dependencies&#39; section of your project settings:&lt;/p&gt; &lt;img src=&#34;https://github.com/lzell/AIProxySwift/assets/35940/ca788c4c-ac38-4d9d-bb4f-928a9487f6eb&#34; alt=&#34;Update package rule&#34; width=&#34;720&#34;&gt; &lt;p&gt;Once the rule is set to include the release version that you&#39;d like to bring in, Xcode should update the package automatically. If it does not, right click on the package in the project tree and select &#39;Update Package&#39;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to contribute to the package&lt;/h2&gt; &#xA;&lt;p&gt;Your additions to AIProxySwift are welcome! I like to develop the library while working in an app that depends on it:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repo&lt;/li&gt; &#xA; &lt;li&gt;Clone your fork&lt;/li&gt; &#xA; &lt;li&gt;Open your app in Xcode&lt;/li&gt; &#xA; &lt;li&gt;Remove AIProxySwift from your app (since this is likely referencing a remote lib)&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;File &amp;gt; Add Package Dependencies&lt;/code&gt;, and in the bottom left of that popup there is a button &#34;Add local&#34;&lt;/li&gt; &#xA; &lt;li&gt;Tap &#34;Add local&#34; and then select the folder where you cloned AIProxySwift on your disk.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you do that, then you can modify the source to AIProxySwift right from within your Xcode project for your app. Once you&#39;re happy with your changes, open a PR here.&lt;/p&gt; &#xA;&lt;h1&gt;Example usage&lt;/h1&gt; &#xA;&lt;p&gt;Along with the snippets below, which you can copy and paste into your Xcode project, we also offer full demo apps to jump-start your development. Please see the &lt;a href=&#34;https://github.com/lzell/AIProxyBootstrap&#34;&gt;AIProxyBootstrap&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#openai&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#gemini&#34;&gt;Gemini&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#anthropic&#34;&gt;Anthropic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#stability-ai&#34;&gt;Stability AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#deepl&#34;&gt;DeepL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#together-ai&#34;&gt;Together AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#replicate&#34;&gt;Replicate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#elevenlabs&#34;&gt;ElevenLabs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#fal&#34;&gt;Fal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#groq&#34;&gt;Groq&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#perplexity&#34;&gt;Perplexity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#mistral&#34;&gt;Mistral&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#eachai&#34;&gt;EachAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#openrouter&#34;&gt;OpenRouter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#deepseek&#34;&gt;DeepSeek&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#fireworks-ai&#34;&gt;Fireworks AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#brave&#34;&gt;Brave&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#advanced-settings&#34;&gt;Advanced Settings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;OpenAI&lt;/h2&gt; &#xA;&lt;h3&gt;Get a non-streaming chat completion from OpenAI:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let response = try await openAIService.chatCompletionRequest(body: .init(&#xA;            model: &#34;gpt-4o&#34;,&#xA;            messages: [.user(content: .text(&#34;hello world&#34;))]&#xA;        ))&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create OpenAI chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a buffered chat completion to OpenAI with extended timeout&lt;/h3&gt; &#xA;&lt;p&gt;This is useful for &lt;code&gt;o1&lt;/code&gt; and &lt;code&gt;o3&lt;/code&gt; models.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = OpenAIChatCompletionRequestBody(&#xA;        model: &#34;o3-mini&#34;,&#xA;        messages: [&#xA;          .developer(content: .text(&#34;You are a coding assistant&#34;)),&#xA;          .user(content: .text(&#34;Build a ruby service that writes latency stats to redis on each request&#34;))&#xA;        ]&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await openAIService.chatCompletionRequest(&#xA;            body: requestBody,&#xA;            secondsToWait: 300&#xA;        )&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch let err as URLError where err.code == URLError.timedOut {&#xA;        print(&#34;Request to OpenAI for a reasoning request timed out&#34;)&#xA;    } catch let err as URLError where [.notConnectedToInternet, .networkConnectionLost].contains(err.code) {&#xA;        print(&#34;Could not complete OpenAI reasoning request. Please check your internet connection&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not complete OpenAI reasoning request: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Get a streaming chat completion from OpenAI:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = OpenAIChatCompletionRequestBody(&#xA;        model: &#34;gpt-4o-mini&#34;,&#xA;        messages: [.user(content: .text(&#34;hello world&#34;))]&#xA;    )&#xA;&#xA;    do {&#xA;        let stream = try await openAIService.streamingChatCompletionRequest(body: requestBody)&#xA;        for try await chunk in stream {&#xA;            print(chunk.choices.first?.delta.content ?? &#34;&#34;)&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create OpenAI streaming chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to include history in chat completion requests to OpenAI&lt;/h3&gt; &#xA;&lt;p&gt;Use this approach to have a conversation with ChatGPT. All previous chat messages, whether issued by the user or the assistant (chatGPT), are fed back into the model on each request.&lt;/p&gt; &#xA;&lt;p&gt;As an alternative, you can use the new ChatGPT Responses API to hold the entire history by passing in the previousResponseId&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    // We&#39;ll start the conversation by asking about the color of a blackberry.&#xA;    // There is no prior history, so we only send up a single user message.&#xA;    //&#xA;    // You can optionally include a .system message to give the model&#xA;    // instructions on how it should behave.&#xA;    let userMessage1: OpenAIChatCompletionRequestBody.Message = .user(&#xA;        content: .text(&#34;What color is a blackberry?&#34;)&#xA;    )&#xA;&#xA;    // Create the first chat completion.&#xA;    var completion1: OpenAIChatCompletionResponseBody? = nil&#xA;    do {&#xA;        completion1 = try await openAIService.chatCompletionRequest(body: .init(&#xA;            model: &#34;gpt-4o-mini&#34;,&#xA;            messages: [&#xA;                userMessage1&#xA;            ]&#xA;        ))&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get first chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&#xA;    // Get the contents of the model&#39;s first response:&#xA;    guard let assistantContent1 = completion1?.choices.first?.message.content else {&#xA;        print(&#34;Completion1: ChatGPT did not respond with any assistant content&#34;)&#xA;        return&#xA;    }&#xA;    print(&#34;Completion1: \(assistantContent1)&#34;)&#xA;&#xA;    // Continue the conversation by asking about a strawberry.&#xA;    // If the history were absent from the request, ChatGPT would respond with general facts.&#xA;    // By including the history, the model continues the conversation, understanding that we&#xA;    // are specifically interested in the strawberry&#39;s color.&#xA;    let userMessage2: OpenAIChatCompletionRequestBody.Message = .user(&#xA;        content: .text(&#34;And a strawberry?&#34;)&#xA;    )&#xA;&#xA;    // Create the second chat completion, note the `messages` array.&#xA;    var completion2: OpenAIChatCompletionResponseBody? = nil&#xA;    do {&#xA;        completion2 = try await openAIService.chatCompletionRequest(body: .init(&#xA;            model: &#34;gpt-4o-mini&#34;,&#xA;            messages: [&#xA;                userMessage1,&#xA;                .assistant(content: .text(assistantContent1)),&#xA;                userMessage2&#xA;            ]&#xA;        ))&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get second chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&#xA;    // Get the contents of the model&#39;s second response:&#xA;    guard let assistantContent2 = completion2?.choices.first?.message.content else {&#xA;        print(&#34;Completion2: ChatGPT did not respond with any assistant content&#34;)&#xA;        return&#xA;    }&#xA;    print(&#34;Completion2: \(assistantContent2)&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Send a multi-modal chat completion request to OpenAI:&lt;/h3&gt; &#xA;&lt;p&gt;On macOS, use &lt;code&gt;NSImage(named:)&lt;/code&gt; in place of &lt;code&gt;UIImage(named:)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let image = UIImage(named: &#34;myImage&#34;) else {&#xA;        print(&#34;Could not find an image named &#39;myImage&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let imageURL = AIProxy.encodeImageAsURL(image: image, compressionQuality: 0.6) else {&#xA;        print(&#34;Could not convert image to OpenAI&#39;s imageURL format&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let response = try await openAIService.chatCompletionRequest(body: .init(&#xA;            model: &#34;gpt-4o&#34;,&#xA;            messages: [&#xA;                .system(&#xA;                    content: .text(&#34;Tell me what you see&#34;)&#xA;                ),&#xA;                .user(&#xA;                    content: .parts(&#xA;                        [&#xA;                            .text(&#34;What do you see?&#34;),&#xA;                            .imageURL(imageURL, detail: .auto)&#xA;                        ]&#xA;                    )&#xA;                )&#xA;            ]&#xA;        ))&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create OpenAI multi-modal chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to generate an image with DALLE&lt;/h3&gt; &#xA;&lt;p&gt;This snippet will print out the URL of an image generated with &lt;code&gt;dall-e-3&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let response = try await openAIService.createImageRequest(&#xA;            body: .init(&#xA;                prompt: &#34;a skier&#34;,&#xA;                model: .dallE3&#xA;            ),&#xA;            secondsToWait: 300&#xA;        )&#xA;        print(response.data.first?.url ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create an image with DALLE 3: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to generate an image with OpenAI&#39;s gpt-image-1&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let response = try await openAIService.createImageRequest(&#xA;            body: .init(&#xA;                prompt: &#34;a skier&#34;,&#xA;                model: .gptImage1&#xA;            ),&#xA;            secondsToWait: 300&#xA;        )&#xA;&#xA;        guard let base64Data = response.data.first?.b64JSON,&#xA;              let imageData = Data(base64Encoded: base64Data),&#xA;              let image = UIImage(data: imageData) else {&#xA;            print(&#34;Could not create a UIImage out of the base64 returned by OpenAI&#34;)&#xA;            return&#xA;        }&#xA;&#xA;        // Do something with &#39;image&#39;&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create OpenAI image generation: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to edit an image with OpenAI&#39;s gpt-image-1&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This snippet uploads two images to &lt;code&gt;gpt-image-1&lt;/code&gt;, transfering the material of one to the other.&lt;/li&gt; &#xA; &lt;li&gt;One image is uploaded as a png and the other as a jpeg.&lt;/li&gt; &#xA; &lt;li&gt;The output quality is chosen to be &lt;code&gt;.low&lt;/code&gt; for speed of generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let image1 = UIImage(named: &#34;my-first-image&#34;) else {&#xA;        print(&#34;Could not find an image named &#39;my-first-image&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let image2 = UIImage(named: &#34;my-second-image&#34;) else {&#xA;        print(&#34;Could not find an image named &#39;my-second-image&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let jpegData = AIProxy.encodeImageAsJpeg(image: image1, compressionQuality: 0.4) else {&#xA;        print(&#34;Could not convert image to jpeg&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let pngData = image2.pngData() else {&#xA;        print(&#34;Could not convert image to png&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let response = try await openAIService.createImageEditRequest(&#xA;            body: .init(&#xA;                images: [&#xA;                    .jpeg(jpegData),&#xA;                    .png(pngData)&#xA;                ],&#xA;                prompt: &#34;Transfer the material of the second image to the first&#34;,&#xA;                model: .gptImage1,&#xA;                quality: .low&#xA;            ),&#xA;            secondsToWait: 300&#xA;        )&#xA;&#xA;        guard let base64Data = response.data.first?.b64JSON,&#xA;              let imageData = Data(base64Encoded: base64Data),&#xA;              let image = UIImage(data: imageData) else {&#xA;            print(&#34;Could not create a UIImage out of the base64 returned by OpenAI&#34;)&#xA;            return&#xA;        }&#xA;&#xA;        // Do something with &#39;image&#39;&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create OpenAI edit image generation: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to ensure OpenAI returns JSON as the chat message content&lt;/h3&gt; &#xA;&lt;p&gt;If you need to enforce a strict JSON contract, please use Structured Outputs (the next example) instead of this approach. This approach is referred to as &#39;JSON mode&#39; in the OpenAI docs, and is the predecessor to Structured Outputs.&lt;/p&gt; &#xA;&lt;p&gt;JSON mode is enabled with &lt;code&gt;responseFormat: .jsonObject&lt;/code&gt;, while Structured Outputs is enabled with &lt;code&gt;responseFormat: .jsonSchema&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you use JSON mode, set &lt;code&gt;responseFormat&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; specify in the prompt that OpenAI should return JSON only:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let requestBody = OpenAIChatCompletionRequestBody(&#xA;            model: &#34;gpt-4o&#34;,&#xA;            messages: [&#xA;                .system(content: .text(&#34;Return valid JSON only&#34;)),&#xA;                .user(content: .text(&#34;Return alice and bob in a list of names&#34;))&#xA;            ],&#xA;            responseFormat: .jsonObject&#xA;        )&#xA;        let response = try await openAIService.chatCompletionRequest(body: requestBody)&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create OpenAI chat completion in JSON mode: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use OpenAI structured outputs (JSON schemas) in a chat response&lt;/h3&gt; &#xA;&lt;p&gt;This example prompts chatGPT to construct a color palette and conform to a strict JSON schema in its response:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let schema: [String: AIProxyJSONValue] = [&#xA;            &#34;type&#34;: &#34;object&#34;,&#xA;            &#34;properties&#34;: [&#xA;                &#34;colors&#34;: [&#xA;                    &#34;type&#34;: &#34;array&#34;,&#xA;                    &#34;items&#34;: [&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: [&#xA;                            &#34;name&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;A descriptive name to give the color&#34;&#xA;                            ],&#xA;                            &#34;hex_code&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;The hex code of the color&#34;&#xA;                            ]&#xA;                        ],&#xA;                        &#34;required&#34;: [&#34;name&#34;, &#34;hex_code&#34;],&#xA;                        &#34;additionalProperties&#34;: false&#xA;                    ]&#xA;                ]&#xA;            ],&#xA;            &#34;required&#34;: [&#34;colors&#34;],&#xA;            &#34;additionalProperties&#34;: false&#xA;        ]&#xA;        let requestBody = OpenAIChatCompletionRequestBody(&#xA;            model: &#34;gpt-4o-2024-08-06&#34;,&#xA;            messages: [&#xA;                .system(content: .text(&#34;Return valid JSON only, and follow the specified JSON structure&#34;)),&#xA;                .user(content: .text(&#34;Return a peaches and cream color palette&#34;))&#xA;            ],&#xA;            responseFormat: .jsonSchema(&#xA;                name: &#34;palette_creator&#34;,&#xA;                description: &#34;A list of colors that make up a color pallete&#34;,&#xA;                schema: schema,&#xA;                strict: true&#xA;            )&#xA;        )&#xA;        let response = try await openAIService.chatCompletionRequest(body: requestBody)&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create OpenAI chat completion with structured outputs: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use OpenAI structured outputs with a function call&lt;/h3&gt; &#xA;&lt;p&gt;This implements the example in OpenAI&#39;s new &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling&#34;&gt;function calling guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more examples, see the &lt;a href=&#34;https://openai.com/index/introducing-structured-outputs-in-the-api&#34;&gt;original structured outputs announcement&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;        func getWeather(location: String?) -&amp;gt; String {&#xA;        // Fill this with your native function logic.&#xA;        // Using a stub for this example.&#xA;        return &#34;Sunny and 65 degrees&#34;&#xA;    }&#xA;&#xA;    // We&#39;ll start the conversation by asking about the weather.&#xA;    // There is no prior history, so we only send up a single user message.&#xA;    //&#xA;    // You can optionally include a .system message to give the model&#xA;    // instructions on how it should behave.&#xA;    let userMessage: OpenAIChatCompletionRequestBody.Message = .user(&#xA;        content: .text(&#34;What is the weather in SF?&#34;)&#xA;    )&#xA;&#xA;    var completion1: OpenAIChatCompletionResponseBody? = nil&#xA;    do {&#xA;        completion1 = try await openAIService.chatCompletionRequest(body: .init(&#xA;            model: &#34;gpt-4o-mini&#34;,&#xA;            messages: [&#xA;                userMessage&#xA;            ],&#xA;            tools: [&#xA;                .function(&#xA;                    name: &#34;get_weather&#34;,&#xA;                    description: &#34;Get current temperature for a given location.&#34;,&#xA;                    parameters: [&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: [&#xA;                            &#34;location&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;City and country e.g. Bogotá, Colombia&#34;&#xA;                            ]&#xA;                        ],&#xA;                        &#34;required&#34;: [&#34;location&#34;],&#xA;                        &#34;additionalProperties&#34;: false&#xA;                    ],&#xA;                    strict: true&#xA;                )&#xA;            ]&#xA;        ))&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get first chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&#xA;    // Get the contents of the model&#39;s first response:&#xA;    guard let toolCall = completion1?.choices.first?.message.toolCalls?.first else {&#xA;        print(&#34;Completion1: ChatGPT did not respond with a tool call&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    // Invoke the function call natively.&#xA;    guard toolCall.function.name == &#34;get_weather&#34; else {&#xA;        print(&#34;We only know how to get the weather&#34;)&#xA;        return&#xA;    }&#xA;    let weather = getWeather(location: toolCall.function.arguments?[&#34;location&#34;] as? String)&#xA;&#xA;    // Pass the results of the function call back to OpenAI.&#xA;    // We create a second chat completion, note the `messages` array in&#xA;    // the completion request. It passes back up:&#xA;    //   1. the original user message&#xA;    //   2. the response from the assistant, which told us to call the get_weather function&#xA;    //   3. the result of our `getWeather` invocation&#xA;    let toolMessage: OpenAIChatCompletionRequestBody.Message = .tool(&#xA;        content: .text(weather),&#xA;        toolCallID: toolCall.id&#xA;    )&#xA;&#xA;    var completion2: OpenAIChatCompletionResponseBody? = nil&#xA;    do {&#xA;        completion2 = try await openAIService.chatCompletionRequest(&#xA;            body: .init(&#xA;                model: &#34;gpt-4o-mini&#34;,&#xA;                messages: [&#xA;                    userMessage,&#xA;                    .assistant(&#xA;                        toolCalls: [&#xA;                            .init(&#xA;                                id: toolCall.id,&#xA;                                function: .init(&#xA;                                    name: toolCall.function.name,&#xA;                                    arguments: toolCall.function.argumentsRaw&#xA;                                )&#xA;                            )&#xA;                        ]&#xA;                    ),&#xA;                    toolMessage&#xA;                ]&#xA;            )&#xA;        )&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get second chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&#xA;    // Get the contents of the model&#39;s second response:&#xA;    guard let assistantContent2 = completion2?.choices.first?.message.content else {&#xA;        print(&#34;Completion2: ChatGPT did not respond with any assistant content&#34;)&#xA;        return&#xA;    }&#xA;    print(assistantContent2)&#xA;    // Prints: &#34;The weather in San Francisco is sunny with a temperature of 65 degrees Fahrenheit.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to stream structured outputs tool calls with OpenAI&lt;/h3&gt; &#xA;&lt;p&gt;This example it taken from OpenAI&#39;s &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling&#34;&gt;function calling guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;    let requestBody = OpenAIChatCompletionRequestBody(&#xA;        model: &#34;gpt-4o-mini&#34;,&#xA;        messages: [&#xA;            .user(content: .text(&#34;What is the weather like in Paris today?&#34;)),&#xA;        ],&#xA;        tools: [&#xA;            .function(&#xA;                name: &#34;get_weather&#34;,&#xA;                description: &#34;Get current temperature for a given location.&#34;,&#xA;                parameters: [&#xA;                    &#34;type&#34;: &#34;object&#34;,&#xA;                    &#34;properties&#34;: [&#xA;                        &#34;location&#34;: [&#xA;                            &#34;type&#34;: &#34;string&#34;,&#xA;                            &#34;description&#34;: &#34;City and country e.g. Bogotá, Colombia&#34;&#xA;                        ],&#xA;                    ],&#xA;                    &#34;required&#34;: [&#34;location&#34;],&#xA;                    &#34;additionalProperties&#34;: false&#xA;                ],&#xA;                strict: true&#xA;            ),&#xA;        ]&#xA;    )&#xA;&#xA;    do {&#xA;        let stream = try await openAIService.streamingChatCompletionRequest(body: requestBody)&#xA;        for try await chunk in stream {&#xA;            guard let delta = chunk.choices.first?.delta else {&#xA;                continue&#xA;            }&#xA;&#xA;            // If the model decided to call a function, this branch will be entered:&#xA;            if let toolCall = delta.toolCalls?.first {&#xA;                if let functionName = toolCall.function?.name {&#xA;                    print(&#34;ChatGPT wants to call function \(functionName) with arguments...&#34;)&#xA;                }&#xA;                print(toolCall.function?.arguments ?? &#34;&#34;)&#xA;            }&#xA;&#xA;            // If the model decided to chat, this branch will be entered:&#xA;            if let content = delta.content {&#xA;                print(content)&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not make a streaming tool call to OpenAI: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to get Whisper word-level timestamps in an audio transcription&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Record an audio file in quicktime and save it as &#34;helloworld.m4a&#34;&lt;/li&gt; &#xA; &lt;li&gt;Add the audio file to your Xcode project. Make sure it&#39;s included in your target: select your audio file in the project tree, type &lt;code&gt;cmd-opt-0&lt;/code&gt; to open the inspect panel, and view &lt;code&gt;Target Membership&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run this snippet:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let url = Bundle.main.url(forResource: &#34;helloworld&#34;, withExtension: &#34;m4a&#34;)!&#xA;        let requestBody = OpenAICreateTranscriptionRequestBody(&#xA;            file: try Data(contentsOf: url),&#xA;            model: &#34;whisper-1&#34;,&#xA;            responseFormat: &#34;verbose_json&#34;,&#xA;            timestampGranularities: [.word, .segment]&#xA;        )&#xA;        let response = try await openAIService.createTranscriptionRequest(body: requestBody)&#xA;        if let words = response.words {&#xA;            for word in words {&#xA;                print(&#34;\(word.word) from \(word.start) to \(word.end)&#34;)&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get word-level timestamps from OpenAI: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use OpenAI text-to-speech&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let requestBody = OpenAITextToSpeechRequestBody(&#xA;            input: &#34;Hello world&#34;,&#xA;            voice: .nova&#xA;        )&#xA;&#xA;        let mpegData = try await openAIService.createTextToSpeechRequest(body: requestBody)&#xA;&#xA;        // Do not use a local `let` or `var` for AVAudioPlayer.&#xA;        // You need the lifecycle of the player to live beyond the scope of this function.&#xA;        // Instead, use file scope or set the player as a member of a reference type with long life.&#xA;        // For example, at the top of this file you may define:&#xA;        //&#xA;        //   fileprivate var audioPlayer: AVAudioPlayer? = nil&#xA;        //&#xA;        // And then use the code below to play the TTS result:&#xA;        audioPlayer = try AVAudioPlayer(data: mpegData)&#xA;        audioPlayer?.prepareToPlay()&#xA;        audioPlayer?.play()&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create OpenAI TTS audio: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to classify text and images as potentially harmful with OpenAI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = OpenAIModerationRequestBody(&#xA;        input: [&#xA;            .text(&#34;is this bad&#34;),&#xA;        ],&#xA;        model: &#34;omni-moderation-latest&#34;&#xA;    )&#xA;    do {&#xA;        let response = try await openAIService.moderationRequest(body: requestBody)&#xA;        print(&#34;Is this content flagged: \(response.results.first?.flagged ?? false)&#34;)&#xA;        //&#xA;        // For a more detailed assessment of the input content, inspect:&#xA;        //&#xA;        //     response.results.first?.categories&#xA;        //&#xA;        // and&#xA;        //&#xA;        //     response.results.first?.categoryScores&#xA;        //&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not perform moderation request to OpenAI&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to get embeddings using OpenAI&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = OpenAIEmbeddingRequestBody(&#xA;        input: .text(&#34;hello world&#34;),&#xA;        model: &#34;text-embedding-3-small&#34;&#xA;    )&#xA;&#xA;    // Or, for multiple embeddings from strings:&#xA;&#xA;    /*&#xA;    let requestBody = OpenAIEmbeddingRequestBody(&#xA;        input: .textArray([&#xA;            &#34;hello world&#34;,&#xA;            &#34;hola mundo&#34;&#xA;        ]),&#xA;        model: &#34;text-embedding-3-small&#34;&#xA;    )&#xA;    */&#xA;&#xA;    // Or, for multiple embeddings from tokens:&#xA;&#xA;    /*&#xA;    let requestBody = OpenAIEmbeddingRequestBody(&#xA;        input: .intArray([0,1,2]),&#xA;        model: &#34;text-embedding-3-small&#34;&#xA;    )&#xA;    */&#xA;&#xA;    do {&#xA;        let response = try await openAIService.embeddingRequest(body: requestBody)&#xA;        print(&#xA;            &#34;&#34;&#34;&#xA;            The response contains \(response.embeddings.count) embeddings.&#xA;&#xA;            The first vector starts with \(response.embeddings.first?.vector.prefix(10) ?? [])&#xA;            &#34;&#34;&#34;&#xA;        )&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not perform embedding request to OpenAI: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How use realtime audio with OpenAI&lt;/h3&gt; &#xA;&lt;p&gt;Use this example to have a conversation with OpenAI&#39;s realtime models.&lt;/p&gt; &#xA;&lt;p&gt;We recommend getting a basic chat completion with OpenAI working before attempting realtime. Realtime is a more involved integration (as you can see from the code snippet below), and getting a basic integration working first narrows down the source of any problem.&lt;/p&gt; &#xA;&lt;p&gt;Take these steps to build and run an OpenAI realtime example:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate a new SwiftUI Xcode project called &lt;code&gt;MyApp&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add the &lt;code&gt;NSMicrophoneUsageDescription&lt;/code&gt; key to your info.plist file&lt;/li&gt; &#xA; &lt;li&gt;If macOS, tap your project &amp;gt; your target &amp;gt; Signing &amp;amp; Capabilities and add the following: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;App Sandbox &amp;gt; Outgoing Connections (client)&lt;/li&gt; &#xA;   &lt;li&gt;App Sandbox &amp;gt; Audio Input&lt;/li&gt; &#xA;   &lt;li&gt;Hardened Runtime &amp;gt; AudioInput&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Replace the contents of &lt;code&gt;MyApp.swift&lt;/code&gt; with the snippet below&lt;/li&gt; &#xA; &lt;li&gt;Replace the placeholders in the snippet &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If connecting directly to OpenAI, replace &lt;code&gt;your-openai-key&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;If protecting your connection through AIProxy, replace &lt;code&gt;aiproxy-partial-key&lt;/code&gt; and &lt;code&gt;aiproxy-service-url&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set the &lt;code&gt;logLevel&lt;/code&gt; argument of the &lt;code&gt;openAIService.realtimeSession&lt;/code&gt; call to your desired level. If you leave it set at &lt;code&gt;.debug&lt;/code&gt;, then you&#39;ll see logs for all audio samples that we send and receive from OpenAI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt; If you would like to protect your connection through AIProxy&#39;s backend, your AIProxy project must be enabled for websocket use. Please reach out if you would like to be added to the private beta.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import SwiftUI&#xA;import AIProxy&#xA;&#xA;@main&#xA;struct MyApp: App {&#xA;&#xA;    let realtimeManager = RealtimeManager()&#xA;&#xA;    var body: some Scene {&#xA;        WindowGroup {&#xA;            Button(&#34;Start conversation&#34;) {&#xA;                Task {&#xA;                    try await realtimeManager.startConversation()&#xA;                }&#xA;            }&#xA;        }&#xA;    }&#xA;}&#xA;&#xA;@RealtimeActor&#xA;final class RealtimeManager {&#xA;    private var realtimeSession: OpenAIRealtimeSession?&#xA;    private var microphonePCMSampleVendor: MicrophonePCMSampleVendor?&#xA;    private var audioPCMPlayer: AudioPCMPlayer?&#xA;&#xA;    nonisolated init() {}&#xA;&#xA;    func startConversation() async throws {&#xA;        /* Uncomment for BYOK use cases */&#xA;        // let openAIService = AIProxy.openAIDirectService(&#xA;        //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;        // )&#xA;&#xA;        /* Uncomment to protect your connection through AIProxy */&#xA;        // let openAIService = AIProxy.openAIService(&#xA;        //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;        //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;        // )&#xA;&#xA;        // Set to false if you want your user to speak first&#xA;        let aiSpeaksFirst = true&#xA;&#xA;        // Initialize an audio player to play PCM16 data that we receive from OpenAI:&#xA;        let audioPCMPlayer = try AudioPCMPlayer()&#xA;&#xA;        // Initialize a microphone vendor to vend PCM16 audio samples that we&#39;ll send to OpenAI:&#xA;        let microphonePCMSampleVendor = MicrophonePCMSampleVendor()&#xA;        let audioStream = try microphonePCMSampleVendor.start()&#xA;&#xA;        // Start the realtime session:&#xA;        let configuration = OpenAIRealtimeSessionConfiguration(&#xA;            inputAudioFormat: .pcm16,&#xA;            inputAudioTranscription: .init(model: &#34;whisper-1&#34;),&#xA;            instructions: &#34;You are a tour guide of Yosemite national park&#34;,&#xA;            maxResponseOutputTokens: .int(4096),&#xA;            modalities: [.audio, .text],&#xA;            outputAudioFormat: .pcm16,&#xA;            temperature: 0.7,&#xA;            turnDetection: .init(&#xA;                type: .serverVAD(&#xA;                    prefixPaddingMs: 300,&#xA;                    silenceDurationMs: 500,&#xA;                    threshold: 0.5&#xA;                )&#xA;            ),&#xA;            voice: &#34;shimmer&#34;&#xA;        )&#xA;&#xA;        let realtimeSession = try await openAIService.realtimeSession(&#xA;            model: &#34;gpt-4o-mini-realtime-preview-2024-12-17&#34;,&#xA;            configuration: configuration,&#xA;            logLevel: .debug&#xA;        )&#xA;&#xA;        // Send audio from the microphone to OpenAI once OpenAI is ready for it:&#xA;        var isOpenAIReadyForAudio = false&#xA;        Task {&#xA;            for await buffer in audioStream {&#xA;                if isOpenAIReadyForAudio, let base64Audio = AIProxy.base64EncodeAudioPCMBuffer(from: buffer) {&#xA;                    await realtimeSession.sendMessage(&#xA;                        OpenAIRealtimeInputAudioBufferAppend(audio: base64Audio)&#xA;                    )&#xA;                }&#xA;            }&#xA;        }&#xA;&#xA;        // Listen for messages from OpenAI:&#xA;        Task {&#xA;            for await message in realtimeSession.receiver {&#xA;                switch message {&#xA;                case .error(_):&#xA;                    realtimeSession.disconnect()&#xA;                case .sessionUpdated:&#xA;                    if aiSpeaksFirst {&#xA;                        await realtimeSession.sendMessage(OpenAIRealtimeResponseCreate())&#xA;                    } else {&#xA;                        isOpenAIReadyForAudio = true&#xA;                    }&#xA;                case .responseAudioDelta(let base64Audio):&#xA;                    audioPCMPlayer.playPCM16Audio(from: base64Audio)&#xA;                case .inputAudioBufferSpeechStarted:&#xA;                    audioPCMPlayer.interruptPlayback()&#xA;                case .responseCreated:&#xA;                    isOpenAIReadyForAudio = true&#xA;                default:&#xA;                    break&#xA;                }&#xA;            }&#xA;        }&#xA;&#xA;        self.microphonePCMSampleVendor = microphonePCMSampleVendor&#xA;        self.audioPCMPlayer = audioPCMPlayer&#xA;        self.realtimeSession = realtimeSession&#xA;    }&#xA;&#xA;    func stopConversation() {&#xA;        self.microphonePCMSampleVendor?.stop()&#xA;        self.audioPCMPlayer?.interruptPlayback()&#xA;        self.realtimeSession?.disconnect()&#xA;        self.microphonePCMSampleVendor = nil&#xA;        self.audioPCMPlayer = nil&#xA;        self.realtimeSession = nil&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a basic request using OpenAI&#39;s Responses API (new)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = OpenAICreateResponseRequestBody(&#xA;        input: .text(&#34;hello world&#34;),&#xA;        previousResponseId: nil,  // Pass this in on future requests to save chat history&#xA;        model: &#34;gpt-4o&#34;&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await openAIService.createResponse(requestBody: requestBody)&#xA;        print(response.outputText)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get a text response from OpenAI: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to upload a file to OpenAI&#39;s file storage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let openAIFile = try await openAIService.uploadFile(&#xA;            contents: pdfData,&#xA;            name: &#34;my.pdf&#34;,&#xA;            purpose: &#34;user_data&#34;&#xA;        )&#xA;        print(&#34;&#34;&#34;&#xA;              File uploaded to OpenAI&#39;s media storage.&#xA;              It will be available until \(openAIFile.expiresAt.flatMap {String($0)} ?? &#34;forever&#34;)&#xA;              Use it in subsequent requests with ID: \(openAIFile.id)&#xA;              &#34;&#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not upload file to OpenAI: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use a stored file in an OpenAI prompt using the Responses API (new)&lt;/h3&gt; &#xA;&lt;p&gt;Replace the &lt;code&gt;fileID&lt;/code&gt; with the ID returned from the snippet above.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;    let requestBody = OpenAICreateResponseRequestBody(&#xA;        input: .items(&#xA;            [&#xA;                .message(role: .user, content: .list(&#xA;                    [&#xA;                        .file(fileID: &#34;your-file-ID&#34;),&#xA;                        .text(&#34;What is the purpose of this doc?&#34;)&#xA;                    ])&#xA;                )&#xA;            ]&#xA;        ),&#xA;        model: &#34;gpt-4o&#34;&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await openAIService.createResponse(requestBody: requestBody)&#xA;        print(response.outputText)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not prompt with file contents: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use image inputs in the OpenAI Responses API&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openAIService = AIProxy.openAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openai-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openAIService = AIProxy.openAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    // Example of using a local image&#xA;    guard let garmentImage = NSImage(named: &#34;tshirt&#34;),&#xA;          let garmentImageURL = AIProxy.encodeImageAsURL(image: garmentImage,&#xA;                                                         compressionQuality: 0.5) else {&#xA;        print(&#34;Could not find an image named &#39;tshirt&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    // Example of using a remote image&#xA;    let remoteImageURL = URL(string: &#34;https://www.aiproxy.com/assets/img/requests.png&#34;)!&#xA;&#xA;    let requestBody = OpenAICreateResponseRequestBody(&#xA;        input: .items(&#xA;            [&#xA;                .message(&#xA;                    role: .user,&#xA;                    content: .list([&#xA;                        .text(&#34;What are in these images?&#34;),&#xA;                        .imageURL(garmentImageURL),&#xA;                        .imageURL(remoteImageURL),&#xA;                    ])&#xA;                ),&#xA;            ]&#xA;        ),&#xA;        model: &#34;gpt-4o-mini&#34;&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await openAIService.createResponse(requestBody: requestBody)&#xA;        print(response.outputText)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not prompt with image inputs: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use OpenAI through an Azure deployment&lt;/h3&gt; &#xA;&lt;p&gt;You can use all of the OpenAI snippets aboves with one change. Initialize the OpenAI service with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    let openAIService = AIProxy.openAIService(&#xA;        partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;        serviceURL: &#34;service-url-from-your-developer-dashboard&#34;,&#xA;        requestFormat: .azureDeployment(apiVersion: &#34;2024-06-01&#34;)&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Gemini&lt;/h2&gt; &#xA;&lt;h3&gt;How to generate text content with Gemini&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = GeminiGenerateContentRequestBody(&#xA;        contents: [&#xA;            .init(&#xA;                parts: [.text(&#34;How do I use product xyz?&#34;)]&#xA;            )&#xA;        ],&#xA;        generationConfig: .init(maxOutputTokens: 1024),&#xA;        systemInstruction: .init(parts: [.text(&#34;Introduce yourself as a customer support person&#34;)])&#xA;    )&#xA;    do {&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-2.0-flash-exp&#34;,&#xA;            secondsToWait: 60&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            switch part {&#xA;            case .text(let text):&#xA;                print(&#34;Gemini sent: \(text)&#34;)&#xA;            case .functionCall(name: let functionName, args: let arguments):&#xA;                print(&#34;Gemini wants us to call function \(functionName) with arguments: \(arguments ?? [:])&#34;)&#xA;            }&#xA;        }&#xA;        if let usage = response.usageMetadata {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokenCount ?? 0) prompt tokens&#xA;                 \(usage.cachedContentTokenCount ?? 0) cached tokens&#xA;                 \(usage.candidatesTokenCount ?? 0) candidate tokens&#xA;                 \(usage.totalTokenCount ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Gemini generate content request: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a tool call with Gemini&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let functionParameters: [String: AIProxyJSONValue] = [&#xA;        &#34;type&#34;: &#34;OBJECT&#34;,&#xA;        &#34;properties&#34;: [&#xA;            &#34;brightness&#34;: [&#xA;                &#34;description&#34;: &#34;Light level from 0 to 100. Zero is off and 100 is full brightness.&#34;,&#xA;                &#34;type&#34;: &#34;NUMBER&#34;&#xA;            ],&#xA;            &#34;colorTemperature&#34;: [&#xA;                &#34;description&#34;: &#34;Color temperature of the light fixture which can be `daylight`, `cool` or `warm`.&#34;,&#xA;                &#34;type&#34;: &#34;STRING&#34;&#xA;            ]&#xA;        ],&#xA;        &#34;required&#34;: [&#xA;            &#34;brightness&#34;,&#xA;            &#34;colorTemperature&#34;&#xA;        ]&#xA;    ]&#xA;&#xA;    let requestBody = GeminiGenerateContentRequestBody(&#xA;        contents: [&#xA;            .init(&#xA;                parts: [.text(&#34;Dim the lights so the room feels cozy and warm.&#34;)],&#xA;                role: &#34;user&#34;&#xA;            )&#xA;        ],&#xA;        /* Uncomment this to enforce that a function is called regardless of prompt contents. */&#xA;        // toolConfig: .init(&#xA;        //     functionCallingConfig: .init(&#xA;        //         allowedFunctionNames: [&#34;controlLight&#34;],&#xA;        //         mode: .anyFunction&#xA;        //     )&#xA;        // ),&#xA;        tools: [&#xA;            .functionDeclarations(&#xA;                [&#xA;                    .init(&#xA;                        name: &#34;controlLight&#34;,&#xA;                        description: &#34;Set the brightness and color temperature of a room light.&#34;,&#xA;                        parameters: functionParameters&#xA;                    )&#xA;                ]&#xA;            )&#xA;        ]&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-2.0-flash-exp&#34;,&#xA;            secondsToWait: 60&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            switch part {&#xA;            case .text(let text):&#xA;                print(&#34;Gemini sent: \(text)&#34;)&#xA;            case .functionCall(name: let functionName, args: let arguments):&#xA;                print(&#34;Gemini wants us to call function \(functionName) with arguments: \(arguments ?? [:])&#34;)&#xA;            }&#xA;        }&#xA;        if let usage = response.usageMetadata {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokenCount ?? 0) prompt tokens&#xA;                 \(usage.cachedContentTokenCount ?? 0) cached tokens&#xA;                 \(usage.candidatesTokenCount ?? 0) candidate tokens&#xA;                 \(usage.totalTokenCount ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Gemini tool (function) call: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a search grounding call with Gemini&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s important that you connect a GCP billing account to your Gemini API key to use this feature. Otherwise, Gemini will return 429s for every call. You can connect your billing account for the API keys you use &lt;a href=&#34;https://aistudio.google.com/app/apikey&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Consider applying to &lt;a href=&#34;https://cloud.google.com/startup?hl=en&#34;&gt;google for startups&lt;/a&gt; to gain credits that you can put towards Gemini.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = GeminiGenerateContentRequestBody(&#xA;        contents: [&#xA;            .init(&#xA;                parts: [.text(&#34;What is the price of Google stock today&#34;)],&#xA;                role: &#34;user&#34;&#xA;            )&#xA;        ],&#xA;        tools: [&#xA;            .googleSearchRetrieval(.init(dynamicThreshold: 0.7, mode: .dynamic))&#xA;        ]&#xA;    )&#xA;    do {&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-1.5-flash&#34;,&#xA;            secondsToWait: 60&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            switch part {&#xA;            case .text(let text):&#xA;                print(&#34;Gemini sent: \(text)&#34;)&#xA;            case .functionCall(name: let functionName, args: let arguments):&#xA;                print(&#34;Gemini wants us to call function \(functionName) with arguments: \(arguments ?? [:])&#34;)&#xA;            }&#xA;        }&#xA;        if let usage = response.usageMetadata {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokenCount ?? 0) prompt tokens&#xA;                 \(usage.cachedContentTokenCount ?? 0) cached tokens&#xA;                 \(usage.candidatesTokenCount ?? 0) candidate tokens&#xA;                 \(usage.totalTokenCount ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Gemini grounding search request: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a google search grounding call with Gemini 2.0&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s important that you connect a GCP billing account to your Gemini API key to use this feature. Otherwise, Gemini will return 429s for every call. You can connect your billing account for the API keys you use &lt;a href=&#34;https://aistudio.google.com/app/apikey&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Consider applying to &lt;a href=&#34;https://cloud.google.com/startup?hl=en&#34;&gt;google for startups&lt;/a&gt; to gain credits that you can put towards Gemini.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = GeminiGenerateContentRequestBody(&#xA;        contents: [&#xA;            .init(&#xA;                parts: [.text(&#34;What is the price of Google stock today&#34;)],&#xA;                role: &#34;user&#34;&#xA;            )&#xA;        ],&#xA;        generationConfig: .init(&#xA;            temperature: 0.7&#xA;        ),&#xA;        systemInstruction: .init(&#xA;            parts: [.text(&#34;You are a helpful assistant&#34;)]&#xA;        ),&#xA;        tools: [&#xA;            .googleSearch(.init())&#xA;        ]&#xA;    )&#xA;    do {&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-2.0-flash&#34;,&#xA;            secondsToWait: 60&#xA;        )&#xA;        for candidate in response.candidates ?? [] {&#xA;            for part in candidate.content?.parts ?? [] {&#xA;                switch part {&#xA;                case .text(let text):&#xA;                    print(&#34;Gemini sent: \(text)\n&#34;)&#xA;                    print(&#34;Gemini used \(candidate.groundingMetadata?.groundingChunks?.count ?? 0) grounding chunks&#34;)&#xA;                    print(&#34;Gemini used \(candidate.groundingMetadata?.groundingSupports?.count ?? 0) grounding supports&#34;)&#xA;                case .functionCall(name: let functionName, args: let arguments):&#xA;                    print(&#34;Gemini wants us to call function \(functionName) with arguments: \(arguments ?? [:])&#34;)&#xA;                }&#xA;            }&#xA;        }&#xA;        if let usage = response.usageMetadata {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                \(usage.promptTokenCount ?? 0) prompt tokens&#xA;                \(usage.cachedContentTokenCount ?? 0) cached tokens&#xA;                \(usage.candidatesTokenCount ?? 0) candidate tokens&#xA;                \(usage.totalTokenCount ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Gemini google search grounding request: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to transcribe audio with Gemini&lt;/h3&gt; &#xA;&lt;p&gt;Add a file called &lt;code&gt;helloworld.m4a&lt;/code&gt; to your Xcode assets before running this sample snippet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let url = Bundle.main.url(forResource: &#34;helloworld&#34;, withExtension: &#34;m4a&#34;) else {&#xA;        print(&#34;Could not find an audio file named helloworld.m4a in your app bundle&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let requestBody = GeminiGenerateContentRequestBody(&#xA;            contents: [&#xA;                .init(&#xA;                    parts: [&#xA;                        .text(&#34;&#34;&#34;&#xA;                              Can you transcribe this interview, in the format of timecode, speaker, caption?&#xA;                              Use speaker A, speaker B, etc. to identify speakers.&#xA;                              &#34;&#34;&#34;),&#xA;                        .inline(data: try Data(contentsOf: url), mimeType: &#34;audio/mp4&#34;)&#xA;                    ]&#xA;                )&#xA;            ]&#xA;        )&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-1.5-flash&#34;,&#xA;            secondsToWait: 60&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            switch part {&#xA;            case .text(let text):&#xA;                print(&#34;Gemini transcript: \(text)&#34;)&#xA;            case .functionCall(name: let functionName, args: let arguments):&#xA;                print(&#34;Gemini wants us to call function \(functionName) with arguments: \(arguments ?? [:])&#34;)&#xA;            }&#xA;        }&#xA;        if let usage = response.usageMetadata {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokenCount ?? 0) prompt tokens&#xA;                 \(usage.cachedContentTokenCount ?? 0) cached tokens&#xA;                 \(usage.candidatesTokenCount ?? 0) candidate tokens&#xA;                 \(usage.totalTokenCount ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create transcript with Gemini: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use images in the prompt to Gemini&lt;/h3&gt; &#xA;&lt;p&gt;Add a file called &#39;my-image.jpg&#39; to Xcode app assets. Then run this snippet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let image = NSImage(named: &#34;my-image&#34;) else {&#xA;        print(&#34;Could not find an image named &#39;my-image&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let jpegData = AIProxy.encodeImageAsJpeg(image: image, compressionQuality: 0.6) else {&#xA;        print(&#34;Could not encode image as Jpeg&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let requestBody = GeminiGenerateContentRequestBody(&#xA;            contents: [&#xA;                .init(&#xA;                    parts: [&#xA;                        .text(&#34;What do you see?&#34;),&#xA;                        .inline(&#xA;                            data: jpegData,&#xA;                            mimeType: &#34;image/jpeg&#34;&#xA;                        )&#xA;                    ]&#xA;                )&#xA;            ],&#xA;            safetySettings: [&#xA;                .init(category: .dangerousContent, threshold: .none),&#xA;                .init(category: .civicIntegrity, threshold: .none),&#xA;                .init(category: .harassment, threshold: .none),&#xA;                .init(category: .hateSpeech, threshold: .none),&#xA;                .init(category: .sexuallyExplicit, threshold: .none)&#xA;            ]&#xA;        )&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-1.5-flash&#34;,&#xA;            secondsToWait: 60&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            switch part {&#xA;            case .text(let text):&#xA;                print(&#34;Gemini sees: \(text)&#34;)&#xA;            case .functionCall(name: let functionName, args: let arguments):&#xA;                print(&#34;Gemini wants us to call function \(functionName) with arguments: \(arguments ?? [:])&#34;)&#xA;            }&#xA;        }&#xA;        if let usage = response.usageMetadata {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokenCount ?? 0) prompt tokens&#xA;                 \(usage.cachedContentTokenCount ?? 0) cached tokens&#xA;                 \(usage.candidatesTokenCount ?? 0) candidate tokens&#xA;                 \(usage.totalTokenCount ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Gemini generate content request: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to upload a video file to Gemini temporary storage&lt;/h3&gt; &#xA;&lt;p&gt;Add a file called &lt;code&gt;my-movie.mov&lt;/code&gt; to your Xcode assets before running this sample snippet. If you use a file like &lt;code&gt;my-movie.mp4&lt;/code&gt;, change the mime type from &lt;code&gt;video/quicktime&lt;/code&gt; to &lt;code&gt;video/mp4&lt;/code&gt; in the snippet below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    // Try to upload the zip file in Xcode assets&#xA;    // Get the images to train with:&#xA;    guard let movieAsset = NSDataAsset(name: &#34;my-movie&#34;) else {&#xA;        print(&#34;&#34;&#34;&#xA;              Drop my-movie.mov into Assets first.&#xA;              &#34;&#34;&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let geminiFile = try await geminiService.uploadFile(&#xA;            fileData: movieAsset.data,&#xA;            mimeType: &#34;video/quicktime&#34;&#xA;        )&#xA;        print(&#34;&#34;&#34;&#xA;              Video file uploaded to Gemini&#39;s media storage.&#xA;              It will be available for 48 hours.&#xA;              Find it at \(geminiFile.uri.absoluteString)&#xA;              &#34;&#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not upload file to Gemini: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to convert video contents to text with Gemini&lt;/h3&gt; &#xA;&lt;p&gt;Use the file URL returned from the snippet above.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    let fileURL = URL(string: &#34;url-from-snippet-above&#34;)!&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = GeminiGenerateContentRequestBody(&#xA;        model: &#34;gemini-1.5-flash&#34;,&#xA;        contents: [&#xA;            .init(&#xA;                parts: [&#xA;                    .text(&#34;Dump the text content in markdown from this video&#34;),&#xA;                    .file(&#xA;                        url: fileURL,&#xA;                        mimeType: &#34;video/quicktime&#34;&#xA;                    )&#xA;                ]&#xA;            )&#xA;        ],&#xA;        safetySettings: [&#xA;            .init(category: .dangerousContent, threshold: .none),&#xA;            .init(category: .civicIntegrity, threshold: .none),&#xA;            .init(category: .harassment, threshold: .none),&#xA;            .init(category: .hateSpeech, threshold: .none),&#xA;            .init(category: .sexuallyExplicit, threshold: .none)&#xA;        ]&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-1.5-flash&#34;,&#xA;            secondsToWait: 60&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            switch part {&#xA;            case .text(let text):&#xA;                print(&#34;Gemini transcript: \(text)&#34;)&#xA;            case .functionCall(name: let functionName, args: let arguments):&#xA;                print(&#34;Gemini wants us to call function \(functionName) with arguments: \(arguments ?? [:])&#34;)&#xA;            }&#xA;        }&#xA;        if let usage = response.usageMetadata {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokenCount ?? 0) prompt tokens&#xA;                 \(usage.cachedContentTokenCount ?? 0) cached tokens&#xA;                 \(usage.candidatesTokenCount ?? 0) candidate tokens&#xA;                 \(usage.totalTokenCount ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Gemini vision request: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to delete a temporary file from Gemini storage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    let fileURL = URL(string: &#34;url-from-snippet-above&#34;)!&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        try await geminiService.deleteFile(fileURL: fileURL)&#xA;        print(&#34;File deleted from \(fileURL.absoluteString)&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not delete file from Gemini temporary storage: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use structured ouputs with Gemini&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let schema: [String: AIProxyJSONValue] = [&#xA;        &#34;description&#34;: &#34;List of recipes&#34;,&#xA;        &#34;type&#34;: &#34;array&#34;,&#xA;        &#34;items&#34;: [&#xA;            &#34;type&#34;: &#34;object&#34;,&#xA;            &#34;properties&#34;: [&#xA;                &#34;recipeName&#34;: [&#xA;                    &#34;type&#34;: &#34;string&#34;,&#xA;                    &#34;description&#34;: &#34;Name of the recipe&#34;,&#xA;                    &#34;nullable&#34;: false&#xA;                ]&#xA;            ],&#xA;            &#34;required&#34;: [&#34;recipeName&#34;]&#xA;        ]&#xA;    ]&#xA;    do {&#xA;        let requestBody = GeminiGenerateContentRequestBody(&#xA;            contents: [&#xA;                .init(&#xA;                    parts: [&#xA;                        .text(&#34;List a few popular cookie recipes.&#34;),&#xA;                    ]&#xA;                )&#xA;            ],&#xA;            generationConfig: .init(&#xA;                responseMimeType: &#34;application/json&#34;,&#xA;                responseSchema: schema&#xA;            )&#xA;        )&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-2.0-flash&#34;,&#xA;            secondsToWait: 60&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            if case .text(let text) = part {&#xA;                print(&#34;Gemini sent: \(text)&#34;)&#xA;            }&#xA;        }&#xA;        if let usage = response.usageMetadata {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokenCount ?? 0) prompt tokens&#xA;                 \(usage.cachedContentTokenCount ?? 0) cached tokens&#xA;                 \(usage.candidatesTokenCount ?? 0) candidate tokens&#xA;                 \(usage.totalTokenCount ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Gemini generate content request: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use structured ouputs and an image as input with Gemini&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let image = NSImage(named: &#34;apple_marketing&#34;) else {&#xA;        print(&#34;Could not find an image named &#39;apple_marketing&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let jpegData = AIProxy.encodeImageAsJpeg(image: image, compressionQuality: 0.4) else {&#xA;        print(&#34;Could not encode image as Jpeg&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    let schema: [String: AIProxyJSONValue] = [&#xA;        &#34;description&#34;: &#34;A list of the important points that the document conveys&#34;,&#xA;        &#34;type&#34;: &#34;array&#34;,&#xA;        &#34;items&#34;: [&#xA;            &#34;type&#34;: &#34;object&#34;,&#xA;            &#34;properties&#34;: [&#xA;                &#34;point&#34;: [&#xA;                    &#34;type&#34;: &#34;string&#34;,&#xA;                    &#34;description&#34;: &#34;One of the important points that the document conveys&#34;,&#xA;                    &#34;nullable&#34;: false&#xA;                ]&#xA;            ],&#xA;            &#34;required&#34;: [&#34;point&#34;]&#xA;        ]&#xA;    ]&#xA;&#xA;    let requestBody = GeminiGenerateContentRequestBody(&#xA;        contents: [&#xA;            .init(&#xA;                parts: [&#xA;                    .text(&#34;Please create the important points of this image&#34;),&#xA;                    .inline(&#xA;                        data: jpegData,&#xA;                        mimeType: &#34;image/jpeg&#34;&#xA;                    )&#xA;                ]&#xA;            )&#xA;        ],&#xA;        generationConfig: .init(&#xA;            responseMimeType: &#34;application/json&#34;,&#xA;            responseSchema: schema&#xA;        ),&#xA;        safetySettings: [&#xA;            .init(category: .dangerousContent, threshold: .none),&#xA;            .init(category: .civicIntegrity, threshold: .none),&#xA;            .init(category: .harassment, threshold: .none),&#xA;            .init(category: .hateSpeech, threshold: .none),&#xA;            .init(category: .sexuallyExplicit, threshold: .none)&#xA;        ]&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-2.0-flash&#34;,&#xA;            secondsToWait: 60&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            if case .text(let text) = part {&#xA;                print(&#34;Gemini sent: \(text)&#34;)&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Gemini generate content request: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to generate an image with Gemini&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = GeminiGenerateContentRequestBody(&#xA;        contents: [&#xA;            .init(&#xA;                parts: [&#xA;                    .text(&#xA;                        &#34;&#34;&#34;&#xA;                        Hi, can you create a 3d rendered image of a pig with wings and a top hat&#xA;                        flying over a happy futuristic scifi city with lots of greenery?&#xA;                        &#34;&#34;&#34;&#xA;                    )&#xA;                ],&#xA;                role: &#34;user&#34;&#xA;            )&#xA;        ],&#xA;        generationConfig: .init(&#xA;            responseModalities: [&#xA;                &#34;Text&#34;,&#xA;                &#34;Image&#34;&#xA;            ]&#xA;        ),&#xA;        safetySettings: [&#xA;            .init(category: .dangerousContent, threshold: .none),&#xA;            .init(category: .civicIntegrity, threshold: .none),&#xA;            .init(category: .harassment, threshold: .none),&#xA;            .init(category: .hateSpeech, threshold: .none),&#xA;            .init(category: .sexuallyExplicit, threshold: .none)&#xA;        ]&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-2.0-flash-exp-image-generation&#34;,&#xA;            secondsToWait: 120&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            if case .inlineData(mimeType: let mimeType, base64Data: let base64Data) = part {&#xA;                print(&#34;Gemini generated inline data with mimetype: \(mimeType) and base64Length: \(base64Data.count)&#34;)&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create image using gemini: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to generate an image with Gemini and Imagen&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = GeminiImagenRequestBody(&#xA;        instances: [&#xA;            .init(prompt: prompt)&#xA;        ],&#xA;        parameters: .init(&#xA;            personGeneration: .allowAdult,&#xA;            safetyLevel: .blockNone,&#xA;            sampleCount: 1&#xA;        )&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await geminiService.makeImagenRequest(&#xA;            body: requestBody,&#xA;            model: &#34;imagen-3.0-generate-002&#34;&#xA;        )&#xA;        if let base64Data = response.predictions.first?.bytesBase64Encoded,&#xA;           let imageData = Data(base64Encoded: base64Data),&#xA;           let image = UIImage(data: imageData) {&#xA;            // Do something with image&#xA;        } else {&#xA;            print(&#34;Imagen response did not include base64 image data&#34;)&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Imagen image: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to edit an image with Gemini&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let geminiService = AIProxy.geminiDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-gemini-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let geminiService = AIProxy.geminiService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let pngData = yourUIImage.pngData() else {&#xA;        print(&#34;Could not get png data from your image&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    let requestBody = GeminiGenerateContentRequestBody(&#xA;        contents: [&#xA;            .init(&#xA;                parts: [&#xA;                    .text(&#34;Add sparkles to this image&#34;),&#xA;                    .inline(&#xA;                        data: pngData,&#xA;                        mimeType: &#34;image/png&#34;&#xA;                    )&#xA;                ],&#xA;                role: &#34;user&#34;&#xA;            )&#xA;        ],&#xA;        generationConfig: .init(&#xA;            responseModalities: [&#xA;                &#34;Text&#34;,&#xA;                &#34;Image&#34;&#xA;            ]&#xA;        ),&#xA;        safetySettings: [&#xA;            .init(category: .dangerousContent, threshold: .none),&#xA;            .init(category: .civicIntegrity, threshold: .none),&#xA;            .init(category: .harassment, threshold: .none),&#xA;            .init(category: .hateSpeech, threshold: .none),&#xA;            .init(category: .sexuallyExplicit, threshold: .none)&#xA;        ]&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await geminiService.generateContentRequest(&#xA;            body: requestBody,&#xA;            model: &#34;gemini-2.0-flash-exp-image-generation&#34;,&#xA;            secondsToWait: 120&#xA;        )&#xA;        for part in response.candidates?.first?.content?.parts ?? [] {&#xA;            if case .inlineData(mimeType: let mimeType, base64Data: let base64Data) = part {&#xA;                print(&#34;Gemini generated inline data with mimetype: \(mimeType) and base64Length: \(base64Data.count)&#34;)&#xA;                if let imageData = Data(base64Encoded: base64Data),&#xA;                   let image = UIImage(data: imageData) {&#xA;                     // Do something with image&#xA;                }&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Gemini image edit request: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Anthropic&lt;/h2&gt; &#xA;&lt;h3&gt;How to send an Anthropic message request&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let anthropicService = AIProxy.anthropicDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-anthropic-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let anthropicService = AIProxy.anthropicService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let response = try await anthropicService.messageRequest(body: AnthropicMessageRequestBody(&#xA;            maxTokens: 1024,&#xA;            messages: [&#xA;                AnthropicInputMessage(content: [.text(&#34;hello world&#34;)], role: .user)&#xA;            ],&#xA;            model: &#34;claude-3-5-sonnet-20240620&#34;&#xA;        ))&#xA;        for content in response.content {&#xA;            switch content {&#xA;            case .text(let message):&#xA;                print(&#34;Claude sent a message: \(message)&#34;)&#xA;            case .toolUse(id: _, name: let toolName, input: let toolInput):&#xA;                print(&#34;Claude used a tool \(toolName) with input: \(toolInput)&#34;)&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create an Anthropic message: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use streaming text messages with Anthropic&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let anthropicService = AIProxy.anthropicDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-anthropic-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let anthropicService = AIProxy.anthropicService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let requestBody = AnthropicMessageRequestBody(&#xA;            maxTokens: 1024,&#xA;            messages: [&#xA;                .init(&#xA;                    content: [.text(&#34;hello world&#34;)],&#xA;                    role: .user&#xA;                )&#xA;            ],&#xA;            model: &#34;claude-3-5-sonnet-20240620&#34;&#xA;        )&#xA;&#xA;        let stream = try await anthropicService.streamingMessageRequest(body: requestBody)&#xA;        for try await chunk in stream {&#xA;            switch chunk {&#xA;            case .text(let text):&#xA;                print(text)&#xA;            case .toolUse(name: let toolName, input: let toolInput):&#xA;                print(&#34;Claude wants to call tool \(toolName) with input \(toolInput)&#34;)&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not use Anthropic&#39;s message stream: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use streaming tool calls with Anthropic&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let anthropicService = AIProxy.anthropicDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-anthropic-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let anthropicService = AIProxy.anthropicService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let requestBody = AnthropicMessageRequestBody(&#xA;            maxTokens: 1024,&#xA;            messages: [&#xA;                .init(&#xA;                    content: [.text(&#34;What is nvidia&#39;s stock price?&#34;)],&#xA;                    role: .user&#xA;                )&#xA;            ],&#xA;            model: &#34;claude-3-5-sonnet-20240620&#34;,&#xA;            tools: [&#xA;                .init(&#xA;                    description: &#34;Call this function when the user wants a stock symbol&#34;,&#xA;                    inputSchema: [&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: [&#xA;                            &#34;ticker&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;The stock ticker symbol, e.g. AAPL for Apple Inc.&#34;&#xA;                            ]&#xA;                        ],&#xA;                        &#34;required&#34;: [&#34;ticker&#34;]&#xA;                    ],&#xA;                    name: &#34;get_stock_symbol&#34;&#xA;                )&#xA;            ]&#xA;        )&#xA;&#xA;        let stream = try await anthropicService.streamingMessageRequest(body: requestBody)&#xA;        for try await chunk in stream {&#xA;            switch chunk {&#xA;            case .text(let text):&#xA;                print(text)&#xA;            case .toolUse(name: let toolName, input: let toolInput):&#xA;                print(&#34;Claude wants to call tool \(toolName) with input \(toolInput)&#34;)&#xA;            }&#xA;        }&#xA;        print(&#34;Done with stream&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(error.localizedDescription)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to send an image to Anthropic&lt;/h3&gt; &#xA;&lt;p&gt;On macOS, use &lt;code&gt;NSImage(named:)&lt;/code&gt; in place of &lt;code&gt;UIImage(named:)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    guard let image = UIImage(named: &#34;myImage&#34;) else {&#xA;        print(&#34;Could not find an image named &#39;myImage&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let jpegData = AIProxy.encodeImageAsJpeg(image: image, compressionQuality: 0.6) else {&#xA;        print(&#34;Could not convert image to jpeg&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let anthropicService = AIProxy.anthropicDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-anthropic-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let anthropicService = AIProxy.anthropicService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let response = try await anthropicService.messageRequest(body: AnthropicMessageRequestBody(&#xA;            maxTokens: 1024,&#xA;            messages: [&#xA;                AnthropicInputMessage(content: [&#xA;                    .text(&#34;Provide a very short description of this image&#34;),&#xA;                    .image(mediaType: .jpeg, data: jpegData.base64EncodedString())&#xA;                ], role: .user)&#xA;            ],&#xA;            model: &#34;claude-3-5-sonnet-20240620&#34;&#xA;        ))&#xA;        for content in response.content {&#xA;            switch content {&#xA;            case .text(let message):&#xA;                print(&#34;Claude sent a message: \(message)&#34;)&#xA;            case .toolUse(id: _, name: let toolName, input: let toolInput):&#xA;                print(&#34;Claude used a tool \(toolName) with input: \(toolInput)&#34;)&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not send a multi-modal message to Anthropic: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use the tools API with Anthropic&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let anthropicService = AIProxy.anthropicDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-anthropic-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let anthropicService = AIProxy.anthropicService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let requestBody = AnthropicMessageRequestBody(&#xA;            maxTokens: 1024,&#xA;            messages: [&#xA;                .init(&#xA;                    content: [.text(&#34;What is nvidia&#39;s stock price?&#34;)],&#xA;                    role: .user&#xA;                )&#xA;            ],&#xA;            model: &#34;claude-3-5-sonnet-20240620&#34;,&#xA;            tools: [&#xA;                .init(&#xA;                    description: &#34;Call this function when the user wants a stock symbol&#34;,&#xA;                    inputSchema: [&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: [&#xA;                            &#34;ticker&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;The stock ticker symbol, e.g. AAPL for Apple Inc.&#34;&#xA;                            ]&#xA;                        ],&#xA;                        &#34;required&#34;: [&#34;ticker&#34;]&#xA;                    ],&#xA;                    name: &#34;get_stock_symbol&#34;&#xA;                )&#xA;            ]&#xA;        )&#xA;        let response = try await anthropicService.messageRequest(body: requestBody)&#xA;        for content in response.content {&#xA;            switch content {&#xA;            case .text(let message):&#xA;                print(&#34;Claude sent a message: \(message)&#34;)&#xA;            case .toolUse(id: _, name: let toolName, input: let toolInput):&#xA;                print(&#34;Claude used a tool \(toolName) with input: \(toolInput)&#34;)&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Anthropic message with tool call: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use Anthropic&#39;s pdf support in a buffered chat completion&lt;/h2&gt; &#xA;&lt;p&gt;This snippet includes a pdf &lt;code&gt;mydocument.pdf&lt;/code&gt; in the Anthropic request. Adjust the filename to match the pdf included in your Xcode project. The snippet expects the pdf in the app bundle.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let anthropicService = AIProxy.anthropicDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-anthropic-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let anthropicService = AIProxy.anthropicService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let pdfFileURL = Bundle.main.url(forResource: &#34;mydocument&#34;, withExtension: &#34;pdf&#34;),&#xA;          let pdfData = try? Data(contentsOf: pdfFileURL)&#xA;    else {&#xA;        print(&#34;&#34;&#34;&#xA;              Drop mydocument.pdf file into your Xcode project first.&#xA;              &#34;&#34;&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let response = try await anthropicService.messageRequest(body: AnthropicMessageRequestBody(&#xA;            maxTokens: 1024,&#xA;            messages: [&#xA;                AnthropicInputMessage(content: [.pdf(data: pdfData.base64EncodedString())], role: .user),&#xA;                AnthropicInputMessage(content: [.text(&#34;Summarize this&#34;)], role: .user)&#xA;            ],&#xA;            model: &#34;claude-3-5-sonnet-20241022&#34;&#xA;        ))&#xA;        for content in response.content {&#xA;            switch content {&#xA;            case .text(let message):&#xA;                print(&#34;Claude sent a message: \(message)&#34;)&#xA;            case .toolUse(id: _, name: let toolName, input: let toolInput):&#xA;                print(&#34;Claude used a tool \(toolName) with input: \(toolInput)&#34;)&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not use Anthropic&#39;s buffered pdf support: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How to use Anthropic&#39;s pdf support in a streaming chat completion&lt;/h2&gt; &#xA;&lt;p&gt;This snippet includes a pdf &lt;code&gt;mydocument.pdf&lt;/code&gt; in the Anthropic request. Adjust the filename to match the pdf included in your Xcode project. The snippet expects the pdf in the app bundle.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let anthropicService = AIProxy.anthropicDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-anthropic-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let anthropicService = AIProxy.anthropicService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let pdfFileURL = Bundle.main.url(forResource: &#34;mydocument&#34;, withExtension: &#34;pdf&#34;),&#xA;          let pdfData = try? Data(contentsOf: pdfFileURL)&#xA;    else {&#xA;        print(&#34;&#34;&#34;&#xA;              Drop mydocument.pdf file into your Xcode project first.&#xA;              &#34;&#34;&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let stream = try await anthropicService.streamingMessageRequest(body: AnthropicMessageRequestBody(&#xA;            maxTokens: 1024,&#xA;            messages: [&#xA;                AnthropicInputMessage(content: [.pdf(data: pdfData.base64EncodedString())], role: .user),&#xA;                AnthropicInputMessage(content: [.text(&#34;Summarize this&#34;)], role: .user)&#xA;            ],&#xA;            model: &#34;claude-3-5-sonnet-20241022&#34;&#xA;        ))&#xA;        for try await chunk in stream {&#xA;            switch chunk {&#xA;            case .text(let text):&#xA;                print(text)&#xA;            case .toolUse(name: let toolName, input: let toolInput):&#xA;                print(&#34;Claude wants to call tool \(toolName) with input \(toolInput)&#34;)&#xA;            }&#xA;        }&#xA;        print(&#34;Done with stream&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not use Anthropic&#39;s streaming pdf support: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Stability.ai&lt;/h2&gt; &#xA;&lt;h3&gt;How to generate an image with Stability.ai&lt;/h3&gt; &#xA;&lt;p&gt;In the snippet below, replace NSImage with UIImage if you are building on iOS. For a SwiftUI example, see &lt;a href=&#34;https://gist.github.com/lzell/a878b787f24cc0dd87a31f4dceccd092&#34;&gt;this gist&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let stabilityService = AIProxy.stabilityDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-stability-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let service = AIProxy.stabilityAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let body = StabilityAIUltraRequestBody(prompt: &#34;Lighthouse on a cliff overlooking the ocean&#34;)&#xA;        let response = try await service.ultraRequest(body: body)&#xA;        let image = NSImage(data: response.imageData)&#xA;        // Do something with `image`&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not generate an image with StabilityAI: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;DeepL&lt;/h2&gt; &#xA;&lt;h3&gt;How to create translations using DeepL&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let deepLService = AIProxy.deepLDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-deepL-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let service = AIProxy.deepLService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let body = DeepLTranslateRequestBody(targetLang: &#34;ES&#34;, text: [&#34;hello world&#34;])&#xA;        let response = try await service.translateRequest(body: body)&#xA;        // Do something with `response.translations`&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create DeepL translation: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&#xA;***&#xA;&#xA;## TogetherAI&#xA;&#xA;### How to create a non-streaming chat completion with TogetherAI&#xA;&#xA;See the [TogetherAI model list](https://docs.together.ai/docs/chat-models) for available&#xA;options to pass as the `model` argument:&#xA;&#xA;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let togetherAIService = AIProxy.togetherAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-togetherAI-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let togetherAIService = AIProxy.togetherAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let requestBody = TogetherAIChatCompletionRequestBody(&#xA;            messages: [TogetherAIMessage(content: &#34;Hello world&#34;, role: .user)],&#xA;            model: &#34;meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo&#34;&#xA;        )&#xA;        let response = try await togetherAIService.chatCompletionRequest(body: requestBody)&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create TogetherAI chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&#xA;&#xA;### How to create a streaming chat completion with TogetherAI&#xA;&#xA;See the [TogetherAI model list](https://docs.together.ai/docs/chat-models) for available&#xA;options to pass as the `model` argument:&#xA;&#xA;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let togetherAIService = AIProxy.togetherAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-togetherAI-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let togetherAIService = AIProxy.togetherAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let requestBody = TogetherAIChatCompletionRequestBody(&#xA;            messages: [TogetherAIMessage(content: &#34;Hello world&#34;, role: .user)],&#xA;            model: &#34;meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo&#34;&#xA;        )&#xA;        let stream = try await togetherAIService.streamingChatCompletionRequest(body: requestBody)&#xA;        for try await chunk in stream {&#xA;            print(chunk.choices.first?.delta.content ?? &#34;&#34;)&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create TogetherAI streaming chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&#xA;&#xA;### How to create a JSON response with TogetherAI&#xA;&#xA;JSON mode is handy for enforcing that the model returns JSON in a structure that your&#xA;application expects. You specify the contract using `schema` below. Note that only some models&#xA;support JSON mode. See [this guide](https://docs.together.ai/docs/json-mode) for a list.&#xA;&#xA;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let togetherAIService = AIProxy.togetherAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-togetherAI-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let togetherAIService = AIProxy.togetherAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let schema: [String: AIProxyJSONValue] = [&#xA;            &#34;type&#34;: &#34;object&#34;,&#xA;            &#34;properties&#34;: [&#xA;                &#34;colors&#34;: [&#xA;                    &#34;type&#34;: &#34;array&#34;,&#xA;                    &#34;items&#34;: [&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: [&#xA;                            &#34;name&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;A descriptive name to give the color&#34;&#xA;                            ],&#xA;                            &#34;hex_code&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;The hex code of the color&#34;&#xA;                            ]&#xA;                        ],&#xA;                        &#34;required&#34;: [&#34;name&#34;, &#34;hex_code&#34;],&#xA;                        &#34;additionalProperties&#34;: false&#xA;                    ]&#xA;                ]&#xA;            ],&#xA;            &#34;required&#34;: [&#34;colors&#34;],&#xA;            &#34;additionalProperties&#34;: false&#xA;        ]&#xA;        let requestBody = TogetherAIChatCompletionRequestBody(&#xA;            messages: [&#xA;                TogetherAIMessage(&#xA;                    content: &#34;You are a helpful assistant that answers in JSON&#34;,&#xA;                    role: .system&#xA;                ),&#xA;                TogetherAIMessage(&#xA;                    content: &#34;Create a peaches and cream color palette&#34;,&#xA;                    role: .user&#xA;                )&#xA;            ],&#xA;            model: &#34;meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo&#34;,&#xA;            responseFormat: .json(schema: schema)&#xA;        )&#xA;        let response = try await togetherAIService.chatCompletionRequest(body: requestBody)&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create TogetherAI JSON chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&#xA;&#xA;### How to make a tool call request with Llama and TogetherAI&#xA;&#xA;If you need this use case, please open a github issue. We don&#39;t currently get the tool call&#xA;result out of the response!&#xA;&#xA;This example is a Swift port of [this guide](https://docs.together.ai/docs/llama-3-function-calling):&#xA;&#xA;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let togetherAIService = AIProxy.togetherAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-togetherAI-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let togetherAIService = AIProxy.togetherAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let function = TogetherAIFunction(&#xA;            description: &#34;Call this when the user wants the weather&#34;,&#xA;            name: &#34;get_weather&#34;,&#xA;            parameters: [&#xA;                &#34;type&#34;: &#34;object&#34;,&#xA;                &#34;properties&#34;: [&#xA;                    &#34;location&#34;: [&#xA;                        &#34;type&#34;: &#34;string&#34;,&#xA;                        &#34;description&#34;: &#34;The city and state, e.g. San Francisco, CA&#34;,&#xA;                    ],&#xA;                    &#34;num_days&#34;: [&#xA;                        &#34;type&#34;: &#34;integer&#34;,&#xA;                        &#34;description&#34;: &#34;The number of days to get the forecast for&#34;,&#xA;                    ],&#xA;                ],&#xA;                &#34;required&#34;: [&#34;location&#34;, &#34;num_days&#34;],&#xA;            ]&#xA;        )&#xA;&#xA;        let toolPrompt = &#34;&#34;&#34;&#xA;        You have access to the following functions:&#xA;&#xA;        Use the function &#39;\(function.name)&#39; to &#39;\(function.description)&#39;:&#xA;        \(try function.serialize())&#xA;&#xA;        If you choose to call a function ONLY reply in the following format with no prefix or suffix:&#xA;&#xA;        &amp;lt;function=example_function_name&amp;gt;{{\&#34;example_name\&#34;: \&#34;example_value\&#34;}}&amp;lt;/function&amp;gt;&#xA;&#xA;        Reminder:&#xA;        - Function calls MUST follow the specified format, start with &amp;lt;function= and end with &amp;lt;/function&amp;gt;&#xA;        - Required parameters MUST be specified&#xA;        - Only call one function at a time&#xA;        - Put the entire function call reply on one line&#xA;        - If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls&#xA;&#xA;        &#34;&#34;&#34;&#xA;&#xA;        let requestBody = TogetherAIChatCompletionRequestBody(&#xA;            messages: [&#xA;                TogetherAIMessage(&#xA;                    content: toolPrompt,&#xA;                    role: .system&#xA;                ),&#xA;                TogetherAIMessage(&#xA;                    content: &#34;What&#39;s the weather like in Tokyo over the next few days?&#34;,&#xA;                    role: .user&#xA;                )&#xA;            ],&#xA;            model: &#34;meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo&#34;,&#xA;            temperature: 0,&#xA;            tools: [&#xA;                TogetherAITool(function: function)&#xA;            ]&#xA;        )&#xA;        let response = try await togetherAIService.chatCompletionRequest(body: requestBody)&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create TogetherAI llama 3.1 tool completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&#xA;&#xA;***&#xA;&#xA;&#xA;## Replicate&#xA;&#xA;### How to generate a Flux-Schnell image by Black Forest Labs, using Replicate&#xA;&#xA;```swift&#xA;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let input = ReplicateFluxSchnellInputSchema(&#xA;            prompt: &#34;Monument valley, Utah&#34;&#xA;        )&#xA;        let urls = try await replicateService.createFluxSchnellImages(&#xA;            input: input,&#xA;            secondsToWait: 30&#xA;        )&#xA;        print(&#34;Done creating Flux-Schnell images: &#34;, urls)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not create Flux-Schnell image: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the full range of controls for generating an image by viewing &lt;code&gt;ReplicateFluxSchnellInputSchema.swift&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to generate a Flux-Dev image by Black Forest Labs, using Replicate&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let input = ReplicateFluxDevInputSchema(&#xA;            prompt: &#34;Monument valley, Utah. High res&#34;,&#xA;            goFast: false&#xA;        )&#xA;        let urls = try await replicateService.createFluxDevImages(&#xA;            input: input,&#xA;            secondsToWait: 30&#xA;        )&#xA;        print(&#34;Done creating Flux-Dev images: &#34;, urls)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not create Flux-Dev image: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the full range of controls for generating an image by viewing &lt;code&gt;ReplicateFluxDevInputSchema.swift&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to generate a Flux-Pro image by Black Forest Labs, using Replicate&lt;/h3&gt; &#xA;&lt;p&gt;This snippet generates a version 1.1 image. If you would like to generate version 1, make the following substitutions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;ReplicateFluxProInputSchema_v1_1&lt;/code&gt; -&amp;gt; &lt;code&gt;ReplicateFluxProInputSchema&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;createFluxProImage_v1_1&lt;/code&gt; -&amp;gt; &lt;code&gt;createFluxProImage&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let input = ReplicateFluxProInputSchema_v1_1(&#xA;            prompt: &#34;Monument valley, Utah. High res&#34;&#xA;            promptUpsampling: true&#xA;        )&#xA;        let url = try await replicateService.createFluxProImage_v1_1(&#xA;            input: input,&#xA;            secondsToWait: 60&#xA;        )&#xA;        print(&#34;Done creating Flux-Pro 1.1 image: &#34;, url)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not create Flux-Pro 1.1 image: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the full range of controls for generating an image by viewing &lt;code&gt;ReplicateFluxProInputSchema_v1_1.swift&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to generate a Flux-PuLID image using Replicate&lt;/h3&gt; &#xA;&lt;p&gt;On macOS, use &lt;code&gt;NSImage(named:)&lt;/code&gt; in place of &lt;code&gt;UIImage(named:)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let image = NSImage(named: &#34;face&#34;) else {&#xA;        print(&#34;Could not find an image named &#39;face&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let imageURL = AIProxy.encodeImageAsURL(&#xA;        image: image,&#xA;        compressionQuality: 0.8&#xA;    ) else {&#xA;        print(&#34;Could not convert image to a local data URI&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let input = ReplicateFluxPulidInputSchema(&#xA;            mainFaceImage: imageURL,&#xA;            prompt: &#34;smiling man holding sign with glowing green text &#39;PuLID for FLUX&#39;&#34;,&#xA;            numOutputs: 1,&#xA;            startStep: 4&#xA;        )&#xA;        let urls = try await replicateService.createFluxPuLIDImages(&#xA;            input: input,&#xA;            secondsToWait: 60&#xA;        )&#xA;        print(&#34;Done creating Flux-PuLID image: &#34;, urls)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not create Flux-Pulid images: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the full range of controls for generating an image by viewing &lt;code&gt;ReplicateFluxPulidInputSchema.swift&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to generate an image from a reference image using Flux ControlNet on Replicate&lt;/h3&gt; &#xA;&lt;p&gt;There are many controls to play with for this use case. Please see &lt;code&gt;ReplicateFluxDevControlNetInputSchema.swift&lt;/code&gt; for the full range of controls.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let input = ReplicateFluxDevControlNetInputSchema(&#xA;            controlImage: URL(string: &#34;https://example.com/your/image&#34;)!,&#xA;            prompt: &#34;a cyberpunk with natural greys and whites and browns&#34;,&#xA;            controlStrength: 0.4&#xA;        )&#xA;        let output = try await replicateService.createFluxDevControlNetImages(&#xA;            input: input,&#xA;            secondsToWait: 60&#xA;        )&#xA;        print(&#34;Done creating Flux-ControlNet image: &#34;, output)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not create Flux-ControlNet image: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to generate an SDXL image by StabilityAI, using Replicate&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let input = ReplicateSDXLInputSchema(&#xA;            prompt: &#34;Monument valley, Utah&#34;&#xA;        )&#xA;        let urls = try await replicateService.createSDXLImages(&#xA;            input: input,&#xA;            secondsToWait: 2&#xA;        )&#xA;        print(&#34;Done creating SDXL images: &#34;, urls)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not create SDXL image: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the full range of controls for generating an image by viewing &lt;code&gt;ReplicateSDXLInputSchema.swift&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to generate an SDXL Fresh Ink image by fofr, using Replicate&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let input = ReplicateSDXLFreshInkInputSchema(&#xA;            prompt: &#34;A fresh ink TOK tattoo of monument valley, Utah&#34;,&#xA;            negativePrompt: &#34;ugly, broken, distorted&#34;&#xA;        )&#xA;        let urls = try await replicateService.createSDXLFreshInkImages(&#xA;            input: input,&#xA;            secondsToWait: 60&#xA;        )&#xA;        print(&#34;Done creating SDXL Fresh Ink images: &#34;, urls)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not create SDXL Fresh Ink images: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the full range of controls for generating an image by viewing &lt;code&gt;ReplicateSDXLFreshInkInputSchema.swift&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to call DeepSeek&#39;s 7B vision model on replicate&lt;/h3&gt; &#xA;&lt;p&gt;Add a file called &#39;my-image.jpg&#39; to Xcode app assets. Then run this snippet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let image = NSImage(named: &#34;my-image&#34;) else {&#xA;        print(&#34;Could not find an image named &#39;my-image&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let imageURL = AIProxy.encodeImageAsURL(image: image, compressionQuality: 0.4) else {&#xA;        print(&#34;Could not encode image as a data URI&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let input = ReplicateDeepSeekVL7BInputSchema(&#xA;            image: imageURL,&#xA;            prompt: &#34;What are the colors in this pic&#34;&#xA;        )&#xA;        let description = try await replicateService.runDeepSeekVL7B(input: input, secondsToWait: 300)&#xA;        print(&#34;Done getting descriptions from DeepSeekVL7B: &#34;, description)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not use deepseek vision on replicate: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to call your own models on Replicate.&lt;/h3&gt; &#xA;&lt;p&gt;Look in the &lt;code&gt;ReplicateService+Convenience.swift&lt;/code&gt; file for inspiration on how to do this.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate the Encodable representation of your input schema. Take a look at any of the input schemas used in &lt;code&gt;ReplicateService+Convenience.swift&lt;/code&gt; for inspiration. Find the schema format that you should conform to using replicate&#39;s web dashboard and tapping through &lt;code&gt;Your Model &amp;gt; API &amp;gt; Schema &amp;gt; Input Schema&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate the Decodable representation of your output schema. The output schema is defined on replicate&#39;s site at &lt;code&gt;Your Model &amp;gt; API &amp;gt; Schema &amp;gt; Output Schema&lt;/code&gt;. I find that unfortunately these schemas are not always accurate, so sometimes you have to look at the network response manually. For simple cases, a typealias will do (for example, if the output schema is just a string or an array of strings). Look at &lt;code&gt;ReplicateFluxOutputSchema.swift&lt;/code&gt; for inspiration. If you need help doing this, please reach out.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Call &lt;code&gt;replicateService.runOfficialModel&lt;/code&gt; or &lt;code&gt;replicateService.runCommunityModel&lt;/code&gt;. Community models have a &lt;code&gt;version&lt;/code&gt; while official models do not.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Call &lt;code&gt;replicateService.getPredictionOutput&lt;/code&gt; on the result from step 3.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You&#39;ll need to change &lt;code&gt;YourInputSchema&lt;/code&gt;, &lt;code&gt;YourOutputSchema&lt;/code&gt; and &lt;code&gt;your-model-version&lt;/code&gt; in this snippet:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let input = YourInputSchema(&#xA;            prompt: &#34;Monument valley, Utah&#34;&#xA;        )&#xA;        let prediction: ReplicatePrediction&amp;lt;YourOutputSchema&amp;gt; = try await replicateService.runCommunityModel( /* or runOfficialModel */&#xA;            version: &#34;your-model-version&#34;,&#xA;            input: input,&#xA;            secondsToWait: secondsToWait&#xA;        )&#xA;        let output: YourOutputSchema = try await replicateService.getPredictionOutput(prediction)&#xA;&#xA;        // Do something with output&#xA;&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not run replicate model: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to upload a file to Replicate&#39;s CDN&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let image = NSImage(named: &#34;face&#34;) else {&#xA;        print(&#34;Drop face.jpeg into Assets first&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let imageData = AIProxy.encodeImageAsJpeg(image: image, compressionQuality: 0.5) else {&#xA;        print(&#34;Could not encode the image as jpeg&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let fileUploadResponse = try await replicateService.uploadFile(&#xA;            contents: imageData,&#xA;            contentType: &#34;image/jpeg&#34;,&#xA;            name: &#34;face.jpg&#34;&#xA;        )&#xA;        print(&#34;&#34;&#34;&#xA;              Image uploaded. Find it at \(fileUploadResponse.urls.get)&#xA;              You can use this file until \(fileUploadResponse.expiresAt ?? &#34;&#34;)&#xA;              &#34;&#34;&#34;)&#xA;&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not upload file to replicate: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to create a replicate model for your own Flux fine tune&lt;/h3&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;&amp;lt;your-account&amp;gt;&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let modelURL = try await replicateService.createModel(&#xA;            owner: &#34;&amp;lt;your-account&amp;gt;&#34;,&#xA;            name: &#34;my-model&#34;,&#xA;            description: &#34;My great model&#34;,&#xA;            hardware: &#34;gpu-t4&#34;,&#xA;            visibility: .private&#xA;        )&#xA;        print(&#34;Your model is at \(modelURL)&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create replicate model: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to upload training data for your own Flux fine tune&lt;/h3&gt; &#xA;&lt;p&gt;Create a zip file called &lt;code&gt;training.zip&lt;/code&gt; and drop it in your Xcode assets. See the &#34;Prepare your training data&#34; section of &lt;a href=&#34;https://replicate.com/blog/fine-tune-flux&#34;&gt;this guide&lt;/a&gt; for tips on what to include in the zip file. Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let trainingData = NSDataAsset(name: &#34;training&#34;) else {&#xA;        print(&#34;&#34;&#34;&#xA;              Drop training.zip file into Assets first.&#xA;              See the &#39;Prepare your training data&#39; of this guide:&#xA;              https://replicate.com/blog/fine-tune-flux&#xA;              &#34;&#34;&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let fileUploadResponse = try await replicateService.uploadTrainingZipFile(&#xA;            zipData: trainingData.data,&#xA;            name: &#34;training.zip&#34;&#xA;        )&#xA;        print(&#34;&#34;&#34;&#xA;              Training file uploaded. Find it at \(fileUploadResponse.urls.get)&#xA;              You you can train with this file until \(fileUploadResponse.expiresAt ?? &#34;&#34;)&#xA;              &#34;&#34;&#34;)&#xA;&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not upload file to replicate: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to train a flux fine-tune&lt;/h3&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;&amp;lt;training-url&amp;gt;&lt;/code&gt; returned from the snippet above. Use the &lt;code&gt;&amp;lt;model-name&amp;gt;&lt;/code&gt; that you used from the snippet above that.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        // You should experiment with the settings in `ReplicateFluxTrainingInput.swift` to&#xA;        // find what works best for your use case.&#xA;        //&#xA;        // The `layersToOptimizeRegex` argument here speeds training and works well for faces.&#xA;        // You could could optionally remove that argument to see if the final trained model&#xA;        // works better for your user case.&#xA;        let trainingInput = ReplicateFluxTrainingInput(&#xA;            inputImages: URL(string: &#34;&amp;lt;training-url&amp;gt;&#34;)!,&#xA;            layersToOptimizeRegex: &#34;transformer.single_transformer_blocks.(7|12|16|20).proj_out&#34;,&#xA;            steps: 200,&#xA;            triggerWord: &#34;face&#34;&#xA;        )&#xA;        let reqBody = ReplicateTrainingRequestBody(destination: &#34;&amp;lt;model-owner&amp;gt;/&amp;lt;model-name&amp;gt;&#34;, input: trainingInput)&#xA;&#xA;&#xA;        // Find valid version numbers here: https://replicate.com/ostris/flux-dev-lora-trainer/train&#xA;        let training = try await replicateService.createTraining(&#xA;            modelOwner: &#34;ostris&#34;,&#xA;            modelName: &#34;flux-dev-lora-trainer&#34;,&#xA;            versionID: &#34;d995297071a44dcb72244e6c19462111649ec86a9646c32df56daa7f14801944&#34;,&#xA;            body: reqBody&#xA;        )&#xA;        print(&#34;Get training status at: \(training.urls?.get?.absoluteString ?? &#34;unknown&#34;)&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create replicate training: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to poll the flux fine-tune for training complete&lt;/h3&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;&amp;lt;url&amp;gt;&lt;/code&gt; that is returned from the snippet above.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    // This URL comes from the output of the sample above&#xA;    let url = URL(string: &#34;&amp;lt;url&amp;gt;&#34;)!&#xA;&#xA;    do {&#xA;        let training = try await replicateService.pollForTrainingCompletion(&#xA;            url: url,&#xA;            pollAttempts: 100,&#xA;            secondsBetweenPollAttempts: 10&#xA;        )&#xA;        print(&#34;&#34;&#34;&#xA;              Flux training status: \(training.status?.rawValue ?? &#34;unknown&#34;)&#xA;              Your model version is: \(training.output?.version ?? &#34;unknown&#34;)&#xA;              &#34;&#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not poll for the replicate training: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to generate images with your own flux fine-tune&lt;/h3&gt; &#xA;&lt;p&gt;Use the &lt;code&gt;&amp;lt;version&amp;gt;&lt;/code&gt; string that was returned from the snippet above, but do not include the model owner and model name in the string.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let replicateService = AIProxy.replicateDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-replicate-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let replicateService = AIProxy.replicateService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let input = ReplicateFluxFineTuneInputSchema(&#xA;        prompt: &#34;an oil painting of my face on a blimp&#34;,&#xA;        model: .dev,&#xA;        numInferenceSteps: 28  // Replicate recommends around 28 steps for `.dev` and 4 for `.schnell`&#xA;    )&#xA;&#xA;    do {&#xA;        let predictionResponse = try await replicateService.createPrediction(&#xA;            version: &#34;&amp;lt;version&amp;gt;&#34;,&#xA;            input: input,&#xA;            output: ReplicatePrediction&amp;lt;[URL]&amp;gt;.self&#xA;        )&#xA;&#xA;        let predictionOutput: [URL] = try await replicateService.pollForPredictionOutput(&#xA;            predictionResponse: predictionResponse,&#xA;            pollAttempts: 30,&#xA;            secondsBetweenPollAttempts: 5&#xA;        )&#xA;        print(&#34;Done creating predictionOutput: \(predictionOutput)&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create replicate prediction: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;ElevenLabs&lt;/h2&gt; &#xA;&lt;h3&gt;How to use ElevenLabs for text-to-speech&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let elevenLabsService = AIProxy.elevenLabsDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-elevenLabs-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let elevenLabsService = AIProxy.elevenLabsService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let body = ElevenLabsTTSRequestBody(&#xA;            text: &#34;Hello world&#34;&#xA;        )&#xA;        let mpegData = try await elevenLabsService.ttsRequest(&#xA;            voiceID: &#34;EXAVITQu4vr4xnSDxMaL&#34;,&#xA;            body: body&#xA;        )&#xA;&#xA;        // Do not use a local `let` or `var` for AVAudioPlayer.&#xA;        // You need the lifecycle of the player to live beyond the scope of this function.&#xA;        // Instead, use file scope or set the player as a member of a reference type with long life.&#xA;        // For example, at the top of this file you may define:&#xA;        //&#xA;        //   fileprivate var audioPlayer: AVAudioPlayer? = nil&#xA;        //&#xA;        // And then use the code below to play the TTS result:&#xA;        audioPlayer = try AVAudioPlayer(data: mpegData)&#xA;        audioPlayer?.prepareToPlay()&#xA;        audioPlayer?.play()&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create ElevenLabs TTS audio: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;See the full range of TTS controls by viewing &lt;code&gt;ElevenLabsTTSRequestBody.swift&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://api.elevenlabs.io/v1/voices&#34;&gt;https://api.elevenlabs.io/v1/voices&lt;/a&gt; for the IDs that you can pass to &lt;code&gt;voiceID&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How to use ElevenLabs for speech-to-speech&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Record an audio file in quicktime and save it as &#34;helloworld.m4a&#34;&lt;/li&gt; &#xA; &lt;li&gt;Add the audio file to your Xcode project. Make sure it&#39;s included in your target: select your audio file in the project tree, type &lt;code&gt;cmd-opt-0&lt;/code&gt; to open the inspect panel, and view &lt;code&gt;Target Membership&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run this snippet:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let elevenLabsService = AIProxy.elevenLabsDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-elevenLabs-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let elevenLabsService = AIProxy.elevenLabsService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let localAudioURL = Bundle.main.url(forResource: &#34;helloworld&#34;, withExtension: &#34;m4a&#34;) else {&#xA;        print(&#34;Could not find an audio file named helloworld.m4a in your app bundle&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let body = ElevenLabsSpeechToSpeechRequestBody(&#xA;            audio: try Data(contentsOf: localAudioURL),&#xA;            modelID: &#34;eleven_english_sts_v2&#34;,&#xA;            removeBackgroundNoise: true&#xA;        )&#xA;        let mpegData = try await elevenLabsService.speechToSpeechRequest(&#xA;            voiceID: &#34;EXAVITQu4vr4xnSDxMaL&#34;,&#xA;            body: body&#xA;        )&#xA;&#xA;        // Do not use a local `let` or `var` for AVAudioPlayer.&#xA;        // You need the lifecycle of the player to live beyond the scope of this function.&#xA;        // Instead, use file scope or set the player as a member of a reference type with long life.&#xA;        // For example, at the top of this file you may define:&#xA;        //&#xA;        //   fileprivate var audioPlayer: AVAudioPlayer? = nil&#xA;        //&#xA;        // And then use the code below to play the TTS result:&#xA;        audioPlayer = try AVAudioPlayer(data: mpegData)&#xA;        audioPlayer?.prepareToPlay()&#xA;        audioPlayer?.play()&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create ElevenLabs STS audio: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use ElevenLabs for speech-to-text&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Record an audio file in quicktime and save it as &#34;helloworld.m4a&#34;&lt;/li&gt; &#xA; &lt;li&gt;Add the audio file to your Xcode project. Make sure it&#39;s included in your target: select your audio file in the project tree, type &lt;code&gt;cmd-opt-0&lt;/code&gt; to open the inspect panel, and view &lt;code&gt;Target Membership&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run this snippet:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let elevenLabsService = AIProxy.elevenLabsDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-elevenLabs-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let elevenLabsService = AIProxy.elevenLabsService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    guard let localAudioURL = Bundle.main.url(forResource: &#34;helloworld&#34;, withExtension: &#34;m4a&#34;) else {&#xA;        print(&#34;Could not find an audio file named helloworld.m4a in your app bundle&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let body = ElevenLabsSpeechToTextRequestBody(&#xA;            modelID: .scribeV1,&#xA;            file: try Data(contentsOf: localAudioURL),&#xA;        )&#xA;        let res = try await elevenLabsService.speechToTextRequest(&#xA;            body: body&#xA;        )&#xA;        print(&#34;ElevenLabs transcribed: \(res.text ?? &#34;&#34;)&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create ElevenLabs STT audio: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Fal&lt;/h2&gt; &#xA;&lt;h3&gt;How to generate a FastSDXL image using Fal&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let falService = AIProxy.falDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-fal-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let falService = AIProxy.falService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let input = FalFastSDXLInputSchema(&#xA;        prompt: &#34;Yosemite Valley&#34;,&#xA;        enableSafetyChecker: false&#xA;    )&#xA;    do {&#xA;        let output = try await falService.createFastSDXLImage(input: input)&#xA;        print(&#34;&#34;&#34;&#xA;              The first output image is at \(output.images?.first?.url?.absoluteString ?? &#34;&#34;)&#xA;              It took \(output.timings?.inference ?? Double.nan) seconds to generate.&#xA;              &#34;&#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Fal SDXL image: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the full range of controls for generating an image by viewing &lt;code&gt;FalFastSDXLInputSchema.swift&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to use the fashn/tryon model on Fal&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;garmentImage&lt;/code&gt; and &lt;code&gt;modelImage&lt;/code&gt; arguments may be:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;A remote URL to the image hosted on a public site&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A local data URL that you construct using &lt;code&gt;AIProxy.encodeImageAsURL&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import AIProxy&#xA;&#xA;/* Uncomment for BYOK use cases */&#xA;// let falService = AIProxy.falDirectService(&#xA;//     unprotectedAPIKey: &#34;your-fal-key&#34;&#xA;// )&#xA;&#xA;/* Uncomment for all other production use cases */&#xA;// let falService = AIProxy.falService(&#xA;//     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;//     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;// )&#xA;&#xA;guard let garmentImage = NSImage(named: &#34;garment-image&#34;),&#xA;      let garmentImageURL = AIProxy.encodeImageAsURL(image: garmentImage&#xA;                                                     compressionQuality: 0.6) else {&#xA;    print(&#34;Could not find an image named &#39;garment-image&#39; in your app assets&#34;)&#xA;    return&#xA;}&#xA;&#xA;guard let modelImage = NSImage(named: &#34;model-image&#34;),&#xA;      let modelImageURL = AIProxy.encodeImageAsURL(image: modelImage,&#xA;                                                   compressionQuality: 0.6) else {&#xA;    print(&#34;Could not find an image named &#39;model-image&#39; in your app assets&#34;)&#xA;    return&#xA;}&#xA;&#xA;let input = FalTryonInputSchema(&#xA;    category: .tops,&#xA;    garmentImage: garmentImageURL,&#xA;    modelImage: modelImageURL&#xA;)&#xA;do {&#xA;    let output = try await falService.createTryonImage(input: input)&#xA;    print(&#34;Tryon image is available at: \(output.images.first?.url.absoluteString ?? &#34;No URL&#34;)&#34;)&#xA;} catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;    print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;} catch {&#xA;    print(&#34;Could not create fashn/tryon image on Fal: \(error.localizedDescription)&#34;)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;How to train Flux on your own images using Fal&lt;/h3&gt; &#xA;&lt;h4&gt;Upload training data to Fal&lt;/h4&gt; &#xA;&lt;p&gt;Your training data must be a zip file of images. You can either pull the zip from assets (what I do here), or construct the zip in memory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let falService = AIProxy.falDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-fal-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let falService = AIProxy.falService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    // Get the images to train with:&#xA;    guard let trainingData = NSDataAsset(name: &#34;training&#34;) else {&#xA;        print(&#34;Drop training.zip file into Assets first&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let url = try await falService.uploadTrainingZipFile(&#xA;            zipData: trainingData.data,&#xA;            name: &#34;training.zip&#34;&#xA;        )&#xA;        print(&#34;Training file uploaded. Find it at \(url.absoluteString)&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not upload file to Fal: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Train &lt;code&gt;fal-ai/flux-lora-fast-training&lt;/code&gt; using your uploaded data&lt;/h4&gt; &#xA;&lt;p&gt;Using the URL returned in the step above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    let input = FalFluxLoRAFastTrainingInputSchema(&#xA;        imagesDataURL: &amp;lt;url-from-step-above&amp;gt;&#xA;        triggerWord: &#34;face&#34;&#xA;    )&#xA;    do {&#xA;        let output = try await falService.createFluxLoRAFastTraining(input: input)&#xA;        print(&#34;&#34;&#34;&#xA;              Fal&#39;s Flux LoRA fast trainer is complete.&#xA;              Your weights are at: \(output.diffusersLoraFile?.url?.absoluteString ?? &#34;&#34;)&#xA;              &#34;&#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Fal Flux training: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;FalFluxLoRAFastTrainingInputSchema.swift&lt;/code&gt; for the full range of training controls.&lt;/p&gt; &#xA;&lt;h4&gt;Run inference on your trained model&lt;/h4&gt; &#xA;&lt;p&gt;Using the LoRA URL returned in the step above:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    let inputSchema = FalFluxLoRAInputSchema(&#xA;        prompt: &#34;face on a blimp over Monument Valley, Utah&#34;,&#xA;        loras: [&#xA;            .init(&#xA;                path: &amp;lt;lora-url-from-step-above&amp;gt;&#xA;                scale: 0.9&#xA;            )&#xA;        ],&#xA;        numImages: 2,&#xA;        outputFormat: .jpeg&#xA;    )&#xA;    do {&#xA;        let output = try await falService.createFluxLoRAImage(input: inputSchema)&#xA;        print(&#34;&#34;&#34;&#xA;              Fal&#39;s Flux LoRA inference is complete.&#xA;              Your images are at: \(output.images?.compactMap {$0.url?.absoluteString} ?? [])&#xA;              &#34;&#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create Fal LoRA image: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;FalFluxLoRAInputSchema.swift&lt;/code&gt; for the full range of inference controls&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Groq&lt;/h2&gt; &#xA;&lt;h3&gt;How to generate a non-streaming chat completion using Groq&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let groqService = AIProxy.groqDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-groq-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let groqService = AIProxy.groqService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let response = try await groqService.chatCompletionRequest(body: .init(&#xA;            messages: [.assistant(content: &#34;hello world&#34;)],&#xA;            model: &#34;mixtral-8x7b-32768&#34;&#xA;        ))&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(error.localizedDescription)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to generate a streaming chat completion using Groq&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let groqService = AIProxy.groqDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-groq-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let groqService = AIProxy.groqService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let stream = try await groqService.streamingChatCompletionRequest(body: .init(&#xA;                messages: [.assistant(content: &#34;hello world&#34;)],&#xA;                model: &#34;mixtral-8x7b-32768&#34;&#xA;            )&#xA;        )&#xA;        for try await chunk in stream {&#xA;            print(chunk.choices.first?.delta.content ?? &#34;&#34;)&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(error.localizedDescription)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to transcribe audio with Groq&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Record an audio file in quicktime and save it as &#34;helloworld.m4a&#34;&lt;/li&gt; &#xA; &lt;li&gt;Add the audio file to your Xcode project. Make sure it&#39;s included in your target: select your audio file in the project tree, type &lt;code&gt;cmd-opt-0&lt;/code&gt; to open the inspect panel, and view &lt;code&gt;Target Membership&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run this snippet:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let groqService = AIProxy.groqDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-groq-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let groqService = AIProxy.groqService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let url = Bundle.main.url(forResource: &#34;helloworld&#34;, withExtension: &#34;m4a&#34;)!&#xA;        let requestBody = GroqTranscriptionRequestBody(&#xA;            file: try Data(contentsOf: url),&#xA;            model: &#34;whisper-large-v3-turbo&#34;,&#xA;            responseFormat: &#34;json&#34;&#xA;        )&#xA;        let response = try await groqService.createTranscriptionRequest(body: requestBody)&#xA;        let transcript = response.text ?? &#34;None&#34;&#xA;        print(&#34;Groq transcribed: \(transcript)&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get audio transcription from Groq: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Perplexity&lt;/h2&gt; &#xA;&lt;h3&gt;How to create a chat completion with Perplexity&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let perplexityService = AIProxy.perplexityDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-perplexity-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let perplexityService = AIProxy.perplexityService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let response = try await perplexityService.chatCompletionRequest(body: .init(&#xA;            messages: [.user(content: &#34;How many national parks in the US?&#34;)],&#xA;            model: &#34;llama-3.1-sonar-small-128k-online&#34;&#xA;        ))&#xA;&#xA;        print(&#xA;            &#34;&#34;&#34;&#xA;            Received from Perplexity:&#xA;            \(response.choices.first?.message?.content ?? &#34;no content&#34;)&#xA;&#xA;            With citations:&#xA;            \(response.citations ?? [&#34;none&#34;])&#xA;&#xA;            Using:&#xA;            \(response.usage?.promptTokens ?? 0) prompt tokens&#xA;            \(response.usage?.completionTokens ?? 0) completion tokens&#xA;            \(response.usage?.totalTokens ?? 0) total tokens&#xA;            &#34;&#34;&#34;&#xA;        )&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create perplexity chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to create a streaming chat completion with Perplexity&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let perplexityService = AIProxy.perplexityDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-perplexity-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let perplexityService = AIProxy.perplexityService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let stream = try await perplexityService.streamingChatCompletionRequest(body: .init(&#xA;            messages: [.user(content: &#34;How many national parks in the US?&#34;)],&#xA;            model: &#34;llama-3.1-sonar-small-128k-online&#34;&#xA;        ))&#xA;&#xA;        var lastChunk: PerplexityChatCompletionResponseBody?&#xA;        for try await chunk in stream {&#xA;            print(chunk.choices.first?.delta?.content ?? &#34;&#34;)&#xA;            lastChunk = chunk&#xA;        }&#xA;&#xA;        if let lastChunk = lastChunk {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Citations:&#xA;                \(lastChunk.citations ?? [&#34;none&#34;])&#xA;&#xA;                Using:&#xA;                \(lastChunk.usage?.promptTokens ?? 0) prompt tokens&#xA;                \(lastChunk.usage?.completionTokens ?? 0) completion tokens&#xA;                \(lastChunk.usage?.totalTokens ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create perplexity streaming chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Mistral&lt;/h2&gt; &#xA;&lt;h3&gt;How to create a chat completion with Mistral&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;api.mistral.ai&lt;/code&gt; as the proxy domain when creating your AIProxy service in the developer dashboard.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let mistralService = AIProxy.mistralDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-mistral-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let mistralService = AIProxy.mistralService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let response = try await mistralService.chatCompletionRequest(body: .init(&#xA;            messages: [.user(content: &#34;Hello world&#34;)],&#xA;            model: &#34;mistral-small-latest&#34;&#xA;        ))&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;        if let usage = response.usage {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokens ?? 0) prompt tokens&#xA;                 \(usage.completionTokens ?? 0) completion tokens&#xA;                 \(usage.totalTokens ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create mistral chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to create a streaming chat completion with Mistral&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;api.mistral.ai&lt;/code&gt; as the proxy domain when creating your AIProxy service in the developer dashboard.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let mistralService = AIProxy.mistralDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-mistral-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let mistralService = AIProxy.mistralService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let stream = try await mistralService.streamingChatCompletionRequest(body: .init(&#xA;            messages: [.user(content: &#34;Hello world&#34;)],&#xA;            model: &#34;mistral-small-latest&#34;&#xA;        ))&#xA;        for try await chunk in stream {&#xA;            print(chunk.choices.first?.delta.content ?? &#34;&#34;)&#xA;            if let usage = chunk.usage {&#xA;                print(&#xA;                    &#34;&#34;&#34;&#xA;                    Used:&#xA;                     \(usage.promptTokens ?? 0) prompt tokens&#xA;                     \(usage.completionTokens ?? 0) completion tokens&#xA;                     \(usage.totalTokens ?? 0) total tokens&#xA;                    &#34;&#34;&#34;&#xA;                )&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not create mistral streaming chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;EachAI&lt;/h2&gt; &#xA;&lt;h3&gt;How to kick off an EachAI workflow&lt;/h3&gt; &#xA;&lt;p&gt;Use &lt;code&gt;flows.eachlabs.ai&lt;/code&gt; as the proxy domain when creating your AIProxy service in the developer dashboard.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let eachAIService = AIProxy.eachAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-eachAI-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let eachAIService = AIProxy.eachAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    // Update the arguments here based on your eachlabs use case:&#xA;    let workflowID = &#34;your-workflow-id&#34;&#xA;    let requestBody = EachAITriggerWorkflowRequestBody(&#xA;        parameters: [&#xA;            &#34;img&#34;: &#34;https://storage.googleapis.com/magicpoint/models/women.png&#34;&#xA;        ]&#xA;    )&#xA;&#xA;    do {&#xA;        let triggerResponse = try await eachAIService.triggerWorkflow(&#xA;            workflowID: workflowID,&#xA;            body: requestBody&#xA;        )&#xA;        let executionResponse = try await eachAIService.pollForWorkflowExecutionComplete(&#xA;            workflowID: workflowID,&#xA;            triggerID: triggerResponse.triggerID&#xA;        )&#xA;        print(&#34;Workflow result is available at \(executionResponse.output ?? &#34;output missing&#34;)&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not execute EachAI workflow: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;OpenRouter&lt;/h2&gt; &#xA;&lt;h3&gt;How to make a streaming DeepSeek R1 completion with OpenRouter&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openRouterService = AIProxy.openRouterDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openRouter-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openRouterService = AIProxy.openRouterService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = OpenRouterChatCompletionRequestBody(&#xA;        messages: [&#xA;            .system(content: .text(&#34;You are a math assistant.&#34;)),&#xA;            .user(content: .text(&#34;Here&#39;s why burgers&#39; equation leads to a breaking nonlinearity in shallow water&#34;))&#xA;        ],&#xA;        models: [&#xA;            &#34;deepseek/deepseek-r1&#34;,&#xA;        ],&#xA;        reasoning: .init(effort: .low),&#xA;        temperature: 0.0 /* Set this based on your use case: https://api-docs.deepseek.com/quick_start/parameter_settings*/&#xA;    )&#xA;&#xA;    do {&#xA;        let stream = try await openRouterService.streamingChatCompletionRequest(&#xA;            body: requestBody&#xA;        )&#xA;        for try await chunk in stream {&#xA;            if let reasoningContent = chunk.choices.first?.delta.reasoning {&#xA;                print(&#34;Reasoning chunk: \(reasoningContent)&#34;)&#xA;            }&#xA;            if let messageContent = chunk.choices.first?.delta.content {&#xA;                print(&#34;Message chunk: \(messageContent)&#34;)&#xA;            }&#xA;            if let usage = chunk.usage {&#xA;                print(&#xA;                    &#34;&#34;&#34;&#xA;                    Served by \(chunk.provider ?? &#34;unspecified&#34;)&#xA;                    using model \(chunk.model ?? &#34;unspecified&#34;)&#xA;                    Used:&#xA;                     \(usage.promptTokens ?? 0) prompt tokens&#xA;                     \(usage.completionTokens ?? 0) completion tokens&#xA;                     \(usage.totalTokens ?? 0) total tokens&#xA;                    &#34;&#34;&#34;&#xA;                )&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch let err as URLError where err.code == URLError.timedOut {&#xA;        print(&#34;Request for OpenRouter buffered chat completion timed out&#34;)&#xA;    } catch let err as URLError where [.notConnectedToInternet, .networkConnectionLost].contains(err.code) {&#xA;        print(&#34;Could not make OpenRouter streaming R1 chat request. Please check your internet connection&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get OpenRouter streaming R1 chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may optionally add your provider preferences as part of the request body:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;OpenRouterChatCompletionRequestBody(&#xA;  // ...&#xA;  provider: .init(order[&#xA;      &#34;first-choice&#34;,&#xA;      &#34;second-choice&#34;&#xA;  ]),&#xA;  // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the provider list here for the viable options: &lt;a href=&#34;https://openrouter.ai/deepseek/deepseek-r1/providers&#34;&gt;https://openrouter.ai/deepseek/deepseek-r1/providers&lt;/a&gt; And then use the corresponding enum from this list: &lt;a href=&#34;https://openrouter.ai/docs/features/provider-routing#json-schema-for-provider-preferences&#34;&gt;https://openrouter.ai/docs/features/provider-routing#json-schema-for-provider-preferences&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to make a buffered DeepSeek R1 completion with OpenRouter&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openRouterService = AIProxy.openRouterDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openRouter-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openRouterService = AIProxy.openRouterService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = OpenRouterChatCompletionRequestBody(&#xA;        messages: [&#xA;            .system(content: .text(&#34;You are a math assistant.&#34;)),&#xA;            .user(content: .text(&#34;Here&#39;s why burgers&#39; equation leads to a breaking nonlinearity in shallow water&#34;))&#xA;        ],&#xA;        models: [&#xA;            &#34;deepseek/deepseek-r1&#34;,&#xA;        ],&#xA;        reasoning: .init(effort: .low),&#xA;        temperature: 0.0 /* Set this based on your use case: https://api-docs.deepseek.com/quick_start/parameter_settings*/&#xA;    )&#xA;&#xA;    do {&#xA;&#xA;        let response = try await openRouterService.chatCompletionRequest(&#xA;            body: requestBody,&#xA;            secondsToWait: 300&#xA;        )&#xA;&#xA;        print(&#34;&#34;&#34;&#xA;            Served by \(response.provider ?? &#34;unspecified&#34;)&#xA;            using model \(response.model ?? &#34;unspecified&#34;)&#xA;&#xA;            DeepSeek used the following reasoning:&#xA;&#xA;            \(response.choices.first?.message.reasoning ?? &#34;&#34;)&#xA;&#xA;            And responded with:&#xA;&#xA;            \(response.choices.first?.message.content ?? &#34;&#34;)&#xA;&#xA;            Used:&#xA;             \(response.usage?.completionTokens ?? 0) completion tokens&#xA;             \(response.usage?.promptTokens ?? 0) prompt tokens&#xA;             \(response.usage?.totalTokens ?? 0) total tokens&#xA;            &#34;&#34;&#34;&#xA;        )&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch let err as URLError where err.code == URLError.timedOut {&#xA;        print(&#34;Request for OpenRouter buffered chat completion timed out&#34;)&#xA;    } catch let err as URLError where [.notConnectedToInternet, .networkConnectionLost].contains(err.code) {&#xA;        print(&#34;Could not make OpenRouter buffered R1 chat request. Please check your internet connection&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get OpenRouter buffered R1 chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may optionally add your provider preferences as part of the request body:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;OpenRouterChatCompletionRequestBody(&#xA;  // ...&#xA;  provider: .init(order[&#xA;      &#34;first-choice&#34;,&#xA;      &#34;second-choice&#34;&#xA;  ]),&#xA;  // ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the provider list here for the viable options: &lt;a href=&#34;https://openrouter.ai/deepseek/deepseek-r1/providers&#34;&gt;https://openrouter.ai/deepseek/deepseek-r1/providers&lt;/a&gt; And then use the corresponding enum from this list: &lt;a href=&#34;https://openrouter.ai/docs/features/provider-routing#json-schema-for-provider-preferences&#34;&gt;https://openrouter.ai/docs/features/provider-routing#json-schema-for-provider-preferences&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How to make a non-streaming chat completion with OpenRouter&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openRouterService = AIProxy.openRouterDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openRouter-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openRouterService = AIProxy.openRouterService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let requestBody = OpenRouterChatCompletionRequestBody(&#xA;            messages: [.user(content: .text(&#34;hello world&#34;))],&#xA;            models: [&#xA;                &#34;deepseek/deepseek-chat&#34;,&#xA;                &#34;google/gemini-2.0-flash-exp:free&#34;,&#xA;                // ...&#xA;            ],&#xA;            route: .fallback&#xA;        )&#xA;        let response = try await openRouterService.chatCompletionRequest(requestBody)&#xA;        print(&#34;&#34;&#34;&#xA;            Received: \(response.choices.first?.message.content ?? &#34;&#34;)&#xA;            Served by \(response.provider ?? &#34;unspecified&#34;)&#xA;            using model \(response.model ?? &#34;unspecified&#34;)&#xA;            &#34;&#34;&#34;&#xA;        )&#xA;        if let usage = response.usage {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokens ?? 0) prompt tokens&#xA;                 \(usage.completionTokens ?? 0) completion tokens&#xA;                 \(usage.totalTokens ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get OpenRouter buffered chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a streaming chat completion with OpenRouter&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openRouterService = AIProxy.openRouterDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openRouter-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openRouterService = AIProxy.openRouterService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = OpenRouterChatCompletionRequestBody(&#xA;        messages: [.user(content: .text(&#34;hello world&#34;))],&#xA;        models: [&#xA;            &#34;deepseek/deepseek-chat&#34;,&#xA;            &#34;google/gemini-2.0-flash-exp:free&#34;,&#xA;            // ...&#xA;        ],&#xA;        route: .fallback&#xA;    )&#xA;&#xA;    do {&#xA;        let stream = try await openRouterService.streamingChatCompletionRequest(body: requestBody)&#xA;        for try await chunk in stream {&#xA;            print(chunk.choices.first?.delta.content ?? &#34;&#34;)&#xA;            if let usage = chunk.usage {&#xA;                print(&#xA;                    &#34;&#34;&#34;&#xA;                    Served by \(chunk.provider ?? &#34;unspecified&#34;)&#xA;                    using model \(chunk.model ?? &#34;unspecified&#34;)&#xA;                    Used:&#xA;                     \(usage.promptTokens ?? 0) prompt tokens&#xA;                     \(usage.completionTokens ?? 0) completion tokens&#xA;                     \(usage.totalTokens ?? 0) total tokens&#xA;                    &#34;&#34;&#34;&#xA;                )&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get OpenRouter streaming chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a structured outputs chat completion with OpenRouter&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openRouterService = AIProxy.openRouterDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openRouter-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openRouterService = AIProxy.openRouterService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let schema: [String: AIProxyJSONValue] = [&#xA;            &#34;type&#34;: &#34;object&#34;,&#xA;            &#34;properties&#34;: [&#xA;                &#34;colors&#34;: [&#xA;                    &#34;type&#34;: &#34;array&#34;,&#xA;                    &#34;items&#34;: [&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: [&#xA;                            &#34;name&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;A descriptive name to give the color&#34;&#xA;                            ],&#xA;                            &#34;hex_code&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;The hex code of the color&#34;&#xA;                            ]&#xA;                        ],&#xA;                        &#34;required&#34;: [&#34;name&#34;, &#34;hex_code&#34;],&#xA;                        &#34;additionalProperties&#34;: false&#xA;                    ]&#xA;                ]&#xA;            ],&#xA;            &#34;required&#34;: [&#34;colors&#34;],&#xA;            &#34;additionalProperties&#34;: false&#xA;        ]&#xA;        let requestBody = OpenRouterChatCompletionRequestBody(&#xA;            messages: [&#xA;                .system(content: .text(&#34;Return valid JSON only, and follow the specified JSON structure&#34;)),&#xA;                .user(content: .text(&#34;Return a peaches and cream color palette&#34;))&#xA;            ],&#xA;            models: [&#xA;                &#34;cohere/command-r7b-12-2024&#34;,&#xA;                &#34;meta-llama/llama-3.3-70b-instruct&#34;,&#xA;                // ...&#xA;            ],&#xA;            responseFormat: .jsonSchema(&#xA;                name: &#34;palette_creator&#34;,&#xA;                description: &#34;A list of colors that make up a color pallete&#34;,&#xA;                schema: schema,&#xA;                strict: true&#xA;            ),&#xA;            route: .fallback&#xA;        )&#xA;        let response = try await openRouterService.chatCompletionRequest(body: requestBody)&#xA;        print(&#34;&#34;&#34;&#xA;            Received: \(response.choices.first?.message.content ?? &#34;&#34;)&#xA;            Served by \(response.provider ?? &#34;unspecified&#34;)&#xA;            using model \(response.model ?? &#34;unspecified&#34;)&#xA;            &#34;&#34;&#34;&#xA;        )&#xA;        if let usage = response.usage {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokens ?? 0) prompt tokens&#xA;                 \(usage.completionTokens ?? 0) completion tokens&#xA;                 \(usage.totalTokens ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get structured outputs response from OpenRouter: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to use vision requests on OpenRouter (multi-modal chat)&lt;/h3&gt; &#xA;&lt;p&gt;On macOS, use &lt;code&gt;NSImage(named:)&lt;/code&gt; in place of &lt;code&gt;UIImage(named:)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openRouterService = AIProxy.openRouterDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openRouter-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openRouterService = AIProxy.openRouterService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;    guard let image = NSImage(named: &#34;myImage&#34;) else {&#xA;        print(&#34;Could not find an image named &#39;myImage&#39; in your app assets&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    guard let imageURL = AIProxy.encodeImageAsURL(image: image, compressionQuality: 0.6) else {&#xA;        print(&#34;Could not encode image as a data URI&#34;)&#xA;        return&#xA;    }&#xA;&#xA;    do {&#xA;        let response = try await openRouterService.chatCompletionRequest(body: .init(&#xA;            messages: [&#xA;                .system(&#xA;                    content: .text(&#34;Tell me what you see&#34;)&#xA;                ),&#xA;                .user(&#xA;                    content: .parts(&#xA;                        [&#xA;                            .text(&#34;What do you see?&#34;),&#xA;                            .imageURL(imageURL)&#xA;                        ]&#xA;                    )&#xA;                )&#xA;            ],&#xA;            models: [&#xA;                &#34;x-ai/grok-2-vision-1212&#34;,&#xA;                &#34;openai/gpt-4o&#34;&#xA;            ],&#xA;            route: .fallback&#xA;        ))&#xA;        print(&#34;&#34;&#34;&#xA;            Received: \(response.choices.first?.message.content ?? &#34;&#34;)&#xA;            Served by \(response.provider ?? &#34;unspecified&#34;)&#xA;            using model \(response.model ?? &#34;unspecified&#34;)&#xA;            &#34;&#34;&#34;&#xA;        )&#xA;        if let usage = response.usage {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokens ?? 0) prompt tokens&#xA;                 \(usage.completionTokens ?? 0) completion tokens&#xA;                 \(usage.totalTokens ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not make a vision request to OpenRouter: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a tool call with OpenRouter&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let openRouterService = AIProxy.openRouterDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-openRouter-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let openRouterService = AIProxy.openRouterService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let completion = try await openRouterService.chatCompletionRequest(body: .init(&#xA;            messages: [&#xA;                .user(&#xA;                   content: .text(&#34;What is the weather in SF?&#34;)&#xA;               )&#xA;            ],&#xA;            models: [&#xA;                &#34;cohere/command-r7b-12-2024&#34;,&#xA;                &#34;meta-llama/llama-3.3-70b-instruct&#34;,&#xA;                // ...&#xA;            ],&#xA;            route: .fallback,&#xA;            tools: [&#xA;                .function(&#xA;                    name: &#34;get_weather&#34;,&#xA;                    description: &#34;Get current temperature for a given location.&#34;,&#xA;                    parameters: [&#xA;                        &#34;type&#34;: &#34;object&#34;,&#xA;                        &#34;properties&#34;: [&#xA;                            &#34;location&#34;: [&#xA;                                &#34;type&#34;: &#34;string&#34;,&#xA;                                &#34;description&#34;: &#34;City and country e.g. Bogotá, Colombia&#34;&#xA;                            ]&#xA;                        ],&#xA;                        &#34;required&#34;: [&#34;location&#34;],&#xA;                        &#34;additionalProperties&#34;: false&#xA;                    ],&#xA;                    strict: true&#xA;                )&#xA;            ]&#xA;        ))&#xA;        if let toolCall = completion.choices.first?.message.toolCalls?.first {&#xA;            print(&#34;&#34;&#34;&#xA;                The model wants us to call function: \(toolCall.function?.name ?? &#34;&#34;)&#xA;                With arguments: \(toolCall.function?.arguments ?? [:])&#xA;                Served by \(completion.provider ?? &#34;unspecified&#34;)&#xA;                using model \(completion.model ?? &#34;unspecified&#34;)&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;        if let usage = completion.usage {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.promptTokens ?? 0) prompt tokens&#xA;                 \(usage.completionTokens ?? 0) completion tokens&#xA;                 \(usage.totalTokens ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get first chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;DeepSeek&lt;/h2&gt; &#xA;&lt;h3&gt;How to make a chat completion request with DeepSeek&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxySwift&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let deepSeekService = AIProxy.deepSeekDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-deepseek-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let deepSeekService = AIProxy.deepSeekService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = DeepSeekChatCompletionRequestBody(&#xA;        messages: [&#xA;            .system(content: &#34;You are a helpful assistant.&#34;),&#xA;            .user(content: &#34;Hello!&#34;)&#xA;        ],&#xA;        model: &#34;deepseek-chat&#34;&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await deepSeekService.chatCompletionRequest(body: requestBody)&#xA;        print(&#34;\(response.choices.first?.message.content ?? &#34;&#34;)&#34;)&#xA;        if let usage = response.usage {&#xA;            print(&#xA;                &#34;&#34;&#34;&#xA;                Used:&#xA;                 \(usage.completionTokens ?? 0) completion tokens&#xA;                 \(usage.completionTokensDetails?.reasoningTokens ?? 0) reasoning tokens&#xA;                 \(usage.promptCacheHitTokens ?? 0) prompt cache hit tokens&#xA;                 \(usage.promptCacheMissTokens ?? 0) prompt cache miss tokens&#xA;                 \(usage.promptTokens ?? 0) prompt tokens&#xA;                 \(usage.totalTokens ?? 0) total tokens&#xA;                &#34;&#34;&#34;&#xA;            )&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get DeepSeek buffered chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a reasoning chat completion with DeepSeek R1&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxySwift&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let deepSeekService = AIProxy.deepSeekDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-deepseek-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let deepSeekService = AIProxy.deepSeekService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let requestBody = DeepSeekChatCompletionRequestBody(&#xA;            messages: [&#xA;                .system(content: &#34;You are a math assistant. Don&#39;t use LaTeX, use utf-8 symbols only.&#34;),&#xA;                .user(content: &#34;Here&#39;s why Burgers&#39; equation leads to a breaking nonlinearity in shallow water&#34;)&#xA;            ],&#xA;            model: &#34;deepseek-reasoner&#34;,&#xA;            temperature: 0.0 /* Set this based on your use case: https://api-docs.deepseek.com/quick_start/parameter_settings*/&#xA;        )&#xA;        let response = try await deepSeekService.chatCompletionRequest(&#xA;            body: requestBody,&#xA;            secondsToWait: 300&#xA;        )&#xA;&#xA;        print(&#xA;            &#34;&#34;&#34;&#xA;            DeepSeek used the following reasoning:&#xA;&#xA;            \(response.choices.first?.message.reasoningContent ?? &#34;&#34;)&#xA;&#xA;            And responded with:&#xA;&#xA;            \(response.choices.first?.message.content ?? &#34;&#34;)&#xA;&#xA;            Used:&#xA;             \(response.usage?.completionTokens ?? 0) completion tokens&#xA;             \(response.usage?.completionTokensDetails?.reasoningTokens ?? 0) reasoning tokens&#xA;             \(response.usage?.promptCacheHitTokens ?? 0) prompt cache hit tokens&#xA;             \(response.usage?.promptCacheMissTokens ?? 0) prompt cache miss tokens&#xA;             \(response.usage?.promptTokens ?? 0) prompt tokens&#xA;             \(response.usage?.totalTokens ?? 0) total tokens&#xA;            &#34;&#34;&#34;&#xA;        )&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch let err as URLError where err.code == URLError.timedOut {&#xA;        print(&#34;Request for DeepSeek buffered chat completion timed out&#34;)&#xA;    } catch let err as URLError where [.notConnectedToInternet, .networkConnectionLost].contains(err.code) {&#xA;        print(&#34;Could not make buffered chat request. Please check your internet connection&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get DeepSeek buffered chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a streaming chat completion with DeepSeek&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxySwift&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let deepSeekService = AIProxy.deepSeekDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-deepseek-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let deepSeekService = AIProxy.deepSeekService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = DeepSeekChatCompletionRequestBody(&#xA;        messages: [&#xA;            .system(content: &#34;You are a helpful assistant.&#34;),&#xA;            .user(content: &#34;Hello!&#34;)&#xA;        ],&#xA;        model: &#34;deepseek-chat&#34;&#xA;    )&#xA;&#xA;    do {&#xA;        let stream = try await deepSeekService.streamingChatCompletionRequest(body: requestBody)&#xA;        for try await chunk in stream {&#xA;            print(chunk.choices.first?.delta.content ?? &#34;&#34;)&#xA;            if let usage = chunk.usage {&#xA;                print(&#xA;                    &#34;&#34;&#34;&#xA;                    Used:&#xA;                    \(usage.completionTokens ?? 0) completion tokens&#xA;                    \(usage.completionTokensDetails?.reasoningTokens ?? 0) reasoning tokens&#xA;                    \(usage.promptCacheHitTokens ?? 0) prompt cache hit tokens&#xA;                    \(usage.promptCacheMissTokens ?? 0) prompt cache miss tokens&#xA;                    \(usage.promptTokens ?? 0) prompt tokens&#xA;                    \(usage.totalTokens ?? 0) total tokens&#xA;                    &#34;&#34;&#34;&#xA;                )&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received \(statusCode) status code with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not make streaming DeepSeek request&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a streaming reasoning chat completion with DeepSeek R1&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxySwift&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let deepSeekService = AIProxy.deepSeekDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-deepseek-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let deepSeekService = AIProxy.deepSeekService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = DeepSeekChatCompletionRequestBody(&#xA;        messages: [&#xA;            .system(content: &#34;You are a math assistant. Don&#39;t use LaTeX, use utf-8 symbols only.&#34;),&#xA;            .user(content: &#34;Here&#39;s why Burgers&#39; equation leads to a breaking nonlinearity in shallow water&#34;)&#xA;        ],&#xA;        model: &#34;deepseek-reasoner&#34;,&#xA;        temperature: 0.0 /* Set this based on your use case: https://api-docs.deepseek.com/quick_start/parameter_settings*/&#xA;    )&#xA;&#xA;    do {&#xA;        let stream = try await deepSeekService.streamingChatCompletionRequest(body: requestBody)&#xA;&#xA;        for try await chunk in stream {&#xA;            if let reasoningContent = chunk.choices.first?.delta.reasoningContent {&#xA;                print(&#34;Reasoning chunk: \(reasoningContent)&#34;)&#xA;            }&#xA;            if let messageContent = chunk.choices.first?.delta.content {&#xA;                print(&#34;Message chunk: \(messageContent)&#34;)&#xA;            }&#xA;            if let usage = chunk.usage {&#xA;                print(&#xA;                    &#34;&#34;&#34;&#xA;                    Used:&#xA;                    \(usage.completionTokens ?? 0) completion tokens&#xA;                    \(usage.completionTokensDetails?.reasoningTokens ?? 0) reasoning tokens&#xA;                    \(usage.promptCacheHitTokens ?? 0) prompt cache hit tokens&#xA;                    \(usage.promptCacheMissTokens ?? 0) prompt cache miss tokens&#xA;                    \(usage.promptTokens ?? 0) prompt tokens&#xA;                    \(usage.totalTokens ?? 0) total tokens&#xA;                    &#34;&#34;&#34;&#xA;                )&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch let err as URLError where err.code == URLError.timedOut {&#xA;        print(&#34;Request for DeepSeek R1 streaming chat completion timed out&#34;)&#xA;    } catch let err as URLError where [.notConnectedToInternet, .networkConnectionLost].contains(err.code) {&#xA;        print(&#34;Could not make DeepSeek R1 streaming chat request. Please check your internet connection&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get DeepSeek R1 streaming chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Fireworks AI&lt;/h2&gt; &#xA;&lt;h3&gt;How to make a streaming DeepSeek R1 request to Fireworks AI&lt;/h3&gt; &#xA;&lt;p&gt;FireworksAI works differently than going to DeepSeek directly in that the reasoning content is not on the messages&#39;s &lt;code&gt;reasoningContent&lt;/code&gt; property. Instead, the reasoning content is included in &lt;code&gt;message.content&lt;/code&gt; enclosed in &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxySwift&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let fireworksAIService = AIProxy.fireworksAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-fireworks-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let fireworksAIService = AIProxy.fireworksAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = DeepSeekChatCompletionRequestBody(&#xA;        messages: [&#xA;            .system(content: &#34;You are a math assistant.&#34;),&#xA;            .user(content: &#34;Here&#39;s why burgers&#39; equation leads to a breaking nonlinearity in shallow water&#34;)&#xA;        ],&#xA;        model: &#34;accounts/fireworks/models/deepseek-r1&#34;,&#xA;        temperature: 0.0 /* Set this based on your use case: https://api-docs.deepseek.com/quick_start/parameter_settings*/&#xA;    )&#xA;&#xA;    do {&#xA;        let stream = try await fireworksAIService.streamingDeepSeekR1Request(&#xA;            body: requestBody,&#xA;            secondsToWait: 300&#xA;        )&#xA;        for try await chunk in stream {&#xA;            print(chunk.choices.first?.delta.content ?? &#34;&#34;)&#xA;            if let usage = chunk.usage {&#xA;                print(&#xA;                    &#34;&#34;&#34;&#xA;                    Used:&#xA;                    \(usage.completionTokens ?? 0) completion tokens&#xA;                    \(usage.promptTokens ?? 0) prompt tokens&#xA;                    \(usage.totalTokens ?? 0) total tokens&#xA;                    &#34;&#34;&#34;&#xA;                )&#xA;            }&#xA;        }&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch let err as URLError where err.code == URLError.timedOut {&#xA;        print(&#34;R1 request to FireworksAI timed out&#34;)&#xA;    } catch let err as URLError where [.notConnectedToInternet, .networkConnectionLost].contains(err.code) {&#xA;        print(&#34;Could not complete R1 request to FireworksAI. Please check your internet connection&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not complete R1 request to FireworksAI: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How to make a buffered DeepSeek R1 request to Fireworks AI&lt;/h3&gt; &#xA;&lt;p&gt;FireworksAI works differently than going to DeepSeek directly in that the reasoning content is not on the messages&#39;s &lt;code&gt;reasoningContent&lt;/code&gt; property. Instead, the reasoning content is included in &lt;code&gt;message.content&lt;/code&gt; enclosed in &lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let fireworksAIService = AIProxy.fireworksAIDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-fireworks-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let fireworksAIService = AIProxy.fireworksAIService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    let requestBody = DeepSeekChatCompletionRequestBody(&#xA;        messages: [&#xA;            .system(content: &#34;You are a math assistant&#34;),&#xA;            .user(content: &#34;Here&#39;s why Burgers&#39; equation leads to a breaking nonlinearity in shallow water&#34;)&#xA;        ],&#xA;        model: &#34;accounts/fireworks/models/deepseek-r1&#34;,&#xA;        temperature: 0.0 /* Set this based on your use case: https://api-docs.deepseek.com/quick_start/parameter_settings*/&#xA;    )&#xA;    do {&#xA;        let response = try await fireworksAIService.deepSeekR1Request(&#xA;            body: requestBody,&#xA;            secondsToWait: 300&#xA;        )&#xA;        print(&#xA;            &#34;&#34;&#34;&#xA;            FireworksAI puts the DeepSeek reasoning steps inside a &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags. Here&#39;s the full response:&#xA;&#xA;            \(response.choices.first?.message.content ?? &#34;&#34;)&#xA;&#xA;            Used:&#xA;             \(response.usage?.completionTokens ?? 0) completion tokens&#xA;             \(response.usage?.promptTokens ?? 0) prompt tokens&#xA;             \(response.usage?.totalTokens ?? 0) total tokens&#xA;            &#34;&#34;&#34;&#xA;        )&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch let err as URLError where err.code == URLError.timedOut {&#xA;        print(&#34;R1 request to FireworksAI timed out&#34;)&#xA;    } catch let err as URLError where [.notConnectedToInternet, .networkConnectionLost].contains(err.code) {&#xA;        print(&#34;Could not complete R1 request to FireworksAI. Please check your internet connection&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not complete R1 request to FireworksAI: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Brave&lt;/h2&gt; &#xA;&lt;p&gt;When you create a service in the AIProxy dashboard, use &lt;code&gt;https://api.search.brave.com&lt;/code&gt; as the proxy base URL.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    /* Uncomment for BYOK use cases */&#xA;    // let braveService = AIProxy.braveDirectService(&#xA;    //     unprotectedAPIKey: &#34;your-brave-key&#34;&#xA;    // )&#xA;&#xA;    /* Uncomment for all other production use cases */&#xA;    // let braveService = AIProxy.braveService(&#xA;    //     partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;    //     serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    // )&#xA;&#xA;    do {&#xA;        let searchResult = try await braveService.webSearchRequest(query: &#34;How does concurrency work in Swift 6&#34;)&#xA;        let resultCount = searchResult.web?.results?.count ?? 0&#xA;        let urls = searchResult.web?.results?.compactMap { $0.url }&#xA;        print(&#xA;            &#34;&#34;&#34;&#xA;            Brave responded with \(resultCount) search results.&#xA;            The search returned these urls: \(urls ?? [])&#xA;            &#34;&#34;&#34;&#xA;        )&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Receivedt non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch {&#xA;        // You may want to catch additional Foundation errors and pop the appropriate UI&#xA;        // to the user. See &#34;How to catch Foundation errors for specific conditions&#34; here:&#xA;        // https://www.aiproxy.com/docs/integration-options.html&#xA;        print(&#34;Could not make brave search: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;OpenMeteo&lt;/h2&gt; &#xA;&lt;h3&gt;How to fetch the weather with OpenMeteo&lt;/h3&gt; &#xA;&lt;p&gt;This pattern is slightly different than the others, because OpenMeteo has an official lib that we&#39;d like to rely on. To run the snippet below, you&#39;ll need to add AIProxySwift and OpenMeteoSDK to your Xcode project. Add OpenMeteoSDK:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;In Xcode, go to &lt;code&gt;File &amp;gt; Add Package Dependences&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enter the package URL &lt;code&gt;https://github.com/open-meteo/sdk&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Choose your dependency rule (e.g. the &lt;code&gt;main&lt;/code&gt; branch for the most up-to-date package)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Next, use AIProxySwift&#39;s core functionality to get a URLRequest and URLSession, and pass those into the OpenMeteoSDK:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;    import OpenMeteoSdk&#xA;&#xA;    do {&#xA;        let request = try await AIProxy.request(&#xA;            partialKey: &#34;partial-key-from-your-aiproxy-developer-dashboard&#34;,&#xA;            serviceURL: &#34;service-url-from-your-aiproxy-developer-dashboard&#34;,&#xA;            proxyPath: &#34;/v1/forecast?latitude=52.52&amp;amp;longitude=13.41&amp;amp;hourly=temperature_2m&amp;amp;format=flatbuffers&#34;&#xA;        )&#xA;        let session = AIProxy.session()&#xA;        let responses = try await WeatherApiResponse.fetch(request: request, session: session)&#xA;        // Do something with `responses`. For a usage example, follow these instructions:&#xA;        // 1. Navigate to https://open-meteo.com/en/docs&#xA;        // 2. Scroll to the &#39;API response&#39; section&#xA;        // 3. Tap on Swift&#xA;        // 4. Scroll to &#39;Usage&#39;&#xA;        print(responses)&#xA;    } catch {&#xA;        print(&#34;Could not fetch the weather: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Advanced Settings&lt;/h2&gt; &#xA;&lt;h3&gt;Specify your own &lt;code&gt;clientID&lt;/code&gt; to annotate requests&lt;/h3&gt; &#xA;&lt;p&gt;If your app already has client or user IDs that you want to annotate AIProxy requests with, pass a second argument to the provider&#39;s service initializer. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    let openAIService = AIProxy.openAIService(&#xA;        partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;        serviceURL: &#34;service-url-from-your-developer-dashboard&#34;,&#xA;        clientID: &#34;&amp;lt;your-id&amp;gt;&#34;&#xA;    )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Requests that are made using &lt;code&gt;openAIService&lt;/code&gt; will be annotated on the AIProxy backend, so that when you view top users, or the timeline of requests, your client IDs will be familiar.&lt;/p&gt; &#xA;&lt;p&gt;If you do not have existing client or user IDs, no problem! Leave the &lt;code&gt;clientID&lt;/code&gt; argument out, and we&#39;ll generate IDs for you. See &lt;code&gt;AIProxyIdentifier.swift&lt;/code&gt; if you would like to see ID generation specifics.&lt;/p&gt; &#xA;&lt;h3&gt;How to catch Foundation errors for specific conditions&lt;/h3&gt; &#xA;&lt;p&gt;We use Foundation&#39;s &lt;code&gt;URL&lt;/code&gt; types such as &lt;code&gt;URLRequest&lt;/code&gt; and &lt;code&gt;URLSession&lt;/code&gt; for all connections to AIProxy. You can view the various errors that Foundation may raise by viewing NSURLError.h (which is easiest to find by punching &lt;code&gt;cmd-shift-o&lt;/code&gt; in Xcode and searching for it).&lt;/p&gt; &#xA;&lt;p&gt;Some errors may be more interesting to you, and worth their own error handler to pop UI for your user. For example, to catch &lt;code&gt;NSURLErrorTimedOut&lt;/code&gt;, &lt;code&gt;NSURLErrorNetworkConnectionLost&lt;/code&gt; and &lt;code&gt;NSURLErrorNotConnectedToInternet&lt;/code&gt;, you could use the following try/catch structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    import AIProxy&#xA;&#xA;    let openAIService = AIProxy.openAIService(&#xA;        partialKey: &#34;partial-key-from-your-developer-dashboard&#34;,&#xA;        serviceURL: &#34;service-url-from-your-developer-dashboard&#34;&#xA;    )&#xA;&#xA;    do {&#xA;        let response = try await openAIService.chatCompletionRequest(body: .init(&#xA;            model: &#34;gpt-4o-mini&#34;,&#xA;            messages: [.assistant(content: .text(&#34;hello world&#34;))]&#xA;        ))&#xA;        print(response.choices.first?.message.content ?? &#34;&#34;)&#xA;    } catch AIProxyError.unsuccessfulRequest(let statusCode, let responseBody) {&#xA;        print(&#34;Received non-200 status code: \(statusCode) with response body: \(responseBody)&#34;)&#xA;    } catch let err as URLError where err.code == URLError.timedOut {&#xA;        print(&#34;Request for OpenAI buffered chat completion timed out&#34;)&#xA;    } catch let err as URLError where [.notConnectedToInternet, .networkConnectionLost].contains(err.code) {&#xA;        print(&#34;Could not make buffered chat request. Please check your internet connection&#34;)&#xA;    } catch {&#xA;        print(&#34;Could not get buffered chat completion: \(error.localizedDescription)&#34;)&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Troubleshooting&lt;/h1&gt; &#xA;&lt;h2&gt;No such module &#39;AIProxy&#39; error&lt;/h2&gt; &#xA;&lt;p&gt;Occassionally, Xcode fails to automatically add the AIProxy library to your target&#39;s dependency list. If you receive the &lt;code&gt;No such module &#39;AIProxy&#39;&lt;/code&gt; error, first ensure that you have added the package to Xcode using the &lt;a href=&#34;https://github.com/lzell/AIProxySwift?tab=readme-ov-file#installation&#34;&gt;Installation steps&lt;/a&gt;. Next, select your project in the Project Navigator (&lt;code&gt;cmd-1&lt;/code&gt;), select your target, and scroll to the &lt;code&gt;Frameworks, Libraries, and Embedded Content&lt;/code&gt; section. Tap on the plus icon:&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/lzell/AIProxySwift/assets/35940/438e2bbb-688c-49bc-aa2a-ea85806818d5&#34; alt=&#34;Add library dependency&#34; width=&#34;820&#34;&gt; &#xA;&lt;p&gt;And add the AIProxy library:&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/lzell/AIProxySwift/assets/35940/f015a181-9591-435c-a37f-6fb0c8c5050c&#34; alt=&#34;Select the AIProxy dependency&#34; width=&#34;400&#34;&gt; &#xA;&lt;h2&gt;macOS network sandbox&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter the error&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;networkd_settings_read_from_file Sandbox is preventing this process from reading networkd settings file at &#34;/Library/Preferences/com.apple.networkd.plist&#34;, please add an exception.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; A server with the specified hostname could not be found&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify your macOS project settings by tapping on your project in the Xcode project tree, then select &lt;code&gt;Signing &amp;amp; Capabilities&lt;/code&gt; and enable &lt;code&gt;Outgoing Connections (client)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&#39;async&#39; call in a function that does not support concurrency&lt;/h2&gt; &#xA;&lt;p&gt;If you use the snippets above and encounter the error&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;async&#39; call in a function that does not support concurrency&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;it is because we assume you are in a structured concurrency context. If you encounter this error, you can use the escape hatch of wrapping your snippet in a &lt;code&gt;Task {}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requests to AIProxy fail in iOS XCTest UI test cases&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to do UI testing and allow the test cases to execute real API requests, you must set the &lt;code&gt;AIPROXY_DEVICE_CHECK_BYPASS&lt;/code&gt; env variable in your test plan &lt;strong&gt;and&lt;/strong&gt; forward the env variable from the test case to the host simulator (Apple does not do this by default, which I consider a bug). Here is how to set it up:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the &lt;code&gt;AIPROXY_DEVICE_CHECK_BYPASS&lt;/code&gt; env variable in your test environment:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Open the scheme editor at &lt;code&gt;Product &amp;gt; Scheme &amp;gt; Edit Scheme&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Select &lt;code&gt;Test&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Tap through to the test plan&lt;/p&gt; &lt;img src=&#34;https://github.com/lzell/AIProxySwift/assets/35940/9a372244-f03e-4fe3-9361-ffc9d729b7d9&#34; alt=&#34;Select test plan&#34; width=&#34;720&#34;&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Select &lt;code&gt;Configurations &amp;gt; Environment Variables&lt;/code&gt;&lt;/p&gt; &lt;img src=&#34;https://github.com/lzell/AIProxySwift/assets/35940/2e042957-2c40-4335-833d-70b2bf56c31a&#34; alt=&#34;Select env variables&#34; width=&#34;780&#34;&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Add the &lt;code&gt;AIPROXY_DEVICE_CHECK_BYPASS&lt;/code&gt; env variable with your value&lt;/p&gt; &lt;img src=&#34;https://github.com/lzell/AIProxySwift/assets/35940/e466097c-1700-401d-add6-07c14dd26ab8&#34; alt=&#34;Enter env variable value&#34; width=&#34;720&#34;&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt; Edit your test cases to forward on the env variable to the host simulator:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;func testExample() throws {&#xA;    let app = XCUIApplication()&#xA;    app.launchEnvironment = [&#xA;        &#34;AIPROXY_DEVICE_CHECK_BYPASS&#34;: ProcessInfo.processInfo.environment[&#34;AIPROXY_DEVICE_CHECK_BYPASS&#34;]!&#xA;    ]&#xA;    app.launch()&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;What is the &lt;code&gt;AIPROXY_DEVICE_CHECK_BYPASS&lt;/code&gt; constant?&lt;/h2&gt; &#xA;&lt;p&gt;AIProxy uses Apple&#39;s &lt;a href=&#34;https://developer.apple.com/documentation/devicecheck&#34;&gt;DeviceCheck&lt;/a&gt; to ensure that requests received by the backend originated from your app on a legitimate Apple device. However, the iOS simulator cannot produce DeviceCheck tokens. Rather than requiring you to constantly build and run on device during development, AIProxy provides a way to skip the DeviceCheck integrity check. The token is intended for use by developers only. If an attacker gets the token, they can make requests to your AIProxy project without including a DeviceCheck token, and thus remove one level of protection.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;AIPROXY_DEVICE_CHECK_BYPASS&lt;/code&gt; is intended for the simulator only. Do not let it leak into a distribution build of your app (including a TestFlight distribution). If you follow the &lt;a href=&#34;https://www.aiproxy.pro/docs/integration-guide.html&#34;&gt;integration steps&lt;/a&gt; we provide, then the constant won&#39;t leak because env variables are not packaged into the app bundle.&lt;/p&gt; &#xA;&lt;h2&gt;What is the &lt;code&gt;aiproxyPartialKey&lt;/code&gt; constant?&lt;/h2&gt; &#xA;&lt;p&gt;This constant is intended to be &lt;strong&gt;included&lt;/strong&gt; in the distributed version of your app. As the name implies, it is a partial representation of your OpenAI key. Specifically, it is one half of an encrypted version of your key. The other half resides on AIProxy&#39;s backend. As your app makes requests to AIProxy, the two encrypted parts are paired, decrypted, and used to fulfill the request to OpenAI.&lt;/p&gt; &#xA;&lt;h2&gt;Community contributions&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome! This library uses the MIT license.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution style guidelines&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Services should conform to a NameService protocol that defines the interface that the direct service and proxied service adopt. Factory methods on AIProxy.swift are typed to return an existential (e.g. NameService) rather than a concrete type (e.g. NameProxiedService)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Why do we do this? Two reason: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt; &lt;p&gt;We want to make it as easy as possible for lib users to swap between the BYOK use case and the proxied use case. By returning an existential, callers can use conditional logic in their app to select which service to use:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;let service = byok ? AIProxy.openaiDirectService() : AIProxy.openaiService()&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;     &lt;li&gt; &lt;p&gt;We prevent the direct and proxied concrete types from diverging in the public interface. As we add more functionality to the service&#39;s protocol, the compiler helps us ensure that the functionality is implemented for our two major use cases.&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In codable representations, fields that are required by the API should be above fields that are optional. Within the two groups (required and optional) all fields should be alphabetically ordered. Separate the two groups with a mark to aid users of ctrl-6:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;// MARK: Optional properties&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Decodables should all have optional properties. Why? We don&#39;t want to fail decoding in live apps if the provider changes something out from under us (which can happen purposefully due to deprecations, or by accident due to regressions). If we use non-optionals in decodable definitions, then a provider removing a field, changing the type of a field, or removing an enum case would cause decoding to fail. You may think this isn&#39;t too bad, since the JSONDecoder throws anyway, and therefore client code will already be wrapped in a do/catch. However, we always want to give the best chance that decodable succeeds &lt;em&gt;for the properties that the client actually uses&lt;/em&gt;. That is, if the provider changes out the enum case of a property unused by the client, we want the client application to continue functioning correctly, not to throw an error and enter the catch branch of the client&#39;s call site.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When a request or response object is deeply nested by the API provider, create nested types in the same namespace as the top level struct. For examples:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct ProviderResponseBody: Decodable {&#xA;&#xA;    public let status: Status?&#xA;&#xA;    // ... other fields ...&#xA;}&#xA;&#xA;// MARK: -&#xA;extension ProviderResponseBody {&#xA;    public enum Status: String, Decodable {&#xA;        case succeeded&#xA;        case failed&#xA;        case canceled&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This pattern avoids collisions, works well with Xcode&#39;s cmd-click to jump to definition, and improves source understanding for folks that use &lt;code&gt;ctrl-6&lt;/code&gt; to navigate through a file.&lt;/p&gt; &lt;p&gt;You may wonder why we don&#39;t nest all types within the original top level type definition:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct ProviderResponseBody: Decodable {&#xA;    public enum Status: String, Decodable {&#xA;        ...&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This approach is readable when the nested types are small and the nesting level is not too deep. When either of those conditions flip, readability suffers. This is particularly true for nested types that require their own coding keys and encodable/decodable logic, which balloon line count with implementation detail that a user of the top level type has no interest in.&lt;/p&gt; &lt;p&gt;You may also wonder why we include the &lt;code&gt;// MARK: -&lt;/code&gt; line. This is makes parsing the ctrl-6 dropdown easier on the eyes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you are implementing an API contract that could reuse a provider&#39;s nested structure, and it&#39;s reasonable to suppose that the two objects will change together, then pull the nested struct into its own file and give it a longer prefix. The example above would become:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;// ProviderResponseBody.swift&#xA;public struct ProviderResponseBody: Decodable {&#xA;&#xA;    // An examples status&#xA;    public let status: ProviderStatus?&#xA;&#xA;    // ... other fields ...&#xA;}&#xA;&#xA;// ProviderStatus.swift&#xA;public enum ProviderStatus: String, Decodable {&#xA;    case succeeded&#xA;    case failed&#xA;    case canceled&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Release naming guidelines&lt;/h3&gt; &#xA;&lt;p&gt;Give each release a semantic version &lt;em&gt;without&lt;/em&gt; a &lt;code&gt;v&lt;/code&gt; prefix on the version name. That is the most reliable way to make Xcode&#39;s &lt;code&gt;File &amp;gt; Add Package Dependency&lt;/code&gt; flow default to sane version values.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;How to use AIProxySwift with custom services&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d like to proxy calls to a service that we don&#39;t have built-in support for, you can do that with the following steps.&lt;/p&gt; &#xA;&lt;p&gt;We recommend that you first go through the &lt;a href=&#34;https://www.aiproxy.com/docs/integration-guide.html&#34;&gt;standard integration video&lt;/a&gt; for a built-in service. This way, any complications that you encounter with DeviceCheck will be overcome before you embark on the custom integration. Once you are seeing 200s from a built-in service, take the following steps to add a custom service to your app:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create an encodable representation of the request body. Let&#39;s say you are looking at a service&#39;s API docs and they specify an endpoint like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;POST api.example.com/chat&#xA;&#xA;Request body:&#xA;&#xA;    - `great_prompt`: String&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You would define a request body that looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    struct ChatRequestBody: Encodable {&#xA;        let greatPrompt: String&#xA;&#xA;        enum CodingKey: String, CodingKeys {&#xA;            case greatPrompt = &#34;great_prompt&#34;&#xA;        }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a decodable represenation of the response body. Imagining an expanded API definition from above:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;POST api.example.com/chat&#xA;&#xA;Request body:&#xA;&#xA;    - `great_prompt`: String&#xA;&#xA;Response body:&#xA;&#xA;    - `generated_message`: String&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You would define a response body that looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;    struct ChatResponseBody: Decodable {&#xA;        let generatedMessage: String?&#xA;&#xA;        enum CodingKey: String, CodingKeys {&#xA;            case generatedMessage = &#34;generated_message&#34;&#xA;        }&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example is straightforward. If the response body has a nested structure, which many do, you will need to add Decodables for the nested types. See the &lt;a href=&#34;https://raw.githubusercontent.com/lzell/AIProxySwift/main/#contribution-style-guidelines&#34;&gt;Contribution style guidelines&lt;/a&gt; above for an example of creating nested decodables.&lt;/p&gt; &lt;p&gt;Pasting the API documentation into an LLM may get you a close representation of the nested structure that you can then polish.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pay attention to the authorization header in your service&#39;s API docs. If it is of the form &lt;code&gt;Authorization: Bearer your-key&lt;/code&gt; then it will work out of the box. If it is another form, please message me as I&#39;ll need to do a backend deploy (it&#39;s quick).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create your service in the AIProxy dashboard entering in the base proxy URL for your service, e.g. in the example above it would be &lt;code&gt;api.example.com&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Submit your API key through the AIProxy dashboard for your service. You will get back a partial key and a service URL.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the information from preceeding steps to craft a request to AIProxy using the top level helper &lt;code&gt;AIProxy.request&lt;/code&gt;. Continuing the example from above:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import AIProxy&#xA;&#xA;func makeTestRequest() async throws {&#xA;    let requestBody = ChatRequestBody(&#xA;      greatPrompt: &#34;hello world&#34;&#xA;    )&#xA;&#xA;    let request = try await AIProxy.request(&#xA;        partialKey: &#34;partial-key-from-step-5&#34;,&#xA;        serviceURL: &#34;service-url-from-step-5&#34;,&#xA;        proxyPath: &#34;/chat&#34;,&#xA;        body: try JSONEncoder().encode(requestBody),&#xA;        verb: .post,&#xA;        headers: [&#xA;            &#34;content-type&#34;: &#34;application/json&#34;&#xA;        ]&#xA;    )&#xA;&#xA;    let session = AIProxy.session()&#xA;    let (data, res) = try await session.data(for: request)&#xA;    guard let httpResponse = res as? HTTPURLResponse else {&#xA;        print(&#34;Network response is not an http response&#34;)&#xA;        return&#xA;    }&#xA;    guard httpResponse.statusCode &amp;gt;= 200 &amp;amp;&amp;amp; httpResponse.statusCode &amp;lt;= 299 else {&#xA;        print(&#34;Non-200 response&#34;)&#xA;        return&#xA;    }&#xA;    let chatResponseBody = try JSONDecoder().decode(&#xA;            ChatResponseBody.self,&#xA;           from: data&#xA;    )&#xA;    print(chatResponseBody.generatedMessage)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Watch the Live Console in the AIProxy dashboard as you make test requests. It will tell you if a status code other than 200 is being returned.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;At this point you should see successful responses in your Xcode project. If you are not, double check your decodable definitions. If you are still not getting successful responses, message me your encodables and decodables and I&#39;ll take a look as as soon as possible.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Traffic sniffing with docker and mitmproxy (advanced)&lt;/h3&gt; &#xA;&lt;p&gt;The method above uses the documentation of a service to build the appropriate request and response structures. There is another way, which takes longer to set up but has the advantage of not relying on potentially stale documentation.&lt;/p&gt; &#xA;&lt;p&gt;Most providers have an official node client. You can run the node client in a sample project inside a docker container and point traffic at mitmproxy to inspect the contents of the request/response structures. You can then take the request/response bodies and paste them into an LLM to generate the encodable/decodable swift representations. Here&#39;s how:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install mitmproxy and run it with &lt;code&gt;mitmproxy --listen-port 9090&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a Docker container using the client you are interested in. For example, to sniff traffic from Gemini&#39;s official lib, I do this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir ~/dev/node_docker_sandbox&#xA;cd ~/dev/node_docker_sandbox&#xA;cp ~/.mitmproxy/mitmproxy-ca-cert.pem .&#xA;docker pull node:22&#xA;vim Dockerfile&#xA;&#xA;    FROM node:22&#xA;    WORKDIR /entrypoint&#xA;    COPY mitmproxy-ca-cert.pem /usr/local/share/ca-certificates/mitmproxy-ca-cert.pem&#xA;    ENV NODE_EXTRA_CA_CERTS=/usr/local/share/ca-certificates/mitmproxy-ca-cert.pem&#xA;    CMD [&#34;node&#34;, &#34;/entrypoint/generative-ai-js/samples/text_generation.js&#34;]&#xA;&#xA;git clone https://github.com/google/generative-ai-js&#xA;npm install --prefix generative-ai-js/samples&#xA;docker --debug build -t node_docker_sandbox .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In Docker Desktop, go to Settings &amp;gt; Resources &amp;gt; Proxies and flip on &#39;Manual proxy configuration&#39;. Set both &#39;Web Server&#39; and &#39;Secure Web Server&#39; to &lt;code&gt;http://localhost:9090&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the docker container:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --volume &#34;$(pwd)/:/entrypoint/&#34; node_docker_sandbox&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If all is set up correctly, you will see requests and responses flow through mitmproxy in plain text. You can use those bodies to build your swift structs, implementing an encodable representation for the request body and decodable representation for response body.&lt;/p&gt;</summary>
  </entry>
</feed>