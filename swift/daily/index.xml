<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-05T01:48:39Z</updated>
  <subtitle>Daily Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>HyperARCo/Mirador</title>
    <updated>2023-06-05T01:48:39Z</updated>
    <id>tag:github.com,2023-06-05:/HyperARCo/Mirador</id>
    <link href="https://github.com/HyperARCo/Mirador" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mirador makes it easy to build impressive “Point of Interest” AR experiences, from anywhere in the world.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/HyperARCo/Mirador/main/banner.jpg&#34; alt=&#34;Mirador&#34; title=&#34;Mirador&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mirador makes it easy to build impressive point-of-interest AR experiences on top of Apple’s new AR platform, RealityKit.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/HyperARCo/Mirador/main/demo.gif&#34; alt=&#34;Mirador demo&#34; title=&#34;Mirador demo&#34; width=&#34;300&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Mirador is built by &lt;a href=&#34;https://twitter.com/andrewhartar&#34;&gt;Andrew Hart&lt;/a&gt;, who pioneered AR navigation and built the largest open-source project for Apple’s first AR platform, ARKit. Andrew is now founder of &lt;a href=&#34;https://HyperAR.com&#34;&gt;Hyper&lt;/a&gt;, bringing the same technology to retail stores.&lt;/p&gt; &#xA;&lt;h2&gt;What is Mirador&lt;/h2&gt; &#xA;&lt;p&gt;There are many use-cases for highlighting points of interest on a skyline in AR: tourism apps, mountain ranges, city viewpoints, observation decks etc.. I receive messages quite often from people who used my first library, asking &#34;How do I do this? There are still some technical challenges in the way.&#34;&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s the problem: RealityKit, and AR in general, is location-agnostic. It tracks device motion in xyz, like a video game, but doesn’t hold the concept of GPS or real-world location.&lt;/p&gt; &#xA;&lt;p&gt;Adding to this challenge, the GPS and compass in phones is also infamously bad - everyone knows the experience of walking in the wrong direction down a busy street because your phone didn’t know where you were or which direction you were facing. Imagine trying to higlight points of interest in AR, while the location is off by 10m and the device heading is off by 90º.&lt;/p&gt; &#xA;&lt;p&gt;Mirador solves all of this. It uses a visual anchor to understand the device location, and then takes care of displaying AR elements in the right place.&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;Mirador uses a visual anchor, provided by the developer, to understand the user’s location. At most viewpoints, there are informational boards, or other landmarks that can be used as visual anchors. It builds on RealityKit’s existing ImageAnchor system, for recognising 2D images.&lt;/p&gt; &#xA;&lt;p&gt;The library then takes care of displaying points interest in the right place, also provided by the developer. There are also a few extra features thrown in to make it a great spatial experience.&lt;/p&gt; &#xA;&lt;h2&gt;Install using Swift Package Manager&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;File &amp;gt; Swift Packages &amp;gt; Add Package Dependency&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;a href=&#34;https://github.com/HyperARCo/Mirador.git&#34;&gt;https://github.com/HyperARCo/Mirador.git&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;There are three steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Setup Points of Interest (POIs) - you can do this in code, or with JSON&lt;/li&gt; &#xA; &lt;li&gt;Provide an anchor image&lt;/li&gt; &#xA; &lt;li&gt;Run MiradorView&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Setup POIs with code&lt;/h2&gt; &#xA;&lt;p&gt;You can setup a Mirador experience with some boilerplate code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import Mirador&#xA;&#xA;//Setup the anchor&#xA;let anchorLocation = Location(coordinate: Coordinate(latitude: 51.47787836, longitude: -0.00084588), altitude: 46)&#xA;let locationAnchor = LocationAnchor(name: &#34;greenwich&#34;, physicalWidth: 0.5, location: anchorLocation, bearing: Float(-30).degreesToRadians, orientation: .horizontal)&#xA;&#xA;//Setup a few points of interest&#xA;let canaryWharfCoordinate = Coordinate(latitude: 51.50493780, longitude: -0.01948017)&#xA;let canaryWharfLocation = Location(coordinate: canaryWharfCoordinate, altitude: 50)&#xA;let canaryWharfPOI = PointOfInterest(name: &#34;Canary Wharf&#34;, location: canaryWharfLocation)&#xA;locationAnchor.pointsOfInterest.append(canaryWharfPOI)&#xA;&#xA;let o2Coordinate = Coordinate(latitude: 51.50296112, longitude: 0.00321850)&#xA;let o2Location = Location(coordinate: o2Coordinate, altitude: 50)&#xA;let o2POI = PointOfInterest(name: &#34;O2 Arena&#34;, location: o2Location)&#xA;locationAnchor.pointsOfInterest.append(o2POI)&#xA;&#xA;let miradorView = MiradorView(locationAnchor: locationAnchor)&#xA;miradorView.run()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setup POIs with JSON&lt;/h2&gt; &#xA;&lt;p&gt;Once you have more than a few points of interest, writing all of this out can be combersome. Mirador also supports setting up the whole experience from a JSON file. Here’s the JSON file you’ll need for the same experience:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;    &#34;anchor&#34;: {&#xA;        &#34;name&#34;: &#34;greenwich&#34;,&#xA;        &#34;physical_width&#34;: 0.5,&#xA;        &#34;coordinate&#34;: [-0.00084588, 51.47787836], //[long, lat]&#xA;        &#34;altitude&#34;: 46.0,&#xA;        &#34;bearing_degrees&#34;: -30,&#xA;        &#34;orientation&#34;: &#34;vertical&#34; //&#34;vertical&#34; or &#34;horizontal&#34;&#xA;    },&#xA;    &#34;points_of_interest&#34;: [&#xA;        {&#xA;            &#34;name&#34;: &#34;Canary Wharf&#34;,&#xA;            &#34;coordinate&#34;: [-0.01948017, 51.50493780],&#xA;            &#34;altitude&#34;: 235&#xA;        },&#xA;        {&#xA;            &#34;name&#34;: &#34;O2 Arena&#34;,&#xA;            &#34;coordinate&#34;: [0.00321850, 51.50296112],&#xA;            &#34;altitude&#34;: 52&#xA;        }&#xA;    },&#xA;    &#34;version&#34;: &#34;1.0&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use this JSON like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;if let path = Bundle.main.path(forResource: “greenwich”, ofType: “.json”) {&#xA;    let anchor = LocationAnchor.anchorFromFile(atPath: path)&#xA;&#xA;    let miradorView = MiradorView(locationAnchor: locationAnchor)&#xA;    miradorView.run()&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Provide an anchor image&lt;/h2&gt; &#xA;&lt;p&gt;Add a clear image of the anchor to your Assets, with the same name specified in the LocationAnchor.name.&lt;/p&gt; &#xA;&lt;p&gt;Try and take the image face-on, without any shadows or harsh reflections. Perspective transforming the image to be perfect doesn&#39;t work - in my testing, RealityKit doesn&#39;t recognise images that have been skewed. An image &lt;em&gt;designed to be an anchor&lt;/em&gt; (such as a QR code) could work best, and could also be seen as a good marketing opportunity, e.g. an image which promotes the app, and is also the anchor.&lt;/p&gt; &#xA;&lt;h2&gt;Final setup steps&lt;/h2&gt; &#xA;&lt;p&gt;If you’re using SwiftUI, you can instantiate in mostly the same way, but using &lt;code&gt;MiradorViewContainer(locationAnchor: locationAnchor)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Call &lt;code&gt;miradorView.run()&lt;/code&gt; when the experience is launched or brought back to the foreground, and &lt;code&gt;miradorView.pause()&lt;/code&gt; when the app is in the background.&lt;/p&gt; &#xA;&lt;p&gt;Finally, add &lt;code&gt;NSCameraUsageDescription&lt;/code&gt; to your info.plist, with a description for accessing the camera.&lt;/p&gt; &#xA;&lt;p&gt;Make sure to run on your device - RealityKit requires the camera and doesn’t work in the Simulator.&lt;/p&gt; &#xA;&lt;h2&gt;Custom elements&lt;/h2&gt; &#xA;&lt;p&gt;All of the Points of Interest come with a standard out-of-the-box appearance, but you might want to customise this.&lt;/p&gt; &#xA;&lt;h3&gt;Displaying an image&lt;/h3&gt; &#xA;&lt;p&gt;You can display your own image within the AR space by using &lt;code&gt;addPOIImage()&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;let cityCoordinate = Coordinate(latitude:51.51438463, longitude: -0.08024839)&#xA;let cityLocation = Location(coordinate: cityCoordinate, altitude: 200)&#xA;let image = UIImage(named: &#34;skyline&#34;)!&#xA;&#xA;miradorView.addPOIImage(location: cityLocation, image: image)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This supports transparency, so if your image is a PNG that&#39;s fine. The AR entity displayed will have ScreenScale and FaceCamera properties:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ScreenScale: The entity scales to always appear at the size it was designed. If your image is 300x300, it&#39;ll appear at 300x300 points on-screen.&lt;/li&gt; &#xA; &lt;li&gt;FaceCamera: The entity always faces the camera.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Typically, RealityKit content can appear dimmed, but we&#39;ve developed a number of advanced rendering techniques, so the images support transparency, drop-shadows and they render with the correct colouring. Dig into &lt;code&gt;setupRealityKitPlane()&lt;/code&gt; within the source code if you&#39;re interested to see how that works.&lt;/p&gt; &#xA;&lt;p&gt;(Massive thanks to &lt;a href=&#34;https://twitter.com/maxxfrazer&#34;&gt;MaxxFrazer&lt;/a&gt; for most of this. Yes, I borrowed the number 1 RealityKit developer to help me build this.)&lt;/p&gt; &#xA;&lt;p&gt;If you want to display your own UI, you can use the SwiftUI &lt;code&gt;ImageRenderer&lt;/code&gt; API to convert this into an image. Check out the &lt;code&gt;addPOILabel()&lt;/code&gt; function within Mirador for some sample code.&lt;/p&gt; &#xA;&lt;p&gt;You can also add a RealityKit &lt;code&gt;Entity&lt;/code&gt; with the same ScreenScale and FaceCamera, functionality:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;addPOIEntity(location: Location, entity: Entity)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can add a RealityKit Entity without ScreenScale or FaceCamera, but still anchored to a location:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;addLocationEntity(location: Location, entity: Entity)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing the app from home&lt;/h2&gt; &#xA;&lt;p&gt;These experiences are designed to be used at iconic viewpoints, but it’s not practical to be on-site all day while developing your app. Here are some suggestions for improving the development process:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can run the experience from home by loading the anchor image on your computer screen, then scanning it with your app. This will let you quickly test if your AR elements are displaying properly, if there are enough/too many etc.&lt;/li&gt; &#xA; &lt;li&gt;You can record a RealityKit session, including video feed and AR motion data, using the Reality Composer app. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Create a new session, tap the three dots in the top right &amp;gt; Developer &amp;gt; Record AR Session.&lt;/li&gt; &#xA;   &lt;li&gt;Record a session, as though you can see the AR elements and are demoing the final app. Make sure to point towards the anchor near the start. You don’t need to enable the location recording in the bottom right, we don’t use that.&lt;/li&gt; &#xA;   &lt;li&gt;Tap share, Save to Files. The file will be really big, over 100MB. AirDrop this file once you’re back at your computer. In my experience, if you save the video to Camera Roll, it saves at the same quality but removes the embedded data, and if you try to send it on WhatsApp or another app it compresses it to be much smaller. So make sure to save to Files, or send directly via AirDrop to stop the file from being compressed.&lt;/li&gt; &#xA;   &lt;li&gt;In Xcode, with your phone plugged in and selected, go to Edit Scheme… Run &amp;gt; Options &amp;gt; ARKit, and enable Replay data, then select the file. Note: This option won&#39;t appear unless you have your phone selected.&lt;/li&gt; &#xA;   &lt;li&gt;When you run the app, it’ll playback the video session, and should recognise your anchor image.&lt;/li&gt; &#xA;   &lt;li&gt;Note: In my experience, it seems that sessions recoded on LiDAR devices can only be played back on other LiDAR devices, and the same for non-LiDAR devices.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Sample project&lt;/h2&gt; &#xA;&lt;p&gt;The sample project provides a straightforward implementation which works at &lt;em&gt;Prime Meridian, Greenwich&lt;/em&gt; (the center of the world!). It demonstrates two ways you can use the library - from code, with some manual elements, or from JSON.&lt;/p&gt; &#xA;&lt;p&gt;Note: In both cases, the &lt;code&gt;greenwich&lt;/code&gt; anchor in the sample project is set to horizontal. You can change this to &lt;code&gt;vertical&lt;/code&gt; if you’re displaying it on your screen. If you’re in Greenwich and trying it in-person, it’ll be horizontal on the ground.&lt;/p&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Each frame where the anchor is visible, RealityKit updates its position. Sometimes the image can only be seen on the edge of a frame, so RealityKit&#39;s update to its position is not as precise, but there&#39;s no way to know which are &lt;em&gt;good&lt;/em&gt; anchor updates. And you can imagine the anchored content can jitter around every time it updates the position of the anchor. So I&#39;ve implemented a Kalman filter on these position updates, which basically averages out the data and should arrive at the correct heading.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Next steps&lt;/h2&gt; &#xA;&lt;p&gt;This is v1. There are endless opportunities to take Mirador further. Feel free to fork, and submit PRs with improvements and new features.&lt;/p&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;p&gt;Mirador was built by &lt;a href=&#34;https://twitter.com/AndrewHartAR&#34;&gt;Andrew Hart&lt;/a&gt;. A common follow-up question I get is &#34;this is great - can this be integrated with retail stores?&#34; That&#39;s the purpose of my startup, &lt;a href=&#34;https://HyperAR.com&#34;&gt;Hyper&lt;/a&gt;. Mirador is free, open-source for Point of Interest applications. Hyper is our full indoor spatial platform for retail (indoor maps, precise location, AR navigation etc.)&lt;/p&gt; &#xA;&lt;p&gt;Massive thanks to &lt;a href=&#34;https://twitter.com/maxxfrazer&#34;&gt;MaxxFrazer&lt;/a&gt;, the king of RealityKit, who helped with many of the rendering issues.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Mirador is available under MIT license.&lt;/p&gt;</summary>
  </entry>
</feed>