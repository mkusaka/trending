<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-11T01:38:23Z</updated>
  <subtitle>Daily Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mortenjust/maple-diffusion</title>
    <updated>2022-11-11T01:38:23Z</updated>
    <id>tag:github.com,2022-11-11:/mortenjust/maple-diffusion</id>
    <link href="https://github.com/mortenjust/maple-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion inference on iOS / macOS using MPSGraph&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üçÅ Maple Diffusion Swift Package&lt;/h1&gt; &#xA;&lt;p&gt;Maple Diffusion runs Stable Diffusion models &lt;strong&gt;locally&lt;/strong&gt; on macOS / iOS devices, in Swift, using the MPSGraph framework (not Python).&lt;/p&gt; &#xA;&lt;p&gt;This is the Swift Package Manager wrapper of &lt;a href=&#34;https://github.com/madebyollin/maple-diffusion&#34;&gt;Maple Diffusion&lt;/a&gt;. It also adds a few Combine publishers and async/await versions of some functions and supports downloading weights from any local or remote URL, including the app bundle itself.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h2&gt;One-line diffusion&lt;/h2&gt; &#xA;&lt;p&gt;In its simplest form it&#39;s as simple as one line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let image = try? await Diffusion.generate(localOrRemote: modelUrl, prompt: &#34;cat astronaut&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can give it a local or remote URL or both. If remote, the downloaded weights are saved for later.&lt;/p&gt; &#xA;&lt;p&gt;The single line version is currently limited in terms of parameters.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;examples/SingleLineDiffusion&lt;/code&gt; for a working example.&lt;/p&gt; &#xA;&lt;h2&gt;As an observable object&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s add some UI. Here&#39;s an entire working image generator app in a single SwiftUI view:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/mortenjust/maple-diffusion/raw/main/Examples/Demos/simple-diffusion.gif&#34; alt=&#34;GIF demo&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;struct ContentView: View {&#xA;    &#xA;    // 1&#xA;    @StateObject var sd = Diffusion()&#xA;    @State var prompt = &#34;&#34;&#xA;    @State var image : CGImage?&#xA;    @State var imagePublisher = Diffusion.placeholderPublisher&#xA;    @State var progress : Double = 0&#xA;    &#xA;    var anyProgress : Double { sd.loadingProgress &amp;lt; 1 ? sd.loadingProgress : progress }&#xA;&#xA;    var body: some View {&#xA;        VStack {&#xA;            &#xA;            DiffusionImage(image: $image, progress: $progress)&#xA;            Spacer()&#xA;            TextField(&#34;Prompt&#34;, text: $prompt)&#xA;            // 3&#xA;                .onSubmit { self.imagePublisher = sd.generate(prompt: prompt) }&#xA;                .disabled(!sd.isModelReady)&#xA;            ProgressView(value: anyProgress)&#xA;                .opacity(anyProgress == 1 || anyProgress == 0 ? 0 : 1)&#xA;        }&#xA;        .task {&#xA;            // 2&#xA;            let path = URL(string: &#34;http://localhost:8080/Diffusion.zip&#34;)!&#xA;            try! await sd.prepModels(remoteURL: path)&#xA;        }&#xA;        &#xA;        // 4&#xA;        .onReceive(imagePublisher) { r in&#xA;            self.image = r.image&#xA;            self.progress = r.progress&#xA;        }&#xA;        .frame(minWidth: 200, minHeight: 200)&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s what it does&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Instantiate a &lt;code&gt;Diffusion&lt;/code&gt; object&lt;/li&gt; &#xA; &lt;li&gt;Prepare the models, download if needed&lt;/li&gt; &#xA; &lt;li&gt;Submit a prompt for generation&lt;/li&gt; &#xA; &lt;li&gt;Receive updates during generation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See &lt;code&gt;examples/SimpleDiffusion&lt;/code&gt; for a working example.&lt;/p&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;p&gt;Add &lt;code&gt;https://github.com/mortenjust/maple-diffusion&lt;/code&gt; in the &lt;a href=&#34;https://developer.apple.com/documentation/xcode/adding_package_dependencies_to_your_app&#34;&gt;&#34;Swift Package Manager&#34; tab in Xcode&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Preparing the weights&lt;/h1&gt; &#xA;&lt;p&gt;Maple Diffusion splits the weights into a binary format that is different from the typical CKPT format. It uses many small files which it then (optionally) swaps in and out of memory, enabling it to run on both macOS and iOS. You can use the converter script in the package to convert your own CKPT file.&lt;/p&gt; &#xA;&lt;h2&gt;Option 1: Pre-converted Standard Stable Diffusion v1.4&lt;/h2&gt; &#xA;&lt;p&gt;By downloading this zip file, you accept the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/raw/main/LICENSE&#34;&gt;creative license from StabilityAI&lt;/a&gt;. &lt;a href=&#34;https://drive.google.com/file/d/1fGPc7-1upu-b68jstdT1vF7uWICc6Vk8/view?usp=sharing&#34;&gt;Download ZIP&lt;/a&gt;. Please don&#39;t use this URL in your software.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll get back to what to do with it in a second.&lt;/p&gt; &#xA;&lt;h2&gt;Option 2: Preparing your own &lt;code&gt;ckpt&lt;/code&gt; file&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;If you want to use your own CKPT file (like a Dreambooth fine-tuning), you can convert it into Maple Diffusion format&lt;summary&gt; &#xA;   &lt;ol&gt; &#xA;    &lt;li&gt; &lt;p&gt;Download a Stable Diffusion model checkpoint to a folder, e.g. &lt;code&gt;~/Downloads/sd&lt;/code&gt; (&lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;&lt;code&gt;sd-v1-4.ckpt&lt;/code&gt;&lt;/a&gt;, or some derivation thereof)&lt;/p&gt; &lt;/li&gt; &#xA;    &lt;li&gt; &lt;p&gt;Setup &amp;amp; install Python with PyTorch, if you haven&#39;t already.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;pre&gt;&lt;code&gt;# Grab the converter script&#xA;cd ~/Downloads/sd&#xA;curl https://raw.githubusercontent.com/mortenjust/maple-diffusion/main/Converter%20Script/maple-convert.py &amp;gt; maple-convert.py&#xA;&#xA;# may need to install conda first https://github.com/conda-forge/miniforge#homebrew&#xA;conda deactivate&#xA;conda remove -n maple-diffusion --all&#xA;conda create -n maple-diffusion python=3.10&#xA;conda activate maple-diffusion&#xA;pip install torch typing_extensions numpy Pillow requests pytorch_lightning&#xA;./maple-convert.py ~/Downloads/sd-v1-4.ckpt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The script will create a new folder called &lt;code&gt;bins&lt;/code&gt;. We&#39;ll get back to what to do with it in a second.&lt;/p&gt; &lt;/summary&gt;&lt;/summary&gt;&#xA;&lt;/details&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;Can I use a Dreambooth model?&lt;/h2&gt; &#xA;&lt;p&gt;Yes. Just copy the &lt;code&gt;alpha*&lt;/code&gt; files from the standard conversion. This repo will include these files in the future. See &lt;a href=&#34;https://github.com/madebyollin/maple-diffusion/issues/22&#34;&gt;this issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Can I contribute? What&#39;s next?&lt;/h2&gt; &#xA;&lt;p&gt;Yes! Some ideas&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Tighten up code quality overall. Most is proof of concept.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add image-to-image&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add in-painting and out-painting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re making changes to the MPSGraph part of the codebase, consider making your contributions to the single-file repo and then integrate the changes in the wrapped file in this repo.&lt;/p&gt; &#xA;&lt;h2&gt;How fast is it?&lt;/h2&gt; &#xA;&lt;p&gt;On my MacBook Pro M1 Max, I get ~0.3s/step, which is significantly faster than any Python/PyTorch/Tensorflow installation I&#39;ve tried.&lt;/p&gt; &#xA;&lt;p&gt;On an iPhone it should take a minute or two.&lt;/p&gt; &#xA;&lt;p&gt;To attain usable performance without tripping over iOS&#39;s 4GB memory limit, Maple Diffusion relies internally on FP16 (NHWC) tensors, operator fusion from MPSGraph, and a truly pitiable degree of swapping models to device storage.&lt;/p&gt;</summary>
  </entry>
</feed>