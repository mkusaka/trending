<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-22T01:34:19Z</updated>
  <subtitle>Daily Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>intsig171/SmartCodable</title>
    <updated>2024-05-22T01:34:19Z</updated>
    <id>tag:github.com,2024-05-22:/intsig171/SmartCodable</id>
    <link href="https://github.com/intsig171/SmartCodable" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SmartCodable is a data parsing library based on Codable. It is simple to use, with robust compatibility being one of its main features. SmartCodable 是基于Codable实现的数据解析库。简单易用，强悍的兼容性是SmartCodable的主要特点。 表层API和功能几乎和HandyJSON一致，支持快速的迁移。&lt;/p&gt;&lt;hr&gt;&lt;p&gt;✨✨✨看起来还不错？给个star✨吧，急需支持✨✨✨&lt;/p&gt; &#xA;&lt;h1&gt;English ReadMe&lt;/h1&gt; &#xA;&lt;p&gt;🌐 If you need,please visit &amp;gt;&amp;gt;&amp;gt; &lt;a href=&#34;https://github.com/intsig171/SmartCodable/raw/main/README-EN.md&#34;&gt;English README&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;SmartCodable - Swift数据解析&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;SmartCodable&lt;/strong&gt; 是一个基于Swift的&lt;strong&gt;Codable&lt;/strong&gt;协议的数据解析库，旨在提供更为强大和灵活的解析能力。通过优化和重写&lt;strong&gt;Codable&lt;/strong&gt;的标准功能，&lt;strong&gt;SmartCodable&lt;/strong&gt; 有效地解决了传统解析过程中的常见问题，并提高了解析的容错性和灵活性。&lt;/p&gt; &#xA;&lt;h2&gt;HandyJSON vs Codable&lt;/h2&gt; &#xA;&lt;p&gt;【✅： 完美支持】【⚠️： 带缺陷的支持】【❌： 不支持】&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;序号&lt;/th&gt; &#xA;   &lt;th&gt;🎯 特性&lt;/th&gt; &#xA;   &lt;th&gt;💬 特性说明 💬&lt;/th&gt; &#xA;   &lt;th&gt;SmartCodable&lt;/th&gt; &#xA;   &lt;th&gt;HandyJSON&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;强大的兼容性&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;完美兼容：&lt;strong&gt;字段缺失&lt;/strong&gt; &amp;amp; &lt;strong&gt;字段值为nul&lt;/strong&gt; &amp;amp; &lt;strong&gt;字段类型错误&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;类型自适应&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;如JSON中是一个Int，但对应Model是String字段，会自动完成转化&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;解析Any&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;支持解析 &lt;strong&gt;[Any], [String: Any]&lt;/strong&gt; 等类型&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;解码回调&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;支持Model解码完成的回调，即：&lt;strong&gt;didFinishingMapping&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;属性初始化值填充&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;当解析失败时，支持使用初始的Model属性的赋值。&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;字符串的Model化&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;字符串是json字符串，支持进行Model化解析&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;枚举的解析&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;当枚举解析失败时，支持兼容。&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;属性的自定义解析&lt;/strong&gt; - 重命名&lt;/td&gt; &#xA;   &lt;td&gt;自定义解码key（对解码的Model属性重命名）&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;属性的自定义解析&lt;/strong&gt; - 忽略&lt;/td&gt; &#xA;   &lt;td&gt;忽略某个Model属性的解码&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;支持designatedPath&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;实现自定义解析路径&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Model的继承&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;在model的继承关系下，Codable的支持力度较弱，使用不便（可以支持）&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;自定义解析路径&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;指定从json的层级开始解析&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;超复杂的数据解码&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;解码过程中，多数据做进一步的整合/处理。如： 数据的扁平化处理&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;⚠️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;解码性能&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;在解码性能上，SmartCodable 平均强 30%&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;⚠️&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;异常解码日志&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;当解码异常进行了兼容处理时，提供排查日志&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;安全性方面&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;底层实现的稳定性和安全性。&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;17&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;支持designatedPath&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;实现自定义解析路径&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;整体来讲： SmartCodable 和 HandyJSON 相比，在功能和使用上相近。&lt;/p&gt; &#xA;&lt;h4&gt;安全性 &amp;amp; 稳定性&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HandyJSON&lt;/strong&gt; 使用Swift的反射特性来实现数据的序列化和反序列化。&lt;strong&gt;该机制是非法的，不安全的&lt;/strong&gt;， 更多的细节请访问 &lt;strong&gt;&lt;a href=&#34;https://github.com/alibaba/HandyJSON/issues/466&#34;&gt;HandyJSON 的466号issue&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Codable&lt;/strong&gt; 是Swift标准库的一部分，提供了一种声明式的方式来进行序列化和反序列化，它更为通用。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;建议 &amp;amp; 回答&lt;/h2&gt; &#xA;&lt;p&gt;有不少使用者提出了优化需求 或 新功能的要求。在这边逐一回复：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;💡 建议列表&lt;/th&gt; &#xA;   &lt;th&gt;是否采纳&lt;/th&gt; &#xA;   &lt;th&gt;理由&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;① &lt;strong&gt;#suggest 1 在mapping方法中支持解析忽略&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/intsig171/SmartCodable/raw/main/Document/Suggest/suggest1.md&#34;&gt;不采纳的理由&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;② &lt;strong&gt;#suggest 2 像HandyJSON一样支持继承关系的解析&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/intsig171/SmartCodable/raw/main/Document/Suggest/suggest2.md&#34;&gt;不采纳的理由&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;③ &lt;strong&gt;#suggest 3 支持初始值填充&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/intsig171/SmartCodable/raw/main/Document/Suggest/suggest3.md&#34;&gt;实现逻辑&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;④ &lt;strong&gt;#suggest 4 提供HandyJSON的替换指导&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/intsig171/SmartCodable/raw/main/Document/Suggest/suggest4.md&#34;&gt;替换指导&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;⑤ &lt;strong&gt;#suggest 5 提供全局的Key映射策略&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/intsig171/SmartCodable/raw/main/Document/Suggest/suggest5.md&#34;&gt;实现逻辑&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;⑥ &lt;strong&gt;#suggest 6 支持UIColor的解析&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/intsig171/SmartCodable/raw/main/Document/Suggest/suggest6.md&#34;&gt;实现逻辑&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;⑦ &lt;strong&gt;#suggest 7 增加单个Value的自定义转换策略&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/intsig171/SmartCodable/raw/main/Document/Suggest/suggest7.md&#34;&gt;实现逻辑&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;⑧ &lt;strong&gt;#suggest 8 支持designatedPath的自定义路径解析&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;参考HandyJSON实现&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;使用SmartCodable 平替 HandyJSON&lt;/h2&gt; &#xA;&lt;p&gt;更多内容请查看： &lt;a href=&#34;https://github.com/intsig171/SmartCodable/raw/main/Document/%E5%BB%BA%E8%AE%AE/%23suggest%204%20%E4%BD%BF%E7%94%A8SmartCodable%20%E5%B9%B3%E6%9B%BF%20HandyJSON.md&#34;&gt;替换指导&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;内容项&lt;/th&gt; &#xA;   &lt;th&gt;内容项说明&lt;/th&gt; &#xA;   &lt;th&gt;使用场景&lt;/th&gt; &#xA;   &lt;th&gt;替换难度&lt;/th&gt; &#xA;   &lt;th&gt;评判理由&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;①声明Model&lt;/td&gt; &#xA;   &lt;td&gt;声明Model&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;★☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;全局将 HandyJSON 替换为 SmartCodable即可。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;②反序列化&lt;/td&gt; &#xA;   &lt;td&gt;数据的模型化（数据转Model）&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;☆☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;完全一样的调用方式，无需处理。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;③序列化&lt;/td&gt; &#xA;   &lt;td&gt;模型的数据化（Model转数据）&lt;/td&gt; &#xA;   &lt;td&gt;★☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;★☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;将 &lt;code&gt;toJSON()&lt;/code&gt; 替换为 &lt;code&gt;toDictionary()&lt;/code&gt; 或 &lt;code&gt;toArray()&lt;/code&gt;。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;④解码完成的回调&lt;/td&gt; &#xA;   &lt;td&gt;解析完成进一步处理数据&lt;/td&gt; &#xA;   &lt;td&gt;★★☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;☆☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;完全一样的调用方式，无需处理。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;⑤自定义解析Key&lt;/td&gt; &#xA;   &lt;td&gt;忽略key的解析 &amp;amp; 自定义Key的映射关系&lt;/td&gt; &#xA;   &lt;td&gt;★★★☆☆&lt;/td&gt; &#xA;   &lt;td&gt;★★★☆☆&lt;/td&gt; &#xA;   &lt;td&gt;需要更改调用方式。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;⑥解析Any&lt;/td&gt; &#xA;   &lt;td&gt;解析Any类型的数据。Any，[String: Any]， [Any]&lt;/td&gt; &#xA;   &lt;td&gt;★☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;★☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;将Any替换为SmartAny&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;⑦处理继承关系&lt;/td&gt; &#xA;   &lt;td&gt;解析存在的继承关系的Model&lt;/td&gt; &#xA;   &lt;td&gt;★☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;★★★★★&lt;/td&gt; &#xA;   &lt;td&gt;建议使用协议实现。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;⑧枚举的解析&lt;/td&gt; &#xA;   &lt;td&gt;解析枚举属性&lt;/td&gt; &#xA;   &lt;td&gt;★☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;★☆☆☆☆&lt;/td&gt; &#xA;   &lt;td&gt;多实现一个 defaultCase&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;集成 SmartCodable&lt;/h2&gt; &#xA;&lt;h3&gt;cocopods 集成&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;source &#39;https://github.com/CocoaPods/Specs.git&#39;&#xA;platform :ios, &#39;11.0&#39;&#xA;use_frameworks!&#xA;&#xA;target &#39;MyApp&#39; do&#xA;  pod &#39;SmartCodable&#39;&#xA;end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SPM 集成&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/intsig171/SmartCodable.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;SmartCodable 使用介绍&lt;/h2&gt; &#xA;&lt;h3&gt;1. 字典的解码&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;import SmartCodable&#xA;&#xA;struct Model: SmartCodable {&#xA;    var name: String = &#34;&#34;&#xA;}&#xA;&#xA;let dict: [String: String] = [&#34;name&#34;: &#34;xiaoming&#34;]&#xA;guard let model = Model.deserialize(from: dict) else { return }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. 数组的解码&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;import SmartCodable&#xA;&#xA;struct Model: SmartCodable {&#xA;    var name: String = &#34;&#34;&#xA;}&#xA;&#xA;let dict: [String: String] = [&#34;name&#34;: &#34;xiaoming&#34;]&#xA;let arr = [dict, dict]&#xA;guard let models = [Model].deserialize(from: arr) else { return }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. 序列化与反序列化&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;// 字典转模型&#xA;guard let xiaoMing = JsonToModel.deserialize(from: dict) else { return }&#xA;&#xA;// 模型转字典&#xA;let studentDict = xiaoMing.toDictionary() ?? [:]&#xA;&#xA;// 模型转json字符串&#xA;let json1 = xiaoMing.toJSONString(prettyPrint: true) ?? &#34;&#34;&#xA;&#xA;// json字符串转模型&#xA;guard let xiaoMing2 = JsonToModel.deserialize(from: json1) else { return }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. 解码Any&lt;/h3&gt; &#xA;&lt;p&gt;Codable是无法解码Any类型的，这样就意味着模型的属性类型不可以是 &lt;strong&gt;Any&lt;/strong&gt;，&lt;strong&gt;[Any]&lt;/strong&gt;，&lt;strong&gt;[String: Any]&lt;strong&gt;等类型， 这对解码造成了一定的困扰。&lt;strong&gt;SmartAny&lt;/strong&gt; 是&lt;/strong&gt;SmartCodable&lt;/strong&gt; 提供的解决Any的方案。可以直接像使用 &lt;strong&gt;Any&lt;/strong&gt; 一样使用它。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct AnyModel: SmartCodable {&#xA;    var name: SmartAny?&#xA;    var dict: [String: SmartAny] = [:]&#xA;    var arr: [SmartAny] = []&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;let dict = [&#xA;    &#34;name&#34;: &#34;xiao ming&#34;,&#xA;    &#34;age&#34;: 20,&#xA;    &#34;dict&#34;: inDict,&#xA;    &#34;arr&#34;: arr&#xA;] as [String : Any]&#xA;&#xA;guard let model = AnyModel.deserialize(from: dict) else { return }&#xA;print(model.name.peel )&#xA;print(model.dict.peel)&#xA;print(model.arr.peel)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;真实的数据被 &lt;strong&gt;SmartAny&lt;/strong&gt; 包裹住了，需要使用 &lt;strong&gt;peel&lt;/strong&gt; 对数据解包。&lt;/p&gt; &#xA;&lt;h4&gt;编码成SmartAny&lt;/h4&gt; &#xA;&lt;p&gt;同时也提供了反向转换的方法：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;From&lt;/th&gt; &#xA;   &lt;th&gt;To&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;Any&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;SmartAny&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;SmartAny(from: &#34;some&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;[String: Any] &lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[String: SmartAny]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;key2&#34;: &#34;value2&#34;].cover&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;[Any]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[SmartAny]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[ [&#34;key3&#34;: &#34;value3&#34;] ].cover&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;5. 模型化解析json字符串&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;let dict: [String: Any] = [&#xA;    &#34;hobby&#34;: &#34;{\&#34;name\&#34;:\&#34;sleep\&#34;}&#34;,&#xA;]&#xA;guard let model = Model.deserialize(from: dict) else { return }&#xA;print(model)&#xA;&#xA;struct Model: SmartCodable {&#xA;    var hobby: Hobby?&#xA;}&#xA;&#xA;struct Hobby: SmartCodable {&#xA;    var name: String = &#34;&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;6. 支持解析UIColor&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;let dict = [&#xA;    &#34;color&#34;: &#34;7DA5E3&#34;&#xA;]&#xA;&#xA;struct Model: SmartCodable {&#xA;    var color: SmartColor?&#xA;}&#xA;&#xA;guard let model = Model.deserialize(from: dict) else { return }&#xA;print(model.color?.peel)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;UIColor&lt;/strong&gt; 是 &lt;code&gt;non-final class&lt;/code&gt;。非最终类不能简单地实现&lt;code&gt;Codable&lt;/code&gt;的&lt;code&gt;init(from:)&lt;/code&gt;。具体可查阅 &lt;strong&gt;suggest 6&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;7. 枚举的解码&lt;/h3&gt; &#xA;&lt;p&gt;让枚举遵循 &lt;strong&gt;SmartCaseDefaultable&lt;/strong&gt; ，当解码失败时使用 &lt;strong&gt;defaultCase&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct CompatibleEnum: SmartCodable {&#xA;&#xA;    var enumTest: TestEnum?&#xA;&#xA;    enum TestEnum: String, SmartCaseDefaultable {&#xA;        case a&#xA;        case b&#xA;        case c = &#34;hello&#34;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;支持关联值枚举的解码&lt;/h4&gt; &#xA;&lt;p&gt;完全交给你自定义解析规则。 如果不自定义，不进行解析。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;/// 关联值枚举的解析， 需要自己接管decode&#xA;enum Sex: SmartAssociatedEnumerable {&#xA;    static var defaultCase: Sex = .women&#xA;    &#xA;    case man&#xA;    case women&#xA;    case other(String)&#xA;}&#xA;struct CompatibleEnum: SmartCodable {&#xA;    var sex: Sex = .man&#xA;    static func mappingForValue() -&amp;gt; [SmartValueTransformer]? {&#xA;        [&#xA;            CodingKeys.sex &amp;lt;--- RelationEnumTranformer()&#xA;        ]&#xA;    }&#xA;}&#xA;&#xA;struct RelationEnumTranformer: ValueTransformable {&#xA;    func transformToJSON(_ value: Introduce_8ViewController.Sex?) -&amp;gt; String? {&#xA;        guard let value = value else { return nil }&#xA;        &#xA;        switch value {&#xA;        case .man:&#xA;            return &#34;man&#34;&#xA;        case .women:&#xA;            return &#34;women&#34;&#xA;        case .other(let v):&#xA;            return v&#xA;        }&#xA;    }&#xA;&#xA;    typealias Object = Sex&#xA;    typealias JSON = String&#xA;    &#xA;    func transformFromJSON(_ value: Any?) -&amp;gt; Sex? {&#xA;        guard let temp = value as? String else { return .man }&#xA;&#xA;        switch temp {&#xA;        case &#34;man&#34;:&#xA;            return .man&#xA;        case &#34;women&#34;:&#xA;            return .women&#xA;        default:&#xA;            return .other(temp)&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;SmartCodable的解码策略&lt;/h2&gt; &#xA;&lt;p&gt;解码策略分为三个阶段的操作：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;解码前 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;忽略某些key的解析&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;解码中 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Key的映射策略&lt;/li&gt; &#xA;   &lt;li&gt;Value的解析策略&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;解码后 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;解码完成后的回调&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;[解码前] 忽略某些key的解析&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct Model: SmartCodable {&#xA;    var name: String = &#34;&#34;&#xA;    var ignore: String = &#34;&#34;&#xA;    var age: Int = 0&#xA;    &#xA;    enum CodingKeys: String, CodingKey {&#xA;        case name&#xA;        case age = &#34;selfAge&#34;&#xA;        // 忽略ignore的解析。&#xA;//            case ignore&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;重写当前的 &lt;strong&gt;CodingKeys&lt;/strong&gt;，不需要解析谁，就删掉谁。留下的是需要解析的。&lt;/p&gt; &#xA;&lt;p&gt;当然，也可以在这里进行key的重命名。&lt;/p&gt; &#xA;&lt;h4&gt;@IgnoredKey&lt;/h4&gt; &#xA;&lt;p&gt;也可以选择使用 &lt;strong&gt;@IgnoredKey&lt;/strong&gt; 属性包装器忽略某个Key的解析。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct Home: SmartCodable {&#xA;    var name: String = &#34;&#34;&#xA;    @IgnoredKey&#xA;    var age: [Any] = [&#34;1&#34;]&#xA;    @IgnoredKey&#xA;    var area: String = &#34;area&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;严格意义上来说： 要忽略解析只能通过重写 &lt;strong&gt;CodingKeys&lt;/strong&gt;。 作者通过**@IgnoredKey** 的标记打断了解析过程（还是会进入解析 ），返回了该属性的初始化值。&lt;/p&gt; &#xA;&lt;h3&gt;[解码中] Key的映射策略&lt;/h3&gt; &#xA;&lt;h4&gt;全局的Key映射策略&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;public enum SmartKeyDecodingStrategy : Sendable {&#xA;    case useDefaultKeys&#xA;    &#xA;    // 蛇形命名转驼峰命名&#xA;    case fromSnakeCase&#xA;    &#xA;    // 首字母大写转小写&#xA;    case firstLetterLower&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;作用于本次解析，本次解析只能使用一种策略，不可混合使用。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;let option1: SmartDecodingOption = .key(.fromSnakeCase)&#xA;guard let model1 = TwoModel.deserialize(from: dict1, options: [option1]) else { return }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;局部的Key映射策略&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持自定义路径解析。&lt;/li&gt; &#xA; &lt;li&gt;支持字段解析的重命名。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;自定义解析路径&lt;/h5&gt; &#xA;&lt;p&gt;跨层解析。将sub里面的name字段，解析到 Model的name属性上。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;let dict = [&#xA;    &#34;age&#34;: 10,&#xA;    &#34;sub&#34;: [&#xA;        &#34;name&#34;: &#34;Mccc&#34;&#xA;    ]&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct Model: SmartCodable {&#xA;    var age: Int = 0&#xA;    var name: String = &#34;&#34;&#xA;    static func mappingForKey() -&amp;gt; [SmartKeyTransformer]? {&#xA;        [ CodingKeys.name &amp;lt;--- &#34;sub.name&#34; ]&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;重命名key&lt;/h5&gt; &#xA;&lt;p&gt;支持自定义映射关系，你需要实现一个可选的&lt;code&gt;mapping&lt;/code&gt;函数。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct Model: SmartCodable {&#xA;    var name: String = &#34;&#34;&#xA;    var age: Int = 0&#xA;    var ignoreKey: String?&#xA;    &#xA;    enum CodingKeys: CodingKey {&#xA;        case name&#xA;        case age&#xA;    }&#xA;    &#xA;    static func mappingForKey() -&amp;gt; [SmartKeyTransformer]? {&#xA;        [&#xA;            CodingKeys.name &amp;lt;--- [&#34;nickName&#34;, &#34;realName&#34;],&#xA;            CodingKeys.age &amp;lt;--- &#34;person_age&#34;&#xA;        ]&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;1对1&lt;/strong&gt; 的映射&lt;/p&gt; &lt;p&gt;你可以选择像 &lt;code&gt;CodingKeys.age &amp;lt;--- &#34;person_age&#34; &lt;/code&gt;，只处理&lt;strong&gt;1对1&lt;/strong&gt;的映射。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;1对多&lt;/strong&gt; 的映射&lt;/p&gt; &lt;p&gt;也可以像 &lt;code&gt;CodingKeys.name &amp;lt;--- [&#34;nickName&#34;, &#34;realName&#34;]&lt;/code&gt; 一样处理 &lt;strong&gt;1对多&lt;/strong&gt; 的映射。如果恰好都有值，将选择第一个。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;[解码中] Value的解析策略&lt;/h3&gt; &#xA;&lt;h4&gt;全局的值解析策略&lt;/h4&gt; &#xA;&lt;p&gt;SmartDecodingOption提供了三种解码选项，分别为：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;public enum SmartDecodingOption {&#xA;    &#xA;    /// 用于解码 “Date” 值的策略&#xA;    case dateStrategy(JSONDecoder.DateDecodingStrategy)&#xA;    &#xA;    /// 用于解码 “Data” 值的策略&#xA;    case dataStrategy(JSONDecoder.DataDecodingStrategy)&#xA;    &#xA;    /// 用于不符合json的浮点值(IEEE 754无穷大和NaN)的策略&#xA;    case floatStrategy(JSONDecoder.NonConformingFloatDecodingStrategy)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Date&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;let dateFormatter = DateFormatter()&#xA; dateFormatter.dateFormat = &#34;yyyy-MM-dd HH:mm:ss&#34;&#xA;let option: JSONDecoder.SmartDecodingOption = .dateStrategy(.formatted(dateFormatter))&#xA;guard let model = FeedOne.deserialize(from: json, options: [option]) else { return }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Data&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;let option: JSONDecoder.SmartDecodingOption = .dataStrategy(.base64)&#xA;guard let model = FeedOne.deserialize(from: json, options: [option]) else { return }&#xA;gurad let data = model.address, let url = String(data: data, encoding: .utf8) { else }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Float&lt;/h5&gt; &#xA;&lt;pre&gt;&lt;code&gt;let option: JSONDecoder.SmartDecodingOption = .floatStrategy(.convertFromString(positiveInfinity: &#34;infinity&#34;, negativeInfinity: &#34;-infinity&#34;, nan: &#34;NaN&#34;))&#xA;guard let model1 = FeedOne.deserialize(from: json, options: [option]) else {  return }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;局部的值解析策略&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;struct SmartModel: SmartCodable {&#xA;    var date1: Date?&#xA;    var date2: Date?&#xA;    var url: URL?&#xA;    var data: Data?&#xA;            &#xA;    // value的解析策略&#xA;    static func mappingForValue() -&amp;gt; [SmartValueTransformer]? {&#xA;        let format = DateFormatter()&#xA;        format.dateFormat = &#34;yyyy-MM-dd&#34;&#xA;        return [&#xA;            CodingKeys.url &amp;lt;--- SmartURLTransformer(prefix: &#34;https://&#34;),&#xA;            CodingKeys.date2 &amp;lt;--- SmartDateTransformer(),&#xA;            CodingKeys.date1 &amp;lt;--- SmartDateFormatTransformer(format),&#xA;            CodingKeys.data &amp;lt;--- SmartDataTransformer()&#xA;        ]&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;您可以实现 &lt;code&gt;mappingForValue&lt;/code&gt; 给每个属性设置不同的解析策略。&lt;/p&gt; &#xA;&lt;p&gt;支持的类型：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Date&lt;/li&gt; &#xA; &lt;li&gt;UIColor&lt;/li&gt; &#xA; &lt;li&gt;URL&lt;/li&gt; &#xA; &lt;li&gt;Data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;如需其他类型可以提 &lt;strong&gt;issue&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;h5&gt;自定义解析策略&lt;/h5&gt; &#xA;&lt;p&gt;遵守该协议，实现协议方法。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;public protocol ValueTransformable {&#xA;    associatedtype Object&#xA;    associatedtype JSON&#xA;    &#xA;    /// transform from ’json‘ to ’object‘&#xA;    func transformFromJSON(_ value: Any?) -&amp;gt; Object?&#xA;    &#xA;    /// transform to ‘json’ from ‘object’&#xA;    func transformToJSON(_ value: Object?) -&amp;gt; JSON?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;[解码后] 解析完成的回调&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;class Model: SmartDecodable {&#xA;&#xA;    var name: String = &#34;&#34;&#xA;    var age: Int = 0&#xA;    var desc: String = &#34;&#34;&#xA;    required init() { }&#xA;    &#xA;    // 解析完成的回调&#xA;    func didFinishMapping() {    &#xA;        if name.isEmpty {&#xA;            desc = &#34;\(age)岁的&#34; + &#34;人&#34;&#xA;        } else {&#xA;            desc = &#34;\(age)岁的&#34; + name&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;调试日志&lt;/h2&gt; &#xA;&lt;p&gt;出现 &lt;strong&gt;SmartLog Error&lt;/strong&gt; 日志代表着 &lt;strong&gt;SmartCodable&lt;/strong&gt; 遇到了解析问题，走进了兼容。 并不代表着本次解析失败。&lt;/p&gt; &#xA;&lt;p&gt;SmartCodable鼓励从根本上解决解析中的问题，即：不需要用到SmartCodable的兼容逻辑。 如果出现解析兼容的情况，修改Model中属性的定义，或要求数据方进行修正。为了更方便的定位问题。&lt;/p&gt; &#xA;&lt;p&gt;调试日志，将提供辅助信息&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; ========================  [Smart Decoding Log]  ========================&#xA; Family 👈🏻 👀&#xA;    |- name    : Expected to decode String but found an array instead.&#xA;    |- location: Expected to decode String but found an array instead.&#xA;    |- date    : Expected to decode Date but found an array instead.&#xA;    |&amp;gt; father: Father&#xA;       |- name: Expected String value but found null instead.&#xA;       |- age : Expected to decode Int but found a string/data instead.&#xA;       |&amp;gt; dog: Dog&#xA;          |- hobby: Expected to decode String but found a number instead.&#xA;    |&amp;gt; sons: [Son]&#xA;       |- [Index 0] hobby: Expected to decode String but found a number instead.&#xA;       |- [Index 0] age  : Expected to decode Int but found a string/data instead.&#xA;       |- [Index 1] age  : Expected to decode Int but found an array instead.&#xA; =========================================================================&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;新版本的日志：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;以Model为单元进行和合并，减少了日志的数量。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;合并相同的解析错误，减少了相同日志的干扰。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;简化了日志内容，让日志一目了然。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;进一步了解&lt;/h2&gt; &#xA;&lt;p&gt;我们提供了详细的示例工程，可以下载工程代码查看。&lt;/p&gt; &#xA;&lt;h3&gt;了解更多关于Codable &amp;amp; SmartCodable&lt;/h3&gt; &#xA;&lt;p&gt;这是Swift数据解析方案的系列文章：&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://juejin.cn/post/7288517000581070902&#34;&gt;Swift数据解析(第一篇) - 技术选型&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://juejin.cn/post/7288517000581087286&#34;&gt;Swift数据解析(第二篇) - Codable 上&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://juejin.cn/post/7288517000581120054&#34;&gt;Swift数据解析(第二篇) - Codable 下&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://juejin.cn/post/7288504491506090023&#34;&gt;Swift数据解析(第三篇) - Codable源码学习&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://juejin.cn/post/7288513881735151670&#34;&gt;Swift数据解析(第四篇) - SmartCodable 上&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://juejin.cn/post/7288517000581169206&#34;&gt;Swift数据解析(第四篇) - SmartCodable 下&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;联系我们&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/intsig171/SmartCodable/assets/87351449/5d3a98fe-17ba-402f-aefe-3e7472f35f82&#34; alt=&#34;QQ&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;加入我们&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;SmartCodable&lt;/strong&gt; 是一个开源项目，我们欢迎所有对提高数据解析性能和健壮性感兴趣的开发者加入。无论是使用反馈、功能建议还是代码贡献，你的参与都将极大地推动 &lt;strong&gt;SmartCodable&lt;/strong&gt; 项目的发展。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jamesrochabrun/SwiftOpenAI</title>
    <updated>2024-05-22T01:34:19Z</updated>
    <id>tag:github.com,2024-05-22:/jamesrochabrun/SwiftOpenAI</id>
    <link href="https://github.com/jamesrochabrun/SwiftOpenAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source Swift package for interacting with OpenAI&#39;s public API.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SwiftOpenAI&lt;/h1&gt; &#xA;&lt;img width=&#34;1090&#34; alt=&#34;repoOpenAI&#34; src=&#34;https://github.com/jamesrochabrun/SwiftOpenAI/assets/5378604/51bc5736-a32f-4a9f-922e-209d950e28f7&#34;&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/iOS-15%2B-blue.svg?sanitize=true&#34; alt=&#34;iOS 15+&#34;&gt; &lt;a href=&#34;https://lbesson.mit-license.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;MIT license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apple/swift&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/swift-5.9-brightgreen.svg?sanitize=true&#34; alt=&#34;swift-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://developer.apple.com/documentation/swiftui&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/swiftui-brightgreen&#34; alt=&#34;swiftui-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://developer.apple.com/xcode/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/xcode-15%20-brightgreen&#34; alt=&#34;xcode-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/apple/swift-package-manager&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/package%20manager-compatible-brightgreen.svg?logo=data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iNjJweCIgaGVpZ2h0PSI0OXB4IiB2aWV3Qm94PSIwIDAgNjIgNDkiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDYzLjEgKDkyNDUyKSAtIGh0dHBzOi8vc2tldGNoLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cDwvdGl0bGU+CiAgICA8ZGVzYz5DcmVhdGVkIHdpdGggU2tldGNoLjwvZGVzYz4KICAgIDxnIGlkPSJQYWdlLTEiIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJHcm91cCIgZmlsbC1ydWxlPSJub256ZXJvIj4KICAgICAgICAgICAgPHBvbHlnb24gaWQ9IlBhdGgiIGZpbGw9IiNEQkI1NTEiIHBvaW50cz0iNTEuMzEwMzQ0OCAwIDEwLjY4OTY1NTIgMCAwIDEzLjUxNzI0MTQgMCA0OSA2MiA0OSA2MiAxMy41MTcyNDE0Ij48L3BvbHlnb24+CiAgICAgICAgICAgIDxwb2x5Z29uIGlkPSJQYXRoIiBmaWxsPSIjRjdFM0FGIiBwb2ludHM9IjI3IDI1IDMxIDI1IDM1IDI1IDM3IDI1IDM3IDE0IDI1IDE0IDI1IDI1Ij48L3BvbHlnb24+CiAgICAgICAgICAgIDxwb2x5Z29uIGlkPSJQYXRoIiBmaWxsPSIjRUZDNzVFIiBwb2ludHM9IjEwLjY4OTY1NTIgMCAwIDE0IDYyIDE0IDUxLjMxMDM0NDggMCI+PC9wb2x5Z29uPgogICAgICAgICAgICA8cG9seWdvbiBpZD0iUmVjdGFuZ2xlIiBmaWxsPSIjRjdFM0FGIiBwb2ludHM9IjI3IDAgMzUgMCAzNyAxNCAyNSAxNCI+PC9wb2x5Z29uPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+&#34; alt=&#34;swift-package-manager&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An open-source Swift package designed for effortless interaction with OpenAI&#39;s public API.&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#description&#34;&gt;Description&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#getting-an-api-key&#34;&gt;Getting an API Key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#azure-openai&#34;&gt;Azure OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#aiproxy&#34;&gt;AIProxy&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#collaboration&#34;&gt;Collaboration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;SwiftOpenAI&lt;/code&gt; is an open-source Swift package that streamlines interactions with &lt;strong&gt;all&lt;/strong&gt; OpenAI&#39;s API endpoints, now with added support for Azure, AIProxy, and Assistant stream APIs.&lt;/p&gt; &#xA;&lt;h3&gt;OpenAI ENDPOINTS&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#audio&#34;&gt;Audio&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#audio-transcriptions&#34;&gt;Transcriptions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#audio-translations&#34;&gt;Translations&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#audio-Speech&#34;&gt;Speech&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#chat&#34;&gt;Chat&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#function-calling&#34;&gt;Function Calling&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#vision&#34;&gt;Vision&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#embeddings&#34;&gt;Embeddings&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#batch&#34;&gt;Batch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#files&#34;&gt;Files&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#images&#34;&gt;Images&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#models&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#moderations&#34;&gt;Moderations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;strong&gt;BETA&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#assistants&#34;&gt;Assistants&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#assistants-file-object&#34;&gt;Assistants File Object&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#threads&#34;&gt;Threads&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#messages&#34;&gt;Messages&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#message-file-object&#34;&gt;Message File Object&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#runs&#34;&gt;Runs&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#run-step-object&#34;&gt;Run Step object&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#run-step-details&#34;&gt;Run Step details&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#assistants-streaming&#34;&gt;Assistants Streaming&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#message-delta-object&#34;&gt;Message Delta Object&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#run-step-delta-object&#34;&gt;Run Step Delta Object&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#vector-stores&#34;&gt;Vector Stores&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#vector-store-file&#34;&gt;Vector store File&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#vector-store-file-batch&#34;&gt;Vector store File Batch&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting an API Key&lt;/h2&gt; &#xA;&lt;p&gt;⚠️ &lt;strong&gt;Important&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To interact with OpenAI services, you&#39;ll need an API key. Follow these steps to obtain one:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Visit &lt;a href=&#34;https://www.openai.com/&#34;&gt;OpenAI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Sign up for an &lt;a href=&#34;https://platform.openai.com/signup&#34;&gt;account&lt;/a&gt; or &lt;a href=&#34;https://platform.openai.com/login&#34;&gt;log in&lt;/a&gt; if you already have one.&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;API key page&lt;/a&gt; and follow the instructions to generate a new API key.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For more information, consult OpenAI&#39;s &lt;a href=&#34;https://platform.openai.com/docs/&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;⚠️ Please take precautions to keep your API key secure per &lt;a href=&#34;https://platform.openai.com/docs/api-reference/authentication&#34;&gt;OpenAI&#39;s guidance&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Remember that your API key is a secret! Do not share it with others or expose it in any client-side code (browsers, apps). Production requests must be routed through your backend server where your API key can be securely loaded from an environment variable or key management service.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;SwiftOpenAI has built-in support for AIProxy, which is a backend for AI apps, to satisfy this requirement. To configure AIProxy, see the instructions &lt;a href=&#34;https://raw.githubusercontent.com/jamesrochabrun/SwiftOpenAI/main/#aiproxy&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Swift Package Manager&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open your Swift project in Xcode.&lt;/li&gt; &#xA; &lt;li&gt;Go to &lt;code&gt;File&lt;/code&gt; -&amp;gt; &lt;code&gt;Add Package Dependency&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In the search bar, enter &lt;a href=&#34;https://github.com/jamesrochabrun/SwiftOpenAI&#34;&gt;this URL&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Choose the version you&#39;d like to install.&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;code&gt;Add Package&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;To use SwiftOpenAI in your project, first import the package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import SwiftOpenAI&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, initialize the service using your OpenAI API key:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let apiKey = &#34;your_openai_api_key_here&#34;&#xA;let service = OpenAIServiceFactory.service(apiKey: apiKey)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can optionally specify an organization name if needed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let apiKey = &#34;your_openai_api_key_here&#34;&#xA;let oganizationID = &#34;your_organixation_id&#34;&#xA;let service = OpenAIServiceFactory.service(apiKey: apiKey, organizationID: oganizationID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s all you need to begin accessing the full range of OpenAI endpoints.&lt;/p&gt; &#xA;&lt;h3&gt;Audio&lt;/h3&gt; &#xA;&lt;h3&gt;Audio Transcriptions&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct AudioTranscriptionParameters: Encodable {&#xA;   &#xA;   /// The name of the file asset is not documented in OpenAI&#39;s official documentation; however, it is essential for constructing the multipart request.&#xA;   let fileName: String&#xA;   /// The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.&#xA;   let file: Data&#xA;   /// ID of the model to use. Only whisper-1 is currently available.&#xA;   let model: String&#xA;   /// The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency.&#xA;   let language: String?&#xA;   /// An optional text to guide the model&#39;s style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text/prompting) should match the audio language.&#xA;   let prompt: String?&#xA;   /// The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. Defaults to json&#xA;   let responseFormat: String?&#xA;   /// The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. Defaults to 0&#xA;   let temperature: Double?&#xA;   &#xA;   public enum Model: String {&#xA;      case whisperOne = &#34;whisper-1&#34;&#xA;   }&#xA;   &#xA;   public init(&#xA;      fileName: String,&#xA;      file: Data,&#xA;      model: Model = .whisperOne,&#xA;      prompt: String? = nil,&#xA;      responseFormat: String? = nil,&#xA;      temperature: Double? = nil,&#xA;      language: String? = nil)&#xA;   {&#xA;      self.fileName = fileName&#xA;      self.file = file&#xA;      self.model = model.rawValue&#xA;      self.prompt = prompt&#xA;      self.responseFormat = responseFormat&#xA;      self.temperature = temperature&#xA;      self.language = language&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct AudioObject: Decodable {&#xA;   &#xA;   /// The transcribed text if the request uses the `transcriptions` API, or the translated text if the request uses the `translations` endpoint.&#xA;   public let text: String&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fileName = &#34;narcos.m4a&#34;&#xA;let data = Data(contentsOfURL:_) // Data retrieved from the file named &#34;narcos.m4a&#34;.&#xA;let parameters = AudioTranscriptionParameters(fileName: fileName, file: data) // **Important**: in the file name always provide the file extension.&#xA;let audioObject =  try await service.createTranscription(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Audio Translations&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct AudioTranslationParameters: Encodable {&#xA;   &#xA;   /// The name of the file asset is not documented in OpenAI&#39;s official documentation; however, it is essential for constructing the multipart request.&#xA;   let fileName: String&#xA;   /// The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.&#xA;   let file: Data&#xA;   /// ID of the model to use. Only whisper-1 is currently available.&#xA;   let model: String&#xA;   /// An optional text to guide the model&#39;s style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text/prompting) should match the audio language.&#xA;   let prompt: String?&#xA;   /// The format of the transcript output, in one of these options: json, text, srt, verbose_json, or vtt. Defaults to json&#xA;   let responseFormat: String?&#xA;   /// The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. Defaults to 0&#xA;   let temperature: Double?&#xA;   &#xA;   public enum Model: String {&#xA;      case whisperOne = &#34;whisper-1&#34;&#xA;   }&#xA;   &#xA;   public init(&#xA;      fileName: String,&#xA;      file: Data,&#xA;      model: Model = .whisperOne,&#xA;      prompt: String? = nil,&#xA;      responseFormat: String? = nil,&#xA;      temperature: Double? = nil)&#xA;   {&#xA;      self.fileName = fileName&#xA;      self.file = file&#xA;      self.model = model.rawValue&#xA;      self.prompt = prompt&#xA;      self.responseFormat = responseFormat&#xA;      self.temperature = temperature&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct AudioObject: Decodable {&#xA;   &#xA;   /// The transcribed text if the request uses the `transcriptions` API, or the translated text if the request uses the `translations` endpoint.&#xA;   public let text: String&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fileName = &#34;german.m4a&#34;&#xA;let data = Data(contentsOfURL:_) // Data retrieved from the file named &#34;german.m4a&#34;.&#xA;let parameters = AudioTranslationParameters(fileName: fileName, file: data) // **Important**: in the file name always provide the file extension.&#xA;let audioObject = try await service.createTranslation(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Audio Speech&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Generates audio from the input text.](https://platform.openai.com/docs/api-reference/audio/createSpeech)&#xA;public struct AudioSpeechParameters: Encodable {&#xA;&#xA;   /// One of the available [TTS models](https://platform.openai.com/docs/models/tts): tts-1 or tts-1-hd&#xA;   let model: String&#xA;   /// The text to generate audio for. The maximum length is 4096 characters.&#xA;   let input: String&#xA;   /// The voice to use when generating the audio. Supported voices are alloy, echo, fable, onyx, nova, and shimmer. Previews of the voices are available in the [Text to speech guide.](https://platform.openai.com/docs/guides/text-to-speech/voice-options)&#xA;   let voice: String&#xA;   /// Defaults to mp3, The format to audio in. Supported formats are mp3, opus, aac, and flac.&#xA;   let responseFormat: String?&#xA;   /// Defaults to 1,  The speed of the generated audio. Select a value from 0.25 to 4.0. 1.0 is the default.&#xA;   let speed: Double?&#xA;&#xA;   public enum TTSModel: String {&#xA;      case tts1 = &#34;tts-1&#34;&#xA;      case tts1HD = &#34;tts-1-hd&#34;&#xA;   }&#xA;&#xA;   public enum Voice: String {&#xA;      case alloy&#xA;      case echo&#xA;      case fable&#xA;      case onyx&#xA;      case nova&#xA;      case shimmer&#xA;   }&#xA;&#xA;   public enum ResponseFormat: String {&#xA;      case mp3&#xA;      case opus&#xA;      case aac&#xA;      case flac&#xA;   }&#xA;   &#xA;   public init(&#xA;      model: TTSModel,&#xA;      input: String,&#xA;      voice: Voice,&#xA;      responseFormat: ResponseFormat? = nil,&#xA;      speed: Double? = nil)&#xA;   {&#xA;       self.model = model.rawValue&#xA;       self.input = input&#xA;       self.voice = voice.rawValue&#xA;       self.responseFormat = responseFormat?.rawValue&#xA;       self.speed = speed&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// The [audio speech](https://platform.openai.com/docs/api-reference/audio/createSpeech) response.&#xA;public struct AudioSpeechObject: Decodable {&#xA;&#xA;   /// The audio file content data.&#xA;   public let output: Data&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let prompt = &#34;Hello, how are you today?&#34;&#xA;let parameters = AudioSpeechParameters(model: .tts1, input: prompt, voice: .shimmer)&#xA;let audioObjectData = try await service.createSpeech(parameters: parameters).output&#xA;playAudio(from: audioObjectData)&#xA;&#xA;// Play data&#xA; private func playAudio(from data: Data) {&#xA;       do {&#xA;           // Initialize the audio player with the data&#xA;           audioPlayer = try AVAudioPlayer(data: data)&#xA;           audioPlayer?.prepareToPlay()&#xA;           audioPlayer?.play()&#xA;       } catch {&#xA;           // Handle errors&#xA;           print(&#34;Error playing audio: \(error.localizedDescription)&#34;)&#xA;       }&#xA;   }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Chat&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Create chat completion](https://platform.openai.com/docs/api-reference/chat/create)&#xA;public struct ChatCompletionParameters: Encodable {&#xA;   &#xA;   /// A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models)&#xA;   public var messages: [Message]&#xA;   /// ID of the model to use. See the [model endpoint compatibility](https://platform.openai.com/docs/models/how-we-use-your-data) table for details on which models work with the Chat API.&#xA;   let model: String&#xA;   /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim. Defaults to 0&#xA;   /// [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)&#xA;   public var frequencyPenalty: Double?&#xA;   /// Controls how the model responds to function calls. none means the model does not call a function, and responds to the end-user. auto means the model can pick between an end-user or calling a function. Specifying a particular function via {&#34;name&#34;: &#34;my_function&#34;} forces the model to call that function. none is the default when no functions are present. auto is the default if functions are present.&#xA;   @available(*, deprecated, message: &#34;Deprecated in favor of tool_choice.&#34;)&#xA;   public var functionCall: FunctionCall?&#xA;   /// Controls which (if any) function is called by the model. none means the model will not call a function and instead generates a message. &#xA;   /// auto means the model can pick between generating a message or calling a function. Specifying a particular function via `{&#34;type: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;my_function&#34;}}` forces the model to call that function.&#xA;   /// `none` is the default when no functions are present. auto is the default if functions are present.&#xA;   public var toolChoice: ToolChoice?&#xA;   /// A list of functions the model may generate JSON inputs for.&#xA;   @available(*, deprecated, message: &#34;Deprecated in favor of tools.&#34;)&#xA;   public var functions: [ChatFunction]?&#xA;   /// A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for.&#xA;   public var tools: [Tool]?&#xA;   /// Modify the likelihood of specified tokens appearing in the completion.&#xA;   /// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. Defaults to null.&#xA;   public var logitBias: [Int: Double]?&#xA;   /// Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message. This option is currently not available on the gpt-4-vision-preview model. Defaults to false.&#xA;   public var logprobs: Bool?&#xA;   /// An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.&#xA;   public var topLogprobs: Int?&#xA;   /// The maximum number of [tokens](https://platform.openai.com/tokenizer) to generate in the chat completion.&#xA;   /// The total length of input tokens and generated tokens is limited by the model&#39;s context length. Example [Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.&#xA;   public var maxTokens: Int?&#xA;   /// How many chat completion choices to generate for each input message. Defaults to 1.&#xA;   public var n: Int?&#xA;   /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model&#39;s likelihood to talk about new topics. Defaults to 0&#xA;   /// [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)&#xA;   public var presencePenalty: Double?&#xA;   /// An object specifying the format that the model must output. Used to enable JSON mode.&#xA;   /// Setting to `{ type: &#34;json_object&#34; }` enables `JSON` mode, which guarantees the message the model generates is valid JSON.&#xA;   ///Important: when using `JSON` mode you must still instruct the model to produce `JSON` yourself via some conversation message, for example via your system message. If you don&#39;t do this, the model may generate an unending stream of whitespace until the generation reaches the token limit, which may take a lot of time and give the appearance of a &#34;stuck&#34; request. Also note that the message content may be partial (i.e. cut off) if `finish_reason=&#34;length&#34;`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.&#xA;   public var responseFormat: ResponseFormat?&#xA;   /// This feature is in `Beta`. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.&#xA;   /// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.&#xA;   public var seed: Int?&#xA;   /// Up to 4 sequences where the API will stop generating further tokens. Defaults to null.&#xA;   public var stop: [String]?&#xA;   /// If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) as they become available, with the stream terminated by a data: [DONE] message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions ).&#xA;   /// Defaults to false.&#xA;   var stream: Bool? = nil&#xA;   /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.&#xA;   /// We generally recommend altering this or `top_p` but not both. Defaults to 1.&#xA;   public var temperature: Double?&#xA;   /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.&#xA;   /// We generally recommend altering this or `temperature` but not both. Defaults to 1&#xA;   public var topP: Double?&#xA;   /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.&#xA;   /// [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).&#xA;   let user: String?&#xA;   &#xA;   public enum Model: String {&#xA;      case gpt35Turbo = &#34;gpt-3.5-turbo&#34;&#xA;      case gpt35Turbo1106 = &#34;gpt-3.5-turbo-1106&#34; // Most updated - Supports parallel function calls&#xA;      case gpt4 = &#34;gpt-4&#34;&#xA;      case gpt41106Preview = &#34;gpt-4-1106-preview&#34;  // Most updated - Supports parallel function calls&#xA;      case gpt35Turbo0613 = &#34;gpt-3.5-turbo-0613&#34; // To be deprecated &#34;2024-06-13&#34;&#xA;      case gpt35Turbo16k0613 = &#34;gpt-3.5-turbo-16k-0613&#34; // To be deprecated &#34;2024-06-13&#34;&#xA;      &#xA;      case gpt4VisionPreview = &#34;gpt-4-vision-preview&#34; // Vision&#xA;   }&#xA;   &#xA;   public struct Message: Encodable {&#xA;      &#xA;      /// The role of the messages author. One of system, user, assistant, or tool message.&#xA;      let role: String&#xA;      /// The contents of the message. content is required for all messages, and may be null for assistant messages with function calls.&#xA;      let content: ContentType&#xA;      /// The name of the author of this message. name is required if role is function, and it should be the name of the function whose response is in the content. May contain a-z, A-Z, 0-9, and underscores, with a maximum length of 64 characters.&#xA;      let name: String?&#xA;      /// The name and arguments of a function that should be called, as generated by the model.&#xA;      @available(*, deprecated, message: &#34;Deprecated and replaced by `tool_calls`&#34;)&#xA;      let functionCall: FunctionCall?&#xA;      /// The tool calls generated by the model, such as function calls.&#xA;      let toolCalls: [ToolCall]?&#xA;      /// Tool call that this message is responding to.&#xA;      let toolCallID: String?&#xA;      &#xA;      public enum ContentType: Encodable {&#xA;         &#xA;         case text(String)&#xA;         case contentArray([MessageContent])&#xA;         &#xA;         public func encode(to encoder: Encoder) throws {&#xA;            var container = encoder.singleValueContainer()&#xA;            switch self {&#xA;            case .text(let text):&#xA;               try container.encode(text)&#xA;            case .contentArray(let contentArray):&#xA;               try container.encode(contentArray)&#xA;            }&#xA;         }&#xA;         &#xA;         public enum MessageContent: Encodable {&#xA;            case text(String)&#xA;            case imageUrl(URL)&#xA;            &#xA;            enum CodingKeys: String, CodingKey {&#xA;               case type&#xA;               case text&#xA;               case imageUrl = &#34;image_url&#34;&#xA;            }&#xA;            &#xA;            public func encode(to encoder: Encoder) throws {&#xA;               var container = encoder.container(keyedBy: CodingKeys.self)&#xA;               switch self {&#xA;               case .text(let text):&#xA;                  try container.encode(&#34;text&#34;, forKey: .type)&#xA;                  try container.encode(text, forKey: .text)&#xA;               case .imageUrl(let url):&#xA;                  try container.encode(&#34;image_url&#34;, forKey: .type)&#xA;                  try container.encode(url, forKey: .imageUrl)&#xA;               }&#xA;            }&#xA;         }&#xA;      }&#xA;      &#xA;      public enum Role: String {&#xA;         case system // content, role&#xA;         case user // content, role&#xA;         case assistant // content, role, tool_calls&#xA;         case tool // content, role, tool_call_id&#xA;      }&#xA;      &#xA;      public init(&#xA;         role: Role,&#xA;         content: ContentType,&#xA;         name: String? = nil,&#xA;         functionCall: FunctionCall? = nil,&#xA;         toolCalls: [ToolCall]? = nil,&#xA;         toolCallID: String? = nil)&#xA;      {&#xA;         self.role = role.rawValue&#xA;         self.content = content&#xA;         self.name = name&#xA;         self.functionCall = functionCall&#xA;         self.toolCalls = toolCalls&#xA;         self.toolCallID = toolCallID&#xA;      }&#xA;   }&#xA;   &#xA;   @available(*, deprecated, message: &#34;Deprecated in favor of ToolChoice.&#34;)&#xA;   public enum FunctionCall: Encodable, Equatable {&#xA;      case none&#xA;      case auto&#xA;      case function(String)&#xA;      &#xA;      enum CodingKeys: String, CodingKey {&#xA;         case none = &#34;none&#34;&#xA;         case auto = &#34;auto&#34;&#xA;         case function = &#34;name&#34;&#xA;      }&#xA;      &#xA;      public func encode(to encoder: Encoder) throws {&#xA;         switch self {&#xA;         case .none:&#xA;            var container = encoder.singleValueContainer()&#xA;            try container.encode(CodingKeys.none.rawValue)&#xA;         case .auto:&#xA;            var container = encoder.singleValueContainer()&#xA;            try container.encode(CodingKeys.auto.rawValue)&#xA;         case .function(let name):&#xA;            var container = encoder.container(keyedBy: CodingKeys.self)&#xA;            try container.encode(name, forKey: .function)&#xA;         }&#xA;      }&#xA;   }&#xA;   &#xA;   /// string `none` means the model will not call a function and instead generates a message.&#xA;   /// `auto` means the model can pick between generating a message or calling a function.&#xA;   /// `object` Specifies a tool the model should use. Use to force the model to call a specific function. The type of the tool. Currently, only` function` is supported. `{&#34;type: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;my_function&#34;}}`&#xA;   public enum ToolChoice: Encodable, Equatable {&#xA;      case none&#xA;      case auto&#xA;      case function(type: String?, name: String)&#xA;      &#xA;      enum CodingKeys: String, CodingKey {&#xA;         case none = &#34;none&#34;&#xA;         case auto = &#34;auto&#34;&#xA;         case name = &#34;name&#34;&#xA;         case type = &#34;type&#34;&#xA;      }&#xA;      &#xA;      public func encode(to encoder: Encoder) throws {&#xA;         switch self {&#xA;         case .none:&#xA;            var container = encoder.singleValueContainer()&#xA;            try container.encode(CodingKeys.none.rawValue)&#xA;         case .auto:&#xA;            var container = encoder.singleValueContainer()&#xA;            try container.encode(CodingKeys.auto.rawValue)&#xA;         case .function(let type, let name):&#xA;            var container = encoder.container(keyedBy: CodingKeys.self)&#xA;            try container.encode(name, forKey: .name)&#xA;            if let type {&#xA;               try container.encode(type, forKey: .type)&#xA;            }&#xA;         }&#xA;      }&#xA;   }&#xA;   &#xA;   public struct Tool: Encodable {&#xA;      &#xA;      /// The type of the tool. Currently, only `function` is supported.&#xA;      let type: String&#xA;      /// object&#xA;      let function: ChatFunction&#xA;      &#xA;      public init(&#xA;         type: String = &#34;function&#34;,&#xA;         function: ChatFunction)&#xA;      {&#xA;         self.type = type&#xA;         self.function = function&#xA;      }&#xA;   }&#xA;   &#xA;   public struct ChatFunction: Encodable, Equatable {&#xA;      &#xA;      /// The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.&#xA;      let name: String&#xA;      /// A description of what the function does, used by the model to choose when and how to call the function.&#xA;      let description: String?&#xA;      /// The parameters the functions accepts, described as a JSON Schema object. See the [guide](https://platform.openai.com/docs/guides/gpt/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema) for documentation about the format.&#xA;      /// To describe a function that accepts no parameters, provide the value `{&#34;type&#34;: &#34;object&#34;, &#34;properties&#34;: {}}`.&#xA;      let parameters: JSONSchema&#xA;      &#xA;      public struct JSONSchema: Encodable, Equatable {&#xA;         &#xA;         public let type: JSONType&#xA;         public let properties: [String: Property]?&#xA;         public let required: [String]?&#xA;         public let pattern: String?&#xA;         public let const: String?&#xA;         public let enumValues: [String]?&#xA;         public let multipleOf: Int?&#xA;         public let minimum: Int?&#xA;         public let maximum: Int?&#xA;         &#xA;         private enum CodingKeys: String, CodingKey {&#xA;            case type, properties, required, pattern, const&#xA;            case enumValues = &#34;enum&#34;&#xA;            case multipleOf, minimum, maximum&#xA;         }&#xA;         &#xA;         public struct Property: Encodable, Equatable {&#xA;            &#xA;            public let type: JSONType&#xA;            public let description: String?&#xA;            public let format: String?&#xA;            public let items: Items?&#xA;            public let required: [String]?&#xA;            public let pattern: String?&#xA;            public let const: String?&#xA;            public let enumValues: [String]?&#xA;            public let multipleOf: Int?&#xA;            public let minimum: Double?&#xA;            public let maximum: Double?&#xA;            public let minItems: Int?&#xA;            public let maxItems: Int?&#xA;            public let uniqueItems: Bool?&#xA;            &#xA;            private enum CodingKeys: String, CodingKey {&#xA;               case type, description, format, items, required, pattern, const&#xA;               case enumValues = &#34;enum&#34;&#xA;               case multipleOf, minimum, maximum&#xA;               case minItems, maxItems, uniqueItems&#xA;            }&#xA;            &#xA;            public init(&#xA;               type: JSONType,&#xA;               description: String? = nil,&#xA;               format: String? = nil,&#xA;               items: Items? = nil,&#xA;               required: [String]? = nil,&#xA;               pattern: String? = nil,&#xA;               const: String? = nil,&#xA;               enumValues: [String]? = nil,&#xA;               multipleOf: Int? = nil,&#xA;               minimum: Double? = nil,&#xA;               maximum: Double? = nil,&#xA;               minItems: Int? = nil,&#xA;               maxItems: Int? = nil,&#xA;               uniqueItems: Bool? = nil)&#xA;            {&#xA;               self.type = type&#xA;               self.description = description&#xA;               self.format = format&#xA;               self.items = items&#xA;               self.required = required&#xA;               self.pattern = pattern&#xA;               self.const = const&#xA;               self.enumValues = enumValues&#xA;               self.multipleOf = multipleOf&#xA;               self.minimum = minimum&#xA;               self.maximum = maximum&#xA;               self.minItems = minItems&#xA;               self.maxItems = maxItems&#xA;               self.uniqueItems = uniqueItems&#xA;            }&#xA;         }&#xA;         &#xA;         public enum JSONType: String, Encodable {&#xA;            case integer = &#34;integer&#34;&#xA;            case string = &#34;string&#34;&#xA;            case boolean = &#34;boolean&#34;&#xA;            case array = &#34;array&#34;&#xA;            case object = &#34;object&#34;&#xA;            case number = &#34;number&#34;&#xA;            case `null` = &#34;null&#34;&#xA;         }&#xA;         &#xA;         public struct Items: Encodable, Equatable {&#xA;            &#xA;            public let type: JSONType&#xA;            public let properties: [String: Property]?&#xA;            public let pattern: String?&#xA;            public let const: String?&#xA;            public let enumValues: [String]?&#xA;            public let multipleOf: Int?&#xA;            public let minimum: Double?&#xA;            public let maximum: Double?&#xA;            public let minItems: Int?&#xA;            public let maxItems: Int?&#xA;            public let uniqueItems: Bool?&#xA;            &#xA;            private enum CodingKeys: String, CodingKey {&#xA;               case type, properties, pattern, const&#xA;               case enumValues = &#34;enum&#34;&#xA;               case multipleOf, minimum, maximum, minItems, maxItems, uniqueItems&#xA;            }&#xA;            &#xA;            public init(&#xA;               type: JSONType,&#xA;               properties: [String : Property]? = nil,&#xA;               pattern: String? = nil,&#xA;               const: String? = nil,&#xA;               enumValues: [String]? = nil,&#xA;               multipleOf: Int? = nil,&#xA;               minimum: Double? = nil,&#xA;               maximum: Double? = nil,&#xA;               minItems: Int? = nil,&#xA;               maxItems: Int? = nil,&#xA;               uniqueItems: Bool? = nil)&#xA;            {&#xA;               self.type = type&#xA;               self.properties = properties&#xA;               self.pattern = pattern&#xA;               self.const = const&#xA;               self.enumValues = enumValues&#xA;               self.multipleOf = multipleOf&#xA;               self.minimum = minimum&#xA;               self.maximum = maximum&#xA;               self.minItems = minItems&#xA;               self.maxItems = maxItems&#xA;               self.uniqueItems = uniqueItems&#xA;            }&#xA;         }&#xA;         &#xA;         public init(&#xA;            type: JSONType,&#xA;            properties: [String : Property]? = nil,&#xA;            required: [String]? = nil,&#xA;            pattern: String? = nil,&#xA;            const: String? = nil,&#xA;            enumValues: [String]? = nil,&#xA;            multipleOf: Int? = nil,&#xA;            minimum: Int? = nil,&#xA;            maximum: Int? = nil)&#xA;         {&#xA;            self.type = type&#xA;            self.properties = properties&#xA;            self.required = required&#xA;            self.pattern = pattern&#xA;            self.const = const&#xA;            self.enumValues = enumValues&#xA;            self.multipleOf = multipleOf&#xA;            self.minimum = minimum&#xA;            self.maximum = maximum&#xA;         }&#xA;      }&#xA;      &#xA;      public init(&#xA;         name: String,&#xA;         description: String?,&#xA;         parameters: JSONSchema)&#xA;      {&#xA;         self.name = name&#xA;         self.description = description&#xA;         self.parameters = parameters&#xA;      }&#xA;   }&#xA;   &#xA;   public struct ResponseFormat: Encodable {&#xA;      &#xA;      /// Defaults to text&#xA;      /// Setting to `json_object` enables JSON mode. This guarantees that the message the model generates is valid JSON.&#xA;      /// Note that your system prompt must still instruct the model to produce JSON, and to help ensure you don&#39;t forget, the API will throw an error if the string JSON does not appear in your system message.&#xA;      /// Also note that the message content may be partial (i.e. cut off) if `finish_reason=&#34;length&#34;`, which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.&#xA;      /// Must be one of `text `or `json_object`.&#xA;      public var type: String?&#xA;      &#xA;      public init(&#xA;         type: String?)&#xA;      {&#xA;         self.type = type&#xA;      }&#xA;   }&#xA;   &#xA;   public init(&#xA;      messages: [Message],&#xA;      model: Model,&#xA;      frequencyPenalty: Double? = nil,&#xA;      functionCall: FunctionCall? = nil,&#xA;      toolChoice: ToolChoice? = nil,&#xA;      functions: [ChatFunction]? = nil,&#xA;      tools: [Tool]? = nil,&#xA;      logitBias: [Int: Double]? = nil,&#xA;      maxTokens: Int? = nil,&#xA;      n: Int? = nil,&#xA;      responseFormat: ResponseFormat? = nil,&#xA;      presencePenalty: Double? = nil,&#xA;      seed: Int? = nil,&#xA;      stop: [String]? = nil,&#xA;      temperature: Double? = nil,&#xA;      topProbability: Double? = nil,&#xA;      user: String? = nil)&#xA;   {&#xA;      self.messages = messages&#xA;      self.model = model.value&#xA;      self.frequencyPenalty = frequencyPenalty&#xA;      self.functionCall = functionCall&#xA;      self.toolChoice = toolChoice&#xA;      self.functions = functions&#xA;      self.tools = tools&#xA;      self.logitBias = logitBias&#xA;      self.maxTokens = maxTokens&#xA;      self.n = n&#xA;      self.responseFormat = responseFormat&#xA;      self.presencePenalty = presencePenalty&#xA;      self.seed = seed&#xA;      self.stop = stop&#xA;      self.temperature = temperature&#xA;      self.topP = topProbability&#xA;      self.user = user&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;h3&gt;Chat completion object&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Represents a chat [completion](https://platform.openai.com/docs/api-reference/chat/object) response returned by model, based on the provided input.&#xA;public struct ChatCompletionObject: Decodable {&#xA;   &#xA;   /// A unique identifier for the chat completion.&#xA;   public let id: String&#xA;   /// A list of chat completion choices. Can be more than one if n is greater than 1.&#xA;   public let choices: [ChatChoice]&#xA;   /// The Unix timestamp (in seconds) of when the chat completion was created.&#xA;   public let created: Int&#xA;   /// The model used for the chat completion.&#xA;   public let model: String&#xA;   /// This fingerprint represents the backend configuration that the model runs with.&#xA;   /// Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.&#xA;   public let systemFingerprint: String?&#xA;   /// The object type, which is always chat.completion.&#xA;   public let object: String&#xA;   /// Usage statistics for the completion request.&#xA;   public let usage: ChatUsage&#xA;   &#xA;   public struct ChatChoice: Decodable {&#xA;      &#xA;      /// The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.&#xA;      public let finishReason: IntOrStringValue?&#xA;      /// The index of the choice in the list of choices.&#xA;      public let index: Int&#xA;      /// A chat completion message generated by the model.&#xA;      public let message: ChatMessage   &#xA;      /// Log probability information for the choice.&#xA;      public let logprobs: LogProb?&#xA;      &#xA;      public struct ChatMessage: Decodable {&#xA;         &#xA;         /// The contents of the message.&#xA;         public let content: String?&#xA;         /// The tool calls generated by the model, such as function calls.&#xA;         public let toolCalls: [ToolCall]?&#xA;         /// The name and arguments of a function that should be called, as generated by the model.&#xA;         @available(*, deprecated, message: &#34;Deprecated and replaced by `tool_calls`&#34;)&#xA;         public let functionCall: FunctionCall?&#xA;         /// The role of the author of this message.&#xA;         public let role: String&#xA;         /// Provided by the Vision API.&#xA;         public let finishDetails: FinishDetails?&#xA;         &#xA;         /// Provided by the Vision API.&#xA;         public struct FinishDetails: Decodable {&#xA;            let type: String&#xA;         }&#xA;      }&#xA;      &#xA;      public struct LogProb: Decodable {&#xA;         /// A list of message content tokens with log probability information.&#xA;         let content: [TokenDetail]&#xA;      }&#xA;      &#xA;      public struct TokenDetail: Decodable {&#xA;         /// The token.&#xA;         let token: String&#xA;         /// The log probability of this token.&#xA;         let logprob: Double&#xA;         /// A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token.&#xA;         let bytes: [Int]?&#xA;         /// List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned.&#xA;         let topLogprobs: [TopLogProb]&#xA;         &#xA;         enum CodingKeys: String, CodingKey {&#xA;            case token, logprob, bytes&#xA;            case topLogprobs = &#34;top_logprobs&#34;&#xA;         }&#xA;         &#xA;         struct TopLogProb: Decodable {&#xA;            /// The token.&#xA;            let token: String&#xA;            /// The log probability of this token.&#xA;            let logprob: Double&#xA;            /// A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token.&#xA;            let bytes: [Int]?&#xA;         }&#xA;      }&#xA;   }&#xA;   &#xA;   public struct ChatUsage: Decodable {&#xA;      &#xA;      /// Number of tokens in the generated completion.&#xA;      public let completionTokens: Int&#xA;      /// Number of tokens in the prompt.&#xA;      public let promptTokens: Int&#xA;      /// Total number of tokens used in the request (prompt + completion).&#xA;      public let totalTokens: Int&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let prompt = &#34;Tell me a joke&#34;&#xA;let parameters = ChatCompletionParameters(messages: [.init(role: .user, content: .text(prompt))], model: .gpt4o)&#xA;let chatCompletionObject = service.startChat(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;h3&gt;Chat completion chunk object&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Represents a [streamed](https://platform.openai.com/docs/api-reference/chat/streaming) chunk of a chat completion response returned by model, based on the provided input.&#xA;public struct ChatCompletionChunkObject: Decodable {&#xA;   &#xA;   /// A unique identifier for the chat completion chunk.&#xA;   public let id: String&#xA;   /// A list of chat completion choices. Can be more than one if n is greater than 1.&#xA;   public let choices: [ChatChoice]&#xA;   /// The Unix timestamp (in seconds) of when the chat completion chunk was created.&#xA;   public let created: Int&#xA;   /// The model to generate the completion.&#xA;   public let model: String&#xA;   /// This fingerprint represents the backend configuration that the model runs with.&#xA;   /// Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.&#xA;   public let systemFingerprint: String?&#xA;   /// The object type, which is always chat.completion.chunk.&#xA;   public let object: String&#xA;   &#xA;   public struct ChatChoice: Decodable {&#xA;      &#xA;      /// A chat completion delta generated by streamed model responses.&#xA;      public let delta: Delta&#xA;      /// The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.&#xA;      public let finishReason: IntOrStringValue?&#xA;      /// The index of the choice in the list of choices.&#xA;      public let index: Int&#xA;      /// Provided by the Vision API.&#xA;      public let finishDetails: FinishDetails?&#xA;      &#xA;      public struct Delta: Decodable {&#xA;         &#xA;         /// The contents of the chunk message.&#xA;         public let content: String?&#xA;         /// The tool calls generated by the model, such as function calls.&#xA;         public let toolCalls: [ToolCall]?&#xA;         /// The name and arguments of a function that should be called, as generated by the model.&#xA;         @available(*, deprecated, message: &#34;Deprecated and replaced by `tool_calls`&#34;)&#xA;         public let functionCall: FunctionCall?&#xA;         /// The role of the author of this message.&#xA;         public let role: String?&#xA;      }&#xA;      &#xA;      public struct LogProb: Decodable {&#xA;         /// A list of message content tokens with log probability information.&#xA;         let content: [TokenDetail]&#xA;      }&#xA;      &#xA;      public struct TokenDetail: Decodable {&#xA;         /// The token.&#xA;         let token: String&#xA;         /// The log probability of this token.&#xA;         let logprob: Double&#xA;         /// A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token.&#xA;         let bytes: [Int]?&#xA;         /// List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested top_logprobs returned.&#xA;         let topLogprobs: [TopLogProb]&#xA;         &#xA;         enum CodingKeys: String, CodingKey {&#xA;            case token, logprob, bytes&#xA;            case topLogprobs = &#34;top_logprobs&#34;&#xA;         }&#xA;         &#xA;         struct TopLogProb: Decodable {&#xA;            /// The token.&#xA;            let token: String&#xA;            /// The log probability of this token.&#xA;            let logprob: Double&#xA;            /// A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be null if there is no bytes representation for the token.&#xA;            let bytes: [Int]?&#xA;         }&#xA;      }&#xA;      &#xA;      /// Provided by the Vision API.&#xA;      public struct FinishDetails: Decodable {&#xA;         let type: String&#xA;      }&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let prompt = &#34;Tell me a joke&#34;&#xA;let parameters = ChatCompletionParameters(messages: [.init(role: .user, content: .text(prompt))], model: .gpt4o)&#xA;let chatCompletionObject = try await service.startStreamedChat(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Function Calling&lt;/h3&gt; &#xA;&lt;p&gt;Chat Completion also supports &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling&#34;&gt;Function Calling&lt;/a&gt; and &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling/parallel-function-calling&#34;&gt;Parallel Function Calling&lt;/a&gt;. &lt;code&gt;functions&lt;/code&gt; has been deprecated in favor of &lt;code&gt;tools&lt;/code&gt; check &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;OpenAI Documentation&lt;/a&gt; for more.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct ToolCall: Codable {&#xA;&#xA;   public let index: Int&#xA;   /// The ID of the tool call.&#xA;   public let id: String?&#xA;   /// The type of the tool. Currently, only `function` is supported.&#xA;   public let type: String?&#xA;   /// The function that the model called.&#xA;   public let function: FunctionCall&#xA;&#xA;   public init(&#xA;      index: Int,&#xA;      id: String,&#xA;      type: String = &#34;function&#34;,&#xA;      function: FunctionCall)&#xA;   {&#xA;      self.index = index&#xA;      self.id = id&#xA;      self.type = type&#xA;      self.function = function&#xA;   }&#xA;}&#xA;&#xA;public struct FunctionCall: Codable {&#xA;&#xA;   /// The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.&#xA;   let arguments: String&#xA;   /// The name of the function to call.&#xA;   let name: String&#xA;&#xA;   public init(&#xA;      arguments: String,&#xA;      name: String)&#xA;   {&#xA;      self.arguments = arguments&#xA;      self.name = name&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Define a `ToolCall`&#xA;var tool: ToolCall {&#xA;   .init(&#xA;      type: &#34;function&#34;, // The type of the tool. Currently, only &#34;function&#34; is supported.&#xA;      function: .init(&#xA;         name: &#34;create_image&#34;,&#xA;         description: &#34;Call this function if the request asks to generate an image&#34;,&#xA;         parameters: .init(&#xA;            type: .object,&#xA;            properties: [&#xA;               &#34;prompt&#34;: .init(type: .string, description: &#34;The exact prompt passed in.&#34;),&#xA;               &#34;count&#34;: .init(type: .integer, description: &#34;The number of images requested&#34;)&#xA;            ],&#xA;            required: [&#34;prompt&#34;, &#34;count&#34;])))&#xA;}&#xA;&#xA;let prompt = &#34;Show me an image of an unicorn eating ice cream&#34;&#xA;let content: ChatCompletionParameters.Message.ContentType = .text(prompt)&#xA;let parameters = ChatCompletionParameters(messages: [.init(role: .user, content: content)], model: .gpt41106Preview, tools: [tool])&#xA;let chatCompletionObject = try await service.startStreamedChat(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details about how to also uploading base 64 encoded images in iOS check the &lt;a href=&#34;https://github.com/jamesrochabrun/SwiftOpenAI/tree/main/Examples/SwiftOpenAIExample/SwiftOpenAIExample/ChatFunctionsCall&#34;&gt;ChatFunctionsCalllDemo&lt;/a&gt; demo on the Examples section of this package.&lt;/p&gt; &#xA;&lt;h3&gt;Vision&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/vision&#34;&gt;Vision&lt;/a&gt; API is available for use; developers must access it through the chat completions API, specifically using the gpt-4-vision-preview model or gpt-4o model. Using any other model will not provide an image description&lt;/p&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let imageURL = &#34;https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg&#34;&#xA;let prompt = &#34;What is this?&#34;&#xA;let messageContent: [ChatCompletionParameters.Message.ContentType.MessageContent] = [.text(prompt), .imageUrl(imageURL)] // Users can add as many `.imageUrl` instances to the service.&#xA;let parameters = ChatCompletionParameters(messages: [.init(role: .user, content: .contentArray(messageContent))], model: .gpt4o)&#xA;let chatCompletionObject = try await service.startStreamedChat(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/jamesrochabrun/SwiftOpenAI/assets/5378604/db2cbb3b-0c80-4ac8-8fe5-dbb782b270da&#34; alt=&#34;Simulator Screen Recording - iPhone 15 - 2023-11-09 at 17 12 06&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more details about how to also uploading base 64 encoded images in iOS check the &lt;a href=&#34;https://github.com/jamesrochabrun/SwiftOpenAI/tree/main/Examples/SwiftOpenAIExample/SwiftOpenAIExample/Vision&#34;&gt;ChatVision&lt;/a&gt; demo on the Examples section of this package.&lt;/p&gt; &#xA;&lt;h3&gt;Embeddings&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Creates](https://platform.openai.com/docs/api-reference/embeddings/create) an embedding vector representing the input text.&#xA;public struct EmbeddingParameter: Encodable {&#xA;   &#xA;   /// ID of the model to use. You can use the List models API to see all of your available models, or see our [Model overview ](https://platform.openai.com/docs/models/overview) for descriptions of them.&#xA;   let model: String&#xA;   /// Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or an array of token arrays. Each input must not exceed the max input tokens for the model (8191 tokens for text-embedding-ada-002) and cannot be an empty string. [How to Count Tokens with `tiktoken`](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)&#xA;   let input: String&#xA;   &#xA;   /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more.](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids)&#xA;   let user: String?&#xA;   &#xA;   public enum Model: String {&#xA;      case textEmbeddingAda002 = &#34;text-embedding-ada-002&#34;&#xA;   }&#xA;   &#xA;   public init(&#xA;      model: Model = .textEmbeddingAda002,&#xA;      input: String,&#xA;      user: String? = nil)&#xA;   {&#xA;      self.model = model.value&#xA;      self.input = input&#xA;      self.user = user&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Represents an embedding vector returned by embedding endpoint.](https://platform.openai.com/docs/api-reference/embeddings/object)&#xA;public struct EmbeddingObject: Decodable {&#xA;   &#xA;   /// The object type, which is always &#34;embedding&#34;.&#xA;   public let object: String&#xA;   /// The embedding vector, which is a list of floats. The length of vector depends on the model as listed in the embedding guide.[https://platform.openai.com/docs/guides/embeddings]&#xA;   public let embedding: [Float]&#xA;   /// The index of the embedding in the list of embeddings.&#xA;   public let index: Int&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let prompt = &#34;Hello world.&#34;&#xA;let embeddingObjects = try await service.createEmbeddings(parameters: parameters).data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Creates a job](https://platform.openai.com/docs/api-reference/fine-tuning/create) that fine-tunes a specified model from a given dataset.&#xA;///Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.&#xA;public struct FineTuningJobParameters: Encodable {&#xA;   &#xA;   /// The name of the model to fine-tune. You can select one of the [supported models](https://platform.openai.com/docs/models/overview).&#xA;   let model: String&#xA;   /// The ID of an uploaded file that contains training data.&#xA;   /// See [upload file](https://platform.openai.com/docs/api-reference/files/upload) for how to upload a file.&#xA;   /// Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with the purpose fine-tune.&#xA;   /// See the [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning) for more details.&#xA;   let trainingFile: String&#xA;   /// The hyperparameters used for the fine-tuning job.&#xA;   let hyperparameters: HyperParameters?&#xA;   /// A string of up to 18 characters that will be added to your fine-tuned model name.&#xA;   /// For example, a suffix of &#34;custom-model-name&#34; would produce a model name like ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel.&#xA;   /// Defaults to null.&#xA;   let suffix: String?&#xA;   /// The ID of an uploaded file that contains validation data.&#xA;   /// If you provide this file, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should not be present in both train and validation files.&#xA;   /// Your dataset must be formatted as a JSONL file. You must upload your file with the purpose fine-tune.&#xA;   /// See the [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning) for more details.&#xA;   let validationFile: String?&#xA;   /// A list of integrations to enable for your fine-tuning job.&#xA;   let integrations: [Integration]?&#xA;   /// The seed controls the reproducibility of the job. Passing in the same seed and job parameters should produce the same results, but may differ in rare cases. If a seed is not specified, one will be generated for you.&#xA;   let seed: Int?&#xA;   &#xA;   /// Fine-tuning is [currently available](https://platform.openai.com/docs/guides/fine-tuning/what-models-can-be-fine-tuned) for the following models:&#xA;   /// gpt-3.5-turbo-0613 (recommended)&#xA;   /// babbage-002&#xA;   /// davinci-002&#xA;   /// OpenAI expects gpt-3.5-turbo to be the right model for most users in terms of results and ease of use, unless you are migrating a legacy fine-tuned model.&#xA;   public enum Model: String {&#xA;      case gpt35 = &#34;gpt-3.5-turbo-0613&#34; /// recommended&#xA;      case babbage002 = &#34;babbage-002&#34;&#xA;      case davinci002 = &#34;davinci-002&#34;&#xA;   }&#xA;   &#xA;   public struct HyperParameters: Encodable {&#xA;      /// The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.&#xA;      /// Defaults to auto.&#xA;      let nEpochs: Int?&#xA;      &#xA;      public init(&#xA;         nEpochs: Int?)&#xA;      {&#xA;         self.nEpochs = nEpochs&#xA;      }&#xA;   }&#xA;   &#xA;   public init(&#xA;      model: Model,&#xA;      trainingFile: String,&#xA;      hyperparameters: HyperParameters? = nil,&#xA;      suffix: String? = nil,&#xA;      validationFile: String? = nil)&#xA;   {&#xA;      self.model = model.rawValue&#xA;      self.trainingFile = trainingFile&#xA;      self.hyperparameters = hyperparameters&#xA;      self.suffix = suffix&#xA;      self.validationFile = validationFile&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// The fine_tuning.job object represents a [fine-tuning job](https://platform.openai.com/docs/api-reference/fine-tuning/object) that has been created through the API.&#xA;public struct FineTuningJobObject: Decodable {&#xA;   &#xA;   /// The object identifier, which can be referenced in the API endpoints.&#xA;   public let id: String&#xA;   /// The Unix timestamp (in seconds) for when the fine-tuning job was created.&#xA;   public let createdAt: Int&#xA;  /// For fine-tuning jobs that have failed, this will contain more information on the cause of the failure.&#xA;   public let error: OpenAIErrorResponse.Error?&#xA;   /// The name of the fine-tuned model that is being created. The value will be null if the fine-tuning job is still running.&#xA;   public let fineTunedModel: String?&#xA;   /// The Unix timestamp (in seconds) for when the fine-tuning job was finished. The value will be null if the fine-tuning job is still running.&#xA;   public let finishedAt: Int?&#xA;   /// The hyperparameters used for the fine-tuning job. See the [fine-tuning guide](https://platform.openai.com/docs/guides/fine-tuning)  for more details.&#xA;   public let hyperparameters: HyperParameters&#xA;   /// The base model that is being fine-tuned.&#xA;   public let model: String&#xA;   /// The object type, which is always &#34;fine_tuning.job&#34;.&#xA;   public let object: String&#xA;   /// The organization that owns the fine-tuning job.&#xA;   public let organizationId: String&#xA;   /// The compiled results file ID(s) for the fine-tuning job. You can retrieve the results with the [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).&#xA;   public let resultFiles: [String]&#xA;   /// The current status of the fine-tuning job, which can be either `validating_files`, `queued`, `running`, `succeeded`, `failed`, or `cancelled`.&#xA;   public let status: String&#xA;   /// The total number of billable tokens processed by this fine-tuning job. The value will be null if the fine-tuning job is still running.&#xA;   public let trainedTokens: Int?&#xA;   &#xA;   /// The file ID used for training. You can retrieve the training data with the [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).&#xA;   public let trainingFile: String&#xA;   /// The file ID used for validation. You can retrieve the validation results with the [Files API](https://platform.openai.com/docs/api-reference/files/retrieve-contents).&#xA;   public let validationFile: String?&#xA;   &#xA;   public enum Status: String {&#xA;      case validatingFiles = &#34;validating_files&#34;&#xA;      case queued&#xA;      case running&#xA;      case succeeded&#xA;      case failed&#xA;      case cancelled&#xA;   }&#xA;   &#xA;   public struct HyperParameters: Decodable {&#xA;      /// The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset. &#34;auto&#34; decides the optimal number of epochs based on the size of the dataset. If setting the number manually, we support any number between 1 and 50 epochs.&#xA;      public let nEpochs: IntOrStringValue&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage List fine-tuning jobs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fineTuningJobs = try await service.istFineTuningJobs()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create fine-tuning job&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let trainingFileID = &#34;file-Atc9okK0MOuQwQzDJCZXnrh6&#34; // The id of the file that has been uploaded using the `Files` API. https://platform.openai.com/docs/api-reference/fine-tuning/create#fine-tuning/create-training_file&#xA;let parameters = FineTuningJobParameters(model: .gpt35, trainingFile: trainingFileID)&#xA;let fineTuningJob = try await service.createFineTuningJob(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve fine-tuning job&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fineTuningJobID = &#34;ftjob-abc123&#34;&#xA;let fineTuningJob = try await service.retrieveFineTuningJob(id: fineTuningJobID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Cancel fine-tuning job&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fineTuningJobID = &#34;ftjob-abc123&#34;&#xA;let canceledFineTuningJob = try await service.cancelFineTuningJobWith(id: fineTuningJobID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Fine-tuning job event object&lt;/h4&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Fine-tuning job event object](https://platform.openai.com/docs/api-reference/fine-tuning/event-object)&#xA;public struct FineTuningJobEventObject: Decodable {&#xA;   &#xA;   public let id: String&#xA;   &#xA;   public let createdAt: Int&#xA;   &#xA;   public let level: String&#xA;   &#xA;   public let message: String&#xA;   &#xA;   public let object: String&#xA;   &#xA;   public let type: String?&#xA;   &#xA;   public let data: Data?&#xA;   &#xA;   public struct Data: Decodable {&#xA;      public let step: Int&#xA;      public let trainLoss: Double&#xA;      public let trainMeanTokenAccuracy: Double&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fineTuningJobID = &#34;ftjob-abc123&#34;&#xA;let jobEvents = try await service.listFineTuningEventsForJobWith(id: id, after: nil, limit: nil).data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Batch&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct BatchParameter: Encodable {&#xA;   &#xA;   /// The ID of an uploaded file that contains requests for the new batch.&#xA;   /// See [upload file](https://platform.openai.com/docs/api-reference/files/create) for how to upload a file.&#xA;   /// Your input file must be formatted as a [JSONL file](https://platform.openai.com/docs/api-reference/batch/requestInput), and must be uploaded with the purpose batch.&#xA;   let inputFileID: String&#xA;   /// The endpoint to be used for all requests in the batch. Currently only /v1/chat/completions is supported.&#xA;   let endpoint: String&#xA;   /// The time frame within which the batch should be processed. Currently only 24h is supported.&#xA;   let completionWindow: String&#xA;   /// Optional custom metadata for the batch.&#xA;   let metadata: [String: String]?&#xA;   &#xA;   enum CodingKeys: String, CodingKey {&#xA;      case inputFileID = &#34;input_file_id&#34;&#xA;      case endpoint&#xA;      case completionWindow = &#34;completion_window&#34;&#xA;      case metadata&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct BatchObject: Decodable {&#xA;   &#xA;   let id: String&#xA;   /// The object type, which is always batch.&#xA;   let object: String&#xA;   /// The OpenAI API endpoint used by the batch.&#xA;   let endpoint: String&#xA;   &#xA;   let errors: Error&#xA;   /// The ID of the input file for the batch.&#xA;   let inputFileID: String&#xA;   /// The time frame within which the batch should be processed.&#xA;   let completionWindow: String&#xA;   /// The current status of the batch.&#xA;   let status: String&#xA;   /// The ID of the file containing the outputs of successfully executed requests.&#xA;   let outputFileID: String&#xA;   /// The ID of the file containing the outputs of requests with errors.&#xA;   let errorFileID: String&#xA;   /// The Unix timestamp (in seconds) for when the batch was created.&#xA;   let createdAt: Int&#xA;   /// The Unix timestamp (in seconds) for when the batch started processing.&#xA;   let inProgressAt: Int&#xA;   /// The Unix timestamp (in seconds) for when the batch will expire.&#xA;   let expiresAt: Int&#xA;   /// The Unix timestamp (in seconds) for when the batch started finalizing.&#xA;   let finalizingAt: Int&#xA;   /// The Unix timestamp (in seconds) for when the batch was completed.&#xA;   let completedAt: Int&#xA;   /// The Unix timestamp (in seconds) for when the batch failed.&#xA;   let failedAt: Int&#xA;   /// The Unix timestamp (in seconds) for when the batch expired.&#xA;   let expiredAt: Int&#xA;   /// The Unix timestamp (in seconds) for when the batch started cancelling.&#xA;   let cancellingAt: Int&#xA;   /// The Unix timestamp (in seconds) for when the batch was cancelled.&#xA;   let cancelledAt: Int&#xA;   /// The request counts for different statuses within the batch.&#xA;   let requestCounts: RequestCount&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   let metadata: [String: String]&#xA;   &#xA;   public struct Error: Decodable {&#xA;      &#xA;      let object: String&#xA;      let data: [Data]&#xA;&#xA;      public struct Data: Decodable {&#xA;         &#xA;         /// An error code identifying the error type.&#xA;         let code: String&#xA;         /// A human-readable message providing more details about the error.&#xA;         let message: String&#xA;         /// The name of the parameter that caused the error, if applicable.&#xA;         let param: String?&#xA;         /// The line number of the input file where the error occurred, if applicable.&#xA;         let line: Int?&#xA;      }&#xA;   }&#xA;   &#xA;   public struct RequestCount: Decodable {&#xA;      &#xA;      /// Total number of requests in the batch.&#xA;      let total: Int&#xA;      /// Number of requests that have been completed successfully.&#xA;      let completed: Int&#xA;      /// Number of requests that have failed.&#xA;      let failed: Int&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;p&gt;Create batch&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let inputFileID = &#34;file-abc123&#34;&#xA;let endpoint = &#34;/v1/chat/completions&#34;&#xA;let completionWindow = &#34;24h&#34;&#xA;let parameter = BatchParameter(inputFileID: inputFileID, endpoint: endpoint, completionWindow: completionWindow, metadata: nil)&#xA;let batch = try await service.createBatch(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve batch&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let batchID = &#34;batch_abc123&#34;&#xA;let batch = try await service.retrieveBatch(id: batchID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Cancel batch&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let batchID = &#34;batch_abc123&#34;&#xA;let batch = try await service.cancelBatch(id: batchID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;List batch&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let batches = try await service.listBatch(after: nil, limit: nil)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Files&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Upload a file](https://platform.openai.com/docs/api-reference/files/create) that can be used across various endpoints/features. Currently, the size of all the files uploaded by one organization can be up to 1 GB. Please contact us if you need to increase the storage limit.&#xA;public struct FileParameters: Encodable {&#xA;   &#xA;   /// The name of the file asset is not documented in OpenAI&#39;s official documentation; however, it is essential for constructing the multipart request.&#xA;   let fileName: String&#xA;   /// The file object (not file name) to be uploaded.&#xA;   /// If the purpose is set to &#34;fine-tune&#34;, the file will be used for fine-tuning.&#xA;   let file: Data&#xA;   /// The intended purpose of the uploaded file.&#xA;   /// Use &#34;fine-tune&#34; for [fine-tuning](https://platform.openai.com/docs/api-reference/fine-tuning). This allows us to validate the format of the uploaded file is correct for fine-tuning.&#xA;   let purpose: String&#xA;   &#xA;   public init(&#xA;      fileName: String,&#xA;      file: Data,&#xA;      purpose: String)&#xA;   {&#xA;      self.fileName = fileName&#xA;      self.file = file&#xA;      self.purpose = purpose&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// The [File object](https://platform.openai.com/docs/api-reference/files/object) represents a document that has been uploaded to OpenAI.&#xA;public struct FileObject: Decodable {&#xA;   &#xA;   /// The file identifier, which can be referenced in the API endpoints.&#xA;   public let id: String&#xA;   /// The size of the file in bytes.&#xA;   public let bytes: Int&#xA;   /// The Unix timestamp (in seconds) for when the file was created.&#xA;   public let createdAt: Int&#xA;   /// The name of the file.&#xA;   public let filename: String&#xA;   /// The object type, which is always &#34;file&#34;.&#xA;   public let object: String&#xA;   /// The intended purpose of the file. Currently, only &#34;fine-tune&#34; is supported.&#xA;   public let purpose: String&#xA;   /// The current status of the file, which can be either uploaded, processed, pending, error, deleting or deleted.&#xA;   public let status: String&#xA;   /// Additional details about the status of the file. If the file is in the error state, this will include a message describing the error.&#xA;   public let statusDetails: String?&#xA;   &#xA;   public enum Status: String {&#xA;      case uploaded&#xA;      case processed&#xA;      case pending&#xA;      case error&#xA;      case deleting&#xA;      case deleted&#xA;   }&#xA;&#xA;   public init(&#xA;      id: String,&#xA;      bytes: Int,&#xA;      createdAt: Int,&#xA;      filename: String,&#xA;      object: String,&#xA;      purpose: String,&#xA;      status: Status,&#xA;      statusDetails: String?)&#xA;   {&#xA;      self.id = id&#xA;      self.bytes = bytes&#xA;      self.createdAt = createdAt&#xA;      self.filename = filename&#xA;      self.object = object&#xA;      self.purpose = purpose&#xA;      self.status = status.rawValue&#xA;      self.statusDetails = statusDetails&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage List files&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let files = try await service.listFiles().data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upload file&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fileName = &#34;worldCupData.jsonl&#34;&#xA;let data = Data(contentsOfURL:_) // Data retrieved from the file named &#34;worldCupData.jsonl&#34;.&#xA;let parameters = FileParameters(fileName: &#34;WorldCupData&#34;, file: data, purpose: &#34;fine-tune&#34;) // Important: make sure to provide a file name.&#xA;let uploadedFile =  try await service.uploadFile(parameters: parameters) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Delete file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fileID = &#34;file-abc123&#34;&#xA;let deletedStatus = try await service.deleteFileWith(id: fileID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fileID = &#34;file-abc123&#34;&#xA;let retrievedFile = try await service.retrieveFileWith(id: fileID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve file content&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let fileID = &#34;file-abc123&#34;&#xA;let fileContent = try await service.retrieveContentForFileWith(id: fileID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Images&lt;/h3&gt; &#xA;&lt;p&gt;For handling image sizes, we utilize the &lt;code&gt;Dalle&lt;/code&gt; model. An enum with associated values has been defined to represent its size constraints accurately.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/models/dall-e&#34;&gt;DALL·E&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;DALL·E is a AI system that can create realistic images and art from a description in natural language. DALL·E 3 currently supports the ability, given a prompt, to create a new image with a specific size. DALL·E 2 also support the ability to edit an existing image, or create variations of a user provided image.&lt;/p&gt; &#xA;&lt;p&gt;DALL·E 3 is available through our Images API along with DALL·E 2. You can try DALL·E 3 through ChatGPT Plus.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;MODEL&lt;/th&gt; &#xA;   &lt;th&gt;DESCRIPTION&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;dall-e-3&lt;/td&gt; &#xA;   &lt;td&gt;DALL·E 3 New&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The latest DALL·E model released in Nov 2023. Learn more.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;dall-e-2&lt;/td&gt; &#xA;   &lt;td&gt;The previous DALL·E model released in Nov 2022.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The 2nd iteration of DALL·E with more realistic, accurate,&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;and 4x greater resolution images than the original model.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;public enum Dalle {&lt;/p&gt; &#xA;&lt;p&gt;case dalle2(Dalle2ImageSize) case dalle3(Dalle3ImageSize)&lt;/p&gt; &#xA;&lt;p&gt;public enum Dalle2ImageSize: String { case small = &#34;256x256&#34; case medium = &#34;512x512&#34; case large = &#34;1024x1024&#34; }&lt;/p&gt; &#xA;&lt;p&gt;public enum Dalle3ImageSize: String { case largeSquare = &#34;1024x1024&#34; case landscape = &#34;1792x1024&#34; case portrait = &#34;1024x1792&#34; }&lt;/p&gt; &#xA;&lt;p&gt;var model: String { switch self { case .dalle2: return Model.dalle2.rawValue case .dalle3: return Model.dalle3.rawValue } }&lt;/p&gt; &#xA;&lt;p&gt;var size: String { switch self { case .dalle2(let dalle2ImageSize): return dalle2ImageSize.rawValue case .dalle3(let dalle3ImageSize): return dalle3ImageSize.rawValue } } }&lt;/p&gt; &#xA;&lt;h4&gt;Image create&lt;/h4&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct ImageCreateParameters: Encodable {&#xA;   &#xA;   /// A text description of the desired image(s). The maximum length is 1000 characters for dall-e-2 and 4000 characters for dall-e-3.&#xA;   let prompt: String&#xA;   /// The model to use for image generation. Defaults to dall-e-2&#xA;   let model: String?&#xA;   /// The number of images to generate. Must be between 1 and 10. For dall-e-3, only n=1 is supported.&#xA;   let n: Int?&#xA;   /// The quality of the image that will be generated. hd creates images with finer details and greater consistency across the image. This param is only supported for dall-e-3. Defaults to standard&#xA;   let quality: String?&#xA;   /// The format in which the generated images are returned. Must be one of url or b64_json. Defaults to url&#xA;   let responseFormat: String?&#xA;   /// The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024 for dall-e-2. Must be one of 1024x1024, 1792x1024, or 1024x1792 for dall-e-3 models. Defaults to 1024x1024&#xA;   let size: String?&#xA;   /// The style of the generated images. Must be one of vivid or natural. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for dall-e-3. Defaults to vivid&#xA;   let style: String?&#xA;   /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices)&#xA;   let user: String?&#xA;   &#xA;   public init(&#xA;      prompt: String,&#xA;      model: Dalle,&#xA;      numberOfImages: Int = 1,&#xA;      quality: String? = nil,&#xA;      responseFormat: ImageResponseFormat? = nil,&#xA;      style: String? = nil,&#xA;      user: String? = nil)&#xA;   {&#xA;   self.prompt = prompt&#xA;   self.model = model.model&#xA;   self.n = numberOfImages&#xA;   self.quality = quality&#xA;   self.responseFormat = responseFormat?.rawValue&#xA;   self.size = model.size&#xA;   self.style = style&#xA;   self.user = user&#xA;   }   &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Image Edit&lt;/h4&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Creates an edited or extended image given an original image and a prompt.](https://platform.openai.com/docs/api-reference/images/createEdit)&#xA;public struct ImageEditParameters: Encodable {&#xA;   &#xA;   /// The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not provided, image must have transparency, which will be used as the mask.&#xA;   let image: Data&#xA;   /// A text description of the desired image(s). The maximum length is 1000 characters.&#xA;   let prompt: String&#xA;   /// An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where image should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as image.&#xA;   let mask: Data?&#xA;   /// The model to use for image generation. Only dall-e-2 is supported at this time. Defaults to dall-e-2&#xA;   let model: String?&#xA;   /// The number of images to generate. Must be between 1 and 10. Defaults to 1&#xA;   let n: Int?&#xA;   /// The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024. Defaults to 1024x1024&#xA;   let size: String?&#xA;   /// The format in which the generated images are returned. Must be one of url or b64_json. Defaults to url&#xA;   let responseFormat: String?&#xA;   /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices)&#xA;   let user: String?&#xA;   &#xA;   public init(&#xA;      image: UIImage,&#xA;      model: Dalle? = nil,&#xA;      mask: UIImage? = nil,&#xA;      prompt: String,&#xA;      numberOfImages: Int? = nil,&#xA;      responseFormat: ImageResponseFormat? = nil,&#xA;      user: String? = nil)&#xA;   {&#xA;      if (image.pngData() == nil) {&#xA;         assertionFailure(&#34;Failed to get PNG data from image&#34;)&#xA;      }&#xA;      if let mask, mask.pngData() == nil {&#xA;         assertionFailure(&#34;Failed to get PNG data from mask&#34;)&#xA;      }&#xA;      if let model, model.model != Model.dalle2.rawValue {&#xA;         assertionFailure(&#34;Only dall-e-2 is supported at this time [https://platform.openai.com/docs/api-reference/images/createEdit]&#34;)&#xA;      }&#xA;      self.image = image.pngData()!&#xA;      self.model = model?.model&#xA;      self.mask = mask?.pngData()&#xA;      self.prompt = prompt&#xA;      self.n = numberOfImages&#xA;      self.size = model?.size&#xA;      self.responseFormat = responseFormat?.rawValue&#xA;      self.user = user&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Image variation&lt;/h4&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Creates a variation of a given image.](https://platform.openai.com/docs/api-reference/images/createVariation)&#xA;public struct ImageVariationParameters: Encodable {&#xA;   &#xA;   /// The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB, and square.&#xA;   let image: Data&#xA;   /// The model to use for image generation. Only dall-e-2 is supported at this time. Defaults to dall-e-2&#xA;   let model: String?&#xA;   /// The number of images to generate. Must be between 1 and 10. Defaults to 1&#xA;   let n: Int?&#xA;   /// The format in which the generated images are returned. Must be one of url or b64_json. Defaults to url&#xA;   let responseFormat: String?&#xA;   /// The size of the generated images. Must be one of 256x256, 512x512, or 1024x1024. Defaults to 1024x1024&#xA;   let size: String?&#xA;   /// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices)&#xA;   let user: String?&#xA;   &#xA;   public init(&#xA;      image: UIImage,&#xA;      model: Dalle? = nil,&#xA;      numberOfImages: Int? = nil,&#xA;      responseFormat: ImageResponseFormat? = nil,&#xA;      user: String? = nil)&#xA;   {&#xA;      if let model, model.model != Model.dalle2.rawValue {&#xA;         assertionFailure(&#34;Only dall-e-2 is supported at this time [https://platform.openai.com/docs/api-reference/images/createEdit]&#34;)&#xA;      }&#xA;      self.image = image.pngData()!&#xA;      self.n = numberOfImages&#xA;      self.model = model?.model&#xA;      self.size = model?.size&#xA;      self.responseFormat = responseFormat?.rawValue&#xA;      self.user = user&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Represents the url or the content of an image generated by the OpenAI API.](https://platform.openai.com/docs/api-reference/images/object)&#xA;public struct ImageObject: Decodable {&#xA;   /// The URL of the generated image, if response_format is url (default).&#xA;   public let url: URL?&#xA;   /// The base64-encoded JSON of the generated image, if response_format is b64_json.&#xA;   public let b64Json: String?&#xA;   /// The prompt that was used to generate the image, if there was any revision to the prompt.&#xA;   public let revisedPrompt: String?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Create image&#xA;let prompt = &#34;A mix of a dragon and an unicorn&#34;&#xA;let createParameters = ImageCreateParameters(prompt: prompt, model: .dalle3(.largeSquare))&#xA;let imageURLS = try await service.createImages(parameters: createParameters).data.map(\.url)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Edit image&#xA;let data = Data(contentsOfURL:_) // the data from an image.&#xA;let image = UIImage(data: data)&#xA;let prompt = &#34;Add a background filled with pink balloons.&#34;&#xA;let editParameters = ImageEditParameters(image: image, prompt: prompt, numberOfImages: 4)  &#xA;let imageURLS = try await service.editImage(parameters: parameters).data.map(\.url)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Image variations&#xA;let data = Data(contentsOfURL:_) // the data from an image.&#xA;let image = UIImage(data: data)&#xA;let variationParameters = ImageVariationParameters(image: image, numberOfImages: 4)&#xA;let imageURLS = try await service.createImageVariations(parameters: parameters).data.map(\.url)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;&#xA;/// Describes an OpenAI [model](https://platform.openai.com/docs/api-reference/models/object) offering that can be used with the API.&#xA;public struct ModelObject: Decodable {&#xA;   &#xA;   /// The model identifier, which can be referenced in the API endpoints.&#xA;   public let id: String&#xA;   /// The Unix timestamp (in seconds) when the model was created.&#xA;   public let created: Int&#xA;   /// The object type, which is always &#34;model&#34;.&#xA;   public let object: String&#xA;   /// The organization that owns the model.&#xA;   public let ownedBy: String&#xA;   /// An array representing the current permissions of a model. Each element in the array corresponds to a specific permission setting. If there are no permissions or if the data is unavailable, the array may be nil.&#xA;   public let permission: [Permission]?&#xA;   &#xA;   public struct Permission: Decodable {&#xA;      public let id: String?&#xA;      public let object: String?&#xA;      public let created: Int?&#xA;      public let allowCreateEngine: Bool?&#xA;      public let allowSampling: Bool?&#xA;      public let allowLogprobs: Bool?&#xA;      public let allowSearchIndices: Bool?&#xA;      public let allowView: Bool?&#xA;      public let allowFineTuning: Bool?&#xA;      public let organization: String?&#xA;      public let group: String?&#xA;      public let isBlocking: Bool?&#xA;   }&#xA;   &#xA;   /// Represents the response from the [delete](https://platform.openai.com/docs/api-reference/models/delete) fine-tuning API&#xA;   public struct DeletionStatus: Decodable {&#xA;      &#xA;      public let id: String&#xA;      public let object: String&#xA;      public let deleted: Bool&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// List models&#xA;let models = try await service.listModels().data&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Retrieve model&#xA;let modelID = &#34;gpt-3.5-turbo-instruct&#34;&#xA;let retrievedModel = try await service.retrieveModelWith(id: modelID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Delete fine tuned model&#xA;let modelID = &#34;fine-tune-model-id&#34;&#xA;let deletionStatus = try await service.deleteFineTuneModelWith(id: modelID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Moderations&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// [Classifies if text violates OpenAI&#39;s Content Policy.](https://platform.openai.com/docs/api-reference/moderations/create)&#xA;public struct ModerationParameter&amp;lt;Input: Encodable&amp;gt;: Encodable {&#xA;   &#xA;   /// The input text to classify, string or array.&#xA;   let input: Input&#xA;   /// Two content moderations models are available: text-moderation-stable and text-moderation-latest.&#xA;   /// The default is text-moderation-latest which will be automatically upgraded over time. This ensures you are always using our most accurate model. If you use text-moderation-stable, we will provide advanced notice before updating the model. Accuracy of text-moderation-stable may be slightly lower than for text-moderation-latest.&#xA;   let model: String?&#xA;   &#xA;   enum Model: String {&#xA;      case stable = &#34;text-moderation-stable&#34;&#xA;      case latest = &#34;text-moderation-latest&#34;&#xA;   }&#xA;   &#xA;   init(&#xA;      input: Input,&#xA;      model: Model? = nil)&#xA;   {&#xA;      self.input = input&#xA;      self.model = model?.rawValue&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// The [moderation object](https://platform.openai.com/docs/api-reference/moderations/object). Represents policy compliance report by OpenAI&#39;s content moderation model against a given input.&#xA;public struct ModerationObject: Decodable {&#xA;   &#xA;   /// The unique identifier for the moderation request.&#xA;   public let id: String&#xA;   /// The model used to generate the moderation results.&#xA;   public let model: String&#xA;   /// A list of moderation objects.&#xA;   public let results: [Moderation]&#xA;   &#xA;   public struct Moderation: Decodable {&#xA;      &#xA;      /// Whether the content violates OpenAI&#39;s usage policies.&#xA;      public let flagged: Bool&#xA;      /// A list of the categories, and whether they are flagged or not.&#xA;      public let categories: Category&amp;lt;Bool&amp;gt;&#xA;      /// A list of the categories along with their scores as predicted by model.&#xA;      public let categoryScores: Category&amp;lt;Double&amp;gt;&#xA;      &#xA;      public struct Category&amp;lt;T: Decodable&amp;gt;: Decodable {&#xA;         &#xA;         /// Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harrassment.&#xA;         public let hate: T&#xA;         /// Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.&#xA;         public let hateThreatening: T&#xA;         /// Content that expresses, incites, or promotes harassing language towards any target.&#xA;         public let harassment: T&#xA;         /// Harassment content that also includes violence or serious harm towards any target.&#xA;         public let harassmentThreatening: T&#xA;         /// Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.&#xA;         public let selfHarm: T&#xA;         /// Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.&#xA;         public let selfHarmIntent: T&#xA;         /// Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.&#xA;         public let selfHarmInstructions: T&#xA;         /// Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).&#xA;         public let sexual: T&#xA;         /// Sexual content that includes an individual who is under 18 years old.&#xA;         public let sexualMinors: T&#xA;         /// Content that depicts death, violence, or physical injury.&#xA;         public let violence: T&#xA;         /// Content that depicts death, violence, or physical injury in graphic detail.&#xA;         public let violenceGraphic: T&#xA;      }&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Single prompt&#xA;let prompt = &#34;I am going to kill him&#34;&#xA;let parameters = ModerationParameter(input: prompt)&#xA;let isFlagged = try await service.createModerationFromText(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Multiple prompts&#xA;let prompts = [&#34;I am going to kill him&#34;, &#34;I am going to die&#34;]&#xA;let parameters = ModerationParameter(input: prompts)&#xA;let isFlagged = try await service.createModerationFromTexts(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;&lt;strong&gt;BETA&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Assistants&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Create an [assistant](https://platform.openai.com/docs/api-reference/assistants/createAssistant) with a model and instructions.&#xA;/// Modifies an [assistant](https://platform.openai.com/docs/api-reference/assistants/modifyAssistant).&#xA;public struct AssistantParameters: Encodable {&#xA;   &#xA;   /// ID of the model to use. You can use the [List models](https://platform.openai.com/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](https://platform.openai.com/docs/models/overview) for descriptions of them.&#xA;   public var model: String?&#xA;   /// The name of the assistant. The maximum length is 256 characters.&#xA;   public var name: String?&#xA;   /// The description of the assistant. The maximum length is 512 characters.&#xA;   public var description: String?&#xA;   /// The system instructions that the assistant uses. The maximum length is 32768 characters.&#xA;   public var instructions: String?&#xA;   /// A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types code_interpreter, retrieval, or function. Defaults to []&#xA;   public var tools: [AssistantObject.Tool] = []&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   public var metadata: [String: String]?&#xA;   /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.&#xA;   /// Defaults to 1&#xA;   public var temperature: Double?&#xA;   /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.&#xA;  /// We generally recommend altering this or temperature but not both.&#xA;   /// Defaults to 1&#xA;   public var topP: Double?&#xA;   /// Specifies the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models since gpt-3.5-turbo-1106.&#xA;   /// Setting to { &#34;type&#34;: &#34;json_object&#34; } enables JSON mode, which guarantees the message the model generates is valid JSON.&#xA;   /// Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly &#34;stuck&#34; request. Also note that the message content may be partially cut off if finish_reason=&#34;length&#34;, which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.&#xA;   /// Defaults to `auto`&#xA;   public var responseFormat: ResponseFormat?&#xA;   &#xA;   public enum Action {&#xA;      case create(model: String) // model is required on creation of assistant.&#xA;      case modify(model: String?) // model is optional on modification of assistant.&#xA;      &#xA;      var model: String? {&#xA;         switch self {&#xA;         case .create(let model): return model&#xA;         case .modify(let model): return model&#xA;         }&#xA;      }&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Represents an [assistant](https://platform.openai.com/docs/api-reference/assistants) that can call the model and use tools.&#xA;public struct AssistantObject: Decodable {&#xA;   &#xA;   /// The identifier, which can be referenced in API endpoints.&#xA;   public let id: String&#xA;   /// The object type, which is always &#34;assistant&#34;.&#xA;   public let object: String&#xA;   /// The Unix timestamp (in seconds) for when the assistant was created.&#xA;   public let createdAt: Int&#xA;   /// The name of the assistant. The maximum length is 256 characters.&#xA;   public let name: String?&#xA;   /// The description of the assistant. The maximum length is 512 characters.&#xA;   public let description: String?&#xA;   /// ID of the model to use. You can use the [List models](https://platform.openai.com/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](https://platform.openai.com/docs/models/overview) for descriptions of them.&#xA;   public let model: String&#xA;   /// The system instructions that the assistant uses. The maximum length is 32768 characters.&#xA;   public let instructions: String?&#xA;   /// A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types code_interpreter, retrieval, or function.&#xA;   public let tools: [Tool]&#xA;   /// A list of [file](https://platform.openai.com/docs/api-reference/files) IDs attached to this assistant. There can be a maximum of 20 files attached to the assistant. Files are ordered by their creation date in ascending order.&#xA;   /// A set of resources that are used by the assistant&#39;s tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs.&#xA;   public let toolResources: ToolResources?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   public let metadata: [String: String]?&#xA;   /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.&#xA;   /// Defaults to 1&#xA;   public var temperature: Double?&#xA;   /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.&#xA;  /// We generally recommend altering this or temperature but not both.&#xA;   /// Defaults to 1&#xA;   public var topP: Double?&#xA;   /// Specifies the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models since gpt-3.5-turbo-1106.&#xA;   /// Setting to { &#34;type&#34;: &#34;json_object&#34; } enables JSON mode, which guarantees the message the model generates is valid JSON.&#xA;   /// Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly &#34;stuck&#34; request. Also note that the message content may be partially cut off if finish_reason=&#34;length&#34;, which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.&#xA;   /// Defaults to `auto`&#xA;   public var responseFormat: ResponseFormat?&#xA;&#xA;   public struct Tool: Codable {&#xA;      &#xA;      /// The type of tool being defined.&#xA;      public let type: String&#xA;      public let function: ChatCompletionParameters.ChatFunction?&#xA;      &#xA;      public enum ToolType: String, CaseIterable {&#xA;         case codeInterpreter = &#34;code_interpreter&#34;&#xA;         case fileSearch = &#34;file_search&#34;&#xA;         case function&#xA;      }&#xA;      &#xA;      /// Helper.&#xA;      public var displayToolType: ToolType? { .init(rawValue: type) }&#xA;      &#xA;      public init(&#xA;         type: ToolType,&#xA;         function: ChatCompletionParameters.ChatFunction? = nil)&#xA;      {&#xA;         self.type = type.rawValue&#xA;         self.function = function&#xA;      }&#xA;   }&#xA;   &#xA;   public struct DeletionStatus: Decodable {&#xA;      public let id: String&#xA;      public let object: String&#xA;      public let deleted: Bool&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;p&gt;Create Assistant&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let parameters = AssistantParameters(action: .create(model: Model.gpt41106Preview.rawValue), name: &#34;Math tutor&#34;)&#xA;let assistant = try await service.createAssistant(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve Assistant&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let assistantID = &#34;asst_abc123&#34;&#xA;let assistant = try await service.retrieveAssistant(id: assistantID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify Assistant&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let assistantID = &#34;asst_abc123&#34;&#xA;let parameters = AssistantParameters(action: .modify, name: &#34;Math tutor for kids&#34;)&#xA;let assistant = try await service.modifyAssistant(id: assistantID, parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Delete Assistant&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let assistantID = &#34;asst_abc123&#34;&#xA;let deletionStatus = try await service.deleteAssistant(id: assistantID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;List Assistants&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let assistants = try await service.listAssistants()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Threads&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Create a [Thread](https://platform.openai.com/docs/api-reference/threads/createThread)&#xA;public struct CreateThreadParameters: Encodable {&#xA;   &#xA;   /// A list of [messages](https://platform.openai.com/docs/api-reference/messages) to start the thread with.&#xA;   public var messages: [MessageObject]?&#xA;      /// A set of resources that are used by the assistant&#39;s tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs.&#xA;   public var toolResources: ToolResources?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   public var metadata: [String: String]?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// A [thread object](https://platform.openai.com/docs/api-reference/threads) represents a thread that contains [messages](https://platform.openai.com/docs/api-reference/messages).&#xA;public struct ThreadObject: Decodable {&#xA;   &#xA;   /// The identifier, which can be referenced in API endpoints.&#xA;   public let id: String&#xA;   /// The object type, which is always thread.&#xA;   public let object: String&#xA;   /// The Unix timestamp (in seconds) for when the thread was created.&#xA;   public let createdAt: Int&#xA;   /// A set of resources that are used by the assistant&#39;s tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs.&#xA;   public var toolResources: ToolResources?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   public let metadata: [String: String]&#xA;   &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;p&gt;Create thread.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let parameters = CreateThreadParameters()&#xA;let thread = try await service.createThread(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve thread.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let thread = try await service.retrieveThread(id: id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify thread.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let paramaters = CreateThreadParameters(metadata: [&#34;modified&#34;: &#34;true&#34;, &#34;user&#34;: &#34;abc123&#34;]&#xA;let thread = try await service.modifyThread(id: id, parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Delete thread.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let thread = try await service.deleteThread(id: id)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Messages&lt;/h3&gt; &#xA;&lt;p&gt;Parameters &lt;a href=&#34;https://platform.openai.com/docs/api-reference/messages/createMessage&#34;&gt;Create a Message&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct MessageParameter: Encodable {&#xA;   &#xA;   /// The role of the entity that is creating the message. Allowed values include:&#xA;   /// user: Indicates the message is sent by an actual user and should be used in most cases to represent user-generated messages.&#xA;   /// assistant: Indicates the message is generated by the assistant. Use this value to insert messages from the assistant into the conversation.&#xA;   let role: String&#xA;   /// The content of the message.&#xA;   let content: String&#xA;   /// A list of files attached to the message, and the tools they should be added to.&#xA;   let attachments: [MessageAttachment]?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   let metadata: [String: String]?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/messages/modifyMessage&#34;&gt;Modify a Message&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct ModifyMessageParameters: Encodable {&#xA;   &#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   public var metadata: [String: String]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Represents a [message](https://platform.openai.com/docs/api-reference/messages) within a [thread](https://platform.openai.com/docs/api-reference/threads).&#xA;public struct MessageObject: Codable {&#xA;   &#xA;   /// The identifier, which can be referenced in API endpoints.&#xA;   public let id: String&#xA;   /// The object type, which is always thread.message.&#xA;   public let object: String&#xA;   /// The Unix timestamp (in seconds) for when the message was created.&#xA;   public let createdAt: Int&#xA;   /// The [thread](https://platform.openai.com/docs/api-reference/threads) ID that this message belongs to.&#xA;   public let threadID: String&#xA;   /// The status of the message, which can be either in_progress, incomplete, or completed.&#xA;   public let status: String&#xA;   /// On an incomplete message, details about why the message is incomplete.&#xA;   public let incompleteDetails: IncompleteDetails?&#xA;   /// The Unix timestamp (in seconds) for when the message was completed.&#xA;   public let completedAt: Int&#xA;   /// The entity that produced the message. One of user or assistant.&#xA;   public let role: String&#xA;   /// The content of the message in array of text and/or images.&#xA;   public let content: [MessageContent]&#xA;   /// If applicable, the ID of the [assistant](https://platform.openai.com/docs/api-reference/assistants) that authored this message.&#xA;   public let assistantID: String?&#xA;   /// If applicable, the ID of the [run](https://platform.openai.com/docs/api-reference/runs) associated with the authoring of this message.&#xA;   public let runID: String?&#xA;   /// A list of files attached to the message, and the tools they were added to.&#xA;   public let attachments: [MessageAttachment]?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   public let metadata: [String: String]?&#xA;   &#xA;   enum Role: String {&#xA;      case user&#xA;      case assistant&#xA;   }&#xA;}&#xA;&#xA;// MARK: MessageContent&#xA;&#xA;public enum MessageContent: Codable {&#xA;   &#xA;   case imageFile(ImageFile)&#xA;   case text(Text)&#xA;}&#xA;&#xA;// MARK: Image File&#xA;&#xA;public struct ImageFile: Codable {&#xA;   /// Always image_file.&#xA;   public let type: String&#xA;   &#xA;   /// References an image [File](https://platform.openai.com/docs/api-reference/files) in the content of a message.&#xA;   public let imageFile: ImageFileContent&#xA;   &#xA;   public struct ImageFileContent: Codable {&#xA;      &#xA;      /// The [File](https://platform.openai.com/docs/api-reference/files) ID of the image in the message content.&#xA;      public let fileID: String&#xA;   }&#xA;}&#xA;&#xA;// MARK: Text&#xA;&#xA;public struct Text: Codable {&#xA;   &#xA;   /// Always text.&#xA;   public let type: String&#xA;   /// The text content that is part of a message.&#xA;   public let text: TextContent&#xA;   &#xA;   public struct TextContent: Codable {&#xA;      // The data that makes up the text.&#xA;      public let value: String&#xA;      &#xA;      public let annotations: [Annotation]&#xA;   }&#xA;}&#xA;&#xA;// MARK: Annotation&#xA;&#xA;public enum Annotation: Codable {&#xA;   &#xA;   case fileCitation(FileCitation)&#xA;   case filePath(FilePath)&#xA;}&#xA;&#xA;// MARK: FileCitation&#xA;&#xA;/// A citation within the message that points to a specific quote from a specific File associated with the assistant or the message. Generated when the assistant uses the &#34;retrieval&#34; tool to search files.&#xA;public struct FileCitation: Codable {&#xA;   &#xA;   /// Always file_citation.&#xA;   public let type: String&#xA;   /// The text in the message content that needs to be replaced.&#xA;   public let text: String&#xA;   public let fileCitation: FileCitation&#xA;   public  let startIndex: Int&#xA;   public let endIndex: Int&#xA;   &#xA;   public struct FileCitation: Codable {&#xA;      &#xA;      /// The ID of the specific File the citation is from.&#xA;      public let fileID: String&#xA;      /// The specific quote in the file.&#xA;      public let quote: String&#xA;&#xA;   }&#xA;}&#xA;&#xA;// MARK: FilePath&#xA;&#xA;/// A URL for the file that&#39;s generated when the assistant used the code_interpreter tool to generate a file.&#xA;public struct FilePath: Codable {&#xA;   &#xA;   /// Always file_path&#xA;   public let type: String&#xA;   /// The text in the message content that needs to be replaced.&#xA;   public let text: String&#xA;   public let filePath: FilePath&#xA;   public let startIndex: Int&#xA;   public let endIndex: Int&#xA;   &#xA;   public struct FilePath: Codable {&#xA;      /// The ID of the file that was generated.&#xA;      public let fileID: String&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;p&gt;Create Message.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let prompt = &#34;Give me some ideas for a birthday party.&#34;&#xA;let parameters = MessageParameter(role: &#34;user&#34;, content: prompt&#34;)&#xA;let message = try await service.createMessage(threadID: threadID, parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve Message.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let messageID = &#34;msg_abc123&#34;&#xA;let message = try await service.retrieveMessage(threadID: threadID, messageID: messageID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify Message.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let messageID = &#34;msg_abc123&#34;&#xA;let parameters = ModifyMessageParameters(metadata: [&#34;modified&#34;: &#34;true&#34;, &#34;user&#34;: &#34;abc123&#34;]&#xA;let message = try await service.modifyMessage(threadID: threadID, messageID: messageID, parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;List Messages&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let messages = try await service.listMessages(threadID: threadID, limit: nil, order: nil, after: nil, before: nil) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Runs&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/createRun&#34;&gt;Create a run&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct RunParameter: Encodable {&#xA;   &#xA;   /// The ID of the [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to execute this run.&#xA;    let assistantID: String&#xA;   /// The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to be used to execute this run. If a value is provided here, it will override the model associated with the assistant. If not, the model associated with the assistant will be used.&#xA;   let model: String?&#xA;   /// Override the default system message of the assistant. This is useful for modifying the behavior on a per-run basis.&#xA;   let instructions: String?&#xA;   /// Appends additional instructions at the end of the instructions for the run. This is useful for modifying the behavior on a per-run basis without overriding other instructions.&#xA;   let additionalInstructions: String?&#xA;   /// Adds additional messages to the thread before creating the run.&#xA;   let additionalMessages: [MessageParameter]?&#xA;   /// Override the tools the assistant can use for this run. This is useful for modifying the behavior on a per-run basis.&#xA;   let tools: [AssistantObject.Tool]?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   let metadata: [String: String]?&#xA;   /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.&#xA;   /// Optional Defaults to 1&#xA;   let temperature: Double?&#xA;   /// If true, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a data: [DONE] message.&#xA;   var stream: Bool&#xA;   /// The maximum number of prompt tokens that may be used over the course of the run. The run will make a best effort to use only the number of prompt tokens specified, across multiple turns of the run. If the run exceeds the number of prompt tokens specified, the run will end with status complete. See incomplete_details for more info.&#xA;   let maxPromptTokens: Int?&#xA;   /// The maximum number of completion tokens that may be used over the course of the run. The run will make a best effort to use only the number of completion tokens specified, across multiple turns of the run. If the run exceeds the number of completion tokens specified, the run will end with status complete. See incomplete_details for more info.&#xA;   let maxCompletionTokens: Int?&#xA;   /// Controls for how a thread will be truncated prior to the run. Use this to control the intial context window of the run.&#xA;   let truncationStrategy: TruncationStrategy?&#xA;   /// Controls which (if any) tool is called by the model. none means the model will not call any tools and instead generates a message. auto is the default value and means the model can pick between generating a message or calling a tool. Specifying a particular tool like {&#34;type&#34;: &#34;file_search&#34;} or {&#34;type&#34;: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;my_function&#34;}} forces the model to call that tool.&#xA;   let toolChoice: ToolChoice?&#xA;   /// Specifies the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.&#xA;   /// Setting to { &#34;type&#34;: &#34;json_object&#34; } enables JSON mode, which guarantees the message the model generates is valid JSON.&#xA;   /// Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly &#34;stuck&#34; request. Also note that the message content may be partially cut off if finish_reason=&#34;length&#34;, which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.&#xA;   let responseFormat: ResponseFormat?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/modifyRun&#34;&gt;Modify a Run&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct ModifyRunParameters: Encodable {&#xA;   &#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   public var metadata: [String: String]&#xA;   &#xA;   public init(&#xA;      metadata: [String : String])&#xA;   {&#xA;      self.metadata = metadata&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/createThreadAndRun&#34;&gt;Creates a Thread and Runs.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct CreateThreadAndRunParameter: Encodable {&#xA;   &#xA;   /// The ID of the [assistant](https://platform.openai.com/docs/api-reference/assistants) to use to execute this run.&#xA;   let assistantId: String&#xA;   /// A thread to create.&#xA;   let thread: CreateThreadParameters?&#xA;   /// The ID of the [Model](https://platform.openai.com/docs/api-reference/models) to be used to execute this run. If a value is provided here, it will override the model associated with the assistant. If not, the model associated with the assistant will be used.&#xA;   let model: String?&#xA;   /// Override the default system message of the assistant. This is useful for modifying the behavior on a per-run basis.&#xA;   let instructions: String?&#xA;   /// Override the tools the assistant can use for this run. This is useful for modifying the behavior on a per-run basis.&#xA;   let tools: [AssistantObject.Tool]?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   let metadata: [String: String]?&#xA;   /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.&#xA;   /// Defaults to 1&#xA;   let temperature: Double?&#xA;   /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.&#xA;   /// We generally recommend altering this or temperature but not both.&#xA;   let topP: Double?&#xA;   /// If true, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a data: [DONE] message.&#xA;   var stream: Bool = false&#xA;   /// The maximum number of prompt tokens that may be used over the course of the run. The run will make a best effort to use only the number of prompt tokens specified, across multiple turns of the run. If the run exceeds the number of prompt tokens specified, the run will end with status incomplete. See incomplete_details for more info.&#xA;   let maxPromptTokens: Int?&#xA;   /// The maximum number of completion tokens that may be used over the course of the run. The run will make a best effort to use only the number of completion tokens specified, across multiple turns of the run. If the run exceeds the number of completion tokens specified, the run will end with status complete. See incomplete_details for more info.&#xA;   let maxCompletionTokens: Int?&#xA;   /// Controls for how a thread will be truncated prior to the run. Use this to control the intial context window of the run.&#xA;   let truncationStrategy: TruncationStrategy?&#xA;   /// Controls which (if any) tool is called by the model. none means the model will not call any tools and instead generates a message. auto is the default value and means the model can pick between generating a message or calling a tool. Specifying a particular tool like {&#34;type&#34;: &#34;file_search&#34;} or {&#34;type&#34;: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;my_function&#34;}} forces the model to call that tool.&#xA;   let toolChoice: ToolChoice?&#xA;   /// Specifies the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.&#xA;   /// Setting to { &#34;type&#34;: &#34;json_object&#34; } enables JSON mode, which guarantees the message the model generates is valid JSON.&#xA;   /// Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly &#34;stuck&#34; request. Also note that the message content may be partially cut off if finish_reason=&#34;length&#34;, which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.&#xA;   let responseFormat: ResponseFormat?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/submitToolOutputs&#34;&gt;Submit tool outputs to run&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct RunToolsOutputParameter: Encodable {&#xA;   &#xA;   /// A list of tools for which the outputs are being submitted.&#xA;   public let toolOutputs: [ToolOutput]&#xA;   /// If true, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a data: [DONE] message.&#xA;   public let stream: Bool&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct RunObject: Decodable {&#xA;   &#xA;   /// The identifier, which can be referenced in API endpoints.&#xA;   public let id: String&#xA;   /// The object type, which is always thread.run.&#xA;   public let object: String&#xA;   /// The Unix timestamp (in seconds) for when the run was created.&#xA;   public let createdAt: Int?&#xA;   /// The ID of the [thread](https://platform.openai.com/docs/api-reference/threads) that was executed on as a part of this run.&#xA;   public let threadID: String&#xA;   /// The ID of the [assistant](https://platform.openai.com/docs/api-reference/assistants) used for execution of this run.&#xA;   public let assistantID: String&#xA;   /// The status of the run, which can be either queued, in_progress, requires_action, cancelling, cancelled, failed, completed, or expired.&#xA;   public let status: String&#xA;   /// Details on the action required to continue the run. Will be null if no action is required.&#xA;   public let requiredAction: RequiredAction?&#xA;   /// The last error associated with this run. Will be null if there are no errors.&#xA;   public let lastError: LastError?&#xA;   /// The Unix timestamp (in seconds) for when the run will expire.&#xA;   public let expiresAt: Int?&#xA;   /// The Unix timestamp (in seconds) for when the run was started.&#xA;   public let startedAt: Int?&#xA;   /// The Unix timestamp (in seconds) for when the run was cancelled.&#xA;   public let cancelledAt: Int?&#xA;   /// The Unix timestamp (in seconds) for when the run failed.&#xA;   public let failedAt: Int?&#xA;   /// The Unix timestamp (in seconds) for when the run was completed.&#xA;   public let completedAt: Int?&#xA;   /// Details on why the run is incomplete. Will be null if the run is not incomplete.&#xA;   public let incompleteDetails: IncompleteDetails?&#xA;   /// The model that the [assistant](https://platform.openai.com/docs/api-reference/assistants) used for this run.&#xA;   public let model: String&#xA;   /// The instructions that the [assistant](https://platform.openai.com/docs/api-reference/assistants) used for this run.&#xA;   public let instructions: String?&#xA;   /// The list of tools that the [assistant](https://platform.openai.com/docs/api-reference/assistants) used for this run.&#xA;   public let tools: [AssistantObject.Tool]&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   public let metadata: [String: String]&#xA;   /// Usage statistics related to the run. This value will be null if the run is not in a terminal state (i.e. in_progress, queued, etc.).&#xA;   public let usage: Usage?&#xA;   /// The sampling temperature used for this run. If not set, defaults to 1.&#xA;   public let temperature: Double?&#xA;   /// The nucleus sampling value used for this run. If not set, defaults to 1.&#xA;   public let topP: Double?&#xA;   /// The maximum number of prompt tokens specified to have been used over the course of the run.&#xA;   public let maxPromptTokens: Int?&#xA;   /// The maximum number of completion tokens specified to have been used over the course of the run.&#xA;   public let maxCompletionTokens: Int?&#xA;   /// Controls for how a thread will be truncated prior to the run. Use this to control the intial context window of the run.&#xA;   public let truncationStrategy: TruncationStrategy?&#xA;   /// Controls which (if any) tool is called by the model. none means the model will not call any tools and instead generates a message. auto is the default value and means the model can pick between generating a message or calling a tool. Specifying a particular tool like {&#34;type&#34;: &#34;TOOL_TYPE&#34;} or {&#34;type&#34;: &#34;function&#34;, &#34;function&#34;: {&#34;name&#34;: &#34;my_function&#34;}} forces the model to call that tool.&#xA;   public let toolChoice: ToolChoice?&#xA;   /// Specifies the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.&#xA;   /// Setting to { &#34;type&#34;: &#34;json_object&#34; } enables JSON mode, which guarantees the message the model generates is valid JSON.&#xA;   /// Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly &#34;stuck&#34; request. Also note that the message content may be partially cut off if finish_reason=&#34;length&#34;, which indicates the generation exceeded max_tokens or the conversation exceeded the max context length.&#xA;   public let responseFormat: ResponseFormat?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;p&gt;Create a Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let assistantID = &#34;asst_abc123&#34;&#xA;ler parameters = RunParameter(assistantID: assistantID)&#xA;let run = try await service.createRun(threadID: threadID, parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Retrieve a Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let runID = &#34;run_abc123&#34;&#xA;let run = try await service.retrieveRun(threadID: threadID, runID: runID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify a Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let runID = &#34;run_abc123&#34;&#xA;let parameters = ModifyRunParameters(metadata: [&#34;modified&#34;: &#34;true&#34;, &#34;user&#34;: &#34;abc123&#34;]&#xA;let message = try await service.modifyRun(threadID: threadID, messageID: messageID, parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;List runs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let runs = try await service.listRuns(threadID: threadID, limit: nil, order: nil, after: nil, before: nil) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Submit tool outputs to Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let runID = &#34;run_abc123&#34;&#xA;let toolCallID = &#34;call_abc123&#34;&#xA;let output = &#34;28C&#34;&#xA;let parameters = RunToolsOutputParameter(toolOutputs: [.init(toolCallId: toolCallID, output: output)])&#xA;let run = try await service.submitToolOutputsToRun(threadID: threadID&#34;, runID: runID&#34;, parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Cancel a Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;/// Cancels a run that is in_progress.&#xA;let threadID = &#34;thread_abc123&#34;&#xA;let runID = &#34;run_abc123&#34;&#xA;let run = try await service.cancelRun(threadID: threadID, runID: runID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create thread and Run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let assistantID = &#34;asst_abc123&#34;&#xA;let parameters = CreateThreadAndRunParameter(assistantID: assistantID)&#xA;let run = service.createThreadAndRun(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Step Object&lt;/h3&gt; &#xA;&lt;p&gt;Represents a &lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/step-object&#34;&gt;step&lt;/a&gt; in execution of a run. Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct RunStepObject: Decodable {&#xA;   &#xA;   /// The identifier of the run step, which can be referenced in API endpoints.&#xA;   public let id: String&#xA;   /// The object type, which is always `thread.run.step``.&#xA;   public let object: String&#xA;   /// The Unix timestamp (in seconds) for when the run step was created.&#xA;   public let createdAt: Int&#xA;   /// The ID of the [assistant](https://platform.openai.com/docs/api-reference/assistants) associated with the run step.&#xA;   public let assistantId: String&#xA;   /// The ID of the [thread](https://platform.openai.com/docs/api-reference/threads) that was run.&#xA;   public let threadId: String&#xA;   ///The ID of the [run](https://platform.openai.com/docs/api-reference/runs) that this run step is a part of.&#xA;   public let runId: String&#xA;   /// The type of run step, which can be either message_creation or tool_calls.&#xA;   public let type: String&#xA;   /// The status of the run step, which can be either in_progress, cancelled, failed, completed, or expired.&#xA;   public let status: String&#xA;   /// The details of the run step.&#xA;   public let stepDetails: RunStepDetails&#xA;   /// The last error associated with this run step. Will be null if there are no errors.&#xA;   public let lastError: RunObject.LastError?&#xA;   /// The Unix timestamp (in seconds) for when the run step expired. A step is considered expired if the parent run is expired.&#xA;   public let expiredAt: Int?&#xA;   /// The Unix timestamp (in seconds) for when the run step was cancelled.&#xA;   public let cancelledAt: Int?&#xA;   /// The Unix timestamp (in seconds) for when the run step failed.&#xA;   public let failedAt: Int?&#xA;   /// The Unix timestamp (in seconds) for when the run step completed.&#xA;   public let completedAt: Int?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   public let metadata: [String: String]?&#xA;   /// Usage statistics related to the run step. This value will be null while the run step&#39;s status is in_progress.&#xA;   public let usage: Usage?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage Retrieve a Run step&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let runID = &#34;run_abc123&#34;&#xA;let stepID = &#34;step_abc123&#34;&#xA;let runStep = try await service.retrieveRunstep(threadID: threadID, runID: runID, stepID: stepID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;List run steps&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let threadID = &#34;thread_abc123&#34;&#xA;let runID = &#34;run_abc123&#34;&#xA;let runSteps = try await service.listRunSteps(threadID: threadID, runID: runID, limit: nil, order: nil, after: nil, before: nil) &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Step Detail&lt;/h3&gt; &#xA;&lt;p&gt;The details of the run step.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct RunStepDetails: Codable {&#xA;   &#xA;   /// `message_creation` or `tool_calls`&#xA;   public let type: String&#xA;   /// Details of the message creation by the run step.&#xA;   public let messageCreation: MessageCreation?&#xA;   /// Details of the tool call.&#xA;   public let toolCalls: [ToolCall]?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Assistants Streaming&lt;/h3&gt; &#xA;&lt;p&gt;Assistants API &lt;a href=&#34;https://platform.openai.com/docs/api-reference/assistants-streaming&#34;&gt;streaming.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stream the result of executing a Run or resuming a Run after submitting tool outputs.&lt;/p&gt; &#xA;&lt;p&gt;You can stream events from the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/createThreadAndRun&#34;&gt;Create Thread and Run&lt;/a&gt;, &lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/createRun&#34;&gt;Create Run&lt;/a&gt;, and &lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/submitToolOutputs&#34;&gt;Submit Tool Outputs&lt;/a&gt; endpoints by passing &#34;stream&#34;: true. The response will be a Server-Sent events stream.&lt;/p&gt; &#xA;&lt;p&gt;OpenAI Python tutorial(&lt;a href=&#34;https://platform.openai.com/docs/assistants/overview?context=with-streaming&#34;&gt;https://platform.openai.com/docs/assistants/overview?context=with-streaming&lt;/a&gt;))&lt;/p&gt; &#xA;&lt;h3&gt;Message Delta Object&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/assistants-streaming/message-delta-object&#34;&gt;MessageDeltaObject&lt;/a&gt; Represents a message delta i.e. any changed fields on a message during streaming.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct MessageDeltaObject: Decodable {&#xA;   &#xA;   /// The identifier of the message, which can be referenced in API endpoints.&#xA;   public let id: String&#xA;   /// The object type, which is always thread.message.delta.&#xA;   public let object: String&#xA;   /// The delta containing the fields that have changed on the Message.&#xA;   public let delta: Delta&#xA;   &#xA;   public struct Delta: Decodable {&#xA;      &#xA;      /// The entity that produced the message. One of user or assistant.&#xA;      public let role: String&#xA;      /// The content of the message in array of text and/or images.&#xA;      public let content: [MessageContent]&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Step Delta Object&lt;/h3&gt; &#xA;&lt;p&gt;Represents a &lt;a href=&#34;https://platform.openai.com/docs/api-reference/assistants-streaming/run-step-delta-object&#34;&gt;run step delta&lt;/a&gt; i.e. any changed fields on a run step during streaming.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct RunStepDeltaObject: Decodable {&#xA;   &#xA;   /// The identifier of the run step, which can be referenced in API endpoints.&#xA;   public let id: String&#xA;   /// The object type, which is always thread.run.step.delta.&#xA;   public let object: String&#xA;   /// The delta containing the fields that have changed on the run step.&#xA;   public let delta: Delta&#xA;   &#xA;   public struct Delta: Decodable {&#xA;      &#xA;      /// The details of the run step.&#xA;      public let stepDetails: RunStepDetails&#xA;      &#xA;      private enum CodingKeys: String, CodingKey {&#xA;         case stepDetails = &#34;step_details&#34;&#xA;      }&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;⚠️ To utilize the &lt;code&gt;createRunAndStreamMessage&lt;/code&gt;, first create an assistant and initiate a thread.&lt;/p&gt; &#xA;&lt;p&gt;Usage &lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/createRun&#34;&gt;Create Run&lt;/a&gt; with stream.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;createRunAndStreamMessage&lt;/code&gt; streams &lt;a href=&#34;https://platform.openai.com/docs/api-reference/assistants-streaming/events&#34;&gt;events&lt;/a&gt;, You can decide which one you need for your implementation. For example, this is how you can access message delta and run step delta objects&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let assistantID = &#34;asst_abc123&#34;&#34;&#xA;let threadID = &#34;thread_abc123&#34;&#xA;let messageParameter = MessageParameter(role: .user, content: &#34;Tell me the square root of 1235&#34;)&#xA;let message = try await service.createMessage(threadID: threadID, parameters: messageParameter)&#xA;let runParameters = RunParameter(assistantID: assistantID)&#xA;let stream = try await service.createRunAndStreamMessage(threadID: threadID, parameters: runParameters)&#xA;&#xA;         for try await result in stream {&#xA;            switch result {&#xA;            case .threadMessageDelta(let messageDelta):&#xA;               let content = messageDelta.delta.content.first&#xA;               switch content {&#xA;               case .imageFile, nil:&#xA;                  break&#xA;               case .text(let textContent):&#xA;                  print(textContent.text.value) // this will print the streamed response for a message.&#xA;               }&#xA;               &#xA;            case .threadRunStepDelta(let runStepDelta):&#xA;               if let toolCall = runStepDelta.delta.stepDetails.toolCalls?.first?.toolCall {&#xA;                  switch toolCall {&#xA;                  case .codeInterpreterToolCall(let toolCall):&#xA;                     print(toolCall.input ?? &#34;&#34;) // this will print the streamed response for code interpreter tool call.&#xA;                  case .fileSearchToolCall(let toolCall):&#xA;                     print(&#34;File search tool call&#34;)&#xA;                  case .functionToolCall(let toolCall):&#xA;                     print(&#34;Function tool call&#34;)&#xA;                  case nil:&#xA;                     break&#xA;                  }&#xA;               }&#xA;            }&#xA;         }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can go to the &lt;a href=&#34;https://github.com/jamesrochabrun/SwiftOpenAI/tree/main/Examples/SwiftOpenAIExample/SwiftOpenAIExample&#34;&gt;Examples folder&lt;/a&gt; in this package, navigate to the &#39;Configure Assistants&#39; tab, create an assistant, and follow the subsequent steps.&lt;/p&gt; &#xA;&lt;h3&gt;Stream support has also been added to:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/createThreadAndRun&#34;&gt;Create Thread and Run&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;   /// Creates a thread and run with stream enabled.&#xA;   ///&#xA;   /// - Parameter parameters: The parameters needed to create a thread and run.&#xA;   /// - Returns: An AsyncThrowingStream of [AssistantStreamEvent](https://platform.openai.com/docs/api-reference/assistants-streaming/events) objects.&#xA;   /// - Throws: An error if the request fails.&#xA;   ///&#xA;   /// For more information, refer to [OpenAI&#39;s  Run API documentation](https://platform.openai.com/docs/api-reference/runs/createThreadAndRun).&#xA;   func createThreadAndRunStream(&#xA;      parameters: CreateThreadAndRunParameter)&#xA;   async throws -&amp;gt; AsyncThrowingStream&amp;lt;AssistantStreamEvent, Error&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/runs/submitToolOutputs&#34;&gt;Submit Tool Outputs&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;   /// When a run has the status: &#34;requires_action&#34; and required_action.type is submit_tool_outputs, this endpoint can be used to submit the outputs from the tool calls once they&#39;re all completed. All outputs must be submitted in a single request. Stream enabled&#xA;   ///&#xA;   /// - Parameter threadID: The ID of the [thread](https://platform.openai.com/docs/api-reference/threads) to which this run belongs.&#xA;   /// - Parameter runID: The ID of the run that requires the tool output submission.&#xA;   /// - Parameter parameters: The parameters needed for the run tools output.&#xA;   /// - Returns: An AsyncThrowingStream of [AssistantStreamEvent](https://platform.openai.com/docs/api-reference/assistants-streaming/events) objects.&#xA;   /// - Throws: An error if the request fails.&#xA;   ///&#xA;   /// For more information, refer to [OpenAI&#39;s  Run API documentation](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs).&#xA;   func submitToolOutputsToRunStream(&#xA;      threadID: String,&#xA;      runID: String,&#xA;      parameters: RunToolsOutputParameter)&#xA;   async throws -&amp;gt; AsyncThrowingStream&amp;lt;AssistantStreamEvent, Error&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Vector Stores&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct VectorStoreParameter: Encodable {&#xA;   &#xA;   /// A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that the vector store should use. Useful for tools like file_search that can access files.&#xA;   let fileIDS: [String]?&#xA;   /// The name of the vector store.&#xA;   let name: String?&#xA;   /// The expiration policy for a vector store.&#xA;   let expiresAfter: ExpirationPolicy?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   let metadata: [String: String]?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct VectorStoreObject: Decodable {&#xA;   &#xA;   /// The identifier, which can be referenced in API endpoints.&#xA;   let id: String&#xA;   /// The object type, which is always vector_store.&#xA;   let object: String&#xA;   /// The Unix timestamp (in seconds) for when the vector store was created.&#xA;   let createdAt: Int&#xA;   /// The name of the vector store.&#xA;   let name: String&#xA;   /// The total number of bytes used by the files in the vector store.&#xA;   let usageBytes: Int&#xA;   &#xA;   let fileCounts: FileCount&#xA;   /// The status of the vector store, which can be either expired, in_progress, or completed. A status of completed indicates that the vector store is ready for use.&#xA;   let status: String&#xA;   /// The expiration policy for a vector store.&#xA;   let expiresAfter: ExpirationPolicy?&#xA;   /// The Unix timestamp (in seconds) for when the vector store will expire.&#xA;   let expiresAt: Int?&#xA;   /// The Unix timestamp (in seconds) for when the vector store was last active.&#xA;   let lastActiveAt: Int?&#xA;   /// Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.&#xA;   let metadata: [String: String]&#xA;   &#xA;   public struct FileCount: Decodable {&#xA;      &#xA;      /// The number of files that are currently being processed.&#xA;      let inProgress: Int&#xA;      /// The number of files that have been successfully processed.&#xA;      let completed: Int&#xA;      /// The number of files that have failed to process.&#xA;      let failed: Int&#xA;      /// The number of files that were cancelled.&#xA;      let cancelled: Int&#xA;      /// The total number of files.&#xA;      let total: Int&#xA;   }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage &lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores/create&#34;&gt;Create vector Store&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let name = &#34;Support FAQ&#34;&#xA;let parameters = VectorStoreParameter(name: name)&#xA;try vectorStore = try await service.createVectorStore(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores/list&#34;&gt;List Vector stores&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStores = try await service.listVectorStores(limit: nil, order: nil, after: nil, before: nil)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores/retrieve&#34;&gt;Retrieve Vector store&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let vectorStore = try await service.retrieveVectorStore(id: vectorStoreID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores/modify&#34;&gt;Modify Vector store&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let vectorStore = try await service.modifyVectorStore(id: vectorStoreID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores/delete&#34;&gt;Delete Vector store&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let deletionStatus = try await service.deleteVectorStore(id: vectorStoreID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Vector Store File&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct VectorStoreFileParameter: Encodable {&#xA;   &#xA;   /// A [File](https://platform.openai.com/docs/api-reference/files) ID that the vector store should use. Useful for tools like file_search that can access files.&#xA;   let fileID: String&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct VectorStoreFileObject: Decodable {&#xA;   &#xA;   /// The identifier, which can be referenced in API endpoints.&#xA;   let id: String&#xA;   /// The object type, which is always vector_store.file.&#xA;   let object: String&#xA;   /// The total vector store usage in bytes. Note that this may be different from the original file size.&#xA;   let usageBytes: Int&#xA;   /// The Unix timestamp (in seconds) for when the vector store file was created.&#xA;   let createdAt: Int&#xA;   /// The ID of the [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object) that the [File](https://platform.openai.com/docs/api-reference/files) is attached to.&#xA;   let vectorStoreID: String&#xA;   /// The status of the vector store file, which can be either in_progress, completed, cancelled, or failed. The status completed indicates that the vector store file is ready for use.&#xA;   let status: String&#xA;   /// The last error associated with this vector store file. Will be null if there are no errors.&#xA;   let lastError: LastError?&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage &lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores-files/createFile&#34;&gt;Create vector store file&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let fileID = &#34;file-abc123&#34;&#xA;let parameters = VectorStoreFileParameter(fileID: fileID)&#xA;let vectoreStoreFile = try await service.createVectorStoreFile(vectorStoreID: vectorStoreID, parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores-files/listFiles&#34;&gt;List vector store files&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let vectorStoreFiles = try await service.listVectorStoreFiles(vectorStoreID: vectorStoreID, limit: nil, order: nil, aftre: nil, before: nil, filter: nil)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores-files/getFile&#34;&gt;Retrieve vector store file&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let fileID = &#34;file-abc123&#34;&#xA;let vectoreStoreFile = try await service.retrieveVectorStoreFile(vectorStoreID: vectorStoreID, fileID: fileID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores-files/deleteFile&#34;&gt;Delete vector store file&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let fileID = &#34;file-abc123&#34;&#xA;let deletionStatus = try await service.deleteVectorStoreFile(vectorStoreID: vectorStoreID, fileID: fileID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Vector Store File Batch&lt;/h3&gt; &#xA;&lt;p&gt;Parameters&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct VectorStoreFileBatchParameter: Encodable {&#xA;   &#xA;   /// A list of [File](https://platform.openai.com/docs/api-reference/files) IDs that the vector store should use. Useful for tools like file_search that can access files.&#xA;   let fileIDS: [String]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Response&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;public struct VectorStoreFileBatchObject: Decodable {&#xA;   &#xA;   /// The identifier, which can be referenced in API endpoints.&#xA;   let id: String&#xA;   /// The object type, which is always vector_store.file_batch.&#xA;   let object: String&#xA;   /// The Unix timestamp (in seconds) for when the vector store files batch was created.&#xA;   let createdAt: Int&#xA;   /// The ID of the [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object) that the [File](https://platform.openai.com/docs/api-reference/files) is attached to.&#xA;   let vectorStoreID: String&#xA;   /// The status of the vector store files batch, which can be either in_progress, completed, cancelled or failed.&#xA;   let status: String&#xA;   &#xA;   let fileCounts: FileCount&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Usage&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores-file-batches/createBatch&#34;&gt;Create vector store file batch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let fileIDS = [&#34;file-abc123&#34;, &#34;file-abc456&#34;]&#xA;let parameters = VectorStoreFileBatchParameter(fileIDS: fileIDS)&#xA;let vectorStoreFileBatch = try await service.&#xA;   createVectorStoreFileBatch(vectorStoreID: vectorStoreID, parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores-file-batches/getBatch&#34;&gt;Retrieve vector store file batch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let batchID = &#34;vsfb_abc123&#34;&#xA;let vectorStoreFileBatch = try await service.retrieveVectorStoreFileBatch(vectorStoreID: vectorStoreID, batchID: batchID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores-file-batches/cancelBatch&#34;&gt;Cancel vector store file batch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let batchID = &#34;vsfb_abc123&#34;&#xA;let vectorStoreFileBatch = try await service.cancelVectorStoreFileBatch(vectorStoreID: vectorStoreID, batchID: batchID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/vector-stores-file-batches/listBatchFiles&#34;&gt;List vector store files in a batch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let vectorStoreID = &#34;vs_abc123&#34;&#xA;let batchID = &#34;vsfb_abc123&#34;&#xA;let vectorStoreFiles = try await service.listVectorStoreFilesInABatch(vectorStoreID: vectorStoreID, batchID: batchID)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;⚠️ We currently support Only Assistants Beta 2. If you need support for Assistants V1, you can access it in the jroch-supported-branch-for-assistants-v1 branch or in the v2.3 release.. &lt;a href=&#34;https://platform.openai.com/docs/assistants/migration&#34;&gt;Check OpenAI Documentation for details on migration.&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;Azure OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;This library provides support for both chat completions and chat stream completions through Azure OpenAI. Currently, &lt;code&gt;DefaultOpenAIAzureService&lt;/code&gt; supports chat completions, including both streamed and non-streamed options.&lt;/p&gt; &#xA;&lt;p&gt;For more information about Azure configuration refer to the &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/reference&#34;&gt;documentation.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To instantiate &lt;code&gt;DefaultOpenAIAzureService&lt;/code&gt; you need to provide a &lt;code&gt;AzureOpenAIConfiguration&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let azureConfiguration = AzureOpenAIConfiguration(&#xA;                           resourceName: &#34;YOUR_RESOURCE_NAME&#34;, &#xA;                           openAIAPIKey: .apiKey(&#34;YOUR_OPENAI_APIKEY), &#xA;                           apiVersion: &#34;THE_API_VERSION&#34;)&#xA;                           &#xA;let service = OpenAIServiceFactory.service(azureConfiguration: azureConfiguration)           &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;supported api version can be found on the azure &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#completions&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Current Supported versions&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;2022-12-01&lt;/code&gt; &lt;code&gt;2023-03-15-preview&lt;/code&gt; &lt;code&gt;2023-05-15&lt;/code&gt; &lt;code&gt;2023-06-01-preview&lt;/code&gt; &lt;code&gt;2023-07-01-preview&lt;/code&gt; &lt;code&gt;2023-08-01-preview&lt;/code&gt; &lt;code&gt;2023-09-01-preview&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Usage on &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions&#34;&gt;Chat completions&lt;/a&gt;:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let parameters = ChatCompletionParameters(&#xA;                     messages: [.init(role: .user, content: .text(prompt))], &#xA;                     model: .custom(&#34;DEPLOYMENT_NAME&#34;) /// The deployment name you chose when you deployed the model. e.g: &#34;gpt-35-turbo-0613&#34;&#xA;let completionObject = try await service.startChat(parameters: parameters)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;AIProxy&lt;/h2&gt; &#xA;&lt;h3&gt;What is it?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.aiproxy.pro&#34;&gt;AIProxy&lt;/a&gt; is a backend for AI apps that proxies requests from your app to OpenAI. You can use this service to avoid exposing your OpenAI key in your app. We offer AIProxy support so that developers can build &lt;strong&gt;and&lt;/strong&gt; distribute apps using SwiftOpenAI.&lt;/p&gt; &#xA;&lt;h3&gt;How does my SwiftOpenAI code change?&lt;/h3&gt; &#xA;&lt;p&gt;SwiftOpenAI supports proxying requests through AIProxy with a small change to your integration code.&lt;/p&gt; &#xA;&lt;p&gt;Instead of initializing &lt;code&gt;service&lt;/code&gt; with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    let apiKey = &#34;your_openai_api_key_here&#34;&#xA;    let service = OpenAIServiceFactory.service(apiKey: apiKey)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    #if DEBUG &amp;amp;&amp;amp; targetEnvironment(simulator)&#xA;    let service = OpenAIServiceFactory.service(&#xA;        aiproxyPartialKey: &#34;hardcode_partial_key_here&#34;,&#xA;        aiproxyDeviceCheckBypass: &#34;hardcode_device_check_bypass_here&#34;&#xA;    )&#xA;    #else&#xA;    let service = OpenAIServiceFactory.service(&#xA;        aiproxyPartialKey: &#34;hardcode_partial_key_here&#34;&#xA;    )&#xA;    #endif&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;aiproxyPartialKey&lt;/code&gt; and &lt;code&gt;aiproxyDeviceCheckBypass&lt;/code&gt; values are provided to you on the &lt;a href=&#34;https://developer.aiproxy.pro&#34;&gt;AIProxy developer dashboard&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;⚠️ It is important that you do not let the &lt;code&gt;aiproxyDeviceCheckBypass&lt;/code&gt; token leak into a distribution build of your app (including TestFlight distributions). Please retain the conditional compilation checks that are present in the sample code above.&lt;/p&gt; &#xA;&lt;h4&gt;What is the &lt;code&gt;aiproxyDeviceCheckBypass&lt;/code&gt; constant?&lt;/h4&gt; &#xA;&lt;p&gt;AIProxy uses Apple&#39;s &lt;a href=&#34;https://developer.apple.com/documentation/devicecheck&#34;&gt;DeviceCheck&lt;/a&gt; to ensure that requests received by the backend originated from your app on a legitimate Apple device. However, the iOS simulator cannot produce DeviceCheck tokens. Rather than requiring you to constantly build and run on device during development, AIProxy provides a way to skip the DeviceCheck integrity check. The token is intended for use by developers only. If an attacker gets the token, they can make requests to your AIProxy project without including a DeviceCheck token, and thus remove one level of protection.&lt;/p&gt; &#xA;&lt;h4&gt;What is the &lt;code&gt;aiproxyPartialKey&lt;/code&gt; constant?&lt;/h4&gt; &#xA;&lt;p&gt;This constant is intended to be &lt;strong&gt;included&lt;/strong&gt; in the distributed version of your app. As the name implies, it is a partial representation of your OpenAI key. Specifically, it is one half of an encrypted version of your key. The other half resides on AIProxy&#39;s backend. As your app makes requests to AIProxy, the two encrypted parts are paired, decrypted, and used to fulfill the request to OpenAI.&lt;/p&gt; &#xA;&lt;h4&gt;How to setup my project on AIProxy?&lt;/h4&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://www.aiproxy.pro/docs/integration-guide.html&#34;&gt;AIProxy integration guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;⚠️ Disclaimer&lt;/h3&gt; &#xA;&lt;p&gt;Contributors of SwiftOpenAI shall not be liable for any damages or losses caused by third parties. Contributors of this library provide third party integrations as a convenience. Any use of a third party&#39;s services are assumed at your own risk.&lt;/p&gt; &#xA;&lt;h2&gt;Collaboration&lt;/h2&gt; &#xA;&lt;p&gt;Open a PR for any proposed change pointing it to &lt;code&gt;main&lt;/code&gt; branch. Unit tests are highly appreciated ❤️&lt;/p&gt;</summary>
  </entry>
</feed>