<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Swift Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-08T01:41:59Z</updated>
  <subtitle>Daily Trending of Swift in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ynagatomo/ImgGenSD2</title>
    <updated>2022-12-08T01:41:59Z</updated>
    <id>tag:github.com,2022-12-08:/ynagatomo/ImgGenSD2</id>
    <link href="https://github.com/ynagatomo/ImgGenSD2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An iOS app that generates images using Stable Diffusion v2.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Image Generator with Stable Diffusion v2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ynagatomo/ImgGenSD2/main/images/appicon180.png&#34; alt=&#34;AppIcon&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;A minimal iOS app that generates images using Stable Diffusion v2. You can create images specifying any prompt (text) such as &#34;a photo of an astronaut riding a horse on mars&#34;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;macOS 13.1 beta 4 or newer, Xcode 14.1&lt;/li&gt; &#xA; &lt;li&gt;iPhone 12+ / iOS 16.2 beta 4 or newer, iPad Pro with M1/M2 / iPadOS 16.2 beta 4 or newer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can run the app on above mobile devices. And you can run the app on Mac, building as a Designed for iPad app.&lt;/p&gt; &#xA;&lt;p&gt;This Xcode project does not contain the CoreML models of Stable Diffusion v2 (SD2). So you need to make them converting the PyTorch SD2 models using Apple converter tools. (see below)&lt;/p&gt; &#xA;&lt;p&gt;The project uses the Apple/ml-stable-diffusion Swift Package. You can see how it works through the simple sample code.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple/ml-stable-diffusion repo: &lt;a href=&#34;https://github.com/apple/ml-stable-diffusion&#34;&gt;https://github.com/apple/ml-stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ynagatomo/ImgGenSD2/main/images/ss_4_imgs.png&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ynagatomo/ImgGenSD2/main/images/ss0_1280.png&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Convert CoreML models&lt;/h2&gt; &#xA;&lt;p&gt;Convert the PyTorch SD2 model to CoreML models, following Apple&#39;s instructions. (&lt;a href=&#34;https://github.com/apple/ml-stable-diffusion&#34;&gt;https://github.com/apple/ml-stable-diffusion&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# create a Python environment and install dependencies&#xA;% conda create -n coremlsd2_38 python=3.8 -y&#xA;% conda activate coremlsd2_38&#xA;% cd SD2ModelConvChunked&#xA;% git clone https://github.com/apple/ml-stable-diffusion&#xA;% cd ml-stable-diffusion&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Visit the Hugging Face Hub - stabilityai/stable-diffusion-2 model&#39;s page. （&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2%EF%BC%89&#34;&gt;https://huggingface.co/stabilityai/stable-diffusion-2）&lt;/a&gt; Check the Terms and Use and accept it. Then you can use the model.&lt;/p&gt; &#xA;&lt;p&gt;And you need a Hugging Face&#39;s &lt;code&gt;User Access Token&lt;/code&gt;, to download huggingface/models. Please visit Hugging Face&#39;s site and make an access token at Account Settings.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# cli login&#xA;% huggingface-cli login&#xA;Token:    # &amp;lt;- input your Access Token&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download and convert the SD2 PyTorch model to CoreML models. If you do this on a Mac/8GB memory, please close all running apps except Terminal, otherwise the converter will be killed due to memory issues.&lt;/p&gt; &#xA;&lt;p&gt;Use these options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model-version stabilityai/stable-diffusion-2-base&lt;/code&gt; ... model version&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--bundle-resources-for-swift-cli&lt;/code&gt; ... compile and output &lt;code&gt;mlmodelc&lt;/code&gt; files into &lt;code&gt;&amp;lt;output-dir&amp;gt;/Resources&lt;/code&gt; folder. The Swift Package uses them.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;chunk-unet&lt;/code&gt; ... split the Unet model into two chunks for iOS/iPadOS execution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--attention-implementation SPLIT_EINSUM&lt;/code&gt; ... use SPLIT_EINSUM for Apple Neural Engine(ANE).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --convert-text-encoder --convert-vae-decoder --convert-safety-checker -o sd2CoremlChunked --model-version stabilityai/stable-diffusion-2-base --bundle-resources-for-swift-cli --chunk-unet --attention-implementation SPLIT_EINSUM --compute-unit CPU_AND_NE&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;How to add Core ML model files to Xcode project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In Finder, make the directory, &lt;code&gt;CoreMLModels&lt;/code&gt;, and put CoreML model files into the directory. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;merges.txt, vacab.json, UnetChunk2.mlmodelc, UnetChunk1.mlmodelc, VAEDecoder.mlmodelc, TextEncoder.mlmodelc&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;when you make an app for only Mac, use the Unet.mlmodelc instead of UnetChunk1/2, which are for mobile devices.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Remove the &lt;code&gt;CoreMLModels&lt;/code&gt; group in the Xcode Project Navigator.&lt;/li&gt; &#xA; &lt;li&gt;Drag and drop the &lt;code&gt;CoreMLModels&lt;/code&gt; directory in Finder into the Xcode Project Navigator, to add the files. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;At &lt;code&gt;Choose options for adding these files&lt;/code&gt; dialog, check the &lt;code&gt;[v] Copy items if needed&lt;/code&gt; and &lt;code&gt;[v] Create folder references&lt;/code&gt;, and &lt;code&gt;Add to targets: [v] imggensd2&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ynagatomo/ImgGenSD2/main/images/ss4_640.png&#34; alt=&#34;Image&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ynagatomo/ImgGenSD2/main/images/ss3_240.png&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now you can build the project, targeting to iPhone, iPad, or My Mac (Designed for iPad)&lt;/p&gt; &#xA;&lt;h2&gt;Consideration&lt;/h2&gt; &#xA;&lt;h3&gt;Large binary file&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Since the model files are very large (about 2.5GB), it causes a large binary of the app.&lt;/li&gt; &#xA; &lt;li&gt;The FAQ of Apple documentation says &#34;The recommended option is to prompt the user to download these assets upon first launch of the app. This keeps the app binary size independent of the Core ML models being deployed. Disclosing the size of the download to the user is extremely important as there could be data charges or storage impact that the user might not be comfortable with.&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Step count&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stable Diffusion v2 can generate good images with fewer steps than v1.4/v1.5.&lt;/li&gt; &#xA; &lt;li&gt;This means that the SD2&#39;s generation time is shorter.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ynagatomo/ImgGenSD2/main/images/ss_4_steps.png&#34; alt=&#34;Image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple Swift Package / ml-stable-diffusion: &lt;a href=&#34;https://github.com/apple/ml-stable-diffusion&#34;&gt;https://github.com/apple/ml-stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stable Diffusion with Core ML on Apple Silicon, Apple Dec 2022: &lt;a href=&#34;https://machinelearning.apple.com/research/stable-diffusion-coreml-apple-silicon&#34;&gt;https://machinelearning.apple.com/research/stable-diffusion-coreml-apple-silicon&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deploying Transformers on the Apple Neural Engine, Apple June 2022: &lt;a href=&#34;https://machinelearning.apple.com/research/neural-engine-transformers&#34;&gt;https://machinelearning.apple.com/research/neural-engine-transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face Hub - stabilityai/stable-diffusion-2: &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2&#34;&gt;https://huggingface.co/stabilityai/stable-diffusion-2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;http://img.shields.io/badge/license-MIT-blue.svg?style=flat&#34; alt=&#34;MIT License&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>