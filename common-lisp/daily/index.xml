<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Common Lisp Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-31T01:31:36Z</updated>
  <subtitle>Daily Trending of Common Lisp in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hikettei/cl-waffe</title>
    <updated>2023-03-31T01:31:36Z</updated>
    <id>tag:github.com,2023-03-31:/hikettei/cl-waffe</id>
    <link href="https://github.com/hikettei/cl-waffe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Deep Learning Framework for Common Lisp.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;cl-waffe&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hikettei/cl-waffe/actions/workflows/ci.yml&#34;&gt;&lt;img src=&#34;https://github.com/hikettei/cl-waffe/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://hikettei.github.io/cl-waffe-docs/docs/cl-waffe-logo.png&#34; alt=&#34;cl-waffe&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This package is still under development and experimental, so don&#39;t use this in your product.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;cl-waffe is a deep learning framework with modern APIs for Common Lisp based on &lt;a href=&#34;https://github.com/melisgl/mgl-mat&#34;&gt;mgl-mat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This is 100% written in Common Lisp (ignoring BLAS/CUBLAS parts). So it is super easy to extend kernel as you will. (In real, properly optimised and parallelised Common Lisp code is surprisingly fast, and not impossible to compete with C/C++.)&lt;/p&gt; &#xA;&lt;p&gt;Not having GPUs, I can&#39;t test my framework on cuda &amp;gt;&amp;lt;. CUDA support is a little further along. (Ignoring some operations like Embedding, most operations are performed via &lt;a href=&#34;https://github.com/melisgl/mgl-mat&#34;&gt;mgl-mat&lt;/a&gt;, so it should work without any modifications.)&lt;/p&gt; &#xA;&lt;h1&gt;What cl-waffe does?&lt;/h1&gt; &#xA;&lt;p&gt;cl-waffe is the two sides of the coin:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;As a &lt;strong&gt;matrix operation library&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;As a &lt;strong&gt;deep learning framework&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;cl-waffe as a matrix operation library&lt;/h2&gt; &#xA;&lt;p&gt;Speaking of the former, cl-waffe aimed to wrap the existing Common Lisp matrix operation libraries with Numpy/PyTorch like APIs. And reduce overheads between cl-waffe and these libraries.&lt;/p&gt; &#xA;&lt;p&gt;So, If you are considering contributing to cl-waffe in terms of boosting its performance, the first thing you should do is &lt;strong&gt;to contribute libraries cl-waffe uses&lt;/strong&gt;, especially, &lt;a href=&#34;https://github.com/melisgl/mgl-mat&#34;&gt;mgl-mat&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As I mentioned at &lt;a href=&#34;https://www.reddit.com/r/Common_Lisp/comments/124da1l/does_anyone_have_any_interest_in_my_deeplearning/?utm_source=share&amp;amp;utm_medium=web2x&amp;amp;context=3&#34;&gt;this reddit post&lt;/a&gt;, &lt;code&gt;a solid foundation must first be in place&lt;/code&gt; to accomplish deep learning on Common Lisp.&lt;/p&gt; &#xA;&lt;p&gt;What cl-waffe cannot currently do on its own:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;FP16 Operation (It&#39;s important for LLMs)&lt;/li&gt; &#xA; &lt;li&gt;Full GPU acceleration&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;mgl-mat on which cl-waffe mainly depends provides &lt;a href=&#34;https://github.com/melisgl/mgl-mat#x-28MGL-MAT-3A-40MAT-FACET-API-20MGL-PAX-3ASECTION-29&#34;&gt;Facet API&lt;/a&gt;, and The &lt;code&gt;Facet API&lt;/code&gt; enables the mgl-mat array to be accessed from CL array. That is, (according to my benchmark, there is some overhead but), existing other matrix operation libraries could be utilised. This is why cl-waffe depends on mgl-mat.&lt;/p&gt; &#xA;&lt;p&gt;True, cl-waffe works like in the relationship shown in this flow.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[cl-waffe]-&amp;gt;[mgl-mat]-&amp;gt;[Any library in Common Lisp]&#xA;                     -&amp;gt;[My implementation]&#xA;&#x9;&#x9;     -&amp;gt;[OpenBLAS]&#xA;&#x9;&#x9;     -&amp;gt;[CUDA]&#xA;&#x9;&#x9;     ....&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;cl-waffe as a deep learning framework&lt;/h2&gt; &#xA;&lt;p&gt;We wrap such existing libraries and define forward and backward propagation via the macro &lt;code&gt;defnode&lt;/code&gt;, thus enabling automatic differentiation.&lt;/p&gt; &#xA;&lt;p&gt;I think this is not an ideal situation because &lt;code&gt;array-to-mat&lt;/code&gt; and &lt;code&gt;numcl:transpose&lt;/code&gt; is both creating copies, but this is simultaneously good example to show what I want to do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defnode TransposeOriginalTensor (shape)&#xA;  :parameters ((prev-shape nil)&#xA;               (shape shape :type cons))&#xA;  :forward ((x)&#xA;&#x9;    (setf (self prev-shape) (!shape x))&#xA;&#x9;    (with-facet (array ((value x) &#39;array :direction :input))&#xA;&#x9;      ; In defnode, it is not always necessary to use the cl-waffe API.&#xA;&#x9;      ; With regard to this example, it defines a transpose with numcl&#39;s API.&#xA;&#x9;      (sysconst (array-to-mat (numcl:transpose array)))))&#xA;  :backward ((dy)&#xA;&#x9;     (list (!transpose1 dy (self prev-shape)))))&#xA;&#xA;(defun !transpose1 (tensor &amp;amp;rest dims)&#xA;  ; defined nodes are called with call&#xA;  (call (TransposeOriginalTensor dims) tensor))&#xA;&#xA;(!transpose (!randn `(10 3)))&#xA;&#xA;;#Const(((-0.21... -1.92... ~ 0.560... -0.90...)        &#xA;;                 ...&#xA;;        (0.580... 0.197... ~ -0.86... 0.765...)) :dtype :float :shape (3 10) :backward &amp;lt;Node: TRANSPOSEORIGINALTENSOR{W2995}&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is the basic idea behind cl-waffe&#39;s automatic differentiation.&lt;/p&gt; &#xA;&lt;p&gt;These nodes are combined to define a &lt;code&gt;model&lt;/code&gt; (via defmodel macro). Also, the model has &lt;code&gt;trainable parameters&lt;/code&gt; and they&#39;re optimized by optimizers, defined by &lt;code&gt;defoptimizer&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;(P.S.: I know not everyone likes this Chainer-like system (define-by-run), so I&#39;m thinking of providing Keras-like APIs.)&lt;/p&gt; &#xA;&lt;p&gt;Anyway, these features have been developed as &lt;strong&gt;extensible APIs&lt;/strong&gt; and do not need to be known by everyone.&lt;/p&gt; &#xA;&lt;p&gt;There are still &lt;strong&gt;very few standard implementations of NNs(As of this writing, only supports RNN/Linear and so on...)/&lt;/strong&gt;, as I think it is important to get the computational fundamentals in place before implementing various deep learning methods. (but contributions are welcome!)&lt;/p&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(2023/03/26) I published the benchmark compared to Numpy/PyTorch. Available at &lt;a href=&#34;https://github.com/hikettei/cl-waffe/raw/main/benchmark/Result.md&#34;&gt;Here&lt;/a&gt;. (Not quite up to my goal.) cl-waffe should peform better... however I guess there&#39;s a room to optimize in the cl-waffe&#39;s codes...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Documents&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hikettei.github.io/cl-waffe-docs&#34;&gt;Documentation&lt;/a&gt; is available.&lt;/p&gt; &#xA;&lt;p&gt;Also, I started writing &lt;a href=&#34;https://github.com/hikettei/cl-waffe/tree/main/tutorials/jp&#34;&gt;Tutorials(Written in Japanese)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As of this writing, available tutorials are written in Japanese and their writing continues, but eventually, I&#39;m willing to complete and translate them into English. So don&#39;t worry if you don&#39;t speak Japanese.&lt;/p&gt; &#xA;&lt;h1&gt;TOC&lt;/h1&gt; &#xA;&lt;!-- START doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;!-- DON&#39;T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE --&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#mnist-example&#34;&gt;MNIST Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#features&#34;&gt;Features&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#broadcasting&#34;&gt;Broadcasting&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#destructive-apis-with-a-simple-rule&#34;&gt;Destructive APIs with a Simple Rule.&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#useful-apis-like-numpypytorch&#34;&gt;Useful APIs like Numpy/PyTorch.&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#automatic-differentiation&#34;&gt;Automatic Differentiation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#useful-lazy-evaluation-system&#34;&gt;Useful Lazy-Evaluation System&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#tracing-jit&#34;&gt;Tracing JIT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#extensible-apis&#34;&gt;Extensible APIs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#switchable-backends&#34;&gt;Switchable Backends&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#install&#34;&gt;Install&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#install-via-github&#34;&gt;Install via Github&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#install-via-roswell&#34;&gt;Install via Roswell&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#install-via-ultralisp&#34;&gt;Install via Ultralisp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#contributing&#34;&gt;Contributing&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#running-the-tests&#34;&gt;Running the tests&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#lakefile&#34;&gt;Lakefile&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#run-mnist-with-roswell&#34;&gt;Run MNIST With Roswell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#currently-problemstodo&#34;&gt;Currently Problems/Todo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#goals&#34;&gt;Goals&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#author&#34;&gt;Author&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/#environment&#34;&gt;Environment&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- END doctoc generated TOC please keep comment here to allow auto update --&gt; &#xA;&lt;h1&gt;MNIST Example&lt;/h1&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe-docs/docs/mnist-tutorial.html&#34;&gt;Document&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;cl-waffe aimed to reduce the amount of total code written.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;; Full Code is in ./examples/mnist.lisp&#xA;&#xA;(defmodel MLP (activation)&#xA;  :parameters ((layer1   (denselayer (* 28 28) 512 t activation))&#xA;&#x9;       (layer2   (denselayer 512 256 t activation))&#xA;&#x9;       (layer3   (linearlayer 256 10 t)))&#xA;  :forward ((x)&#xA;&#x9;    (with-calling-layers x&#xA;&#x9;      (layer1 x)&#xA; &#x9;      (layer2 x)&#xA;&#x9;      (layer3 x))))&#xA;&#xA;(deftrainer MLPTrainer (activation lr)&#xA;  :model          (MLP activation)&#xA;  :optimizer      cl-waffe.optimizers:Adam&#xA;  :optimizer-args (:lr lr)&#xA;  :step-model ((x y)&#xA;&#x9;       (zero-grad)&#xA;&#x9;       (let ((out (cl-waffe.nn:softmax-cross-entropy (call (model) x) y)))&#xA;&#x9;&#x9; (backward out)&#xA;&#x9;&#x9; (update)&#xA;&#x9;&#x9; out))&#xA; :predict ((x) (call (model) x)))&#xA; &#xA;(let ((model (MLPTrainer :relu 1e-3)))&#xA;  (step-model model (!randn `(10 784))))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;p&gt;I&#39;ve only just started developing it, so I&#39;m trying out a lot of features by hand. (That is some features below may well work, some may not.)&lt;/p&gt; &#xA;&lt;p&gt;As of this writing:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Broadcasting&lt;/li&gt; &#xA; &lt;li&gt;Destructive APIs with a Simple Rule.&lt;/li&gt; &#xA; &lt;li&gt;Useful APIs like Numpy/PyTorch&lt;/li&gt; &#xA; &lt;li&gt;Automatic Differentiation&lt;/li&gt; &#xA; &lt;li&gt;Useful Lazy-Evaluation System&lt;/li&gt; &#xA; &lt;li&gt;Tracing JIT&lt;/li&gt; &#xA; &lt;li&gt;Extensible APIs&lt;/li&gt; &#xA; &lt;li&gt;Switchable Backends&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Broadcasting&lt;/h2&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe-docs/docs/using-tensor.html#broadcasting&#34;&gt;Document&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;cl-waffe has a broadcasting operations like other frameworks.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(setq a (!randn `(1 100 200)))&#xA;;#Const((((1.900... -0.70... ~ 0.609... 1.397...)         &#xA;;                   ...&#xA;;         (0.781... 1.735... ~ -1.01... 0.152...))) :mgl t :shape (1 100 200))&#xA;(setq b (!randn `(100 100 200)))&#xA;;#Const((((-1.21... 0.823... ~ 2.001... -0.21...)         &#xA;;                   ...&#xA;;         (-0.34... 0.441... ~ -0.07... -0.38...))        &#xA;;                 ...&#xA;;        ((1.627... 1.127... ~ 0.705... 0.798...)         &#xA;;                   ...&#xA;;         (0.070... 1.883... ~ 1.850... -0.47...))) :mgl t :shape (100 100 200))&#xA;&#xA;(time (!add a b))&#xA;;Evaluation took:&#xA;;  0.003 seconds of real time&#xA;;  0.002999 seconds of total run time (0.002799 user, 0.000200 system)&#xA;;  100.00% CPU&#xA;;  6,903,748 processor cycles&#xA;;  8,163,776 bytes consed&#xA;  &#xA;;#Const((((0.689... 0.115... ~ 2.611... 1.183...)         &#xA;;                   ...&#xA;;         (0.435... 2.177... ~ -1.08... -0.23...))        &#xA;;                 ...&#xA;;        ((3.528... 0.419... ~ 1.315... 2.195...)         &#xA;;                   ...&#xA;;         (0.851... 3.619... ~ 0.839... -0.32...))) :mgl t :shape (100 100 200))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Destructive APIs with a Simple Rule.&lt;/h2&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe-docs/docs/using-tensor.html#compute-tensors-in-a-destructive-way&#34;&gt;Document&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Internally, Just add to your code &lt;code&gt;(!allow-destruct tensor)&lt;/code&gt;, cl-waffe regards the tensor as unnecessary and destruct it. This is how implemented destructive operations are.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(setq a (!randn `(100 100 100)))&#xA;(setq b (!randn `(100 100 100)))&#xA;&#xA;(time (!!add a b))&#xA;;Evaluation took:&#xA;;  0.000 seconds of real time&#xA;;  0.000662 seconds of total run time (0.000605 user, 0.000057 system)&#xA;;  100.00% CPU&#xA;;  1,422,578 processor cycles&#xA;;  0 bytes consed&#xA;  &#xA;;#Const((((-1.47... 1.016... ~ -1.29... -1.71...)         &#xA;;                   ...&#xA;;         (2.276... 0.878... ~ -1.35... 0.466...))        &#xA;;                 ...&#xA;;        ((1.712... 1.318... ~ 0.213... 1.262...)         &#xA;;                   ...&#xA;;         (1.084... -0.18... ~ -1.42... 0.552...))) :mgl t :shape (100 100 100))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Useful APIs like Numpy/PyTorch.&lt;/h2&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe-docs/docs/cl-waffe.html&#34;&gt;Document&lt;/a&gt;, Here&#39;s the list of all APIs in cl-waffe.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s API like &lt;code&gt;SliceTensor&lt;/code&gt; in Numpy/PyTorch. Of course, they&#39;re differentiable.&lt;/p&gt; &#xA;&lt;p&gt;However, in practical, using offsets (in lisp, we call it displacement) will perform better. (e.g.: setting batch, applying word-by-word processing in RNN). so it is just extra.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(setq a (!randn `(100 100 100)))&#xA;;#Const((((-1.45... -0.70... ~ -0.87... -0.52...)         &#xA;;                   ...&#xA;;         (0.655... -1.47... ~ -2.10... -1.79...))        &#xA;;                 ...&#xA;;        ((-0.28... -1.75... ~ -1.28... 0.381...)         &#xA;;                   ...&#xA;;         (-0.55... -0.53... ~ 0.421... -0.13...))) :mgl t :shape (100 100 100))&#xA;&#xA;(time (!aref a 0 0 0))&#xA;;Evaluation took:&#xA;;  0.000 seconds of real time&#xA;;  0.000163 seconds of total run time (0.000135 user, 0.000028 system)&#xA;;  100.00% CPU&#xA;;  235,060 processor cycles&#xA;;  0 bytes consed&#xA;  &#xA;;#Const((((-1.45...))) :mgl t :shape (1 1 1))&#xA;&#xA;(time (!aref a t 0 0))&#xA;;Evaluation took:&#xA;;  0.000 seconds of real time&#xA;;  0.000477 seconds of total run time (0.000455 user, 0.000022 system)&#xA;;  100.00% CPU&#xA;;  963,246 processor cycles&#xA;;  98,256 bytes consed&#xA;  &#xA;;#Const((((-1.45...))        &#xA;;                 ...&#xA;;        ((-0.28...))) :mgl t :shape (100 1 1))&#xA;&#xA;(time (!aref a &#39;(0 3) &#39;(10 -1) t))&#xA;&#xA;;Evaluation took:&#xA;;  0.001 seconds of real time&#xA;;  0.001489 seconds of total run time (0.001445 user, 0.000044 system)&#xA;;  100.00% CPU&#xA;;  3,518,516 processor cycles&#xA;;  322,144 bytes consed&#xA;  &#xA;;#Const((((-0.10... 0.226... ~ -1.68... 0.662...)         &#xA;;                   ...&#xA;;         (-0.14... 1.239... ~ -0.90... -0.60...))        &#xA;;                 ...&#xA;;        ((-0.97... 1.588... ~ 0.558... -1.79...)         &#xA;;                   ...&#xA;;         (-0.80... -1.50... ~ -1.11... -0.21...))) :mgl t :shape (3 89 100))&#xA;&#xA;(time (!aref a t t t))&#xA;;Evaluation took:&#xA;;  0.024 seconds of real time&#xA;;  0.024426 seconds of total run time (0.024367 user, 0.000059 system)&#xA;;  100.00% CPU&#xA;;  56,193,050 processor cycles&#xA;;  12,675,952 bytes consed&#xA;  &#xA;;#Const((((-1.45... -0.70... ~ -0.87... -0.52...)         &#xA;;                   ...&#xA;;         (0.655... -1.47... ~ -2.10... -1.79...))        &#xA;;                 ...&#xA;;        ((-0.28... -1.75... ~ -1.28... 0.381...)         &#xA;;                   ...&#xA;;         (-0.55... -0.53... ~ 0.421... -0.13...))) :mgl t :shape (100 100 100))&#xA;&#xA;(setq b (!ones `(100 3)))&#xA;(time (setf (!aref a &#39;(0 3)) b))&#xA;;Evaluation took:&#xA;;  0.001 seconds of real time&#xA;;  0.001312 seconds of total run time (0.001274 user, 0.000038 system)&#xA;;  100.00% CPU&#xA;;  2,898,956 processor cycles&#xA;;  262,048 bytes consed&#xA;;#Const((((1.0 1.0 ~ -0.87... -0.52...)         &#xA;;                   ...&#xA;;         (1.0 1.0 ~ -2.10... -1.79...))        &#xA;;                 ...&#xA;;        ((-0.28... -1.75... ~ -1.28... 0.381...)         &#xA;;                   ...&#xA;;         (-0.55... -0.53... ~ 0.421... -0.13...))) :mgl t :shape (100 100 100))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Automatic Differentiation&lt;/h2&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe-docs/docs/using-tensor.html#basic-tensor-operations&#34;&gt;Document&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once forward is defined, backward is also automatically defined. This feature is indispensable for Deep Learning Framework. Of course it is available.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(setq a (parameter (!randn `(10 10))))&#xA;(setq b (parameter (!randn `(10 10))))&#xA;(setq c (parameter (!randn `(10))))&#xA;&#xA;&#xA;(setq z (!sum (!add (!mul a b) c)))&#xA;&#xA;(time (backward z))&#xA;;Evaluation took:&#xA;;  0.001 seconds of real time&#xA;;  0.001469 seconds of total run time (0.001344 user, 0.000125 system)&#xA;;  100.00% CPU&#xA;;  3,239,008 processor cycles&#xA;;  130,048 bytes consed&#xA;  &#xA;;NIL&#xA;&#xA;(print (const (grad a)))&#xA;;#Const(((0.004... -0.00... ~ 0.004... 5.721...)&#xA;;                 ...&#xA;;        (0.001... 5.919... ~ 7.748... -0.00...)) :mgl t :shape (10 10))&#xA;(print (const (grad b)))&#xA;;#Const(((0.004... -0.00... ~ 0.004... 5.721...)&#xA;;                 ...&#xA;;        (0.001... 5.919... ~ 7.748... -0.00...)) :mgl t :shape (10 10))&#xA;(print (const (grad c)))&#xA;;#Const((0.01 0.01 ~ 0.01 0.01) :mgl t :shape (10))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Useful Lazy-Evaluation System&lt;/h2&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe-docs/docs/using-tensor.html#lazy-evaluation&#34;&gt;Document&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;cl-waffe provides zero-cost transpose by using lazy-evaluation.&lt;/p&gt; &#xA;&lt;p&gt;Just use &lt;code&gt;!transpose&lt;/code&gt; before &lt;code&gt;!matmul&lt;/code&gt;, &lt;code&gt;!matmul&lt;/code&gt; automatically recognises it and the retuend tensor is applied &lt;code&gt;transpose&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s &lt;code&gt;!transpose1&lt;/code&gt; for the case when you just want a transposed tensor.&lt;/p&gt; &#xA;&lt;p&gt;The lazy-evaluated tensors are evaluated via function &lt;code&gt;(value tensor)&lt;/code&gt;. Once this function called. the content of tensor is fulfilled with a new evaluated matrix. Don&#39;t worry, &lt;code&gt;(value tensor)&lt;/code&gt; are scattered all over the place in cl-waffe&#39;s code, so no additional codes are required.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(setq a (!randn `(10 3)))&#xA;;#Const(((0.411... 0.244... 2.828...)        &#xA;;                 ...&#xA;;        (-1.26... -1.41... 0.821...)) :mgl t :shape (10 3))&#xA;&#xA;(!transpose a)&#xA;;#Const(#&amp;lt;FUNCTION (LABELS CL-WAFFE.BACKENDS.MGL::LAZYTRANSPOSE :IN CL-WAFFE.BACKENDS.MGL::LAZY-EVAL-TRANSPOSE) {100CA0F5EB}&amp;gt;)&#xA;&#xA;; Generally, using delayed evaluation does not require additional new code.&#xA;(!add * 1)&#xA;;#Const(((1.411... 0.862... ~ 1.590... -0.26...)        &#xA;;                 ...&#xA;;        (3.828... 0.582... ~ -0.57... 1.821...)) :mgl t :shape (3 10))&#xA;&#xA;; Using !transpose is much faster for !matmul (even when the tensors are 3d/4d...).&#xA;(time (!matmul a (!transpose a)))&#xA;;Evaluation took:&#xA;;  0.000 seconds of real time&#xA;;  0.000107 seconds of total run time (0.000101 user, 0.000006 system)&#xA;;  100.00% CPU&#xA;;  147,434 processor cycles&#xA;;  0 bytes consed&#xA;  &#xA;;#Const(((8.227... -1.29... ~ -4.10... 1.458...)        &#xA;;                 ...&#xA;;        (1.458... 0.180... ~ -2.59... 4.273...)) :mgl t :shape (10 10))&#xA;&#xA;; Compared to !transpose1...&#xA;(time (!matmul a (!transpose1 a)))&#xA;;Evaluation took:&#xA;;  0.178 seconds of real time&#xA;;  0.176052 seconds of total run time (0.120406 user, 0.055646 system)&#xA;;  98.88% CPU&#xA;;  4 forms interpreted&#xA;;  773 lambdas converted&#xA;;  410,887,630 processor cycles&#xA;;  25,051,200 bytes consed&#xA;  &#xA;;#Const(((8.227... -1.29... ~ -4.10... 1.458...)        &#xA;;                 ...&#xA;;        (1.458... 0.180... ~ -2.59... 4.273...)) :mgl t :shape (10 10))&#xA;&#xA;; PS (2023/05/26). The lazy-evaluated tensors have been modified to display more elegant.&#xA;(print a) ; #Const(&amp;lt;Transposed Tensor&amp;gt; :shape (10 10) :backward &amp;lt;Node: TRANSPOSETENSOR{W2126}&amp;gt;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(setq a (!randn `(10 10)))&#xA;;#Const(((0.570... -0.13... ~ 0.217... 0.862...)        &#xA;;                 ...&#xA;;        (-0.12... 0.384... ~ -0.25... -0.91...)) :mgl t :shape (10 10))&#xA;&#xA;(setq lazy-evaluated-a (!transpose a))&#xA;;#Const(#&amp;lt;FUNCTION (LABELS CL-WAFFE.BACKENDS.MGL::LAZYTRANSPOSE :IN CL-WAFFE.BACKENDS.MGL::LAZY-EVAL-TRANSPOSE) {100E48135B}&amp;gt;)&#xA;&#xA;(print lazy-evaluated-a)&#xA;;#Const(#&amp;lt;FUNCTION (LABELS CL-WAFFE.BACKENDS.MGL::LAZYTRANSPOSE :IN CL-WAFFE.BACKENDS.MGL::LAZY-EVAL-TRANSPOSE) {100E48135B}&amp;gt;)&#xA;&#xA;; value will accept and evaluated lazy-evaluated tensor.&#xA;(value lazy-evaluated-a)&#xA;&#xA;(print lazy-evaluated-a)&#xA;;#Const(((0.570... 1.228... ~ 0.050... -0.12...)        &#xA;;                 ...&#xA;;        (0.862... -0.82... ~ 1.360... -0.91...)) :mgl t :shape (10 10))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Tracing JIT&lt;/h2&gt; &#xA;&lt;p&gt;This is still experimental...&lt;/p&gt; &#xA;&lt;p&gt;In the &lt;code&gt;(with-jit)&lt;/code&gt; macro, cl-waffe dynamically defines the kernel functions with a lazy-evaluation system. (currently, it is only when blas).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;&#xA;; In default...&#xA;&#xA;(time (!log (!exp a)))&#xA;;Evaluation took:&#xA;;  0.000 seconds of real time&#xA;;  0.000171 seconds of total run time (0.000130 user, 0.000041 system)&#xA;;  100.00% CPU&#xA;;  248,100 processor cycles&#xA;;  3,232 bytes consed&#xA;  &#xA;;#Const(((0.570... -0.13... ~ 0.217... 0.862...)        &#xA;;                 ...&#xA;;        (-0.12... 0.384... ~ -0.25... -0.91...)) :mgl t :shape (10 10))&#xA;&#xA;(defun trace-operate ()&#xA;  (with-jit&#xA;     (time (const (value (!log (!exp a)))))))&#xA;&#xA;; The first call of trace-operate, it seems slower because cl-waffe traces and compiles code.&#xA;(trace-operate)&#xA;;Evaluation took:&#xA;;  0.000 seconds of real time&#xA;;  0.000183 seconds of total run time (0.000122 user, 0.000061 system)&#xA;;  100.00% CPU&#xA;;  240,442 processor cycles&#xA;;  32,512 bytes consed&#xA;  &#xA;;#Const(((0.570... -0.13... ~ 0.217... 0.862...)        &#xA;;                 ...&#xA;;        (-0.12... 0.384... ~ -0.25... -0.91...)) :mgl t :shape (10 10))&#xA;&#xA;; However, after the second one, it will be faster.&#xA;(trace-operate)&#xA;;Evaluation took:&#xA;;  0.000 seconds of real time&#xA;;  0.000096 seconds of total run time (0.000087 user, 0.000009 system)&#xA;;  100.00% CPU&#xA;;  187,848 processor cycles&#xA;;  0 bytes consed&#xA;  &#xA;;#Const(((0.570... -0.13... ~ 0.217... 0.862...)        &#xA;;                 ...&#xA;;        (-0.12... 0.384... ~ -0.25... -0.91...)) :mgl t :shape (10 10))&#xA;&#xA;; P.S.: Back propagation is available even when with-jit is enabled&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Extensible APIs&lt;/h2&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe-docs/docs/extend-library.html&#34;&gt;Document&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;cl-waffe&#39;s features are based on these macro:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;defmodel&lt;/code&gt; (describes forward process and parameters that to be optimized.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;defnode&lt;/code&gt; (describes forward process and backward process, that not necessary to use cl-waffe/mgl-mat, you can use libraries you like.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;deftrainer&lt;/code&gt; (describes the predict/training process. (e.g.: criterion, optimizer&#39;s setting), which contributes to reduce the amount of total codes.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;defoptimizer&lt;/code&gt; (describes the optimizing function)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;defdataset&lt;/code&gt; (describes how the dataset is itearted.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;True, almost implementations are using it (See also: &lt;code&gt;./source/optimizers/optimizers.lisp&lt;/code&gt;, or &lt;code&gt;./source/operators.lisp&lt;/code&gt;). In the same way, All macros are exported, and users can make extensions of the framework as required.&lt;/p&gt; &#xA;&lt;p&gt;(The codes below is using mgl-mat and numcl. For details about with-facet, numcl: &lt;a href=&#34;https://github.com/melisgl/mgl-mat#x-28MGL-MAT-3A-40MAT-FACET-API-20MGL-PAX-3ASECTION-29&#34;&gt;with-facet(mgl-mat)&lt;/a&gt;, &lt;a href=&#34;https://github.com/numcl/numcl&#34;&gt;numcl&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;; in ./source/operators.lisp at 202th line&#xA;&#xA;(defnode TransposeOriginalTensor (shape)&#xA;  :optimize t&#xA;  :parameters ((prev-shape nil) (shape shape))&#xA;  :forward ((x)&#xA;&#x9;    (setf (self prev-shape) (!shape x))&#xA;&#x9;    (with-facet (array ((value x) &#39;array :direction :input))&#xA;&#x9;      ; In defnode, it is not always necessary to use the cl-waffe API.&#xA;&#x9;      ; With regard to this example, it defines a transpose with numcl&#39;s API.&#xA;&#x9;      (sysconst (array-to-mat (numcl:transpose array)))))&#xA;  :backward ((dy)&#xA;&#x9;     (list (!transpose1 dy (self prev-shape)))))&#xA;&#xA;(defun !transpose1 (tensor &amp;amp;rest dims)&#xA;  ; defined nodes are called with call&#xA;  (call (TransposeOriginalTensor dims) tensor))&#xA;  &#xA;; in ./source/optimizers/optimizers.lisp at 4th line&#xA;&#xA;(defoptimizer SGD (params &amp;amp;key (lr 1e-3))&#xA;  :parameters ((params params) (lr lr))&#xA;  :update (()&#xA;&#x9;   (dotimes (i (hash-table-count (self params)))&#xA;&#x9;     (copy! (data (!sub (gethash i (self params))&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;   (!mul (self lr) (grad (gethash i (self params))))))&#xA;&#x9;&#x9;&#x9;  (data (gethash i (self params)))))))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Switchable Backends&lt;/h2&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe-docs/docs/using-tensor.html#backends&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;It is allowed to redefine the original node in cl-waffe. Such nodes are managed by using &lt;code&gt;backend&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;define-node-extension&lt;/code&gt; is available to extend the existing nodes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;; in ./t/node-extension.lisp&#xA;(define-node-extension cl-waffe::AddTensor&#xA;  :backend :my-extension&#xA;  :forward ((x y)&#xA;            (const (+ 100 100)))&#xA;  :backward ((dy)&#xA;             (list dy dy)))&#xA;&#xA;(defun operate-in-mgl ()&#xA;  (with-backend :mgl&#xA;    (= (data (!add 1 1)) 2)))&#xA;&#xA;(defun operate-in-extension ()&#xA;  (with-backend :my-extension&#xA;    (= (data (!add 1 1)) 200)))&#xA;&#xA;(defun operate-restart-test () ; if the operation doesn&#39;t exists...&#xA;  (with-backend :does-not-exists&#xA;    (= (data (!add 1 1)) 2)))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;p&gt;It is recommended to install following in advance:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.sbcl.org/&#34;&gt;SBCL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/roswell/roswell&#34;&gt;Roswell&lt;/a&gt; (If you&#39;re new to Common Lisp, I recommend you to install it first.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/takagi/lake&#34;&gt;Lake&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;cl-waffe is available in one of the following ways:&lt;/p&gt; &#xA;&lt;h3&gt;Install via Github&lt;/h3&gt; &#xA;&lt;p&gt;For Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ git clone git@github.com:hikettei/cl-waffe.git&#xA;$ cd cl-waffe&#xA;$ sbcl&#xA;* (load &#34;cl-waffe.asd&#34;)&#xA;* (ql:quickload :cl-waffe) ; all is done!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install via Roswell&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ ros install hikettei/cl-waffe&#xA;$ ros run&#xA;* (ql:quickload :cl-waffe)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install via Ultralisp&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ultralisp.org/&#34;&gt;ultralisp&lt;/a&gt; dist is available.&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://raw.githubusercontent.com/hikettei/cl-waffe/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us. Don&#39;t hesitate to send me issues if you have any trouble!&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;SBCL&lt;/li&gt; &#xA; &lt;li&gt;Roswell&lt;/li&gt; &#xA; &lt;li&gt;Lakefile&lt;/li&gt; &#xA; &lt;li&gt;Quicklisp&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Running the tests&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ lake test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;should work.&lt;/p&gt; &#xA;&lt;h2&gt;Lakefile&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/leanprover/lake&#34;&gt;Lakefile&lt;/a&gt; is available at github repository. (Also it requires &lt;a href=&#34;https://github.com/roswell/roswell&#34;&gt;Roswell&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ lake&#xA;Usage: lake [command]&#xA;&#xA;Tasks:&#xA;  test                     Operate tests&#xA;  benchmark                Start Benchmarking&#xA;  benchmark-python         Start Benchmarking with cl-waffe and numpy/pytorch.&#xA;  gendoc                   Generating Documentations&#xA;  example:install          Install training data for examples&#xA;  example:mnist            Run example model with MNIST&#xA;  example:rnn              Run example model with Seq2Seq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Run MNIST With Roswell&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ cd examples&#xA;$ sh install.sh&#xA;$ cd ..&#xA;$ ./run-test-model.ros mnist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ lake example:install&#xA;$ lake example:fnn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;should work. &lt;code&gt;lake example:mnist&lt;/code&gt; is also OK.&lt;/p&gt; &#xA;&lt;h1&gt;Currently Problems/Todo&lt;/h1&gt; &#xA;&lt;p&gt;As of writing, I&#39;m working on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;del&gt;Á†¥Â£äÁöÑ‰ª£ÂÖ•„ÅÆ„Çµ„Éù„Éº„Éà(Support more destructive operations)&lt;/del&gt;(Done)&lt;/li&gt; &#xA; &lt;li&gt;Neural Network„ÅÆËøΩÂä† (Add cl-waffe.nn models)&lt;/li&gt; &#xA; &lt;li&gt;RNNs are too much slower than PyTorch...&lt;/li&gt; &#xA; &lt;li&gt;„É¢„Éá„É´„ÅÆ‰øùÂ≠ò„Å´ÂØæÂøú (Save and restore trained models.)&lt;/li&gt; &#xA; &lt;li&gt;„Ç∞„É©„Éï„ÅÆË°®Á§∫„Å´ÂØæÂøú (Plotting losses and so on)&lt;/li&gt; &#xA; &lt;li&gt;Êßò„ÄÖ„Å™„Éá„Éº„ÇøÊßãÈÄ†„ÇíÊâ±„Åà„Çã„Çà„ÅÜ„Å´ (Support more types of data structure)&lt;/li&gt; &#xA; &lt;li&gt;ÊÄßËÉΩÂêë‰∏äÔºà„É°„É¢„É™‰ΩøÁî®Èáè/CPU‰ΩøÁî®Áéá„ÅÆË¶≥ÁÇπ„Åã„Çâ Ôºâ(In term of cpu-usage rate/memory-usage, cl-waffe has a lot of challenge to performance.)&lt;/li&gt; &#xA; &lt;li&gt;CUDA„Å´ÂØæÂøú (Support CUDA)&lt;/li&gt; &#xA; &lt;li&gt;‰ªñ„ÅÆÂá¶ÁêÜÁ≥ª„ÅßÂãï„Åè„ÅãË©¶„Åô (Try on another systems (e.g.: CCL))&lt;/li&gt; &#xA; &lt;li&gt;Improving the quality of documentation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Goals&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Making cl-waffe a modern and fast framework.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fix: high memory usage&lt;/li&gt; &#xA;   &lt;li&gt;Add: More APIs&lt;/li&gt; &#xA;   &lt;li&gt;Add: Clear distinction between slow and fast APIs.&lt;/li&gt; &#xA;   &lt;li&gt;Add: Simple rules to make it fast and lacklustre and documentations for it&lt;/li&gt; &#xA;   &lt;li&gt;Goal: Training Transformer Model&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Making cl-waffe practical&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support: cl-jupyter, any plotting library, matplotlib, etc...&lt;/li&gt; &#xA;   &lt;li&gt;Support: CUDA with Full Performance!&lt;/li&gt; &#xA;   &lt;li&gt;Add: Mathematical Functions&lt;/li&gt; &#xA;   &lt;li&gt;Add: High power APIs (such features are rooted in !aref, broadcasting. they need to be optimized more)&lt;/li&gt; &#xA;   &lt;li&gt;Add: DataLoader like PyTorch&lt;/li&gt; &#xA;   &lt;li&gt;Add: Save and Restore Models, (Compatible with PyTorch if possible...)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go faster cl-waffe&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Support: more parallelized operators&lt;/li&gt; &#xA;   &lt;li&gt;Keep whole codes abstracted and extensible&lt;/li&gt; &#xA;   &lt;li&gt;Apply full optimisation when some functionality is reached enough.&lt;/li&gt; &#xA;   &lt;li&gt;More benchmarks are needed and put it all in a table somewhere.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I love Common Lisp very much and there are many excellent libraries for numerical operations with great ideas.&lt;/p&gt; &#xA;&lt;p&gt;However, I know I&#39;m really reckless, but the one I want to make is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Making full use of Common Lisp&#39;s nice features.&lt;/li&gt; &#xA; &lt;li&gt;I want to have a range of functions comparable to Python&#39;s frameworks.&lt;/li&gt; &#xA; &lt;li&gt;Simple/Compact notations and APIs&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Having started on 2022/12/26, this project will take a long time before these features are realised.&lt;/p&gt; &#xA;&lt;p&gt;Does anyone have any ideas? Please share with me on issues!&lt;/p&gt; &#xA;&lt;p&gt;Also, bug reports and more are welcome!&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The author of &lt;a href=&#34;https://github.com/melisgl/mgl-mat&#34;&gt;mgl-mat&lt;/a&gt;, and anyone who has contributed to it.&lt;/li&gt; &#xA; &lt;li&gt;To all those who gave me ideas, helps and knowledgement.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Author&lt;/h1&gt; &#xA;&lt;p&gt;hikettei&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/ichndm&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hikettei&#34;&gt;Github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Discord: ruliaüåô#5298&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Environment&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SBCL &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;it is recommended to use SBCL, I&#39;ve not tested on others&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>