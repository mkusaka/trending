<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Common Lisp Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-24T01:32:12Z</updated>
  <subtitle>Daily Trending of Common Lisp in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hikettei/cl-waffe2</title>
    <updated>2023-07-24T01:32:12Z</updated>
    <id>tag:github.com,2023-07-24:/hikettei/cl-waffe2</id>
    <link href="https://github.com/hikettei/cl-waffe2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[TBD] Programmable Deep Learning Framework&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/hikettei/cl-waffe2&#34;&gt; &lt;img alt=&#34;Logo&#34; src=&#34;https://hikettei.github.io/cl-waffe-docs/cl-waffe.png&#34; width=&#34;45%&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h3 align=&#34;center&#34;&gt;Programmable Deep Learning Framework&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://hikettei.github.io/cl-waffe2/&#34;&gt;&lt;strong&gt;Visit the docs »&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/hikettei/cl-waffe2/issues&#34;&gt;Issues&lt;/a&gt; · &lt;a href=&#34;https://hikettei.github.io/cl-waffe2/install/&#34;&gt;Installing&lt;/a&gt; · &lt;a href=&#34;https://hikettei.github.io/cl-waffe2/overview/&#34;&gt;Tutorials&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hikettei/cl-waffe2/actions/workflows/Cl.yml&#34;&gt;&lt;img src=&#34;https://github.com/hikettei/cl-waffe2/actions/workflows/Cl.yml/badge.svg?branch=master&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;cl-waffe2&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;⚠️ cl-waffe2 is still in the experimental stage, things are subject to change. DO NOT USE CL-WAFFE2 IN YOUR PRODUCT.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;cl-waffe2 provides a set of differentiable matrix operations which is aimed to apply to building a neural network model. Operations in cl-waffe2 are accelerated by &lt;code&gt;Lazy Evaluation&lt;/code&gt; and &lt;code&gt;JIT Compiling with optimizing nodes.&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Visit my preceding project: &lt;a href=&#34;https://github.com/hikettei/cl-waffe&#34;&gt;cl-waffe&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Concepts/Features&lt;/h1&gt; &#xA;&lt;h2&gt;Multiple Backends Support&lt;/h2&gt; &#xA;&lt;p&gt;All classes that are subtypes of &lt;code&gt;AbstractTensor&lt;/code&gt; are tensors that cl-waffe2 can handle.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;;; MyTensor extends CPUTensor extends AbstractTensor&#xA;(defclass MyTensor (CPUTensor) nil)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which devices the function is to operate on can be declared along with its priority using the &lt;code&gt;with-devices&lt;/code&gt; macro.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(with-devices (MyTensor CPUTensor)&#xA;    ;; Under this scope, Priority = (MyTensor -&amp;gt; CPUTensor)&#xA;    (!add (randn `(3 3)) (randn `(3 3))))&#xA;&#xA;{MYTENSOR[float] :shape (3 3) :named ChainTMP12737 &#xA;  :vec-state [maybe-not-computed]&#xA;  &amp;lt;&amp;lt;Not-Embodied (3 3) Tensor&amp;gt;&amp;gt;&#xA;  :facet :input&#xA;  :requires-grad NIL&#xA;  :backward &amp;lt;Node: ADDNODE-CPUTENSOR (A[~] B[~] -&amp;gt; A[~])&amp;gt;}&#xA;&#xA;;; Proceed is a differentiable operation which compiles/evaluates previous computation nodes.&#xA;(proceed *) ;; MyTensor has no any implementation for AddNode, so CPUTensor is returned.&#xA;&#xA;{CPUTENSOR[float] :shape (3 3) :named ChainTMP12759 &#xA;  :vec-state [computed]&#xA;  ((-0.9171257  0.4143868   0.9511917)&#xA;   (2.224929    1.4860398   0.8402364)&#xA;   (0.051592022 0.5673465   -0.46694738))&#xA;  :facet :input&#xA;  :requires-grad NIL&#xA;  :backward &amp;lt;Node: PROCEEDNODE-T (A[~] -&amp;gt; A[~])&amp;gt;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This indicates not only is cl-waffe2 extensible to a wide variety of backends, but it also minimises the need to rewrite code to the greatest extent possible.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://hikettei.github.io/cl-waffe2/base-impl-nodes/&#34;&gt;this section&lt;/a&gt; for the specifications under which computation nodes are defined.&lt;/p&gt; &#xA;&lt;h2&gt;JIT Compiler&lt;/h2&gt; &#xA;&lt;p&gt;Since &lt;code&gt;cl-waffe2&lt;/code&gt; is a lazy-evaluation first framework, all operations need to be compiled at a certain point in time. It could be unintuitive for some users, however, at the same time, the cl-waffe2 compiler can obtain more information for optimisation.&lt;/p&gt; &#xA;&lt;p&gt;For example, the function &lt;code&gt;!add&lt;/code&gt; initially makes a copy of given arguments to avoid side effects, because &lt;code&gt;AddNode&lt;/code&gt; is defined as in-place operation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defun !add (x y)&#xA;    (forward (AddNode) (!copy a) b))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is natural to think this !copy is just a waste of memory space in some conditions, but tracing nodes can detect unused copies and delete them.&lt;/p&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe2/overview/#in-place-optimizing&#34;&gt;https://hikettei.github.io/cl-waffe2/overview/#in-place-optimizing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s more: &lt;code&gt;pre-computing of view offsets&lt;/code&gt; &lt;code&gt;generating optimal lisp code in real time for certain scalar operations&lt;/code&gt; &lt;code&gt;memory-allocation in advance&lt;/code&gt; &lt;code&gt;(TO BE) multi-threading&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;(TODO: Benchmarks on a different scales)&lt;/p&gt; &#xA;&lt;h2&gt;Tools for formulating networks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;defmodel&lt;/code&gt; defines a set of nodes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defmodel (Softmax-Model (self)&#xA;       :where (X[~] -&amp;gt; OUT[~])&#xA;       :on-call-&amp;gt; ((self x)&#xA;               (declare (ignore self))&#xA;               (let* ((x1 (!sub x (!mean x  :axis 1 :keepdims t)))&#xA;                      (z  (!sum   (!exp x1) :axis 1 :keepdims t)))&#xA;                  (!div (!exp x1) z)))))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It can be used to lazily evaluate and compile later, or to define functions for immediate execution.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;call&lt;/code&gt; to keep using lazy-evaluation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(call (Softmax-Model) (randn `(10 10)))&#xA;&#xA;{CPUTENSOR[float] :shape (10 10) :named ChainTMP13029 &#xA;  :vec-state [maybe-not-computed]&#xA;  &amp;lt;&amp;lt;Not-Embodied (10 10) Tensor&amp;gt;&amp;gt;&#xA;  :facet :input&#xA;  :requires-grad NIL&#xA;  :backward &amp;lt;Node: DIVNODE-LISPTENSOR (A[~] B[~] -&amp;gt; A[~])&amp;gt;}&#xA;&#xA;(proceed *) ;; Being Compiler Later, by proceed or build&#xA;{CPUTENSOR[float] :shape (10 10) :named ChainTMP13158 &#xA;  :vec-state [computed]&#xA;  ((0.05213483   0.11118897   0.107058994  ~ 0.1897892    0.055277593  0.028915826)                    &#xA;   (0.0025042535 0.3952663    0.0109358365 ~ 0.033085804  0.04627693   0.14064543)   &#xA;                 ...&#xA;   (0.067338936  0.06604112   0.065211095  ~ 0.051910892  0.10963429   0.060249455)&#xA;   (0.029982507  0.31893584   0.18214627   ~ 0.015864253  0.2993634    0.02982553))&#xA;  :facet :input&#xA;  :requires-grad NIL&#xA;  :backward &amp;lt;Node: PROCEEDNODE-T (A[~] -&amp;gt; A[~])&amp;gt;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;define-composite-function&lt;/code&gt; to define a function.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(define-composite-function (Softmax-Model) !softmax-static)&#xA;&#xA;(time (!softmax-static (randn `(10 10)))) ;; No compiling time for second and subsequent calls.&#xA;Evaluation took:&#xA;  0.000 seconds of real time&#xA;  0.000460 seconds of total run time (0.000418 user, 0.000042 system)&#xA;  100.00% CPU&#xA;  1,073,976 processor cycles&#xA;  32,512 bytes consed&#xA;  &#xA;{CPUTENSOR[float] :shape (10 10) :named ChainTMP13578 &#xA;  ((0.06095955   0.06010023   0.03573166   ~ 0.01910117   0.036269512  0.03422032)                    &#xA;   (0.3116705    0.041012052  0.012784039  ~ 0.08029219   0.062023237  0.03468513)   &#xA;                 ...&#xA;   (0.057693116  0.19069833   0.061993677  ~ 0.20243406   0.02019287   0.07737376)&#xA;   (0.35623857   0.038911298  0.028082697  ~ 0.050502267  0.024571734  0.10532298))&#xA;  :facet :input&#xA;  :requires-grad NIL&#xA;  :backward NIL}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There&#39;s more, &lt;code&gt;defnode&lt;/code&gt; is a generic definiiton of &lt;code&gt;AbstractNode&lt;/code&gt;, being implemented by &lt;code&gt;define-impl&lt;/code&gt; which works like a macro in Common Lisp. On the other hand, &lt;code&gt;define-static-node&lt;/code&gt; works like a &lt;code&gt;defun&lt;/code&gt;. For details, visit docs: &lt;a href=&#34;https://hikettei.github.io/cl-waffe2/overview/#network-units-node-and-composite&#34;&gt;https://hikettei.github.io/cl-waffe2/overview/#network-units-node-and-composite&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Numpy-like APIs&lt;/h2&gt; &#xA;&lt;p&gt;Except that you need to call &lt;code&gt;proceed&lt;/code&gt; or &lt;code&gt;build&lt;/code&gt; at the end of the operation, cl-waffe2 APIs was made to be similar to Numpy. In addition, cl-waffe2 is intended to work with REPL. (ease of debugging needs to be improved though...)&lt;/p&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://hikettei.github.io/cl-waffe2/base-impl/&#34;&gt;https://hikettei.github.io/cl-waffe2/base-impl/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;From the top level, it works simply.&lt;/h2&gt; &#xA;&lt;p&gt;The combination of delay evaluation and node definition mechanisms allows all the shapes of the network to be specified without the need to write special code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(defsequence MLP-Sequence (in-features hidden-dim out-features&#xA;               &amp;amp;key (activation #&#39;!tanh))&#xA;         &#34;3 Layers MLP&#34;&#xA;         (LinearLayer in-features hidden-dim)&#xA;         (asnode activation)&#xA;         (LinearLayer hidden-dim hidden-dim)&#xA;         (asnode activation)&#xA;         (LinearLayer hidden-dim out-features)&#xA;         (asnode #&#39;!softmax))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(MLP-Sequence 784 512 256)&#xA;&#xA;&amp;lt;Composite: MLP-SEQUENCE{W23852}(&#xA;    &amp;lt;&amp;lt;6 Layers Sequence&amp;gt;&amp;gt;&#xA;&#xA;[1/6]          ↓ &#xA;&amp;lt;Composite: LINEARLAYER{W23682}(&#xA;    &amp;lt;Input : ((~ BATCH-SIZE 784)) -&amp;gt; Output: ((~ BATCH-SIZE 512))&amp;gt;&#xA;&#xA;    WEIGHTS -&amp;gt; (512 784)&#xA;    BIAS    -&amp;gt; (512)&#xA;)&amp;gt;&#xA;[2/6]          ↓ &#xA;&amp;lt;Composite: ENCAPSULATED-NODE{W23680}(&#xA;    #&amp;lt;FUNCTION !TANH&amp;gt;&#xA;)&amp;gt;&#xA;[3/6]          ↓ &#xA;&amp;lt;Composite: LINEARLAYER{W23510}(&#xA;    &amp;lt;Input : ((~ BATCH-SIZE 512)) -&amp;gt; Output: ((~ BATCH-SIZE 512))&amp;gt;&#xA;&#xA;    WEIGHTS -&amp;gt; (512 512)&#xA;    BIAS    -&amp;gt; (512)&#xA;)&amp;gt;&#xA;[4/6]          ↓ &#xA;&amp;lt;Composite: ENCAPSULATED-NODE{W23508}(&#xA;    #&amp;lt;FUNCTION !TANH&amp;gt;&#xA;)&amp;gt;&#xA;[5/6]          ↓ &#xA;&amp;lt;Composite: LINEARLAYER{W23338}(&#xA;    &amp;lt;Input : ((~ BATCH-SIZE 512)) -&amp;gt; Output: ((~ BATCH-SIZE 256))&amp;gt;&#xA;&#xA;    WEIGHTS -&amp;gt; (256 512)&#xA;    BIAS    -&amp;gt; (256)&#xA;)&amp;gt;&#xA;[6/6]          ↓ &#xA;&amp;lt;Composite: ENCAPSULATED-NODE{W23336}(&#xA;    #&amp;lt;FUNCTION CL-WAFFE2/NN:!SOFTMAX&amp;gt;&#xA;)&amp;gt;)&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;References/Acknowledgments&lt;/h1&gt; &#xA;&lt;p&gt;All comments on this reddit post &lt;a href=&#34;https://www.reddit.com/r/Common_Lisp/comments/124da1l/does_anyone_have_any_interest_in_my_deeplearning/&#34;&gt;https://www.reddit.com/r/Common_Lisp/comments/124da1l/does_anyone_have_any_interest_in_my_deeplearning/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Features of Common Lispy approach to (Better) Numpy. (&lt;a href=&#34;https://gist.github.com/digikar99/ba2f0bb34021bfdc086b9c1c712ca228&#34;&gt;https://gist.github.com/digikar99/ba2f0bb34021bfdc086b9c1c712ca228&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.jsoftware.com/papers/RationalizedAPL.htm&#34;&gt;https://www.jsoftware.com/papers/RationalizedAPL.htm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1201.6035.pdf&#34;&gt;https://arxiv.org/pdf/1201.6035.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.european-lisp-symposium.org/static/2018/heisig.pdf&#34;&gt;https://www.european-lisp-symposium.org/static/2018/heisig.pdf&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/marcoheisig/Petalisp/tree/master&#34;&gt;https://github.com/marcoheisig/Petalisp/tree/master&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/numpy/numpy/tree/main&#34;&gt;https://github.com/numpy/numpy/tree/main&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/melisgl/mgl-mat&#34;&gt;https://github.com/melisgl/mgl-mat&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/359460.359482&#34;&gt;https://dl.acm.org/doi/pdf/10.1145/359460.359482&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://andantesoft.hatenablog.com/entry/2023/04/30/183032&#34;&gt;https://andantesoft.hatenablog.com/entry/2023/04/30/183032&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Marsaglia, G., &amp;amp; Tsang, W. W. (2000). The ziggurat method for generating random variables. Journal of statistical software.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://marui.hatenablog.com/entry/2023/01/23/194507&#34;&gt;https://marui.hatenablog.com/entry/2023/01/23/194507&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1912.01703&#34;&gt;https://arxiv.org/abs/1912.01703&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(More to be added...)&lt;/p&gt; &#xA;&lt;h1&gt;Workloads&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add Baseline: AbstractTensor&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multiple Backends&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pre-Inspection of Shapes&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fundamental APIs of view (Broadcasting, Slice, Indexing etc...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Obvious and Intuitive message of Shape-Error.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A small DSL to write subscripts (i.e.: :where keyword)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A fundamental of forward/backward, and JIT. (acceptor)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fundamental Dtypes&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Displaying Tensor&#39;s all element in a small area.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Scheduling and Optimizing the allocation of array, by analyzing the computation node.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Pruning the rebundant computation node.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;del&gt;Precompute the constant-node.&lt;/del&gt; (Partially done?)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Basic Arithmetic Operation (+ - * /, and gemm)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support: Row/Column Major Tensor, and &lt;code&gt;gemm&lt;/code&gt; (currently tests is too few.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;del&gt;Sampling distributions (dense) -&amp;gt; Add More: gamma, chisquare distribution.&lt;/del&gt; -&amp;gt; Add: &lt;code&gt;Orthogonal&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Sampling distributions (sparse)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Features on casting dtype is still not enough.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Parallelize the computation node by lparallel. (No need to do this?)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Let View APIs Support :indices :tflist (with cmp operations, bit-matrix)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; View API: permute first iteration&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add: &lt;del&gt;!reshape with -1, unsqueeze, squeeze, repeat.&lt;/del&gt; -&amp;gt; repeat/unsqueeze/squeeze is remained to be implemented.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; CUDA Backend (Also, metal backends etc could be realised).&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Give a strong features to cl-waffe2/viz&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Prepare documentations and examples (ongoing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Basic APIs for both LispTensor and CPUTensor. (To Add: gemm without BLAS, impelement it as NoBlasMatmulTensor because it is signifcantly slow)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Formulate specifications of nodes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Use Cl/CD&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;del&gt;REPL-Friendly Node, (Implemented as proceed function)&lt;/del&gt;, &lt;del&gt;with-dynamically-mode (no need to do this)&lt;/del&gt;, set-config&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; ascognitious&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; node debugtools&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Clarify runtime error, &lt;del&gt;backward error(OK)&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; NN Features (Optimizers, etc...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train MLP&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;del&gt;More clever Memory-management.&lt;/del&gt; -&amp;gt; Added memory-pool, but theres a lot of room to be improved.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mathematical and Dense Operations (exp log sin cos etc...)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Operations like: argmax/argmin, reshape, transpose, swapaxes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Optimize call-with-view, to minimize the number of using funcall. (i.e.: reshape (10 10) into (100) tensor)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Fix the issue where [~ a b] can&#39;t be applied to 2D Tensor.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimized Sparse Matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; FP16 Matrix&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add/Implement a SIMD Powered Backend for mathematical APIs. (named MathTensor), which provides (for example) approximation of exp in AVX512. It is not portable but written in C/C++ can called via cffi. (use SLEEF?)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; (After released v1.0) cl-waffe2 for coalton.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; cl-waffe2/linalg, SVD&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Distinguish the differences between Computed Tensor, and Not-Computed Tensor.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; AOT Subscript-p&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; optimize forward/backward&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; BugFix: !add x y &amp;lt;- x never resets. (the definition of sum contributed to this problem)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fix: a ton of style warning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; lparallel -&amp;gt; optimized-memory-allocation -&amp;gt; fast-math kernel, fp8 fp16, uint4 etc...&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add: Restarting&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>