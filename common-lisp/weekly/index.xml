<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub Common Lisp Weekly Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-28T01:56:07Z</updated>
  <subtitle>Weekly Trending of Common Lisp in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ck46/neuralisp</title>
    <updated>2023-05-28T01:56:07Z</updated>
    <id>tag:github.com,2023-05-28:/ck46/neuralisp</id>
    <link href="https://github.com/ck46/neuralisp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Neuralisp is a modular machine learning framework for Common Lisp, focused on deep learning models. It offers a high-performance tensor library, various neural network layers, activation functions, optimizers, and loss functions. Designed for rapid model creation, training, and testing across diverse tasks.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NeuraLisp&lt;/h1&gt; &#xA;&lt;p&gt;NeuraLisp is a machine learning framework for Common Lisp, designed for ease of use and extensibility. It provides a modular implementation for building, training, and evaluating neural networks for various machine learning tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Core components for tensor operations, autograd, GPU support, and more&lt;/li&gt; &#xA; &lt;li&gt;A variety of layers, including linear, convolutional, recurrent, and attention-based layers&lt;/li&gt; &#xA; &lt;li&gt;Common activation functions like ReLU, Sigmoid, and Tanh&lt;/li&gt; &#xA; &lt;li&gt;Loss functions like mean squared error and cross-entropy&lt;/li&gt; &#xA; &lt;li&gt;Optimizers like Stochastic Gradient Descent (SGD) and Adam&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To install NeuraLisp, clone the repository and load the system definition using Quicklisp or ASDF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/yourusername/NeuraLisp.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, in your Common Lisp REPL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(ql:quickload :NeuraLisp)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s an example of how to create a simple neural network using NeuraLisp:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-lisp&#34;&gt;(use-package :NeuraLisp.core.tensor)&#xA;(use-package :NeuraLisp.layers.linear)&#xA;(use-package :NeuraLisp.activations.relu)&#xA;&#xA;;; Create a linear layer&#xA;(defvar *layer* (make-instance &#39;linear :input-dim 3 :output-dim 2))&#xA;&#xA;;; Create input tensor&#xA;(defvar *input* (make-tensor #(1.0 2.0 3.0) #(1 3)))&#xA;&#xA;;; Apply the linear layer&#xA;(defvar *output* (forward *layer* *input*))&#xA;&#xA;;; Apply ReLU activation&#xA;(defvar *relu* (make-instance &#39;relu))&#xA;(defvar *activated-output* (activate *relu* *output*))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more examples, please refer to the &lt;code&gt;examples/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Detailed documentation for each component can be found in the &lt;code&gt;docs/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to NeuraLisp! If you find a bug, want to improve the code quality, or have ideas for new features, please feel free to create an issue or submit a pull request on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;NeuraLisp is licensed under the MIT License. Please see the &lt;code&gt;LICENSE&lt;/code&gt; file for more information.&lt;/p&gt;</summary>
  </entry>
</feed>