<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-24T01:28:53Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin</title>
    <updated>2022-12-24T01:28:53Z</updated>
    <id>tag:github.com,2022-12-24:/AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin</id>
    <link href="https://github.com/AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A user-friendly plug-in that makes it easy to generate stable diffusion images inside Photoshop using Automatic1111-sd-webui as a backend.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Auto-Photoshop-StableDiffusion-Plugin&lt;/h1&gt; &#xA;&lt;p&gt;With Auto-Photoshop-StableDiffusion-Plugin, you can directly use the capabilities of Automatic1111 Stable Diffusion in Photoshop without switching between programs. This allows you to easily use Stable Diffusion AI in a familiar environment. You can edit your Stable Diffusion image with all your favorite tools and save it right in Photoshop.&lt;/p&gt; &#xA;&lt;h3&gt;Click the picture to watch the demo:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/VL_gbQai79E&#34; title=&#34;Stable diffusion AI Photoshop Plugin Free and Open Source&#34;&gt;&lt;img src=&#34;https://i3.ytimg.com/vi/VL_gbQai79E/maxresdefault.jpg&#34; alt=&#34;Click Here to Watch Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;How to install:&lt;/h1&gt; &#xA;&lt;h2&gt;First time runing the plugin:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;download the plugin:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/AbdullahAlfaraj/Auto-Photoshop-StableDiffusion-Plugin.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;run &#34;start_server.bat&#34; inside &#34;Auto-Photoshop-StableDiffusion-Plugin&#34; directory&lt;/li&gt; &#xA; &lt;li&gt;go to where you have &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;automatic1111&lt;/a&gt; installed. Edit the &#34;webui-user.bat&#34; in automatic1111 change this line&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;set COMMANDLINE_ARGS= &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;set COMMANDLINE_ARGS= --api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;that will allow the plugin to communicate with the automatic1111 project. After saving close the &#34;webui-user.bat&#34; file and run it normally.&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;run photoshop. go to edit -&amp;gt; prefrences -&amp;gt; plugins &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;make sure you check &#34;Enable Developer Mode&#34; checkbox&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;install &#34;Adobe UXP Developer Tool&#34; from here &lt;a href=&#34;https://developer.adobe.com/photoshop/uxp/devtool/installation/&#34;&gt;Installation (adobe.com)&lt;/a&gt; this tool will add the plugin into photoshop&lt;/li&gt; &#xA; &lt;li&gt;run Adobe UXP Developer Tool and click on &#34;Add Plugin&#34; button in the top right. Navigate to where you have &#34;Auto-Photoshop-StableDiffusion-Plugin&#34; folder and open &#34;manifest.json&#34;&lt;/li&gt; &#xA; &lt;li&gt;select the plugin and click on Actions -&amp;gt; Load Selected that&#39;s it.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Steps to run the plugin for second time and onward:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;start &#34;webui-user.bat&#34;&lt;/li&gt; &#xA; &lt;li&gt;start &#34;start_server.bat&#34;&lt;/li&gt; &#xA; &lt;li&gt;start &#34;Photoshop&#34;&lt;/li&gt; &#xA; &lt;li&gt;start &#34; Adobe UXP Developer Tool&#34; and load the plugin&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to Use the Plugin:&lt;/h2&gt; &#xA;&lt;h3&gt;Quick test:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In photoshop, start a new project.&lt;/li&gt; &#xA; &lt;li&gt;After loading the plugin, don&#39;t select or change anything. Just click on the &#34;Generate&#34; button&lt;/li&gt; &#xA; &lt;li&gt;If you see an image of a cat get loaded onto the canvas, then everything is set up correctly.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you have an issue even after reading the following section. Please consider starting a new issue and/or join us on our &lt;a href=&#34;https://discord.gg/3mVEtrddXJ&#34;&gt;discord&lt;/a&gt; for real-time feedback.&lt;/p&gt; &#xA;&lt;h3&gt;txt2Img:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;use the rectangular marquee tool and select square (1x1 ratio) &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;in the selection tool you can set the ratio to something like 512x512 or 512x768&lt;/li&gt; &#xA;   &lt;li&gt;just make sure the ratio of selection is equal to the size of the image you are generating.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Write a prompt and click &#34;Generate&#34;.&lt;/li&gt; &#xA; &lt;li&gt;The result will be resized to fit the selected area. But don&#39;t worry, the image will be loaded into photoshop as a smart object. So you can resize it without losing quality.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;img2img:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Select an image that is on its own layer. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Optional: &#34;Ctrl + click&#34; the layer thumbnail. If you want the generated image to be place perfectly on the Initial image.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Change the mode to &#34;img2img&#34; in the plugin UI.&lt;/li&gt; &#xA; &lt;li&gt;Click on &#34;Set Init Image&#34; button. If the image doesn&#39;t change to the selected layer. Click multiple times. (I&#39;m fixing this problem)&lt;/li&gt; &#xA; &lt;li&gt;Click on generate&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;inpaint:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Change the plugin mode to inpaint&lt;/li&gt; &#xA; &lt;li&gt;Create a square selection on top of an image you want to inpaint: &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;through the rectangular marquee tool&lt;/li&gt; &#xA;   &lt;li&gt;or through &#34;ctrl+click&#34; the layer thumbnail&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create a new layer and paint white within the selected area. Note that anything white will be regenerated by Stable Diffusion.&lt;/li&gt; &#xA; &lt;li&gt;Click &#34;Init Inpaint Mask&#34; button. &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Before clicking make sure you are still on the white layer&lt;/li&gt; &#xA;   &lt;li&gt;after clicking the plugin will generate an appropriate black and white mask and set it as the mask to be used by Stable Diffusion.&lt;/li&gt; &#xA;   &lt;li&gt;It will also create a snapshot of the canvas under the selected area, and will use this snapshot as the init image.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Click the &#34;Generate&#34; button.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;outpaint:&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;change the plugin mode to inpaint. Remember, outpainting is just a special case of inpainting.&lt;/li&gt; &#xA; &lt;li&gt;Create a &#34;rectangular selction&#34; that intersect with an image you want to extend.&lt;/li&gt; &#xA; &lt;li&gt;Click on &#34;Init Outpaint Mask&#34; this will : &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;create a snapshot of the canvas under your selection and use it as &#34;init image&#34; for Stable Diffusion&lt;/li&gt; &#xA;   &lt;li&gt;create a black and white mask and will use it as the &#34;init mask&#34;&lt;/li&gt; &#xA;   &lt;li&gt;it will update both the &#34;init image&#34; and the &#34;init mask&#34; in plugin UI&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Rule of thumbs:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To get the best result for inpainting and outpainting you must select the &#34;stable-diffusion inpainting model&#34;&lt;/li&gt; &#xA; &lt;li&gt;An &#34;init image&#34; of img2img must not have transparency in it.&lt;/li&gt; &#xA; &lt;li&gt;Always check the &#34;init image&#34; and the &#34;init mask&#34; in plugin UI and make sure they match the layers on the canvas. To fix a mismatch, do: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Select the &#34;group_mask&#34; in the layers panel and click on &#34;set init mask&#34; button.&lt;/li&gt; &#xA;   &lt;li&gt;Select the &#34;group_init_image&#34; in the layers panel and click on &#34;set init image&#34; button.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;After selecting a model, you may need to wait for it to be loaded in Stable Diffusion before you should hit Generate.&lt;/li&gt; &#xA; &lt;li&gt;Check the progress bar if it&#39;s stuck at 0% or 1%: You could always cancel/interrupt the request if you think it&#39;s taking too long and regenerated again.&lt;/li&gt; &#xA; &lt;li&gt;Don&#39;t open multiple photoshop documents. For now, this will break the plugin. Only work on one project at a time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Issues with img2img, inpaint or outpaint:&lt;/h3&gt; &#xA;&lt;p&gt;img2img, inpaint and outpaint use photoshop &#34;quick export as png&#34; feature under the hood.&lt;/p&gt; &#xA;&lt;p&gt;At random quick export as png will break, this is a known photoshop bug.&lt;/p&gt; &#xA;&lt;p&gt;To test if the feature is broken and is the cause of your issue, do the following:&lt;/p&gt; &#xA;&lt;p&gt;select a layer and right click on it, then select quick export as png. if you get prompted with a windows, then the quick export works fine. However if nothing happen then you will need to restart photoshop and try the export feature again, until it works.&lt;/p&gt; &#xA;&lt;p&gt;if it work then you could procced to use the plugin.&lt;/p&gt; &#xA;&lt;p&gt;until you get it to work the img2img, inpaint and outpaint will not function correctly.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>JhumanJ/OpnForm</title>
    <updated>2022-12-24T01:28:53Z</updated>
    <id>tag:github.com,2022-12-24:/JhumanJ/OpnForm</id>
    <link href="https://github.com/JhumanJ/OpnForm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple and open-source form builder&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpnForm&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/JhumanJ/OpnForm/raw/main/public/img/social-preview.jpg?raw=true&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/jhumanj/OpnForm/actions&#34;&gt;&lt;img src=&#34;https://github.com/jhumanj/laravel-vue-tailwind-spa/workflows/tests/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;An open-source form builder. It&#39;s an alternative to products like Typeform, JotForm, Tally etc.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No-code form builder, with infinite number of fields &amp;amp; submissions&lt;/li&gt; &#xA; &lt;li&gt;Text inputs, Date inputs, URL inputs, Phone inputs, Email inputs, Checkboxes, Select and Multi-Select inputs, Number Inputs, Star-ratings, File uploads &amp;amp; more&lt;/li&gt; &#xA; &lt;li&gt;Embed anywhere (on your website, in your Notion page, etc)&lt;/li&gt; &#xA; &lt;li&gt;Email notifications (for both form owner &amp;amp; form respondents)&lt;/li&gt; &#xA; &lt;li&gt;Hidden fields&lt;/li&gt; &#xA; &lt;li&gt;Form passwords&lt;/li&gt; &#xA; &lt;li&gt;URL form pre-fill&lt;/li&gt; &#xA; &lt;li&gt;Slack integration&lt;/li&gt; &#xA; &lt;li&gt;Webhooks&lt;/li&gt; &#xA; &lt;li&gt;Form logic&lt;/li&gt; &#xA; &lt;li&gt;Customize colors, add images or even some custom code&lt;/li&gt; &#xA; &lt;li&gt;Captcha form protection&lt;/li&gt; &#xA; &lt;li&gt;Form closing date&lt;/li&gt; &#xA; &lt;li&gt;Limit the number of submissions allowed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And much more!&lt;/p&gt; &#xA;&lt;h2&gt;Getting started with OpnForm&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to get started with OpnForm is with the &lt;a href=&#34;https://opnform.com/&#34;&gt;official managed service in the Cloud&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It takes 1 minute to try out the builder for free. You&#39;ll have high availability, backups, security, and maintenance all managed for you.&lt;/p&gt; &#xA;&lt;h2&gt;Self-hosting&lt;/h2&gt; &#xA;&lt;p&gt;This section hasn&#39;t been written yet, we need to work on this.&lt;/p&gt; &#xA;&lt;h2&gt;Tech Stack&lt;/h2&gt; &#xA;&lt;p&gt;OpnForm is a standard web application built with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://laravel.com/&#34;&gt;Laravel&lt;/a&gt; PHP framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vuejs.org/&#34;&gt;Vue.js&lt;/a&gt; front-end framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tailwindcss.com/&#34;&gt;TailwindCSS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;re more than welcome to contribute to this project. We don&#39;t have guidelines on this yet, but we will soon. In the meantime, feel free to ask &lt;a href=&#34;https://github.com/JhumanJ/OpnForm/discussions&#34;&gt;any question here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;OpnForm is open-source under the GNU Affero General Public License Version 3 (AGPLv3) or any later version. You can find it &lt;a href=&#34;https://github.com/JhumanJ/OpnForm/raw/main/LICENSE&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>m-bain/whisperX</title>
    <updated>2022-12-24T01:28:53Z</updated>
    <id>tag:github.com,2022-12-24:/m-bain/whisperX</id>
    <link href="https://github.com/m-bain/whisperX" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WhisperX: Automatic Speech Recognition with Accurate Word-level Timestamps.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;WhisperX&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/m-bain/whisperX/stargazers&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/m-bain/whisperX.svg?colorA=orange&amp;amp;colorB=orange&amp;amp;logo=github&#34; alt=&#34;GitHub stars&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/m-bain/whisperX/issues&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues/m-bain/whisperx.svg?sanitize=true&#34; alt=&#34;GitHub issues&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/m-bain/whisperX/raw/master/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/m-bain/whisperX.svg?sanitize=true&#34; alt=&#34;GitHub license&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/intent/tweet?text=&amp;amp;url=https%3A%2F%2Fgithub.com%2Fm-bain%2FwhisperX&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/url/https/github.com/m-bain/whisperX.svg?style=social&#34; alt=&#34;Twitter&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/m-bain/whisperX/main/#what-is-it&#34;&gt;What is it&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/m-bain/whisperX/main/#setup&#34;&gt;Setup&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/m-bain/whisperX/main/#example&#34;&gt;Usage&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/m-bain/whisperX/main/#other-languages&#34;&gt;Multilingual&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/m-bain/whisperX/main/#python-usage&#34;&gt;Python&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/m-bain/whisperX/main/#contribute&#34;&gt;Contribute&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/m-bain/whisperX/main/EXAMPLES.md&#34;&gt;More examples&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h6 align=&#34;center&#34;&gt;Made by Max Bain • &lt;span&gt;🌐&lt;/span&gt; &lt;a href=&#34;https://www.maxbain.com&#34;&gt;https://www.maxbain.com&lt;/a&gt;&lt;/h6&gt; &#xA;&lt;img width=&#34;1216&#34; align=&#34;center&#34; alt=&#34;whisperx-arch&#34; src=&#34;https://user-images.githubusercontent.com/36994049/208313881-903ab3ea-4932-45fd-b3dc-70876cddaaa2.png&#34;&gt; &#xA;&lt;p align=&#34;left&#34;&gt;Whisper-Based Automatic Speech Recognition (ASR) with improved timestamp accuracy using forced alignment. &lt;/p&gt; &#xA;&lt;h2 align=&#34;left&#34; , id=&#34;what-is-it&#34;&gt;What is it 🔎&lt;/h2&gt; &#xA;&lt;p&gt;This repository refines the timestamps of openAI&#39;s Whisper model via forced aligment with phoneme-based ASR models (e.g. wav2vec2.0), multilingual use-case.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Whisper&lt;/strong&gt; is an ASR model &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;developed by OpenAI&lt;/a&gt;, trained on a large dataset of diverse audio. Whilst it does produces highly accurate transcriptions, the corresponding timestamps are at the utterance-level, not per word, and can be inaccurate by several seconds.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Phoneme-Based ASR&lt;/strong&gt; A suite of models finetuned to recognise the smallest unit of speech distinguishing one word from another, e.g. the element p in &#34;tap&#34;. A popular example model is &lt;a href=&#34;https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self&#34;&gt;wav2vec2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Forced Alignment&lt;/strong&gt; refers to the process by which orthographic transcriptions are aligned to audio recordings to automatically generate phone level segmentation.&lt;/p&gt; &#xA;&lt;h2 align=&#34;left&#34; id=&#34;setup&#34;&gt;Setup ⚙️&lt;/h2&gt; Install this package using &#xA;&lt;p&gt;&lt;code&gt;pip install git+https://github.com/m-bain/whisperx.git&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You may also need to install ffmpeg, rust etc. Follow openAI instructions here &lt;a href=&#34;https://github.com/openai/whisper#setup&#34;&gt;https://github.com/openai/whisper#setup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2 align=&#34;left&#34; id=&#34;example&#34;&gt;Usage 💬 (command line)&lt;/h2&gt; &#xA;&lt;h3&gt;English&lt;/h3&gt; &#xA;&lt;p&gt;Run whisper on example segment (using default params)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisperx examples/sample01.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For increased timestamp accuracy, at the cost of higher gpu mem, use bigger models e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisperx examples/sample01.wav --model large.en --align_model WAV2VEC2_ASR_LARGE_LV60K_960H&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Result using &lt;em&gt;WhisperX&lt;/em&gt; with forced alignment to wav2vec2.0 large:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/36994049/208253969-7e35fe2a-7541-434a-ae91-8e919540555d.mp4&#34;&gt;https://user-images.githubusercontent.com/36994049/208253969-7e35fe2a-7541-434a-ae91-8e919540555d.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Compare this to original whisper out the box, where many transcriptions are out of sync:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/36994049/207743923-b4f0d537-29ae-4be2-b404-bb941db73652.mov&#34;&gt;https://user-images.githubusercontent.com/36994049/207743923-b4f0d537-29ae-4be2-b404-bb941db73652.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Other languages&lt;/h3&gt; &#xA;&lt;p&gt;The phoneme ASR alignment model is &lt;em&gt;language-specific&lt;/em&gt;, for tested languages these models are &lt;a href=&#34;https://github.com/m-bain/whisperX/raw/e909f2f766b23b2000f2d95df41f9b844ac53e49/whisperx/transcribe.py#L22&#34;&gt;automatically picked from torchaudio pipelines or huggingface&lt;/a&gt;. Just pass in the &lt;code&gt;--language&lt;/code&gt; code, and use the whisper &lt;code&gt;--model large&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Currently default models provided for &lt;code&gt;{en, fr, de, es, it, ja, zh, nl}&lt;/code&gt;. If the detected language is not in this list, you need to find a phoneme-based ASR model from &lt;a href=&#34;https://huggingface.co/models&#34;&gt;huggingface model hub&lt;/a&gt; and test it on your data.&lt;/p&gt; &#xA;&lt;h4&gt;E.g. German&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;whisperx --model large --language de examples/sample_de_01.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/36994049/208298811-e36002ba-3698-4731-97d4-0aebd07e0eb3.mov&#34;&gt;https://user-images.githubusercontent.com/36994049/208298811-e36002ba-3698-4731-97d4-0aebd07e0eb3.mov&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Python usage 🐍&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import whisperx&#xA;&#xA;device = &#34;cuda&#34; &#xA;audio_file = &#34;audio.mp3&#34;&#xA;&#xA;# transcribe with original whisper&#xA;model = whisperx.load_model(&#34;large&#34;, device)&#xA;result = model.transcribe(audio_file)&#xA;&#xA;# load alignment model and metadata&#xA;model_a, metadata = whisperx.load_align_model(language_code=result[&#34;language&#34;], device=device)&#xA;&#xA;# align whisper output&#xA;result_aligned = whisperx.align(result[&#34;segments&#34;], model_a, metadata, audio_file, device)&#xA;&#xA;print(result[&#34;segments&#34;]) # before alignment&#xA;&#xA;print(result_aligned[&#34;segments&#34;]) # after alignment&#xA;print(result_aligned[&#34;word_segments&#34;]) # after alignment&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2 align=&#34;left&#34; id=&#34;limitations&#34;&gt;Limitations ⚠️&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Not thoroughly tested, especially for non-english, results may vary -- please post issue to let me know the results on your data&lt;/li&gt; &#xA; &lt;li&gt;Whisper normalises spoken numbers e.g. &#34;fifty seven&#34; to arabic numerals &#34;57&#34;. Need to perform this normalization after alignment, so the phonemes can be aligned. Currently just ignores numbers.&lt;/li&gt; &#xA; &lt;li&gt;Assumes the initial whisper timestamps are accurate to some degree (within margin of 2 seconds, adjust if needed -- bigger margins more prone to alignment errors)&lt;/li&gt; &#xA; &lt;li&gt;Hacked this up quite quickly, there might be some errors, please raise an issue if you encounter any.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2 align=&#34;left&#34; id=&#34;contribute&#34;&gt;Contribute 🧑‍🏫&lt;/h2&gt; &#xA;&lt;p&gt;If you are multilingual, a major way you can contribute to this project is to find phoneme models on huggingface (or train your own) and test them on speech for the target language. If the results look good send a merge request and some examples showing its success.&lt;/p&gt; &#xA;&lt;p&gt;The next major upgrade we are working on is whisper with speaker diarization, so if you have any experience on this please share.&lt;/p&gt; &#xA;&lt;h2 align=&#34;left&#34; id=&#34;coming-soon&#34;&gt;Coming Soon 🗓&lt;/h2&gt; &#xA;&lt;p&gt;[x] &lt;del&gt;Multilingual init&lt;/del&gt; done&lt;/p&gt; &#xA;&lt;p&gt;[x] &lt;del&gt;Subtitle .ass output&lt;/del&gt; done&lt;/p&gt; &#xA;&lt;p&gt;[x] &lt;del&gt;Automatic align model selection based on language detection&lt;/del&gt; done&lt;/p&gt; &#xA;&lt;p&gt;[x] &lt;del&gt;Python usage&lt;/del&gt; done&lt;/p&gt; &#xA;&lt;p&gt;[ ] Incorporating word-level speaker diarization&lt;/p&gt; &#xA;&lt;p&gt;[ ] Inference speedup with batch processing&lt;/p&gt; &#xA;&lt;h2 align=&#34;left&#34; id=&#34;contact&#34;&gt;Contact 📇&lt;/h2&gt; &#xA;&lt;p&gt;Contact maxbain[at]robots[dot]ox[dot]ac[dot]uk for business things.&lt;/p&gt; &#xA;&lt;h2 align=&#34;left&#34; id=&#34;acks&#34;&gt;Acknowledgements 🙏&lt;/h2&gt; &#xA;&lt;p&gt;Of course, this is mostly just a modification to &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;openAI&#39;s whisper&lt;/a&gt;. As well as accreditation to this &lt;a href=&#34;https://pytorch.org/tutorials/intermediate/forced_alignment_with_torchaudio_tutorial.html&#34;&gt;PyTorch tutorial on forced alignment&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2 align=&#34;left&#34; id=&#34;cite&#34;&gt;Citation&lt;/h2&gt; If you use this in your research, just cite the repo, &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{bain2022whisperx,&#xA;  author = {Bain, Max},&#xA;  title = {WhisperX},&#xA;  year = {2022},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/m-bain/whisperX}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;as well as the whisper paper,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{radford2022robust,&#xA;  title={Robust speech recognition via large-scale weak supervision},&#xA;  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},&#xA;  journal={arXiv preprint arXiv:2212.04356},&#xA;  year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and any alignment model used, e.g. wav2vec2.0.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{baevski2020wav2vec,&#xA;  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},&#xA;  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},&#xA;  journal={Advances in Neural Information Processing Systems},&#xA;  volume={33},&#xA;  pages={12449--12460},&#xA;  year={2020}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>