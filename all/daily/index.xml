<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-22T01:29:01Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>haotian-liu/LLaVA</title>
    <updated>2023-04-22T01:29:01Z</updated>
    <id>tag:github.com,2023-04-22:/haotian-liu/LLaVA</id>
    <link href="https://github.com/haotian-liu/LLaVA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large Language-and-Vision Assistant built towards multimodal GPT-4 level capabilities.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🌋 LLaVA: Large Language and Vision Assistant&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Visual instruction tuning towards large language and vision models with GPT-4 level capabilities.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;Project Page&lt;/a&gt;] [&lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;Paper&lt;/a&gt;] [&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;Demo&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;Data&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0&#34;&gt;Model&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Visual Instruction Tuning&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://hliu.cc&#34;&gt;Haotian Liu*&lt;/a&gt;, &lt;a href=&#34;https://chunyuan.li/&#34;&gt;Chunyuan Li*&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.ca/citations?user=HDiw-TsAAAAJ&amp;amp;hl=en/&#34;&gt;Qingyang Wu&lt;/a&gt;, &lt;a href=&#34;https://pages.cs.wisc.edu/~yongjaelee/&#34;&gt;Yong Jae Lee&lt;/a&gt; (*Equal Contribution)&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥 We released &lt;strong&gt;LLaVA: Large Language and Vision Assistant&lt;/strong&gt;. We propose visual instruction tuning, towards building large language and vision models with GPT-4 level capabilities. Checkout the &lt;a href=&#34;https://arxiv.org/abs/2304.08485&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/assets/demo.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;strong&gt;Usage and License Notices&lt;/strong&gt;: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#data-donwnload&#34;&gt;Data Donwnload&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#llava-weights&#34;&gt;LLaVA Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#serving&#34;&gt;Serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Data Donwnload&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data file name&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/llava_instruct_150k.json&#34;&gt;llava_instruct_150k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;229 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/conversation_58k.json&#34;&gt;conversation_58k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;126 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/detail_23k.json&#34;&gt;detail_23k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;20.5 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/raw/main/complex_reasoning_77k.json&#34;&gt;complex_reasoning_77k.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.6 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To download our langauge-image multimodal instruction-folllowing dataset &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;&lt;code&gt;LLaVA-Instruct-150K&lt;/code&gt;&lt;/a&gt;, please run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh download_data.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretraining Dataset&lt;/h3&gt; &#xA;&lt;p&gt;The pretraining dataset used in this release is a subset of CC-3M dataset, filtered with a more balanced concept coverage distribution. Please see &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K&#34;&gt;here&lt;/a&gt; for a detailed description on the dataset structure and how to download the images.&lt;/p&gt; &#xA;&lt;p&gt;If you already have CC-3M dataset on your disk, the image names follow this format: &lt;code&gt;GCC_train_000000000.jpg&lt;/code&gt;. You may edit the &lt;code&gt;image&lt;/code&gt; field correspondingly if necessary.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;Chat File&lt;/th&gt; &#xA;   &lt;th&gt;Meta Data&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CC-3M Pretrain 595K&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/raw/main/chat.json&#34;&gt;chat.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K/raw/main/metadata.json&#34;&gt;metadata.json&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;211 MB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to LLaVA folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/haotian-liu/LLaVA.git&#xA;cd LLaVA&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n llava python=3.10 -y&#xA;conda activate llava&#xA;pip install --upgrade pip  # enable PEP 660 support&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: In this research preview, we used a modified version of huggingface/transformers library to support multimodal models and the LLaMA tokenizer. Make sure that you create a new Conda environment and you are using the correct transformers library from &lt;a href=&#34;https://github.com/haotian-liu/transformers_llava&#34;&gt;https://github.com/haotian-liu/transformers_llava&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may try running the following command to make sure the version is correct.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;pip install git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install additional packages for training cases&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install ninja&#xA;pip install flash-attn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LLaVA Weights&lt;/h2&gt; &#xA;&lt;p&gt;We release &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt; weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the LLaVA weights.&lt;/p&gt; &#xA;&lt;p&gt;Instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the original LLaMA weights in the huggingface format by following the instructions &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the following scripts to get LLaVA weights by applying &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-delta-v0&#34;&gt;our delta&lt;/a&gt;. It will automatically download delta weights from our Hugging Face account.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;LLaVA-13B&lt;/h3&gt; &#xA;&lt;p&gt;This conversion command needs around 60 GB of CPU RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m llava.model.apply_delta \&#xA;    --base /path/to/llama-13b \&#xA;    --target /output/path/to/LLaVA-13B-v0 \&#xA;    --delta liuhaotian/LLaVA-13b-delta-v0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;LLaVA-7B&lt;/h3&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt; &#xA;&lt;h3&gt;LLaVA pretrained projector weights&lt;/h3&gt; &#xA;&lt;p&gt;The initial release is pretrained on &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K&#34;&gt;LLaVA-filtered CC3M 595K&lt;/a&gt; with 1 epoch. The pretrained weights are released &lt;a href=&#34;https://huggingface.co/liuhaotian/LLaVA-13b-pretrain-projector-v0&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You may perform instruction tuning on our pretrained checkpoints, by using our &lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;visual instruction tuning&lt;/a&gt; data following the instructions &lt;a href=&#34;https://github.com/haotian-liu/LLaVA#fine-tuning-with-local-gpus&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Serving&lt;/h2&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.controller --host 0.0.0.0 --port 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 --multi-modal&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;.&lt;/p&gt; &#xA;&lt;h4&gt;Launch a model worker (Multiple GPUs, when GPU VRAM &amp;lt;= 24GB)&lt;/h4&gt; &#xA;&lt;p&gt;If your the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.), you may try running it with multiple GPUs.&lt;/p&gt; &#xA;&lt;p&gt;If you install our repo before 4/20/23, you may need to reinstall our fork of &lt;code&gt;transformers&lt;/code&gt;: &lt;code&gt;pip install git+https://github.com/haotian-liu/transformers_llava.git@988b6abb3b7da9a5cbb5051e994706f7f88c2565&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.model_worker --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path ./checkpoints/LLaVA-13B-v0 --multi-modal --num-gpus 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;.&lt;/p&gt; &#xA;&lt;h4&gt;Send a test message&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.test_message --model-name LLaVA-13B-v0 --controller http://localhost:10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python -m llava.serve.gradio_web_server --controller http://localhost:10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;You can open your browser and chat with a model now.&lt;/h4&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;GPT-assisted Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Our GPT-assisted evaluation pipeline for multimodal modeling is provided for a comprehensive understanding of the capabilities of vision-language models. Please see our paper for more details.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Generate LLaVA responses&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python model_vqa.py \&#xA;    --model-name ./checkpoints/LLaVA-13B-v0 \&#xA;    --question-file \&#xA;    playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --image-folder \&#xA;    /path/to/coco2014_val \&#xA;    --answers-file \&#xA;    /path/to/answer-file.jsonl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Evaluate the generated responses&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;OPENAI_API_KEY=&#34;sk-***********************************&#34; python eval_gpt_review_visual.py \&#xA;    --question playground/data/coco2014_val_qa_eval/qa90_questions.jsonl \&#xA;    --context table/caps_boxes_coco2014_val_80.jsonl \&#xA;    --answer-list \&#xA;    /path/to/answer-file-1.jsonl \&#xA;    /path/to/answer-file-2.jsonl \&#xA;    --rule table/rule.json \&#xA;    --output /path/to/review.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Summarize the evaluation results&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python summarize_gpt_review.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ScienceQA&lt;/h3&gt; &#xA;&lt;p&gt;Please see ScienceQA &lt;a href=&#34;https://github.com/lupantech/ScienceQA&#34;&gt;repo&lt;/a&gt; for setting up the dataset. You may either use the official ScienceQA evaluation script or &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/eval_science_qa.py&#34;&gt;our script&lt;/a&gt; to evaluate the model.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;The current version of LLaVA is fine-tuned from a Vicuna-13B model. We use approximately 600K filtered CC3M in feature alignment pretraining and 150K GPT-generated multimodal instruction-following data in finetuning. For detailed description of the data generation pipeline, please refer see our &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We are working on a more capable model that is pretrained with the data at a larger scale. Stay tuned!&lt;/p&gt; &#xA;&lt;p&gt;We release all three types of multimodal instruction-following data. The use of these data is subject to OpenAI &lt;a href=&#34;https://raw.githubusercontent.com/haotian-liu/LLaVA/main/#&#34;&gt;TOS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Code and Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;We fine-tune the model using the code from &lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;FastChat&lt;/a&gt;. We use a similar set of hyperparameters as Vicuna in finetuning. Both hyperparameters used in pretraining and finetuning are provided below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Fine-tuning with Local GPUs&lt;/h3&gt; &#xA;&lt;p&gt;LLaVA is trained on 8 A100 GPUs with 80GB memory with the following code. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly to keep the global batch size the same.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Pretraining&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path ./checkpoints/llama-vicuna-13b \&#xA;    --data_path /path/to/cc3m_595k.json \&#xA;    --image_folder /path/to/cc3m_595k \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --tune_mm_mlp_adapter True \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints/llava-13b-pretrain \&#xA;    --num_train_epochs 1 \&#xA;    --per_device_train_batch_size 16 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 2400 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-3 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Extract projector features&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;python scripts/extract_mm_projector.py \&#xA;  --model_name_or_path ./checkpoints/llava-13b-pretrain \&#xA;  --output ./checkpoints/mm_projector/llava-13b-pretrain.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Finetuning&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=25001 \&#xA;    llava/train/train_mem.py \&#xA;    --model_name_or_path /path/to/llama-vicuna-13b \&#xA;    --data_path /path/to/llava_instruct_150k.json \&#xA;    --image_folder /Data/haotian/coco/train2017 \&#xA;    --vision_tower openai/clip-vit-large-patch14 \&#xA;    --pretrain_mm_mlp_adapter ./checkpoints/mm_projector/llava-13b-pretrain.bin \&#xA;    --mm_vision_select_layer -2 \&#xA;    --mm_use_im_start_end True \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 5000 \&#xA;    --save_total_limit 3 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --tf32 True \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True \&#xA;    --report_to wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the codebase we built upon, and our base model Vicuna-13B that has the amazing language capabilities!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you find LLaVA useful for your your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{liu2023llava,&#xA;      title={Visual Instruction Tuning}, &#xA;      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},&#xA;      publisher={arXiv:2304.08485},&#xA;      year={2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM&#34;&gt;Instruction Tuning with GPT-4&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For future project ideas, checkout the &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; to detect, segment, and generate anything by marrying &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>tsale/EDR-Telemetry</title>
    <updated>2023-04-22T01:29:01Z</updated>
    <id>tag:github.com,2023-04-22:/tsale/EDR-Telemetry</id>
    <link href="https://github.com/tsale/EDR-Telemetry" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project aims to compare and evaluate the telemetry of various EDR products.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EDR Telemetry&lt;/h1&gt; &#xA;&lt;p&gt;This repo provides a list of &lt;em&gt;&lt;strong&gt;telemetry features&lt;/strong&gt;&lt;/em&gt; from EDR products and other endpoint agents such as &lt;a href=&#34;https://learn.microsoft.com/en-us/sysinternals/downloads/sysmon&#34;&gt;Sysmon&lt;/a&gt; broken down by category. The main motivation behind this project is to enable security practitioners to compare and evaluate the telemetry potential from those tools while encouraging EDR vendors to be more transparent about the telemetry features they do provide to their users and customers.&lt;/p&gt; &#xA;&lt;p&gt;Besides compliance, investigations and forensics benefits, rich log telemetry empowers cyber defense teams to develop custom hunting, detection and analytics capabilities tailored to their needs.&lt;/p&gt; &#xA;&lt;p&gt;Read details about this project in the initial release blog post &lt;a href=&#34;https://kostas-ts.medium.com/edr-telemetry-project-a-comprehensive-comparison-d5ed1745384b&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Telemetry Definition&lt;/h2&gt; &#xA;&lt;p&gt;There are many types of &lt;em&gt;telemetry&lt;/em&gt; when it comes to Security Instrumentation. Here we focus on agents or sensors generating telemetry in the form of &lt;em&gt;log data&lt;/em&gt;, regardless of the format (json, key-value, csv), as long as the data is automatically generated and transmitted or streamed in near real-time.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ &amp;amp; Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Please check our &lt;a href=&#34;https://github.com/tsale/EDR-Telemetry/wiki/FAQ&#34;&gt;FAQ&lt;/a&gt; page to know more and feel free to get in contact in case you cannot find an answer there.&lt;/p&gt; &#xA;&lt;p&gt;In case you ware willing to contribute, please check the &lt;a href=&#34;https://github.com/tsale/EDR-Telemetry/wiki#contribution-guidelines&#34;&gt;Contributions&lt;/a&gt; page.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;br&gt; The telemetry of the EDR products below could improve with time. The &lt;code&gt;last_updated&lt;/code&gt; field is the last time the data sources have been updated. This might NOT always be up to date with the current telemetry capabilities of each product.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Telemetry Comparison Table&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;br&gt; The data below do not represent the capability of each of the EDR products to detect or prevent a threat. This is ONLY a comparison regarding the available telemetry for each product. Some products, such as Elastic EDR, make additional telemetry available in free or paid modules. Add-on modules, as well as signals, will not be taken into consideration for this project. Please read more about this on our FAQ page &lt;a href=&#34;https://github.com/tsale/EDR-Telemetry/wiki/FAQ#7-what-is-the-scope-of-the-telemetry-comparison-table-for-edr-products&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;br&gt; 🟩 = Implemented&lt;br&gt; 🟥 = Not Implemented&lt;br&gt; 🟧 = Partially Implemented&lt;br&gt; ❓ = Pending Response&lt;br&gt; 🪵 = Via Windows EventLogs (with proper Audit policy)&lt;br&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Last Updated:&lt;/strong&gt; Wed Apr 19 2023&lt;br&gt; &lt;strong&gt;Google SpreadSheet Table:&lt;/strong&gt; &lt;a href=&#34;https://docs.google.com/spreadsheets/d/1ZMFrD6F6tvPtf_8McC-kWrNBBec_6Si3NW6AoWf3Kbg/edit?usp=sharing&#34;&gt;Link&lt;/a&gt; &lt;br&gt; &lt;strong&gt;References to Documentation for each EDR product:&lt;/strong&gt; &lt;a href=&#34;https://github.com/tsale/EDR-Telemetry/wiki#product-documentation-references&#34;&gt;Link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Telemetry Feature Category&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Sub-Category&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Sysmon&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;CrowdStrike&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Elastic&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;LimaCharlie&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;MDE&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Sentinel One&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;WatchGuard&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Process Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Process Creation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Process Termination&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Process Access&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image/Library Loaded&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Remote Thread Creation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Process Tampering Activity&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;File Manipulation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;File Creation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;File Opened&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;File Deletion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;File Modification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;File Renaming&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;User Account Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Account Creation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Account Modification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Account Deletion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Account Login&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Account Logoff&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Network Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TCP Connection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;UDP Connection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;URL&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DNS Query&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;File Downloaded&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Hash Algorithms&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;MD5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SHA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;IMPHASH&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Registry Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Key/Value Creation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Key/Value Modification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Key/Value Deletion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Schedule Task Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scheduled Task Creation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scheduled Task Modification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Scheduled Task Deletion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Service Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Service Creation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Service Modification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Service Deletion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟧&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;❓&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Driver/Module Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Driver Loaded&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Driver Modification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Driver Unloaded&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Device Operations&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Virtual Disk Mount&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;USB Device Unmount&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;USB Device Mount&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Other Relevant Events&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Group Policy Modification&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Named Pipe Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Pipe Creation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Pipe Connection&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;EDR SysOps&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Agent Start&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Agent Stop&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Agent Install&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Agent Uninstall&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Agent Tampering&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Agent Keep-Alive&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Agent Errors&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;WMI Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WmiEventConsumerToFilter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WmiEventConsumer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;****&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WmiEventFilter&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;BIT JOBS Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;BIT JOBS Activity&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;PowerShell Activity&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Script-Block Activity&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟩&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;🟥&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Current Primary Maintainers&lt;/h2&gt; &#xA;&lt;p&gt;Kostas - &lt;a href=&#34;https://twitter.com/Kostastsale&#34;&gt;@kostastsale&lt;/a&gt;&lt;br&gt; Alex - &lt;a href=&#34;https://twitter.com/ateixei&#34;&gt;@ateixei&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>a16z/magi</title>
    <updated>2023-04-22T01:29:01Z</updated>
    <id>tag:github.com,2023-04-22:/a16z/magi</id>
    <link href="https://github.com/a16z/magi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A blazing fast OP Stack rollup client written in Rust&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Magi &amp;nbsp;&lt;span&gt;🟠&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/a16z/magi/actions/workflows/test.yml&#34;&gt;&lt;img src=&#34;https://github.com/a16z/magi/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/license/agpl-v3/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-AGPL_v3-blue.svg?sanitize=true&#34; alt=&#34;license: AGPL v3&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://t.me/+6zrIsnaLO0hjNmZh&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/chat-telegram-blue&#34; alt=&#34;chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Magi is an OP Stack rollup client written in Rust, designed to perform the same functionality as op-node. It is compatible with execution clients like op-geth. As an independent implementation, Magi aims to enhance the safety and liveness of the entire OP Stack ecosystem. Magi is still new, so we expect to find some bugs in the coming months. For critical infrastructure, we recommend using op-node.&lt;/p&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;For convenience, we provide a simple Docker setup to run Magi and op-geth together. This guide assumes you have both docker and git installed on your machine.&lt;/p&gt; &#xA;&lt;p&gt;Start by cloning the Magi repository and entering the docker subdirectory&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone git@github.com:a16z/magi.git &amp;amp;&amp;amp; cd magi/docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next copy &lt;code&gt;.env.default&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cp .env.default .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the &lt;code&gt;.env&lt;/code&gt; file, modify the &lt;code&gt;L1_RPC_URL&lt;/code&gt; field to contain a valid Ethereum RPC. For the Optimism and Base testnets, this must be a Goerli RPC URL. This RPC can either be from a local node, or a provider such as Alchemy or Infura.&lt;/p&gt; &#xA;&lt;p&gt;By default, the &lt;code&gt;NETWORK&lt;/code&gt; field in &lt;code&gt;.env&lt;/code&gt; is &lt;code&gt;optimism-goerli&lt;/code&gt;, however &lt;code&gt;base-goerli&lt;/code&gt; is also supported.&lt;/p&gt; &#xA;&lt;p&gt;Start the docker containers&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the previous step fails with a permission denied error, try running the command with &lt;code&gt;sudo&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The docker setup contains a Grafana dashboard. To view sync progress, you can check the dashboard at &lt;code&gt;http://localhost:3000&lt;/code&gt; with the username &lt;code&gt;magi&lt;/code&gt; and password &lt;code&gt;op&lt;/code&gt;. Alternatively, you can view Magi&#39;s logs by running &lt;code&gt;docker logs magi --follow&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;All contributions to Magi are welcome. Before opening a PR, please submit an issue detailing the bug or feature. Please ensure that your contribution builds on the stable Rust toolchain, has been linted with &lt;code&gt;cargo fmt&lt;/code&gt;, passes &lt;code&gt;cargo clippy&lt;/code&gt;, and contains tests when applicable.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;This code is being provided as is. No guarantee, representation or warranty is being made, express or implied, as to the safety or correctness of the code. It has not been audited and as such there can be no assurance it will work as intended, and users may experience delays, failures, errors, omissions or loss of transmitted information. Nothing in this repo should be construed as investment advice or legal advice for any particular facts or circumstances and is not meant to replace competent counsel. It is strongly advised for you to contact a reputable attorney in your jurisdiction for any questions or concerns with respect thereto. a16z is not liable for any use of the foregoing, and users should proceed with caution and use at their own risk. See a16z.com/disclosures for more info.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
</feed>