<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-16T01:29:15Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>comfyanonymous/ComfyUI</title>
    <updated>2023-03-16T01:29:15Z</updated>
    <id>tag:github.com,2023-03-16:/comfyanonymous/ComfyUI</id>
    <link href="https://github.com/comfyanonymous/ComfyUI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A powerful and modular stable diffusion GUI with a graph/nodes interface.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI&lt;/h1&gt; &#xA;&lt;h2&gt;A powerful and modular stable diffusion GUI.&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/comfyui_screenshot.png&#34; alt=&#34;ComfyUI Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This ui will let you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. For some workflow examples and see what ComfyUI can do you can check out:&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/&#34;&gt;ComfyUI Examples&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.&lt;/li&gt; &#xA; &lt;li&gt;Fully supports SD1.x and SD2.x&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous Queue system&lt;/li&gt; &#xA; &lt;li&gt;Many optimizations: Only re-executes the parts of the workflow that changes between executions.&lt;/li&gt; &#xA; &lt;li&gt;Command line option: &lt;code&gt;--lowvram&lt;/code&gt; to make it work on GPUs with less than 3GB vram (enabled automatically on GPUs with low vram)&lt;/li&gt; &#xA; &lt;li&gt;Works even if you don&#39;t have a GPU with: &lt;code&gt;--cpu&lt;/code&gt; (slow)&lt;/li&gt; &#xA; &lt;li&gt;Can load both ckpt and safetensors models/checkpoints. Standalone VAEs and CLIP models.&lt;/li&gt; &#xA; &lt;li&gt;Embeddings/Textual inversion&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/lora/&#34;&gt;Loras (regular and locon)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Loading full workflows (with seeds) from generated PNG files.&lt;/li&gt; &#xA; &lt;li&gt;Saving/Loading workflows as Json files.&lt;/li&gt; &#xA; &lt;li&gt;Nodes interface can be used to create complex workflows like one for &lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/&#34;&gt;Hires fix&lt;/a&gt; or much more advanced ones.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/area_composition/&#34;&gt;Area Composition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/inpaint/&#34;&gt;Inpainting&lt;/a&gt; with both regular and inpainting models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/controlnet/&#34;&gt;ControlNet and T2I-Adapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/&#34;&gt;Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Starts up very fast.&lt;/li&gt; &#xA; &lt;li&gt;Works fully offline: will never download anything.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Workflow examples can be found on the &lt;a href=&#34;https://comfyanonymous.github.io/ComfyUI_examples/&#34;&gt;Examples page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installing&lt;/h1&gt; &#xA;&lt;h2&gt;Windows&lt;/h2&gt; &#xA;&lt;p&gt;There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI/releases&#34;&gt;releases page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_or_cpu.7z&#34;&gt;Direct link to download&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Just download, extract and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints&lt;/p&gt; &#xA;&lt;h2&gt;Colab Notebook&lt;/h2&gt; &#xA;&lt;p&gt;To run it on colab or paperspace you can use my &lt;a href=&#34;https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/notebooks/comfyui_colab.ipynb&#34;&gt;Colab Notebook&lt;/a&gt; here: &lt;a href=&#34;https://colab.research.google.com/github/comfyanonymous/ComfyUI/blob/master/notebooks/comfyui_colab.ipynb&#34;&gt;Link to open with google colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Manual Install (Windows, Linux)&lt;/h2&gt; &#xA;&lt;p&gt;Git clone this repo.&lt;/p&gt; &#xA;&lt;p&gt;Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints&lt;/p&gt; &#xA;&lt;p&gt;Put your VAE in: models/vae&lt;/p&gt; &#xA;&lt;p&gt;At the time of writing this pytorch has issues with python versions higher than 3.10 so make sure your python/pip versions are 3.10.&lt;/p&gt; &#xA;&lt;h3&gt;AMD (Linux only)&lt;/h3&gt; &#xA;&lt;p&gt;AMD users can install rocm and pytorch with pip if you don&#39;t have it already installed, this is the command to install the stable version:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.4.2&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;NVIDIA&lt;/h3&gt; &#xA;&lt;p&gt;Nvidia users should install torch using this command:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Nvidia users should also install Xformers for a speed boost but can still run the software without it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install xformers&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Troubleshooting&lt;/h4&gt; &#xA;&lt;p&gt;If you get the &#34;Torch not compiled with CUDA enabled&#34; error, uninstall torch with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip uninstall torch&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And install it again with the command above.&lt;/p&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install the dependencies by opening your terminal inside the ComfyUI folder and:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;After this you should have everything installed and can proceed to running ComfyUI.&lt;/p&gt; &#xA;&lt;h3&gt;I already have another UI for Stable Diffusion installed do I really have to install all of these dependencies?&lt;/h3&gt; &#xA;&lt;p&gt;You don&#39;t. If you have another UI installed and working with it&#39;s own python venv you can use that venv to run ComfyUI. You can open up your favorite terminal and activate it:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;source path_to_other_sd_gui/venv/bin/activate&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or on Windows:&lt;/p&gt; &#xA;&lt;p&gt;With Powershell: &lt;code&gt;&#34;path_to_other_sd_gui\venv\Scripts\Activate.ps1&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;With cmd.exe: &lt;code&gt;&#34;path_to_other_sd_gui\venv\Scripts\activate.bat&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;And then you can use that terminal to run Comfyui without installing any dependencies. Note that the venv folder might be called something else depending on the SD UI.&lt;/p&gt; &#xA;&lt;h1&gt;Running&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;python main.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;For AMD 6700, 6600 and maybe others&lt;/h3&gt; &#xA;&lt;p&gt;Try running it with this command if you have issues:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Notes&lt;/h1&gt; &#xA;&lt;p&gt;Only parts of the graph that have an output with all the correct inputs will be executed.&lt;/p&gt; &#xA;&lt;p&gt;Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.&lt;/p&gt; &#xA;&lt;p&gt;Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.&lt;/p&gt; &#xA;&lt;p&gt;You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \( or \).&lt;/p&gt; &#xA;&lt;p&gt;You can use {day|night}, for wildcard/dynamic prompts. With this syntax &#34;{wild|card|test}&#34; will be randomly replaced by either &#34;wild&#34;, &#34;card&#34; or &#34;test&#34; by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \{ or \}.&lt;/p&gt; &#xA;&lt;p&gt;To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;embedding:embedding_filename.pt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fedora&lt;/h3&gt; &#xA;&lt;p&gt;To get python 3.10 on fedora: &lt;code&gt;dnf install python3.10&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then you can:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python3.10 -m ensurepip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will let you use: pip3.10 to install all the dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;How to increase generation speed?&lt;/h2&gt; &#xA;&lt;p&gt;The fp16 model configs in the CheckpointLoader can be used to load them in fp16 mode, depending on your GPU this will increase your gen speed by a significant amount.&lt;/p&gt; &#xA;&lt;p&gt;You can also set this command line setting to disable the upcasting to fp32 in some cross attention operations which will increase your speed. Note that this will very likely give you black images on SD2.x models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--dont-upcast-attention&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Support and dev channel&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.element.io/#/room/%23comfyui_space%3Amatrix.org&#34;&gt;Matrix space: #comfyui_space:matrix.org&lt;/a&gt; (it&#39;s like discord but open source).&lt;/p&gt; &#xA;&lt;h1&gt;QA&lt;/h1&gt; &#xA;&lt;h3&gt;Why did you make this?&lt;/h3&gt; &#xA;&lt;p&gt;I wanted to learn how Stable Diffusion worked in detail. I also wanted something clean and powerful that would let me experiment with SD without restrictions.&lt;/p&gt; &#xA;&lt;h3&gt;Who is this for?&lt;/h3&gt; &#xA;&lt;p&gt;This is for anyone that wants to make complex workflows with SD or that wants to learn more how SD works. The interface follows closely how SD works and the code should be much more simple to understand than other SD UIs.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cogentapps/chat-with-gpt</title>
    <updated>2023-03-16T01:29:15Z</updated>
    <id>tag:github.com,2023-03-16:/cogentapps/chat-with-gpt</id>
    <link href="https://github.com/cogentapps/chat-with-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open-source ChatGPT app with a voice&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chat with GPT&lt;/h1&gt; &#xA;&lt;p&gt;Chat with GPT is an open-source, unofficial ChatGPT app with extra features and more ways to customize your experience. It connects ChatGPT with ElevenLabs to give ChatGPT a realistic human voice.&lt;/p&gt; &#xA;&lt;p&gt;Try out the hosted version at: &lt;a href=&#34;https://chatwithgpt.netlify.app&#34;&gt;https://chatwithgpt.netlify.app&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or &lt;a href=&#34;https://raw.githubusercontent.com/cogentapps/chat-with-gpt/main/#running-on-your-own-computer&#34;&gt;self-host with Docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Powered by the new ChatGPT API from OpenAI, this app has been developed using TypeScript + React. We welcome pull requests from the community!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/127109874/223613258-0c4fef2e-1d05-43a1-ac38-e972dafc2f98.mp4&#34;&gt;https://user-images.githubusercontent.com/127109874/223613258-0c4fef2e-1d05-43a1-ac38-e972dafc2f98.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöÄ &lt;strong&gt;Fast&lt;/strong&gt; response times.&lt;/li&gt; &#xA; &lt;li&gt;üîé &lt;strong&gt;Search&lt;/strong&gt; through your past chat conversations.&lt;/li&gt; &#xA; &lt;li&gt;üìÑ View and customize the System Prompt - the &lt;strong&gt;secret prompt&lt;/strong&gt; the system shows the AI before your messages.&lt;/li&gt; &#xA; &lt;li&gt;üå° Adjust the &lt;strong&gt;creativity and randomness&lt;/strong&gt; of responses by setting the Temperature setting. Higher temperature means more creativity.&lt;/li&gt; &#xA; &lt;li&gt;üí¨ Give ChatGPT AI a &lt;strong&gt;realistic human voice&lt;/strong&gt; by connecting your ElevenLabs text-to-speech account.&lt;/li&gt; &#xA; &lt;li&gt;‚úâ &lt;strong&gt;Share&lt;/strong&gt; your favorite chat sessions online using public share URLs.&lt;/li&gt; &#xA; &lt;li&gt;üìã Easily &lt;strong&gt;copy-and-paste&lt;/strong&gt; ChatGPT messages.&lt;/li&gt; &#xA; &lt;li&gt;‚úèÔ∏è Edit your messages&lt;/li&gt; &#xA; &lt;li&gt;üîÅ Regenerate ChatGPT messages&lt;/li&gt; &#xA; &lt;li&gt;üñº &lt;strong&gt;Full markdown support&lt;/strong&gt; including code, tables, and math.&lt;/li&gt; &#xA; &lt;li&gt;ü´∞ Pay for only what you use with the ChatGPT API.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Bring your own API keys&lt;/h2&gt; &#xA;&lt;h3&gt;OpenAI&lt;/h3&gt; &#xA;&lt;p&gt;To get started with Chat with GPT, you will need to add your OpenAI API key on the settings screen. Click &#34;Connect your OpenAI account to get started&#34; on the home page to begin. Once you have added your API key, you can start chatting with ChatGPT.&lt;/p&gt; &#xA;&lt;p&gt;Your API key is stored only on your device and is never transmitted to anyone except OpenAI. Please note that OpenAI API key usage is billed at a pay-as-you-go rate, separate from your ChatGPT subscription.&lt;/p&gt; &#xA;&lt;h3&gt;ElevenLabs&lt;/h3&gt; &#xA;&lt;p&gt;To use the realistic AI text-to-speech feature, you will need to add your ElevenLabs API key by clicking &#34;Play&#34; next to any message.&lt;/p&gt; &#xA;&lt;p&gt;Your API key is stored only on your device and never transmitted to anyone except ElevenLabs.&lt;/p&gt; &#xA;&lt;h2&gt;Running on your own computer&lt;/h2&gt; &#xA;&lt;p&gt;To run on your own device, you can use Docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -v $(pwd)/data:/app/data -p 3000:3000 ghcr.io/cogentapps/chat-with-gpt:release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then navigate to &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; to view the app.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Chat with GPT is licensed under the MIT license. See the LICENSE file for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cocktailpeanut/dalai</title>
    <updated>2023-03-16T01:29:15Z</updated>
    <id>tag:github.com,2023-03-16:/cocktailpeanut/dalai</id>
    <link href="https://github.com/cocktailpeanut/dalai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The simplest way to run LLaMA on your local machine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Dalai&lt;/h1&gt; &#xA;&lt;p&gt;Run LLaMA on your computer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; Github&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/XahBUrbVwz&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;JUST RUN THIS:&lt;/h4&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/terminal.png&#34; class=&#34;round&#34;&gt; &#xA;&lt;h4&gt;TO GET:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/dalai.gif&#34; alt=&#34;dalai.gif&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Powered by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/shawwn/llama-dl&#34;&gt;llama-dl CDN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hackable web app included&lt;/li&gt; &#xA; &lt;li&gt;Ships with JavaScript API&lt;/li&gt; &#xA; &lt;li&gt;Ships with &lt;a href=&#34;https://socket.io/&#34;&gt;Socket.io&lt;/a&gt; API&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Intro&lt;/h1&gt; &#xA;&lt;h2&gt;1. Cross platform&lt;/h2&gt; &#xA;&lt;p&gt;Dalai runs on all of the following operating systems:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Linux&lt;/li&gt; &#xA; &lt;li&gt;Mac&lt;/li&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;2. System Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Runs on most modern computers. Unless your computer is very very old, it should work.&lt;/p&gt; &#xA;&lt;h2&gt;3. Disk Space Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You need a lot of space for storing the models.&lt;/p&gt; &#xA;&lt;p&gt;You do NOT have to install all models, you can install one by one. Let&#39;s take a look at how much space each model takes up:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTE&lt;/p&gt; &#xA; &lt;p&gt;The following numbers assume that you DO NOT touch the original model files and keep BOTH the original model files AND the quantized versions.&lt;/p&gt; &#xA; &lt;p&gt;You can optimize this if you delete the original models (which are much larger) after installation and keep only the quantized versions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;7B&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 31.17GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.21GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/7b.png&#34; alt=&#34;7b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;13B&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 60.21GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 4.07GB * 2 = 8.14GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/13b.png&#34; alt=&#34;13b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;30B&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 150.48GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.09GB * 4 = 20.36GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/30b.png&#34; alt=&#34;30b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;65B&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Full: The model takes up 432.64GB&lt;/li&gt; &#xA; &lt;li&gt;Quantized: 5.11GB * 8 = 40.88GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/65b.png&#34; alt=&#34;65b.png&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;h2&gt;Mac&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install node.js&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Install Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 2. Install Dalai&lt;/h3&gt; &#xA;&lt;p&gt;Basic install (7B model only)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, install all models&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama 7B 13B 30B 65B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The install command :&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Creates a folder named &lt;code&gt;dalai&lt;/code&gt; under your home directory (&lt;code&gt;~&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Installs and builds the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; project under &lt;code&gt;~/llama.cpp&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Downloads all the requested models from the &lt;a href=&#34;https://github.com/shawwn/llama-dl&#34;&gt;llama-dl CDN&lt;/a&gt; to &lt;code&gt;~/llama.cpp/models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Runs some tasks to convert the LLaMA models so they can be used&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Windows&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install Visual Studio&lt;/h3&gt; &#xA;&lt;p&gt;On windows, you need to install Visual Studio before installing Dalai.&lt;/p&gt; &#xA;&lt;p&gt;Press the button below to visit the Visual Studio downloads page and download:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34; class=&#34;btn&#34;&gt;Download Microsoft Visual Studio&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT!!!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;When installing Visual Studio, make sure to check the 3 options as highlighted below:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python development&lt;/li&gt; &#xA; &lt;li&gt;Node.js development&lt;/li&gt; &#xA; &lt;li&gt;Desktop development with C++&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cocktailpeanut/dalai/main/vs.png&#34; alt=&#34;vs.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.1. Install Dalai&lt;/h3&gt; &#xA;&lt;p&gt;Basic install (7B model only)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, install all models&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama 7B 13B 30B 65B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The install command :&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Creates a folder named &lt;code&gt;dalai&lt;/code&gt; under your home directory (&lt;code&gt;~&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Installs and builds the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; project under &lt;code&gt;~/llama.cpp&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Downloads all the requested models from the &lt;a href=&#34;https://github.com/shawwn/llama-dl&#34;&gt;llama-dl CDN&lt;/a&gt; to &lt;code&gt;~/llama.cpp/models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Runs some tasks to convert the LLaMA models so they can be used&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If this worked without any errors, go to step 3.&lt;/p&gt; &#xA;&lt;p&gt;Ohterwise try the troubleshoot below:&lt;/p&gt; &#xA;&lt;h3&gt;Step 2.2. Troubleshoot (optional)&lt;/h3&gt; &#xA;&lt;p&gt;In case above steps fail to install, try installing node.js and python separately.&lt;/p&gt; &#xA;&lt;p&gt;Install Python:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/ftp/python/3.10.10/python-3.10.10-embed-amd64.zip&#34; class=&#34;btn&#34;&gt;Download Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install Node.js:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en/download/&#34; class=&#34;btn&#34;&gt;Download Node.js&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After both have been installed, open powershell and type &lt;code&gt;python&lt;/code&gt; to see if the application exists. And also type &lt;code&gt;node&lt;/code&gt; to see if the application exists as well.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve checked that they both exist, try the &lt;code&gt;npx dalai llama&lt;/code&gt; command again.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Linux&lt;/h2&gt; &#xA;&lt;h3&gt;Step 1. Install&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;p&gt;Basic install (7B model only)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, install all models&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai llama 7B 13B 30B 65B&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The install command :&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Creates a folder named &lt;code&gt;dalai&lt;/code&gt; under your home directory (&lt;code&gt;~&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Installs and builds the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; project under &lt;code&gt;~/llama.cpp&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Downloads all the requested models from the &lt;a href=&#34;https://github.com/shawwn/llama-dl&#34;&gt;llama-dl CDN&lt;/a&gt; to &lt;code&gt;~/llama.cpp/models&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Runs some tasks to convert the LLaMA models so they can be used&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Step 2. Run Web UI&lt;/h3&gt; &#xA;&lt;p&gt;After everything has been installed, open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; in your browser. Have fun!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;API&lt;/h1&gt; &#xA;&lt;p&gt;Dalai is also an NPM package:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;programmatically install&lt;/li&gt; &#xA; &lt;li&gt;locally make requests to the model&lt;/li&gt; &#xA; &lt;li&gt;run a dalai server (powered by socket.io)&lt;/li&gt; &#xA; &lt;li&gt;programmatically make requests to a remote dalai server (via socket.io)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Dalai is an NPM package. You can install it using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install dalai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;1. constructor()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(home)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;home&lt;/code&gt;: (optional) manually specify the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;By default, Dalai automatically stores the entire &lt;code&gt;llama.cpp&lt;/code&gt; repository under &lt;code&gt;~/llama.cpp&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;However, often you may already have a &lt;code&gt;llama.cpp&lt;/code&gt; repository somewhere else on your machine and want to just use that folder. In this case you can pass in the &lt;code&gt;home&lt;/code&gt; attribute.&lt;/p&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;Basic&lt;/h4&gt; &#xA;&lt;p&gt;Creates a workspace at &lt;code&gt;~/llama.cpp&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Custom path&lt;/h4&gt; &#xA;&lt;p&gt;Manually set the &lt;code&gt;llama.cpp&lt;/code&gt; path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const dalai = new Dalai(&#34;/Documents/llama.cpp&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;2. request()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.request(req, callback)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;req&lt;/code&gt;: a request object. made up of the following attributes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;prompt&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The prompt string&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;model&lt;/code&gt;: &lt;strong&gt;(required)&lt;/strong&gt; The model name to query (&#34;7B&#34;, &#34;13B&#34;, etc.)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;url&lt;/code&gt;: only needed if connecting to a remote dalai server &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;if unspecified, it uses the node.js API to directly run dalai locally&lt;/li&gt; &#xA;     &lt;li&gt;if specified (for example &lt;code&gt;ws://localhost:3000&lt;/code&gt;) it looks for a socket.io endpoint at the URL and connects to it.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;threads&lt;/code&gt;: The number of threads to use (The default is 8 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;n_predict&lt;/code&gt;: The number of tokens to return (The default is 128 if unspecified)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;seed&lt;/code&gt;: The seed. The default is -1 (none)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_k&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;top_p&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_last_n&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;repeat_penalty&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;temp&lt;/code&gt;: temperature&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;batch_size&lt;/code&gt;: batch size&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;skip_end&lt;/code&gt;: by default, every session ends with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;, which can be used as a marker to know when the full response has returned. However sometimes you may not want this suffix. Set &lt;code&gt;skip_end: true&lt;/code&gt; and the response will no longer end with &lt;code&gt;\n\n&amp;lt;end&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;callback&lt;/code&gt;: the streaming callback function that gets called every time the client gets any token response back from the model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;h4&gt;1. Node.js&lt;/h4&gt; &#xA;&lt;p&gt;Using node.js, you just need to initialize a Dalai object with &lt;code&gt;new Dalai()&lt;/code&gt; and then use it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().request({&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  process.stdout.write(token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Non node.js (socket.io)&lt;/h4&gt; &#xA;&lt;p&gt;To make use of this in a browser or any other language, you can use thie socket.io API.&lt;/p&gt; &#xA;&lt;h5&gt;Step 1. start a server&lt;/h5&gt; &#xA;&lt;p&gt;First you need to run a Dalai socket server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;// server.js&#xA;const Dalai = require(&#39;dalai&#39;)&#xA;new Dalai().serve(3000)     // port 3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h5&gt;Step 2. connect to the server&lt;/h5&gt; &#xA;&lt;p&gt;Then once the server is running, simply make requests to it by passing the &lt;code&gt;ws://localhost:3000&lt;/code&gt; socket url when initializing the Dalai object:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().request({&#xA;  url: &#34;ws://localhost:3000&#34;,&#xA;  model: &#34;7B&#34;,&#xA;  prompt: &#34;The following is a conversation between a boy and a girl:&#34;,&#xA;}, (token) =&amp;gt; {&#xA;  console.log(&#34;token&#34;, token)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;3. serve()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;Starts a socket.io server at &lt;code&gt;port&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.serve(port)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;)&#xA;new Dalai().serve(3000)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;4. http()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;p&gt;connect with an existing &lt;code&gt;http&lt;/code&gt; instance (The &lt;code&gt;http&lt;/code&gt; npm package)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;dalai.http(http)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;http&lt;/code&gt;: The &lt;a href=&#34;https://nodejs.org/api/http.html&#34;&gt;http&lt;/a&gt; object&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;This is useful when you&#39;re trying to plug dalai into an existing node.js web app&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const app = require(&#39;express&#39;)();&#xA;const http = require(&#39;http&#39;).Server(app);&#xA;dalai.http(http)&#xA;http.listen(3000, () =&amp;gt; {&#xA;  console.log(&#34;server started&#34;)&#xA;})&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;5. install()&lt;/h2&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;await dalai.install(model1, model2, ...)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;models&lt;/code&gt;: the model names to install (&#34;7B&#34;`, &#34;13B&#34;, &#34;30B&#34;, &#34;65B&#34;, etc)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;p&gt;Install the &#34;7B&#34; and &#34;13B&#34; models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;await dalai.install(&#34;7B&#34;, &#34;13B&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;6. installed()&lt;/h2&gt; &#xA;&lt;p&gt;returns the array of installed models&lt;/p&gt; &#xA;&lt;h3&gt;Syntax&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const models = await dalai.installed()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const Dalai = require(&#34;dalai&#34;);&#xA;const dalai = new Dalai()&#xA;const models = await dalai.installed()&#xA;console.log(models)     // prints [&#34;7B&#34;, &#34;13B&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h2&gt;Updating to the latest&lt;/h2&gt; &#xA;&lt;p&gt;Dalai is a young project and will evolve quickly.&lt;/p&gt; &#xA;&lt;p&gt;To update dalai, you will need to run the dalai command with a version number specified (You only need to do this once when you update).&lt;/p&gt; &#xA;&lt;p&gt;For example, let&#39;s say you&#39;ve been using &lt;code&gt;dalai@0.1.0&lt;/code&gt; but a new version &lt;code&gt;dalai@0.2.0&lt;/code&gt; came out.&lt;/p&gt; &#xA;&lt;p&gt;The simplest way to update is to just run the dalai server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx dalai@0.2.0 serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once you run the command it will ask you if you want to update. Confirm, and it will now install &lt;code&gt;0.2.0&lt;/code&gt;, and from that point on you don&#39;t need to specify the version. You can just run &lt;code&gt;npx dalai serve&lt;/code&gt; and the new version will be executed from that point on.&lt;/p&gt; &#xA;&lt;h2&gt;Staying up to date&lt;/h2&gt; &#xA;&lt;p&gt;Have questions or feedback? Follow the project through the following outlets:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/cocktailpeanut/dalai&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-github&#34;&gt;&lt;/i&gt; Github&lt;/a&gt; &lt;a href=&#34;https://twitter.com/cocktailpeanut&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-twitter&#34;&gt;&lt;/i&gt; Twitter&lt;/a&gt; &lt;a href=&#34;https://discord.gg/XahBUrbVwz&#34; class=&#34;inverse btn&#34;&gt;&lt;i class=&#34;fa-brands fa-discord&#34;&gt;&lt;/i&gt; Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt;</summary>
  </entry>
</feed>