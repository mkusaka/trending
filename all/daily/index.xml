<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-16T01:22:39Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>vosen/ZLUDA</title>
    <updated>2024-02-16T01:22:39Z</updated>
    <id>tag:github.com,2024-02-16:/vosen/ZLUDA</id>
    <link href="https://github.com/vosen/ZLUDA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CUDA on AMD GPUs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ZLUDA&lt;/h1&gt; &#xA;&lt;p&gt;ZLUDA lets you run unmodified CUDA applications with near-native performance on &lt;del&gt;Intel&lt;/del&gt; AMD GPUs.&lt;/p&gt; &#xA;&lt;p&gt;ZLUDA is currently alpha quality, but it has been confirmed to work with a variety of native CUDA applications: Geekbench, 3DF Zephyr, Blender, Reality Capture, LAMMPS, NAMD, waifu2x, OpenFOAM, Arnold (proof of concept) and more.&lt;/p&gt; &#xA;&lt;p&gt;If you want to give it a try, download it from Release page to the right and read &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/#usage&#34;&gt;Usage&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/#known-issues&#34;&gt;Known Issues&lt;/a&gt; sections below. If you are interested in its history and future read &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/#faq&#34;&gt;FAQ&lt;/a&gt; section further below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/geekbench.svg?sanitize=true&#34; alt=&#34;geekbench.svg&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;Using command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;ZLUDA_DIRECTORY&amp;gt;\zluda.exe -- &amp;lt;APPLICATION&amp;gt; &amp;lt;APPLICATION_ARGUMENTS&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you downloaded a ZIP file with the release and unpacked it, then &lt;code&gt;&amp;lt;ZLUDA_DIRECTORY&amp;gt;&lt;/code&gt; is the &lt;code&gt;zluda&lt;/code&gt; directory you have just unpacked.&lt;br&gt; If you are building from source, then &lt;code&gt;&amp;lt;ZLUDA_DIRECTORY&amp;gt;&lt;/code&gt; is subdirectory &lt;code&gt;target\release&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;p&gt;Using command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;LD_LIBRARY_PATH=&#34;&amp;lt;ZLUDA_DIRECTORY&amp;gt;:$LD_LIBRARY_PATH&#34; &amp;lt;APPLICATION&amp;gt; &amp;lt;APPLICATION_ARGUMENTS&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you downloaded a ZIP file with the release and unpacked it, then &lt;code&gt;&amp;lt;ZLUDA_DIRECTORY&amp;gt;&lt;/code&gt; is the &lt;code&gt;zluda&lt;/code&gt; directory you have just unpacked.&lt;br&gt; If you are building from source, then &lt;code&gt;&amp;lt;ZLUDA_DIRECTORY&amp;gt;&lt;/code&gt; is subdirectory &lt;code&gt;target\release&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you have the following installed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Git&lt;/li&gt; &#xA; &lt;li&gt;CMake&lt;/li&gt; &#xA; &lt;li&gt;Python 3&lt;/li&gt; &#xA; &lt;li&gt;Rust (1.66.1 or newer)&lt;/li&gt; &#xA; &lt;li&gt;C++ compiler&lt;/li&gt; &#xA; &lt;li&gt;(Linux only) ROCm 5.7+ (&lt;em&gt;not ROCm 6&lt;/em&gt;) (&lt;a href=&#34;https://rocm.docs.amd.com/en/latest/deploy/linux/install_overview.html&#34;&gt;https://rocm.docs.amd.com/en/latest/deploy/linux/install_overview.html&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;(Windows only) Recent AMD Radeon Software Adrenalin (&lt;a href=&#34;https://rocm.docs.amd.com/en/latest/deploy/linux/install_overview.html&#34;&gt;https://rocm.docs.amd.com/en/latest/deploy/linux/install_overview.html&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;(Recommended, optional) Ninja (&lt;a href=&#34;https://ninja-build.org/&#34;&gt;https://ninja-build.org/&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Alternatively, if you are building for Linux, &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/.devcontainer&#34;&gt;.devcontainer&lt;/a&gt; directory contains various developer Dockerfiles with all the required dependencies&lt;/p&gt; &#xA;&lt;h3&gt;Checkout&lt;/h3&gt; &#xA;&lt;p&gt;Checkout ZLUDA code with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recurse-submodules https://github.com/vosen/zluda.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;Build by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo xtask --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Unknown issues&lt;/h2&gt; &#xA;&lt;p&gt;If an application fails to start under ZLUDA or crashes please check &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/#known-issues&#34;&gt;Known Issues&lt;/a&gt; section below. If nothing there applies, then please read &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/TROUBLESHOOTING.md&#34;&gt;TROUBLESHOOTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;h3&gt;Hardware&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If both integrated AMD GPU and dedicated AMD GPU are present in the system, ZLUDA uses the integrated GPU.&lt;/p&gt; &lt;p&gt;This is a bug in underying ROCm/HIP runtime. You can work around it by disabling the integrated GPU.&lt;/p&gt; &lt;p&gt;On Windows we recommend you use environment variable &lt;code&gt;HIP_VISIBLE_DEVICES=1&lt;/code&gt; environment variable (more &lt;a href=&#34;https://rocmdocs.amd.com/en/latest/conceptual/gpu-isolation.html#hip-visible-devices&#34;&gt;here&lt;/a&gt;) or disable it system-wide in Device Manager.&lt;/p&gt; &lt;p&gt;On Linux we recommend you use environment variable &lt;code&gt;ROCR_VISIBLE_DEVICES=&amp;lt;UUID&amp;gt;&lt;/code&gt; where &lt;code&gt;&amp;lt;UUID&amp;gt;&lt;/code&gt; is the UUID of the dedicated GPU as reported by &lt;code&gt;rocminfo&lt;/code&gt; command line tool (you can also use &lt;code&gt;HIP_VISIBLE_DEVICES=1&lt;/code&gt;, but this does not seem to be stable). Alternatively you can disable integrated GPU system-wide by passing &lt;code&gt;pci-stub.ids=&amp;lt;DEVICE_VENDOR&amp;gt;:&amp;lt;DEVICE_CODE&amp;gt;&lt;/code&gt; to the kernel options. On Ubuntu you can pass additional kernel options by adding them to &lt;code&gt;/etc/default/grub&lt;/code&gt; to the option &lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT&lt;/code&gt;. You can find &lt;code&gt;&amp;lt;DEVICE_VENDOR&amp;gt;:&amp;lt;DEVICE_CODE&amp;gt;&lt;/code&gt; with the help of &lt;code&gt;lspci -nn&lt;/code&gt;. This will emit a series of lines with one of them matching you integrated GPU, for example:&lt;br&gt; &lt;code&gt;1b:00.0 VGA compatible controller [0300]: Advanced Micro Devices, Inc. [AMD/ATI] Device [1002:164e] (rev c1)&lt;/code&gt;&lt;br&gt; &lt;code&gt;&amp;lt;DEVICE_VENDOR&amp;gt;:&amp;lt;DEVICE_CODE&amp;gt;&lt;/code&gt; ar at the end, in this case &lt;code&gt;1002:164e&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Integrated GPUs (as tested with Radeon 680M) work in a limited way. Some rarely used GPU operations (abort, printf, etc.) will hang or crash the application. Additionally, performance library support (cuBLAS, cuDNN, etc.) might be limited, rendering more complex applications inoperable.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ZLUDA can use AMD server GPUs (as tested with Instinct MI200) with a caveat.&lt;/p&gt; &lt;p&gt;On Server GPUs, ZLUDA can compile CUDA GPU code to run in one of two modes:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fast mode, which is faster, but can make exotic (but correct) GPU code hang.&lt;/li&gt; &#xA;   &lt;li&gt;Slow mode, which should make GPU code more stable, but can prevent some applications from running on ZLUDA.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;By default, ZLUDA uses fast mode. That&#39;s because:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;There&#39;s a huge performance difference, fast mode can be twice as fast.&lt;/li&gt; &#xA;   &lt;li&gt;The code patterns that can trip fast mode were not encountered across multiple projects (SPECFEM3D, QUDA, CHroma, MILC, Kokkos, LAMMPS, OpenFOAM, XGBoost, NAMD, LAMMPS).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;You can use environment variable &lt;code&gt;ZLUDA_WAVE64_SLOW_MODE=1&lt;/code&gt; to force compilation in slow mode.&lt;/p&gt; &lt;p&gt;Nothing of that applies to desktop and integrated GPUs (RDNA family).&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Software&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Applications using ZLUDA are slow to start.&lt;/p&gt; &lt;p&gt;On the first start ZLUDA needs to compile GPU code for the application. This is a one-time cost, compiled GPU code is cached in &lt;code&gt;%LOCALAPPDATA%&lt;/code&gt; on Windows and in &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt; or &lt;code&gt;$HOME/.cache&lt;/code&gt; on Linux.&lt;br&gt; Some applications will gradually load the GPU code as it is used. If that is undesirable you can try setting environment variable &lt;code&gt;CUDA_MODULE_LOADING=EAGER&lt;/code&gt;. It depends on how the application was programmed, but it might force to load (and compile) all the kernels on startup, no matter if they are used or not.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Applications running ZLUDA might produce slightly different values&lt;/p&gt; &lt;p&gt;Firstly, ZLUDA ignores some of the floating point denormal and rounding mode information present in the kernels. Secondly, for certain approximate (not IEEE 754) NVIDIA floating point operations in CUDA, ZLUDA blindly uses approximate AMD floating point operations. The two might have a different precision.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;CUDA 12+&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Application built with CUDA 12 and using Thrust crashes with &lt;code&gt;LLVM ERROR: unsupported libcall legalization&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This is a ROCm/HIP bug. Currently, CUDA applications built with CUDA versions pre-12 work the best. Building with CUDA 12 and a pre-CUDA 12 Thrust might also work.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;OptiX&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ZLUDA has a bare-minimum OptiX implementation for Arnold. See details in &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/#arnold&#34;&gt;Arnold&lt;/a&gt; section.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Windows&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Antivirus flags ZLUDA as malware.&lt;/p&gt; &lt;p&gt;ZLUDA launcher (&lt;code&gt;zluda.exe&lt;/code&gt;) uses some of the techniques used by malware, but for good. &lt;code&gt;zluda.exe&lt;/code&gt; hijacks the process and redirects all uses of the original NVIDIA&#39;s CUDA libraries to use ZLUDA&#39;s CUDA instead.&lt;/p&gt; &lt;p&gt;&lt;u&gt;Don&#39;t use &lt;code&gt;zluda.exe&lt;/code&gt; with games that use anti-cheat.&lt;/u&gt; ZLUDA does not support CUDA gaming workloads (PhysX or DLSS) and anti-cheat might mistake &lt;code&gt;zluda.exe&lt;/code&gt; for a malware or a cheat.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Following error when launching an application with &lt;code&gt;zluda.exe&lt;/code&gt;: &lt;code&gt;Error: OsError { function: &#34;DetourCreateProcessWithDllsW&#34;, error_code: 740, message: &#34;The requested operation requires elevation.&#34; }&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You are launching an application that requires Administrator rights through &lt;code&gt;zluda.exe&lt;/code&gt;. Try launching &lt;code&gt;zluda.exe&lt;/code&gt; from an Administrator command line.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ZLUDA offers limited support for performance libraries (cuDNN, cuBLAS, cuSPARSE, cuFFT, OptiX, NCCL). Currently, this support is Linux-only and not available on Windows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ZLUDA launcher (&lt;code&gt;zluda.exe&lt;/code&gt;) does not support 32 bit processes. If an application launches 32 bit subprocess &lt;code&gt;a.exe&lt;/code&gt; neither the 32 bit process &lt;code&gt;a.exe&lt;/code&gt;, nor its 64 bit subprocess &lt;code&gt;a64.exe&lt;/code&gt; will be able to use ZLUDA. This affects e.g. SiSoft Sandra.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Applications&lt;/h3&gt; &#xA;&lt;h4&gt;llama.cpp&lt;/h4&gt; &#xA;&lt;p&gt;If you are building llama.cpp with cmake and don&#39;t want it to crash on ZLUDA then you should use &lt;code&gt;CUDA_DOCKER_ARCH=compute_61&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make CUDA_DOCKER_ARCH=compute_61 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, building with cmake should work with no changes.&lt;/p&gt; &#xA;&lt;p&gt;Performance is currently much lower than the native HIP backend, see the discussion in #102.&lt;/p&gt; &#xA;&lt;h4&gt;Arnold&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ZLUDA implements minimum of OptiX framework to support Arnold. ZLUDA&#39;s OptiX is buggy, unoptimized and incomplete. It&#39;s been tested with Arnold 7.1.4.1 command line rendering on Linux.&lt;/p&gt; &lt;p&gt;ZLUDA-OptiX is not built by default or redistributed in the release. To use it follow those steps:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Firstly build a newer version of ROCm LLVM. Version shipped with 5.7.1 is known to miscompile Arnold code. Get it here: &lt;a href=&#34;https://github.com/ROCm/llvm-project&#34;&gt;https://github.com/ROCm/llvm-project&lt;/a&gt;. Switch to a known good commit: &lt;code&gt;0c7fd5b6d1bbf471d2c068c2b6172d9cfd76b08d&lt;/code&gt; and build it.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Then build amd_comgr: &lt;a href=&#34;https://github.com/ROCm/ROCm-CompilerSupport&#34;&gt;https://github.com/ROCm/ROCm-CompilerSupport&lt;/a&gt; with the LLVM built in the previous step. I&#39;m using the last commit from &lt;a href=&#34;https://github.com/ROCm/ROCm-CompilerSupport&#34;&gt;https://github.com/ROCm/ROCm-CompilerSupport&lt;/a&gt; (&lt;code&gt;8276083301409001ec7643e68f5ad58b057c21fd&lt;/code&gt;).&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Now build ZLUDA-OptiX:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cargo ctask --release&#xA;cargo build -p zluda_rt --release&#xA;cd target/release&#xA;ln -s libnvoptix.so liboptix.so.6.6.0 &#xA;cp ../../hiprt-sys/lib/libhiprt64.so .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;After those quick and easy steps you can use it with the command line Arnold renderer:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;LD_LIBRARY_PATH=&amp;lt;PATH_TO_ZLUDA&amp;gt;/target/release/ LD_PRELOAD=&#34;&amp;lt;PATH_TO_COMGR&amp;gt;/build/libamd_comgr.so.2 &amp;lt;PATH_TO_ZLUDA&amp;gt;/liboptix.so.6.6.0&#34; /usr/autodesk/arnold/maya2023/bin/kick attic.ass  -device gpu -o /tmp/attic.jpg -v 6 -sl&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Keep in mind that ZLUDA-OptiX can only successfully render the simplest Arnold scene (and possibly one more):&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Cornell box (from &lt;a href=&#34;https://help.autodesk.com/view/ARNOL/ENU/?guid=arnold_user_guide_ac_scene_source_ac_ass_examples_html&#34;&gt;here&lt;/a&gt;):&lt;br&gt; &lt;a href=&#34;https://imgur.com/4Vv3GO8&#34;&gt;&lt;img src=&#34;https://imgur.com/4Vv3GO8s.jpg&#34; alt=&#34;cornell&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;(used to work, broken now) Attic scene (from &lt;a href=&#34;https://github.com/wahn/export_multi/tree/master/17_attic&#34;&gt;here&lt;/a&gt;):&lt;br&gt; &lt;a href=&#34;https://imgur.com/a/2jF9Kb5&#34;&gt;&lt;img src=&#34;https://imgur.com/Sut2YMys.jpg&#34; alt=&#34;cornell&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;PyTorch&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;PyTorch received very little testing. ZLUDA&#39;s coverage of cuDNN APIs is very minimal (just enough to run ResNet-50) and realistically you won&#39;t get much running.&lt;br&gt; However if you are interested in trying it out you need to build it from sources with the settings below. Default PyTorch does not ship PTX and uses bundled NCCL which also builds without PTX:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export TORCH_CUDA_ARCH_LIST=&#34;6.1+PTX&#34;&#xA;export CUDAARCHS=61&#xA;export CMAKE_CUDA_ARCHITECTURES=61&#xA;export USE_SYSTEM_NCCL=1&#xA;export NCCL_ROOT_DIR=/usr&#xA;export NCCL_INCLUDE_DIR=/usr/include&#xA;export NCCL_LIB_DIR=/usr/lib/x86_64-linux-gnu&#xA;export USE_EXPERIMENTAL_CUDNN_V8_API=OFF&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or (untested):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export TORCH_CUDA_ARCH_LIST=&#34;6.1+PTX&#34;&#xA;export CUDAARCHS=61&#xA;export CMAKE_CUDA_ARCHITECTURES=61&#xA;export USE_SYSTEM_NCCL=1&#xA;export USE_NCCL=0&#xA;export USE_EXPERIMENTAL_CUDNN_V8_API=OFF&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When running use the following environment variable:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DISABLE_ADDMM_CUDA_LT=1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;3DF Zephyr&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ZLUDA is much slower than CUDA.&lt;/p&gt; &lt;p&gt;3DF Zephyr is triggering an underlying ROCm/HIP performance issue.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Reality Capture&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;ZLUDA is much slower than CUDA.&lt;/p&gt; &lt;p&gt;Reality Capture is triggering an underlying ROCm/HIP performance issue.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;CompuBench&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;When running multiple tests, first test passes and the subsequent tests fail with &lt;code&gt;CUDA_ERROR_UNKNOWN&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This is a ROCm/HIP bug. Currently, CompuBench tests have to be run one at a time.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Some tests output black screen.&lt;/p&gt; &lt;p&gt;This is due to a bug (or an unintended hardware feature) in CompuBench that just happens to work on NVIDIA GPUs.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;V-Ray Benchmark&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Currently, ZLUDA crashes when running V-Ray benchmark. Nonetheless, certain &#34;lucky&#34; older combinations of ZLUDA and ROCm/HIP are known to run V-Ray Benchmark successfully.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Cinebench CUDA&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ZLUDA can&#39;t run it. Cinebench CUDA benchmark has not been fully compiled with PTX. It may be possible to run it (sans OptiX) if the OctaneBench developers re-compile it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;OctaneBench&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ZLUDA can&#39;t run it. OctaneBench has not been fully compiled with PTX. It may be possible to run it (sans OptiX) if the OctaneBench developers re-compile it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Why is this project suddenly back after 3 years? What happened to Intel GPU support?&lt;/p&gt; &lt;p&gt;In 2021 I was contacted by Intel about the development of ZLUDA. I was an Intel employee at the time. While we were building a case for ZLUDA internally, I was asked for a far-reaching discretion: not to advertise the fact that Intel was evaluating ZLUDA and definitely not to make any commits to the public ZLUDA repo. After some deliberation, Intel decided that there is no business case for running CUDA applications on Intel GPUs.&lt;/p&gt; &lt;p&gt;Shortly thereafter I got in contact with AMD and in early 2022 I have left Intel and signed a ZLUDA development contract with AMD. Once again I was asked for a far-reaching discretion: not to advertise the fact that AMD is evaluating ZLUDA and definitely not to make any commits to the public ZLUDA repo. After two years of development and some deliberation, AMD decided that there is no business case for running CUDA applications on AMD GPUs.&lt;/p&gt; &lt;p&gt;One of the terms of my contract with AMD was that if AMD did not find it fit for further development, I could release it. Which brings us to today.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;What&#39;s the future of the project?&lt;/p&gt; &lt;p&gt;With neither Intel nor AMD interested, we&#39;ve run out of GPU companies. I&#39;m open though to any offers of that could move the project forward.&lt;/p&gt; &lt;p&gt;Realistically, it&#39;s now abandoned and will only possibly receive updates to run workloads I am personally interested in (DLSS).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;What underlying GPU API does ZLUDA use? Is it OpenCL? ROCm? Vulkan?&lt;/p&gt; &lt;p&gt;ZLUDA is built purely on ROCm/HIP. On both Windows and Linux.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I am a developer writing CUDA code, does this project help me port my code to ROCm/HIP?&lt;/p&gt; &lt;p&gt;Currently no, this project is strictly for end users. However this project could be used for a much more gradual porting from CUDA to HIP than anything else. You could start with an unmodified application running on ZLUDA, then have ZLUDA expose the underlying HIP objects (streams, modules, etc.), allowing to rewrite GPU kernels one at a time. Or you could have a mixed CUDA-HIP application where only the most performance sensitive GPU kernels are written in the native AMD language.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;For developers&lt;/h2&gt; &#xA;&lt;p&gt;If you are curious about ZLUDA&#39;s architecture, you can read a broad overview in &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/ARCHITECTURE.md&#34;&gt;ARCHITECTURE.md&lt;/a&gt;. If you want to debug ZLUDA check the &#34;Debugging&#34; section in &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/TROUBLESHOOTING.md#debugging&#34;&gt;TROUBLESHOOTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This software is dual-licensed under either the Apache 2.0 license or the MIT license. See &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/LICENSE-APACHE&#34;&gt;LICENSE-APACHE&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/vosen/ZLUDA/master/LICENSE-MIT&#34;&gt;LICENSE-MIT&lt;/a&gt; for details&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>saurabhnemade/will-you-be-my-valentine</title>
    <updated>2024-02-16T01:22:39Z</updated>
    <id>tag:github.com,2024-02-16:/saurabhnemade/will-you-be-my-valentine</id>
    <link href="https://github.com/saurabhnemade/will-you-be-my-valentine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple application to impress your loved ones on valentines day!!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;will-you-be-my-valentine&lt;/h1&gt; &#xA;&lt;p&gt;Demo : &lt;a href=&#34;https://saurabhnemade.github.io/will-you-be-my-valentine/&#34;&gt;https://saurabhnemade.github.io/will-you-be-my-valentine/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a fun project for valentines day to bring smile on face of your special person!!&lt;/p&gt; &#xA;&lt;p&gt;This project is inspired from &lt;a href=&#34;https://gist.github.com/tnarla/0c09a11fea366145ba684fe6ebf578c5&#34;&gt;https://gist.github.com/tnarla/0c09a11fea366145ba684fe6ebf578c5&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://www.tiktok.com/@mewtru/video/7331131143112166698&#34;&gt;https://www.tiktok.com/@mewtru/video/7331131143112166698&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;How to start&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install -g pnpm&#xA;pnpm i&#xA;pnpm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Preview&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/saurabhnemade/will-you-be-my-valentine/main/demo.gif&#34; alt=&#34;image description&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;How to deploy it&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;pnpm run deploy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Made with love in Berlin!❤️&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Stability-AI/StableCascade</title>
    <updated>2024-02-16T01:22:39Z</updated>
    <id>tag:github.com,2024-02-16:/Stability-AI/StableCascade</id>
    <link href="https://github.com/Stability-AI/StableCascade" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Cascade&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/collage_1.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This is the official codebase for &lt;strong&gt;Stable Cascade&lt;/strong&gt;. We provide training &amp;amp; inference scripts, as well as a variety of different models you can use. &lt;br&gt;&lt;br&gt; This model is built upon the &lt;a href=&#34;https://openreview.net/forum?id=gU58d5QeGv&#34;&gt;Würstchen&lt;/a&gt; architecture and its main difference to other models, like Stable Diffusion, is that it is working at a much smaller latent space. Why is this important? The smaller the latent space, the &lt;strong&gt;faster&lt;/strong&gt; you can run inference and the &lt;strong&gt;cheaper&lt;/strong&gt; the training becomes. How small is the latent space? Stable Diffusion uses a compression factor of 8, resulting in a 1024x1024 image being encoded to 128x128. Stable Cascade achieves a compression factor of 42, meaning that it is possible to encode a 1024x1024 image to 24x24, while maintaining crisp reconstructions. The text-conditional model is then trained in the highly compressed latent space. Previous versions of this architecture, achieved a 16x cost reduction over Stable Diffusion 1.5. &lt;br&gt; &lt;br&gt; Therefore, this kind of model is well suited for usages where efficiency is important. Furthermore, all known extensions like finetuning, LoRA, ControlNet, IP-Adapter, LCM etc. are possible with this method as well. A few of those are already provided (finetuning, ControlNet, LoRA) in the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/train&#34;&gt;training&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference&#34;&gt;inference&lt;/a&gt; sections.&lt;/p&gt; &#xA;&lt;p&gt;Moreover, Stable Cascade achieves impressive results, both visually and evaluation wise. According to our evaluation, Stable Cascade performs best in both prompt alignment and aesthetic quality in almost all comparisons. The above picture shows the results from a human evaluation using a mix of parti-prompts (link) and aesthetic prompts. Specifically, Stable Cascade (30 inference steps) was compared against Playground v2 (50 inference steps), SDXL (50 inference steps), SDXL Turbo (1 inference step) and Würstchen v2 (30 inference steps). &lt;br&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img height=&#34;300&#34; src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/comparison.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Stable Cascade´s focus on efficiency is evidenced through its architecture and a higher compressed latent space. Despite the largest model containing 1.4 billion parameters more than Stable Diffusion XL, it still features faster inference times, as can be seen in the figure below.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img height=&#34;300&#34; src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/comparison-inference-speed.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/collage_2.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Model Overview&lt;/h2&gt; &#xA;&lt;p&gt;Stable Cascade consists of three models: Stage A, Stage B and Stage C, representing a cascade for generating images, hence the name &#34;Stable Cascade&#34;. Stage A &amp;amp; B are used to compress images, similarly to what the job of the VAE is in Stable Diffusion. However, as mentioned before, with this setup a much higher compression of images can be achieved. Furthermore, Stage C is responsible for generating the small 24 x 24 latents given a text prompt. The following picture shows this visually. Note that Stage A is a VAE and both Stage B &amp;amp; C are diffusion models.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/model-overview.jpg&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;For this release, we are providing two checkpoints for Stage C, two for Stage B and one for Stage A. Stage C comes with a 1 billion and 3.6 billion parameter version, but we highly recommend using the 3.6 billion version, as most work was put into its finetuning. The two versions for Stage B amount to 700 million and 1.5 billion parameters. Both achieve great results, however the 1.5 billion excels at reconstructing small and fine details. Therefore, you will achieve the best results if you use the larger variant of each. Lastly, Stage A contains 20 million parameters and is fixed due to its small size.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;This section will briefly outline how you can get started with &lt;strong&gt;Stable Cascade&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Running the model can be done through the notebooks provided in the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference&#34;&gt;inference&lt;/a&gt; section. You will find more details regarding downloading the models, compute requirements as well as some tutorials on how to use the models. Specifically, there are four notebooks provided for the following use-cases:&lt;/p&gt; &#xA;&lt;h4&gt;Text-to-Image&lt;/h4&gt; &#xA;&lt;p&gt;A compact &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference/text_to_image.ipynb&#34;&gt;notebook&lt;/a&gt; that provides you with basic functionality for text-to-image, image-variation and image-to-image.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text-to-Image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;code&gt;Cinematic photo of an anthropomorphic penguin sitting in a cafe reading a book and having a coffee.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/text-to-image-example-penguin.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image Variation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The model can also understand image embeddings, which makes it possible to generate variations of a given image (left). There was no prompt given here.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/image-variations-example-headset.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image-to-Image&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This works just as usual, by noising an image up to a specific point and then letting the model generate from that starting point. Here the left image is noised to 80% and the caption is: &lt;code&gt;A person riding a rodent.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/image-to-image-example-rodent.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Furthermore, the model is also accessible in the diffusers 🤗 library. You can find the documentation and usage &lt;a href=&#34;https://huggingface.co/stabilityai/stable-cascade&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;ControlNet&lt;/h4&gt; &#xA;&lt;p&gt;This &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference/controlnet.ipynb&#34;&gt;notebook&lt;/a&gt; shows how to use ControlNets that were trained by us or how to use one that you trained yourself for Stable Cascade. With this release, we provide the following ControlNets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inpainting / Outpainting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/controlnet-paint.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Face Identity&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/controlnet-face.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: The Face Identity ControlNet will be released at a later point.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Canny&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/controlnet-canny.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Super Resolution&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/controlnet-sr.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;These can all be used through the same notebook and only require changing the config for each ControlNet. More information is provided in the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference&#34;&gt;inference guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;LoRA&lt;/h4&gt; &#xA;&lt;p&gt;We also provide our own implementation for training and using LoRAs with Stable Cascade, which can be used to finetune the text-conditional model (Stage C). Specifically, you can add and learn new tokens and add LoRA layers to the model. This &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference/lora.ipynb&#34;&gt;notebook&lt;/a&gt; shows how you can use a trained LoRA. For example, training a LoRA on my dog with the following kind of training images:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/fernando_original.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Lets me generate the following images of my dog given the prompt: &lt;code&gt;Cinematic photo of a dog [fernando] wearing a space suit.&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/fernando.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Image Reconstruction&lt;/h4&gt; &#xA;&lt;p&gt;Lastly, one thing that might be very interesting for people, especially if you want to train your own text-conditional model from scratch, maybe even with a completely different architecture than our Stage C, is to use the (Diffusion) Autoencoder that Stable Cascade uses to be able to work in the highly compressed space. Just like people use Stable Diffusion&#39;s VAE to train their own models (e.g. Dalle3), you could use Stage A &amp;amp; B in the same way, while benefiting from a much higher compression, allowing you to train and run models faster. &lt;br&gt; The notebook shows how to encode and decode images and what specific benefits you get. For example, say you have the following batch of images of dimension &lt;code&gt;4 x 3 x 1024 x 1024&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/original.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;You can encode these images to a compressed size of &lt;code&gt;4 x 16 x 24 x 24&lt;/code&gt;, giving you a spatial compression factor of &lt;code&gt;1024 / 24 = 42.67&lt;/code&gt;. Afterwards you can use Stage A &amp;amp; B to decode the images back to &lt;code&gt;4 x 3 x 1024 x 1024&lt;/code&gt;, giving you the following output:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/figures/reconstructed.jpg&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;As you can see, the reconstructions are surprisingly close, even for small details. Such reconstructions are not possible with a standard VAE etc. The &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/inference/reconstruct_images.ipynb&#34;&gt;notebook&lt;/a&gt; gives you more information and easy code to try it out.&lt;/p&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;We provide code for training Stable Cascade from scratch, finetuning, ControlNet and LoRA. You can find a comprehensive explanation for how to do so in the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/train&#34;&gt;training folder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Remarks&lt;/h2&gt; &#xA;&lt;p&gt;The codebase is in early development. You might encounter unexpected errors or not perfectly optimized training and inference code. We apologize for that in advance. If there is interest, we will continue releasing updates to it, aiming to bring in the latest improvements and optimizations. Moreover, we would be more than happy to receive ideas, feedback or even updates from people that would like to contribute. Cheers.&lt;/p&gt; &#xA;&lt;h2&gt;Gradio App&lt;/h2&gt; &#xA;&lt;p&gt;First install gradio and diffusers by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install gradio&#xA;pip3 install accelerate # optionally&#xA;pip3 install git+https://github.com/kashif/diffusers.git@wuerstchen-v3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then from the root of the project run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;PYTHONPATH=./ python3 gradio_app/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{pernias2023wuerstchen,&#xA;      title={Wuerstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models}, &#xA;      author={Pablo Pernias and Dominic Rampas and Mats L. Richter and Christopher J. Pal and Marc Aubreville},&#xA;      year={2023},&#xA;      eprint={2306.00637},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;LICENSE&lt;/h2&gt; &#xA;&lt;p&gt;All the code from this repo is under an &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/LICENSE&#34;&gt;MIT LICENSE&lt;/a&gt;&lt;br&gt; The model weights, that you can get from Hugginface following &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/models/readme.md&#34;&gt;these instructions&lt;/a&gt;, are under a &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/StableCascade/master/WEIGHTS_LICENSE&#34;&gt;STABILITY AI NON-COMMERCIAL RESEARCH COMMUNITY LICENSE&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>