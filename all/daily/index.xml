<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-29T01:22:54Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Plachtaa/VALL-E-X</title>
    <updated>2023-08-29T01:22:54Z</updated>
    <id>tag:github.com,2023-08-29:/Plachtaa/VALL-E-X</id>
    <link href="https://github.com/Plachtaa/VALL-E-X" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open source implementation of Microsoft&#39;s VALL-E X zero-shot TTS model. Demo is available in https://plachtaa.github.io&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VALL-E X: Multilingual Text-to-Speech Synthesis and Voice Cloning üîä&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/qCBRmAnTxg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-%235865F2.svg?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;br&gt; English | &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/README-ZH.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; &lt;br&gt; An open source implementation of Microsoft&#39;s &lt;a href=&#34;https://arxiv.org/pdf/2303.03926&#34;&gt;VALL-E X&lt;/a&gt; zero-shot TTS model.&lt;br&gt; &lt;strong&gt;We release our trained model to the public for research or application usage.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/images/vallex_framework.jpg&#34; alt=&#34;vallex-framework&#34; title=&#34;VALL-E X framework&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;VALL-E X is an amazing multilingual text-to-speech (TTS) model proposed by Microsoft. While Microsoft initially publish in their research paper, they did not release any code or pretrained models. Recognizing the potential and value of this technology, our team took on the challenge to reproduce the results and train our own model. We are glad to share our trained VALL-E X model with the community, allowing everyone to experience the power next-generation TTS! üéß &lt;br&gt; &lt;br&gt; More details about the model are presented in &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/model-card.md&#34;&gt;model card&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìñ Quick Index&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-updates&#34;&gt;üöÄ Updates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-features&#34;&gt;üì¢ Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-installation&#34;&gt;üíª Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-demos&#34;&gt;üéß Demos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-usage-in-python&#34;&gt;üêç Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-faq&#34;&gt;‚ùì FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/#-todo&#34;&gt;üß† TODO&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023.08.23&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added long text generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023.08.20&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Added &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/README-ZH.md&#34;&gt;Chinese README&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023.08.14&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pretrained VALL-E X checkpoint is now released. Download it &lt;a href=&#34;https://drive.google.com/file/d/10gdQWvP-K_e1undkvv0p2b7SU6I4Egyl/view?usp=sharing&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíª Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install with pip, recommended with Python 3.10, CUDA 11.7 ~ 12.0, PyTorch 2.0+&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;git clone https://github.com/Plachtaa/VALL-E-X.git&#xA;cd VALL-E-X&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note: If you want to make prompt, you need to install ffmpeg and add its folder to the environment variable PATH.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üéß Demos&lt;/h2&gt; &#xA;&lt;p&gt;Not ready to set up the environment on your local machine just yet? No problem! We&#39;ve got you covered with our online demos. You can try out VALL-E X directly on Hugging Face or Google Colab, experiencing the model&#39;s capabilities hassle-free! &lt;br&gt; &lt;a href=&#34;https://huggingface.co/spaces/Plachta/VALL-E-X&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue.svg?sanitize=true&#34; alt=&#34;Open in Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1yyD_sz531QntLKowMHo-XxorsFBCfKul?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üì¢ Features&lt;/h2&gt; &#xA;&lt;p&gt;VALL-E X comes packed with cutting-edge functionalities:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Multilingual TTS&lt;/strong&gt;: Speak in three languages - English, Chinese, and Japanese - with natural and expressive speech synthesis.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Zero-shot Voice Cloning&lt;/strong&gt;: Enroll a short 3~10 seconds recording of an unseen speaker, and watch VALL-E X create personalized, high-quality speech that sounds just like them!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/a7baa51d-a53a-41cc-a03d-6970f25fcca7&#34;&gt;prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/b895601a-d126-4138-beff-061aabdc7985&#34;&gt;output.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speech Emotion Control&lt;/strong&gt;: Experience the power of emotions! VALL-E X can synthesize speech with the same emotion as the acoustic prompt provided, adding an extra layer of expressiveness to your audio.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/56fa9988-925e-4757-82c5-83ecb0df6266&#34;&gt;https://github.com/Plachtaa/VALL-E-X/assets/112609742/56fa9988-925e-4757-82c5-83ecb0df6266&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/699c47a3-d502-4801-8364-bd89bcc0b8f1&#34;&gt;https://github.com/Plachtaa/VALL-E-X/assets/112609742/699c47a3-d502-4801-8364-bd89bcc0b8f1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero-shot Cross-Lingual Speech Synthesis&lt;/strong&gt;: Take monolingual speakers on a linguistic journey! VALL-E X can produce personalized speech in another language without compromising on fluency or accent. Below is a Japanese speaker talk in Chinese &amp;amp; English. üáØüáµ üó£&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/ea6e2ee4-139a-41b4-837e-0bd04dda6e19&#34;&gt;jp-prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/db8f9782-923f-425e-ba94-e8c1bd48f207&#34;&gt;en-output.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/15829d79-e448-44d3-8965-fafa7a3f8c28&#34;&gt;zh-output.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Accent Control&lt;/strong&gt;: Get creative with accents! VALL-E X allows you to experiment with different accents, like speaking Chinese with an English accent or vice versa. üá®üá≥ üí¨&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/f688d7f6-70ef-46ec-b1cc-355c31e78b3b&#34;&gt;en-prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/be59c7ca-b45b-44ca-a30d-4d800c950ccc&#34;&gt;zh-accent-output.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/8b4f4f9b-f299-4ea4-a548-137437b71738&#34;&gt;en-accent-output.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Acoustic Environment Maintenance&lt;/strong&gt;: No need for perfectly clean audio prompts! VALL-E X adapts to the acoustic environment of the input, making speech generation feel natural and immersive.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h5&gt;see example&lt;/h5&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/68986d88-abd0-4d1d-96e4-4f893eb9259e&#34;&gt;noise-prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/96c4c612-4516-4683-8804-501b70938608&#34;&gt;noise-output.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Explore our &lt;a href=&#34;https://plachtaa.github.io/&#34;&gt;demo page&lt;/a&gt; for a lot more examples!&lt;/p&gt; &#xA;&lt;h2&gt;üêç Usage in Python&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;ü™ë Basics&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from utils.generation import SAMPLE_RATE, generate_audio, preload_models&#xA;from scipy.io.wavfile import write as write_wav&#xA;from IPython.display import Audio&#xA;&#xA;# download and load all models&#xA;preload_models()&#xA;&#xA;# generate audio from text&#xA;text_prompt = &#34;&#34;&#34;&#xA;Hello, my name is Nose. And uh, and I like hamburger. Hahaha... But I also have other interests such as playing tactic toast.&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt)&#xA;&#xA;# save audio to disk&#xA;write_wav(&#34;vallex_generation.wav&#34;, SAMPLE_RATE, audio_array)&#xA;&#xA;# play text in notebook&#xA;Audio(audio_array, rate=SAMPLE_RATE)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/578d7bbe-cda9-483e-898c-29646edc8f2e&#34;&gt;hamburger.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;üåé Foreign Language&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;br&gt; This VALL-E X implementation also supports Chinese and Japanese. All three languages have equally awesome performance! &#xA; &lt;br&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;text_prompt = &#34;&#34;&#34;&#xA;    „ÉÅ„É•„ÇΩ„ÇØ„ÅØÁßÅ„ÅÆ„ÅäÊ∞ó„Å´ÂÖ•„Çä„ÅÆÁ•≠„Çä„Åß„Åô„ÄÇ ÁßÅ„ÅØÊï∞Êó•Èñì‰ºë„Çì„Åß„ÄÅÂèã‰∫∫„ÇÑÂÆ∂Êóè„Å®„ÅÆÊôÇÈñì„ÇíÈÅé„Åî„Åô„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/ee57a688-3e83-4be5-b0fe-019d16eec51c&#34;&gt;vallex_japanese.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;Note: VALL-E X controls accent perfectly even when synthesizing code-switch text. However, you need to manually denote language of respective sentences (since our g2p tool is rule-base)&lt;/em&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_prompt = &#34;&#34;&#34;&#xA;    [EN]The Thirty Years&#39; War was a devastating conflict that had a profound impact on Europe.[EN]&#xA;    [ZH]ËøôÊòØÂéÜÂè≤ÁöÑÂºÄÂßã„ÄÇ Â¶ÇÊûúÊÇ®ÊÉ≥Âê¨Êõ¥Â§öÔºåËØ∑ÁªßÁª≠„ÄÇ[ZH]&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt, language=&#39;mix&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/d8667abf-bd08-499f-a383-a861d852f98a&#34;&gt;vallex_codeswitch.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;üìº Voice Presets&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;VALL-E X provides tens of speaker voices which you can directly used for inference! Browse all voices in the &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/presets&#34;&gt;code&lt;/a&gt;&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;VALL-E X tries to match the tone, pitch, emotion and prosody of a given preset. The model also attempts to preserve music, ambient noise, etc.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_prompt = &#34;&#34;&#34;&#xA;I am an innocent boy with a smoky voice. It is a great honor for me to speak at the United Nations today.&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt, prompt=&#34;dingzhen&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/d3f55732-b1cd-420f-87d6-eab60db14dc5&#34;&gt;smoky.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;üéôVoice Cloning&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;VALL-E X supports voice cloning! You can make a voice prompt with any person, character or even your own voice, and use it like other voice presets.&lt;br&gt; To make a voice prompt, you need to provide a speech of 3~10 seconds long, as well as the transcript of the speech. You can also leave the transcript blank to let the &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; model to generate the transcript.&lt;/p&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;VALL-E X tries to match the tone, pitch, emotion and prosody of a given prompt. The model also attempts to preserve music, ambient noise, etc.&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from utils.prompt_making import make_prompt&#xA;&#xA;### Use given transcript&#xA;make_prompt(name=&#34;paimon&#34;, audio_prompt_path=&#34;paimon_prompt.wav&#34;,&#xA;                transcript=&#34;Just, what was that? Paimon thought we were gonna get eaten.&#34;)&#xA;&#xA;### Alternatively, use whisper&#xA;make_prompt(name=&#34;paimon&#34;, audio_prompt_path=&#34;paimon_prompt.wav&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Now let&#39;s try out the prompt we&#39;ve just made!&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text_prompt = &#34;&#34;&#34;&#xA;Hey, Traveler, Listen to this, This machine has taken my voice, and now it can talk just like me!&#xA;&#34;&#34;&#34;&#xA;audio_array = generate_audio(text_prompt, prompt=&#34;paimon&#34;)&#xA;&#xA;write_wav(&#34;paimon_cloned.wav&#34;, SAMPLE_RATE, audio_array)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/e7922859-9d12-4e2a-8651-e156e4280311&#34;&gt;paimon_prompt.webm&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/assets/112609742/60d3b7e9-5ead-4024-b499-a897ce5f3d5e&#34;&gt;paimon_cloned.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;h3&gt;üé¢User Interface&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;Not comfortable with codes? No problem! We&#39;ve also created a user-friendly graphical interface for VALL-E X. It allows you to interact with the model effortlessly, making voice cloning and multilingual speech synthesis a breeze. &lt;br&gt; You can launch the UI by the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;python launch-ui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Hardware and Inference Speed&lt;/h2&gt; &#xA;&lt;p&gt;VALL-E X works well on both CPU and GPU (&lt;code&gt;pytorch 2.0+&lt;/code&gt;, CUDA 11.7 and CUDA 12.0).&lt;/p&gt; &#xA;&lt;p&gt;A GPU VRAM of 6GB is enough for running VALL-E X without offloading.&lt;/p&gt; &#xA;&lt;h2&gt;‚öôÔ∏è Details&lt;/h2&gt; &#xA;&lt;p&gt;VALL-E X is similar to &lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;Bark&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;VALL-E&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2209.03143&#34;&gt;AudioLM&lt;/a&gt;, which generates audio in GPT-style by predicting audio tokens quantized by &lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;EnCodec&lt;/a&gt;. &lt;br&gt; Comparing to &lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;Bark&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úî &lt;strong&gt;Light-weighted&lt;/strong&gt;: 3Ô∏è‚É£ ‚úñ smaller,&lt;/li&gt; &#xA; &lt;li&gt;‚úî &lt;strong&gt;Efficient&lt;/strong&gt;: 4Ô∏è‚É£ ‚úñ faster,&lt;/li&gt; &#xA; &lt;li&gt;‚úî &lt;strong&gt;Better quality on Chinese &amp;amp; Japanese&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úî &lt;strong&gt;Cross-lingual speech without foreign accent&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úî &lt;strong&gt;Easy voice-cloning&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚ùå &lt;strong&gt;Less languages&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚ùå &lt;strong&gt;No special tokens for music / sound effects&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Supported Languages&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;English (en)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Japanese (ja)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese, simplified (zh)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;‚ùì FAQ&lt;/h2&gt; &#xA;&lt;h4&gt;Where can I download the model checkpoint?&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We use &lt;code&gt;gdown&lt;/code&gt; to download the model to directory &lt;code&gt;./checkpoints/&lt;/code&gt; when you run &lt;code&gt;preload_models()&lt;/code&gt; for the first time.&lt;/li&gt; &#xA; &lt;li&gt;If you cannot access Google, please manually download from &lt;a href=&#34;https://huggingface.co/Plachta/VALL-E-X/resolve/main/vallex-checkpoint.pt&#34;&gt;this link&lt;/a&gt;, and put the file under directory &lt;code&gt;./checkpoints/&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;How much VRAM do I need?&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;6GB GPU VRAM - Almost all NVIDIA GPUs satisfy the requirement.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Why the model fails to generate long text?&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Transformer&#39;s computation complexity increases quadratically while the sequence length increases. Hence, all training are kept under 22 seconds. Please make sure the total length of audio prompt and generated audio is less than 22 seconds to ensure acceptable performance.&lt;/li&gt; &#xA; &lt;li&gt;To generate long text, a huge paragraph must be breakdown into short sentences. We are currently working on this.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;MORE TO BE ADDED...&lt;/h4&gt; &#xA;&lt;h2&gt;üß† TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add Chinese README&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;code&gt;.bat&lt;/code&gt; scripts for non-python users&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Long text generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine-tuning for better voice adaptation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Replace Encodec decoder with Vocos decoder&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; To be added...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üôè Appreciation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.03926&#34;&gt;VALL-E X paper&lt;/a&gt; for the brilliant idea&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lifeiteng/vall-e&#34;&gt;lifeiteng&#39;s vall-e&lt;/a&gt; for related training code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/suno-ai/bark&#34;&gt;bark&lt;/a&gt; for the amazing pioneering work in neuro-codec TTS model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚≠êÔ∏è Show Your Support&lt;/h2&gt; &#xA;&lt;p&gt;If you find VALL-E X interesting and useful, give us a star on GitHub! ‚≠êÔ∏è It encourages us to keep improving the model and adding exciting features.&lt;/p&gt; &#xA;&lt;h2&gt;üìú License&lt;/h2&gt; &#xA;&lt;p&gt;VALL-E X is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/Plachtaa/VALL-E-X/master/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Have questions or need assistance? Feel free to &lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X/issues/new&#34;&gt;open an issue&lt;/a&gt; or join our &lt;a href=&#34;https://discord.gg/qCBRmAnTxg&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Happy voice cloning! üé§&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bitcoin/bitcoin</title>
    <updated>2023-08-29T01:22:54Z</updated>
    <id>tag:github.com,2023-08-29:/bitcoin/bitcoin</id>
    <link href="https://github.com/bitcoin/bitcoin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bitcoin Core integration/staging tree&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Bitcoin Core integration/staging tree&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bitcoincore.org&#34;&gt;https://bitcoincore.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For an immediately usable, binary version of the Bitcoin Core software, see &lt;a href=&#34;https://bitcoincore.org/en/download/&#34;&gt;https://bitcoincore.org/en/download/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;What is Bitcoin Core?&lt;/h2&gt; &#xA;&lt;p&gt;Bitcoin Core connects to the Bitcoin peer-to-peer network to download and fully validate blocks and transactions. It also includes a wallet and graphical user interface, which can be optionally built.&lt;/p&gt; &#xA;&lt;p&gt;Further information about Bitcoin Core is available in the &lt;a href=&#34;https://raw.githubusercontent.com/bitcoin/bitcoin/master/doc&#34;&gt;doc folder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Bitcoin Core is released under the terms of the MIT license. See &lt;a href=&#34;https://raw.githubusercontent.com/bitcoin/bitcoin/master/COPYING&#34;&gt;COPYING&lt;/a&gt; for more information or see &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;https://opensource.org/licenses/MIT&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development Process&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;master&lt;/code&gt; branch is regularly built (see &lt;code&gt;doc/build-*.md&lt;/code&gt; for instructions) and tested, but it is not guaranteed to be completely stable. &lt;a href=&#34;https://github.com/bitcoin/bitcoin/tags&#34;&gt;Tags&lt;/a&gt; are created regularly from release branches to indicate new official, stable release versions of Bitcoin Core.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/bitcoin-core/gui&#34;&gt;https://github.com/bitcoin-core/gui&lt;/a&gt; repository is used exclusively for the development of the GUI. Its master branch is identical in all monotree repositories. Release branches and tags do not exist, so please do not fork that repository unless it is for development reasons.&lt;/p&gt; &#xA;&lt;p&gt;The contribution workflow is described in &lt;a href=&#34;https://raw.githubusercontent.com/bitcoin/bitcoin/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; and useful hints for developers can be found in &lt;a href=&#34;https://raw.githubusercontent.com/bitcoin/bitcoin/master/doc/developer-notes.md&#34;&gt;doc/developer-notes.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;Testing and code review is the bottleneck for development; we get more pull requests than we can review and test on short notice. Please be patient and help out by testing other people&#39;s pull requests, and remember this is a security-critical project where any mistake might cost people lots of money.&lt;/p&gt; &#xA;&lt;h3&gt;Automated Testing&lt;/h3&gt; &#xA;&lt;p&gt;Developers are strongly encouraged to write &lt;a href=&#34;https://raw.githubusercontent.com/bitcoin/bitcoin/master/src/test/README.md&#34;&gt;unit tests&lt;/a&gt; for new code, and to submit new unit tests for old code. Unit tests can be compiled and run (assuming they weren&#39;t disabled in configure) with: &lt;code&gt;make check&lt;/code&gt;. Further details on running and extending unit tests can be found in &lt;a href=&#34;https://raw.githubusercontent.com/bitcoin/bitcoin/master/src/test/README.md&#34;&gt;/src/test/README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There are also &lt;a href=&#34;https://raw.githubusercontent.com/bitcoin/bitcoin/master/test&#34;&gt;regression and integration tests&lt;/a&gt;, written in Python. These tests can be run (if the &lt;a href=&#34;https://raw.githubusercontent.com/bitcoin/bitcoin/master/test&#34;&gt;test dependencies&lt;/a&gt; are installed) with: &lt;code&gt;test/functional/test_runner.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The CI (Continuous Integration) systems make sure that every pull request is built for Windows, Linux, and macOS, and that unit/sanity tests are run automatically.&lt;/p&gt; &#xA;&lt;h3&gt;Manual Quality Assurance (QA) Testing&lt;/h3&gt; &#xA;&lt;p&gt;Changes should be tested by somebody other than the developer who wrote the code. This is especially important for large or high-risk changes. It is useful to add a test plan to the pull request description if testing the changes is not straightforward.&lt;/p&gt; &#xA;&lt;h2&gt;Translations&lt;/h2&gt; &#xA;&lt;p&gt;Changes to translations as well as new translations can be submitted to &lt;a href=&#34;https://www.transifex.com/bitcoin/bitcoin/&#34;&gt;Bitcoin Core&#39;s Transifex page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Translations are periodically pulled from Transifex and merged into the git repository. See the &lt;a href=&#34;https://raw.githubusercontent.com/bitcoin/bitcoin/master/doc/translation_process.md&#34;&gt;translation process&lt;/a&gt; for details on how this works.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: We do not accept translation changes as GitHub pull requests because the next pull from Transifex would automatically overwrite them again.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>neulab/prompt2model</title>
    <updated>2023-08-29T01:22:54Z</updated>
    <id>tag:github.com,2023-08-29:/neulab/prompt2model</id>
    <link href="https://github.com/neulab/prompt2model" rel="alternate"></link>
    <summary type="html">&lt;p&gt;prompt2model - Generate Deployable Models from Natural Language Instructions&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;prompt2model - Generate Deployable Models from Instructions&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/prompt2model&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/prompt2model.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/neulab/prompt2model/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;Github Actions CI tests&#34;&gt; &lt;a href=&#34;https://lbesson.mit-license.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue.svg?sanitize=true&#34; alt=&#34;MIT license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/UCy9csEmFc&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1144245269001678959&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Prompt2Model&lt;/code&gt; is a system that takes a natural language task description (like the prompts used for LLMs such as ChatGPT) to train a small special-purpose model that is conducive for deployment.&lt;/p&gt; &#xA;&lt;img width=&#34;360&#34; alt=&#34;prompt2model_teaser&#34; src=&#34;https://github.com/neulab/prompt2model/assets/2577384/39ca466a-5355-4d82-8312-303e52ba2bca&#34;&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install prompt2model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Our current &lt;code&gt;prompt2model&lt;/code&gt; implementation uses the OpenAI API. Accordingly, you need to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Sign up on the OpenAI website and obtain an OpenAI API key.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the environment variable &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to your API key by running the following command in your terminal:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&amp;lt;your key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to create a small model from a prompt, as shown in the demo video below. This script must be run on a device with an internet connection to access the OpenAI API. For best results, run this script on a device with a GPU for training your model.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/neulab/prompt2model/assets/2577384/8d73394b-3028-4a0b-bdc3-c127082868f2&#34;&gt;https://github.com/neulab/prompt2model/assets/2577384/8d73394b-3028-4a0b-bdc3-c127082868f2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Tips and Examples to Write a Good Prompt&lt;/h2&gt; &#xA;&lt;p&gt;You can see the tips and examples to write a good prompt in &lt;a href=&#34;https://raw.githubusercontent.com/neulab/prompt2model/main/prompt_examples.md&#34;&gt;prompt_examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Components&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;prompt2model&lt;/code&gt; package is composed of several components, each designed to fulfill a specific purpose. To gain a comprehensive understanding of how to utilize each component effectively, please consult the &lt;code&gt;readme.md&lt;/code&gt; file situated in the directory of the respective component. These files can be found at &lt;code&gt;./prompt2model/&amp;lt;component&amp;gt;/readme.md&lt;/code&gt;. They provide detailed information and instructions on customizing and maximizing the functionality of each component within the package.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re interested in contributing to the &lt;code&gt;prompt2model&lt;/code&gt; project, please&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;refer to &lt;a href=&#34;https://raw.githubusercontent.com/neulab/prompt2model/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;open an &lt;a href=&#34;https://github.com/neulab/prompt2model/issues&#34;&gt;issue&lt;/a&gt; or submit a PR&lt;/li&gt; &#xA; &lt;li&gt;join us on &lt;a href=&#34;https://discord.gg/UCy9csEmFc&#34;&gt;discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;or reach out to &lt;a href=&#34;https://twitter.com/vijaytarian&#34;&gt;@vijaytarian&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/Chenan3_Zhao&#34;&gt;@Chenan3_Zhao&lt;/a&gt; on Twitter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cite&lt;/h2&gt; &#xA;&lt;p&gt;We have &lt;a href=&#34;https://arxiv.org/abs/2308.12261&#34;&gt;written a paper describing Prompt2Model in detail&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you use Prompt2Model in your research, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{prompt2model,&#xA;      title={Prompt2Model: Generating Deployable Models from Natural Language Instructions},&#xA;      author={Vijay Viswanathan and Chenyang Zhao and Amanda Bertsch and Tongshuang Wu and Graham Neubig},&#xA;      year={2023},&#xA;      eprint={2308.12261},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>