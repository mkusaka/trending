<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-07T01:30:18Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>oobabooga/text-generation-webui</title>
    <updated>2023-03-07T01:30:18Z</updated>
    <id>tag:github.com,2023-03-07:/oobabooga/text-generation-webui</id>
    <link href="https://github.com/oobabooga/text-generation-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, GPT-Neo, and Pygmalion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Text generation web UI&lt;/h1&gt; &#xA;&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, GPT-Neo, and Pygmalion.&lt;/p&gt; &#xA;&lt;p&gt;Its goal is to become the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt; of text generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/oobabooga/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb&#34;&gt;[Try it on Google Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/qa.png&#34; alt=&#34;Image1&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/cai3.png&#34; alt=&#34;Image2&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/gpt4chan.png&#34; alt=&#34;Image3&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/galactica.png&#34; alt=&#34;Image4&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Switch between different models using a dropdown menu.&lt;/li&gt; &#xA; &lt;li&gt;Notebook mode that resembles OpenAI&#39;s playground.&lt;/li&gt; &#xA; &lt;li&gt;Chat mode for conversation and role playing.&lt;/li&gt; &#xA; &lt;li&gt;Generate nice HTML output for GPT-4chan.&lt;/li&gt; &#xA; &lt;li&gt;Generate Markdown output for &lt;a href=&#34;https://github.com/paperswithcode/galai&#34;&gt;GALACTICA&lt;/a&gt;, including LaTeX support.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;a href=&#34;https://huggingface.co/models?search=pygmalionai/pygmalion&#34;&gt;Pygmalion&lt;/a&gt; and custom characters in JSON or TavernAI Character Card formats (&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Pygmalion-chat-model-FAQ&#34;&gt;FAQ&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Advanced chat features (send images, get audio responses with TTS).&lt;/li&gt; &#xA; &lt;li&gt;Stream the text output in real time.&lt;/li&gt; &#xA; &lt;li&gt;Load parameter presets from text files.&lt;/li&gt; &#xA; &lt;li&gt;Load large models in 8-bit mode (see &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/147#issuecomment-1456040134&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/20#issuecomment-1411650652&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://www.reddit.com/r/PygmalionAI/comments/1115gom/running_pygmalion_6b_with_8gb_of_vram/&#34;&gt;here&lt;/a&gt; if you are on Windows).&lt;/li&gt; &#xA; &lt;li&gt;Split large models across your GPU(s), CPU, and disk.&lt;/li&gt; &#xA; &lt;li&gt;CPU mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/FlexGen&#34;&gt;FlexGen offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/DeepSpeed&#34;&gt;DeepSpeed ZeRO-3 offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Get responses via API, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example-streaming.py&#34;&gt;with&lt;/a&gt; or &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example.py&#34;&gt;without&lt;/a&gt; streaming.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/RWKV-model&#34;&gt;Supports the RWKV model&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Supports softprompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Extensions&#34;&gt;Supports extensions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Running-on-Colab&#34;&gt;Works on Google Colab&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation option 1: conda&lt;/h2&gt; &#xA;&lt;p&gt;Open a terminal and copy and paste these commands one at a time (&lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;install conda&lt;/a&gt; first if you don&#39;t have it already):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n textgen&#xA;conda activate textgen&#xA;conda install torchvision torchaudio pytorch-cuda=11.7 git -c pytorch -c nvidia&#xA;git clone https://github.com/oobabooga/text-generation-webui&#xA;cd text-generation-webui&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The third line assumes that you have an NVIDIA GPU.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have an AMD GPU, replace the third command with this one:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you are running in CPU mode, replace the third command with this one:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision torchaudio git -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation option 2: one-click installers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga-windows.zip&#34;&gt;oobabooga-windows.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga-linux.zip&#34;&gt;oobabooga-linux.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just download the zip above, extract it, and double click on &#34;install&#34;. The web UI and all its dependencies will be installed in the same folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To download a model, double click on &#34;download-model&#34;&lt;/li&gt; &#xA; &lt;li&gt;To start the web UI, double click on &#34;start-webui&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloading models&lt;/h2&gt; &#xA;&lt;p&gt;Models should be placed under &lt;code&gt;models/model-name&lt;/code&gt;. For instance, &lt;code&gt;models/gpt-j-6B&lt;/code&gt; for &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Hugging Face&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;Hugging Face&lt;/a&gt; is the main place to download models. These are some noteworthy examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&amp;amp;search=eleutherai+%2F+gpt-neo&#34;&gt;GPT-Neo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=eleutherai/pythia&#34;&gt;Pythia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/opt&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/galactica&#34;&gt;GALACTICA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=erebus&#34;&gt;*-Erebus&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=pygmalion&#34;&gt;Pygmalion&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can automatically download a model from HF using the script &lt;code&gt;download-model.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py organization/model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py facebook/opt-1.3b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to download a model manually, note that all you need are the json, txt, and pytorch*.bin (or model*.safetensors) files. The remaining files are not necessary.&lt;/p&gt; &#xA;&lt;h4&gt;GPT-4chan&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/ykilcher/gpt-4chan&#34;&gt;GPT-4chan&lt;/a&gt; has been shut down from Hugging Face, so you need to download it elsewhere. You have two options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Torrent: &lt;a href=&#34;https://archive.org/details/gpt4chan_model_float16&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://archive.org/details/gpt4chan_model&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Direct download: &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model_float16/&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model/&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The 32-bit version is only relevant if you intend to run the model in CPU mode. Otherwise, you should use the 16-bit version.&lt;/p&gt; &#xA;&lt;p&gt;After downloading the model, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Place the files under &lt;code&gt;models/gpt4chan_model_float16&lt;/code&gt; or &lt;code&gt;models/gpt4chan_model&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Place GPT-J 6B&#39;s config.json file in that same folder: &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/raw/main/config.json&#34;&gt;config.json&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download GPT-J 6B&#39;s tokenizer files (they will be automatically detected when you attempt to load GPT-4chan):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py EleutherAI/gpt-j-6B --text-only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the web UI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate textgen&#xA;python server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then browse to&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://localhost:7860/?__theme=dark&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Optionally, you can use the following command-line flags:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Flag&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;show this help message and exit&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model MODEL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Name of the model to load by default.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--notebook&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in notebook mode, where the output is written to the same text box as the input.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cai-chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode with a style similar to Character.AI&#39;s. If the file &lt;code&gt;img_bot.png&lt;/code&gt; or &lt;code&gt;img_bot.jpg&lt;/code&gt; exists in the same folder as server.py, this image will be used as the bot&#39;s profile picture. Similarly, &lt;code&gt;img_me.png&lt;/code&gt; or &lt;code&gt;img_me.jpg&lt;/code&gt; will be used as your profile picture.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use the CPU to generate text.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--load-in-8bit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with 8-bit precision.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--bf16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--auto-devices&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Automatically split the model across the available GPU(s) and CPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;If the model is too large for your GPU(s) and CPU combined, send the remaining layers to the disk.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk-cache-dir DISK_CACHE_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory to save the disk cache to. Defaults to &lt;code&gt;cache/&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--gpu-memory GPU_MEMORY [GPU_MEMORY ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maxmimum GPU memory in GiB to be allocated per GPU. Example: &lt;code&gt;--gpu-memory 10&lt;/code&gt; for a single GPU, &lt;code&gt;--gpu-memory 10 5&lt;/code&gt; for two GPUs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu-memory CPU_MEMORY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maximum CPU memory in GiB to allocate for offloaded weights. Must be an integer number. Defaults to 99.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--flexgen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of FlexGen offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--percent PERCENT [PERCENT ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: allocation percentages. Must be 6 numbers separated by spaces (default: 0, 100, 100, 0, 100, 0).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--compress-weight&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: Whether to compress weight (default: False).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--pin-weight [PIN_WEIGHT]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: whether to pin weights (setting this to False reduces CPU memory by 20%).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--deepspeed&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of DeepSpeed ZeRO-3 for inference via the Transformers integration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--nvme-offload-dir NVME_OFFLOAD_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Directory to use for ZeRO-3 NVME offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--local_rank LOCAL_RANK&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Optional argument for distributed setups.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-strategy RWKV_STRATEGY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: The strategy to use while loading the model. Examples: &#34;cpu fp32&#34;, &#34;cuda fp16&#34;, &#34;cuda fp16i8&#34;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-cuda-on&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: Compile the CUDA kernel for better performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--no-stream&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Don&#39;t stream the text output in real time. This improves the text generation performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--settings SETTINGS_FILE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the default interface settings from this json file. See &lt;code&gt;settings-template.json&lt;/code&gt; for an example. If you create a file called &lt;code&gt;settings.json&lt;/code&gt;, this file will be loaded by default without the need to use the &lt;code&gt;--settings&lt;/code&gt; flag.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--extensions EXTENSIONS [EXTENSIONS ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The list of extensions to load. If you want to load more than one extension, write the names separated by spaces.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Make the web UI reachable from your local network.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen-port LISTEN_PORT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The listening port that the server will use.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--share&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Create a public URL. This is useful for running the web UI on Google Colab or similar.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--verbose&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Print the prompts to the terminal.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Out of memory errors? &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide&#34;&gt;Check this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Presets&lt;/h2&gt; &#xA;&lt;p&gt;Inference settings presets can be created under &lt;code&gt;presets/&lt;/code&gt; as text files. These files are detected automatically at startup.&lt;/p&gt; &#xA;&lt;p&gt;By default, 10 presets by NovelAI and KoboldAI are included. These were selected out of a sample of 43 presets after applying a K-Means clustering algorithm and selecting the elements closest to the average of each cluster.&lt;/p&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/System-requirements&#34;&gt;wiki&lt;/a&gt; for some examples of VRAM and RAM usage in both GPU and CPU mode.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests, suggestions, and issue reports are welcome.&lt;/p&gt; &#xA;&lt;p&gt;Before reporting a bug, make sure that you have created a conda environment and installed the dependencies exactly as in the &lt;em&gt;Installation&lt;/em&gt; section above.&lt;/p&gt; &#xA;&lt;p&gt;These issues are known:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8-bit doesn&#39;t work properly on Windows or older GPUs.&lt;/li&gt; &#xA; &lt;li&gt;DeepSpeed doesn&#39;t work properly on Windows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For these two, please try commenting on an existing issue instead of creating a new one.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NovelAI and KoboldAI presets: &lt;a href=&#34;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&#34;&gt;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pygmalion preset, code for early stopping in chat mode, code for some of the sliders, --chat mode colors: &lt;a href=&#34;https://github.com/PygmalionAI/gradio-ui/&#34;&gt;https://github.com/PygmalionAI/gradio-ui/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Verbose preset: Anonymous 4chan user.&lt;/li&gt; &#xA; &lt;li&gt;Instruct-Joi preset: &lt;a href=&#34;https://huggingface.co/Rallio67/joi_12B_instruct_alpha&#34;&gt;https://huggingface.co/Rallio67/joi_12B_instruct_alpha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gradio dropdown menu refresh button: &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>GaiZhenbiao/ChuanhuChatGPT</title>
    <updated>2023-03-07T01:30:18Z</updated>
    <id>tag:github.com,2023-03-07:/GaiZhenbiao/ChuanhuChatGPT</id>
    <link href="https://github.com/GaiZhenbiao/ChuanhuChatGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GUI for ChatGPT API&lt;/p&gt;&lt;hr&gt;&lt;img height=&#34;128&#34; align=&#34;left&#34; src=&#34;https://user-images.githubusercontent.com/51039745/222689546-7612df0e-e28b-4693-9f5f-4ef2be3daf48.png&#34; alt=&#34;Logo&#34;&gt; &#xA;&lt;h1&gt;å·è™ ChatGPT / Chuanhu ChatGPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/GaiZhenbiao/ChuanhuChatGPT&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gradio.app/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Base-Gradio-fb7d1a?style=flat&#34; alt=&#34;Base&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1mo4y1r7eE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Bilibili-%E8%A7%86%E9%A2%91%E6%95%99%E7%A8%8B-ff69b4?style=flat&amp;amp;logo=bilibili&#34; alt=&#34;Bilibili&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;ä¸ºChatGPT APIæä¾›äº†ä¸€ä¸ªWebå›¾å½¢ç•Œé¢ã€‚åœ¨Bilibiliä¸Š&lt;a href=&#34;https://www.bilibili.com/video/BV1mo4y1r7eE/&#34;&gt;è§‚çœ‹è§†é¢‘æ•™ç¨‹&lt;/a&gt;ã€‚ä¹Ÿå¯ä»¥åœ¨Hugging Faceä¸Š&lt;a href=&#34;https://huggingface.co/spaces/JohnSmith9982/ChuanhuChatGPT&#34;&gt;åœ¨çº¿ä½“éªŒ&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/51039745/223148794-f4fd2fcb-3e48-4cdf-a759-7aa463d3f14c.gif&#34; alt=&#34;Animation Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ‰ğŸ‰ğŸ‰ é‡å¤§æ›´æ–°&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ç²¾ç®€äº†UI&lt;/li&gt; &#xA; &lt;li&gt;åƒå®˜æ–¹ChatGPTé‚£æ ·å®æ—¶å›å¤&lt;/li&gt; &#xA; &lt;li&gt;æ”¹è¿›çš„ä¿å­˜/åŠ è½½åŠŸèƒ½&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;åŠŸèƒ½&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; åƒå®˜æ–¹å®¢æˆ·ç«¯é‚£æ ·æ”¯æŒå®æ—¶æ˜¾ç¤ºå›ç­”ï¼&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; é‡è¯•å¯¹è¯ï¼Œè®©ChatGPTå†å›ç­”ä¸€æ¬¡ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ä¼˜åŒ–Tokensï¼Œå‡å°‘Tokenså ç”¨ï¼Œä»¥æ”¯æŒæ›´é•¿çš„å¯¹è¯ã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; è®¾ç½®System Promptï¼Œæœ‰æ•ˆåœ°è®¾å®šå‰ç½®æ¡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ä¿å­˜/åŠ è½½å¯¹è¯å†å²è®°å½•&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; åœ¨å›¾å½¢ç•Œé¢ä¸­æ·»åŠ API key&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; System Promptæ¨¡æ¿åŠŸèƒ½ï¼Œä»é¢„ç½®çš„Promptåº“ä¸­é€‰æ‹©&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; å®æ—¶æ˜¾ç¤ºTokensç”¨é‡&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ä½¿ç”¨æŠ€å·§&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä½¿ç”¨System Promptå¯ä»¥å¾ˆæœ‰æ•ˆåœ°è®¾å®šå‰ææ¡ä»¶&lt;/li&gt; &#xA; &lt;li&gt;å¯¹äºé•¿å¯¹è¯ï¼Œå¯ä»¥ä½¿ç”¨â€œä¼˜åŒ–Tokensâ€æŒ‰é’®å‡å°‘Tokenså ç”¨ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœéƒ¨ç½²åˆ°æœåŠ¡å™¨ï¼Œå°†ç¨‹åºæœ€åä¸€å¥æ”¹æˆ&lt;code&gt;demo.launch(server_name=&#34;0.0.0.0&#34;, server_port=99999)&lt;/code&gt;ã€‚å…¶ä¸­&lt;code&gt;99999&lt;/code&gt;æ˜¯ç«¯å£å·ï¼Œåº”è¯¥æ˜¯1000-65535ä»»æ„å¯ç”¨ç«¯å£ï¼Œè¯·è‡ªè¡Œæ›´æ”¹ä¸ºå®é™…ç«¯å£å·ã€‚&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœéœ€è¦è·å–å…¬å…±é“¾æ¥ï¼Œå°†ç¨‹åºæœ€åä¸€å¥æ”¹æˆ&lt;code&gt;demo.launch(share=True)&lt;/code&gt;ã€‚æ³¨æ„ç¨‹åºå¿…é¡»åœ¨è¿è¡Œï¼Œæ‰èƒ½é€šè¿‡å…¬å…±é“¾æ¥è®¿é—®&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;å®‰è£…æ–¹å¼&lt;/h2&gt; &#xA;&lt;h3&gt;å¡«å†™APIå¯†é’¥&lt;/h3&gt; &#xA;&lt;h4&gt;åœ¨å›¾å½¢ç•Œé¢ä¸­å¡«å†™ä½ çš„APIå¯†é’¥&lt;/h4&gt; &#xA;&lt;p&gt;è¿™æ ·è®¾ç½®çš„å¯†é’¥ä¼šåœ¨é¡µé¢åˆ·æ–°åè¢«æ¸…é™¤&lt;/p&gt; &#xA;&lt;img width=&#34;760&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/51039745/222873756-3858bb82-30b9-49bc-9019-36e378ee624d.png&#34;&gt; &#xA;&lt;h4&gt;â€¦â€¦æˆ–è€…åœ¨ä»£ç ä¸­å¡«å…¥ä½ çš„ OpenAI API å¯†é’¥&lt;/h4&gt; &#xA;&lt;p&gt;è¿™æ ·è®¾ç½®çš„å¯†é’¥ä¼šæˆä¸ºé»˜è®¤å¯†é’¥&lt;/p&gt; &#xA;&lt;img width=&#34;552&#34; alt=&#34;SCR-20230302-sula&#34; src=&#34;https://user-images.githubusercontent.com/51039745/222445258-248f2789-81d2-4f0a-8697-c720f588d8de.png&#34;&gt; &#xA;&lt;h3&gt;å®‰è£…ä¾èµ–&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¦‚æœæŠ¥é”™ï¼Œè¯•è¯•&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œè¯·å…ˆ&lt;a href=&#34;https://www.runoob.com/python/python-install.html&#34;&gt;å®‰è£…Python&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœä¸‹è½½æ…¢ï¼Œå»ºè®®&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/help/pypi/&#34;&gt;é…ç½®æ¸…åæº&lt;/a&gt;ï¼Œæˆ–è€…ç§‘å­¦ä¸Šç½‘ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;å¯åŠ¨&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ChuanhuChatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¦‚æœæŠ¥é”™ï¼Œè¯•è¯•&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 ChuanhuChatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œè¯·å…ˆ&lt;a href=&#34;https://www.runoob.com/python/python-install.html&#34;&gt;å®‰è£…Python&lt;/a&gt;ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;æˆ–è€…ï¼Œä½¿ç”¨Docker å®‰è£…ä¸è¿è¡Œ&lt;/h2&gt; &#xA;&lt;h3&gt;ä»æœ¬é¡¹ç›®çš„Packagesé¡µé¢æ‹‰å–&lt;/h3&gt; &#xA;&lt;p&gt;ä»æœ¬é¡¹ç›®çš„&lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/pkgs/container/chuanhuchatgpt&#34;&gt;Packages&lt;/a&gt;é¡µé¢æ‹‰å–Dockeré•œåƒï¼Œä½¿ç”¨Github Actionsè‡ªåŠ¨åˆ›å»ºã€‚ä¹Ÿå¯ä»¥å»æœ¬é¡¹ç›®çš„&lt;a href=&#34;https://hub.docker.com/r/tuchuanhuhuhu/chuanhuchatgpt&#34;&gt;Dockerhubé¡µé¢&lt;/a&gt;æ‹‰å–ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;æ‰‹åŠ¨æ„å»ºé•œåƒ&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t chuanhuchatgpt:latest .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;è¿è¡Œ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -d --name chatgpt -e my_api_key=&#34;æ›¿æ¢æˆAPI&#34;  --network host chuanhuchatgpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;æŸ¥çœ‹æœ¬åœ°è®¿é—®åœ°å€&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker logs chatgpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;éƒ¨ç½²ç›¸å…³&lt;/h2&gt; &#xA;&lt;h3&gt;éƒ¨ç½²åˆ°å…¬ç½‘æœåŠ¡å™¨&lt;/h3&gt; &#xA;&lt;p&gt;å°†æœ€åä¸€å¥ä¿®æ”¹ä¸º&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;demo.queue().launch(server_name=&#34;0.0.0.0&#34;, server_port=7860, share=False) # å¯è‡ªå®šä¹‰ç«¯å£&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ç”¨è´¦å·å¯†ç ä¿æŠ¤é¡µé¢&lt;/h3&gt; &#xA;&lt;p&gt;å°†æœ€åä¸€å¥ä¿®æ”¹ä¸º&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;demo.queue().launch(server_name=&#34;0.0.0.0&#34;, server_port=7860,auth=(&#34;åœ¨è¿™é‡Œå¡«å†™ç”¨æˆ·å&#34;, &#34;åœ¨è¿™é‡Œå¡«å†™å¯†ç &#34;)) # å¯è®¾ç½®ç”¨æˆ·åä¸å¯†ç &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ç–‘éš¾æ‚ç—‡è§£å†³&lt;/h2&gt; &#xA;&lt;h3&gt;No module named &#39;_bz2&#39;&lt;/h3&gt; &#xA;&lt;p&gt;å¤ªç©ºæ€¥å…ˆé”‹ï¼šéƒ¨ç½²åœ¨CentOS7.6,Python3.11.0ä¸Š,æœ€åæŠ¥é”™ModuleNotFoundError: No module named &#39;_bz2&#39;&lt;/p&gt; &#xA;&lt;p&gt;è§£å†³æ–¹æ¡ˆï¼šå®‰è£…pythonå‰å¾—ä¸‹ä¸ªbzipç¼–è¯‘ç¯å¢ƒ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo yum install bzip2-devel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;openai.error.APIConnectionError&lt;/h3&gt; &#xA;&lt;p&gt;æˆ‘æ˜¯ä¸€åªå­¤çŒ« &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/issues/5&#34;&gt;#5&lt;/a&gt;ï¼š&lt;/p&gt; &#xA;&lt;p&gt;å¦‚æœæœ‰äººä¹Ÿå‡ºç°äº†&lt;code&gt;openai.error.APIConnectionError&lt;/code&gt;æç¤ºçš„æŠ¥é”™ï¼Œé‚£å¯èƒ½æ˜¯&lt;code&gt;urllib3&lt;/code&gt;çš„ç‰ˆæœ¬å¯¼è‡´çš„ã€‚&lt;code&gt;urllib3&lt;/code&gt;ç‰ˆæœ¬å¤§äº&lt;code&gt;1.25.11&lt;/code&gt;ï¼Œå°±ä¼šå‡ºç°è¿™ä¸ªé—®é¢˜ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è§£å†³æ–¹æ¡ˆæ˜¯å¸è½½&lt;code&gt;urllib3&lt;/code&gt;ç„¶åé‡è£…è‡³&lt;code&gt;1.25.11&lt;/code&gt;ç‰ˆæœ¬å†é‡æ–°è¿è¡Œä¸€éå°±å¯ä»¥&lt;/p&gt; &#xA;&lt;p&gt;åœ¨ç»ˆç«¯æˆ–å‘½ä»¤æç¤ºç¬¦ä¸­å¸è½½&lt;code&gt;urllib3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall urllib3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;ç„¶åï¼Œæ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨æŒ‡å®šç‰ˆæœ¬å·çš„&lt;code&gt;pip install&lt;/code&gt;å‘½ä»¤æ¥å®‰è£…æ‰€éœ€çš„ç‰ˆæœ¬ï¼š&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install urllib3==1.25.11&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å‚è€ƒè‡ªï¼š &lt;a href=&#34;https://zhuanlan.zhihu.com/p/611080662&#34;&gt;è§£å†³OpenAI API æŒ‚äº†ä»£ç†è¿˜æ˜¯è¿æ¥ä¸ä¸Šçš„é—®é¢˜&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;API è¢«å¢™äº†æ€ä¹ˆåŠ&lt;/h3&gt; &#xA;&lt;p&gt;å»ºè®®æŠŠ&lt;code&gt;openai.com&lt;/code&gt;åŠ å…¥Clashç­‰è½¯ä»¶çš„åˆ†æµè§„åˆ™ä¸­ã€‚&lt;/p&gt; &#xA;&lt;p&gt;è·‘èµ·æ¥ä¹‹åï¼Œè¾“å…¥é—®é¢˜å¥½åƒå°±æ²¡ååº”äº†ï¼Œä¹Ÿæ²¡æŠ¥é”™ &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/issues/25&#34;&gt;#25&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;åœ¨ Python æ–‡ä»¶é‡Œ è®¾å®š API Key ä¹‹åéªŒè¯å¤±è´¥&lt;/h3&gt; &#xA;&lt;p&gt;åœ¨ChuanhuChatbot.pyä¸­è®¾ç½®APIkeyåéªŒè¯å‡ºé”™ï¼Œæç¤ºâ€œå‘ç”Ÿäº†æœªçŸ¥é”™è¯¯Orzâ€ &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/issues/26&#34;&gt;#26&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;é‡è£… gradio&lt;/h3&gt; &#xA;&lt;p&gt;å¾ˆå¤šæ—¶å€™ï¼Œè¿™æ ·å°±å¯ä»¥è§£å†³é—®é¢˜ã€‚&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gradio --upgrade --force_reinstall&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;ç½‘é¡µæç¤ºé”™è¯¯&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Something went wrong&#xA;Expecting value: 1ine 1 column 1 (char o)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;å‡ºç°è¿™ä¸ªé”™è¯¯çš„åŸå› æ˜¯&lt;code&gt;127.0.0.1&lt;/code&gt;è¢«ä»£ç†äº†ï¼Œå¯¼è‡´ç½‘é¡µæ— æ³•å’Œåç«¯é€šä¿¡ã€‚è¯·è®¾ç½®ä»£ç†è½¯ä»¶ï¼Œå°†&lt;code&gt;127.0.0.1&lt;/code&gt;åŠ å…¥ç›´è¿ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;No matching distribution found for openai&amp;gt;=0.27.0&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;openai&lt;/code&gt;è¿™ä¸ªä¾èµ–å·²ç»è¢«ç§»é™¤äº†ã€‚è¯·å°è¯•ä¸‹è½½æœ€æ–°ç‰ˆè„šæœ¬ã€‚&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mckaywrigley/paul-graham-gpt</title>
    <updated>2023-03-07T01:30:18Z</updated>
    <id>tag:github.com,2023-03-07:/mckaywrigley/paul-graham-gpt</id>
    <link href="https://github.com/mckaywrigley/paul-graham-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI search &amp; chat for all of Paul Grahamâ€™s essays.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Paul Graham GPT&lt;/h1&gt; &#xA;&lt;p&gt;AI-powered search and chat for &lt;a href=&#34;https://twitter.com/paulg&#34;&gt;Paul Graham&#39;s&lt;/a&gt; &lt;a href=&#34;http://www.paulgraham.com/articles.html&#34;&gt;essays&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All code &amp;amp; data used is 100% open-source.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;The dataset is a CSV file containing all text &amp;amp; embeddings used.&lt;/p&gt; &#xA;&lt;p&gt;Download it &lt;a href=&#34;https://drive.google.com/file/d/1BxcPw2mn0VYFucc62wlt9H0nQiOu38ki/view?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;I recommend getting familiar with fetching, cleaning, and storing data as outlined in the scraping and embedding scripts below, but feel free to skip those steps and just use the dataset.&lt;/p&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;Paul Graham GPT provides 2 things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A search interface.&lt;/li&gt; &#xA; &lt;li&gt;A chat interface.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Search&lt;/h3&gt; &#xA;&lt;p&gt;Search was created with &lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings&#34;&gt;OpenAI Embeddings&lt;/a&gt; (&lt;code&gt;text-embedding-ada-002&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;First, we loop over the essays and generate embeddings for each chunk of text.&lt;/p&gt; &#xA;&lt;p&gt;Then in the app we take the user&#39;s search query, generate an embedding, and use the result to find the most similar passages from the book.&lt;/p&gt; &#xA;&lt;p&gt;The comparison is done using cosine similarity across our database of vectors.&lt;/p&gt; &#xA;&lt;p&gt;Our database is a Postgres database with the &lt;a href=&#34;https://github.com/pgvector/pgvector&#34;&gt;pgvector&lt;/a&gt; extension hosted on &lt;a href=&#34;https://supabase.com/&#34;&gt;Supabase&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Results are ranked by similarity score and returned to the user.&lt;/p&gt; &#xA;&lt;h3&gt;Chat&lt;/h3&gt; &#xA;&lt;p&gt;Chat builds on top of search. It uses search results to create a prompt that is fed into GPT-3.5-turbo.&lt;/p&gt; &#xA;&lt;p&gt;This allows for a chat-like experience where the user can ask questions about the book and get answers.&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a quick overview of how to run it locally.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set up OpenAI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You&#39;ll need an OpenAI API key to generate embeddings.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Set up Supabase and create a database&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: You don&#39;t have to use Supabase. Use whatever method you prefer to store your data. But I like Supabase and think it&#39;s easy to use.&lt;/p&gt; &#xA;&lt;p&gt;There is a schema.sql file in the root of the repo that you can use to set up the database.&lt;/p&gt; &#xA;&lt;p&gt;Run that in the SQL editor in Supabase as directed.&lt;/p&gt; &#xA;&lt;p&gt;I recommend turning on Row Level Security and setting up a service role to use with the app.&lt;/p&gt; &#xA;&lt;h3&gt;Repo Setup&lt;/h3&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Clone repo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/mckaywrigley/paul-graham-gpt.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm i&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Set up environment variables&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Create a .env.local file in the root of the repo with the following variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=&#xA;&#xA;NEXT_PUBLIC_SUPABASE_URL=&#xA;SUPABASE_SERVICE_ROLE_KEY=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Run scraping script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run scrape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This scrapes all of the essays from Paul Graham&#39;s website and saves them to a json file.&lt;/p&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Run embedding script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run embed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This reads the json file, generates embeddings for each chunk of text, and saves the results to your database.&lt;/p&gt; &#xA;&lt;p&gt;There is a 200ms delay between each request to avoid rate limiting.&lt;/p&gt; &#xA;&lt;p&gt;This process will take 20-30 minutes.&lt;/p&gt; &#xA;&lt;h3&gt;App&lt;/h3&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;Run app&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://twitter.com/paulg&#34;&gt;Paul Graham&lt;/a&gt; for his writing.&lt;/p&gt; &#xA;&lt;p&gt;I highly recommend you read his essays.&lt;/p&gt; &#xA;&lt;p&gt;3 years ago they convinced me to learn to code, and it changed my life.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, feel free to reach out to me on &lt;a href=&#34;https://twitter.com/mckaywrigley&#34;&gt;Twitter&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;p&gt;I sacrificed composability for simplicity in the app.&lt;/p&gt; &#xA;&lt;p&gt;Yes, you can make things more modular and reusable.&lt;/p&gt; &#xA;&lt;p&gt;But I kept pretty much everything in the homepage component for the sake of simplicity.&lt;/p&gt;</summary>
  </entry>
</feed>