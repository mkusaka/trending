<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-25T01:31:21Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>chat2db/Chat2DB</title>
    <updated>2023-06-25T01:31:21Z</updated>
    <id>tag:github.com,2023-06-25:/chat2db/Chat2DB</id>
    <link href="https://github.com/chat2db/Chat2DB" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üî• üî• üî• An intelligent and versatile general-purpose SQL client and reporting tool for databases which integrates ChatGPT capabilities.(Êô∫ËÉΩÁöÑÈÄöÁî®Êï∞ÊçÆÂ∫ìSQLÂÆ¢Êà∑Á´ØÂíåÊä•Ë°®Â∑•ÂÖ∑)&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;Chat2DB&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;An intelligent and versatile general-purpose SQL client and reporting tool for databases which integrates ChatGPT capabilities.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/alibaba/fastjson2?color=4D7A97&amp;amp;logo=apache&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/alibaba/ali-dbhub/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/alibaba/ali-dbhub&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/alibaba/ali-dbhub/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/alibaba/ali-dbhub&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/alibaba/ali-dbhub/fork&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/alibaba/ali-dbhub&#34; alt=&#34;GitHub Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/alibaba/ali-dbhub/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/alibaba/ali-dbhub&#34; alt=&#34;GitHub Contributors&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt;&lt;b&gt;Share Chat2DB Repository &lt;/b&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://twitter.com/intent/tweet?text=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.&amp;amp;url=https://github.com/alibaba/Chat2DB&amp;amp;hashtags=ChatGPT,AGI,SQL%20Client,Reporting%20tool&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/_Chat2DB?label=Share Repo on Twitter&amp;amp;style=social&#34; alt=&#34;&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://t.me/share/url?text=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.&amp;amp;url=https://github.com/alibaba/Chat2DB&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?label=Telegram&amp;amp;logo=Telegram&amp;amp;style=social&amp;amp;url=https://github.com/alibaba/Chat2DB&#34; alt=&#34;Share on Telegram&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://api.whatsapp.com/send?text=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.%20https://github.com/alibaba/Chat2DB&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?label=whatsapp&amp;amp;logo=whatsapp&amp;amp;style=social&amp;amp;url=https://github.com/alibaba/Chat2DB&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.reddit.com/submit?url=https://github.com/alibaba/Chat2DB&amp;amp;title=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.&#34; target=&#34;blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?label=Reddit&amp;amp;logo=Reddit&amp;amp;style=social&amp;amp;url=https://github.com/alibaba/Chat2DB&#34; alt=&#34;Share on Reddit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;mailto:?subject=Check%20this%20GitHub%20repository%20out.&amp;amp;body=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.%3A%0Ahttps://github.com/alibaba/Chat2DB&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url?label=Gmail&amp;amp;logo=Gmail&amp;amp;style=social&amp;amp;url=https://github.com/alibaba/Chat2DB&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;License Notation&lt;/strong&gt;: Chat2DB is constructed and distributed for personal and non-commercial use only. For commercial use of this project, please contact corresponding authors.&lt;/p&gt; &#xA; &lt;p&gt;LanguagesÔºö English | &lt;a href=&#34;https://raw.githubusercontent.com/chat2db/Chat2DB/main/README_CN.md&#34;&gt;‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üìñ Introduction&lt;/h2&gt; &#xA;&lt;p&gt;‚ÄÉ ‚ÄÉChat2DB is a multi-database client tool that is open-source and free from Alibaba. It supports local installation on Windows and Mac, as well as server-side deployment and web page access. Compared to traditional database client software such as Navicat and DBeaver, Chat2DB integrates AIGC&#39;s capabilities and is able to convert natural language into SQL. It can also convert SQL into natural language and provide optimization suggestions for SQL to greatly enhance the efficiency of developers. It is a tool for database developers in the AI era, and even non-SQL business operators in the future can use it to quickly query business data and generate reports.&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üåà AI intelligent assistant, supporting natural language to SQL conversion, SQL to natural language conversion, and SQL optimization suggestions&lt;/li&gt; &#xA; &lt;li&gt;üë≠ Support team collaboration, developers do not need to know the online database password, solving the problem of enterprise database account security&lt;/li&gt; &#xA; &lt;li&gt;‚öôÔ∏è Powerful data management capability, supporting management of data tables, views, stored procedures, functions, triggers, indexes, sequences, users, roles, authorizations, etc.&lt;/li&gt; &#xA; &lt;li&gt;üîå Powerful extension capability, currently supporting MySQL, PostgreSQL, Oracle, SQLServer, ClickHouse, OceanBase, H2, SQLite, etc., and more databases will be supported in the future&lt;/li&gt; &#xA; &lt;li&gt;üõ° Front-end development using Electron, providing a solution that integrates Windows, Mac, Linux clients, and web versions&lt;/li&gt; &#xA; &lt;li&gt;üéÅ Support environment isolation, online, and daily data permission separation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚è¨ Download and Install&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Windows&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss-chat2db.alibaba.com/release/1.0.11/Chat2DB%20Setup%201.0.11.exe&#34;&gt;https://oss-chat2db.alibaba.com/release/1.0.11/Chat2DB%20Setup%201.0.11.exe&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MacOS ARM64&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss-chat2db.alibaba.com/release/1.0.11/Chat2DB-1.0.11-arm64.dmg&#34;&gt;https://oss-chat2db.alibaba.com/release/1.0.11/Chat2DB-1.0.11-arm64.dmg&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MacOS X64&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss-chat2db.alibaba.com/release/1.0.11/Chat2DB-1.0.11.dmg&#34;&gt;https://oss-chat2db.alibaba.com/release/1.0.11/Chat2DB-1.0.11.dmg&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JarÂåÖ&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://oss-chat2db.alibaba.com/release/1.0.11/ali-dbhub-server-start.jar&#34;&gt;https://oss-chat2db.alibaba.com/release/1.0.11/ali-dbhub-server-start.jar&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üöÄ Supported databases&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Databases&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mysql&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;H2&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Oracle&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PostgreSQL&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SQLServer&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SQLLite&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MariaDB&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ClickHouse&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DM&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Presto&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DB2&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OceanBase&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Redis&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hive&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;KingBase&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MongoDB&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hbase&lt;/td&gt; &#xA;   &lt;td&gt;Planning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Elasticsearch&lt;/td&gt; &#xA;   &lt;td&gt;Planning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openGauss&lt;/td&gt; &#xA;   &lt;td&gt;Planning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TiDB&lt;/td&gt; &#xA;   &lt;td&gt;Planning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InfluxDB&lt;/td&gt; &#xA;   &lt;td&gt;Planning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üå∞ Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Create data source&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a&gt;&lt;img src=&#34;https://gw.alicdn.com/imgextra/i3/O1CN01PlpLYy1hIq5aMugpg_!!6000000004255-0-tps-3446-1750.jpg&#34; width=&#34;100%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Data source management&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a&gt;&lt;img src=&#34;https://gw.alicdn.com/imgextra/i2/O1CN01DpzZJL1T7w2Xv9VMl_!!6000000002336-0-tps-3410-1662.jpg&#34; width=&#34;100%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;SQL console&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a&gt;&lt;img src=&#34;https://gw.alicdn.com/imgextra/i2/O1CN01aidnkx1Oo0LJ1Pdty_!!6000000001751-0-tps-3440-1736.jpg&#34; width=&#34;100%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AI intelligent assistant&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a&gt;&lt;img src=&#34;https://gw.alicdn.com/imgextra/i4/O1CN01iaSXot1W6VeaDFbK2_!!6000000002739-0-tps-3430-1740.jpg&#34; width=&#34;100%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üî• AI Configuration&lt;/h2&gt; &#xA;&lt;h3&gt;CONFIGURE OPENAI&lt;/h3&gt; &#xA;&lt;p&gt;Option 1 (recommended): To use the ChatSql function of OPENAI, two conditions must be met:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need an OPENAI_API_KEY.&lt;/li&gt; &#xA; &lt;li&gt;The client&#39;s network can connect to the OPENAI website, and for users in China, a VPN is required. Note: If the local VPN is not fully effective, the network connectivity can be ensured by setting the network proxy HOST and PORT in the client.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a&gt;&lt;img src=&#34;https://img.alicdn.com/imgextra/i2/O1CN01anrJMI1FEtSBbmTau_!!6000000000456-0-tps-1594-964.jpg&#34; width=&#34;60%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Option 2 (recommended): We provide a unified proxy service.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No OPENAI_API_KEY is required.&lt;/li&gt; &#xA; &lt;li&gt;No proxy or VPN is required, as long as the network is connected.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To facilitate users&#39; quick use of AI capabilities, you can scan the QR code below to follow our WeChat public account and apply for our custom API_KEY.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a&gt;&lt;img src=&#34;https://oss-chat2db.alibaba.com/static/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg&#34; width=&#34;40%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After the application is completed, refer to the following figure for configuration and usage. Config Api Host as &lt;a href=&#34;http://test.sqlgpt.cn/gateway/api/&#34;&gt;http://test.sqlgpt.cn/gateway/api/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a&gt;&lt;img src=&#34;https://img.alicdn.com/imgextra/i2/O1CN01xNobD21mo3B1ILrs2_!!6000000005000-0-tps-592-515.jpg&#34; width=&#34;60%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;CONFIGURE CUSTOM AI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Customized AI can be any LLM that you deployed, such as ChatGLM„ÄÅChatGPT„ÄÅERNIE Bot„ÄÅTongyi Qianwen, and so on. However, the customized interface need to conform to the protocol definition. Otherwise, secondary development may be required. Two DEMOs are provided in the code, the configuration is as shown below. In specific use, you can refer to the DEMO interface to write a custom interface, or directly perform secondary development in the DEMO interface.&lt;/li&gt; &#xA; &lt;li&gt;DEMO for configuring customized stream output interface. &lt;a&gt;&lt;img src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01xMqnRH1DlkdSekvSF_!!6000000000257-0-tps-591-508.jpg&#34; width=&#34;60%&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DEMO for configuring customized non-stream output interface. &lt;a&gt;&lt;img src=&#34;https://img.alicdn.com/imgextra/i1/O1CN01JqmbGo1fW0GAQhRu4_!!6000000004013-0-tps-587-489.jpg&#34; width=&#34;60%&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üì¶ Docker installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull chat2db/chat2db:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üéØ Operating Environment&lt;/h2&gt; &#xA;&lt;p&gt;Note: If local debugging is required&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Java runtime Open JDK 17&lt;/li&gt; &#xA; &lt;li&gt;JRE reference packaging and deployment method of jre.&lt;/li&gt; &#xA; &lt;li&gt;Node runtime environment Node16 Node.js.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíª Local Debugging&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;git clone to local&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone git@github.com:alibaba/Chat2DB.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Front-End installation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd Chat2DB/ali-dbhub-client&#xA;$ npm install # Mounting front-end dependency&#xA;$ npm run build:prod # Package js to the source directory on the back end&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Backend debug&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd ../ali-dbhub-server&#xA;$ mvn clean install # maven 3.8 or later needs to be installed&#xA;$ cd ali-dbhub-server/ali-dbhub-server-start/target/&#xA;$ java -jar -Dchatgpt.apiKey=xxxxx ali-dbhub-server-start.jar  # To launch the chat application, you need to enter the ChatGPT key for the chatgpt.apiKey. Without entering it, you won&#39;t be able to use the AIGC function.&#xA;$ # open http://127.0.0.1:10821 to start debug Note: Front-end installation is required&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Front-End debug&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd Chat2DB/ali-dbhub-client&#xA;$ npm install &#xA;$ npm run start &#xA;$ # open http://127.0.0.1:10821  to start Front-End debug&#xA;$ # Note Front-end page completely depends on the service, so front-end students need to debug the back-end project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;But front debugging need mapping of resources, you can download &lt;a href=&#34;https://chrome.google.com/webstore/detail/idkjhjggpffolpidfkikidcokdkdaogg&#34;&gt;XSwitch&lt;/a&gt;, add the following configuration file&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;proxy&#34;: [&#xA;    [&#xA;      &#34;http://127.0.0.1:10821/(.*).js$&#34;,&#xA;      &#34;http://127.0.0.1:8001/$1.js&#34;,&#xA;    ],&#xA;    [&#xA;      &#34;http://127.0.0.1:10821/(.*).css$&#34;,&#xA;      &#34;http://127.0.0.1:8001/$1.css&#34;,&#xA;    ],&#xA;    [&#xA;      &#34;http://127.0.0.1:10821/static/front/(.*)&#34;,&#xA;      &#34;http://127.0.0.1:8001/$1&#34;,&#xA;    ],&#xA;    [&#xA;      &#34;http://127.0.0.1:10821/static/(.*)$&#34;,&#xA;      &#34;http://127.0.0.1:8001/static/$1&#34;,&#xA;    ],&#xA;  ],&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìë Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chat2db.opensource.alibaba.com&#34;&gt;Official website document&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/alibaba/ali-dbhub/issues&#34;&gt;Issue &lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Stargazers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/alibaba/Chat2DB/stargazers&#34;&gt;&lt;img src=&#34;https://reporoster.com/stars/alibaba/Chat2DB&#34; alt=&#34;Stargazers repo roster for @alibaba/Chat2DB&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Forkers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/alibaba/Chat2DB/network/members&#34;&gt;&lt;img src=&#34;https://reporoster.com/forks/alibaba/Chat2DB&#34; alt=&#34;Forkers repo roster for @alibaba/Chat2DB&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚òéÔ∏è Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;Please star and fork on GitHub before joining the group. Follow our WeChat public account &lt;a&gt;&lt;img src=&#34;https://oss-chat2db.alibaba.com/static/%E5%85%AC%E4%BC%97%E5%8F%B7.jpg&#34; width=&#34;40%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chat2db/Chat2DB/main/document/qrcode/weixinqun1.png&#34; width=&#34;30%&#34;&gt;&lt;/a&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chat2db/Chat2DB/main/document/qrcode/weixinqun2.png&#34; width=&#34;30%&#34;&gt;&lt;/a&gt; &lt;a&gt;&lt;img src=&#34;https://raw.githubusercontent.com/chat2db/Chat2DB/main/document/qrcode/weixinqun3.png&#34; width=&#34;30%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ding TalkÔºö9135032392&lt;/p&gt; &#xA;&lt;p&gt;QQ:863576619&lt;/p&gt; &#xA;&lt;h2&gt;‚ù§Ô∏è Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to all the students who contributed to Chat2DB~&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/alibaba/ali-dbhub/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=alibaba/ali-dbhub&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;a href=&#34;https://star-history.com/#alibaba/chat2db&amp;amp;Date&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=alibaba/chat2db&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=alibaba/chat2db&amp;amp;type=Date&#34;&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=alibaba/chat2db&amp;amp;type=Date&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>Stability-AI/generative-models</title>
    <updated>2023-06-25T01:31:21Z</updated>
    <id>tag:github.com,2023-06-25:/Stability-AI/generative-models</id>
    <link href="https://github.com/Stability-AI/generative-models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative Models by Stability AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Generative Models by Stability AI&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/generative-models/main/assets/000.jpg&#34; alt=&#34;sample1&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;June 22, 2023&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We are releasing two new diffusion models for research purposes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;SD-XL 0.9-base&lt;/code&gt;: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The base model uses &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;OpenCLIP-ViT/G&lt;/a&gt; and &lt;a href=&#34;https://github.com/openai/CLIP/tree/main&#34;&gt;CLIP-ViT/L&lt;/a&gt; for text encoding whereas the refiner model only uses the OpenCLIP model.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SD-XL 0.9-refiner&lt;/code&gt;: The refiner has been trained to denoise small noise levels of high quality data and as such is not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you would like to access these models for your research, please apply using one of the following links: &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9&#34;&gt;SDXL-0.9-Base model&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9&#34;&gt;SDXL-0.9-Refiner&lt;/a&gt;. This means that you can apply for any of the two links - and if you are granted - you can access both. Please log in to your HuggingFace Account with your organization email to request access. &lt;strong&gt;We plan to do a full release soon (July).&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;The codebase&lt;/h2&gt; &#xA;&lt;h3&gt;General Philosophy&lt;/h3&gt; &#xA;&lt;p&gt;Modularity is king. This repo implements a config-driven approach where we build and combine submodules by calling &lt;code&gt;instantiate_from_config()&lt;/code&gt; on objects defined in yaml configs. See &lt;code&gt;configs/&lt;/code&gt; for many examples.&lt;/p&gt; &#xA;&lt;h3&gt;Changelog from the old &lt;code&gt;ldm&lt;/code&gt; codebase&lt;/h3&gt; &#xA;&lt;p&gt;For training, we use &lt;a href=&#34;https://www.pytorchlightning.ai/index.html&#34;&gt;pytorch-lightning&lt;/a&gt;, but it should be easy to use other training wrappers around the base modules. The core diffusion model class (formerly &lt;code&gt;LatentDiffusion&lt;/code&gt;, now &lt;code&gt;DiffusionEngine&lt;/code&gt;) has been cleaned up:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial conditionings, and all combinations thereof) in a single class: &lt;code&gt;GeneralConditioner&lt;/code&gt;, see &lt;code&gt;sgm/modules/encoders/modules.py&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We separate guiders (such as classifier-free guidance, see &lt;code&gt;sgm/modules/diffusionmodules/guiders.py&lt;/code&gt;) from the samplers (&lt;code&gt;sgm/modules/diffusionmodules/sampling.py&lt;/code&gt;), and the samplers are independent of the model.&lt;/li&gt; &#xA; &lt;li&gt;We adopt the &lt;a href=&#34;https://arxiv.org/abs/2206.00364&#34;&gt;&#34;denoiser framework&#34;&lt;/a&gt; for both training and inference (most notable change is probably now the option to train continuous time models): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Discrete times models (denoisers) are simply a special case of continuous time models (denoisers); see &lt;code&gt;sgm/modules/diffusionmodules/denoiser.py&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The following features are now independent: weighting of the diffusion loss function (&lt;code&gt;sgm/modules/diffusionmodules/denoiser_weighting.py&lt;/code&gt;), preconditioning of the network (&lt;code&gt;sgm/modules/diffusionmodules/denoiser_scaling.py&lt;/code&gt;), and sampling of noise levels during training (&lt;code&gt;sgm/modules/diffusionmodules/sigma_sampling.py&lt;/code&gt;).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Autoencoding models have also been cleaned up.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a name=&#34;installation&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;1. Clone the repo&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:Stability-AI/generative-models.git&#xA;cd generative-models&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Setting up the virtualenv&lt;/h4&gt; &#xA;&lt;p&gt;This is assuming you have navigated to the &lt;code&gt;generative-models&lt;/code&gt; root after cloning it.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; This is tested under &lt;code&gt;python3.8&lt;/code&gt; and &lt;code&gt;python3.10&lt;/code&gt;. For other python versions, you might encounter version conflicts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyTorch 1.13&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# install required packages from pypi&#xA;python3 -m venv .pt1&#xA;source .pt1/bin/activate&#xA;pip3 install wheel&#xA;pip3 install -r requirements_pt13.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;PyTorch 2.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# install required packages from pypi&#xA;python3 -m venv .pt2&#xA;source .pt2/bin/activate&#xA;pip3 install wheel&#xA;pip3 install -r requirements_pt2.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference:&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://streamlit.io/&#34;&gt;streamlit&lt;/a&gt; demo for text-to-image and image-to-image sampling in &lt;code&gt;scripts/demo/sampling.py&lt;/code&gt;. The following models are currently supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9&#34;&gt;SD-XL 0.9-base&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9&#34;&gt;SD-XL 0.9-refiner&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors&#34;&gt;SD 2.1-512&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors&#34;&gt;SD 2.1-768&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Weights for SDXL&lt;/strong&gt;: If you would like to access these models for your research, please apply using one of the following links: &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9&#34;&gt;SDXL-0.9-Base model&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9&#34;&gt;SDXL-0.9-Refiner&lt;/a&gt;. This means that you can apply for any of the two links - and if you are granted - you can access both. Please log in to your HuggingFace Account with your organization email to request access.&lt;/p&gt; &#xA;&lt;p&gt;After obtaining the weights, place them into &lt;code&gt;checkpoints/&lt;/code&gt;. Next, start the demo using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamlit run scripts/demo/sampling.py --server.port &amp;lt;your_port&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Invisible Watermark Detection&lt;/h3&gt; &#xA;&lt;p&gt;Images generated with our code use the &lt;a href=&#34;https://github.com/ShieldMnt/invisible-watermark/&#34;&gt;invisible-watermark&lt;/a&gt; library to embed an invisible watermark into the model output. We also provide a script to easily detect that watermark. Please note that this watermark is not the same as in previous Stable Diffusion 1.x/2.x versions.&lt;/p&gt; &#xA;&lt;p&gt;To run the script you need to either have a working installation as above or try an &lt;em&gt;experimental&lt;/em&gt; import using only a minimal amount of packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m venv .detect&#xA;source .detect/bin/activate&#xA;&#xA;pip install &#34;numpy&amp;gt;=1.17&#34; &#34;PyWavelets&amp;gt;=1.1.1&#34; &#34;opencv-python&amp;gt;=4.1.0.25&#34;&#xA;pip install --no-deps invisible-watermark&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the script you need to have a working installation as above. The script is then useable in the following ways (don&#39;t forget to activate your virtual environment beforehand, e.g. &lt;code&gt;source .pt1/bin/activate&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# test a single file&#xA;python scripts/demo/detect.py &amp;lt;your filename here&amp;gt;&#xA;# test multiple files at once&#xA;python scripts/demo/detect.py &amp;lt;filename 1&amp;gt; &amp;lt;filename 2&amp;gt; ... &amp;lt;filename n&amp;gt;&#xA;# test all files in a specific folder&#xA;python scripts/demo/detect.py &amp;lt;your folder name here&amp;gt;/*&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training:&lt;/h2&gt; &#xA;&lt;p&gt;We are providing example training configs in &lt;code&gt;configs/example_training&lt;/code&gt;. To launch a training, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main.py --base configs/&amp;lt;config1.yaml&amp;gt; configs/&amp;lt;config2.yaml&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where configs are merged from left to right (later configs overwrite the same values). This can be used to combine model, training and data configs. However, all of them can also be defined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py --base configs/example_training/toy/mnist_cond.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE 1:&lt;/strong&gt; Using the non-toy-dataset configs &lt;code&gt;configs/example_training/imagenet-f8_cond.yaml&lt;/code&gt;, &lt;code&gt;configs/example_training/txt2img-clipl.yaml&lt;/code&gt; and &lt;code&gt;configs/example_training/txt2img-clipl-legacy-ucg-training.yaml&lt;/code&gt; for training will require edits depdending on the used dataset (which is expected to stored in tar-file in the &lt;a href=&#34;https://github.com/webdataset/webdataset&#34;&gt;webdataset-format&lt;/a&gt;). To find the parts which have to be adapted, search for comments containing &lt;code&gt;USER:&lt;/code&gt; in the respective config.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE 2:&lt;/strong&gt; This repository supports both &lt;code&gt;pytorch1.13&lt;/code&gt; and &lt;code&gt;pytorch2&lt;/code&gt;for training generative models. However for autoencoder training as e.g. in &lt;code&gt;configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml&lt;/code&gt;, only &lt;code&gt;pytorch1.13&lt;/code&gt; is supported.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE 3:&lt;/strong&gt; Training latent generative models (as e.g. in &lt;code&gt;configs/example_training/imagenet-f8_cond.yaml&lt;/code&gt;) requires retrieving the checkpoint from &lt;a href=&#34;https://huggingface.co/stabilityai/sdxl-vae/tree/main&#34;&gt;Hugging Face&lt;/a&gt; and replacing the &lt;code&gt;CKPT_PATH&lt;/code&gt; placeholder in &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/generative-models/main/configs/example_training/imagenet-f8_cond.yaml#81&#34;&gt;this line&lt;/a&gt;. The same is to be done for the provided text-to-image configs.&lt;/p&gt; &#xA;&lt;h3&gt;Building New Diffusion Models&lt;/h3&gt; &#xA;&lt;h4&gt;Conditioner&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;code&gt;GeneralConditioner&lt;/code&gt; is configured through the &lt;code&gt;conditioner_config&lt;/code&gt;. Its only attribute is &lt;code&gt;emb_models&lt;/code&gt;, a list of different embedders (all inherited from &lt;code&gt;AbstractEmbModel&lt;/code&gt;) that are used to condition the generative model. All embedders should define whether or not they are trainable (&lt;code&gt;is_trainable&lt;/code&gt;, default &lt;code&gt;False&lt;/code&gt;), a classifier-free guidance dropout rate is used (&lt;code&gt;ucg_rate&lt;/code&gt;, default &lt;code&gt;0&lt;/code&gt;), and an input key (&lt;code&gt;input_key&lt;/code&gt;), for example, &lt;code&gt;txt&lt;/code&gt; for text-conditioning or &lt;code&gt;cls&lt;/code&gt; for class-conditioning. When computing conditionings, the embedder will get &lt;code&gt;batch[input_key]&lt;/code&gt; as input. We currently support two to four dimensional conditionings and conditionings of different embedders are concatenated appropriately. Note that the order of the embedders in the &lt;code&gt;conditioner_config&lt;/code&gt; is important.&lt;/p&gt; &#xA;&lt;h4&gt;Network&lt;/h4&gt; &#xA;&lt;p&gt;The neural network is set through the &lt;code&gt;network_config&lt;/code&gt;. This used to be called &lt;code&gt;unet_config&lt;/code&gt;, which is not general enough as we plan to experiment with transformer-based diffusion backbones.&lt;/p&gt; &#xA;&lt;h4&gt;Loss&lt;/h4&gt; &#xA;&lt;p&gt;The loss is configured through &lt;code&gt;loss_config&lt;/code&gt;. For standard diffusion model training, you will have to set &lt;code&gt;sigma_sampler_config&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Sampler config&lt;/h4&gt; &#xA;&lt;p&gt;As discussed above, the sampler is independent of the model. In the &lt;code&gt;sampler_config&lt;/code&gt;, we set the type of numerical solver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free guidance.&lt;/p&gt; &#xA;&lt;h3&gt;Dataset Handling&lt;/h3&gt; &#xA;&lt;p&gt;For large scale training we recommend using the datapipelines from our &lt;a href=&#34;https://github.com/Stability-AI/datapipelines&#34;&gt;datapipelines&lt;/a&gt; project. The project is contained in the requirement and automatically included when following the steps from the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/generative-models/main/#installation&#34;&gt;Installation section&lt;/a&gt;. Small map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of data keys/values, e.g.,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;example = {&#34;jpg&#34;: x,  # this is a tensor -1...1 chw&#xA;           &#34;txt&#34;: &#34;a beautiful image&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where we expect images in -1...1, channel-first format.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>a16z-infra/ai-getting-started</title>
    <updated>2023-06-25T01:31:21Z</updated>
    <id>tag:github.com,2023-06-25:/a16z-infra/ai-getting-started</id>
    <link href="https://github.com/a16z-infra/ai-getting-started" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Javascript AI getting started stack for weekend projects, including image/text models, vector stores, auth, and deployment configs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Getting Started&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ai-getting-started.com/&#34;&gt;Live Demo (deployed on fly.io)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;1305&#34; alt=&#34;Screen Shot 2023-06-20 at 1 30 56 PM&#34; src=&#34;https://github.com/a16z-infra/ai-getting-started/assets/3489963/bcc762d2-68f5-4c4e-8c49-14602bee4995&#34;&gt; &#xA;&lt;h2&gt;Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Auth: &lt;a href=&#34;https://clerk.com/&#34;&gt;Clerk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;App logic: &lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VectorDB: &lt;a href=&#34;https://www.pinecone.io/&#34;&gt;Pinecone&lt;/a&gt; / &lt;a href=&#34;https://supabase.com/docs/guides/database/extensions/pgvector&#34;&gt;Supabase pgvector&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LLM Orchestration: &lt;a href=&#34;https://js.langchain.com/docs/&#34;&gt;Langchain.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Image Model: &lt;a href=&#34;https://replicate.com/&#34;&gt;Replicate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Text Model: &lt;a href=&#34;https://platform.openai.com/docs/models&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Text streaming: &lt;a href=&#34;https://github.com/vercel-labs/ai&#34;&gt;ai sdk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Deployment: &lt;a href=&#34;https://fly.io/&#34;&gt;Fly&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöÄ &lt;a href=&#34;https://raw.githubusercontent.com/a16z-infra/ai-getting-started/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üíª &lt;a href=&#34;https://raw.githubusercontent.com/a16z-infra/ai-getting-started/main/#how-to-contribute-to-this-repo&#34;&gt;Contribute to this repo&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;The simplest way to try out this stack is to test it out locally and traverse through code files to understand how each component work. Here are the steps to get started.&lt;/p&gt; &#xA;&lt;h3&gt;1. Fork and Clone repo&lt;/h3&gt; &#xA;&lt;p&gt;Fork the repo to your Github account, then run the following command to clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:[YOUR_GITHUB_ACCOUNT_NAME]/ai-getting-started.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Install dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd ai-getting-started&#xA;npm install &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Fill out secrets&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cp .env.local.example .env.local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;a. &lt;strong&gt;Clerk Secrets&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://dashboard.clerk.com/&#34;&gt;https://dashboard.clerk.com/&lt;/a&gt; -&amp;gt; &#34;Add Application&#34; -&amp;gt; Fill in Application name/select how your users should sign in -&amp;gt; Create Application Now you should see both &lt;code&gt;NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY&lt;/code&gt; and &lt;code&gt;CLERK_SECRET_KEY&lt;/code&gt; on the screen &lt;img width=&#34;1011&#34; alt=&#34;clerk&#34; src=&#34;https://github.com/a16z-infra/ai-getting-started/assets/3489963/6ce72263-4e83-406d-838e-08a95ea79023&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;b. &lt;strong&gt;OpenAI API key&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;https://platform.openai.com/account/api-keys&lt;/a&gt; to get your OpenAI API key&lt;/p&gt; &#xA;&lt;p&gt;c. &lt;strong&gt;Replicate API key&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://replicate.com/account/api-tokens&#34;&gt;https://replicate.com/account/api-tokens&lt;/a&gt; to get your Replicate API key&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;em&gt;NOTE:&lt;/em&gt;&lt;/strong&gt; By default, this template uses Pinecone as vector store, but you can turn on Supabase pgvector easily. This means you only need to fill out either Pinecone API key &lt;em&gt;or&lt;/em&gt; Supabase API key.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;d. &lt;strong&gt;Pinecone API key&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a Pinecone index by visiting &lt;a href=&#34;https://app.pinecone.io/&#34;&gt;https://app.pinecone.io/&lt;/a&gt; and click on &#34;Create Index&#34;&lt;/li&gt; &#xA; &lt;li&gt;Give it an index name (this will be the environment variable &lt;code&gt;PINECONE_INDEX&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Fill in Dimension as &lt;code&gt;1536&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Once the index is successfully created, click on &#34;API Keys&#34; on the left side nav and create an API key: copy &#34;Environment&#34; value to &lt;code&gt;PINECONE_ENVIRONMENT&lt;/code&gt; variable, and &#34;Value&#34; to &lt;code&gt;PINECONE_API_KEY&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;e. &lt;strong&gt;Supabase API key&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a Supabase instance &lt;a href=&#34;https://supabase.com/dashboard/projects&#34;&gt;here&lt;/a&gt;; then go to Project Settings -&amp;gt; API&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SUPABASE_URL&lt;/code&gt; is the URL value under &#34;Project URL&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SUPABASE_PRIVATE_KEY&lt;/code&gt; is the key starts with &lt;code&gt;ey&lt;/code&gt; under Project API Keys&lt;/li&gt; &#xA; &lt;li&gt;Now, you should enable pgvector on Supabase and create a schema. You can do this easily by clicking on &#34;SQL editor&#34; on the left hand side on supabase UI and then clicking on &#34;+New Query&#34;. Copy paste &lt;a href=&#34;https://github.com/a16z-infra/ai-getting-started/raw/main/pgvector.sql&#34;&gt;this code snippet&lt;/a&gt; in the SQL editor and click &#34;Run&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;4. Generate embeddings&lt;/h3&gt; &#xA;&lt;p&gt;There are a few markdown files under &lt;code&gt;/blogs&lt;/code&gt; directory as examples so you can do Q&amp;amp;A on them. To generate embeddings and store them in the vector database for future queries, you can run the following command:&lt;/p&gt; &#xA;&lt;h4&gt;If using Pinecone&lt;/h4&gt; &#xA;&lt;p&gt;Run the following command to generate embeddings and store them in Pinecone:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run generate-embeddings-pinecone&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;If using Supabase pgvector&lt;/h4&gt; &#xA;&lt;p&gt;In &lt;code&gt;QAModel.tsx&lt;/code&gt;, replace &lt;code&gt;/api/qa-pinecone&lt;/code&gt; with &lt;code&gt;/api/qa-pg-vector&lt;/code&gt;. Then run the following command to generate embeddings and store them in Supabase pgvector:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run generate-embeddings-supabase&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5. Run app locally&lt;/h3&gt; &#xA;&lt;p&gt;Now you are ready to test out the app locally! To do this, simply run &lt;code&gt;npm run dev&lt;/code&gt; under the project root.&lt;/p&gt; &#xA;&lt;h3&gt;6. Deploy the app&lt;/h3&gt; &#xA;&lt;h4&gt;Deploy to fly.io&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Register an account on fly.io and then &lt;a href=&#34;https://fly.io/docs/hands-on/install-flyctl/&#34;&gt;install flyctl&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;fly launch&lt;/code&gt; under project root -- this will generate a &lt;code&gt;fly.toml&lt;/code&gt; that includes all the configurations you will need&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;fly deploy -ha=false&lt;/code&gt; to deploy the app -- the -ha flag makes sure fly only spins up one instance, which is included in the free plan. You also want to run &lt;code&gt;fly scale memory 512&lt;/code&gt; to scale up the fly vm memory for this app.&lt;/li&gt; &#xA; &lt;li&gt;For any other non-localhost environment, the existing Clerk development instance should continue to work. You can upload the secrets to Fly by running &lt;code&gt;cat .env.local | fly secrets import&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you are ready to deploy to production, you should create a prod environment under the &lt;a href=&#34;https://dashboard.clerk.com/&#34;&gt;current Clerk instance&lt;/a&gt;. For more details on deploying a production app with Clerk, check out their documentation &lt;a href=&#34;https://clerk.com/docs/deployments/overview&#34;&gt;here&lt;/a&gt;. &lt;strong&gt;Note that you will likely need to manage your own domain and do domain verification as part of the process.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a new file &lt;code&gt;.env.prod&lt;/code&gt; locally and fill in all the production-environment secrets. Remember to update &lt;code&gt;NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY&lt;/code&gt; and &lt;code&gt;CLERK_SECRET_KEY&lt;/code&gt; by copying secrets from Clerk&#39;s production instance -&lt;code&gt;cat .env.prod | fly secrets import&lt;/code&gt; to upload secrets&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Other deployment options&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.netlify.com/&#34;&gt;Netlify&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vercel.com/&#34;&gt;Vercel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to contribute to this repo&lt;/h2&gt; &#xA;&lt;h3&gt;Code contribution workflow&lt;/h3&gt; &#xA;&lt;p&gt;You can fork this repo, make changes, and create a PR. Add &lt;strong&gt;@ykhli or @timqian&lt;/strong&gt; as reviewers.&lt;/p&gt; &#xA;&lt;p&gt;If you are new to contributing on github, here is a step-by-step guide:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clcik on &lt;code&gt;Fork&lt;/code&gt; on the top right of this page&lt;/li&gt; &#xA; &lt;li&gt;Work on your change and push it to your forked repo. Now when you navigate to the forked repo&#39;s UI, you should see something like the following:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img width=&#34;904&#34; alt=&#34;pr-preview&#34; src=&#34;https://github.com/a16z-infra/ai-getting-started/assets/3489963/631e5f45-39ec-4b54-b9d1-b963e279dcc6&#34;&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Click on &#34;Contribute&#34; -&amp;gt; &#34;Open Pull Request&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Once you have a PR, you can add reviewers.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Other contributions&lt;/h3&gt; &#xA;&lt;p&gt;Feel free to open feature requests, bug reports etc under Issues.&lt;/p&gt; &#xA;&lt;h2&gt;Refs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/pinecone&#34;&gt;https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/pinecone&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://js.langchain.com/docs/modules/models/llms/integrations#replicate&#34;&gt;https://js.langchain.com/docs/modules/models/llms/integrations#replicate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://js.langchain.com/docs/modules/chains/index_related_chains/retrieval_qa&#34;&gt;https://js.langchain.com/docs/modules/chains/index_related_chains/retrieval_qa&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>