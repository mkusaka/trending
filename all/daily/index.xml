<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-10T01:22:59Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PawanOsman/ChatGPT</title>
    <updated>2024-04-10T01:22:59Z</updated>
    <id>tag:github.com,2024-04-10:/PawanOsman/ChatGPT</id>
    <link href="https://github.com/PawanOsman/ChatGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenAI API Free Reverse Proxy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT API Free Reverse Proxy&lt;/h1&gt; &#xA;&lt;p&gt;Welcome to the &lt;strong&gt;ChatGPT API Free Reverse Proxy&lt;/strong&gt; project, a complimentary resource allowing seamless access to OpenAI&#39;s API. This project mirrors the official OpenAI API endpoints, enabling users to leverage OpenAI functionalities without direct cost. Dive into our documentation to discover how to set up your reverse proxy or connect with our hosted service for an even smoother experience.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Links&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.pawan.krd&#34;&gt;Join our Discord Community&lt;/a&gt; for support and questions. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;⚡Note: Your Discord account must be at least 7 days old to be able join our Discord community.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Option 1: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#installingself-hosting-guide&#34;&gt;Installing/Self-Hosting Guide&lt;/a&gt; (Without using any API key) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Method 1: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#using-docker&#34;&gt;Using Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Method 2: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#your-pcserver&#34;&gt;Your PC/Server&lt;/a&gt; (manually)&lt;/li&gt; &#xA;   &lt;li&gt;Method 3: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#termux-on-android-phones&#34;&gt;Termux on Android Phones&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Option 2: &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#accessing-our-hosted-api&#34;&gt;Accessing Our Hosted API&lt;/a&gt; (Free)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#usage-examples&#34;&gt;Usage Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Streaming Response&lt;/strong&gt;: The API supports streaming response, so you can get the response as soon as it&#39;s available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;API Endpoint Compatibility&lt;/strong&gt;: Full alignment with official OpenAI API endpoints, ensuring hassle-free integration with existing OpenAI libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Complimentary Access&lt;/strong&gt;: No charges for API usage, making advanced AI accessible to everyone even &lt;strong&gt;without an API key&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installing/Self-Hosting Guide&lt;/h2&gt; &#xA;&lt;h3&gt;Using Docker&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure Docker is installed by referring to the &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker Installation Docs&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run the following command: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -dp 3040:3040 pawanosman/chatgpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Done! You can now connect to your local server&#39;s API at: &lt;pre&gt;&lt;code&gt;http://localhost:3040/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; Note that the base URL is &lt;code&gt;http://localhost:3040/v1&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Your PC/Server&lt;/h3&gt; &#xA;&lt;p&gt;To install and run the ChatGPT API Reverse Proxy on your PC/Server by following these steps:&lt;/p&gt; &#xA;&lt;p&gt;Note: This option is not available to all countries yet. if you are from a country that is not supported, you can use a &lt;strong&gt;U.S. VPN&lt;/strong&gt; or use &lt;strong&gt;our hosted API&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure NodeJs (v19+) is installed: &lt;a href=&#34;https://nodejs.org/en/download&#34;&gt;Download NodeJs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/PawanOsman/ChatGPT.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Open &lt;code&gt;start.bat&lt;/code&gt; (Windows) or &lt;code&gt;start.sh&lt;/code&gt; (Linux with &lt;code&gt;bash start.sh&lt;/code&gt; command) to install dependencies and launch the server.&lt;/li&gt; &#xA; &lt;li&gt;Done, you can connect to your local server&#39;s API at: &lt;pre&gt;&lt;code&gt;http://localhost:3040/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; Note that the base url will be &lt;code&gt;http://localhost:3040/v1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To include installation instructions for Termux on Android devices, you can add the following section right after the instructions for Linux in the &lt;strong&gt;Installing/Self-Hosting Guide&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;h3&gt;Termux on Android Phones&lt;/h3&gt; &#xA;&lt;p&gt;To install and run the ChatGPT API Reverse Proxy on Android using Termux, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.termux&#34;&gt;Termux&lt;/a&gt; from the Play Store.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Update Termux packages:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt update&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Upgrade Termux packages:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt upgrade&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install git, Node.js, and npm:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt install -y git nodejs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/PawanOsman/ChatGPT.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the cloned directory:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ChatGPT&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start the server with:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash start.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Your local server will now be running and accessible at:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;http://localhost:3040/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the base url will be &lt;code&gt;http://localhost:3040/v1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You can now use this address to connect to your self-hosted ChatGPT API Reverse Proxy from Android applications/websites that support reverse proxy configurations, on the same device.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Accessing Our Hosted API&lt;/h2&gt; &#xA;&lt;p&gt;Utilize our pre-hosted ChatGPT-like API for free by:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Joining our &lt;a href=&#34;https://discord.pawan.krd&#34;&gt;Discord server&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Obtaining an API key from the &lt;code&gt;#Bot&lt;/code&gt; channel with the &lt;code&gt;/key&lt;/code&gt; command.&lt;/li&gt; &#xA; &lt;li&gt;Incorporating the API key into your requests to: &lt;pre&gt;&lt;code&gt;https://api.pawan.krd/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;p&gt;Leverage the same integration code as OpenAI&#39;s official libraries by simply adjusting the API key and base URL in your requests. For self-hosted setups, ensure to switch the base URL to your local server&#39;s address as mentioned above.&lt;/p&gt; &#xA;&lt;h3&gt;Example Usage with OpenAI Libraries&lt;/h3&gt; &#xA;&lt;h4&gt;Python Example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;&#xA;openai.api_key = &#39;anything&#39;&#xA;openai.base_url = &#34;http://localhost:3040/v1/&#34;&#xA;&#xA;completion = openai.chat.completions.create(&#xA;    model=&#34;gpt-3.5-turbo&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;How do I list all files in a directory using Python?&#34;},&#xA;    ],&#xA;)&#xA;&#xA;print(completion.choices[0].message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Node.js Example&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const openai = new OpenAI({&#xA;&#x9;apiKey: &#34;anything&#34;,&#xA;&#x9;baseURL: &#34;http://localhost:3040/v1&#34;,&#xA;});&#xA;&#xA;const chatCompletion = await openai.chat.completions.create({&#xA;  messages: [{ role: &#39;user&#39;, content: &#39;Say this is a test&#39; }],&#xA;  model: &#39;gpt-3.5-turbo&#39;,&#xA;});&#xA;&#xA;console.log(chatCompletion.choices[0].message.content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is under the AGPL-3.0 License. Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/PawanOsman/ChatGPT/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for detailed information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/text-generation-inference</title>
    <updated>2024-04-10T01:22:59Z</updated>
    <id>tag:github.com,2024-04-10:/huggingface/text-generation-inference</id>
    <link href="https://github.com/huggingface/text-generation-inference" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Large Language Model Text Generation Inference&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.youtube.com/watch?v=jlMAX2Oaht0&#34;&gt; &lt;img width=&#34;315&#34; alt=&#34;Making TGI deployment optimal&#34; src=&#34;https://huggingface.co/datasets/Narsil/tgi_assets/resolve/main/thumbnail.png&#34;&gt; &lt;/a&gt; &#xA; &lt;h1&gt;Text Generation Inference&lt;/h1&gt; &#xA; &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/huggingface/text-generation-inference?style=social&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.github.io/text-generation-inference&#34;&gt; &lt;img alt=&#34;Swagger API documentation&#34; src=&#34;https://img.shields.io/badge/API-Swagger-informational&#34;&gt; &lt;/a&gt; &#xA; &lt;p&gt;A Rust, Python and gRPC server for text generation inference. Used in production at &lt;a href=&#34;https://huggingface.co&#34;&gt;HuggingFace&lt;/a&gt; to power Hugging Chat, the Inference API and Inference Endpoint.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#get-started&#34;&gt;Get Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#api-documentation&#34;&gt;API Documentation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#using-a-private-or-gated-model&#34;&gt;Using a private or gated model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#a-note-on-shared-memory-shm&#34;&gt;A note on Shared Memory&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#distributed-tracing&#34;&gt;Distributed Tracing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#local-install&#34;&gt;Local Install&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#cuda-kernels&#34;&gt;CUDA Kernels&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#optimized-architectures&#34;&gt;Optimized architectures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#run-a-model&#34;&gt;Run Mistral&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#run&#34;&gt;Run&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#quantization&#34;&gt;Quantization&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#develop&#34;&gt;Develop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/text-generation-inference/main/#testing&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Text Generation Inference (TGI) is a toolkit for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and &lt;a href=&#34;https://huggingface.co/docs/text-generation-inference/supported_models&#34;&gt;more&lt;/a&gt;. TGI implements many features, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple launcher to serve most popular LLMs&lt;/li&gt; &#xA; &lt;li&gt;Production ready (distributed tracing with Open Telemetry, Prometheus metrics)&lt;/li&gt; &#xA; &lt;li&gt;Tensor Parallelism for faster inference on multiple GPUs&lt;/li&gt; &#xA; &lt;li&gt;Token streaming using Server-Sent Events (SSE)&lt;/li&gt; &#xA; &lt;li&gt;Continuous batching of incoming requests for increased total throughput&lt;/li&gt; &#xA; &lt;li&gt;Optimized transformers code for inference using &lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;Flash Attention&lt;/a&gt; and &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;Paged Attention&lt;/a&gt; on the most popular architectures&lt;/li&gt; &#xA; &lt;li&gt;Quantization with : &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.17323&#34;&gt;GPT-Q&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/NetEase-FuXi/EETQ&#34;&gt;EETQ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/casper-hansen/AutoAWQ&#34;&gt;AWQ&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/safetensors&#34;&gt;Safetensors&lt;/a&gt; weight loading&lt;/li&gt; &#xA; &lt;li&gt;Watermarking with &lt;a href=&#34;https://arxiv.org/abs/2301.10226&#34;&gt;A Watermark for Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Logits warper (temperature scaling, top-p, top-k, repetition penalty, more details see &lt;a href=&#34;https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor&#34;&gt;transformers.LogitsProcessor&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Stop sequences&lt;/li&gt; &#xA; &lt;li&gt;Log probabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/text-generation-inference/conceptual/speculation&#34;&gt;Speculation&lt;/a&gt; ~2x latency&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/text-generation-inference/conceptual/guidance&#34;&gt;Guidance/JSON&lt;/a&gt;. Specify output format to speed up inference and make sure the output is valid according to some specs..&lt;/li&gt; &#xA; &lt;li&gt;Custom Prompt Generation: Easily generate text by providing custom prompts to guide the model&#39;s output&lt;/li&gt; &#xA; &lt;li&gt;Fine-tuning Support: Utilize fine-tuned models for specific tasks to achieve higher accuracy and performance&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Hardware support&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference&#34;&gt;Nvidia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/text-generation-inference/pkgs/container/text-generation-inference&#34;&gt;AMD&lt;/a&gt; (-rocm)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/optimum-neuron/tree/main/text-generation-inference&#34;&gt;Inferentia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/text-generation-inference/pull/1475&#34;&gt;Intel GPU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/tgi-gaudi&#34;&gt;Gaudi&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;For a detailed starting guide, please see the &lt;a href=&#34;https://huggingface.co/docs/text-generation-inference/quicktour&#34;&gt;Quick Tour&lt;/a&gt;. The easiest way of getting started is using the official Docker container:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=HuggingFaceH4/zephyr-7b-beta&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;&#xA;docker run --gpus all --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.4 --model-id $model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then you can make requests like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl 127.0.0.1:8080/generate_stream \&#xA;    -X POST \&#xA;    -d &#39;{&#34;inputs&#34;:&#34;What is Deep Learning?&#34;,&#34;parameters&#34;:{&#34;max_new_tokens&#34;:20}}&#39; \&#xA;    -H &#39;Content-Type: application/json&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To use NVIDIA GPUs, you need to install the &lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;. We also recommend using NVIDIA drivers with CUDA version 12.2 or higher. For running the Docker container on a machine with no GPUs or CUDA support, it is enough to remove the &lt;code&gt;--gpus all&lt;/code&gt; flag and add &lt;code&gt;--disable-custom-kernels&lt;/code&gt;, please note CPU is not the intended platform for this project, so performance might be subpar.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; TGI supports AMD Instinct MI210 and MI250 GPUs. Details can be found in the &lt;a href=&#34;https://huggingface.co/docs/text-generation-inference/supported_models#supported-hardware&#34;&gt;Supported Hardware documentation&lt;/a&gt;. To use AMD GPUs, please use &lt;code&gt;docker run --device /dev/kfd --device /dev/dri --shm-size 1g -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.4-rocm --model-id $model&lt;/code&gt; instead of the command above.&lt;/p&gt; &#xA;&lt;p&gt;To see all options to serve your models (in the &lt;a href=&#34;https://github.com/huggingface/text-generation-inference/raw/main/launcher/src/main.rs&#34;&gt;code&lt;/a&gt; or in the cli):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;text-generation-launcher --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;API documentation&lt;/h3&gt; &#xA;&lt;p&gt;You can consult the OpenAPI documentation of the &lt;code&gt;text-generation-inference&lt;/code&gt; REST API using the &lt;code&gt;/docs&lt;/code&gt; route. The Swagger UI is also available at: &lt;a href=&#34;https://huggingface.github.io/text-generation-inference&#34;&gt;https://huggingface.github.io/text-generation-inference&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Using a private or gated model&lt;/h3&gt; &#xA;&lt;p&gt;You have the option to utilize the &lt;code&gt;HUGGING_FACE_HUB_TOKEN&lt;/code&gt; environment variable for configuring the token employed by &lt;code&gt;text-generation-inference&lt;/code&gt;. This allows you to gain access to protected resources.&lt;/p&gt; &#xA;&lt;p&gt;For example, if you want to serve the gated Llama V2 model variants:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Go to &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;https://huggingface.co/settings/tokens&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Copy your cli READ token&lt;/li&gt; &#xA; &lt;li&gt;Export &lt;code&gt;HUGGING_FACE_HUB_TOKEN=&amp;lt;your cli READ token&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;or with Docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model=meta-llama/Llama-2-7b-chat-hf&#xA;volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run&#xA;token=&amp;lt;your cli READ token&amp;gt;&#xA;&#xA;docker run --gpus all --shm-size 1g -e HUGGING_FACE_HUB_TOKEN=$token -p 8080:80 -v $volume:/data ghcr.io/huggingface/text-generation-inference:1.4 --model-id $model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;A note on Shared Memory (shm)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html&#34;&gt;&lt;code&gt;NCCL&lt;/code&gt;&lt;/a&gt; is a communication framework used by &lt;code&gt;PyTorch&lt;/code&gt; to do distributed training/inference. &lt;code&gt;text-generation-inference&lt;/code&gt; make use of &lt;code&gt;NCCL&lt;/code&gt; to enable Tensor Parallelism to dramatically speed up inference for large language models.&lt;/p&gt; &#xA;&lt;p&gt;In order to share data between the different devices of a &lt;code&gt;NCCL&lt;/code&gt; group, &lt;code&gt;NCCL&lt;/code&gt; might fall back to using the host memory if peer-to-peer using NVLink or PCI is not possible.&lt;/p&gt; &#xA;&lt;p&gt;To allow the container to use 1G of Shared Memory and support SHM sharing, we add &lt;code&gt;--shm-size 1g&lt;/code&gt; on the above command.&lt;/p&gt; &#xA;&lt;p&gt;If you are running &lt;code&gt;text-generation-inference&lt;/code&gt; inside &lt;code&gt;Kubernetes&lt;/code&gt;. You can also add Shared Memory to the container by creating a volume with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;- name: shm&#xA;  emptyDir:&#xA;   medium: Memory&#xA;   sizeLimit: 1Gi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and mounting it to &lt;code&gt;/dev/shm&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Finally, you can also disable SHM sharing by using the &lt;code&gt;NCCL_SHM_DISABLE=1&lt;/code&gt; environment variable. However, note that this will impact performance.&lt;/p&gt; &#xA;&lt;h3&gt;Distributed Tracing&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;text-generation-inference&lt;/code&gt; is instrumented with distributed tracing using OpenTelemetry. You can use this feature by setting the address to an OTLP collector with the &lt;code&gt;--otlp-endpoint&lt;/code&gt; argument.&lt;/p&gt; &#xA;&lt;h3&gt;Architecture&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/TGI.png&#34; alt=&#34;TGI architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Local install&lt;/h3&gt; &#xA;&lt;p&gt;You can also opt to install &lt;code&gt;text-generation-inference&lt;/code&gt; locally.&lt;/p&gt; &#xA;&lt;p&gt;First &lt;a href=&#34;https://rustup.rs/&#34;&gt;install Rust&lt;/a&gt; and create a Python virtual environment with at least Python 3.9, e.g. using &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh&#xA;&#xA;conda create -n text-generation-inference python=3.11&#xA;conda activate text-generation-inference&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may also need to install Protoc.&lt;/p&gt; &#xA;&lt;p&gt;On Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PROTOC_ZIP=protoc-21.12-linux-x86_64.zip&#xA;curl -OL https://github.com/protocolbuffers/protobuf/releases/download/v21.12/$PROTOC_ZIP&#xA;sudo unzip -o $PROTOC_ZIP -d /usr/local bin/protoc&#xA;sudo unzip -o $PROTOC_ZIP -d /usr/local &#39;include/*&#39;&#xA;rm -f $PROTOC_ZIP&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On MacOS, using Homebrew:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;brew install protobuf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;BUILD_EXTENSIONS=True make install # Install repository and HF/transformer fork with CUDA kernels&#xA;text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; on some machines, you may also need the OpenSSL libraries and gcc. On Linux machines, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;sudo apt-get install libssl-dev gcc -y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Optimized architectures&lt;/h2&gt; &#xA;&lt;p&gt;TGI works out of the box to serve optimized models for all modern models. They can be found in &lt;a href=&#34;https://huggingface.co/docs/text-generation-inference/supported_models&#34;&gt;this list&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Other architectures are supported on a best-effort basis using:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;AutoModelForCausalLM.from_pretrained(&amp;lt;model&amp;gt;, device_map=&#34;auto&#34;)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;AutoModelForSeq2SeqLM.from_pretrained(&amp;lt;model&amp;gt;, device_map=&#34;auto&#34;)&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Run locally&lt;/h2&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;p&gt;You can also quantize the weights with bitsandbytes to reduce the VRAM requirement:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;text-generation-launcher --model-id mistralai/Mistral-7B-Instruct-v0.2 --quantize&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;4bit quantization is available using the &lt;a href=&#34;https://arxiv.org/pdf/2305.14314.pdf&#34;&gt;NF4 and FP4 data types from bitsandbytes&lt;/a&gt;. It can be enabled by providing &lt;code&gt;--quantize bitsandbytes-nf4&lt;/code&gt; or &lt;code&gt;--quantize bitsandbytes-fp4&lt;/code&gt; as a command line argument to &lt;code&gt;text-generation-launcher&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Develop&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make server-dev&#xA;make router-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# python&#xA;make python-server-tests&#xA;make python-client-tests&#xA;# or both server and client tests&#xA;make python-tests&#xA;# rust cargo tests&#xA;make rust-tests&#xA;# integration tests&#xA;make integration-tests&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>nodejs/nodejs.org</title>
    <updated>2024-04-10T01:22:59Z</updated>
    <id>tag:github.com,2024-04-10:/nodejs/nodejs.org</id>
    <link href="https://github.com/nodejs/nodejs.org" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Node.js® Website&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://nodejs.org&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;./public/static/logos/nodejsDark.svg&#34;&gt; &#xA;   &lt;img src=&#34;https://raw.githubusercontent.com/nodejs/nodejs.org/main/public/static/logos/nodejsLight.svg?sanitize=true&#34; width=&#34;200px&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://nodejs.org&#34;&gt;Node.js&lt;/a&gt; Website built using Next.js with TypeScript, SCSS and MDXv2 &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a title=&#34;MIT License&#34; href=&#34;https://raw.githubusercontent.com/nodejs/nodejs.org/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue&#34; alt=&#34;MIT License&#34;&gt; &lt;/a&gt; &lt;a title=&#34;Localised&#34; href=&#34;https://crowdin.com/project/nodejs-web&#34;&gt; &lt;img src=&#34;https://badges.crowdin.net/nodejs-web/localized.svg?sanitize=true&#34; alt=&#34;Crowdin Badge&#34;&gt; &lt;/a&gt; &lt;a title=&#34;Vercel&#34; href=&#34;https://vercel.com&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://img.shields.io/badge/powered%20by-Vercel%20%E2%96%B2-white&#34;&gt; &#xA;   &lt;img src=&#34;https://img.shields.io/badge/powered%20by-Vercel%20%E2%96%B2-black&#34; alt=&#34;Powered by Vercel&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;br&gt; &lt;img src=&#34;https://github.com/nodejs/nodejs.org/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;Build and Analysis Checks&#34;&gt; &lt;a title=&#34;scorecard&#34; href=&#34;https://securityscorecards.dev/viewer/?uri=github.com/nodejs/nodejs.org&#34;&gt; &lt;img src=&#34;https://api.securityscorecards.dev/projects/github.com/nodejs/nodejs.org/badge&#34; alt=&#34;nodejs.org scorecard badge&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;What is this repo?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/&#34;&gt;Nodejs.org&lt;/a&gt; by the &lt;a href=&#34;https://openjsf.org/&#34;&gt;OpenJS Foundation&lt;/a&gt; is the official website for the Node.js® JavaScript runtime. This repo is the source code for the website. It is built using &lt;a href=&#34;https://nextjs.org&#34;&gt;Next.js&lt;/a&gt;, a React Framework.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm ci&#xA;npx turbo serve&#xA;&#xA;# listening at localhost:3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project adopts the Node.js &lt;a href=&#34;https://github.com/nodejs/admin/raw/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Any person who wants to contribute to the Website is welcome! Please read &lt;a href=&#34;https://github.com/nodejs/nodejs.org/raw/main/CONTRIBUTING.md&#34;&gt;Contribution Guidelines&lt;/a&gt; and see the &lt;a href=&#34;https://www.figma.com/file/pu1vZPqNIM7BePd6W8APA5/Node.js&#34;&gt;Figma Design&lt;/a&gt; to understand better the structure of this repository.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br&gt; Please read our &lt;a href=&#34;https://github.com/nodejs/nodejs.org/raw/main/TRANSLATION.md&#34;&gt;Translation Guidelines&lt;/a&gt; before contributing to Translation and Localization of the Website&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;br&gt; We recommend a read of all Relevant Links below before doing code changes; Including Dependency changes, Content changes, and Code changes.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Deployment&lt;/h3&gt; &#xA;&lt;p&gt;The Website is automatically deployed to &lt;a href=&#34;https://vercel.com&#34;&gt;Vercel&lt;/a&gt; through its GitHub App integration when new pushes happen on the &lt;code&gt;main&lt;/code&gt; branch.&lt;/p&gt; &#xA;&lt;p&gt;Details regarding the deployment are only accessible to the maintainers of the Website Team due to certain limitations.&lt;/p&gt; &#xA;&lt;p&gt;The current integration is owned by the OpenJS Foundation and managed by the Website Team.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Legacy Deployment&lt;/summary&gt; &#xA; &lt;p&gt;The full setup is in &lt;a href=&#34;https://github.com/nodejs/build/tree/master/ansible/www-standalone&#34;&gt;https://github.com/nodejs/build/tree/master/ansible/www-standalone&lt;/a&gt; minus secrets and certificates.&lt;/p&gt; &#xA; &lt;p&gt;The webhook is set up on GitHub for this project and talks to a small Node server on the host, which does the work. See the &lt;a href=&#34;https://github.com/rvagg/github-webhook&#34;&gt;github-webhook&lt;/a&gt; package for this.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Node.js Binaries &amp;amp; API Docs&lt;/h2&gt; &#xA;&lt;p&gt;This repository does not contain the codebase or related infrastructure that serves &lt;code&gt;https://nodejs.org/api/&lt;/code&gt;, &lt;code&gt;https://nodejs.org/docs/&lt;/code&gt; or &lt;code&gt;https://nodejs.org/dist/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These are maintained in different repositories and we urge users to open &lt;strong&gt;issues in their respective repositories&lt;/strong&gt;, for bug reports, feature requests or any matter related to these endpoints.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nodejs/release-cloudflare-worker&#34;&gt;&lt;code&gt;release-cloudflare-worker&lt;/code&gt;&lt;/a&gt;: The codebase responsible for serving the Node.js Distribution Binaries, API Docs and any other assets from the links mentioned above. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We use Cloudflare R2 Buckets for storing our Assets and Cloudflare Workers for serving these Assets to the Web.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nodejs/node/tree/main/doc/api&#34;&gt;&lt;code&gt;node/doc/api&lt;/code&gt;&lt;/a&gt;: The source code of our API docs, it contains all the Node.js API Documentation Markdown files &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/nodejs/node/tree/main/doc&#34;&gt;&lt;code&gt;node/doc&lt;/code&gt;&lt;/a&gt; contains the HTML templates, CSS styles and JavaScript code that runs on the client-side of our API Docs generated pages.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/nodejs/node/tree/main/tools/doc&#34;&gt;&lt;code&gt;node/tools/doc&lt;/code&gt;&lt;/a&gt; contains the tooling that validates, lints, builds and compiles our API Docs. Also responsible for generating what you see when accessing &lt;code&gt;https://nodejs.org/api/&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Relevant Links&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nodejs/admin/raw/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nodejs/nodejs.org/raw/main/CONTRIBUTING.md&#34;&gt;Contribution Guidelines&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nodejs/nodejs.org/raw/main/COLLABORATOR_GUIDE.md&#34;&gt;Collaborator Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.figma.com/file/pu1vZPqNIM7BePd6W8APA5/Node.js&#34;&gt;Figma Design&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nodejs/nodejs.org/raw/main/CONTENT_VS_CODE.md&#34;&gt;Content vs Code&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nodejs/nodejs.org/raw/main/DEPENDENCY_PINNING.md&#34;&gt;Dependency Pinning&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nodejs/nodejs.org/raw/main/TRANSLATION.md&#34;&gt;Translation Guidelines&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to all contributors and collaborators that make this project possible.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://www.chromatic.com/&#34;&gt;Chromatic&lt;/a&gt; for providing the visual testing platform that helps us review UI changes and catch visual regressions.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://www.vercel.com/&#34;&gt;Vercel&lt;/a&gt; for providing the infrastructure that serves and powers the Node.js Website&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://cloudflare.com&#34;&gt;Cloudflare&lt;/a&gt; for providing the infrastructure that serves Node.js&#39;s Website, Node.js&#39;s CDN and more. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A really warm thank you to Cloudflare as we would not be able to serve our community without their immense support.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://sentry.io/welcome/&#34;&gt;Sentry&lt;/a&gt; for providing an open source license for their error reporting, monitoring and diagnostic tools.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://crowdin.com/&#34;&gt;Crowdin&lt;/a&gt; for providing a platform that allows us to localize the Node.js Website and collaborate with translators.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://docs.oramasearch.com/&#34;&gt;Orama&lt;/a&gt; for providing a search platform that indexes our expansive content and provides lightning-fast results for our users.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>