<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-04T01:29:10Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mckaywrigley/ai-code-translator</title>
    <updated>2023-04-04T01:29:10Z</updated>
    <id>tag:github.com,2023-04-04:/mckaywrigley/ai-code-translator</id>
    <link href="https://github.com/mckaywrigley/ai-code-translator" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Use AI to translate code from one language to another.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AI Code Translator&lt;/h1&gt; &#xA;&lt;p&gt;Use AI to translate code from one language to another.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mckaywrigley/ai-code-translator/main/public/screenshot.png&#34; alt=&#34;AI Code Translator&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Clone Repo&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/mckaywrigley/ai-code-translator.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Install Dependencies&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm i&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Run App&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, feel free to reach out to me on &lt;a href=&#34;https://twitter.com/mckaywrigley&#34;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lm-sys/FastChat</title>
    <updated>2023-04-04T01:29:10Z</updated>
    <id>tag:github.com,2023-04-04:/lm-sys/FastChat</id>
    <link href="https://github.com/lm-sys/FastChat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The release repo for &#34;Vicuna: An Open Chatbot Impressing GPT-4&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FastChat&lt;/h1&gt; &#xA;&lt;p&gt;An open platform for training, serving, and evaluating large language model based chatbots.&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://vicuna.lmsys.org&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/assets/vicuna-logo.jpeg&#34; width=&#34;20%&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥 We released &lt;strong&gt;Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality&lt;/strong&gt;. Checkout the blog &lt;a href=&#34;https://vicuna.lmsys.org&#34;&gt;post&lt;/a&gt; and &lt;a href=&#34;https://chat.lmsys.org/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chat.lmsys.org&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/assets/demo-narrow.gif&#34; width=&#34;70%&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Join our &lt;a href=&#34;https://discord.gg/h6kCZb72G7&#34;&gt;Discord&lt;/a&gt; server and follow our &lt;a href=&#34;https://twitter.com/lmsysorg&#34;&gt;Twitter&lt;/a&gt; to get the latest updates.&lt;/p&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/#install&#34;&gt;Install&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/#vicuna-weights&#34;&gt;Vicuna Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/#serving&#34;&gt;Serving&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;Method 1: With pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install FastChat&#xA;pip3 install fschat&#xA;&#xA;# Install a specific commit of huggingface/transformers&#xA;# Our released weights do not work with commits after this due to some upstream changes in the tokenizer.&#xA;pip3 install git+https://github.com/huggingface/transformers@c612628045822f909020f7eb6784c79700813eda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Method 2: From source&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository and navigate to FastChat folder&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/lm-sys/FastChat.git&#xA;cd FastChat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install Package&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install --upgrade pip  # enable PEP 660 support&#xA;pip3 install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Vicuna Weights&lt;/h2&gt; &#xA;&lt;p&gt;We release &lt;a href=&#34;https://vicuna.lmsys.org/&#34;&gt;Vicuna&lt;/a&gt; weights as delta weights to comply with the LLaMA model license. You can add our delta to the original LLaMA weights to obtain the Vicuna weights. Instructions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Get the orignal LLaMA weights in the huggingface format by following the instructions &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the following scripts to get Vicuna weights by applying our delta. It will automatically download delta weights from our Hugging Face account.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: Our released weights are only compatible with huggingface/transformers commit: &lt;code&gt;c612628045822f909020f7eb6784c79700813eda&lt;/code&gt;. The weights do not work with commits after this due to some upstream changes in the tokenizer. We install the correct version of transformers when fastchat is installed.&lt;/p&gt; &#xA;&lt;h3&gt;Vicuna-13B&lt;/h3&gt; &#xA;&lt;p&gt;This conversion command needs around 60 GB of CPU RAM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m fastchat.model.apply_delta \&#xA;    --base /path/to/llama-13b \&#xA;    --target /output/path/to/vicuna-13b \&#xA;    --delta lmsys/vicuna-13b-delta-v0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Vicuna-7B&lt;/h3&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Serving&lt;/h2&gt; &#xA;&lt;h3&gt;Command Line Interface&lt;/h3&gt; &#xA;&lt;h4&gt;Single GPU&lt;/h4&gt; &#xA;&lt;p&gt;The command below requires around 28GB of GPU memory for Vicuna-13B.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m fastchat.serve.cli --model-name /path/to/vicuna/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi GPU&lt;/h4&gt; &#xA;&lt;p&gt;If you do not have enough GPU memory, you can use model parallelism to aggregate memory from multiple GPUs on the same machine.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m fastchat.serve.cli --model-name /path/to/vicuna/weights --num-gpus 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;CPU Only&lt;/h4&gt; &#xA;&lt;p&gt;This runs on the CPU only and does not require GPU. It requires around 60GB of CPU memory for Vicuna-13B.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m fastchat.serve.cli --model-name /path/to/vicuna/weights --device cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Others (Quantization, More Platforms)&lt;/h4&gt; &#xA;&lt;p&gt;Currently, we only provide some basic commands for running the model. We are actively exploring methods to make the model easier to run on more platforms. Contributions and pull requests are welcome.&lt;/p&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;h4&gt;Launch a controller&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m fastchat.serve.controller&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a model worker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m fastchat.serve.model_worker --model-path /path/to/vicuna/weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait until the process finishes loading the model and you see &#34;Uvicorn running on ...&#34;.&lt;/p&gt; &#xA;&lt;h4&gt;Send a test message&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m fastchat.serve.test_message --model-name vicuna-13b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Launch a gradio web server.&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m fastchat.serve.gradio_web_server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;You can open your browser and chat with a model now.&lt;/h4&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Our AI-enhanced &lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/eval&#34;&gt;evaluation&lt;/a&gt; pipeline is based on GPT-4. Here are some high-level instructions for using the pipeline:&lt;/p&gt; &#xA;&lt;p&gt;First, generate answers from different models. Use &lt;code&gt;qa_baseline_gpt35.py&lt;/code&gt; for ChatGPT, or specify the model checkpoint and run &lt;code&gt;model_qa.py&lt;/code&gt; for Vicuna and other models.&lt;/p&gt; &#xA;&lt;p&gt;Then, use GPT-4 to generate reviews automatically, which can be done manually if the GPT-4 API is not available to you. Once you have your evaluation data, visualize the results by running &lt;code&gt;generate_webpage_data_from_table.py&lt;/code&gt;, which generates data for a static website.&lt;/p&gt; &#xA;&lt;p&gt;Finally, serve a static website under the &lt;code&gt;webpage&lt;/code&gt; directory. You can simply use &lt;code&gt;python3 -m http.server&lt;/code&gt; to serve the website locally.&lt;/p&gt; &#xA;&lt;p&gt;Besides the evaluation workflow, we also document the data format used for evaluation, which is encoded with JSON Lines and includes information on models, prompts, reviewers, questions, answers, and reviews. You can customize the evaluation process or contribute to our project by accessing relevant &lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/eval/table/&#34;&gt;data&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Check &lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/fastchat/eval&#34;&gt;evaluation&lt;/a&gt; for detailed instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;Vicuna is created by fine-tuning a LLaMA base model using approximately 70K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model&#39;s maximum context length. For detailed instructions to clean the ShareGPT data, check out &lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/docs/commands/data_cleaning.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Due to some concerns, we may not release the data at the moment. If you would like to try the fine-tuning code, you can try to run it with our &lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/playground/data/alpaca-data-conversation.json&#34;&gt;preprocessed alpaca dataset&lt;/a&gt; (originally from &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;here&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h3&gt;Code and Hyperparameters&lt;/h3&gt; &#xA;&lt;p&gt;We fine-tune the model using the code from &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt;, with some modifications to support gradient checkpointing and &lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;Flash Attention&lt;/a&gt;. We use similar hyperparameters as the Stanford Alpaca.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Global Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Learning rate&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Epochs&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Max length&lt;/th&gt; &#xA;   &lt;th align=&#34;right&#34;&gt;Weight decay&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;128&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2e-5&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;3&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Fine-tuning on Any Cloud with SkyPilot&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/skypilot-org/skypilot&#34;&gt;SkyPilot&lt;/a&gt; is a framework built by UC Berkeley for easily and cost effectively running ML workloads on any cloud (AWS, GCP, Azure, Lambda, etc.). To use SkyPilot, install it with the following command and setup the cloud credentials locally following the instructions &lt;a href=&#34;https://skypilot.readthedocs.io/en/latest/getting-started/installation.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install skypilot from the master branch&#xA;pip install git+https://github.com/skypilot-org/skypilot.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Vicuna&lt;/h4&gt; &#xA;&lt;p&gt;Vicuna can be trained on 8 A100 GPUs with 80GB memory. The following command will automatically launch a node satisfying the requirement, setup and run the training job on it.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sky launch -c vicuna -s scripts/train-vicuna.yaml --env WANDB_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other options are also valid:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Launch it on managed spot to save 3x cost (train Vicuna-13B with around $300)&#xA;sky spot launch -n vicuna scripts/train-vicuna.yaml --env WANDB_API_KEY&#xA;&#xA;# Train a 7B model&#xA;sky launch -c vicuna -s scripts/train-vicuna.yaml --env WANDB_API_KEY --env MODEL_SIZE=7&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Please make sure the &lt;code&gt;WANDB_API_KEY&lt;/code&gt; has been setup on your local machine. You can find the API key on your &lt;a href=&#34;https://wandb.ai/authorize&#34;&gt;wandb profile page&lt;/a&gt;. If you would like to train the model without using wandb, you can replace the &lt;code&gt;--env WANDB_API_KEY&lt;/code&gt; flag with &lt;code&gt;--env WANDB_MODE=offline&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Alpaca&lt;/h4&gt; &#xA;&lt;p&gt;Launch the training job with the following line (will be launched on a single node with 4 A100-80GB GPUs)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sky launch -c alpaca -s scripts/train-alpaca.yaml --env WANDB_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning with Local GPUs&lt;/h3&gt; &#xA;&lt;p&gt;Vicuna can also be trained on 8 A100 GPUs with 80GB memory with the following code. To train on fewer GPUs, you can reduce the &lt;code&gt;per_device_train_batch_size&lt;/code&gt; and increase the &lt;code&gt;gradient_accumulation_steps&lt;/code&gt; accordingly to keep the global batch size the same. To setup the environment, please see the setup section in &lt;a href=&#34;https://raw.githubusercontent.com/lm-sys/FastChat/main/scripts/train-vicuna.yaml&#34;&gt;scripts/train-vicuna.yaml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nnodes=1 --nproc_per_node=8 --master_port=&amp;lt;your_random_port&amp;gt; \&#xA;    fastchat/train/train_mem.py \&#xA;    --model_name_or_path &amp;lt;path-to-llama-model-weight&amp;gt; \&#xA;    --data_path &amp;lt;path-to-data&amp;gt; \&#xA;    --bf16 True \&#xA;    --output_dir ./checkpoints \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 4 \&#xA;    --per_device_eval_batch_size 4 \&#xA;    --gradient_accumulation_steps 1 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 1200 \&#xA;    --save_total_limit 100 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --fsdp &#34;full_shard auto_wrap&#34; \&#xA;    --fsdp_transformer_layer_cls_to_wrap &#39;LlamaDecoderLayer&#39; \&#xA;    --tf32 True \&#xA;    --model_max_length 2048 \&#xA;    --gradient_checkpointing True \&#xA;    --lazy_preprocess True&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>stochasticai/xturing</title>
    <updated>2023-04-04T01:29:10Z</updated>
    <id>tag:github.com,2023-04-04:/stochasticai/xturing</id>
    <link href="https://github.com/stochasticai/xturing" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build and control your own LLMs&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/.github/stochastic_logo_light.svg#gh-light-mode-only&#34; width=&#34;250&#34; alt=&#34;Stochastic.ai&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/.github/stochastic_logo_dark.svg#gh-dark-mode-only&#34; width=&#34;250&#34; alt=&#34;Stochastic.ai&#34;&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;Build and control your own LLMs&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;code&gt;xturing&lt;/code&gt; provides fast, efficient and simple fine-tuning of LLMs, such as LLaMA, GPT-J, GPT-2, OPT, Cerebras-GPT, Galactica, and more. By providing an easy-to-use interface for personalizing LLMs to your own data and application, xTuring makes it simple to build and control LLMs. The entire process can be done inside your computer or in your private cloud, ensuring data privacy and security.&lt;/p&gt; &#xA;&lt;p&gt;With &lt;code&gt;xturing&lt;/code&gt; you can,&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ingest data from different sources and preprocess them to a format LLMs can understand&lt;/li&gt; &#xA; &lt;li&gt;Scale from single to multiple GPUs for faster fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;Leverage memory-efficient techniques (i.e. LoRA fine-tuning) to reduce your hardware costs by up to 90% of the time&lt;/li&gt; &#xA; &lt;li&gt;Explore different fine-tuning methods and benchmark them to find the best performing model&lt;/li&gt; &#xA; &lt;li&gt;Evalate fine-tuned models on well-defined metrics for in-depth analysis&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pypi.org/project/xturing/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/xturing?style=for-the-badge&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://xturing.stochastic.ai/&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Documentation-blue?logo=GitBook&amp;amp;logoColor=white&amp;amp;style=for-the-badge&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/TgHXuSJEk6&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Chat-FFFFFF?logo=discord&amp;amp;style=for-the-badge&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;CLI playground&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/.github/cli-playground.gif&#34; width=&#34;100%&#34; style=&#34;margin: 0 1%;&#34;&gt; &#xA;&lt;h2&gt;UI playground&lt;/h2&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/.github/ui-playground2.gif&#34; width=&#34;100%&#34; style=&#34;margin: 0 1%;&#34;&gt; &#xA;&lt;h2&gt;⚙️ Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install xturing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;🚀 Quickstart&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from xturing.datasets import InstructionDataset&#xA;from xturing.models import BaseModel&#xA;&#xA;# Load the dataset&#xA;instruction_dataset = InstructionDataset(&#34;./alpaca_data&#34;)&#xA;&#xA;# Initialize the model&#xA;model = BaseModel.create(&#34;llama_lora&#34;)&#xA;&#xA;# Finetune the model&#xA;model.finetune(dataset=instruction_dataset)&#xA;&#xA;# Perform inference&#xA;output = model.generate(texts=[&#34;Why LLM models are becoming so important?&#34;])&#xA;&#xA;print(&#34;Generated output by the model: {}&#34;.format(output))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find the data folder &lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/alpaca_data&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;📚 Tutorials&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/preparing_your_dataset.py&#34;&gt;Preparing your dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/cerebras/cerebras_lora_int8.ipynb&#34;&gt;Cerebras-GPT efficient fine-tuning with LoRA and INT8&lt;/a&gt;   &lt;a href=&#34;https://colab.research.google.com/drive/1eKq3oF7dnK8KuIfsTE70Gvvniwr1O9D0?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/cerebras/cerebras_lora.ipynb&#34;&gt;Cerebras-GPT efficient fine-tuning with LoRA&lt;/a&gt;   &lt;a href=&#34;https://colab.research.google.com/drive/1VjqQhstm5pT4EjPjx4Je7b3W2X1V3vDo?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/llama_lora_int8.py&#34;&gt;LLaMA efficient fine-tuning with LoRA and INT8&lt;/a&gt;   &lt;a href=&#34;https://colab.research.google.com/drive/1SQUXq1AMZPSLD4mk3A3swUIc6Y2dclme?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/llama_lora.py&#34;&gt;LLaMA efficient fine-tuning with LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/llama.py&#34;&gt;LLaMA fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/gptj/gptj_lora_int8.py&#34;&gt;GPT-J efficient fine-tuning with LoRA and INT8&lt;/a&gt;   &lt;a href=&#34;https://colab.research.google.com/drive/1hB_8s1V9K4IzifmlmN2AovGEJzTB1c7e?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/gptj/gptj_lora.py&#34;&gt;GPT-J efficient fine-tuning with LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/galactica/galactica_lora_int8.py&#34;&gt;Galactica efficient fine-tuning with LoRA and INT8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/galactica/galactica_lora.py&#34;&gt;Galactica efficient fine-tuning with LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/opt/opt_lora_int8.py&#34;&gt;OPT efficient fine-tuning with LoRA and INT8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/opt/opt_lora.py&#34;&gt;OPT efficient fine-tuning with LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/gpt2/gpt2_lora.py&#34;&gt;GPT-2 efficient fine-tuning with LoRA&lt;/a&gt;   &lt;a href=&#34;https://drive.google.com/file/d/1Sh-ocNpKn9pS7jv6oBb_Q8DitFyj1avL/view?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;📊 Performance&lt;/h2&gt; &#xA;&lt;p&gt;Here is a comparison for the performance of different fine-tuning techniques on the LLaMA 7B model. We use the &lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/examples/llama/alpaca_data/&#34;&gt;Alpaca dataset&lt;/a&gt; for fine-tuning. The dataset contains 52K instructions.&lt;/p&gt; &#xA;&lt;p&gt;Hardware:&lt;/p&gt; &#xA;&lt;p&gt;4xA100 40GB GPU, 335GB CPU RAM&lt;/p&gt; &#xA;&lt;p&gt;Fine-tuning parameters:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;{&#xA;  &#39;maximum sequence length&#39;: 512,&#xA;  &#39;batch size&#39;: 1,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;LLaMA 7B&lt;/th&gt; &#xA;   &lt;th&gt;DeepSpeed + CPU Offloading&lt;/th&gt; &#xA;   &lt;th&gt;LoRA + DeepSpeed&lt;/th&gt; &#xA;   &lt;th&gt;LoRA + DeepSpeed + CPU Offloading&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;   &lt;td&gt;33.5 GB&lt;/td&gt; &#xA;   &lt;td&gt;23.7 GB&lt;/td&gt; &#xA;   &lt;td&gt;21.9 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt;190 GB&lt;/td&gt; &#xA;   &lt;td&gt;10.2 GB&lt;/td&gt; &#xA;   &lt;td&gt;14.9 GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Time per epoch&lt;/td&gt; &#xA;   &lt;td&gt;21 hours&lt;/td&gt; &#xA;   &lt;td&gt;20 mins&lt;/td&gt; &#xA;   &lt;td&gt;20 mins&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please submit your performance results on other GPUs. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;📎 Fine-tuned model checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;We have already fine-tuned some models that you can use as your base or start playing with. Here is how you would load them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from xturing.models import BaseModel&#xA;model = BaseModel.load(&#34;x/distilgpt2_lora_finetuned_alpaca&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;dataset&lt;/th&gt; &#xA;   &lt;th&gt;Path&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DistilGPT-2 LoRA&lt;/td&gt; &#xA;   &lt;td&gt;alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;x/distilgpt2_lora_finetuned_alpaca&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA LoRA&lt;/td&gt; &#xA;   &lt;td&gt;alpaca&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;x/llama_lora_finetuned_alpaca&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;📈 Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support for LLaMA, GPT-J, GPT-2, OPT, Cerebras-GPT, Galactica and Bloom models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Dataset generation using self-instruction&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 2x more memory-efficient fine-tuning vs LoRA and unsupervised fine-tuning&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; INT8 low-precision fine-tuning support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Supports OpenAI, Cohere and AI21 Studio model APIs for dataset generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Added fine-tuned checkpoints for some models to the hub&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Evaluation of LLM models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support for Stable Diffusion&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;🤝 Help and Support&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, you can create an issue on this repository.&lt;/p&gt; &#xA;&lt;p&gt;You can also join our &lt;a href=&#34;https://discord.gg/TgHXuSJEk6&#34;&gt;Discord server&lt;/a&gt; and start a discussion in the &lt;code&gt;#xturing&lt;/code&gt; channel.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;📝 License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache License 2.0 - see the &lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;🌎 Contributing&lt;/h2&gt; &#xA;&lt;p&gt;As an open source project in a rapidly evolving field, we welcome contributions of all kinds, including new features and better documentation. Please read our &lt;a href=&#34;https://raw.githubusercontent.com/stochasticai/xturing/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to learn how you can get involved.&lt;/p&gt;</summary>
  </entry>
</feed>