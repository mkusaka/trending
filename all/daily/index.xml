<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-09T01:25:28Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>JJTech0130/pypush</title>
    <updated>2023-12-09T01:25:28Z</updated>
    <id>tag:github.com,2023-12-09:/JJTech0130/pypush</id>
    <link href="https://github.com/JJTech0130/pypush" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Cross-platform iMessage POC&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;pypush&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;pypush&lt;/code&gt; is a POC demo of my recent iMessage reverse-engineering. It can currently register as a new device on an Apple ID, set up encryption keys, and &lt;em&gt;&lt;strong&gt;send and receive iMessages&lt;/strong&gt;&lt;/em&gt;!&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pypush&lt;/code&gt; is completely platform-independent, and does not require a Mac or other Apple device to use!&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;It&#39;s pretty self explanatory:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/JJTech0130/pypush&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;python3 ./demo.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;If you have any issues, please join &lt;a href=&#34;https://discord.gg/BVvNukmfTC&#34;&gt;the Discord&lt;/a&gt; and ask for help.&lt;/p&gt; &#xA;&lt;h2&gt;Operation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;pypush&lt;/code&gt; will generate a &lt;code&gt;config.json&lt;/code&gt; in the repository when you run demo.py. DO NOT SHARE THIS FILE. It contains all the encryption keys necessary to log into you Apple ID and send iMessages as you.&lt;/p&gt; &#xA;&lt;p&gt;Once it loads, it should prompt you with &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt;. Type &lt;code&gt;help&lt;/code&gt; and press enter for a list of supported commands.&lt;/p&gt; &#xA;&lt;h2&gt;Special Notes&lt;/h2&gt; &#xA;&lt;h3&gt;Unicorn dependency&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;pypush&lt;/code&gt; currently uses the Unicorn CPU emulator and a custom MachO loader to load a framework from an old version of macOS, in order to call some obfuscated functions.&lt;/p&gt; &#xA;&lt;p&gt;This is only necessary during initial registration, so theoretically you can register on one device, and then copy the &lt;code&gt;config.json&lt;/code&gt; to another device that doesn&#39;t support the Unicorn emulator. Or you could switch out the emulator for another x86 emulator if you really wanted to.&lt;/p&gt; &#xA;&lt;h2&gt;&#34;data.plist&#34; and Mac serial numbers&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains a sample &lt;a href=&#34;https://github.com/JJTech0130/pypush/raw/main/emulated/data.plist&#34;&gt;&lt;code&gt;data.plist&lt;/code&gt;&lt;/a&gt;, which contains the serial number and several other identifiers from a real Mac device. If you run into issues related to rate-limiting or messages failing to deliver, you may regenerate this file by cloning &lt;a href=&#34;https://github.com/JJTech0130/nacserver&#34;&gt;nacserver&lt;/a&gt; and running &lt;code&gt;build.sh&lt;/code&gt; on a non-M1 Mac. It should place the generated file in the current directory, which you can then copy to the emulated/ folder in pypush.&lt;/p&gt; &#xA;&lt;h2&gt;Licensing&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the terms of the &lt;a href=&#34;https://www.mongodb.com/licensing/server-side-public-license&#34;&gt;SSPL&lt;/a&gt;. Portions of this project are based on &lt;a href=&#34;https://github.com/aaronst/macholibre/raw/master/LICENSE&#34;&gt;macholibre by Aaron Stephens&lt;/a&gt; under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;p&gt;This project has been purchased by &lt;a href=&#34;https://github.com/beeper&#34;&gt;Beeper&lt;/a&gt;, please contact them with any questions about licensing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ml-explore/mlx-examples</title>
    <updated>2023-12-09T01:25:28Z</updated>
    <id>tag:github.com,2023-12-09:/ml-explore/mlx-examples</id>
    <link href="https://github.com/ml-explore/mlx-examples" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Examples in the MLX framework&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MLX Examples&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains a variety of standalone examples using the &lt;a href=&#34;https://github.com/ml-explore/mlx&#34;&gt;MLX framework&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/mnist&#34;&gt;MNIST&lt;/a&gt; example is a good starting point to learn how to use MLX.&lt;/p&gt; &#xA;&lt;p&gt;Some more useful examples include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/transformer_lm&#34;&gt;Transformer language model&lt;/a&gt; training.&lt;/li&gt; &#xA; &lt;li&gt;Large scale text generation with &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/llama&#34;&gt;LLaMA&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/mistral&#34;&gt;Mistral&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Parameter efficient fine-tuning with &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/lora&#34;&gt;LoRA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Generating images with &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/stable_diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Speech recognition with &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/whisper&#34;&gt;OpenAI&#39;s Whisper&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://raw.githubusercontent.com/ml-explore/mlx-examples/main/CONTRIBUTING.md&#34;&gt;contribution guidelines&lt;/a&gt; for more information on contributing to this repo.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yformer/EfficientSAM</title>
    <updated>2023-12-09T01:25:28Z</updated>
    <id>tag:github.com,2023-12-09:/yformer/EfficientSAM</id>
    <link href="https://github.com/yformer/EfficientSAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EfficientSAM&lt;/h1&gt; &#xA;&lt;p&gt;EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;[Dec.6 2023] EfficientSAM demo is available on the &lt;a href=&#34;https://huggingface.co/spaces/yunyangx/EfficientSAM&#34;&gt;Hugging Face Space&lt;/a&gt; (huge thanks to all the HF team for their support).&lt;/p&gt; &#xA;&lt;p&gt;[Dec.5 2023] We release the torchscript version of EfficientSAM and share a colab.&lt;/p&gt; &#xA;&lt;h2&gt;Online Demo &amp;amp; Examples&lt;/h2&gt; &#xA;&lt;p&gt;Online demo and examples can be found in the &lt;a href=&#34;https://yformer.github.io/efficient-sam/&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;EfficientSAM Instance Segmentation Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Point-prompt&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yformer/EfficientSAM/main/figs/examples/demo_point.png&#34; alt=&#34;point-prompt&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Box-prompt&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yformer/EfficientSAM/main/figs/examples/demo_box.png&#34; alt=&#34;box-prompt&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Segment everything&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yformer/EfficientSAM/main/figs/examples/demo_everything.png&#34; alt=&#34;segment everything&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Saliency&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/yformer/EfficientSAM/main/figs/examples/demo_saliency.png&#34; alt=&#34;Saliency&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Model&lt;/h2&gt; &#xA;&lt;p&gt;Models for GPU/CPU are available at the file folder of &lt;a href=&#34;https://huggingface.co/spaces/yunyangx/EfficientSAM/&#34;&gt;Hugging Face Space&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;EfficientSAM-S&lt;/th&gt; &#xA;   &lt;th&gt;EfficientSAM-Ti&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/ziif8xudwbyyphb4tohza/efficientsam_s_gpu.jit?rlkey=8aflq9kf0bfujz5ex4lxuoq56&amp;amp;dl=0&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.dropbox.com/scl/fi/lup5s4gthmlv6qf3f5zz3/efficientsam_ti_gpu.jit?rlkey=pap1xktxw50qiaey17no16bqz&amp;amp;dl=0&#34;&gt;Download&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can directly use EfficientSAM,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import torch&#xA;&#xA;efficientsam = torch.jit.load(efficientsam_s_gpu.jit)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Colab&lt;/h2&gt; &#xA;&lt;p&gt;The colab is shared &lt;a href=&#34;https://colab.research.google.com/drive/150dvh_lwbliC3020fWO9qASgy-so6sUZ?usp=sharing&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;SAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM&#34;&gt;MobileSAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CASIA-IVA-Lab/FastSAM&#34;&gt;FastSAM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xuebinqin/U-2-Net&#34;&gt;U-2-Net&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;re using EfficientSAM in your research or applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;&#xA;&#xA;@article{xiong2023efficientsam,&#xA;  title={EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything},&#xA;  author={Yunyang Xiong, Bala Varadarajan, Lemeng Wu, Xiaoyu Xiang, Fanyi Xiao, Chenchen Zhu, Xiaoliang Dai, Dilin Wang, Fei Sun, Forrest Iandola, Raghuraman Krishnamoorthi, Vikas Chandra},&#xA;  journal={arXiv:2312.00863},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>