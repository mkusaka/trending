<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-22T01:29:27Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mudler/LocalAI</title>
    <updated>2024-04-22T01:29:27Z</updated>
    <id>tag:github.com,2024-04-22:/mudler/LocalAI</id>
    <link href="https://github.com/mudler/LocalAI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü§ñ The free, Open Source OpenAI alternative. Self-hosted, community-driven and local-first. Drop-in replacement for OpenAI running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more models architectures. It allows to generate Text, Audio, Video, Images. Also with voice cloning capabilities.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;img height=&#34;300&#34; src=&#34;https://github.com/go-skynet/LocalAI/assets/2420543/0966aa2a-166e-4f99-a3e5-6c915fc997dd&#34;&gt; &lt;br&gt; LocalAI &lt;br&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/fork&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge&#34; alt=&#34;LocalAI forks&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/stargazers&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge&#34; alt=&#34;LocalAI stars&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/pulls&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge&#34; alt=&#34;LocalAI pull-requests&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/releases&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/go-skynet/LocalAI?&amp;amp;label=Latest&amp;amp;style=for-the-badge&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://hub.docker.com/r/localai/localai&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker&#34; alt=&#34;LocalAI Docker hub&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://quay.io/repository/go-skynet/local-ai?tab=tags&amp;amp;tag=latest&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/quay.io-images-important.svg?&#34; alt=&#34;LocalAI Quay.io&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://twitter.com/LocalAI_API&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/twitter/follow/LocalAI_API?label=Follow: LocalAI_API&amp;amp;style=social&#34; alt=&#34;Follow LocalAI_API&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/uJAeKSAGDy&#34; target=&#34;blank&#34;&gt; &lt;img src=&#34;https://dcbadge.vercel.app/api/server/uJAeKSAGDy?style=flat-square&amp;amp;theme=default-inverted&#34; alt=&#34;Join LocalAI Discord Community&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;span&gt;üí°&lt;/span&gt; Get help - &lt;a href=&#34;https://localai.io/faq/&#34;&gt;‚ùìFAQ&lt;/a&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/discussions&#34;&gt;üí≠Discussions&lt;/a&gt; &lt;a href=&#34;https://discord.gg/uJAeKSAGDy&#34;&gt;&lt;span&gt;üí¨&lt;/span&gt; Discord&lt;/a&gt; &lt;a href=&#34;https://localai.io/&#34;&gt;&lt;span&gt;üìñ&lt;/span&gt; Documentation website&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://localai.io/basics/getting_started/&#34;&gt;üíª Quickstart&lt;/a&gt; &lt;a href=&#34;https://localai.io/basics/news/&#34;&gt;üì£ News&lt;/a&gt; &lt;a href=&#34;https://github.com/go-skynet/LocalAI/tree/master/examples/&#34;&gt; üõ´ Examples &lt;/a&gt; &lt;a href=&#34;https://localai.io/models/&#34;&gt; üñºÔ∏è Models &lt;/a&gt; &lt;a href=&#34;https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap&#34;&gt; üöÄ Roadmap &lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/test.yml&#34;&gt;&lt;img src=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/test.yml/badge.svg?sanitize=true&#34; alt=&#34;tests&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml&#34;&gt;&lt;img src=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml/badge.svg?sanitize=true&#34; alt=&#34;Build and Release&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/image.yml&#34;&gt;&lt;img src=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/image.yml/badge.svg?sanitize=true&#34; alt=&#34;build container images&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml&#34;&gt;&lt;img src=&#34;https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml/badge.svg?sanitize=true&#34; alt=&#34;Bump dependencies&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://artifacthub.io/packages/search?repo=localai&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/localai&#34; alt=&#34;Artifact Hub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LocalAI&lt;/strong&gt; is the free, Open Source OpenAI alternative. LocalAI act as a drop-in replacement REST API that‚Äôs compatible with OpenAI (Elevenlabs, Anthropic... ) API specifications for local AI inferencing. It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families. Does not require GPU.&lt;/p&gt; &#xA;&lt;h2&gt;üî•üî• Hot topics / Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;llama3: &lt;a href=&#34;https://github.com/mudler/LocalAI/discussions/2076&#34;&gt;https://github.com/mudler/LocalAI/discussions/2076&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Parler-TTS: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/2027&#34;&gt;https://github.com/mudler/LocalAI/pull/2027&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Landing page: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/1922&#34;&gt;https://github.com/mudler/LocalAI/pull/1922&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Openvino support: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/1892&#34;&gt;https://github.com/mudler/LocalAI/pull/1892&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vector store: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/1795&#34;&gt;https://github.com/mudler/LocalAI/pull/1795&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;All-in-one container image: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1855&#34;&gt;https://github.com/mudler/LocalAI/issues/1855&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Parallel function calling: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/1726&#34;&gt;https://github.com/mudler/LocalAI/pull/1726&lt;/a&gt; / Tools API support: &lt;a href=&#34;https://github.com/mudler/LocalAI/pull/1715&#34;&gt;https://github.com/mudler/LocalAI/pull/1715&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Hot topics (looking for contributors):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Backends v2: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1126&#34;&gt;https://github.com/mudler/LocalAI/issues/1126&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improving UX v2: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1373&#34;&gt;https://github.com/mudler/LocalAI/issues/1373&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Assistant API: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1273&#34;&gt;https://github.com/mudler/LocalAI/issues/1273&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Moderation endpoint: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/999&#34;&gt;https://github.com/mudler/LocalAI/issues/999&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vulkan: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues/1647&#34;&gt;https://github.com/mudler/LocalAI/issues/1647&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to help and contribute, issues up for grabs: &lt;a href=&#34;https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3A%22up+for+grabs%22&#34;&gt;https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3A%22up+for+grabs%22&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üíª &lt;a href=&#34;https://localai.io/basics/getting_started/index.html&#34;&gt;Getting started&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;For a detailed step-by-step introduction, refer to the &lt;a href=&#34;https://localai.io/basics/getting_started/index.html&#34;&gt;Getting Started&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;p&gt;For those in a hurry, here&#39;s a straightforward one-liner to launch a LocalAI AIO(All-in-one) Image using &lt;code&gt;docker&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu&#xA;# or, if you have an Nvidia GPU:&#xA;# docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ &lt;a href=&#34;https://localai.io/features/&#34;&gt;Features&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìñ &lt;a href=&#34;https://localai.io/features/text-generation/&#34;&gt;Text generation with GPTs&lt;/a&gt; (&lt;code&gt;llama.cpp&lt;/code&gt;, &lt;code&gt;gpt4all.cpp&lt;/code&gt;, ... &lt;a href=&#34;https://localai.io/model-compatibility/index.html#model-compatibility-table&#34;&gt;&lt;span&gt;üìñ&lt;/span&gt; and more&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;üó£ &lt;a href=&#34;https://localai.io/features/text-to-audio/&#34;&gt;Text to Audio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üîà &lt;a href=&#34;https://localai.io/features/audio-to-text/&#34;&gt;Audio to Text&lt;/a&gt; (Audio transcription with &lt;code&gt;whisper.cpp&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;üé® &lt;a href=&#34;https://localai.io/features/image-generation&#34;&gt;Image generation with stable diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üî• &lt;a href=&#34;https://localai.io/features/openai-functions/&#34;&gt;OpenAI functions&lt;/a&gt; üÜï&lt;/li&gt; &#xA; &lt;li&gt;üß† &lt;a href=&#34;https://localai.io/features/embeddings/&#34;&gt;Embeddings generation for vector databases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úçÔ∏è &lt;a href=&#34;https://localai.io/features/constrained_grammars/&#34;&gt;Constrained grammars&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üñºÔ∏è &lt;a href=&#34;https://localai.io/models/&#34;&gt;Download Models directly from Huggingface &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üÜï &lt;a href=&#34;https://localai.io/features/gpt-vision/&#34;&gt;Vision API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíª Usage&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://localai.io/basics/getting_started/index.html&#34;&gt;Getting started&lt;/a&gt; section in our documentation.&lt;/p&gt; &#xA;&lt;h3&gt;üîó Community and integrations&lt;/h3&gt; &#xA;&lt;p&gt;Build and deploy custom containers:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sozercan/aikit&#34;&gt;https://github.com/sozercan/aikit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;WebUIs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Jirubizu/localai-admin&#34;&gt;https://github.com/Jirubizu/localai-admin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/go-skynet/LocalAI-frontend&#34;&gt;https://github.com/go-skynet/LocalAI-frontend&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Model galleries&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/go-skynet/model-gallery&#34;&gt;https://github.com/go-skynet/model-gallery&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Helm chart &lt;a href=&#34;https://github.com/go-skynet/helm-charts&#34;&gt;https://github.com/go-skynet/helm-charts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VSCode extension &lt;a href=&#34;https://github.com/badgooooor/localai-vscode-plugin&#34;&gt;https://github.com/badgooooor/localai-vscode-plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Local Smart assistant &lt;a href=&#34;https://github.com/mudler/LocalAGI&#34;&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Home Assistant &lt;a href=&#34;https://github.com/sammcj/homeassistant-localai&#34;&gt;https://github.com/sammcj/homeassistant-localai&lt;/a&gt; / &lt;a href=&#34;https://github.com/drndos/hass-openai-custom-conversation&#34;&gt;https://github.com/drndos/hass-openai-custom-conversation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Discord bot &lt;a href=&#34;https://github.com/mudler/LocalAGI/tree/main/examples/discord&#34;&gt;https://github.com/mudler/LocalAGI/tree/main/examples/discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Slack bot &lt;a href=&#34;https://github.com/mudler/LocalAGI/tree/main/examples/slack&#34;&gt;https://github.com/mudler/LocalAGI/tree/main/examples/slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Telegram bot &lt;a href=&#34;https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot&#34;&gt;https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Examples: &lt;a href=&#34;https://github.com/mudler/LocalAI/tree/master/examples/&#34;&gt;https://github.com/mudler/LocalAI/tree/master/examples/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üîó Resources&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üÜï New! &lt;a href=&#34;https://localai.io/docs/advanced/fine-tuning/&#34;&gt;LLM finetuning guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://localai.io/basics/build/index.html&#34;&gt;How to build locally&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://localai.io/basics/getting_started/index.html#run-localai-in-kubernetes&#34;&gt;How to install in Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://localai.io/docs/integrations/&#34;&gt;Projects integrating LocalAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://io.midori-ai.xyz/howtos/&#34;&gt;How tos section&lt;/a&gt; (curated by our community)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìñ&lt;/span&gt; üé• &lt;a href=&#34;https://localai.io/basics/news/#media-blogs-social&#34;&gt;Media, Blogs, Social&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pulumi.com/ai/answers/tiZMDoZzZV6TLxgDXNBnFE/deploying-helm-charts-on-aws-eks&#34;&gt;Run LocalAI on AWS EKS with Pulumi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://staleks.hashnode.dev/installing-localai-on-aws-ec2-instance&#34;&gt;Run LocalAI on AWS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mudler.pm/posts/smart-slackbot-for-teams/&#34;&gt;Create a slackbot for teams and OSS projects that answer to documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=PKrDNuJ_dfE&#34;&gt;LocalAI meets k8sgpt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mudler.pm/posts/localai-question-answering/&#34;&gt;Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@tyler_97636/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65&#34;&gt;Tutorial to use k8sgpt with LocalAI&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you utilize this repository, data in a downstream project, please consider citing it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{localai,&#xA;  author = {Ettore Di Giacinto},&#xA;  title = {LocalAI: The free, Open source OpenAI alternative},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/go-skynet/LocalAI}},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚ù§Ô∏è Sponsors&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Do you find LocalAI useful?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Support the project by becoming &lt;a href=&#34;https://github.com/sponsors/mudler&#34;&gt;a backer or sponsor&lt;/a&gt;. Your logo will show up here with a link to your website.&lt;/p&gt; &#xA;&lt;p&gt;A huge thank you to our generous sponsors who support this project:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/go-skynet/LocalAI/assets/2420543/68a6f3cb-8a65-4a4d-99b5-6417a8905512&#34; alt=&#34;Spectro Cloud logo_600x600px_transparent bg&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.spectrocloud.com/&#34;&gt;Spectro Cloud&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Spectro Cloud kindly supports LocalAI by providing GPU and computing resources to run tests on lamdalabs!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;And a huge shout-out to individuals sponsoring the project by donating hardware or backing the project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sponsors/mudler&#34;&gt;Sponsor list&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;JDAM00 (donating HW for the CI)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåü Star history&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#go-skynet/LocalAI&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=go-skynet/LocalAI&amp;amp;type=Date&#34; alt=&#34;LocalAI Star history Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìñ License&lt;/h2&gt; &#xA;&lt;p&gt;LocalAI is a community-driven project created by &lt;a href=&#34;https://github.com/mudler/&#34;&gt;Ettore Di Giacinto&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;MIT - Author Ettore Di Giacinto&lt;/p&gt; &#xA;&lt;h2&gt;üôá Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;LocalAI couldn&#39;t have been built without the help of great software already available from the community. Thank you!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cornelk/llama-go&#34;&gt;https://github.com/cornelk/llama-go&lt;/a&gt; for the initial ideas&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/antimatter15/alpaca.cpp&#34;&gt;https://github.com/antimatter15/alpaca.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EdVince/Stable-Diffusion-NCNN&#34;&gt;https://github.com/EdVince/Stable-Diffusion-NCNN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;https://github.com/ggerganov/whisper.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/saharNooby/rwkv.cpp&#34;&gt;https://github.com/saharNooby/rwkv.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rhasspy/piper&#34;&gt;https://github.com/rhasspy/piper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ó Contributors&lt;/h2&gt; &#xA;&lt;p&gt;This is a community project, a special thanks to our contributors! ü§ó &lt;a href=&#34;https://github.com/go-skynet/LocalAI/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=go-skynet/LocalAI&#34;&gt; &lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>meta-llama/codellama</title>
    <updated>2024-04-22T01:29:27Z</updated>
    <id>tag:github.com,2024-04-22:/meta-llama/codellama</id>
    <link href="https://github.com/meta-llama/codellama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference code for CodeLlama models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Introducing Code Llama&lt;/h1&gt; &#xA;&lt;p&gt;Code Llama is a family of large language models for code based on &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;Llama 2&lt;/a&gt; providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama was developed by fine-tuning Llama 2 using a higher sampling of code. As with Llama 2, we applied considerable safety mitigations to the fine-tuned versions of the model. For detailed information on model training, architecture and parameters, evaluations, responsible AI and safety refer to our &lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;research paper&lt;/a&gt;. Output generated by code generation features of the Llama Materials, including Code Llama, may be subject to third party licenses, including, without limitation, open source licenses.&lt;/p&gt; &#xA;&lt;p&gt;We are unlocking the power of large language models and our latest version of Code Llama is now accessible to individuals, creators, researchers and businesses of all sizes so that they can experiment, innovate and scale their ideas responsibly. This release includes model weights and starting code for pretrained and fine-tuned Llama language models ‚Äî ranging from 7B to 34B parameters.&lt;/p&gt; &#xA;&lt;p&gt;This repository is intended as a minimal example to load &lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;Code Llama&lt;/a&gt; models and run inference.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;In order to download the model weights and tokenizers, please visit the &lt;a href=&#34;https://ai.meta.com/resources/models-and-libraries/llama-downloads/&#34;&gt;Meta website&lt;/a&gt; and accept our License.&lt;/p&gt; &#xA;&lt;p&gt;Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Make sure that you copy the URL text itself, &lt;strong&gt;do not use the &#39;Copy link address&#39; option&lt;/strong&gt; when you right click the URL. If the copied URL text starts with: &lt;a href=&#34;https://download.llamameta.net&#34;&gt;https://download.llamameta.net&lt;/a&gt;, you copied it correctly. If the copied URL text starts with: &lt;a href=&#34;https://l.facebook.com&#34;&gt;https://l.facebook.com&lt;/a&gt;, you copied it the wrong way.&lt;/p&gt; &#xA;&lt;p&gt;Pre-requisites: make sure you have &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;md5sum&lt;/code&gt; installed. Then to run the script: &lt;code&gt;bash download.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as &lt;code&gt;403: Forbidden&lt;/code&gt;, you can always re-request a link.&lt;/p&gt; &#xA;&lt;h3&gt;Model sizes&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;~12.55GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;24GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;34B&lt;/td&gt; &#xA;   &lt;td&gt;63GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;131GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;In a conda environment with PyTorch / CUDA available, clone the repo and run in the top-level directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Different models require different model-parallel (MP) values:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;34B&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All models, except the 70B python and instruct versions, support sequence lengths up to 100,000 tokens, but we pre-allocate the cache according to &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; values. So set those according to your hardware and use-case.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Code Models&lt;/h3&gt; &#xA;&lt;p&gt;The Code Llama and Code Llama - Python models are not fine-tuned to follow instructions. They should be prompted so that the expected answer is the natural continuation of the prompt.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_completion.py&lt;/code&gt; for some examples. To illustrate, see command below to run it with the &lt;code&gt;CodeLlama-7b&lt;/code&gt; model (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_completion.py \&#xA;    --ckpt_dir CodeLlama-7b/ \&#xA;    --tokenizer_path CodeLlama-7b/tokenizer.model \&#xA;    --max_seq_len 128 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pretrained code models are: the Code Llama models &lt;code&gt;CodeLlama-7b&lt;/code&gt;, &lt;code&gt;CodeLlama-13b&lt;/code&gt;, &lt;code&gt;CodeLlama-34b&lt;/code&gt;, &lt;code&gt;CodeLlama-70b&lt;/code&gt; and the Code Llama - Python models &lt;code&gt;CodeLlama-7b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-34b-Python&lt;/code&gt;, &lt;code&gt;CodeLlama-70b-Python&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Code Infilling&lt;/h3&gt; &#xA;&lt;p&gt;Code Llama and Code Llama - Instruct 7B and 13B models are capable of filling in code given the surrounding context.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_infilling.py&lt;/code&gt; for some examples. The &lt;code&gt;CodeLlama-7b&lt;/code&gt; model can be run for infilling with the command below (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_infilling.py \&#xA;    --ckpt_dir CodeLlama-7b/ \&#xA;    --tokenizer_path CodeLlama-7b/tokenizer.model \&#xA;    --max_seq_len 192 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Pretrained infilling models are: the Code Llama models &lt;code&gt;CodeLlama-7b&lt;/code&gt; and &lt;code&gt;CodeLlama-13b&lt;/code&gt; and the Code Llama - Instruct models &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Fine-tuned Instruction Models&lt;/h3&gt; &#xA;&lt;p&gt;Code Llama - Instruct models are fine-tuned to follow instructions. To get the expected features and performance for the 7B, 13B and 34B variants, a specific formatting defined in &lt;a href=&#34;https://github.com/facebookresearch/codellama/raw/main/llama/generation.py#L319-L361&#34;&gt;&lt;code&gt;chat_completion()&lt;/code&gt;&lt;/a&gt; needs to be followed, including the &lt;code&gt;INST&lt;/code&gt; and &lt;code&gt;&amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&lt;/code&gt; tags, &lt;code&gt;BOS&lt;/code&gt; and &lt;code&gt;EOS&lt;/code&gt; tokens, and the whitespaces and linebreaks in between (we recommend calling &lt;code&gt;strip()&lt;/code&gt; on inputs to avoid double-spaces). &lt;code&gt;CodeLlama-70b-Instruct&lt;/code&gt; requires a separate turn-based prompt format defined in &lt;a href=&#34;https://github.com/facebookresearch/codellama/raw/main/llama/generation.py#L506-L548&#34;&gt;&lt;code&gt;dialog_prompt_tokens()&lt;/code&gt;&lt;/a&gt;. You can use &lt;code&gt;chat_completion()&lt;/code&gt; directly to generate answers with all instruct models; it will automatically perform the required formatting.&lt;/p&gt; &#xA;&lt;p&gt;You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes/raw/main/src/llama_recipes/inference/safety_utils.py&#34;&gt;an example&lt;/a&gt; of how to add a safety checker to the inputs and outputs of your inference code.&lt;/p&gt; &#xA;&lt;p&gt;Examples using &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_instructions.py \&#xA;    --ckpt_dir CodeLlama-7b-Instruct/ \&#xA;    --tokenizer_path CodeLlama-7b-Instruct/tokenizer.model \&#xA;    --max_seq_len 512 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fine-tuned instruction-following models are: the Code Llama - Instruct models &lt;code&gt;CodeLlama-7b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-13b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-34b-Instruct&lt;/code&gt;, &lt;code&gt;CodeLlama-70b-Instruct&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Code Llama is a new technology that carries potential risks with use. Testing conducted to date has not ‚Äî and could not ‚Äî cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/Responsible-Use-Guide.pdf&#34;&gt;Responsible Use Guide&lt;/a&gt;. More details can be found in our research papers as well.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please report any software ‚Äúbug‚Äù, or other problems with the models through one of the following means:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reporting issues with the model: &lt;a href=&#34;http://github.com/facebookresearch/codellama&#34;&gt;github.com/facebookresearch/codellama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting risky content generated by the model: &lt;a href=&#34;http://developers.facebook.com/llama_output_feedback&#34;&gt;developers.facebook.com/llama_output_feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting bugs and security concerns: &lt;a href=&#34;http://facebook.com/whitehat/info&#34;&gt;facebook.com/whitehat/info&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Card&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/codellama/main/MODEL_CARD.md&#34;&gt;MODEL_CARD.md&lt;/a&gt; for the model card of Code Llama.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file, as well as our accompanying &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/USE_POLICY.md&#34;&gt;Acceptable Use Policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;Code Llama Research Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/blog/code-llama-large-language-model-coding/&#34;&gt;Code Llama Blog Post&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>meta-llama/llama3</title>
    <updated>2024-04-22T01:29:27Z</updated>
    <id>tag:github.com,2024-04-22:/meta-llama/llama3</id>
    <link href="https://github.com/meta-llama/llama3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official Meta Llama 3 GitHub site&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/meta-llama/llama3/raw/main/Llama3_Repo.jpeg&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/meta-Llama&#34;&gt; Models on Hugging Face&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/blog/&#34;&gt; Blog&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://llama.meta.com/&#34;&gt;Website&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://llama.meta.com/get-started/&#34;&gt;Get Started&lt;/a&gt;&amp;nbsp; &lt;br&gt; &lt;/p&gt;&#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Meta Llama 3&lt;/h1&gt; &#xA;&lt;p&gt;We are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers, and businesses of all sizes so that they can experiment, innovate, and scale their ideas responsibly.&lt;/p&gt; &#xA;&lt;p&gt;This release includes model weights and starting code for pre-trained and instruction tuned Llama 3 language models ‚Äî including sizes of 8B to 70B parameters.&lt;/p&gt; &#xA;&lt;p&gt;This repository is intended as a minimal example to load Llama 3 models and run inference. For more detailed examples, see &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes/&#34;&gt;llama-recipes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;In order to download the model weights and tokenizer, please visit the &lt;a href=&#34;https://llama.meta.com/llama-downloads/&#34;&gt;Meta Llama website&lt;/a&gt; and accept our License.&lt;/p&gt; &#xA;&lt;p&gt;Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download.&lt;/p&gt; &#xA;&lt;p&gt;Pre-requisites: Make sure you have &lt;code&gt;wget&lt;/code&gt; and &lt;code&gt;md5sum&lt;/code&gt; installed. Then run the script: &lt;code&gt;./download.sh&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as &lt;code&gt;403: Forbidden&lt;/code&gt;, you can always re-request a link.&lt;/p&gt; &#xA;&lt;h3&gt;Access to Hugging Face&lt;/h3&gt; &#xA;&lt;p&gt;We are also providing downloads on &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Hugging Face&lt;/a&gt;, in both transformers and native &lt;code&gt;llama3&lt;/code&gt; formats. To download the weights from Hugging Face, please follow these steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visit one of the repos, for example &lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct&#34;&gt;meta-llama/Meta-Llama-3-8B-Instruct&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Read and accept the license. Once your request is approved, you&#39;ll be granted access to all the Llama 3 models. Note that requests use to take up to one hour to get processed.&lt;/li&gt; &#xA; &lt;li&gt;To download the original native weights to use with this repo, click on the &#34;Files and versions&#34; tab and download the contents of the &lt;code&gt;original&lt;/code&gt; folder. You can also download them from the command line if you &lt;code&gt;pip install huggingface-hub&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include &#34;original/*&#34; --local-dir meta-llama/Meta-Llama-3-8B-Instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To use with transformers, the following &lt;a href=&#34;https://huggingface.co/docs/transformers/en/main_classes/pipelines&#34;&gt;pipeline&lt;/a&gt; snippet will download and cache the weights:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import transformers&#xA;import torch&#xA;&#xA;model_id = &#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;&#xA;&#xA;pipeline = transformers.pipeline(&#xA;  &#34;text-generation&#34;,&#xA;  model=&#34;meta-llama/Meta-Llama-3-8B-Instruct&#34;,&#xA;  model_kwargs={&#34;torch_dtype&#34;: torch.bfloat16},&#xA;  device=&#34;cuda&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You can follow the steps below to quickly get up and running with Llama 3 models. These steps will let you run quick inference locally. For more examples, see the &lt;a href=&#34;https://github.com/facebookresearch/llama-recipes&#34;&gt;Llama recipes repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;In a conda env with PyTorch / CUDA available clone and download this repository.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the top-level directory run:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Visit the &lt;a href=&#34;https://llama.meta.com/llama-downloads/&#34;&gt;Meta Llama website&lt;/a&gt; and register to download the model/s.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once you get the email, navigate to your downloaded llama repository and run the download.sh script.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Make sure to grant execution permissions to the download.sh script&lt;/li&gt; &#xA;   &lt;li&gt;During this process, you will be prompted to enter the URL from the email.&lt;/li&gt; &#xA;   &lt;li&gt;Do not use the ‚ÄúCopy Link‚Äù option but rather make sure to manually copy the link from the email.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once the model/s you want have been downloaded, you can run the model locally using the command below:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node 1 example_chat_completion.py \&#xA;    --ckpt_dir Meta-Llama-3-8B-Instruct/ \&#xA;    --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \&#xA;    --max_seq_len 512 --max_batch_size 6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Replace &lt;code&gt;Meta-Llama-3-8B-Instruct/&lt;/code&gt; with the path to your checkpoint directory and &lt;code&gt;Meta-Llama-3-8B-Instruct/tokenizer.model&lt;/code&gt; with the path to your tokenizer model.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;‚Äìnproc_per_node&lt;/code&gt; should be set to the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama3/main/#inference&#34;&gt;MP&lt;/a&gt; value for the model you are using.&lt;/li&gt; &#xA; &lt;li&gt;Adjust the &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; parameters as needed.&lt;/li&gt; &#xA; &lt;li&gt;This example runs the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama3/main/example_chat_completion.py&#34;&gt;example_chat_completion.py&lt;/a&gt; found in this repository but you can change that to a different .py file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Different models require different model-parallel (MP) values:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MP&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;All models support sequence length up to 8192 tokens, but we pre-allocate the cache according to &lt;code&gt;max_seq_len&lt;/code&gt; and &lt;code&gt;max_batch_size&lt;/code&gt; values. So set those according to your hardware.&lt;/p&gt; &#xA;&lt;h3&gt;Pretrained Models&lt;/h3&gt; &#xA;&lt;p&gt;These models are not finetuned for chat or Q&amp;amp;A. They should be prompted so that the expected answer is the natural continuation of the prompt.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;code&gt;example_text_completion.py&lt;/code&gt; for some examples. To illustrate, see the command below to run it with the llama-3-8b model (&lt;code&gt;nproc_per_node&lt;/code&gt; needs to be set to the &lt;code&gt;MP&lt;/code&gt; value):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_text_completion.py \&#xA;    --ckpt_dir Meta-Llama-3-8B/ \&#xA;    --tokenizer_path Meta-Llama-3-8B/tokenizer.model \&#xA;    --max_seq_len 128 --max_batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Instruction-tuned Models&lt;/h3&gt; &#xA;&lt;p&gt;The fine-tuned models were trained for dialogue applications. To get the expected features and performance for them, a specific formatting defined in &lt;a href=&#34;https://github.com/meta-llama/llama3/raw/main/llama/tokenizer.py#L202&#34;&gt;&lt;code&gt;ChatFormat&lt;/code&gt;&lt;/a&gt; needs to be followed: The prompt begins with a &lt;code&gt;&amp;lt;|begin_of_text|&amp;gt;&lt;/code&gt; special token, after which one or more messages follow. Each message starts with the &lt;code&gt;&amp;lt;|start_header_id|&amp;gt;&lt;/code&gt; tag, the role &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt; or &lt;code&gt;assistant&lt;/code&gt;, and the &lt;code&gt;&amp;lt;|end_header_id|&amp;gt;&lt;/code&gt; tag. After a double newline &lt;code&gt;\n\n&lt;/code&gt; the contents of the message follow. The end of each message is marked by the &lt;code&gt;&amp;lt;|eot_id|&amp;gt;&lt;/code&gt; token.&lt;/p&gt; &#xA;&lt;p&gt;You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for &lt;a href=&#34;https://github.com/meta-llama/llama-recipes/raw/main/recipes/inference/local_inference/inference.py&#34;&gt;an example&lt;/a&gt; of how to add a safety checker to the inputs and outputs of your inference code.&lt;/p&gt; &#xA;&lt;p&gt;Examples using llama-3-8b-chat:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node 1 example_chat_completion.py \&#xA;    --ckpt_dir Meta-Llama-3-8B-Instruct/ \&#xA;    --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \&#xA;    --max_seq_len 512 --max_batch_size 6&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Llama 3 is a new technology that carries potential risks with use. Testing conducted to date has not ‚Äî and could not ‚Äî cover all scenarios. In order to help developers address these risks, we have created the &lt;a href=&#34;https://ai.meta.com/static-resource/responsible-use-guide/&#34;&gt;Responsible Use Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please report any software ‚Äúbug‚Äù, or other problems with the models through one of the following means:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reporting issues with the model: &lt;a href=&#34;https://github.com/meta-llama/llama3/issues&#34;&gt;https://github.com/meta-llama/llama3/issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting risky content generated by the model: &lt;a href=&#34;http://developers.facebook.com/llama_output_feedback&#34;&gt;developers.facebook.com/llama_output_feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting bugs and security concerns: &lt;a href=&#34;http://facebook.com/whitehat/info&#34;&gt;facebook.com/whitehat/info&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model Card&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama3/main/MODEL_CARD.md&#34;&gt;MODEL_CARD.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama3/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file, as well as our accompanying &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama3/main/USE_POLICY.md&#34;&gt;Acceptable Use Policy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Questions&lt;/h2&gt; &#xA;&lt;p&gt;For common questions, the FAQ can be found &lt;a href=&#34;https://llama.meta.com/faq&#34;&gt;here&lt;/a&gt; which will be kept up to date over time as new questions arise.&lt;/p&gt;</summary>
  </entry>
</feed>