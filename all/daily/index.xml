<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-05T01:26:25Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>TabbyML/tabby</title>
    <updated>2023-12-05T01:26:25Z</updated>
    <id>tag:github.com,2023-12-05:/TabbyML/tabby</id>
    <link href="https://github.com/TabbyML/tabby" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Self-hosted AI coding assistant&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;ğŸ¾ Tabby&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/TabbyML/tabby/releases/latest&#34;&gt;&lt;img src=&#34;https://shields.io/github/v/release/TabbyML/tabby?sort=semver&#34; alt=&#34;latest release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://makeapullrequest.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/tabbyml/tabby&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/tabbyml/tabby&#34; alt=&#34;Docker pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://join.slack.com/t/tabbycommunity/shared_invite/zt-1xeiddizp-bciR2RtFTaJ37RBxr8VxpA&#34;&gt;&lt;img src=&#34;https://shields.io/badge/Join-Tabby%20Slack-red?logo=slack&#34; alt=&#34;Slack Community&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://calendly.com/tabby_ml/chat-with-tabbyml&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Book-Office%20Hours-purple?logo=googlecalendar&amp;amp;logoColor=white&#34; alt=&#34;Office Hours&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Tabby is a self-hosted AI coding assistant, offering an open-source and on-premises alternative to GitHub Copilot. It boasts several key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Self-contained, with no need for a DBMS or cloud service.&lt;/li&gt; &#xA; &lt;li&gt;OpenAPI interface, easy to integrate with existing infrastructure (e.g Cloud IDE).&lt;/li&gt; &#xA; &lt;li&gt;Supports consumer-grade GPUs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://tabbyml.github.io/tabby/playground&#34;&gt;&lt;img alt=&#34;Open in Playground&#34; src=&#34;https://img.shields.io/badge/OPEN%20IN%20PLAYGROUND-blue?logo=xcode&amp;amp;style=for-the-badge&amp;amp;logoColor=green&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Demo&#34; src=&#34;https://user-images.githubusercontent.com/388154/230440226-9bc01d05-9f57-478b-b04d-81184eba14ca.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ğŸ”¥ What&#39;s New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;11/27/2023&lt;/strong&gt; &lt;a href=&#34;https://github.com/TabbyML/tabby/releases/tag/v0.6.0&#34;&gt;v0.6.0&lt;/a&gt; released!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;10/24/2023&lt;/strong&gt; â›³ï¸ Major updates for Tabby IDE plugins across &lt;a href=&#34;https://tabby.tabbyml.com/docs/extensions&#34;&gt;VSCode/Vim/IntelliJ&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;10/15/2023&lt;/strong&gt; RAG-based code completion is enabled by detail in &lt;a href=&#34;https://github.com/TabbyML/tabby/releases/tag/v0.3.0&#34;&gt;v0.3.0&lt;/a&gt;ğŸ‰! Check out the &lt;a href=&#34;https://tabby.tabbyml.com/blog/2023/10/16/repository-context-for-code-completion/&#34;&gt;blogpost&lt;/a&gt; explaining how Tabby utilizes repo-level context to get even smarter!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Archived&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;strong&gt;11/09/2023&lt;/strong&gt; &lt;a href=&#34;https://github.com/TabbyML/tabby/releases/tag/v0.5.5&#34;&gt;v0.5.5&lt;/a&gt; released! With a redesign of UI + performance improvement.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;10/04/2023&lt;/strong&gt; Check out the &lt;a href=&#34;https://tabby.tabbyml.com/docs/models/&#34;&gt;model directory&lt;/a&gt; for the latest models supported by Tabby.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;09/18/2023&lt;/strong&gt; Apple&#39;s M1/M2 Metal inference support has landed in &lt;a href=&#34;https://github.com/TabbyML/tabby/releases/tag/v0.1.1&#34;&gt;v0.1.1&lt;/a&gt;!&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;08/31/2023&lt;/strong&gt; Tabby&#39;s first stable release &lt;a href=&#34;https://github.com/TabbyML/tabby/releases/tag/v0.0.1&#34;&gt;v0.0.1&lt;/a&gt; ğŸ¥³.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;08/28/2023&lt;/strong&gt; Experimental support for the &lt;a href=&#34;https://github.com/TabbyML/tabby/issues/370&#34;&gt;CodeLlama 7B&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;08/24/2023&lt;/strong&gt; Tabby is now on &lt;a href=&#34;https://plugins.jetbrains.com/plugin/22379-tabby&#34;&gt;JetBrains Marketplace&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;ğŸ‘‹ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;You can find our documentation &lt;a href=&#34;https://tabby.tabbyml.com/docs/getting-started&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ğŸ“š &lt;a href=&#34;https://tabby.tabbyml.com/docs/installation/&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ğŸ’» &lt;a href=&#34;https://tabby.tabbyml.com/docs/extensions/&#34;&gt;IDE/Editor Extensions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;âš™ï¸ &lt;a href=&#34;https://tabby.tabbyml.com/docs/configuration&#34;&gt;Configuration&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run Tabby in 1 Minute&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to start a Tabby server is by using the following Docker command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it \&#xA;  --gpus all -p 8080:8080 -v $HOME/.tabby:/data \&#xA;  tabbyml/tabby \&#xA;  serve --model TabbyML/StarCoder-1B --device cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional options (e.g inference type, parallelism), please refer to the &lt;a href=&#34;https://tabbyml.github.io/tabby&#34;&gt;documentation page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ¤ Contributing&lt;/h2&gt; &#xA;&lt;h3&gt;Get the Code&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recurse-submodules https://github.com/TabbyML/tabby&#xA;cd tabby&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have already cloned the repository, you could run the &lt;code&gt;git submodule update --recursive --init&lt;/code&gt; command to fetch all submodules.&lt;/p&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up the Rust environment by following this &lt;a href=&#34;https://www.rust-lang.org/learn/get-started&#34;&gt;tutorial&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required dependencies:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For MacOS&#xA;brew install protobuf&#xA;&#xA;# For Ubuntu / Debian&#xA;apt-get install protobuf-compiler libopenblas-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Now, you can build Tabby by running the command &lt;code&gt;cargo build&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Start Hacking!&lt;/h3&gt; &#xA;&lt;p&gt;... and don&#39;t forget to submit a &lt;a href=&#34;https://github.com/TabbyML/tabby/compare&#34;&gt;Pull Request&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ğŸŒ Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;#ï¸âƒ£ &lt;a href=&#34;https://join.slack.com/t/tabbycommunity/shared_invite/zt-1xeiddizp-bciR2RtFTaJ37RBxr8VxpA&#34;&gt;Slack&lt;/a&gt; - connect with the TabbyML community&lt;/li&gt; &#xA; &lt;li&gt;ğŸ¤ &lt;a href=&#34;https://twitter.com/Tabby_ML&#34;&gt;Twitter / X&lt;/a&gt; - engage with TabbyML for all things possible&lt;/li&gt; &#xA; &lt;li&gt;ğŸ“š &lt;a href=&#34;https://www.linkedin.com/company/tabbyml/&#34;&gt;LinkedIn&lt;/a&gt; - follow for the latest from the community&lt;/li&gt; &#xA; &lt;li&gt;ğŸ’Œ &lt;a href=&#34;https://tinyletter.com/tabbyml/&#34;&gt;Newsletter&lt;/a&gt; - subscribe to unlock Tabby insights and secrets&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ğŸŒŸ Star History&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#tabbyml/tabby&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=tabbyml/tabby&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>unslothai/unsloth</title>
    <updated>2023-12-05T01:26:25Z</updated>
    <id>tag:github.com,2023-12-05:/unslothai/unsloth</id>
    <link href="https://github.com/unslothai/unsloth" rel="alternate"></link>
    <summary type="html">&lt;p&gt;5X faster 50% less memory LLM finetuning&lt;/p&gt;&lt;hr&gt;&lt;div class=&#34;align-center&#34;&gt; &#xA; &lt;img src=&#34;./images/unsloth new logo.png&#34; width=&#34;400&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/u54VK8m8tk&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord.png&#34; width=&#34;180&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;80% faster 50% less memory local QLoRA finetuning&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Manual autograd engine - hand derived backprop steps.&lt;/li&gt; &#xA; &lt;li&gt;QLoRA / LoRA 80% faster, 50% less memory.&lt;/li&gt; &#xA; &lt;li&gt;All kernels written in OpenAI&#39;s Triton language.&lt;/li&gt; &#xA; &lt;li&gt;0% loss in accuracy - no approximation methods - all exact.&lt;/li&gt; &#xA; &lt;li&gt;No change of hardware necessary. Supports NVIDIA GPUs since 2018+. CUDA 7.5+. Tesla T4, RTX 20, 30, 40 series, A100, H100s&lt;/li&gt; &#xA; &lt;li&gt;Flash Attention support via Xformers.&lt;/li&gt; &#xA; &lt;li&gt;Supports 4bit and 16bit LoRA finetuning.&lt;/li&gt; &#xA; &lt;li&gt;Train Slim Orca &lt;strong&gt;fully locally in 260 hours from 1301 hours (5x faster).&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open source version trains 5x faster or you can check out &lt;a href=&#34;https://unsloth.ai/&#34;&gt;Unsloth Pro and Max&lt;/a&gt; codepaths for &lt;strong&gt;30x faster training&lt;/strong&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div class=&#34;align-center&#34;&gt; &#xA; &lt;img src=&#34;./images/Slim Orca 2GPUs.png&#34; width=&#34;400&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/unslothai/unsloth/main/images/LAION%202GPU.svg?sanitize=true&#34; width=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Try our Colab examples for &lt;a href=&#34;https://colab.research.google.com/drive/1oW55fBmwzCOrBVX66RcpptL3a99qWBxb?usp=sharing&#34;&gt;the Alpaca 52K dataset&lt;/a&gt; or &lt;a href=&#34;https://colab.research.google.com/drive/1VNqLARpE8N8eYwNrUSDoHVjtbR9W0_c7?usp=sharing&#34;&gt;the Slim Orca 518K dataset&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Try our Kaggle example for &lt;a href=&#34;https://www.kaggle.com/danielhanchen/unsloth-laion-chip2-kaggle&#34;&gt;the LAION OIG Chip2 dataset&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Join our &lt;a href=&#34;https://discord.gg/nsS4V5Z6ge&#34;&gt;Discord&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Installation Instructions - Conda&lt;/h1&gt; &#xA;&lt;p&gt;Unsloth currently only supports Linux distros and Pytorch &amp;gt;= 2.1.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install cudatoolkit xformers bitsandbytes pytorch pytorch-cuda=12.1 \&#xA;  -c pytorch -c nvidia -c xformers -c conda-forge -y&#xA;pip install &#34;unsloth[kaggle] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Installation Instructions - Pip&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Find your CUDA version via&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;import torch; torch.version.cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Select either cu118 for CUDA 11.8 or cu121 for CUDA 12.1&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install &#34;unsloth[cu118] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;pip install &#34;unsloth[cu121] @ git+https://github.com/unslothai/unsloth.git&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;We only support Pytorch 2.1: You can update Pytorch via Pip:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir torch triton \&#xA;  --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Change &lt;code&gt;cu121&lt;/code&gt; to &lt;code&gt;cu118&lt;/code&gt; for CUDA version 11.8 or 12.1. Go to &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt; to learn more.&lt;/p&gt; &#xA;&lt;h1&gt;Alpaca Example&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;from unsloth import FastLlamaModel&#xA;import torch&#xA;max_seq_length = 2048 # Can change to any number &amp;lt;= 4096&#xA;dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+&#xA;load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.&#xA;&#xA;# Load Llama model&#xA;model, tokenizer = FastLlamaModel.from_pretrained(&#xA;    model_name = &#34;unsloth/llama-2-7b&#34;, # Supports any llama model&#xA;    max_seq_length = max_seq_length,&#xA;    dtype = dtype,&#xA;    load_in_4bit = load_in_4bit,&#xA;    # token = &#34;hf_...&#34;, # use one if using gated models like meta-llama/Llama-2-7b-hf&#xA;)&#xA;&#xA;# Do model patching and add fast LoRA weights&#xA;model = FastLlamaModel.get_peft_model(&#xA;    model,&#xA;    r = 16,&#xA;    target_modules = [&#34;q_proj&#34;, &#34;k_proj&#34;, &#34;v_proj&#34;, &#34;o_proj&#34;,&#xA;                      &#34;gate_proj&#34;, &#34;up_proj&#34;, &#34;down_proj&#34;,],&#xA;    lora_alpha = 16,&#xA;    lora_dropout = 0, # Currently only supports dropout = 0&#xA;    bias = &#34;none&#34;,    # Currently only supports bias = &#34;none&#34;&#xA;    use_gradient_checkpointing = True,&#xA;    random_state = 3407,&#xA;    max_seq_length = max_seq_length,&#xA;)&#xA;&#xA;trainer = .... Use Huggingface&#39;s Trainer and dataset loading&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you trained a model with Unsloth, we made a cool sticker!! &lt;img src=&#34;./images/unsloth made with love.png&#34; width=&#34;200&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Future Milestones and limitations&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Support sqrt gradient checkpointing which further slashes memory usage by 25%.&lt;/li&gt; &#xA; &lt;li&gt;Does not support non Llama models - we do so in the future.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Performance comparisons on 1 Tesla T4 GPU:&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Time taken for 1 epoch&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Alpaca (52K)&lt;/th&gt; &#xA;   &lt;th&gt;LAION OIG (210K)&lt;/th&gt; &#xA;   &lt;th&gt;Open Assistant (10K)&lt;/th&gt; &#xA;   &lt;th&gt;SlimOrca (518K)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Huggingface&lt;/td&gt; &#xA;   &lt;td&gt;1 T4&lt;/td&gt; &#xA;   &lt;td&gt;23h 15m&lt;/td&gt; &#xA;   &lt;td&gt;56h 28m&lt;/td&gt; &#xA;   &lt;td&gt;8h 38m&lt;/td&gt; &#xA;   &lt;td&gt;391h 41m&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Open&lt;/td&gt; &#xA;   &lt;td&gt;1 T4&lt;/td&gt; &#xA;   &lt;td&gt;13h 7m (1.8x)&lt;/td&gt; &#xA;   &lt;td&gt;31h 47m (1.8x)&lt;/td&gt; &#xA;   &lt;td&gt;4h 27m (1.9x)&lt;/td&gt; &#xA;   &lt;td&gt;240h 4m (1.6x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Pro&lt;/td&gt; &#xA;   &lt;td&gt;1 T4&lt;/td&gt; &#xA;   &lt;td&gt;3h 6m (7.5x)&lt;/td&gt; &#xA;   &lt;td&gt;5h 17m (10.7x)&lt;/td&gt; &#xA;   &lt;td&gt;1h 7m (7.7x)&lt;/td&gt; &#xA;   &lt;td&gt;59h 53m (6.5x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Max&lt;/td&gt; &#xA;   &lt;td&gt;1 T4&lt;/td&gt; &#xA;   &lt;td&gt;2h 39m (8.8x)&lt;/td&gt; &#xA;   &lt;td&gt;4h 31m (12.5x)&lt;/td&gt; &#xA;   &lt;td&gt;0h 58m (8.9x)&lt;/td&gt; &#xA;   &lt;td&gt;51h 30m (7.6x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Peak Memory Usage&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Alpaca (52K)&lt;/th&gt; &#xA;   &lt;th&gt;LAION OIG (210K)&lt;/th&gt; &#xA;   &lt;th&gt;Open Assistant (10K)&lt;/th&gt; &#xA;   &lt;th&gt;SlimOrca (518K)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Huggingface&lt;/td&gt; &#xA;   &lt;td&gt;1 T4&lt;/td&gt; &#xA;   &lt;td&gt;7.3GB&lt;/td&gt; &#xA;   &lt;td&gt;5.9GB&lt;/td&gt; &#xA;   &lt;td&gt;14.0GB&lt;/td&gt; &#xA;   &lt;td&gt;13.3GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Open&lt;/td&gt; &#xA;   &lt;td&gt;1 T4&lt;/td&gt; &#xA;   &lt;td&gt;6.8GB&lt;/td&gt; &#xA;   &lt;td&gt;5.7GB&lt;/td&gt; &#xA;   &lt;td&gt;7.8GB&lt;/td&gt; &#xA;   &lt;td&gt;7.7GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Pro&lt;/td&gt; &#xA;   &lt;td&gt;1 T4&lt;/td&gt; &#xA;   &lt;td&gt;6.4GB&lt;/td&gt; &#xA;   &lt;td&gt;6.4GB&lt;/td&gt; &#xA;   &lt;td&gt;6.4GB&lt;/td&gt; &#xA;   &lt;td&gt;6.4GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Max&lt;/td&gt; &#xA;   &lt;td&gt;1 T4&lt;/td&gt; &#xA;   &lt;td&gt;11.4GB&lt;/td&gt; &#xA;   &lt;td&gt;12.4GB&lt;/td&gt; &#xA;   &lt;td&gt;11.9GB&lt;/td&gt; &#xA;   &lt;td&gt;14.4GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Performance comparisons on 2 Tesla T4 GPUs via DDP:&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Time taken for 1 epoch&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Alpaca (52K)&lt;/th&gt; &#xA;   &lt;th&gt;LAION OIG (210K)&lt;/th&gt; &#xA;   &lt;th&gt;Open Assistant (10K)&lt;/th&gt; &#xA;   &lt;th&gt;SlimOrca (518K)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Huggingface&lt;/td&gt; &#xA;   &lt;td&gt;2 T4&lt;/td&gt; &#xA;   &lt;td&gt;84h 47m&lt;/td&gt; &#xA;   &lt;td&gt;163h 48m&lt;/td&gt; &#xA;   &lt;td&gt;30h 51m&lt;/td&gt; &#xA;   &lt;td&gt;1301h 24m&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Pro&lt;/td&gt; &#xA;   &lt;td&gt;2 T4&lt;/td&gt; &#xA;   &lt;td&gt;3h 20m (25.4x)&lt;/td&gt; &#xA;   &lt;td&gt;5h 43m (28.7x)&lt;/td&gt; &#xA;   &lt;td&gt;1h 12m (25.7x)&lt;/td&gt; &#xA;   &lt;td&gt;71h 40m (18.1x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Max&lt;/td&gt; &#xA;   &lt;td&gt;2 T4&lt;/td&gt; &#xA;   &lt;td&gt;3h 4m (27.6x)&lt;/td&gt; &#xA;   &lt;td&gt;5h 14m (31.3x)&lt;/td&gt; &#xA;   &lt;td&gt;1h 6m (28.1x)&lt;/td&gt; &#xA;   &lt;td&gt;54h 20m (23.9x)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Peak Memory Usage on a Multi GPU System (2 GPUs)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;System&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Alpaca (52K)&lt;/th&gt; &#xA;   &lt;th&gt;LAION OIG (210K)&lt;/th&gt; &#xA;   &lt;th&gt;Open Assistant (10K)&lt;/th&gt; &#xA;   &lt;th&gt;SlimOrca (518K)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Huggingface&lt;/td&gt; &#xA;   &lt;td&gt;2 T4&lt;/td&gt; &#xA;   &lt;td&gt;8.4GB | 6GB&lt;/td&gt; &#xA;   &lt;td&gt;7.2GB | 5.3GB&lt;/td&gt; &#xA;   &lt;td&gt;14.3GB | 6.6GB&lt;/td&gt; &#xA;   &lt;td&gt;10.9GB | 5.9GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Pro&lt;/td&gt; &#xA;   &lt;td&gt;2 T4&lt;/td&gt; &#xA;   &lt;td&gt;7.7GB | 4.9GB&lt;/td&gt; &#xA;   &lt;td&gt;7.5GB | 4.9GB&lt;/td&gt; &#xA;   &lt;td&gt;8.5GB | 4.9GB&lt;/td&gt; &#xA;   &lt;td&gt;6.2GB | 4.7GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Unsloth Max&lt;/td&gt; &#xA;   &lt;td&gt;2 T4&lt;/td&gt; &#xA;   &lt;td&gt;10.5GB | 5GB&lt;/td&gt; &#xA;   &lt;td&gt;10.6GB | 5GB&lt;/td&gt; &#xA;   &lt;td&gt;10.6GB | 5GB&lt;/td&gt; &#xA;   &lt;td&gt;10.5GB | 5GB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Troubleshooting&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Sometimes &lt;code&gt;bitsandbytes&lt;/code&gt; or &lt;code&gt;xformers&lt;/code&gt; does not link properly. Try running:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;!ldconfig /usr/lib64-nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Windows is not supported as of yet - we rely on Xformers and Triton support, so until both packages support Windows officially, Unsloth will then support Windows.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If it doesn&#39;t install - maybe try updating &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;./images/unsloth loading page render.png&#34; width=&#34;300&#34;&gt;</summary>
  </entry>
  <entry>
    <title>go-gost/gost</title>
    <updated>2023-12-05T01:26:25Z</updated>
    <id>tag:github.com,2023-12-05:/go-gost/gost</id>
    <link href="https://github.com/go-gost/gost" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GO Simple Tunnel - a simple tunnel written in golang&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GO Simple Tunnel&lt;/h1&gt; &#xA;&lt;h3&gt;GOè¯­è¨€å®ç°çš„å®‰å…¨éš§é“&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/go-gost/gost/master/README_en.md&#34;&gt;English README&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;åŠŸèƒ½ç‰¹æ€§&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/getting-started/quick-start/&#34;&gt;å¤šç«¯å£ç›‘å¬&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/concepts/chain/&#34;&gt;å¤šçº§è½¬å‘é“¾&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/tutorials/protocols/overview/&#34;&gt;å¤šåè®®æ”¯æŒ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/tutorials/port-forwarding/&#34;&gt;TCP/UDPç«¯å£è½¬å‘&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/tutorials/reverse-proxy/&#34;&gt;åå‘ä»£ç†&lt;/a&gt;å’Œ&lt;a href=&#34;https://gost.run/tutorials/reverse-proxy-tunnel/&#34;&gt;éš§é“&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/tutorials/redirect/&#34;&gt;TCP/UDPé€æ˜ä»£ç†&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; DNS&lt;a href=&#34;https://gost.run/concepts/resolver/&#34;&gt;è§£æ&lt;/a&gt;å’Œ&lt;a href=&#34;https://gost.run/tutorials/dns/&#34;&gt;ä»£ç†&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/tutorials/tuntap/&#34;&gt;TUN/TAPè®¾å¤‡&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/concepts/selector/&#34;&gt;è´Ÿè½½å‡è¡¡&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/concepts/bypass/&#34;&gt;è·¯ç”±æ§åˆ¶&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/concepts/admission/&#34;&gt;å‡†å…¥æ§åˆ¶&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/concepts/limiter/&#34;&gt;é™é€Ÿé™æµ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/concepts/plugin/&#34;&gt;æ’ä»¶ç³»ç»Ÿ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/tutorials/metrics/&#34;&gt;Prometheusç›‘æ§æŒ‡æ ‡&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/tutorials/api/config/&#34;&gt;åŠ¨æ€é…ç½®&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://gost.run/tutorials/api/overview/&#34;&gt;Web API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Web UI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æ¦‚è§ˆ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://gost.run/images/overview.png&#34; alt=&#34;Overview&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;GOSTä½œä¸ºéš§é“æœ‰ä¸‰ç§ä¸»è¦ä½¿ç”¨æ–¹å¼ã€‚&lt;/p&gt; &#xA;&lt;h3&gt;æ­£å‘ä»£ç†&lt;/h3&gt; &#xA;&lt;p&gt;ä½œä¸ºä»£ç†æœåŠ¡è®¿é—®ç½‘ç»œï¼Œå¯ä»¥ç»„åˆä½¿ç”¨å¤šç§åè®®ç»„æˆè½¬å‘é“¾è¿›è¡Œè½¬å‘ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://gost.run/images/proxy.png&#34; alt=&#34;Proxy&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;ç«¯å£è½¬å‘&lt;/h3&gt; &#xA;&lt;p&gt;å°†ä¸€ä¸ªæœåŠ¡çš„ç«¯å£æ˜ å°„åˆ°å¦å¤–ä¸€ä¸ªæœåŠ¡çš„ç«¯å£ï¼ŒåŒæ ·å¯ä»¥ç»„åˆä½¿ç”¨å¤šç§åè®®ç»„æˆè½¬å‘é“¾è¿›è¡Œè½¬å‘ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://gost.run/images/forward.png&#34; alt=&#34;Forward&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;åå‘ä»£ç†&lt;/h3&gt; &#xA;&lt;p&gt;åˆ©ç”¨éš§é“å’Œå†…ç½‘ç©¿é€å°†å†…ç½‘æœåŠ¡æš´éœ²åˆ°å…¬ç½‘è®¿é—®ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://gost.run/images/reverse-proxy.png&#34; alt=&#34;Reverse Proxy&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ä¸‹è½½å®‰è£…&lt;/h2&gt; &#xA;&lt;h3&gt;äºŒè¿›åˆ¶æ–‡ä»¶&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/go-gost/gost/releases&#34;&gt;https://github.com/go-gost/gost/releases&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;å®‰è£…è„šæœ¬&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# å®‰è£…æœ€æ–°ç‰ˆæœ¬ [https://github.com/go-gost/gost/releases](https://github.com/go-gost/gost/releases)&#xA;bash &amp;lt;(curl -fsSL https://github.com/go-gost/gost/raw/master/install.sh) --install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# é€‰æ‹©è¦å®‰è£…çš„ç‰ˆæœ¬&#xA;bash &amp;lt;(curl -fsSL https://github.com/go-gost/gost/raw/master/install.sh)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;æºç ç¼–è¯‘&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/go-gost/gost.git&#xA;cd gost/cmd/gost&#xA;go build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --rm gogost/gost -V&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Shadowsocks Androidæ’ä»¶&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/xausky/ShadowsocksGostPlugin&#34;&gt;xausky/ShadowsocksGostPlugin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;å¸®åŠ©ä¸æ”¯æŒ&lt;/h2&gt; &#xA;&lt;p&gt;Wikiç«™ç‚¹ï¼š&lt;a href=&#34;https://gost.run&#34;&gt;https://gost.run&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Telegramè®¨è®ºç¾¤ï¼š&lt;a href=&#34;https://t.me/gogost&#34;&gt;https://t.me/gogost&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Googleè®¨è®ºç»„ï¼š&lt;a href=&#34;https://groups.google.com/d/forum/go-gost&#34;&gt;https://groups.google.com/d/forum/go-gost&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;æ—§ç‰ˆå…¥å£ï¼š&lt;a href=&#34;https://v2.gost.run&#34;&gt;v2.gost.run&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>