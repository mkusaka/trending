<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-07T01:31:25Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>facebookincubator/AITemplate</title>
    <updated>2022-10-07T01:31:25Z</updated>
    <id>tag:github.com,2022-10-07:/facebookincubator/AITemplate</id>
    <link href="https://github.com/facebookincubator/AITemplate" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AITemplate is a Python framework which renders neural network into high performance CUDA/HIP C++ code. Specialized for FP16 TensorCore (NVIDIA GPU) and MatrixCore (AMD GPU) inference.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AITemplate&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/facebookincubator/AITemplate/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-brightgreen.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; | &lt;a href=&#34;https://facebookincubator.github.io/AITemplate&#34;&gt;&lt;img src=&#34;https://github.com/facebookincubator/AITemplate/actions/workflows/docs.yml/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; | &lt;a href=&#34;https://app.circleci.com/pipelines/github/facebookincubator/AITemplate&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/facebookincubator/AITemplate.svg?style=svg&#34; alt=&#34;CircleCI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;AITemplate (AIT) is a Python framework that transforms deep neural networks into CUDA (NVIDIA GPU) / HIP (AMD GPU) C++ code for lightning-fast inference serving. AITemplate highlights include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;High performance: close to roofline fp16 TensorCore (NVIDIA GPU) / MatrixCore (AMD GPU) performance on major models, including ResNet, MaskRCNN, BERT, VisionTransformer, Stable Diffusion, etc.&lt;/li&gt; &#xA; &lt;li&gt;Unified, open, and flexible. Seamless fp16 deep neural network models for NVIDIA GPU or AMD GPU. Fully open source, Lego-style easy extendable high-performance primitives for new model support. Supports a significantly more comprehensive range of fusions than existing solutions for both GPU platforms.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;More about AITemplate&lt;/h2&gt; &#xA;&lt;h3&gt;Excellent Backward Capability&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate doesn&#39;t depend on third-party libraries or runtimes, such as cuBLAS, cuDNN, rocBLAS, MIOpen, TensorRT, MIGraphX, etc. Each model is compiled into a self-contained portable binary, which can be used on any software environment with the same hardware.&lt;/p&gt; &#xA;&lt;h3&gt;Horizontal Fusion&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate provides unique advanced horizontal fusion. AITemplate can fuse parallel GEMM, LayerNorm, and other operators with different input shapes into a single GPU kernel.&lt;/p&gt; &#xA;&lt;h3&gt;Vertical Fusion&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate provides strong vertical fusion. AITemplate can fuse a large range of operations into TensorCore/MatrixCore operations, such as elementwise operations, reduction operations, and layout permutation operations. AITemplate also provides back-to-back style TensorCore / MatrixCore operation fusion.&lt;/p&gt; &#xA;&lt;h3&gt;Memory Fusion&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate provides innovative memory fusions. AITemplate can fuse GEMM, LayerNorm, and other operators, followed by memory operations such as concatenation, split, and slice into a single operator.&lt;/p&gt; &#xA;&lt;h3&gt;Working w/wo PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;The AITemplate-generated Python runtime can take PyTorch tensors as inputs and outputs without an extra copy. For environments without PyTorch, the AITemplate Python/C++ runtime is self-contained.&lt;/p&gt; &#xA;&lt;h3&gt;Extensions without suffering&lt;/h3&gt; &#xA;&lt;p&gt;AITemplate provides a straightforward approach for making an extension in codegen. To add a new operator or a new fused kernel into AITemplate, most of the time one only needs to add two Python files: one for a graph node definition and another for the backend codegen. The CUDA/HIP kernel in a text header file can be directly utilized in the codegen.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hardware requirement:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;NVIDIA&lt;/strong&gt;: AIT is only tested on SM80+ GPUs (Ampere etc). Not all kernels work with old SM75/SM70 (T4/V100) GPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AMD&lt;/strong&gt;: AIT is only tested on CDNA2 (MI-210/250) GPUs. There may be compiler issues for old CDNA1 (MI-100) GPUs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Clone the code&lt;/h2&gt; &#xA;&lt;p&gt;When cloning the code, please use the following command to also clone the submodules:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone --recursive https://github.com/facebookincubator/AITemplate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Image&lt;/h3&gt; &#xA;&lt;p&gt;We highly recommend using AITemplate with Docker to avoid accidentally using a wrong version of NVCC or HIPCC.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA: &lt;code&gt;./docker/build.sh cuda&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;ROCM: &lt;code&gt;DOCKER_BUILDKIT=1 ./docker/build.sh rocm&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This will build a docker image with tag &lt;code&gt;ait:latest&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;p&gt;The following command will create a Python wheel for AITemplate. Please ensure you have correct CUDA/ROCm compiler installed.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA: CUDA 11.6&lt;/li&gt; &#xA; &lt;li&gt;ROCm: We tested on ROCm 5.2.3 with a customized build HIPCC with the command in docker/Dockerfile.rocm#L87-L96&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Incorrect compiler will lead performance regression.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please check all submodules are cloned correctly before go to next step.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd python&#xA;python setup.py bdist_wheel&#xA;pip install dist/*.whl --force-reinstall&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://facebookincubator.github.io/AITemplate&#34;&gt;AITemplate Documentation&lt;/a&gt; for API reference.&lt;/p&gt; &#xA;&lt;p&gt;There are a few tutorials for onboarding:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;01: &lt;a href=&#34;https://facebookincubator.github.io/AITemplate/tutorial/how_to_infer_pt.html&#34;&gt;How to inference a PyTorch model with AIT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;02: &lt;a href=&#34;https://facebookincubator.github.io/AITemplate/tutorial/how_to_add_op.html&#34;&gt;How to add an op to AIT codegen&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;03: &lt;a href=&#34;https://facebookincubator.github.io/AITemplate/tutorial/how_to_visualize.html&#34;&gt;How to visualize AIT&#39;s optimization&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples &amp;amp; Performance&lt;/h2&gt; &#xA;&lt;p&gt;AITemplate provides the following model templates &amp;amp; reference performance data on A100/MI-250&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/01_resnet-50/&#34;&gt;01_ResNet-50&lt;/a&gt; with PyTorch Image Models (TIMM)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/02_detectron2/&#34;&gt;02_MaskRCNN-FPN&lt;/a&gt; with Detectron2&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/03_bert/&#34;&gt;03_BERT&lt;/a&gt; with HuggingFace Transformer&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/04_vit/&#34;&gt;04_Vision Transformer&lt;/a&gt; with PyTorch Image Models (TIMM)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/examples/05_stable_diffusion/&#34;&gt;05_Stable Diffusion&lt;/a&gt; with HuggingFace Diffusers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;p&gt;AITemplate has a 90 days release cycle. In the next one or two releases, we will focus on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Deprecating FlashAttention: Unify CUDA Attention computation to Composable Kernel (AMD GPU) style back-to-back fusion to improve performance and increase flexibility for NVIDIA GPU Transformer users.&lt;/li&gt; &#xA; &lt;li&gt;Remove kernel profiling requirement.&lt;/li&gt; &#xA; &lt;li&gt;GEMM + LayerNorm fusion, GEMM + GEMM fusion, Conv + Conv fusion.&lt;/li&gt; &#xA; &lt;li&gt;Better dynamic shape support: Focus on the dynamic sequence in Transformers.&lt;/li&gt; &#xA; &lt;li&gt;More model templates: Provide model templates with control flow and containers.&lt;/li&gt; &#xA; &lt;li&gt;More automatic graph passes: Relief manual rewrite models to obtain the best performance.&lt;/li&gt; &#xA; &lt;li&gt;Enable more fusions on AMD backend.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Some ongoing/potential work that won&#39;t appear in the next short-term release:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automatic Pytorch-FX, ONNX, Open-XLA and other format model conversion.&lt;/li&gt; &#xA; &lt;li&gt;Quantized model (int8/fp8/int4) support.&lt;/li&gt; &#xA; &lt;li&gt;Composable Kernel CPU extension on AVX2/AVX-512 for AMD Epyc CPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Check our &lt;a href=&#34;https://raw.githubusercontent.com/facebookincubator/AITemplate/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to learn about how to contribute to the project.&lt;/p&gt; &#xA;&lt;h2&gt;The Team&lt;/h2&gt; &#xA;&lt;p&gt;AITemplate is co-created by Meta engineers: &lt;a href=&#34;https://github.com/antinucleon&#34;&gt;Bing Xu&lt;/a&gt;, &lt;a href=&#34;https://github.com/ipiszy&#34;&gt;Ying Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/hlu1&#34;&gt;Hao Lu&lt;/a&gt;, &lt;a href=&#34;https://github.com/chenyang78&#34;&gt;Yang Chen&lt;/a&gt;, and &lt;a href=&#34;https://github.com/terrychenism&#34;&gt;Terry Chen&lt;/a&gt;, with major contributions coming from more talented engineers. A non-exhaustive list to mention is Mike Iovine, Mu-Chu Lee, Scott Wolchok, Oleg Khabinov, Shirong Wu, Huaming Li, Hui Guo, Zhijing Li, Max Podkorytov. We also want to thank the discussions with Andrew Tulloch, Yinghai Lu, Lu Fang.&lt;/p&gt; &#xA;&lt;p&gt;AITemplate is currently maintained by Meta engineers: &lt;a href=&#34;https://github.com/ipiszy&#34;&gt;Ying Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/hlu1&#34;&gt;Hao Lu&lt;/a&gt;, &lt;a href=&#34;https://github.com/chenyang78&#34;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&#34;https://github.com/terrychenism&#34;&gt;Terry Chen&lt;/a&gt;, &lt;a href=&#34;https://github.com/mikeiovine&#34;&gt;Mike Iovine&lt;/a&gt;, &lt;a href=&#34;https://github.com/muchulee8&#34;&gt;Mu-Chu Lee&lt;/a&gt; and &lt;a href=&#34;https://github.com/antinucleon&#34;&gt;Bing Xu&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;AITemplate team works deeply with NVIDIA &lt;a href=&#34;https://github.com/NVIDIA/cutlass&#34;&gt;CUTLASS&lt;/a&gt; Team (Led by Andrew Kerr, Haicheng Wu) and AMD &lt;a href=&#34;https://github.com/ROCmSoftwarePlatform/composable_kernel&#34;&gt;Composable Kernel&lt;/a&gt; Team (Led by Chao Liu, Jing Zhang). We co-designed many advanced GPU optimizations specialized for each platform, and nothing is possible without our close collaboration.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;AITemplate is licensed under the &lt;a href=&#34;https://github.com/facebookincubator/AITemplate/raw/main/LICENSE&#34;&gt;Apache 2.0 License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>torvalds/linux</title>
    <updated>2022-10-07T01:31:25Z</updated>
    <id>tag:github.com,2022-10-07:/torvalds/linux</id>
    <link href="https://github.com/torvalds/linux" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Linux kernel source tree&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Linux kernel&lt;/h1&gt; &#xA;&lt;p&gt;There are several guides for kernel developers and users. These guides can be rendered in a number of formats, like HTML and PDF. Please read Documentation/admin-guide/README.rst first.&lt;/p&gt; &#xA;&lt;p&gt;In order to build the documentation, use &lt;code&gt;make htmldocs&lt;/code&gt; or &lt;code&gt;make pdfdocs&lt;/code&gt;. The formatted documentation can also be read online at:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://www.kernel.org/doc/html/latest/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are various text files in the Documentation/ subdirectory, several of them using the Restructured Text markup notation.&lt;/p&gt; &#xA;&lt;p&gt;Please read the Documentation/process/changes.rst file, as it contains the requirements for building and running the kernel, and information about the problems which may result by upgrading your kernel.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>copy/v86</title>
    <updated>2022-10-07T01:31:25Z</updated>
    <id>tag:github.com,2022-10-07:/copy/v86</id>
    <link href="https://github.com/copy/v86" rel="alternate"></link>
    <summary type="html">&lt;p&gt;x86 virtualization in your browser, recompiling x86 to wasm on the fly&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://gitter.im/copy/v86&#34;&gt;&lt;img src=&#34;https://badges.gitter.im/Join%20Chat.svg?sanitize=true&#34; alt=&#34;Join the chat at https://gitter.im/copy/v86&#34;&gt;&lt;/a&gt; or #v86 on &lt;a href=&#34;https://libera.chat/&#34;&gt;irc.libera.chat&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;v86 emulates an x86-compatible CPU and hardware. Machine code is translated to WebAssembly modules at runtime in order to achieve decent performance. Here&#39;s a list of emulated hardware:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An x86-compatible CPU. The instruction set is around Pentium III level, including full SSE2 support. Some features are missing, in particular: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Task gates, far calls in protected mode&lt;/li&gt; &#xA;   &lt;li&gt;Some 16 bit protected mode features&lt;/li&gt; &#xA;   &lt;li&gt;Single stepping (trap flag, debug registers)&lt;/li&gt; &#xA;   &lt;li&gt;Some exceptions, especially floating point and SSE&lt;/li&gt; &#xA;   &lt;li&gt;Multicore&lt;/li&gt; &#xA;   &lt;li&gt;64-bit extensions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;A floating point unit (FPU). Calculations are done using the Berkeley SoftFloat library and therefore should be precise (but slow). Trigonometric and log functions are emulated using 64-bit floats and may be less precise. Not all FPU exceptions are supported.&lt;/li&gt; &#xA; &lt;li&gt;A floppy disk controller (8272A).&lt;/li&gt; &#xA; &lt;li&gt;An 8042 Keyboard Controller, PS2. With mouse support.&lt;/li&gt; &#xA; &lt;li&gt;An 8254 Programmable Interval Timer (PIT).&lt;/li&gt; &#xA; &lt;li&gt;An 8259 Programmable Interrupt Controller (PIC).&lt;/li&gt; &#xA; &lt;li&gt;Partial APIC support.&lt;/li&gt; &#xA; &lt;li&gt;A CMOS Real Time Clock (RTC).&lt;/li&gt; &#xA; &lt;li&gt;A generic VGA card with SVGA support and Bochs VBE Extensions.&lt;/li&gt; &#xA; &lt;li&gt;A PCI bus. This one is partly incomplete and not used by every device.&lt;/li&gt; &#xA; &lt;li&gt;An IDE disk controller.&lt;/li&gt; &#xA; &lt;li&gt;An NE2000 (8390) PCI network card.&lt;/li&gt; &#xA; &lt;li&gt;A virtio filesystem.&lt;/li&gt; &#xA; &lt;li&gt;A SoundBlaster 16 sound card.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://copy.sh/v86/?profile=archlinux&#34;&gt;Arch Linux&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=dsl&#34;&gt;Damn Small Linux&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=buildroot&#34;&gt;Buildroot Linux&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=reactos&#34;&gt;ReactOS&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=windows2000&#34;&gt;Windows 2000&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=windows98&#34;&gt;Windows 98&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=windows95&#34;&gt;Windows 95&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=windows1&#34;&gt;Windows 1.01&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=msdos&#34;&gt;MS-DOS&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=freedos&#34;&gt;FreeDOS&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=freebsd&#34;&gt;FreeBSD&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=openbsd&#34;&gt;OpenBSD&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=9front&#34;&gt;9front&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=haiku&#34;&gt;Haiku&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=oberon&#34;&gt;Oberon&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=kolibrios&#34;&gt;KolibriOS&lt;/a&gt; — &lt;a href=&#34;https://copy.sh/v86/?profile=qnx&#34;&gt;QNX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s an overview of the operating systems supported in v86:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux works pretty well. 64-bit kernels are not supported. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Damn Small Linux (2.4 Kernel) works.&lt;/li&gt; &#xA;   &lt;li&gt;All tested versions of TinyCore work.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://buildroot.uclibc.org&#34;&gt;Buildroot&lt;/a&gt; can be used to build a minimal image. &lt;a href=&#34;https://github.com/humphd/browser-vm&#34;&gt;humphd/browser-vm&lt;/a&gt; and &lt;a href=&#34;https://github.com/Darin755/browser-buildroot&#34;&gt;darin755/browser-buildroot&lt;/a&gt; have some useful scripts for building one.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/skiffos/SkiffOS/tree/master/configs/browser/v86&#34;&gt;SkiffOS&lt;/a&gt; (based on Buildroot) can cross-compile a custom image.&lt;/li&gt; &#xA;   &lt;li&gt;Archlinux works. See &lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/docs/archlinux.md&#34;&gt;archlinux.md&lt;/a&gt; for building an image.&lt;/li&gt; &#xA;   &lt;li&gt;Debian works. An image can be built from a Dockerfile, see &lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/tools/docker/debian/&#34;&gt;tools/docker/debian/&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Ubuntu up to 16.04 works.&lt;/li&gt; &#xA;   &lt;li&gt;Alpine Linux works.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;ReactOS works.&lt;/li&gt; &#xA; &lt;li&gt;FreeDOS, Windows 1.01 and MS-DOS run very well.&lt;/li&gt; &#xA; &lt;li&gt;KolibriOS works.&lt;/li&gt; &#xA; &lt;li&gt;Haiku works.&lt;/li&gt; &#xA; &lt;li&gt;Android x86 1.6-r2 works if one selects VESA mode at the boot prompt. Newer versions may work if compiled without SSE3. See &lt;a href=&#34;https://github.com/copy/v86/issues/224&#34;&gt;#224&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Windows 1, 3.0, 95, 98, ME and 2000 work. Other versions currently don&#39;t (see &lt;a href=&#34;https://github.com/copy/v86/issues/86&#34;&gt;#86&lt;/a&gt;, &lt;a href=&#34;https://github.com/copy/v86/issues/208&#34;&gt;#208&lt;/a&gt;). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;In Windows 2000 and higher the PC type has to be changed from ACPI PC to Standard PC&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Many hobby operating systems work.&lt;/li&gt; &#xA; &lt;li&gt;9front works.&lt;/li&gt; &#xA; &lt;li&gt;Plan 9 doesn&#39;t work.&lt;/li&gt; &#xA; &lt;li&gt;QNX works.&lt;/li&gt; &#xA; &lt;li&gt;OS/2 doesn&#39;t work.&lt;/li&gt; &#xA; &lt;li&gt;FreeBSD works.&lt;/li&gt; &#xA; &lt;li&gt;OpenBSD works with a specific boot configuration. At the &lt;code&gt;boot&amp;gt;&lt;/code&gt; prompt type &lt;code&gt;boot -c&lt;/code&gt;, then at the &lt;code&gt;UKC&amp;gt;&lt;/code&gt; prompt &lt;code&gt;disable mpbios&lt;/code&gt; and &lt;code&gt;exit&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;NetBSD works only with a custom kernel, see &lt;a href=&#34;https://github.com/copy/v86/issues/350&#34;&gt;#350&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;SerenityOS works.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can get some infos on the disk images here: &lt;a href=&#34;https://github.com/copy/images&#34;&gt;https://github.com/copy/images&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to build, run and embed?&lt;/h2&gt; &#xA;&lt;p&gt;You need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;make&lt;/li&gt; &#xA; &lt;li&gt;Rust with the wasm32-unknown-unknown target&lt;/li&gt; &#xA; &lt;li&gt;A version of clang compatible with Rust&lt;/li&gt; &#xA; &lt;li&gt;java (for Closure Compiler, not necessary when using &lt;code&gt;debug.html&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;nodejs (a recent version is required, v16.11.1 is known to be working)&lt;/li&gt; &#xA; &lt;li&gt;To run tests: nasm, gdb, qemu-system, gcc, libc-i386 and rustfmt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/tools/docker/test-image/Dockerfile&#34;&gt;tools/docker/test-image/Dockerfile&lt;/a&gt; for a full setup on Debian or &lt;a href=&#34;https://docs.microsoft.com/en-us/windows/wsl/install&#34;&gt;WSL&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make&lt;/code&gt; to build the debug build (at &lt;code&gt;debug.html&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make all&lt;/code&gt; to build the optimized build (at &lt;code&gt;index.html&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;ROM and disk images are loaded via XHR, so if you want to try out &lt;code&gt;index.html&lt;/code&gt; locally, make sure to serve it from a local webserver. You can use &lt;code&gt;make run&lt;/code&gt; to serve the files using Python&#39;s http module.&lt;/li&gt; &#xA; &lt;li&gt;If you only want to embed v86 in a webpage you can use libv86.js. For usage, check out the &lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/examples/&#34;&gt;examples&lt;/a&gt;. You can download it from the release section.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Alternatively, to build using docker&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have docker installed, you can run the whole system inside a container.&lt;/li&gt; &#xA; &lt;li&gt;See &lt;code&gt;tools/docker/exec&lt;/code&gt; to find Dockerfile required for this.&lt;/li&gt; &#xA; &lt;li&gt;You can run &lt;code&gt;docker build -f tools/docker/exec/Dockerfile -t v86:alpine-3.14 .&lt;/code&gt; from the root directory to generate docker image.&lt;/li&gt; &#xA; &lt;li&gt;Then you can simply run &lt;code&gt;docker run -it -p 8000:8000 v86:alpine-3.14&lt;/code&gt; to start the server.&lt;/li&gt; &#xA; &lt;li&gt;Check &lt;code&gt;localhost:8000&lt;/code&gt; for hosted server.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;The disk images for testing are not included in this repository. You can download them directly from the website using:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;wget -P images/ https://k.copy.sh/{linux.iso,linux4.iso,buildroot-bzimage.bin,openbsd-floppy.img,kolibri.img,windows101.img,os8.img,freedos722.img}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run all tests: &lt;code&gt;make jshint rustfmt kvm-unit-test nasmtests nasmtests-force-jit expect-tests jitpagingtests qemutests rust-test tests&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/tests/Readme.md&#34;&gt;tests/Readme.md&lt;/a&gt; for more infos.&lt;/p&gt; &#xA;&lt;h2&gt;API examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/examples/basic.html&#34;&gt;Basic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/examples/serial.html&#34;&gt;Programatically using the serial terminal&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/examples/lua.html&#34;&gt;A Lua interpreter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/examples/two_instances.html&#34;&gt;Two instances in one window&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/examples/save_restore.html&#34;&gt;Saving and restoring emulator state&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Using v86 for your own purposes is as easy as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;var emulator = new V86Starter({&#xA;    screen_container: document.getElementById(&#34;screen_container&#34;),&#xA;    bios: {&#xA;        url: &#34;../../bios/seabios.bin&#34;,&#xA;    },&#xA;    vga_bios: {&#xA;        url: &#34;../../bios/vgabios.bin&#34;,&#xA;    },&#xA;    cdrom: {&#xA;        url: &#34;../../images/linux.iso&#34;,&#xA;    },&#xA;    autostart: true,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/src/browser/starter.js&#34;&gt;starter.js&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;v86 is distributed under the terms of the Simplified BSD License, see &lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/LICENSE&#34;&gt;LICENSE&lt;/a&gt;. The following third-party dependencies are included in the repository under their own licenses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/lib/softfloat/softfloat.c&#34;&gt;&lt;code&gt;lib/softfloat/softfloat.c&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/lib/zstd/zstddeclib.c&#34;&gt;&lt;code&gt;lib/zstd/zstddeclib.c&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/tests/kvm-unit-tests&#34;&gt;&lt;code&gt;tests/kvm-unit-tests/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/copy/v86/master/tests/qemutests&#34;&gt;&lt;code&gt;tests/qemutests/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CPU test cases via &lt;a href=&#34;https://wiki.qemu.org/Main_Page&#34;&gt;QEMU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;More tests via &lt;a href=&#34;https://www.linux-kvm.org/page/KVM-unit-tests&#34;&gt;kvm-unit-tests&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebook/zstd&#34;&gt;zstd&lt;/a&gt; support is included for better compression of state images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.jhauser.us/arithmetic/SoftFloat.html&#34;&gt;Berkeley SoftFloat&lt;/a&gt; is included to precisely emulate 80-bit floating point numbers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/s-macke/jor1k&#34;&gt;The jor1k project&lt;/a&gt; for 9p, filesystem and uart drivers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://winworldpc.com/&#34;&gt;WinWorld&lt;/a&gt; sources of some old operating systems&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;More questions?&lt;/h2&gt; &#xA;&lt;p&gt;Shoot me an email to &lt;code&gt;copy@copy.sh&lt;/code&gt;. Please report bugs on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;Fabian Hemmer (&lt;a href=&#34;https://copy.sh/&#34;&gt;https://copy.sh/&lt;/a&gt;, &lt;code&gt;copy@copy.sh&lt;/code&gt;)&lt;/p&gt;</summary>
  </entry>
</feed>