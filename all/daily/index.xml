<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-11-14T01:28:28Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>QwenLM/Qwen2.5-Coder</title>
    <updated>2024-11-14T01:28:28Z</updated>
    <id>tag:github.com,2024-11-14:/QwenLM/Qwen2.5-Coder</id>
    <link href="https://github.com/QwenLM/Qwen2.5-Coder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qwen2.5-Coder is the code version of Qwen2.5, the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a name=&#34;readme-top&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder/qwen2.5-coder-logo&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5/Qwen2.5-Coder-Family/main_fig_32b_white.jpg&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen25-coder-66eaa22e6f99801bf65b0c2f&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&#34;https://modelscope.cn/organization/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üíª &lt;a href=&#34;https://www.kaggle.com/models/qwen-lm/qwen2.5-coder&#34;&gt;Kaggle&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-coder-family&#34;&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ÔΩú &amp;nbsp;&amp;nbsp;üìñ &lt;a href=&#34;https://qwen.readthedocs.io/&#34;&gt;Documentation&lt;/a&gt; &lt;br&gt; üñ•Ô∏è &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo&#34;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | üñº &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-Artifacts&#34;&gt;Artifacts&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìÑ&lt;a href=&#34;https://arxiv.org/abs/2409.12186&#34;&gt;Arxiv&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen2.5-Coder-&lt;/code&gt;, and you will find all you need! Enjoy!&lt;/p&gt; &#xA;&lt;h1&gt;Qwen2.5-Coder Series: Powerful, Diverse, Practical.&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Today, we are excited to open source the ‚ÄúPowerful‚Äù, ‚ÄúDiverse‚Äù, and ‚ÄúPractical‚Äù &lt;strong&gt;Qwen2.5-Coder&lt;/strong&gt; series (formerly known as CodeQwen1.5), dedicated to continuously promoting the development of Open CodeLLMs.&lt;/p&gt; &#xA;&lt;p&gt;üíª Powerful: Qwen2.5-Coder-32B-Instruct has become the current SOTA open-source code model, matching the coding capabilities of GPT-4o. While demonstrating strong and comprehensive coding abilities, it also possesses good general and mathematical skills;&lt;/p&gt; &#xA;&lt;p&gt;üìö Diverse: Building on the previously open-sourced two sizes of 1.5B / 7B, this release brings four model sizes, including 0.5B / 3B / 14B / 32B. As of now, Qwen2.5-Coder has covered six mainstream model sizes to meet the needs of different developers;&lt;/p&gt; &#xA;&lt;p&gt;üõ† Practical: We explore the practicality of Qwen2.5-Coder in two scenarios, including code assistants and Artifacts, with some examples showcasing the potential applications of Qwen2.5-Coder in real-world scenarios;&lt;/p&gt; &#xA;&lt;h2&gt;basic information&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;‚ú® Supporting long context understanding and generation with the context length of 128K tokens;&lt;/li&gt; &#xA; &lt;li&gt;‚ú® Supporting 92 coding languages;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#39;ada&#39;, &#39;agda&#39;, &#39;alloy&#39;, &#39;antlr&#39;, &#39;applescript&#39;, &#39;assembly&#39;, &#39;augeas&#39;, &#39;awk&#39;, &#39;batchfile&#39;, &#39;bluespec&#39;, &#39;c&#39;, &#39;c#&#39;, &#39;c++&#39;, &#39;clojure&#39;, &#39;cmake&#39;, &#39;coffeescript&#39;, &#39;common-lisp&#39;, &#39;css&#39;, &#39;cuda&#39;, &#39;dart&#39;, &#39;dockerfile&#39;, &#39;elixir&#39;, &#39;elm&#39;, &#39;emacs-lisp&#39;, &#39;erlang&#39;, &#39;f#&#39;, &#39;fortran&#39;, &#39;glsl&#39;, &#39;go&#39;, &#39;groovy&#39;, &#39;haskell&#39;, &#39;html&#39;, &#39;idris&#39;, &#39;isabelle&#39;, &#39;java&#39;, &#39;java-server-pages&#39;, &#39;javascript&#39;, &#39;json&#39;, &#39;julia&#39;, &#39;jupyter-notebook&#39;, &#39;kotlin&#39;, &#39;lean&#39;, &#39;literate-agda&#39;, &#39;literate-coffeescript&#39;, &#39;literate-haskell&#39;, &#39;lua&#39;, &#39;makefile&#39;, &#39;maple&#39;, &#39;markdown&#39;, &#39;mathematica&#39;, &#39;matlab&#39;, &#39;objectc++&#39;, &#39;ocaml&#39;, &#39;pascal&#39;, &#39;perl&#39;, &#39;php&#39;, &#39;powershell&#39;, &#39;prolog&#39;, &#39;protocol-buffer&#39;, &#39;python&#39;, &#39;r&#39;, &#39;racket&#39;, &#39;restructuredtext&#39;, &#39;rmarkdown&#39;, &#39;ruby&#39;, &#39;rust&#39;, &#39;sas&#39;, &#39;scala&#39;, &#39;scheme&#39;, &#39;shell&#39;, &#39;smalltalk&#39;, &#39;solidity&#39;, &#39;sparql&#39;, &#39;sql&#39;, &#39;stan&#39;, &#39;standard-ml&#39;, &#39;stata&#39;, &#39;swift&#39;, &#39;systemverilog&#39;, &#39;tcl&#39;, &#39;tcsh&#39;, &#39;tex&#39;, &#39;thrift&#39;, &#39;typescript&#39;, &#39;verilog&#39;, &#39;vhdl&#39;, &#39;visual-basic&#39;, &#39;vue&#39;, &#39;xslt&#39;, &#39;yacc&#39;, &#39;yaml&#39;, &#39;zig&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;‚ú® Retain strengths in math and general capabilities from base model&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Important] We updates both the special tokens and their corresponding token ids, in order to maintain consistency with Qwen2.5. The new special tokens are as the following:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;&amp;lt;|fim_prefix|&amp;gt;&#34;: 151659, &#xA;  &#34;&amp;lt;|fim_middle|&amp;gt;&#34;: 151660, &#xA;  &#34;&amp;lt;|fim_suffix|&amp;gt;&#34;: 151661, &#xA;  &#34;&amp;lt;|fim_pad|&amp;gt;&#34;: 151662, &#xA;  &#34;&amp;lt;|repo_name|&amp;gt;&#34;: 151663, &#xA;  &#34;&amp;lt;|file_sep|&amp;gt;&#34;: 151664, &#xA;  &#34;&amp;lt;|im_start|&amp;gt;&#34;: 151644, &#xA;  &#34;&amp;lt;|im_end|&amp;gt;&#34;: 151645&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model name&lt;/th&gt; &#xA;   &lt;th&gt;type&lt;/th&gt; &#xA;   &lt;th&gt;length&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B&lt;/td&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-instruct&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-1.5B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-3B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;32k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-3B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-7B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-7B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-14B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-14B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-Instruct-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-AWQ&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-AWQ&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-Instruct-GGUF&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-GGUF&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-Instruct-GPTQ-Int4&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int4&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2.5-Coder-32B-Instruct-GPTQ-Int8&lt;/td&gt; &#xA;   &lt;td&gt;instruct&lt;/td&gt; &#xA;   &lt;td&gt;128k&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8&#34;&gt;Hugging Face&lt;/a&gt; ‚Ä¢ ü§ñ &lt;a href=&#34;https://modelscope.cn/models/Qwen/Qwen2.5-Coder-32B-Instruct-GPTQ-Int8&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Detailed performance and introduction are shown in this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5-coder-family&#34;&gt; üìë blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;python&amp;gt;=3.9&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;transformers&amp;gt;4.37.0&lt;/code&gt; for Qwen2.5 dense models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning]&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;b&gt; üö® This is a must because `transformers` integrated Qwen2 codes since `4.37.0`. &lt;/b&gt; &#xA; &lt;/div&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;You can install the required packages with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Important] &lt;strong&gt;Qwen2.5-Coder-[0.5-32]B-Instrcut&lt;/strong&gt; are instruction models for chatting;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Qwen2.5-Coder-[0.5-32]B&lt;/strong&gt; is a base model typically used for completion, serving as a better starting point for fine-tuning.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;üëâüèª Chat with Qwen2.5-Coder-32B-Instruct&lt;/h3&gt; &#xA;&lt;p&gt;You can just write several lines of code with &lt;code&gt;transformers&lt;/code&gt; to chat with Qwen2.5-Coder-32B-Instruct. Essentially, we build the tokenizer and the model with &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use generate method to perform chatting with the help of chat template provided by the tokenizer. Below is an example of how to chat with Qwen2.5-Coder-32B-Instruct:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model_name = &#34;Qwen/Qwen2.5-Coder-32B-Instruct&#34;&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;&#xA;prompt = &#34;write a quick sort algorithm.&#34;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;text = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True&#xA;)&#xA;model_inputs = tokenizer([text], return_tensors=&#34;pt&#34;).to(model.device)&#xA;&#xA;generated_ids = model.generate(&#xA;    **model_inputs,&#xA;    max_new_tokens=512&#xA;)&#xA;generated_ids = [&#xA;    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)&#xA;]&#xA;&#xA;response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;apply_chat_template()&lt;/code&gt; function is used to convert the messages into a format that the model can understand. The &lt;code&gt;add_generation_prompt&lt;/code&gt; argument is used to add a generation prompt, which refers to &lt;code&gt;&amp;lt;|im_start|&amp;gt;assistant\n&lt;/code&gt; to the input. Notably, we apply ChatML template for chat models following our previous practice. The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;tokenizer.batch_decode()&lt;/code&gt; function is used to decode the response. In terms of the input, the above messages is an example to show how to format your dialog history and system prompt. You can use the other size of instruct model in the same way.&lt;/p&gt; &#xA;&lt;h3&gt;üëâüèª Code with Qwen2.5-Coder-32B&lt;/h3&gt; &#xA;&lt;h4&gt;1. Basic Usage&lt;/h4&gt; &#xA;&lt;p&gt;The model completes the code snippets according to the given prompts, without any additional formatting, which is usually termed as &lt;code&gt;code completion&lt;/code&gt; in the code generation tasks.&lt;/p&gt; &#xA;&lt;p&gt;Essentially, we build the tokenizer and the model with &lt;code&gt;from_pretrained&lt;/code&gt; method, and we use generate method to perform code completion. Below is an example on how to chat with Qwen2.5-Coder-32B:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;# Now you do not need to add &#34;trust_remote_code=True&#34;&#xA;TOKENIZER = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;MODEL = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;, device_map=&#34;auto&#34;).eval()&#xA;&#xA;# tokenize the input into tokens&#xA;input_text = &#34;#write a quick sort algorithm&#34;&#xA;model_inputs = TOKENIZER([input_text], return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;# Use `max_new_tokens` to control the maximum output length.&#xA;generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]&#xA;# The generated_ids include prompt_ids, so we only need to decode the tokens after prompt_ids.&#xA;output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)&#xA;&#xA;print(f&#34;Prompt: {input_text}\n\nGenerated text: {output_text}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;max_new_tokens&lt;/code&gt; argument is used to set the maximum length of the response. The &lt;code&gt;input_text&lt;/code&gt; could be any text that you would like model to continue with.&lt;/p&gt; &#xA;&lt;h4&gt;2. Processing Long Texts&lt;/h4&gt; &#xA;&lt;p&gt;The current &lt;code&gt;config.json&lt;/code&gt; is set for context length up to 32,768 tokens. To handle extensive inputs exceeding 32,768 tokens, we utilize &lt;a href=&#34;https://arxiv.org/abs/2309.00071&#34;&gt;YaRN&lt;/a&gt;, a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.&lt;/p&gt; &#xA;&lt;p&gt;For supported frameworks, you could add the following to &lt;code&gt;config.json&lt;/code&gt; to enable YaRN:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  ...,&#xA;  &#34;rope_scaling&#34;: {&#xA;    &#34;factor&#34;: 4.0,&#xA;    &#34;original_max_position_embeddings&#34;: 32768,&#xA;    &#34;type&#34;: &#34;yarn&#34;&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. File-Level Code Completion (Fill in the middle)&lt;/h4&gt; &#xA;&lt;p&gt;The code insertion task, also referred to as the &#34;fill-in-the-middle&#34; challenge, requires the insertion of code segments in a manner that bridges the gaps within a given code context. For an approach aligned with best practices, we recommend adhering to the formatting guidelines outlined in the paper &#34;Efficient Training of Language Models to Fill in the Middle&#34;[&lt;a href=&#34;https://arxiv.org/abs/2207.14255&#34;&gt;arxiv&lt;/a&gt;]. This involves the use of three specialized tokens&lt;code&gt;&amp;lt;fim_prefix&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;fim_suffix&amp;gt;&lt;/code&gt;, and &lt;code&gt;&amp;lt;fim_middle&amp;gt;&lt;/code&gt; to denote the respective segments of the code structure. The prompt should be structured as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#39;&amp;lt;|fim_prefix|&amp;gt;&#39; + prefix_code + &#39;&amp;lt;|fim_suffix|&amp;gt;&#39; + suffix_code + &#39;&amp;lt;|fim_middle|&amp;gt;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Following the approach mentioned, an example would be structured in this manner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;# load model&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;TOKENIZER = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;MODEL = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;, device_map=&#34;auto&#34;).eval()&#xA;&#xA;input_text = &#34;&#34;&#34;&amp;lt;|fim_prefix|&amp;gt;def quicksort(arr):&#xA;    if len(arr) &amp;lt;= 1:&#xA;        return arr&#xA;    pivot = arr[len(arr) // 2]&#xA;    &amp;lt;|fim_suffix|&amp;gt;&#xA;    middle = [x for x in arr if x == pivot]&#xA;    right = [x for x in arr if x &amp;gt; pivot]&#xA;    return quicksort(left) + middle + quicksort(right)&amp;lt;|fim_middle|&amp;gt;&#34;&#34;&#34;&#xA;&#xA;model_inputs = TOKENIZER([input_text], return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;# Use `max_new_tokens` to control the maximum output length.&#xA;generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=False)[0]&#xA;# The generated_ids include prompt_ids, we only need to decode the tokens after prompt_ids.&#xA;output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)&#xA;&#xA;print(f&#34;Prompt: {input_text}\n\nGenerated text: {output_text}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4. Repository-Level Code Completion&lt;/h4&gt; &#xA;&lt;p&gt;The repository level code completion task involves feeding the model the content of multiple files from the same repository. This enables the model to understand the interrelationships between different calls within these files, thereby facilitating the completion of code content. We recommend using the two special tokens &lt;code&gt;&amp;lt;|repo_name|&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;|file_sep|&amp;gt;&lt;/code&gt; to indicate the repository structure. For example, assuming the repository name is stored in &lt;code&gt;repo_name&lt;/code&gt;, and it contains files with their respective paths and contents listed as [(&lt;code&gt;file_path1&lt;/code&gt;, &lt;code&gt;file_content1&lt;/code&gt;), (&lt;code&gt;file_path2&lt;/code&gt;, &lt;code&gt;file_content2&lt;/code&gt;)], the format of the final input prompt would be as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;input_text = f&#39;&#39;&#39;&amp;lt;|repo_name|&amp;gt;{repo_name}&#xA;&amp;lt;|file_sep|&amp;gt;{file_path1} &#xA;{file_content1}&#xA;&amp;lt;|file_sep|&amp;gt;{file_path2} &#xA;{file_content2}&#39;&#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;üëáüèª Below is a complete example of a repository level code completion task: &lt;i&gt;:: click to expand ::&lt;/i&gt;&lt;/summary&gt; &#xA; &lt;div&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;device = &#34;cuda&#34; # the device to load the model onto&#xA;&#xA;# Now you do not need to add &#34;trust_remote_code=True&#34;&#xA;TOKENIZER = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;MODEL = AutoModelForCausalLM.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;, device_map=&#34;auto&#34;).eval()&#xA;&#xA;# tokenize the input into tokens&#xA;input_text = &#34;&#34;&#34;&amp;lt;|repo_name|&amp;gt;library-system&#xA;&amp;lt;|file_sep|&amp;gt;library.py&#xA;class Book:&#xA;    def __init__(self, title, author, isbn, copies):&#xA;        self.title = title&#xA;        self.author = author&#xA;        self.isbn = isbn&#xA;        self.copies = copies&#xA;&#xA;    def __str__(self):&#xA;        return f&#34;Title: {self.title}, Author: {self.author}, ISBN: {self.isbn}, Copies: {self.copies}&#34;&#xA;&#xA;class Library:&#xA;    def __init__(self):&#xA;        self.books = []&#xA;&#xA;    def add_book(self, title, author, isbn, copies):&#xA;        book = Book(title, author, isbn, copies)&#xA;        self.books.append(book)&#xA;&#xA;    def find_book(self, isbn):&#xA;        for book in self.books:&#xA;            if book.isbn == isbn:&#xA;                return book&#xA;        return None&#xA;&#xA;    def list_books(self):&#xA;        return self.books&#xA;&#xA;&amp;lt;|file_sep|&amp;gt;student.py&#xA;class Student:&#xA;    def __init__(self, name, id):&#xA;        self.name = name&#xA;        self.id = id&#xA;        self.borrowed_books = []&#xA;&#xA;    def borrow_book(self, book, library):&#xA;        if book and book.copies &amp;gt; 0:&#xA;            self.borrowed_books.append(book)&#xA;            book.copies -= 1&#xA;            return True&#xA;        return False&#xA;&#xA;    def return_book(self, book, library):&#xA;        if book in self.borrowed_books:&#xA;            self.borrowed_books.remove(book)&#xA;            book.copies += 1&#xA;            return True&#xA;        return False&#xA;&#xA;&amp;lt;|file_sep|&amp;gt;main.py&#xA;from library import Library&#xA;from student import Student&#xA;&#xA;def main():&#xA;    # Set up the library with some books&#xA;    library = Library()&#xA;    library.add_book(&#34;The Great Gatsby&#34;, &#34;F. Scott Fitzgerald&#34;, &#34;1234567890&#34;, 3)&#xA;    library.add_book(&#34;To Kill a Mockingbird&#34;, &#34;Harper Lee&#34;, &#34;1234567891&#34;, 2)&#xA;    &#xA;    # Set up a student&#xA;    student = Student(&#34;Alice&#34;, &#34;S1&#34;)&#xA;    &#xA;    # Student borrows a book&#xA;&#34;&#34;&#34;&#xA;model_inputs = TOKENIZER([input_text], return_tensors=&#34;pt&#34;).to(device)&#xA;&#xA;# Use `max_new_tokens` to control the maximum output length.&#xA;generated_ids = MODEL.generate(model_inputs.input_ids, max_new_tokens=1024, do_sample=False)[0]&#xA;# The generated_ids include prompt_ids, so we only need to decode the tokens after prompt_ids.&#xA;output_text = TOKENIZER.decode(generated_ids[len(model_inputs.input_ids[0]):], skip_special_tokens=True)&#xA;&#xA;print(f&#34;Prompt: \n{input_text}\n\nGenerated text: \n{output_text}&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;The expected output as following:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Generated text:&#xA;    book = library.find_book(&#34;1234567890&#34;)&#xA;    if student.borrow_book(book, library):&#xA;    print(f&#34;{student.name} borrowed {book.title}&#34;)&#xA;    else:&#xA;    print(f&#34;{student.name} could not borrow {book.title}&#34;)&#xA;    &#xA;        # Student returns a book&#xA;        if student.return_book(book, library):&#xA;            print(f&#34;{student.name} returned {book.title}&#34;)&#xA;        else:&#xA;            print(f&#34;{student.name} could not return {book.title}&#34;)&#xA;        &#xA;        # List all books in the library&#xA;        print(&#34;All books in the library:&#34;)&#xA;        for book in library.list_books():&#xA;            print(book)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;main()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;üëâüèª Deploying Qwen2.5-Coder with vLLM&lt;/h3&gt; &#xA;&lt;p&gt;As a family member of Qwen2.5, Qwen2.5-Coder are supported by vLLM. The detail tutorial could be found in &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/deployment/vllm.html&#34;&gt;Qwen tutorial&lt;/a&gt;. Here, we give you an simple example of offline batched inference in vLLM.&lt;/p&gt; &#xA;&lt;h4&gt;Offline Batched Inference&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer&#xA;from vllm import LLM, SamplingParams&#xA;# Initialize the tokenizer&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;&#xA;# Pass the default decoding hyperparameters of Qwen1.5-32B-Chat&#xA;# max_tokens is for the maximum length for generation.&#xA;sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=1024)&#xA;&#xA;# Input the model name or path. Can be GPTQ or AWQ models.&#xA;llm = LLM(model=&#34;Qwen/Qwen2.5-Coder-32B&#34;)&#xA;&#xA;# Prepare your prompts&#xA;prompt = &#34;#write a quick sort algorithm.\ndef quick_sort(&#34;&#xA;&#xA;# generate outputs&#xA;outputs = llm.generate([prompt], sampling_params)&#xA;&#xA;# Print the outputs.&#xA;for output in outputs:&#xA;    prompt = output.prompt&#xA;    generated_text = output.outputs[0].text&#xA;    print(f&#34;Prompt: {prompt!r}, Generated text: {generated_text!r}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi-GPU Distributed Serving&lt;/h4&gt; &#xA;&lt;p&gt;To scale up your serving throughputs, distributed serving helps you by leveraging more GPU devices. When using ultra-long sequences for inference, it might cause insufficient GPU memory. Here, we demonstrate how to run Qwen2.5-Coder-32B with tensor parallelism just by passing in the argument &lt;code&gt;tensor_parallel_size&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;llm = LLM(model=&#34;Qwen/Qwen2.5-Coder-32B&#34;, tensor_parallel_size=8)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üëâüèª Gradio interface ü§ó&lt;/h3&gt; &#xA;&lt;p&gt;We also provide a Gradio &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/gradio-app/gradio&#34;&gt;&lt;/a&gt; interface for a better experience, just run by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd demo/chatbot/&#xA;# For Linux and Windows users (and macOS with Intel??)&#xA;python app.py &#xA;&#xA;# For macOS with Apple Silicon users, Intel not supported, this maybe 20x slower than RTX 4090&#xA;PYTORCH_ENABLE_MPS_FALLBACK=1 python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide a Gradio interface of artifacts mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd demo/artifacts/&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify the &lt;code&gt;--server_port&lt;/code&gt;, &lt;code&gt;--share&lt;/code&gt;, &lt;code&gt;--server_name&lt;/code&gt; arguments to satisfy your needs!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Or, try it out effortlessly on HuggingFace: &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-demo&#34;&gt;„Äåchatbot demo„Äç&lt;/a&gt; ü§ó &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-Coder-Artifacts&#34;&gt;„Äåartifacts demo„Äç&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;For more information, please refer to the &lt;a href=&#34;https://arxiv.org/abs/2409.12186&#34;&gt;Qwen2.5-Coder Technical Report&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#QwenLM/Qwen2.5-Coder&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=QwenLM/Qwen2.5-Coder&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{hui2024qwen2,&#xA;  title={Qwen2. 5-Coder Technical Report},&#xA;  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},&#xA;  journal={arXiv preprint arXiv:2409.12186},&#xA;  year={2024}&#xA;}&#xA;@article{qwen2,&#xA;    title={Qwen2 Technical Report},&#xA;    author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},&#xA;    journal={arXiv preprint arXiv:2407.10671},&#xA;    year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt; or &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p align=&#34;right&#34; style=&#34;font-size: 14px; color: #555; margin-top: 20px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5-Coder/main/#readme-top&#34; style=&#34;text-decoration: none; color: #007bff; font-weight: bold;&#34;&gt; ‚Üë Back to Top ‚Üë &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dockur/macos</title>
    <updated>2024-11-14T01:28:28Z</updated>
    <id>tag:github.com,2024-11-14:/dockur/macos</id>
    <link href="https://github.com/dockur/macos" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OSX (macOS) inside a Docker container.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;OSX&lt;br&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://github.com/dockur/macos/&#34;&gt;&lt;img src=&#34;https://github.com/dockur/macos/raw/master/.github/logo.png&#34; title=&#34;Logo&#34; style=&#34;max-width:100%;&#34; width=&#34;128&#34;&gt;&lt;/a&gt; &#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/dockur/macos/&#34;&gt;&lt;img src=&#34;https://github.com/dockur/macos/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/dockurr/macos/tags&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/dockurr/macos/latest?arch=amd64&amp;amp;sort=semver&amp;amp;color=066da5&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/dockurr/macos/tags&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/image-size/dockurr/macos/latest?color=066da5&amp;amp;label=size&#34; alt=&#34;Size&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/dockur/macos/pkgs/container/macos&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fipitio.github.io%2Fbackage%2Fdockur%2Fmacos%2Fmacos.json&amp;amp;query=%24.downloads&amp;amp;logo=github&amp;amp;style=flat&amp;amp;color=066da5&amp;amp;label=pulls&#34; alt=&#34;Package&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/dockurr/macos/&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/pulls/dockurr/macos.svg?style=flat&amp;amp;label=pulls&amp;amp;logo=docker&#34; alt=&#34;Pulls&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;/div&gt;&lt;/h1&gt; &#xA;&lt;p&gt;OSX (macOS) inside a Docker container.&lt;/p&gt; &#xA;&lt;h2&gt;Features ‚ú®&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;KVM acceleration&lt;/li&gt; &#xA; &lt;li&gt;Web-based viewer&lt;/li&gt; &#xA; &lt;li&gt;Automatic download&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage üê≥&lt;/h2&gt; &#xA;&lt;p&gt;Via Docker Compose:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;services:&#xA;  macos:&#xA;    image: dockurr/macos&#xA;    container_name: macos&#xA;    environment:&#xA;      VERSION: &#34;13&#34;&#xA;    devices:&#xA;      - /dev/kvm&#xA;    cap_add:&#xA;      - NET_ADMIN&#xA;    ports:&#xA;      - 8006:8006&#xA;      - 5900:5900/tcp&#xA;      - 5900:5900/udp&#xA;    stop_grace_period: 2m&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Via Docker CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run -it --rm -p 8006:8006 --device=/dev/kvm --cap-add NET_ADMIN --stop-timeout 120 dockurr/macos&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Via Kubernetes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;kubectl apply -f https://raw.githubusercontent.com/dockur/macos/refs/heads/master/kubernetes.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Compatibility ‚öôÔ∏è&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Product&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Platform&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docker Engine&lt;/td&gt; &#xA;   &lt;td&gt;Linux&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docker Desktop&lt;/td&gt; &#xA;   &lt;td&gt;Linux&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docker Desktop&lt;/td&gt; &#xA;   &lt;td&gt;macOS&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docker Desktop&lt;/td&gt; &#xA;   &lt;td&gt;Windows 11&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docker Desktop&lt;/td&gt; &#xA;   &lt;td&gt;Windows 10&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;FAQ üí¨&lt;/h2&gt; &#xA;&lt;h3&gt;How do I use it?&lt;/h3&gt; &#xA;&lt;p&gt;Very simple! These are the steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Start the container and connect to &lt;a href=&#34;http://localhost:8006&#34;&gt;port 8006&lt;/a&gt; using your web browser.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Choose &lt;code&gt;Disk Utility&lt;/code&gt; and then select the largest &lt;code&gt;Apple Inc. VirtIO Block Media&lt;/code&gt; disk.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click the &lt;code&gt;Erase&lt;/code&gt; button to format the disk, and give it any recognizable name you like.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Close the current window and proceed the installation by clicking &lt;code&gt;Reinstall macOS&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;When prompted where you want to install it, select the disk you just created previously.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After all files are copied, select your region, language, and account settings.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Enjoy your brand new machine, and don&#39;t forget to star this repo!&lt;/p&gt; &#xA;&lt;h3&gt;How do I select the macOS version?&lt;/h3&gt; &#xA;&lt;p&gt;By default, macOS 13 (Ventura) will be installed, as it offers the best performance.&lt;/p&gt; &#xA;&lt;p&gt;But you can add the &lt;code&gt;VERSION&lt;/code&gt; environment variable to your compose file, in order to specify an alternative macOS version to be downloaded:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;environment:&#xA;  VERSION: &#34;13&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Select from the values below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Version&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Name&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;15&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 15&lt;/td&gt; &#xA;   &lt;td&gt;Sequoia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;14&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 14&lt;/td&gt; &#xA;   &lt;td&gt;Sonoma&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;13&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 13&lt;/td&gt; &#xA;   &lt;td&gt;Ventura&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;12&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 12&lt;/td&gt; &#xA;   &lt;td&gt;Monterey&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;11&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;macOS 11&lt;/td&gt; &#xA;   &lt;td&gt;Big Sur&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;How do I change the storage location?&lt;/h3&gt; &#xA;&lt;p&gt;To change the storage location, include the following bind mount in your compose file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;volumes:&#xA;  - /var/osx:/storage&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace the example path &lt;code&gt;/var/osx&lt;/code&gt; with the desired storage folder.&lt;/p&gt; &#xA;&lt;h3&gt;How do I change the size of the disk?&lt;/h3&gt; &#xA;&lt;p&gt;To expand the default size of 64 GB, add the &lt;code&gt;DISK_SIZE&lt;/code&gt; setting to your compose file and set it to your preferred capacity:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;environment:&#xA;  DISK_SIZE: &#34;256G&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] This can also be used to resize the existing disk to a larger capacity without any data loss.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;How do I change the amount of CPU or RAM?&lt;/h3&gt; &#xA;&lt;p&gt;By default, the container will be allowed to use a maximum of 2 CPU cores and 4 GB of RAM.&lt;/p&gt; &#xA;&lt;p&gt;If you want to adjust this, you can specify the desired amount using the following environment variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;environment:&#xA;  RAM_SIZE: &#34;8G&#34;&#xA;  CPU_CORES: &#34;4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How do I pass-through a USB device?&lt;/h3&gt; &#xA;&lt;p&gt;To pass-through a USB device, first lookup its vendor and product id via the &lt;code&gt;lsusb&lt;/code&gt; command, then add them to your compose file like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;environment:&#xA;  ARGUMENTS: &#34;-device usb-host,vendorid=0x1234,productid=0x1234&#34;&#xA;devices:&#xA;  - /dev/bus/usb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How do I verify if my system supports KVM?&lt;/h3&gt; &#xA;&lt;p&gt;Only Linux and Windows 11 support KVM virtualization, macOS and Windows 10 do not unfortunately.&lt;/p&gt; &#xA;&lt;p&gt;You can run the following commands in Linux to check your system:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install cpu-checker&#xA;sudo kvm-ok&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you receive an error from &lt;code&gt;kvm-ok&lt;/code&gt; indicating that KVM cannot be used, please check whether:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;the virtualization extensions (&lt;code&gt;Intel VT-x&lt;/code&gt; or &lt;code&gt;AMD SVM&lt;/code&gt;) are enabled in your BIOS.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;you enabled &#34;nested virtualization&#34; if you are running the container inside a virtual machine.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;you are not using a cloud provider, as most of them do not allow nested virtualization for their VPS&#39;s.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you do not receive any error from &lt;code&gt;kvm-ok&lt;/code&gt; but the container still complains about KVM, please check whether:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;you are not using &#34;Docker Desktop for Linux&#34; as it does not support KVM, instead make use of Docker Engine directly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;it could help to add &lt;code&gt;privileged: true&lt;/code&gt; to your compose file (or &lt;code&gt;sudo&lt;/code&gt; to your &lt;code&gt;docker run&lt;/code&gt; command), to rule out any permission issue.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How do I run Windows in a container?&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;a href=&#34;https://github.com/dockur/windows&#34;&gt;dockur/windows&lt;/a&gt; for that. It shares many of the same features, and even has completely automatic installation.&lt;/p&gt; &#xA;&lt;h3&gt;Is this project legal?&lt;/h3&gt; &#xA;&lt;p&gt;Yes, this project contains only open-source code and does not distribute any copyrighted material. Neither does it try to circumvent any copyright protection measures. So under all applicable laws, this project will be considered legal.&lt;/p&gt; &#xA;&lt;p&gt;However, by installing Apple&#39;s macOS, you must accept their end-user license agreement, which does not permit installation on non-official hardware. So only run this container on hardware sold by Apple, as any other use will be a violation of their terms and conditions.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements üôè&lt;/h2&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://github.com/seitenca&#34;&gt;seitenca&lt;/a&gt;, this project would not exist without her invaluable work.&lt;/p&gt; &#xA;&lt;h2&gt;Stars üåü&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/dockur/macos&#34;&gt;&lt;img src=&#34;https://starchart.cc/dockur/macos.svg?variant=adaptive&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer ‚öñÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Only run this container on Apple hardware, any other use is not permitted by their EULA. The product names, logos, brands, and other trademarks referred to within this project are the property of their respective trademark holders. This project is not affiliated, sponsored, or endorsed by Apple Inc.&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/vscode-ai-toolkit</title>
    <updated>2024-11-14T01:28:28Z</updated>
    <id>tag:github.com,2024-11-14:/microsoft/vscode-ai-toolkit</id>
    <link href="https://github.com/microsoft/vscode-ai-toolkit" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Developer Document: AI Toolkit for Visual Studio Code&lt;/h1&gt; &#xA;&lt;p&gt;This is the temporary documentation site for AI Toolkit for VS Code.&lt;/p&gt; &#xA;&lt;p&gt;For now we have the following topics covered:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/vscode-ai-toolkit/main/doc/overview.md&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/vscode-ai-toolkit/main/doc/get_started.md&#34;&gt;Get Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/vscode-ai-toolkit/main/doc/models.md&#34;&gt;Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/vscode-ai-toolkit/main/doc/playground.md&#34;&gt;Playground&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/vscode-ai-toolkit/main/doc/finetune.md&#34;&gt;Finetune&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/microsoft/vscode-ai-toolkit/main/doc/faq.md&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you have any question not yet covered, or you have suggestion of the doc topics and contents, you can create a GitHub issue on this repo.&lt;/p&gt;</summary>
  </entry>
</feed>