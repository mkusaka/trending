<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-16T01:23:44Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tonyke-bot/ore-miner</title>
    <updated>2024-04-16T01:23:44Z</updated>
    <id>tag:github.com,2024-04-16:/tonyke-bot/ore-miner</id>
    <link href="https://github.com/tonyke-bot/ore-miner" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ORE Miner built on top of Jito bundle with both CPU and GPU support.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;$ORE Miner&lt;/h1&gt; &#xA;&lt;p&gt;ORE Miner built on top of Jito bundle service by &lt;a href=&#34;https://x.com/tonyke_bot&#34;&gt;@tonyke_bot&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/shoucccc&#34;&gt;@shoucccc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Shipped with both CPU and GPU hashing support.&lt;/p&gt; &#xA;&lt;p&gt;Each miner is able to carry 400 wallets on a single RTX 4090 card. Should expect 10~20% improvement if the code is optimized.&lt;/p&gt; &#xA;&lt;h2&gt;Preparations&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Get a reliable, fastest Solana RPC&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repo and build&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/tonyke-bot/ore-miner.git&#xA;cd ore-miner&#xA;cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) Install CUDA development environment&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(Optional) Build CUDA miner&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./build-cuda-worker.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Generate wallets and fund them with SOL&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Feature&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Evenly consumed SOL: Choose richest wallet to tip bundle and richest wallet in a transaction to pay the transaction fee.&lt;/li&gt; &#xA; &lt;li&gt;Adaptive tip: Automatically adjust tip based on the Jito tip stream.&lt;/li&gt; &#xA; &lt;li&gt;Bulk operation support: mine, register, claim, batch transfer&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h4&gt;Mine with GPU&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;export CUDA_VISIBLE_DEVICES=&amp;lt;GPU_INDEX&amp;gt;&#xA;&#xA;cargo run --release -- \&#xA;    --rpc &amp;lt;RPC_URL&amp;gt; \&#xA;    --priority-fee 500000 \                     # Tip used for Jito bundle. If max adaptive tip is set, this will be the initial tip.&#xA;    bundle-mine-gpu \&#xA;    --key-folder &amp;lt;FOLDER_CONTAINS_YOUR_KEYS&amp;gt; \  # Folder contains your Solana keys&#xA;    --max-adaptive-tip 400000 \                 # Max tip used, if this is set, use tip min(tips.p50, max)****&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Multi Claim&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --release -- \&#xA;    --rpc &amp;lt;RPC_URL&amp;gt; \&#xA;    --priority-fee 500000 \                     # Tip used for Jito bundle. &#xA;    claim \&#xA;    --key-folder &amp;lt;FOLDER_CONTAINS_YOUR_KEYS&amp;gt; \  # Folder contains your Solana keys&#xA;    --beneficiary &amp;lt;YOUR_PUBKEY_TO_RECEIVE_ORE&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Register&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --release -- \&#xA;    --rpc &amp;lt;RPC_URL&amp;gt; \&#xA;    --priority-fee 500000 \                     # Tip used for Jito bundle. &#xA;    register \&#xA;    --key-folder &amp;lt;FOLDER_CONTAINS_YOUR_KEYS&amp;gt; \  # Folder contains your Solana keys&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Buy me ☕️&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SOL: &lt;code&gt;tonyi4UznxNzae5RBinHTU8Gxr91RRGBcdx7mmimN8F&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;EVM: &lt;code&gt;0x45Fce32abB76fd0722882326FBf2d1182e6b982B&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Appreciate your support!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenBMB/MiniCPM-V</title>
    <updated>2024-04-16T01:23:44Z</updated>
    <id>tag:github.com,2024-04-16:/OpenBMB/MiniCPM-V</id>
    <link href="https://github.com/OpenBMB/MiniCPM-V" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MiniCPM-V 2.0: An Efficient End-side MLLM with Strong OCR and Understanding Capabilities&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;!-- &lt;h1 style=&#34;color: #33A6B8; font-family: Helvetica&#34;&gt; OmniLMM &lt;/h1&gt; --&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv-omnilmm.png&#34; width=&#34;400em&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;性能领先且部署高效的多模态大模型&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;中文 | &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/README_en.md&#34;&gt;English&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; MiniCPM-V 2.0 &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2/&#34;&gt;🤗&lt;/a&gt; &lt;a href=&#34;http://120.92.209.146:80/&#34;&gt;🤖&lt;/a&gt; | OmniLMM-12B &lt;a href=&#34;https://huggingface.co/openbmb/OmniLMM-12B/&#34;&gt;🤗&lt;/a&gt; &lt;a href=&#34;http://120.92.209.146:8081&#34;&gt;🤖&lt;/a&gt; | &lt;a href=&#34;https://openbmb.vercel.app/minicpm-v-2&#34;&gt;MiniCPM-V 2.0 技术博客&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-V&lt;/strong&gt;和&lt;strong&gt;OmniLMM&lt;/strong&gt; 是面向图文理解的开源多模态大模型系列。该系列模型接受图像和文本输入，并提供高质量的文本输出。我们发布了两个版本的模型，旨在实现&lt;strong&gt;领先的性能和高效的部署&lt;/strong&gt;：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 2.8B&lt;/strong&gt;：可在终端设备上部署的先进多模态大模型。最新发布的 MiniCPM-V 2.0 可以接受 180 万像素的任意长宽比图像输入，实现了和 Gemini Pro 相近的场景文字识别能力以及和 GPT-4V 相匹的低幻觉率。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;OmniLMM-12B&lt;/strong&gt;：相比同规模其他模型在多个基准测试中具有领先性能，实现了相比 GPT-4V 更低的幻觉率。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;更新日志 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024.04.12] 我们开源了 MiniCPM-V 2.0，该模型刷新了 OCRBench 开源模型最佳成绩，在场景文字识别能力上比肩 Gemini Pro，同时还在综合了 11 个主流多模态大模型评测基准的 &lt;a href=&#34;https://rank.opencompass.org.cn/leaderboard-multimodal&#34;&gt;OpenCompass&lt;/a&gt; 榜单上超过了 Qwen-VL-Chat 10B、CogVLM-Chat 17B 和 Yi-VL 34B 等更大参数规模的模型！点击&lt;a href=&#34;https://openbmb.vercel.app/minicpm-v-2&#34;&gt;这里&lt;/a&gt;查看 MiniCPM-V 2.0 技术博客。&lt;/li&gt; &#xA; &lt;li&gt;[2024.03.14] MiniCPM-V 现在支持 SWIFT 框架下的&lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;微调&lt;/a&gt;了，感谢 &lt;a href=&#34;https://github.com/Jintao-Huang&#34;&gt;Jintao&lt;/a&gt; 的贡献！&lt;/li&gt; &#xA; &lt;li&gt;[2024.03.01] MiniCPM-V 现在支持在 Mac 电脑上进行部署！&lt;/li&gt; &#xA; &lt;li&gt;[2024.02.01] 我们开源了 MiniCPM-V 和 OmniLMM-12B，分别可以支持高效的端侧部署和同规模领先的多模态能力！&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;目录 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;!-- TOC --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#minicpm-v-28b&#34;&gt;MiniCPM-V 2.8B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#omnilmm-12b&#34;&gt;OmniLMM-12B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E5%AE%89%E8%A3%85&#34;&gt;安装&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%8E%A8%E7%90%86&#34;&gt;推理&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%A8%A1%E5%9E%8B%E5%BA%93&#34;&gt;模型库&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D&#34;&gt;多轮对话&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#mac-%E6%8E%A8%E7%90%86&#34;&gt;Mac 推理&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%89%8B%E6%9C%BA%E7%AB%AF%E9%83%A8%E7%BD%B2&#34;&gt;手机端部署&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/#%E6%9C%AA%E6%9D%A5%E8%AE%A1%E5%88%92&#34;&gt;未来计划&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;!-- /TOC --&gt; &#xA;&lt;h2&gt;MiniCPM-V 2.8B&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-V 2.8B&lt;/strong&gt;可以高效部署到终端设备。该模型基于 SigLip-400M 和 &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/&#34;&gt;MiniCPM-2.4B&lt;/a&gt;构建，通过perceiver resampler连接。最新发布的 MiniCPM-V 2.0 的特点包括：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;优秀的性能。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 在多个测试基准（如 OCRBench, TextVQA, MME, MMB, MathVista 等）中实现了 7B 以下模型的&lt;strong&gt;最佳性能&lt;/strong&gt;。&lt;strong&gt;在综合了 11 个主流多模态大模型评测基准的 OpenCompass 榜单上超过了 Qwen-VL-Chat 9.6B、CogVLM-Chat 17.4B 和 Yi-VL 34B 等更大参数规模的模型&lt;/strong&gt;。MiniCPM-V 2.0 还展现出&lt;strong&gt;领先的 OCR 能力&lt;/strong&gt;，在场景文字识别能力上&lt;strong&gt;接近 Gemini Pro&lt;/strong&gt;，OCRBench 得分达到&lt;strong&gt;开源模型第一&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🏆 &lt;strong&gt;可信行为。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;多模态大模型深受幻觉问题困扰，模型经常生成和图像中的事实不符的文本。MiniCPM-V 2.0 是 &lt;strong&gt;第一个通过多模态 RLHF 对齐的端侧多模态大模型&lt;/strong&gt;（借助 &lt;a href=&#34;https://rlhf-v.github.io/&#34;&gt;RLHF-V&lt;/a&gt; [CVPR&#39;24] 系列技术）。该模型在 &lt;a href=&#34;https://arxiv.org/abs/2312.00849&#34;&gt;Object HalBench&lt;/a&gt; 达到&lt;strong&gt;和 GPT-4V 相仿&lt;/strong&gt;的性能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🌟 &lt;strong&gt;高清图像高效编码。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 可以接受 &lt;strong&gt;180 万像素的任意长宽比图像输入&lt;/strong&gt;（基于最新的&lt;a href=&#34;https://arxiv.org/pdf/2403.11703.pdf&#34;&gt;LLaVA-UHD&lt;/a&gt; 技术），这使得模型可以感知到小物体、密集文字等更加细粒度的视觉信息。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;⚡️ &lt;strong&gt;高效部署。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 可以&lt;strong&gt;高效部署在大多数消费级显卡和个人电脑上&lt;/strong&gt;，包括&lt;strong&gt;移动手机等终端设备&lt;/strong&gt;。在视觉编码方面，我们通过perceiver resampler将图像表示压缩为更少的 token。这使得 MiniCPM-V 2.0 即便是&lt;strong&gt;面对高分辨率图像，也能占用较低的存储并展现优秀的推理速度&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🙌 &lt;strong&gt;双语支持。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MiniCPM-V 2.0 &lt;strong&gt;提供领先的中英双语多模态能力支持&lt;/strong&gt;。 该能力通过 &lt;a href=&#34;https://arxiv.org/abs/2308.12038&#34;&gt;VisCPM&lt;/a&gt; [ICLR&#39;24] 论文中提出的多模态能力的跨语言泛化技术实现。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;性能评估 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv-2-peformance.png&#34; width=&#34;66%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;TextVQA, DocVQA, OCRBench, OpenCompass, MME, MMBench, MMMU, MathVista, LLaVA Bench, Object HalBench 上的详细评测结果。 &lt;/summary&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;Size&lt;/th&gt; &#xA;     &lt;th&gt;TextVQA val&lt;/th&gt; &#xA;     &lt;th&gt;DocVQA test&lt;/th&gt; &#xA;     &lt;th&gt;OCRBench&lt;/th&gt; &#xA;     &lt;th&gt;OpenCompass&lt;/th&gt; &#xA;     &lt;th nowrap&gt;MME&lt;/th&gt; &#xA;     &lt;th&gt;MMB dev(en)&lt;/th&gt; &#xA;     &lt;th&gt;MMB dev(zh)&lt;/th&gt; &#xA;     &lt;th&gt;MMMU val&lt;/th&gt; &#xA;     &lt;th&gt;MathVista&lt;/th&gt; &#xA;     &lt;th&gt;LLaVA Bench&lt;/th&gt; &#xA;     &lt;th nowrap&gt;Object HalBench&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody align=&#34;center&#34;&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;12&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary models&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Gemini Pro Vision&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;74.6&lt;/td&gt; &#xA;     &lt;td&gt;88.1&lt;/td&gt; &#xA;     &lt;td&gt;680&lt;/td&gt; &#xA;     &lt;td&gt;63.8&lt;/td&gt; &#xA;     &lt;td&gt;2148.9&lt;/td&gt; &#xA;     &lt;td&gt;75.2&lt;/td&gt; &#xA;     &lt;td&gt;74.0&lt;/td&gt; &#xA;     &lt;td&gt;48.9&lt;/td&gt; &#xA;     &lt;td&gt;45.8&lt;/td&gt; &#xA;     &lt;td&gt;79.9&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT-4V&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;78.0&lt;/td&gt; &#xA;     &lt;td&gt;88.4&lt;/td&gt; &#xA;     &lt;td&gt;645&lt;/td&gt; &#xA;     &lt;td&gt;63.2&lt;/td&gt; &#xA;     &lt;td&gt;1771.5&lt;/td&gt; &#xA;     &lt;td&gt;75.1&lt;/td&gt; &#xA;     &lt;td&gt;75.0&lt;/td&gt; &#xA;     &lt;td&gt;53.8&lt;/td&gt; &#xA;     &lt;td&gt;47.8&lt;/td&gt; &#xA;     &lt;td&gt;93.1&lt;/td&gt; &#xA;     &lt;td&gt;86.4 / 92.7&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;12&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source models 6B~34B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Yi-VL-6B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;6.7B&lt;/td&gt; &#xA;     &lt;td&gt;45.5*&lt;/td&gt; &#xA;     &lt;td&gt;17.1*&lt;/td&gt; &#xA;     &lt;td&gt;290&lt;/td&gt; &#xA;     &lt;td&gt;49.3&lt;/td&gt; &#xA;     &lt;td&gt;1915.1 &lt;/td&gt; &#xA;     &lt;td&gt;68.6 &lt;/td&gt; &#xA;     &lt;td&gt;68.3 &lt;/td&gt; &#xA;     &lt;td&gt;40.3 &lt;/td&gt; &#xA;     &lt;td&gt;28.8 &lt;/td&gt; &#xA;     &lt;td&gt;51.9 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;9.6B&lt;/td&gt; &#xA;     &lt;td&gt;61.5&lt;/td&gt; &#xA;     &lt;td&gt;62.6&lt;/td&gt; &#xA;     &lt;td&gt;488 &lt;/td&gt; &#xA;     &lt;td&gt;52.1 &lt;/td&gt; &#xA;     &lt;td&gt;1860.0 &lt;/td&gt; &#xA;     &lt;td&gt;60.6 &lt;/td&gt; &#xA;     &lt;td&gt;56.7 &lt;/td&gt; &#xA;     &lt;td&gt;37.0 &lt;/td&gt; &#xA;     &lt;td&gt;33.8 &lt;/td&gt; &#xA;     &lt;td&gt;67.7 &lt;/td&gt; &#xA;     &lt;td&gt;56.2 / 80.0&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Yi-VL-34B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;34B&lt;/td&gt; &#xA;     &lt;td&gt;43.4*&lt;/td&gt; &#xA;     &lt;td&gt;16.9*&lt;/td&gt; &#xA;     &lt;td&gt;290&lt;/td&gt; &#xA;     &lt;td&gt;52.6 &lt;/td&gt; &#xA;     &lt;td&gt;2050.2&lt;/td&gt; &#xA;     &lt;td&gt;71.1&lt;/td&gt; &#xA;     &lt;td&gt;71.4&lt;/td&gt; &#xA;     &lt;td&gt;45.1&lt;/td&gt; &#xA;     &lt;td&gt;30.7&lt;/td&gt; &#xA;     &lt;td&gt;62.3&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;DeepSeek-VL-7B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;7.3B&lt;/td&gt; &#xA;     &lt;td&gt;64.7*&lt;/td&gt; &#xA;     &lt;td&gt;47.0* &lt;/td&gt; &#xA;     &lt;td&gt;435&lt;/td&gt; &#xA;     &lt;td&gt;55.6 &lt;/td&gt; &#xA;     &lt;td&gt;1765.4 &lt;/td&gt; &#xA;     &lt;td&gt;74.1 &lt;/td&gt; &#xA;     &lt;td&gt;72.8 &lt;/td&gt; &#xA;     &lt;td&gt;38.3 &lt;/td&gt; &#xA;     &lt;td&gt;36.8&lt;/td&gt; &#xA;     &lt;td&gt;77.8 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;TextMonkey&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;9.7B&lt;/td&gt; &#xA;     &lt;td&gt;64.3&lt;/td&gt; &#xA;     &lt;td&gt;66.7 &lt;/td&gt; &#xA;     &lt;td&gt;558&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;CogVLM-Chat&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;17.4B&lt;/td&gt; &#xA;     &lt;td&gt;70.4&lt;/td&gt; &#xA;     &lt;td&gt;33.3*&lt;/td&gt; &#xA;     &lt;td&gt;590 &lt;/td&gt; &#xA;     &lt;td&gt;52.5 &lt;/td&gt; &#xA;     &lt;td&gt;1736.6 &lt;/td&gt; &#xA;     &lt;td&gt;63.7 &lt;/td&gt; &#xA;     &lt;td&gt;53.8 &lt;/td&gt; &#xA;     &lt;td&gt;37.3 &lt;/td&gt; &#xA;     &lt;td&gt;34.7 &lt;/td&gt; &#xA;     &lt;td&gt;73.9 &lt;/td&gt; &#xA;     &lt;td&gt;73.6 / 87.4 &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;12&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source models 1B~3B &lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;DeepSeek-VL-1.3B&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;1.7B&lt;/td&gt; &#xA;     &lt;td&gt;58.4*&lt;/td&gt; &#xA;     &lt;td&gt;37.9*&lt;/td&gt; &#xA;     &lt;td&gt;413&lt;/td&gt; &#xA;     &lt;td&gt;46.0 &lt;/td&gt; &#xA;     &lt;td&gt;1531.6 &lt;/td&gt; &#xA;     &lt;td&gt;64.0 &lt;/td&gt; &#xA;     &lt;td&gt;61.2 &lt;/td&gt; &#xA;     &lt;td&gt;33.8 &lt;/td&gt; &#xA;     &lt;td&gt;29.4 &lt;/td&gt; &#xA;     &lt;td&gt;51.1 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MobileVLM V2&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;3.1B&lt;/td&gt; &#xA;     &lt;td&gt;57.5&lt;/td&gt; &#xA;     &lt;td&gt;19.4*&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1440.5(P) &lt;/td&gt; &#xA;     &lt;td&gt;63.2 &lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Mini-Gemini&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;2.2B&lt;/td&gt; &#xA;     &lt;td&gt;56.2&lt;/td&gt; &#xA;     &lt;td&gt;34.2*&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1653.0 &lt;/td&gt; &#xA;     &lt;td&gt;59.8 &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;31.7 &lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;     &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;2.8B &lt;/td&gt; &#xA;     &lt;td&gt;60.6&lt;/td&gt; &#xA;     &lt;td&gt;38.2 &lt;/td&gt; &#xA;     &lt;td&gt;366&lt;/td&gt; &#xA;     &lt;td&gt;47.6&lt;/td&gt; &#xA;     &lt;td&gt;1650.2 &lt;/td&gt; &#xA;     &lt;td&gt;67.9 &lt;/td&gt; &#xA;     &lt;td&gt;65.3 &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;38.3&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;28.9&lt;/td&gt; &#xA;     &lt;td&gt;51.3 &lt;/td&gt; &#xA;     &lt;td&gt;78.4 / 88.5 &lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;&lt;strong&gt;MiniCPM-V 2.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td align=&#34;right&#34;&gt;2.8B &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;74.1&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;71.9&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;605&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;1808.6&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;69.6&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;68.1&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;38.2 &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;38.7&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;69.2&lt;/strong&gt; &lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;85.5 / 92.2 &lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt; * 我们自己评测了正式开源的模型权重。 &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;典型示例 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/minicpmv2-cases_2.png&#34; width=&#34;95%/&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;p&gt;我们将 MiniCPM-V 2.0 部署在小米 14 Pro 上，并录制了以下演示视频，未经任何视频剪辑。&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/station.gif&#34; width=&#34;36%/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/gif_cases/london_car.gif&#34; width=&#34;36%/&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;MiniCPM-V 1.0 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;请参考&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/minicpm_v1.md&#34;&gt;这里&lt;/a&gt;了解 MiniCPM-V 1.0 的信息和使用教程。&lt;/p&gt; &#xA;&lt;h2&gt;OmniLMM-12B&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;OmniLMM-12B&lt;/strong&gt; 是当前系列中性能最佳的版本。该模型基于EVA02-5B和Zephyr-7B-β初始化构建，并使用perceiver resampler连接，采用了课程学习的方法在多模态数据上进行训练。该模型具有三个特点：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;🔥 &lt;strong&gt;性能领先。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;OmniLMM-12B 相比其他同规模模型在多个基准测试中取得&lt;strong&gt;领先的性能&lt;/strong&gt;（包括 MME、MMBench、SEED-Bench 等），模型掌握了较为丰富的多模态世界知识。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🏆 &lt;strong&gt;行为可信。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;多模态大模型的幻觉问题备受关注，模型经常生成和图像中的事实不符的文本（例如，确信地描述图片中并不存在的物体）。OmniLMM-12B是 &lt;strong&gt;第一个通过多模态 RLHF 对齐的综合能力优秀的开源多模态大模型&lt;/strong&gt;（借助 &lt;a href=&#34;https://rlhf-v.github.io/&#34;&gt;RLHF-V&lt;/a&gt; [CVPR&#39;24] 系列技术）。该模型在 &lt;a href=&#34;https://huggingface.co/datasets/Shengcao1006/MMHal-Bench&#34;&gt;MMHal-Bench&lt;/a&gt; 幻觉评测基准上达到&lt;strong&gt;开源模型最佳水平&lt;/strong&gt;，并在 &lt;a href=&#34;https://arxiv.org/abs/2312.00849&#34;&gt;Object HalBench&lt;/a&gt; 中&lt;strong&gt;优于GPT-4V&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;🕹 &lt;strong&gt;实时多模态交互。&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;我们尝试结合OmniLMM-12B和GPT-3.5 (纯文本模型) ，实现&lt;strong&gt;实时多模态交互助手&lt;/strong&gt;。该模型接受来自摄像头的视频流，并借助工具处理语音输入输出。虽然还很初步，我们发现该模型无需视频编辑可以&lt;strong&gt;复现Gemini演示视频中的一些有趣例子&lt;/strong&gt;。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;评测结果 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/radar_omnilmm12b.png&#34; width=&#34;66%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; MME, MMBench, MMMU, MMBench, MMHal-Bench, Object HalBench, SeedBench, LLaVA Bench W, MathVista 上的详细评测结果。 &lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;MME&lt;/th&gt; &#xA;    &lt;th nowrap&gt;MMB dev (en)&lt;/th&gt; &#xA;    &lt;th nowrap&gt;MMMU val&lt;/th&gt; &#xA;    &lt;th nowrap&gt;MMHal-Bench&lt;/th&gt; &#xA;    &lt;th nowrap&gt;Object HalBench&lt;/th&gt; &#xA;    &lt;th nowrap&gt;SeedBench-I&lt;/th&gt; &#xA;    &lt;th&gt;MathVista&lt;/th&gt; &#xA;    &lt;th nowrap&gt;LLaVA Bench&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody align=&#34;center&#34;&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;GPT-4V†&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;1771.5&lt;/td&gt; &#xA;    &lt;td&gt;75.1 &lt;/td&gt; &#xA;    &lt;td&gt;56.8&lt;/td&gt; &#xA;    &lt;td&gt;3.53 / 70.8&lt;/td&gt; &#xA;    &lt;td&gt;86.4 / 92.7&lt;/td&gt; &#xA;    &lt;td&gt;71.6 &lt;/td&gt; &#xA;    &lt;td&gt;47.8 &lt;/td&gt; &#xA;    &lt;td&gt;93.1 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Plus†&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;2183.4&lt;/td&gt; &#xA;    &lt;td&gt;66.2 &lt;/td&gt; &#xA;    &lt;td&gt;45.2&lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;65.7 &lt;/td&gt; &#xA;    &lt;td&gt;36.0 &lt;/td&gt; &#xA;    &lt;td&gt;73.7 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;Yi-VL 6B&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;6.7B &lt;/td&gt; &#xA;    &lt;td&gt;1915.1 &lt;/td&gt; &#xA;    &lt;td&gt;68.6 &lt;/td&gt; &#xA;    &lt;td&gt;40.3 &lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;- &lt;/td&gt; &#xA;    &lt;td&gt;67.5 &lt;/td&gt; &#xA;    &lt;td&gt;28.8 &lt;/td&gt; &#xA;    &lt;td&gt;51.9 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;9.6B&lt;/td&gt; &#xA;    &lt;td&gt;1860.0&lt;/td&gt; &#xA;    &lt;td&gt;60.6 &lt;/td&gt; &#xA;    &lt;td&gt;35.9&lt;/td&gt; &#xA;    &lt;td&gt;2.93 / 59.4&lt;/td&gt; &#xA;    &lt;td&gt;56.2 / 80.0&lt;/td&gt; &#xA;    &lt;td&gt;64.8 &lt;/td&gt; &#xA;    &lt;td&gt;33.8 &lt;/td&gt; &#xA;    &lt;td&gt;67.7 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;CogVLM-Chat&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;17.4B&lt;/td&gt; &#xA;    &lt;td&gt;1736.6&lt;/td&gt; &#xA;    &lt;td&gt;63.7 &lt;/td&gt; &#xA;    &lt;td&gt;32.1 &lt;/td&gt; &#xA;    &lt;td&gt;2.68 / 52.1 &lt;/td&gt; &#xA;    &lt;td&gt;73.6 / 87.4 &lt;/td&gt; &#xA;    &lt;td&gt;68.8 &lt;/td&gt; &#xA;    &lt;td&gt;34.7 &lt;/td&gt; &#xA;    &lt;td&gt;73.9 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;left&#34;&gt;LLaVA 1.5&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;13.6B &lt;/td&gt; &#xA;    &lt;td&gt;1808.4 &lt;/td&gt; &#xA;    &lt;td&gt;68.2 &lt;/td&gt; &#xA;    &lt;td&gt;36.4 &lt;/td&gt; &#xA;    &lt;td&gt;2.71 / 51.0 &lt;/td&gt; &#xA;    &lt;td&gt;53.7 / 77.4 &lt;/td&gt; &#xA;    &lt;td&gt;68.1 &lt;/td&gt; &#xA;    &lt;td&gt;26.4 &lt;/td&gt; &#xA;    &lt;td&gt;64.6 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;&lt;b&gt;OmniLMM-12B&lt;/b&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;right&#34;&gt;11.6B &lt;/td&gt; &#xA;    &lt;td&gt;1935.8 &lt;/td&gt; &#xA;    &lt;td&gt;71.6 &lt;/td&gt; &#xA;    &lt;td&gt;40.7 &lt;/td&gt; &#xA;    &lt;td&gt;3.45 / 68.8 &lt;/td&gt; &#xA;    &lt;td&gt;90.3 / 95.5 &lt;/td&gt; &#xA;    &lt;td&gt;71.1 &lt;/td&gt; &#xA;    &lt;td&gt;34.9 &lt;/td&gt; &#xA;    &lt;td&gt;72.0 &lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;small&gt;†: 闭源模型&lt;/small&gt; &#xA; &lt;br&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;典型示例 &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/omnilmm-12b-examples_2.png&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;p&gt;我们结合 OmniLMM-12B 和 ChatGPT-3.5 (纯文本模型) 尝试构建 &lt;strong&gt;实时多模态交互助手&lt;/strong&gt;. OmniLMM-12B 将视频帧转为对应的图像描述并输入给ChatGPT-3.5来生成对用户指令的响应。演示视频未经编辑。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;video controls src=&#34;https://github.com/OpenBMB/OmniLMM/assets/157115220/8fec13bf-bb47-4bf8-8f8c-d0b716a964ec&#34; type=&#34;video/mp4&#34; width=&#34;80%/&#34;&gt; &#xA; &lt;/video&gt;&#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;欢迎通过以下链接使用我们的网页端推理服务： &lt;a href=&#34;http://120.92.209.146:8081&#34;&gt;OmniLMM-12B&lt;/a&gt; ｜ &lt;a href=&#34;http://120.92.209.146:80&#34;&gt;MiniCPM-V 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;安装&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;克隆我们的仓库并跳转到相应目录&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/OpenBMB/MiniCPM-V.git&#xA;cd MiniCPM-V&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;创建 conda 环境&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;conda create -n MiniCPMV python=3.10 -y&#xA;conda activate MiniCPMV&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;安装依赖&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;推理&lt;/h2&gt; &#xA;&lt;h3&gt;模型库&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;模型&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;简介&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;下载链接&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;最新版本，提供高效而领先的端侧双语多模态理解能力。&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V-2/files&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;第一版 MiniCPM-V&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V/files&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OmniLMM-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;性能最强的版本&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/OmniLMM-12B&#34;&gt;🤗&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/OmniLMM-12B/files&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;多轮对话&lt;/h3&gt; &#xA;&lt;p&gt;请参考以下代码使用 &lt;code&gt;MiniCPM-V&lt;/code&gt; 和 &lt;code&gt;OmniLMM&lt;/code&gt; 进行推理。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/hk_OCR.jpg&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from chat import OmniLMMChat, img2base64&#xA;&#xA;chat_model = OmniLMMChat(&#39;openbmb/MiniCPM-V-2&#39;) # or &#39;openbmb/OmniLMM-12B&#39;&#xA;&#xA;im_64 = img2base64(&#39;./assets/hk_OCR.jpg&#39;)&#xA;&#xA;# First round chat &#xA;msgs = [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Where should I go to buy a camera?&#34;}]&#xA;&#xA;inputs = {&#34;image&#34;: im_64, &#34;question&#34;: json.dumps(msgs)}&#xA;answer = chat_model.chat(inputs)&#xA;print(answer)&#xA;&#xA;# Second round chat &#xA;# pass history context of multi-turn conversation&#xA;msgs.append({&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: answer})&#xA;msgs.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Where is this store in the image?&#34;})&#xA;&#xA;inputs = {&#34;image&#34;: im_64, &#34;question&#34;: json.dumps(msgs)}&#xA;answer = chat_model.chat(inputs)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;可以得到以下输出:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;You should go to the Canon store for a camera.&#34;&#xA;&#xA;&#34;The Canon store is located on the right side of the image.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mac 推理&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;点击查看 MiniCPM-V 2.0 基于Mac MPS运行 (Apple silicon or AMD GPUs)的示例。 &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# test.py&#xA;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-V-2&#39;, trust_remote_code=True, torch_dtype=torch.bfloat16)&#xA;model = model.to(device=&#39;mps&#39;, dtype=torch.float16)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-V-2&#39;, trust_remote_code=True)&#xA;model.eval()&#xA;&#xA;image = Image.open(&#39;./assets/hk_OCR.jpg&#39;).convert(&#39;RGB&#39;)&#xA;question = &#39;Where is this photo taken?&#39;&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: question}]&#xA;&#xA;answer, context, _ = model.chat(&#xA;    image=image,&#xA;    msgs=msgs,&#xA;    context=None,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;运行:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;手机端部署&lt;/h3&gt; &#xA;&lt;p&gt;MiniCPM-V 2.0 目前可以部署在Android和Harmony操作系统的手机上。 🚀 点击&lt;a href=&#34;https://github.com/OpenBMB/mlc-MiniCPM&#34;&gt;这里&lt;/a&gt;开始手机端部署。&lt;/p&gt; &#xA;&lt;h2&gt;未来计划&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 支持模型微调&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 本地用户图形界面部署&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 实时多模态交互代码开源&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;模型协议 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;本仓库中代码依照 Apache-2.0 协议开源&lt;/p&gt; &#xA;&lt;p&gt;OmniLMM 模型权重的使用遵循 “&lt;a href=&#34;https://github.com/OpenBMB/General-Model-License/raw/main/%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE-%E6%9D%A5%E6%BA%90%E8%AF%B4%E6%98%8E-%E5%AE%A3%E4%BC%A0%E9%99%90%E5%88%B6-%E5%95%86%E4%B8%9A%E6%8E%88%E6%9D%83.md&#34;&gt;通用模型许可协议-来源说明-宣传限制-商业授权&lt;/a&gt;”。&lt;/p&gt; &#xA;&lt;p&gt;OmniLMM 模型权重对学术研究完全开放。&lt;/p&gt; &#xA;&lt;p&gt;如需将模型用于商业用途，请联系 &lt;a href=&#34;mailto:cpm@modelbest.cn&#34;&gt;cpm@modelbest.cn&lt;/a&gt; 来获取书面授权，登记后可以免费商业使用。&lt;/p&gt; &#xA;&lt;h2&gt;声明 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;作为多模态大模型，MiniCPM-V 和 OmniLMM 通过学习大量的多模态数据来生成内容，但它无法理解、表达个人观点或价值判断，它所输出的任何内容都不代表模型开发者的观点和立场。&lt;/p&gt; &#xA;&lt;p&gt;因此用户在使用 MiniCPM-V 和 OmniLMM 生成的内容时，应自行负责对其进行评估和验证。如果由于使用 OmniLMM 开源模型而导致的任何问题，包括但不限于数据安全问题、公共舆论风险，或模型被误导、滥用、传播或不当利用所带来的任何风险和问题，我们将不承担任何责任。&lt;/p&gt; &#xA;&lt;h2&gt;机构 &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;本项目由以下机构共同开发：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/thunlp.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://nlp.csai.tsinghua.edu.cn/&#34;&gt;清华大学自然语言处理实验室&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/modelbest.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://modelbest.cn/&#34;&gt;面壁智能&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-V/main/assets/zhihu.webp&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://www.zhihu.com/&#34;&gt;知乎&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>stanford-oval/storm</title>
    <updated>2024-04-16T01:23:44Z</updated>
    <id>tag:github.com,2024-04-16:/stanford-oval/storm</id>
    <link href="https://github.com/stanford-oval/storm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An LLM-powered knowledge curation system that researches a topic and generates a full-length report with citations.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the code for our NAACL 2024 paper &lt;a href=&#34;https://arxiv.org/abs/2402.14207&#34;&gt;Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models&lt;/a&gt; by &lt;a href=&#34;https://cs.stanford.edu/~shaoyj&#34;&gt;Yijia Shao&lt;/a&gt;, &lt;a href=&#34;https://yucheng-jiang.github.io/&#34;&gt;Yucheng Jiang&lt;/a&gt;, Theodore A. Kanell, Peter Xu, &lt;a href=&#34;https://omarkhattab.com/&#34;&gt;Omar Khattab&lt;/a&gt;, and &lt;a href=&#34;https://suif.stanford.edu/~lam/&#34;&gt;Monica S. Lam&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Overview &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;(Try STORM now!)&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/overview.png&#34; style=&#34;width: 90%; height: auto;&#34;&gt; &lt;/p&gt; STORM is a LLM system that writes Wikipedia-like articles from scratch based on Internet search. &#xA;&lt;p&gt;While the system cannot produce publication-ready articles that often require a significant number of edits, experienced Wikipedia editors have found it helpful in their pre-writing stage.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Try out our &lt;a href=&#34;https://storm.genie.stanford.edu/&#34;&gt;live demo&lt;/a&gt; to see how STORM can help your knowledge exploration journey and please provide feedback to help us improve the system 🙏!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Research Before Writing&lt;/h2&gt; &#xA;&lt;p&gt;STORM breaks down generating long articles with citations into two steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-writing stage&lt;/strong&gt;: The system conducts Internet-based research to collect references and generates an outline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Writing stage&lt;/strong&gt;: The system uses the outline and references to generate the full-length article with citations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/assets/two_stages.jpg&#34; style=&#34;width: 60%; height: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;STORM identifies the core of automating the research process as automatically coming up with good questions to ask. Directly prompting the language model to ask questions does not work well. To improve the depth and breadth of the questions, STORM adopts two strategies:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Perspective-Guided Question Asking&lt;/strong&gt;: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Simulated Conversation&lt;/strong&gt;: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Based on the separation of the two stages, STORM is implemented in a highly modular way (see &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/engine.py&#34;&gt;engine.py&lt;/a&gt;) using &lt;a href=&#34;https://github.com/stanfordnlp/dspy&#34;&gt;dspy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We view STORM as an example of automated knowledge curation. We are working on enhancing our codebase to increase its extensibility. Stay tuned!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below, we provide a quick start guide to run STORM locally to reproduce our experiments.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install the required packages. &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n storm python=3.11&#xA;conda activate storm&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Set up OpenAI API key and &lt;a href=&#34;https://api.you.com/&#34;&gt;You.com search API&lt;/a&gt; key. Create a file &lt;code&gt;secrets.toml&lt;/code&gt; under the root directory and add the following content: &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Set up OpenAI API key.&#xA;OPENAI_API_KEY=&amp;lt;your_openai_api_key&amp;gt;&#xA;# If you are using the API service provided by OpenAI, include the following line:&#xA;OPENAI_API_TYPE=&#34;openai&#34;&#xA;# If you are using the API service provided by Microsoft Azure, include the following lines:&#xA;OPENAI_API_TYPE=&#34;azure&#34;&#xA;AZURE_API_BASE=&amp;lt;your_azure_api_base_url&amp;gt;&#xA;AZURE_API_VERSION=&amp;lt;your_azure_api_version&amp;gt;&#xA;# Set up You.com search API key.&#xA;YDC_API_KEY=&amp;lt;your_youcom_api_key&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Paper Experiments&lt;/h2&gt; &#xA;&lt;p&gt;The FreshWiki dataset used in our experiments can be found in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/FreshWiki&#34;&gt;./FreshWiki&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Run the following commands under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src&#34;&gt;./src&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-writing Stage&lt;/h3&gt; &#xA;&lt;p&gt;For batch experiment on FreshWiki dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_prewriting --input-source file --input-path ../FreshWiki/topic_list.csv  --engine gpt-4 --do-research --max-conv-turn 5 --max-perspective 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--engine&lt;/code&gt; (choices=[&lt;code&gt;gpt-4&lt;/code&gt;, &lt;code&gt;gpt-35-turbo&lt;/code&gt;]): the LLM engine used for generating the outline&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-research&lt;/code&gt;: if True, simulate conversation to research the topic; otherwise, load the results.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max-conv-turn&lt;/code&gt;: the maximum number of questions for each information-seeking conversation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max-perspective&lt;/code&gt;: the maximum number of perspectives to be considered, each perspective corresponds to an information-seeking conversation. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;STORM also uses a general conversation to collect basic information about the topic. So, the maximum number of QA pairs is &lt;code&gt;max_turn * (max_perspective + 1)&lt;/code&gt;. &lt;span&gt;💡&lt;/span&gt; Reducing &lt;code&gt;max_turn&lt;/code&gt; or &lt;code&gt;max_perspective&lt;/code&gt; can speed up the process and reduce the cost but may result in less comprehensive outline.&lt;/li&gt; &#xA;   &lt;li&gt;The parameter will not have any effect if &lt;code&gt;--disable-perspective&lt;/code&gt; is set (the perspective-driven question asking is disabled).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To run the experiment on a single topic:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_prewriting --input-source console --engine gpt-4 --max-conv-turn 5 --max-perspective 5 --do-research&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The script will ask you to enter the &lt;code&gt;Topic&lt;/code&gt; and the &lt;code&gt;Ground truth url&lt;/code&gt; that will be excluded. If you do not have any url to exclude, leave that field empty.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The generated outline will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_outline.txt&lt;/code&gt; and the collected references will be saved in &lt;code&gt;{output_dir}/{topic}/raw_search_results.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Writing Stage&lt;/h3&gt; &#xA;&lt;p&gt;For batch experiment on FreshWiki dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_writing --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-polish-article --remove-duplicate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--do-polish-article&lt;/code&gt;: if True, polish the article by adding a summarization section and removing duplicate content if &lt;code&gt;--remove-duplicate&lt;/code&gt; is set True.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To run the experiment on a single topic:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.run_writing --input-source console --engine gpt-4 --do-polish-article --remove-duplicate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The script will ask you to enter the &lt;code&gt;Topic&lt;/code&gt;. Please enter the same topic as the one used in the pre-writing stage.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The generated article will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_article.txt&lt;/code&gt; and the references corresponding to citation index will be saved in &lt;code&gt;{output_dir}/{topic}/url_to_info.json&lt;/code&gt;. If &lt;code&gt;--do-polish-article&lt;/code&gt; is set, the polished article will be saved in &lt;code&gt;{output_dir}/{topic}/storm_gen_article_polished.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Customize the STORM Configurations&lt;/h2&gt; &#xA;&lt;p&gt;We set up the default LLM configuration in &lt;code&gt;LLMConfigs&lt;/code&gt; in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/src/modules/utils.py&#34;&gt;src/modules/utils.py&lt;/a&gt;. You can use &lt;code&gt;set_conv_simulator_lm()&lt;/code&gt;,&lt;code&gt;set_question_asker_lm()&lt;/code&gt;, &lt;code&gt;set_outline_gen_lm()&lt;/code&gt;, &lt;code&gt;set_article_gen_lm()&lt;/code&gt;, &lt;code&gt;set_article_polish_lm()&lt;/code&gt; to override the default configuration. These functions take in an instance from &lt;code&gt;dspy.dsp.LM&lt;/code&gt; or &lt;code&gt;dspy.dsp.HFModel&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;💡&lt;/span&gt; &lt;strong&gt;For a good practice,&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;choose a cheaper/faster model for &lt;code&gt;conv_simulator_lm&lt;/code&gt; which is used to split queries, synthesize answers in the conversation.&lt;/li&gt; &#xA; &lt;li&gt;if you need to conduct the actual writing step, choose a more powerful model for &lt;code&gt;article_gen_lm&lt;/code&gt;. Based on our experiments, weak models are bad at generating text with citations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Automatic Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;In our paper, we break down the evaluation into two parts: outline quality and full-length article quality.&lt;/p&gt; &#xA;&lt;h3&gt;Outline Quality&lt;/h3&gt; &#xA;&lt;p&gt;We introduce &lt;em&gt;heading soft recall&lt;/em&gt; and &lt;em&gt;heading entity recall&lt;/em&gt; to evaluate the outline quality. This makes it easier to prototype methods for pre-writing.&lt;/p&gt; &#xA;&lt;p&gt;Run the following command under &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval&#34;&gt;./eval&lt;/a&gt; to compute the metrics on FreshWiki dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_outline_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --pred-file-name storm_gen_outline.txt --result-output-path ../results/storm_outline_quality.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Full-length Article Quality&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/eval_article_quality.py&#34;&gt;eval/eval_article_quality.py&lt;/a&gt; provides the entry point of evaluating full-length article quality using ROUGE, entity recall, and rubric grading. Run the following command under &lt;code&gt;eval&lt;/code&gt; to compute the metrics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python eval_article_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --gt-dir ../FreshWiki --output-dir ../results/storm_article_eval_results --pred-file-name storm_gen_article_polished.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Use the Metric Yourself&lt;/h3&gt; &#xA;&lt;p&gt;The similarity-based metrics (i.e., ROUGE, entity recall, and heading entity recall) are implemented in &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/metrics.py&#34;&gt;eval/metrics.py&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For rubric grading, we use the &lt;a href=&#34;https://huggingface.co/kaist-ai/prometheus-13b-v1.0&#34;&gt;prometheus-13b-v1.0&lt;/a&gt; introduced in &lt;a href=&#34;https://arxiv.org/abs/2310.08491&#34;&gt;this paper&lt;/a&gt;. &lt;a href=&#34;https://raw.githubusercontent.com/stanford-oval/storm/main/eval/evaluation_prometheus.py&#34;&gt;eval/evaluation_prometheus.py&lt;/a&gt; provides the entry point of using the metric.&lt;/p&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or suggestions, please feel free to open an issue or pull request. We welcome contributions to improve the system and the codebase!&lt;/p&gt; &#xA;&lt;p&gt;Contact person: &lt;a href=&#34;mailto:shaoyj@stanford.edu&#34;&gt;Yijia Shao&lt;/a&gt; and &lt;a href=&#34;mailto:yuchengj@stanford.edu&#34;&gt;Yucheng Jiang&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite our paper if you use this code or part of it in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{shao2024assisting,&#xA;      title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, &#xA;      author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam},&#xA;      year={2024},&#xA;      booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>