<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-06-13T01:39:05Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>atom/atom</title>
    <updated>2022-06-13T01:39:05Z</updated>
    <id>tag:github.com,2022-06-13:/atom/atom</id>
    <link href="https://github.com/atom/atom" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The hackable text editor&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Atom&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://dev.azure.com/github/Atom/_build/latest?definitionId=32&amp;amp;branchName=master&#34;&gt;&lt;img src=&#34;https://dev.azure.com/github/Atom/_apis/build/status/Atom%20Production%20Branches?branchName=master&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Atom and all repositories under Atom will be archived on December 15, 2022. Learn more in our &lt;a href=&#34;https://github.blog/2022-06-08-sunsetting-atom/&#34;&gt;official announcement&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Atom is a hackable text editor for the 21st century, built on &lt;a href=&#34;https://github.com/electron/electron&#34;&gt;Electron&lt;/a&gt;, and based on everything we love about our favorite editors. We designed it to be deeply customizable, but still approachable using the default configuration.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/378023/49132477-f4b77680-f31f-11e8-8357-ac6491761c6c.png&#34; alt=&#34;Atom&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/378023/49132478-f4b77680-f31f-11e8-9e10-e8454d8d9b7e.png&#34; alt=&#34;Atom Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Visit &lt;a href=&#34;https://atom.io&#34;&gt;atom.io&lt;/a&gt; to learn more or visit the &lt;a href=&#34;https://github.com/atom/atom/discussions&#34;&gt;Atom forum&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://twitter.com/atomeditor&#34;&gt;@AtomEditor&lt;/a&gt; on Twitter for important announcements.&lt;/p&gt; &#xA;&lt;p&gt;This project adheres to the Contributor Covenant &lt;a href=&#34;https://raw.githubusercontent.com/atom/atom/master/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;. By participating, you are expected to uphold this code. Please report unacceptable behavior to &lt;a href=&#34;mailto:atom@github.com&#34;&gt;atom@github.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;If you want to read about using Atom or developing packages in Atom, the &lt;a href=&#34;https://flight-manual.atom.io&#34;&gt;Atom Flight Manual&lt;/a&gt; is free and available online. You can find the source to the manual in &lt;a href=&#34;https://github.com/atom/flight-manual.atom.io&#34;&gt;atom/flight-manual.atom.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://atom.io/docs/api&#34;&gt;API reference&lt;/a&gt; for developing packages is also documented on Atom.io.&lt;/p&gt; &#xA;&lt;h2&gt;Installing&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://git-scm.com&#34;&gt;Git&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;macOS&lt;/h3&gt; &#xA;&lt;p&gt;Download the latest &lt;a href=&#34;https://github.com/atom/atom/releases/latest&#34;&gt;Atom release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Atom will automatically update when a new release is available.&lt;/p&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;Download the latest &lt;a href=&#34;https://github.com/atom/atom/releases/latest&#34;&gt;Atom installer&lt;/a&gt;. &lt;code&gt;AtomSetup.exe&lt;/code&gt; is 32-bit. For 64-bit systems, download &lt;code&gt;AtomSetup-x64.exe&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Atom will automatically update when a new release is available.&lt;/p&gt; &#xA;&lt;p&gt;You can also download &lt;code&gt;atom-windows.zip&lt;/code&gt; (32-bit) or &lt;code&gt;atom-x64-windows.zip&lt;/code&gt; (64-bit) from the &lt;a href=&#34;https://github.com/atom/atom/releases/latest&#34;&gt;releases page&lt;/a&gt;. The &lt;code&gt;.zip&lt;/code&gt; version will not automatically update.&lt;/p&gt; &#xA;&lt;p&gt;Using &lt;a href=&#34;https://chocolatey.org&#34;&gt;Chocolatey&lt;/a&gt;? Run &lt;code&gt;cinst Atom&lt;/code&gt; to install the latest version of Atom.&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;p&gt;Atom is only available for 64-bit Linux systems.&lt;/p&gt; &#xA;&lt;p&gt;Configure your distribution&#39;s package manager to install and update Atom by following the &lt;a href=&#34;https://flight-manual.atom.io/getting-started/sections/installing-atom/#platform-linux&#34;&gt;Linux installation instructions&lt;/a&gt; in the Flight Manual. You will also find instructions on how to install Atom&#39;s official Linux packages without using a package repository, though you will not get automatic updates after installing Atom this way.&lt;/p&gt; &#xA;&lt;h4&gt;Archive extraction&lt;/h4&gt; &#xA;&lt;p&gt;An archive is available for people who don&#39;t want to install &lt;code&gt;atom&lt;/code&gt; as root.&lt;/p&gt; &#xA;&lt;p&gt;This version enables you to install multiple Atom versions in parallel. It has been built on Ubuntu 64-bit, but should be compatible with other Linux distributions.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies (on Ubuntu):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo apt install git libasound2 libcurl4 libgbm1 libgcrypt20 libgtk-3-0 libnotify4 libnss3 libglib2.0-bin xdg-utils libx11-xcb1 libxcb-dri3-0 libxss1 libxtst6 libxkbfile1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Download &lt;code&gt;atom-amd64.tar.gz&lt;/code&gt; from the &lt;a href=&#34;https://github.com/atom/atom/releases/latest&#34;&gt;Atom releases page&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;tar xf atom-amd64.tar.gz&lt;/code&gt; in the directory where you want to extract the Atom folder.&lt;/li&gt; &#xA; &lt;li&gt;Launch Atom using the installed &lt;code&gt;atom&lt;/code&gt; command from the newly extracted directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The Linux version does not currently automatically update so you will need to repeat these steps to upgrade to future releases.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flight-manual.atom.io/hacking-atom/sections/hacking-on-atom-core/#platform-linux&#34;&gt;Linux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flight-manual.atom.io/hacking-atom/sections/hacking-on-atom-core/#platform-mac&#34;&gt;macOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://flight-manual.atom.io/hacking-atom/sections/hacking-on-atom-core/#platform-windows&#34;&gt;Windows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Discussion&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Discuss Atom on &lt;a href=&#34;https://github.com/atom/atom/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/atom/atom/raw/master/LICENSE.md&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;When using the Atom or other GitHub logos, be sure to follow the &lt;a href=&#34;https://github.com/logos&#34;&gt;GitHub logo guidelines&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>digitalocean/nginxconfig.io</title>
    <updated>2022-06-13T01:39:05Z</updated>
    <id>tag:github.com,2022-06-13:/digitalocean/nginxconfig.io</id>
    <link href="https://github.com/digitalocean/nginxconfig.io" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚öôÔ∏è NGINX config generator on steroids üíâ&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/digitalocean/nginxconfig.io.svg?sanitize=true&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/digitalocean/nginxconfig.io.svg?color=blue&#34; alt=&#34;GitHub contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/digitalocean/nginxconfig.io.svg?color=blue&#34; alt=&#34;MIT License&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/issues?q=is%3Aissue+is%3Aclosed&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-closed-raw/digitalocean/nginxconfig.io.svg?color=brightgreen&#34; alt=&#34;Closed issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/pulls?q=is%3Apr+is%3Aclosed&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr-closed-raw/digitalocean/nginxconfig.io.svg?color=brightgreen&#34; alt=&#34;Closed PR&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/digitalocean/nginxconfig.io.svg?sanitize=true&#34; alt=&#34;Open issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr-raw/digitalocean/nginxconfig.io.svg?sanitize=true&#34; alt=&#34;Open PR&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://do.co/nginxconfig&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/digitalocean/nginxconfig.io/master/src/static/banner.png&#34; alt=&#34;nginxconfig&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt;‚öôÔ∏è NGINX configuration generator on steroids üíâ&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; The only tool you&#39;ll ever need to configure your NGINX server. &lt;br&gt; &lt;a href=&#34;https://do.co/nginxconfig&#34;&gt;&lt;strong&gt;do.co/nginxconfig ¬ª&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/issues/new?template=report-a-bug.md&#34;&gt;Report a bug&lt;/a&gt; ¬∑ &lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/issues/new?template=request-a-feature.md&#34;&gt;Request a feature&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;‚ú® &lt;a href=&#34;https://do.co/nginxconfig&#34;&gt;NGINX Config&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;NGINX is so much more than just a webserver. You already knew that, probably.&lt;/p&gt; &#xA;&lt;p&gt;We love NGINX, because:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Low memory usage&lt;/li&gt; &#xA; &lt;li&gt;High concurrency&lt;/li&gt; &#xA; &lt;li&gt;Asynchronous event-driven architecture&lt;/li&gt; &#xA; &lt;li&gt;Load balancing&lt;/li&gt; &#xA; &lt;li&gt;Reverse proxying&lt;/li&gt; &#xA; &lt;li&gt;FastCGI support with caching (PHP)&lt;/li&gt; &#xA; &lt;li&gt;Amazing fast handling of static files&lt;/li&gt; &#xA; &lt;li&gt;TLS/SSL with SNI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A lot of features with corresponding configuration directives. You can deep dive into the &lt;a href=&#34;http://nginx.org/en/docs/&#34;&gt;NGINX documentation&lt;/a&gt; right now OR you can &lt;a href=&#34;https://do.co/nginxconfig&#34;&gt;use this tool&lt;/a&gt; to check how NGINX works, observe how your inputs are affecting the output, and &lt;strong&gt;generate the best config for your specific use-case&lt;/strong&gt; (in parallel you can also still use the docs).&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;GOTO&lt;/code&gt; &lt;strong&gt;&lt;a href=&#34;https://do.co/nginxconfig&#34;&gt;&lt;code&gt;do.co/nginxconfig&lt;/code&gt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt; HTTPS, HTTP/2, IPv6, certbot, HSTS, security headers, SSL profiles, OCSP resolvers, caching, gzip, brotli, fallback routing, reverse proxy, www/non-www redirect, CDN, PHP (TCP/socket, WordPress, Drupal, Magento, Joomla), Node.js support, Python (Django) server, etc.&lt;/p&gt; &#xA;&lt;h2&gt;üë®‚Äçüíª Author&lt;/h2&gt; &#xA;&lt;h3&gt;Rewrite &amp;amp; Maintenance&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Matt (IPv4) Cowley &amp;lt;&lt;a href=&#34;mailto:me@mattcowley.co.uk&#34;&gt;me@mattcowley.co.uk&lt;/a&gt;&amp;gt; (&lt;a href=&#34;https://mattcowley.co.uk&#34;&gt;https://mattcowley.co.uk&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/MattIPv4&#34;&gt;@MattIPv4&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Original version&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;B√°lint Szekeres &amp;lt;&lt;a href=&#34;mailto:balint@szekeres.me&#34;&gt;balint@szekeres.me&lt;/a&gt;&amp;gt; (&lt;a href=&#34;https://balint.szekeres.me&#34;&gt;https://balint.szekeres.me&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/0xB4LINT&#34;&gt;@0xB4LINT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LinkedIn: &lt;a href=&#34;https://www.linkedin.com/in/0xB4LINT/&#34;&gt;@0xB4LINT&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚ñ∂Ô∏è Development&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/digitalocean/nginxconfig.io.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install NPM packages&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm ci&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the development server &lt;em&gt;(with file watchers)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the development site &lt;strong&gt;&lt;a href=&#34;http://localhost:8080&#34;&gt;localhost:8080&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Lint your code &lt;em&gt;(eslint &amp;amp; stylelint)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm test&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build for production &lt;em&gt;(to the &lt;code&gt;dist&lt;/code&gt; directory)&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are &lt;strong&gt;greatly appreciated&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the Project&lt;/li&gt; &#xA; &lt;li&gt;Create your Feature Branch (&lt;code&gt;git checkout -b feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Commit your Changes (&lt;code&gt;git commit -m &#39;Add some AmazingFeature&#39;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Push to the Branch (&lt;code&gt;git push origin feature/AmazingFeature&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Open a Pull Request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;‚öíÔ∏è Built With&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vuejs.org/&#34;&gt;Vue.js&lt;/a&gt; - Template handling &amp;amp; app generation&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bulma.io/&#34;&gt;Bulma&lt;/a&gt; - Base styling, customised by &lt;a href=&#34;https://github.com/do-community/do-bulma&#34;&gt;do-bulma&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prismjs.com/&#34;&gt;Prism&lt;/a&gt; - Bash &amp;amp; NGINX syntax highlighting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ssl-config.mozilla.org&#34;&gt;Mozilla SSL Configuration Generator v5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mozilla.github.io/server-side-tls/ssl-config-generator/&#34;&gt;Mozilla SSL Configuration Generator&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OWASP/CheatSheetSeries/raw/master/cheatsheets/TLS_Cipher_String_Cheat_Sheet.md&#34;&gt;OWASP TLS Cipher String Cheat Sheet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://thoughts.t37.net/nginx-optimization-understanding-sendfile-tcp-nodelay-and-tcp-nopush-c55cdd276765&#34;&gt;Nginx Optimization: understanding sendfile, tcp_nodelay and tcp_nopush&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/denji/8359866&#34;&gt;NGINX Tuning For Best Performance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.keycdn.com/blog/http-security-headers/&#34;&gt;Hardening Your HTTP Security Headers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/h5bp/server-configs-nginx&#34;&gt;h5bp/server-configs-nginx&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://security.stackexchange.com/questions/95178/diffie-hellman-parameters-still-calculating-after-24-hours/95184#95184&#34;&gt;Diffie-Hellman DSA-like parameters&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://hstspreload.org&#34;&gt;hstspreload.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://serverfault.com/questions/787919/optimal-value-for-nginx-worker-connections&#34;&gt;Optimal value for nginx worker_connections&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚≠êÔ∏è Show your support&lt;/h2&gt; &#xA;&lt;p&gt;Give a ‚≠êÔ∏è if this project helped you!&lt;/p&gt; &#xA;&lt;h2&gt;üìù License&lt;/h2&gt; &#xA;&lt;p&gt;Copyright ¬© 2020 &lt;a href=&#34;https://www.digitalocean.com&#34;&gt;DigitalOcean, Inc &amp;lt;contact@digitalocean.com&amp;gt; (https://www.digitalocean.com)&lt;/a&gt;. &lt;br&gt; This project is licensed under the &lt;a href=&#34;https://github.com/digitalocean/nginxconfig.io/raw/master/LICENSE&#34;&gt;MIT&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/eg3d</title>
    <updated>2022-06-13T01:39:05Z</updated>
    <id>tag:github.com,2022-06-13:/NVlabs/eg3d</id>
    <link href="https://github.com/NVlabs/eg3d" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Efficient Geometry-aware 3D Generative Adversarial Networks (EG3D)&lt;br&gt;&lt;sub&gt;Official PyTorch implementation of the CVPR 2022 paper&lt;/sub&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/teaser.jpeg&#34; alt=&#34;Teaser image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Efficient Geometry-aware 3D Generative Adversarial Networks&lt;/strong&gt;&lt;br&gt; Eric R. Chan*, Connor Z. Lin*, Matthew A. Chan*, Koki Nagano*, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein&lt;br&gt;&lt;em&gt;* equal contribution&lt;/em&gt;&lt;br&gt; &lt;br&gt;&lt;a href=&#34;https://nvlabs.github.io/eg3d/&#34;&gt;https://nvlabs.github.io/eg3d/&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Abstract: &lt;em&gt;Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute-intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit-implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;For business inquiries, please visit our website and submit the form: &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA Research Licensing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We recommend Linux for performance and compatibility reasons.&lt;/li&gt; &#xA; &lt;li&gt;1‚Äì8 high-end NVIDIA GPUs. We have done all testing and development using V100, RTX3090, and A100 GPUs.&lt;/li&gt; &#xA; &lt;li&gt;64-bit Python 3.8 and PyTorch 1.11.0 (or later). See &lt;a href=&#34;https://pytorch.org&#34;&gt;https://pytorch.org&lt;/a&gt; for PyTorch install instructions.&lt;/li&gt; &#xA; &lt;li&gt;CUDA toolkit 11.3 or later. (Why is a separate CUDA toolkit installation required? See &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/troubleshooting.md#why-is-cuda-toolkit-installation-necessary&#34;&gt;Troubleshooting&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Python libraries: see &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/environment.yml&#34;&gt;environment.yml&lt;/a&gt; for exact library dependencies. You can use the following commands with Miniconda3 to create and activate your Python environment: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;cd eg3d&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;conda env create -f environment.yml&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;conda activate eg3d&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Pre-trained networks are stored as &lt;code&gt;*.pkl&lt;/code&gt; files that can be referenced using local filenames. See &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/models.md&#34;&gt;Models&lt;/a&gt; for download links to pre-trained checkpoints.&lt;/p&gt; &#xA;&lt;h2&gt;Generating media&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate videos using pre-trained model&#xA;&#xA;python gen_videos.py --outdir=out --trunc=0.7 --seeds=0-3 --grid=2x2 \&#xA;    --network=networks/network_snapshot.pkl&#xA;&#xA;# Generate the same 4 seeds in an interpolation sequence&#xA;&#xA;python gen_videos.py --outdir=out --trunc=0.7 --seeds=0-3 --grid=1x1 \&#xA;    --network=networks/network_snapshot.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Generate images and shapes (as .mrc files) using pre-trained model&#xA;&#xA;python gen_samples.py --outdir=out --trunc=0.7 --shapes=true --seeds=0-3 \&#xA;    --network=networks/network_snapshot.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We visualize our .mrc shape files with &lt;a href=&#34;https://www.cgl.ucsf.edu/chimerax/&#34;&gt;UCSF Chimerax&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To visualize a shape in ChimeraX do the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Import the &lt;code&gt;.mrc&lt;/code&gt; file with &lt;code&gt;File &amp;gt; Open&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Find the selected shape in the Volume Viewer tool &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;The Volume Viewer tool is located under &lt;code&gt;Tools &amp;gt; Volume Data &amp;gt; Volume Viewer&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;Change volume type to &#34;Surface&#34;&lt;/li&gt; &#xA; &lt;li&gt;Change step size to 1&lt;/li&gt; &#xA; &lt;li&gt;Change level set to 10 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;Note that the optimal level can vary by each object, but is usually between 2 and 20. Individual adjustment may make certain shapes slightly sharper&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;In the &lt;code&gt;Lighting&lt;/code&gt; menu in the top bar, change lighting to &#34;Full&#34;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Interactive visualization&lt;/h2&gt; &#xA;&lt;p&gt;This release contains an interactive model visualization tool that can be used to explore various characteristics of a trained model. To start it, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;python visualizer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/visualizer_guide.md&#34;&gt;&lt;code&gt;Visualizer Guide&lt;/code&gt;&lt;/a&gt; for a description of important options.&lt;/p&gt; &#xA;&lt;h2&gt;Using networks from Python&lt;/h2&gt; &#xA;&lt;p&gt;You can use pre-trained networks in your own Python code as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.python&#34;&gt;with open(&#39;ffhq.pkl&#39;, &#39;rb&#39;) as f:&#xA;    G = pickle.load(f)[&#39;G_ema&#39;].cuda()  # torch.nn.Module&#xA;z = torch.randn([1, G.z_dim]).cuda()    # latent codes&#xA;c = torch.cat([cam2world_pose.reshape(-1, 16), intrinsics.reshape(-1, 9)], 1) # camera parameters&#xA;img = G(z, c)[&#39;image&#39;]                           # NCHW, float32, dynamic range [-1, +1], no truncation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above code requires &lt;code&gt;torch_utils&lt;/code&gt; and &lt;code&gt;dnnlib&lt;/code&gt; to be accessible via &lt;code&gt;PYTHONPATH&lt;/code&gt;. It does not need source code for the networks themselves ‚Äî their class definitions are loaded from the pickle via &lt;code&gt;torch_utils.persistence&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The pickle contains three networks. &lt;code&gt;&#39;G&#39;&lt;/code&gt; and &lt;code&gt;&#39;D&#39;&lt;/code&gt; are instantaneous snapshots taken during training, and &lt;code&gt;&#39;G_ema&#39;&lt;/code&gt; represents a moving average of the generator weights over several training steps. The networks are regular instances of &lt;code&gt;torch.nn.Module&lt;/code&gt;, with all of their parameters and buffers placed on the CPU at import and gradient computation disabled by default.&lt;/p&gt; &#xA;&lt;p&gt;The generator consists of two submodules, &lt;code&gt;G.mapping&lt;/code&gt; and &lt;code&gt;G.synthesis&lt;/code&gt;, that can be executed separately. They also support various additional options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.python&#34;&gt;w = G.mapping(z, conditioning_params, truncation_psi=0.5, truncation_cutoff=8)&#xA;img = G.synthesis(w, camera_params)[&#39;image]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/eg3d/gen_samples.py&#34;&gt;&lt;code&gt;gen_samples.py&lt;/code&gt;&lt;/a&gt; for complete code example.&lt;/p&gt; &#xA;&lt;h2&gt;Preparing datasets&lt;/h2&gt; &#xA;&lt;p&gt;Datasets are stored as uncompressed ZIP archives containing uncompressed PNG files and a metadata file &lt;code&gt;dataset.json&lt;/code&gt; for labels. Each label is a 25-length list of floating point numbers, which is the concatenation of the flattened 4x4 camera extrinsic matrix and flattened 3x3 camera intrinsic matrix. Custom datasets can be created from a folder containing images; see &lt;code&gt;python dataset_tool.py --help&lt;/code&gt; for more information. Alternatively, the folder can also be used directly as a dataset, without running it through &lt;code&gt;dataset_tool.py&lt;/code&gt; first, but doing so may lead to suboptimal performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;FFHQ&lt;/strong&gt;: Download and process the &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;Flickr-Faces-HQ dataset&lt;/a&gt; using the following commands.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Ensure the &lt;a href=&#34;https://github.com/sicxu/Deep3DFaceRecon_pytorch/tree/6ba3d22f84bf508f0dde002da8fff277196fef21&#34;&gt;Deep3DFaceRecon_pytorch&lt;/a&gt; submodule is properly initialized&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the following commands&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;cd dataset_preprocessing/ffhq&#xA;python runme.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;AFHQv2&lt;/strong&gt;: Download and process the &lt;a href=&#34;https://github.com/clovaai/stargan-v2/raw/master/README.md#animal-faces-hq-dataset-afhq&#34;&gt;AFHQv2 dataset&lt;/a&gt; with the following.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the AFHQv2 images zipfile from the &lt;a href=&#34;https://github.com/clovaai/stargan-v2/&#34;&gt;StarGAN V2 repository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run the following commands:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;cd dataset_preprocessing/afhq&#xA;python runme.py &#34;path/to/downloaded/afhq.zip&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;ShapeNet Cars&lt;/strong&gt;: Download and process renderings of the cars category of &lt;a href=&#34;https://shapenet.org/&#34;&gt;ShapeNet&lt;/a&gt; using the following commands. NOTE: the following commands download renderings of the ShapeNet cars from the &lt;a href=&#34;https://www.vincentsitzmann.com/srns/&#34;&gt;Scene Representation Networks repository&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;cd dataset_preprocessing/shapenet&#xA;python runme.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;You can train new networks using &lt;code&gt;train.py&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Train with FFHQ from scratch with raw neural rendering resolution=64, using 8 GPUs.&#xA;python train.py --outdir=~/training-runs --cfg=ffhq --data=~/datasets/FFHQ_512.zip \&#xA;  --gpus=8 --batch=32 --gamma=1 --gen_pose_cond=True&#xA;&#xA;# Second stage finetuning of FFHQ to 128 neural rendering resolution (optional).&#xA;python train.py --outdir=~/training-runs --cfg=ffhq --data=~/datasets/FFHQ_512.zip \&#xA;  --resume=~/training-runs/ffhq_experiment_dir/network-snapshot-025000.pkl \&#xA;  --gpus=8 --batch=32 --gamma=1 --gen_pose_cond=True --neural_rendering_resolution_final=128&#xA;&#xA;# Train with Shapenet from scratch, using 8 GPUs.&#xA;python train.py --outdir=~/training-runs --cfg=shapenet --data=~/datasets/cars_train.zip \&#xA;  --gpus=8 --batch=32 --gamma=0.3&#xA;&#xA;# Train with AFHQ, finetuning from FFHQ with ADA, using 8 GPUs.&#xA;python train.py --outdir=~/training-runs --cfg=afhq --data=~/datasets/afhq.zip \&#xA;  --gpus=8 --batch=32 --gamma=5 --aug=ada --neural_rendering_resolution_final=128 --gen_pose_cond=True --gpc_reg_prob=0.8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/training_guide.md&#34;&gt;Training Guide&lt;/a&gt; for a guide to setting up a training run on your own data.&lt;/p&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/eg3d/main/docs/models.md&#34;&gt;Models&lt;/a&gt; for recommended training configurations and download links for pre-trained checkpoints.&lt;/p&gt; &#xA;&lt;p&gt;The results of each training run are saved to a newly created directory, for example &lt;code&gt;~/training-runs/00000-ffhq-ffhq512-gpus8-batch32-gamma1&lt;/code&gt;. The training loop exports network pickles (&lt;code&gt;network-snapshot-&amp;lt;KIMG&amp;gt;.pkl&lt;/code&gt;) and random image grids (&lt;code&gt;fakes&amp;lt;KIMG&amp;gt;.png&lt;/code&gt;) at regular intervals (controlled by &lt;code&gt;--snap&lt;/code&gt;). For each exported pickle, it evaluates FID (controlled by &lt;code&gt;--metrics&lt;/code&gt;) and logs the result in &lt;code&gt;metric-fid50k_full.jsonl&lt;/code&gt;. It also records various statistics in &lt;code&gt;training_stats.jsonl&lt;/code&gt;, as well as &lt;code&gt;*.tfevents&lt;/code&gt; if TensorBoard is installed.&lt;/p&gt; &#xA;&lt;h2&gt;Quality metrics&lt;/h2&gt; &#xA;&lt;p&gt;By default, &lt;code&gt;train.py&lt;/code&gt; automatically computes FID for each network pickle exported during training. We recommend inspecting &lt;code&gt;metric-fid50k_full.jsonl&lt;/code&gt; (or TensorBoard) at regular intervals to monitor the training progress. When desired, the automatic computation can be disabled with &lt;code&gt;--metrics=none&lt;/code&gt; to speed up the training slightly.&lt;/p&gt; &#xA;&lt;p&gt;Additional quality metrics can also be computed after the training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-.bash&#34;&gt;# Previous training run: look up options automatically, save result to JSONL file.&#xA;python calc_metrics.py --metrics=fid50k_full \&#xA;    --network=~/training-runs/network-snapshot-000000.pkl&#xA;&#xA;# Pre-trained network pickle: specify dataset explicitly, print result to stdout.&#xA;python calc_metrics.py --metrics=fid50k_full --data=~/datasets/ffhq_512.zip \&#xA;    --network=ffhq-128.pkl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the metrics can be quite expensive to compute (up to 1h), and many of them have an additional one-off cost for each new dataset (up to 30min). Also note that the evaluation is done using a different random seed each time, so the results will vary if the same metric is computed multiple times.&lt;/p&gt; &#xA;&lt;p&gt;References:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.08500&#34;&gt;GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium&lt;/a&gt;, Heusel et al. 2017&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1801.01401&#34;&gt;Demystifying MMD GANs&lt;/a&gt;, Bi≈Ñkowski et al. 2018&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;!-- ## License&#xA;&#xA;Copyright &amp;copy; 2021, NVIDIA Corporation &amp; affiliates. All rights reserved.&#xA;&#xA;This work is made available under the [Nvidia Source Code License](https://github.com/NVlabs/stylegan3/blob/main/LICENSE.txt). --&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{Chan2022,&#xA;  author = {Eric R. Chan and Connor Z. Lin and Matthew A. Chan and Koki Nagano and Boxiao Pan and Shalini De Mello and Orazio Gallo and Leonidas Guibas and Jonathan Tremblay and Sameh Khamis and Tero Karras and Gordon Wetzstein},&#xA;  title = {Efficient Geometry-aware {3D} Generative Adversarial Networks},&#xA;  booktitle = {CVPR},&#xA;  year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;This is a research reference implementation and is treated as a one-time code drop. As such, we do not accept outside code contributions in the form of pull requests.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We thank David Luebke, Jan Kautz, Jaewoo Seo, Jonathan Granskog, Simon Yuen, Alex Evans, Stan Birchfield, Alexander Bergman, and Joy Hsu for feedback on drafts, Alex Chan, Giap Nguyen, and Trevor Chan for help with diagrams, and Colette Kress and Bryan Catanzaro for allowing use of their photographs. This project was in part supported by Stanford HAI and a Samsung GRO. Koki Nagano and Eric Chan were partially supported by DARPA‚Äôs Semantic Forensics (SemaFor) contract (HR0011-20-3-0005). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government. Distribution Statement &#34;A&#34; (Approved for Public Release, Distribution Unlimited).&lt;/p&gt;</summary>
  </entry>
</feed>