<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-06T01:28:56Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pydantic/pydantic-ai</title>
    <updated>2024-12-06T01:28:56Z</updated>
    <id>tag:github.com,2024-12-06:/pydantic/pydantic-ai</id>
    <link href="https://github.com/pydantic/pydantic-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agent Framework / shim to use Pydantic with LLMs&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://ai.pydantic.dev/&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://ai.pydantic.dev/img/pydantic-ai-dark.svg&#34;&gt; &#xA;   &lt;img src=&#34;https://ai.pydantic.dev/img/pydantic-ai-light.svg?sanitize=true&#34; alt=&#34;PydanticAI&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;em&gt;Agent Framework / shim to use Pydantic with LLMs&lt;/em&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai&#34;&gt;&lt;img src=&#34;https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://pypi.python.org/pypi/pydantic-ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pydantic-ai.svg?sanitize=true&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/pydantic/pydantic-ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pydantic-ai.svg?sanitize=true&#34; alt=&#34;versions&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/pydantic/pydantic-ai/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/pydantic/pydantic-ai.svg?v&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href=&#34;https://ai.pydantic.dev/&#34;&gt;ai.pydantic.dev&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;When I first found FastAPI, I got it immediately. I was excited to find something so innovative and ergonomic built on Pydantic.&lt;/p&gt; &#xA;&lt;p&gt;Virtually every Agent Framework and LLM library in Python uses Pydantic, but when we began to use LLMs in &lt;a href=&#34;https://pydantic.dev/logfire&#34;&gt;Pydantic Logfire&lt;/a&gt;, I couldn&#39;t find anything that gave me the same feeling.&lt;/p&gt; &#xA;&lt;p&gt;PydanticAI is a Python Agent Framework designed to make it less painful to build production grade applications with Generative AI.&lt;/p&gt; &#xA;&lt;h2&gt;Why use PydanticAI&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Built by the team behind Pydantic (the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more)&lt;/li&gt; &#xA; &lt;li&gt;Model-agnostic — currently OpenAI, Gemini, and Groq are supported. And there is a simple interface to implement support for other models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.pydantic.dev/agents/#static-type-checking&#34;&gt;Type-safe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Control flow and agent composition is done with vanilla Python, allowing you to make use of the same Python development best practices you&#39;d use in any other (non-AI) project&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.pydantic.dev/results/#structured-result-validation&#34;&gt;Structured response&lt;/a&gt; validation with Pydantic&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.pydantic.dev/results/#streamed-results&#34;&gt;Streamed responses&lt;/a&gt;, including validation of streamed &lt;em&gt;structured&lt;/em&gt; responses with Pydantic&lt;/li&gt; &#xA; &lt;li&gt;Novel, type-safe &lt;a href=&#34;https://ai.pydantic.dev/dependencies/&#34;&gt;dependency injection system&lt;/a&gt;, useful for testing and eval-driven iterative development&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.pydantic.dev/logfire/&#34;&gt;Logfire integration&lt;/a&gt; for debugging and monitoring the performance and general behavior of your LLM-powered application&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;In Beta!&lt;/h2&gt; &#xA;&lt;p&gt;PydanticAI is in early beta, the API is still subject to change and there&#39;s a lot more to do. &lt;a href=&#34;https://github.com/pydantic/pydantic-ai/issues&#34;&gt;Feedback&lt;/a&gt; is very welcome!&lt;/p&gt; &#xA;&lt;h2&gt;Hello World Example&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a minimal example of PydanticAI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from pydantic_ai import Agent&#xA;&#xA;# Define a very simple agent including the model to use, you can also set the model when running the agent.&#xA;agent = Agent(&#xA;    &#39;gemini-1.5-flash&#39;,&#xA;    # Register a static system prompt using a keyword argument to the agent.&#xA;    # For more complex dynamically-generated system prompts, see the example below.&#xA;    system_prompt=&#39;Be concise, reply with one sentence.&#39;,&#xA;)&#xA;&#xA;# Run the agent synchronously, conducting a conversation with the LLM.&#xA;# Here the exchange should be very short: PydanticAI will send the system prompt and the user query to the LLM,&#xA;# the model will return a text response. See below for a more complex run.&#xA;result = agent.run_sync(&#39;Where does &#34;hello world&#34; come from?&#39;)&#xA;print(result.data)&#xA;&#34;&#34;&#34;&#xA;The first known use of &#34;hello, world&#34; was in a 1974 textbook about the C programming language.&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;(This example is complete, it can be run &#34;as is&#34;)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Not very interesting yet, but we can easily add &#34;tools&#34;, dynamic system prompts, and structured responses to build more powerful agents.&lt;/p&gt; &#xA;&lt;h2&gt;Tools &amp;amp; Dependency Injection Example&lt;/h2&gt; &#xA;&lt;p&gt;Here is a concise example using PydanticAI to build a support agent for a bank:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;(Better documented example &lt;a href=&#34;https://ai.pydantic.dev/#tools-dependency-injection-example&#34;&gt;in the docs&lt;/a&gt;)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from dataclasses import dataclass&#xA;&#xA;from pydantic import BaseModel, Field&#xA;from pydantic_ai import Agent, RunContext&#xA;&#xA;from bank_database import DatabaseConn&#xA;&#xA;&#xA;# SupportDependencies is used to pass data, connections, and logic into the model that will be needed when running&#xA;# system prompt and tool functions. Dependency injection provides a type-safe way to customise the behavior of your agents.&#xA;@dataclass&#xA;class SupportDependencies:&#xA;    customer_id: int&#xA;    db: DatabaseConn&#xA;&#xA;&#xA;# This pydantic model defines the structure of the result returned by the agent.&#xA;class SupportResult(BaseModel):&#xA;    support_advice: str = Field(description=&#39;Advice returned to the customer&#39;)&#xA;    block_card: bool = Field(description=&#34;Whether to block the customer&#39;s card&#34;)&#xA;    risk: int = Field(description=&#39;Risk level of query&#39;, ge=0, le=10)&#xA;&#xA;&#xA;# This agent will act as first-tier support in a bank.&#xA;# Agents are generic in the type of dependencies they accept and the type of result they return.&#xA;# In this case, the support agent has type `Agent[SupportDependencies, SupportResult]`.&#xA;support_agent = Agent(&#xA;    &#39;openai:gpt-4o&#39;,&#xA;    deps_type=SupportDependencies,&#xA;    # The response from the agent will, be guaranteed to be a SupportResult,&#xA;    # if validation fails the agent is prompted to try again.&#xA;    result_type=SupportResult,&#xA;    system_prompt=(&#xA;        &#39;You are a support agent in our bank, give the &#39;&#xA;        &#39;customer support and judge the risk level of their query.&#39;&#xA;    ),&#xA;)&#xA;&#xA;&#xA;# Dynamic system prompts can can make use of dependency injection.&#xA;# Dependencies are carried via the `RunContext` argument, which is parameterized with the `deps_type` from above.&#xA;# If the type annotation here is wrong, static type checkers will catch it.&#xA;@support_agent.system_prompt&#xA;async def add_customer_name(ctx: RunContext[SupportDependencies]) -&amp;gt; str:&#xA;    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)&#xA;    return f&#34;The customer&#39;s name is {customer_name!r}&#34;&#xA;&#xA;&#xA;# `tool` let you register functions which the LLM may call while responding to a user.&#xA;# Again, dependencies are carried via `RunContext`, any other arguments become the tool schema passed to the LLM.&#xA;# Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.&#xA;@support_agent.tool&#xA;async def customer_balance(&#xA;    ctx: RunContext[SupportDependencies], include_pending: bool&#xA;) -&amp;gt; float:&#xA;    &#34;&#34;&#34;Returns the customer&#39;s current account balance.&#34;&#34;&#34;&#xA;    # The docstring of a tool is also passed to the LLM as the description of the tool.&#xA;    # Parameter descriptions are extracted from the docstring and added to the parameter schema sent to the LLM.&#xA;    balance = await ctx.deps.db.customer_balance(&#xA;        id=ctx.deps.customer_id,&#xA;        include_pending=include_pending,&#xA;    )&#xA;    return balance&#xA;&#xA;&#xA;...  # In a real use case, you&#39;d add more tools and a longer system prompt&#xA;&#xA;&#xA;async def main():&#xA;    deps = SupportDependencies(customer_id=123, db=DatabaseConn())&#xA;    # Run the agent asynchronously, conducting a conversation with the LLM until a final response is reached.&#xA;    # Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve a result.&#xA;    result = await support_agent.run(&#39;What is my balance?&#39;, deps=deps)&#xA;    # The result will be validated with Pydantic to guarantee it is a `SupportResult`, since the agent is generic,&#xA;    # it&#39;ll also be typed as a `SupportResult` to aid with static type checking.&#xA;    print(result.data)&#xA;    &#34;&#34;&#34;&#xA;    support_advice=&#39;Hello John, your current account balance, including pending transactions, is $123.45.&#39; block_card=False risk=1&#xA;    &#34;&#34;&#34;&#xA;&#xA;    result = await support_agent.run(&#39;I just lost my card!&#39;, deps=deps)&#xA;    print(result.data)&#xA;    &#34;&#34;&#34;&#xA;    support_advice=&#34;I&#39;m sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.&#34; block_card=True risk=8&#xA;    &#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Next Steps&lt;/h2&gt; &#xA;&lt;p&gt;To try PydanticAI yourself, follow the instructions &lt;a href=&#34;https://ai.pydantic.dev/examples/&#34;&gt;in the examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://ai.pydantic.dev/agents/&#34;&gt;docs&lt;/a&gt; to learn more about building applications with PydanticAI.&lt;/p&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://ai.pydantic.dev/api/agent/&#34;&gt;API Reference&lt;/a&gt; to understand PydanticAI&#39;s interface.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/BitNet</title>
    <updated>2024-12-06T01:28:56Z</updated>
    <id>tag:github.com,2024-12-06:/microsoft/BitNet</id>
    <link href="https://github.com/microsoft/BitNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official inference framework for 1-bit LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;bitnet.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/version-1.0-blue&#34; alt=&#34;version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support &lt;strong&gt;fast&lt;/strong&gt; and &lt;strong&gt;lossless&lt;/strong&gt; inference of 1.58-bit models on CPU (with NPU and GPU support coming next).&lt;/p&gt; &#xA;&lt;p&gt;The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of &lt;strong&gt;1.37x&lt;/strong&gt; to &lt;strong&gt;5.07x&lt;/strong&gt; on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by &lt;strong&gt;55.4%&lt;/strong&gt; to &lt;strong&gt;70.0%&lt;/strong&gt;, further boosting overall efficiency. On x86 CPUs, speedups range from &lt;strong&gt;2.37x&lt;/strong&gt; to &lt;strong&gt;6.17x&lt;/strong&gt; with energy reductions between &lt;strong&gt;71.9%&lt;/strong&gt; to &lt;strong&gt;82.2%&lt;/strong&gt;. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the &lt;a href=&#34;https://arxiv.org/abs/2410.16144&#34;&gt;technical report&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/m2_performance.jpg&#34; alt=&#34;m2_performance&#34; width=&#34;800&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/BitNet/main/assets/intel_performance.jpg&#34; alt=&#34;m2_performance&#34; width=&#34;800&#34;&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The tested models are dummy setups used in a research context to demonstrate the inference performance of bitnet.cpp.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;A demo of bitnet.cpp running a BitNet b1.58 3B model on Apple M2:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&#34;&gt;https://github.com/user-attachments/assets/7f46b736-edec-4828-b809-4be780a3e5b1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;11/08/2024 &lt;a href=&#34;https://arxiv.org/abs/2411.04965&#34;&gt;BitNet a4.8: 4-bit Activations for 1-bit LLMs&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/NEW-red&#34; alt=&#34;NEW&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;10/21/2024 &lt;a href=&#34;https://arxiv.org/abs/2410.16144&#34;&gt;1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;10/17/2024 bitnet.cpp 1.0 released.&lt;/li&gt; &#xA; &lt;li&gt;03/21/2024 &lt;a href=&#34;https://github.com/microsoft/unilm/raw/master/bitnet/The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ.pdf&#34;&gt;The-Era-of-1-bit-LLMs__Training_Tips_Code_FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;02/27/2024 &lt;a href=&#34;https://arxiv.org/abs/2402.17764&#34;&gt;The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;10/17/2023 &lt;a href=&#34;https://arxiv.org/abs/2310.11453&#34;&gt;BitNet: Scaling 1-bit Transformers for Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on the &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; framework. We would like to thank all the authors for their contributions to the open-source community. Also, bitnet.cpp&#39;s kernels are built on top of the Lookup Table methodologies pioneered in &lt;a href=&#34;https://github.com/microsoft/T-MAC/&#34;&gt;T-MAC&lt;/a&gt;. For inference of general low-bit LLMs beyond ternary models, we recommend using T-MAC.&lt;/p&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;❗️&lt;strong&gt;We use existing 1-bit LLMs available on &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; to demonstrate the inference capabilities of bitnet.cpp. These models are neither trained nor released by Microsoft. We hope the release of bitnet.cpp will inspire the development of 1-bit LLMs in large-scale settings in terms of model size and training tokens.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt;  &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Parameters&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;CPU&lt;/th&gt; &#xA;   &lt;th colspan=&#34;3&#34;&gt;Kernel&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;I2_S&lt;/th&gt; &#xA;   &lt;th&gt;TL1&lt;/th&gt; &#xA;   &lt;th&gt;TL2&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/1bitLLM/bitnet_b1_58-large&#34;&gt;bitnet_b1_58-large&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;0.7B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/1bitLLM/bitnet_b1_58-3B&#34;&gt;bitnet_b1_58-3B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;3.3B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;a href=&#34;https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens&#34;&gt;Llama3-8B-1.58-100B-tokens&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;8.0B&lt;/td&gt; &#xA;   &lt;td&gt;x86&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ARM&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python&amp;gt;=3.9&lt;/li&gt; &#xA; &lt;li&gt;cmake&amp;gt;=3.22&lt;/li&gt; &#xA; &lt;li&gt;clang&amp;gt;=18 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;For Windows users, install &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;Visual Studio 2022&lt;/a&gt;. In the installer, toggle on at least the following options(this also automatically installs the required additional tools like CMake):&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Desktop-development with C++&lt;/li&gt; &#xA;     &lt;li&gt;C++-CMake Tools for Windows&lt;/li&gt; &#xA;     &lt;li&gt;Git for Windows&lt;/li&gt; &#xA;     &lt;li&gt;C++-Clang Compiler for Windows&lt;/li&gt; &#xA;     &lt;li&gt;MS-Build Support for LLVM-Toolset (clang)&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;For Debian/Ubuntu users, you can download with &lt;a href=&#34;https://apt.llvm.org/&#34;&gt;Automatic installation script&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash -c &#34;$(wget -O - https://apt.llvm.org/llvm.sh)&#34;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;conda (highly recommend)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build from source&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] If you are using Windows, please remember to always use a Developer Command Prompt / PowerShell for VS2022 for the following commands&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --recursive https://github.com/microsoft/BitNet.git&#xA;cd BitNet&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install the dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# (Recommended) Create a new conda environment&#xA;conda create -n bitnet-cpp python=3.9&#xA;conda activate bitnet-cpp&#xA;&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Build the project&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download the model from Hugging Face, convert it to quantized gguf format, and build the project&#xA;python setup_env.py --hf-repo HF1BitLLM/Llama3-8B-1.58-100B-tokens -q i2_s&#xA;&#xA;# Or you can manually download the model and run with local path&#xA;huggingface-cli download HF1BitLLM/Llama3-8B-1.58-100B-tokens --local-dir models/Llama3-8B-1.58-100B-tokens&#xA;python setup_env.py -md models/Llama3-8B-1.58-100B-tokens -q i2_s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&#xA;usage: setup_env.py [-h] [--hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens}] [--model-dir MODEL_DIR] [--log-dir LOG_DIR] [--quant-type {i2_s,tl1}] [--quant-embd]&#xA;                    [--use-pretuned]&#xA;&#xA;Setup the environment for running inference&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --hf-repo {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens}, -hr {1bitLLM/bitnet_b1_58-large,1bitLLM/bitnet_b1_58-3B,HF1BitLLM/Llama3-8B-1.58-100B-tokens}&#xA;                        Model used for inference&#xA;  --model-dir MODEL_DIR, -md MODEL_DIR&#xA;                        Directory to save/load the model&#xA;  --log-dir LOG_DIR, -ld LOG_DIR&#xA;                        Directory to save the logging info&#xA;  --quant-type {i2_s,tl1}, -q {i2_s,tl1}&#xA;                        Quantization type&#xA;  --quant-embd          Quantize the embeddings to f16&#xA;  --use-pretuned, -p    Use the pretuned kernel parameters&#xA;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Basic usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run inference with the quantized model&#xA;python run_inference.py -m models/Llama3-8B-1.58-100B-tokens/ggml-model-i2_s.gguf -p &#34;Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?\nAnswer:&#34; -n 6 -temp 0&#xA;&#xA;# Output:&#xA;# Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. Mary went back to the garden. Where is Mary?&#xA;# Answer: Mary is in the garden.&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&#xA;usage: run_inference.py [-h] [-m MODEL] [-n N_PREDICT] -p PROMPT [-t THREADS] [-c CTX_SIZE] [-temp TEMPERATURE]&#xA;&#xA;Run inference&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  -m MODEL, --model MODEL&#xA;                        Path to model file&#xA;  -n N_PREDICT, --n-predict N_PREDICT&#xA;                        Number of tokens to predict when generating text&#xA;  -p PROMPT, --prompt PROMPT&#xA;                        Prompt to generate text from&#xA;  -t THREADS, --threads THREADS&#xA;                        Number of threads to use&#xA;  -c CTX_SIZE, --ctx-size CTX_SIZE&#xA;                        Size of the prompt context&#xA;  -temp TEMPERATURE, --temperature TEMPERATURE&#xA;                        Temperature, a hyperparameter that controls the randomness of the generated text&#xA;&lt;/pre&gt; &#xA;&lt;h3&gt;Benchmark&lt;/h3&gt; &#xA;&lt;p&gt;We provide scripts to run the inference benchmark providing a model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: e2e_benchmark.py -m MODEL [-n N_TOKEN] [-p N_PROMPT] [-t THREADS]  &#xA;   &#xA;Setup the environment for running the inference  &#xA;   &#xA;required arguments:  &#xA;  -m MODEL, --model MODEL  &#xA;                        Path to the model file. &#xA;   &#xA;optional arguments:  &#xA;  -h, --help  &#xA;                        Show this help message and exit. &#xA;  -n N_TOKEN, --n-token N_TOKEN  &#xA;                        Number of generated tokens. &#xA;  -p N_PROMPT, --n-prompt N_PROMPT  &#xA;                        Prompt to generate text from. &#xA;  -t THREADS, --threads THREADS  &#xA;                        Number of threads to use. &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here&#39;s a brief explanation of each argument:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;-m&lt;/code&gt;, &lt;code&gt;--model&lt;/code&gt;: The path to the model file. This is a required argument that must be provided when running the script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-n&lt;/code&gt;, &lt;code&gt;--n-token&lt;/code&gt;: The number of tokens to generate during the inference. It is an optional argument with a default value of 128.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-p&lt;/code&gt;, &lt;code&gt;--n-prompt&lt;/code&gt;: The number of prompt tokens to use for generating text. This is an optional argument with a default value of 512.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-t&lt;/code&gt;, &lt;code&gt;--threads&lt;/code&gt;: The number of threads to use for running the inference. It is an optional argument with a default value of 2.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;: Show the help message and exit. Use this argument to display usage information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python utils/e2e_benchmark.py -m /path/to/model -n 200 -p 256 -t 4  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command would run the inference benchmark using the model located at &lt;code&gt;/path/to/model&lt;/code&gt;, generating 200 tokens from a 256 token prompt, utilizing 4 threads.&lt;/p&gt; &#xA;&lt;p&gt;For the model layout that do not supported by any public model, we provide scripts to generate a dummy model with the given model layout, and run the benchmark on your machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python utils/generate-dummy-bitnet-model.py models/bitnet_b1_58-large --outfile models/dummy-bitnet-125m.tl1.gguf --outtype tl1 --model-size 125M&#xA;&#xA;# Run benchmark with the generated model, use -m to specify the model path, -p to specify the prompt processed, -n to specify the number of token to generate&#xA;python utils/e2e_benchmark.py -m models/dummy-bitnet-125m.tl1.gguf -p 512 -n 128&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>spaceandtimelabs/sxt-proof-of-sql</title>
    <updated>2024-12-06T01:28:56Z</updated>
    <id>tag:github.com,2024-12-06:/spaceandtimelabs/sxt-proof-of-sql</id>
    <link href="https://github.com/spaceandtimelabs/sxt-proof-of-sql" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Space and Time | Proof of SQL&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Proof of SQL&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/docs/ProofOfSQLBanner.png&#34; alt=&#34;Proof of SQL&#34; width=&#34;100%&#34;&gt; &lt;a href=&#34;https://twitter.com/intent/follow?screen_name=spaceandtimedb&#34;&gt;&lt;img alt=&#34;Twitter&#34; src=&#34;https://img.shields.io/twitter/follow/spaceandtimedb.svg?style=social&amp;amp;label=Follow&#34;&gt;&lt;/a&gt;&lt;a href=&#34;http://discord.gg/SpaceandTimeDB&#34;&gt;&lt;img alt=&#34;Discord Server&#34; src=&#34;https://img.shields.io/discord/953025874154893342?logo=discord&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Proof of SQL is a high performance zero knowledge (ZK) prover developed by the &lt;a href=&#34;https://www.spaceandtime.io/&#34;&gt;Space and Time&lt;/a&gt; team, which cryptographically guarantees SQL queries were computed accurately against untampered data. It targets online latencies while proving computations over entire chain histories, an order of magnitude faster than state-of-the art zkVMs and coprocessors.&lt;/p&gt; &#xA;&lt;p&gt;As the first sub-second ZK prover, the protocol can execute analytic queries over 100k-row tables in less than a second on a single GPU (see benchmarks below). It can aggregate over millions of rows of indexed data within Ethereum block time on a single NVIDIA T4. The protocol is designed to support both onchain and offchain verification, leveraging a novel commitment scheme which significantly lowers gas fees with onchain verification.&lt;/p&gt; &#xA;&lt;p&gt;Using Proof of SQL, developers can compute over both onchain and offchain datasets in a trustless manner, proving the result back to their smart contract (or offchain verifier) just-in-time during a transaction to power more sophisticated DeFi protocols with data-driven contracts. Proof of SQL can be integrated into any SQL database (such as &lt;a href=&#34;https://cloud.google.com/blog/topics/partners/how-space-and-times-proof-of-sql-integrates-with-bigquery&#34;&gt;Google BigQuery&lt;/a&gt;), centralized or decentralized, and is already securing some of the most prominent Web3 apps, financial institutions, and enterprises.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Proof of SQL is in active development, and not all SQL functions are supported yet. Proof of SQL is most powerful as a community-driven project. We hope to foster a large group of contributers that can help maintain, improve, and use this project to create a trustless and data-driven future. Please create an Issue, file a PR, or reach out via Discord if you want to add a SQL feature, integrate into another ZK solution, use this in your project, or anything else! Check out our guidelines: &lt;a href=&#34;https://github.com/spaceandtimelabs/sxt-proof-of-sql/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;For Developers&lt;/h2&gt; &#xA;&lt;p&gt;Get started with Proof of SQL by using the published crate on &lt;a href=&#34;https://crates.io/&#34;&gt;crates.io&lt;/a&gt; or clone the repo and check out the examples. Check out the following sections of the README:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/#benchmarks&#34;&gt;Benchmarks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/#supported-sql-syntax&#34;&gt;Supported SQL Syntax&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/#roadmap&#34;&gt;Roadmap&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/#protocol-overview&#34;&gt;Protocol Overview&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux &lt;code&gt;x86_64&lt;/code&gt; (NOTE: Most of the codebase &lt;em&gt;should&lt;/em&gt; work for most rust targets. However, proofs are accelerated using NVIDIA GPUs, so other targets would run very slowly and may require modification.)&lt;/li&gt; &#xA; &lt;li&gt;NVIDIA GPU &amp;amp; Drivers (Strongly Recommended)&lt;/li&gt; &#xA; &lt;li&gt;lld (&lt;code&gt;sudo apt install lld&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;clang (&lt;code&gt;sudo apt install clang&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;Rust 1.81.0&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Workaround for non-Linux and/or non-GPU machines. &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Workaround #1: enable the CPU version of Blitzar by setting the &lt;code&gt;BLITZAR_BACKEND&lt;/code&gt; environment variable. Example: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export BLITZAR_BACKEND=cpu&#xA;cargo test --all-features --all-targets&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Workaround #2: disable the &lt;code&gt;blitzar&lt;/code&gt; feature in the repo. Example &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo test --no-default-features --features=&#34;arrow cpu-perf&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Proof of SQL comes with example code demonstrating its usage. You can find the examples in the &lt;code&gt;crates/proof-of-sql/examples&lt;/code&gt; folder. Below are explainations of how to run some of these examples:&lt;/p&gt; &#xA;&lt;h3&gt;&#34;Hello World&#34; Example&lt;/h3&gt; &#xA;&lt;p&gt;The &#34;Hello World&#34; example demonstrates generating and verifying a proof of the query &lt;code&gt;SELECT b FROM table WHERE a = 2&lt;/code&gt; for the table:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;a&lt;/th&gt; &#xA;   &lt;th&gt;b&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;hi&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;hello&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;there&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;world&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Run&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo run --example hello_world &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] To run this example without the &lt;code&gt;blitzar&lt;/code&gt; (i.e CPU only) feature:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo run --example hello_world --no-default-features --features=&#34;rayon test&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Output&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;Warming up GPU... 520.959485ms&#xA;Loading data... 3.229767ms&#xA;Parsing Query... 1.870256ms&#xA;Generating Proof... 467.45371ms&#xA;Verifying Proof... 7.106864ms&#xA;Valid proof!&#xA;Query result: OwnedTable { table: {Identifier { name: &#34;b&#34; }: VarChar([&#34;hello&#34;, &#34;world&#34;])} }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a detailed explanation of the example and its implementation, refer to the &lt;a href=&#34;https://github.com/spaceandtimelabs/sxt-proof-of-sql/raw/main/crates/proof-of-sql/examples/hello_world/README.md&#34;&gt;README&lt;/a&gt; and source code in &lt;a href=&#34;https://github.com/spaceandtimelabs/sxt-proof-of-sql/raw/main/crates/proof-of-sql/examples/hello_world/main.rs&#34;&gt;hello_world/main.rs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;CSV Database Example&lt;/h3&gt; &#xA;&lt;p&gt;The CSV Database example demonstrates an implementation of a simple CSV-backed database with Proof of SQL capabilities.&lt;/p&gt; &#xA;&lt;p&gt;To install the example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo install --example posql_db --path crates/proof-of-sql #TODO: update once this is published to crates.io&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For detailed usage instructions and examples of how to create, append to, prove, and verify queries in the CSV-backed database, refer to the &lt;a href=&#34;https://github.com/spaceandtimelabs/sxt-proof-of-sql/raw/main/crates/proof-of-sql/examples/posql_db/README.md&#34;&gt;README&lt;/a&gt; and source code in &lt;a href=&#34;https://github.com/spaceandtimelabs/sxt-proof-of-sql/raw/main/crates/proof-of-sql/examples/posql_db/main.rs&#34;&gt;posql_db/main.rs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;Proof of SQL is optimized for speed and efficiency. Here&#39;s how it&#39;s so fast:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We use &lt;strong&gt;native, precomputed commitments&lt;/strong&gt; to the data. In other words, when adding data to the database, we compute a &#34;digest&#34; of the data, which effectively &#34;locks in&#34; the data. Instead of using a merkle tree based commitment, like those use in most blockchains, we use the commitment scheme that is inherent to Proof of SQL itself.&lt;/li&gt; &#xA; &lt;li&gt;SQL is conducive to a &lt;strong&gt;natural arithmatization&lt;/strong&gt;, meaning that there is very little overhead compared with other proof systems that are designed around instructions/sequential compute. Instead, Proof of SQL is designed from the ground up with data processing and parallelism in mind.&lt;/li&gt; &#xA; &lt;li&gt;We use &lt;strong&gt;GPU acceleration&lt;/strong&gt; on the most expensive cryptography in the prover. We use &lt;a href=&#34;https://github.com/spaceandtimelabs/blitzar&#34;&gt;Blitzar&lt;/a&gt; as our acceleration framework.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Setup&lt;/h3&gt; &#xA;&lt;p&gt;We run benchmarks using both a machine with multiple NVIDIA A100 GPUs (NC A100 v4-series Azure VM) and a machine with a single NVIDIA T4 GPU (NCasT4_v3-series Azure VM).&lt;/p&gt; &#xA;&lt;p&gt;To run these benchmarks we first generate a large, randomly-filled table of data such as the following:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;a (BIGINT)&lt;/th&gt; &#xA;   &lt;th&gt;b (BIGINT)&lt;/th&gt; &#xA;   &lt;th&gt;c (VARCHAR)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;17717&lt;/td&gt; &#xA;   &lt;td&gt;-1&lt;/td&gt; &#xA;   &lt;td&gt;Z&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11651&lt;/td&gt; &#xA;   &lt;td&gt;-3&lt;/td&gt; &#xA;   &lt;td&gt;W&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-9563&lt;/td&gt; &#xA;   &lt;td&gt;-2&lt;/td&gt; &#xA;   &lt;td&gt;dS&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-6435&lt;/td&gt; &#xA;   &lt;td&gt;-2&lt;/td&gt; &#xA;   &lt;td&gt;x&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;-8338&lt;/td&gt; &#xA;   &lt;td&gt;-1&lt;/td&gt; &#xA;   &lt;td&gt;jI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12420&lt;/td&gt; &#xA;   &lt;td&gt;-2&lt;/td&gt; &#xA;   &lt;td&gt;DX&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11546&lt;/td&gt; &#xA;   &lt;td&gt;-3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;18292&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6500&lt;/td&gt; &#xA;   &lt;td&gt;-1&lt;/td&gt; &#xA;   &lt;td&gt;C&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16219&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;D5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then, we run the following 3 queries against these data, prove, and verify the results:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Query #1 - &lt;code&gt;SELECT b FROM table WHERE a = 0&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Query #2 - &lt;code&gt;SELECT * FROM table WHERE ((a = 0) or (b = 1)) and (not (c = &#39;a&#39;))&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Query #3 - &lt;code&gt;SELECT b, SUM(a) as sum_a, COUNT(*) as c FROM table WHERE (c = &#39;a&#39; OR c = &#39;b&#39;) AND b &amp;gt; 0 GROUP BY b&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;An example result for the 3rd query looks like this:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;code&gt;b&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;sum_a&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;code&gt;c&lt;/code&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;-45585&lt;/td&gt; &#xA;   &lt;td&gt;301&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;-137574,&lt;/td&gt; &#xA;   &lt;td&gt;300&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;-107073&lt;/td&gt; &#xA;   &lt;td&gt;282&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;p&gt;The results are shown in the graphs below for the T4 machine and the A100 machine on all three of the queries listed above. Broadly the results are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A query against 200 thousand rows of data can be proven in sub-second time.&lt;/li&gt; &#xA; &lt;li&gt;A query against 100 million rows of data can be proven in roughly a minute.&lt;/li&gt; &#xA; &lt;li&gt;Verification time is roughly 20ms across the board.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/docs/ProofOfSQLBenchmarks200kT4.svg?sanitize=true&#34; alt=&#34;Proof Of SQL Benchmarks (200k - T4)&#34; width=&#34;50%&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/docs/ProofOfSQLBenchmarks200kA100.svg?sanitize=true&#34; alt=&#34;Proof Of SQL Benchmarks (200k - A100)&#34; width=&#34;50%&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/docs/ProofOfSQLBenchmarks10mT4.svg?sanitize=true&#34; alt=&#34;Proof Of SQL Benchmarks (10m - T4)&#34; width=&#34;50%&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/docs/ProofOfSQLBenchmarks10mA100.svg?sanitize=true&#34; alt=&#34;Proof Of SQL Benchmarks (10m - A100)&#34; width=&#34;50%&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Supported SQL Syntax&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/spaceandtimelabs/sxt-proof-of-sql/raw/main/docs/SQLSyntaxSpecification.md&#34;&gt;SQL specification&lt;/a&gt; for more details. Broadly, we support the following with more SQL features being added quickly:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;SELECT ... WHERE&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GROUP BY&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Comparison operations: &lt;code&gt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, etc.&lt;/li&gt; &#xA; &lt;li&gt;Logical operations: &lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;OR&lt;/code&gt;, &lt;code&gt;NOT&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Numerical operations &lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Aggregations: &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;COUNT&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Data Types: &lt;code&gt;BOOLEAN&lt;/code&gt;, Integer types, &lt;code&gt;VARCHAR&lt;/code&gt;, &lt;code&gt;DECIMAL75&lt;/code&gt;, &lt;code&gt;TIMESTAMP&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Proof of SQL is in active development. Here are some items that we are currently working on. We are happy to recieve feedback on additional features that the community requests.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Expanded SQL support - in particular, multi-table queries (like JOIN) and subqueries&lt;/li&gt; &#xA; &lt;li&gt;Cluster scale proofs - this means faster proofs over larger tables!&lt;/li&gt; &#xA; &lt;li&gt;Solidity (EVM) verifier - for more efficient onchain verification.&lt;/li&gt; &#xA; &lt;li&gt;A novel commitment scheme - while we support a variety of commitment schemes, we are developing a commitment scheme specifically for database operations, ensuring lower-gas onchain verification.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are also currently undergoing robust security audits. Keep this in mind as you use this code.&lt;/p&gt; &#xA;&lt;h2&gt;Protocol Overview&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://assets-global.website-files.com/642d91209f1e772d3740afa0/658edf3cf26933c4878ec965_whitepaper.pdf&#34;&gt;Space and Time Whitepaper&lt;/a&gt; for a more in depth explanation. We will also be adding more technical documentation to this repo soon.&lt;/p&gt; &#xA;&lt;p&gt;We created this protocol with a few key goals. First, it needs to be super fast for data processing, both for verification and round-trip execution. This requires a design that is built from the ground up, as opposed to using arbitrary zkVMs. Second, we made it very developer-friendly. Using SQL, the most popular data query language, ensures a familiar experience for anyone building data-focused applications, or sophisticated data-driven contracts. Finally, our protocol is designed to handle complex data processing, not just simple serial compute or data retrieval.&lt;/p&gt; &#xA;&lt;p&gt;In this protocol, there are two main roles: the client sending the query (Verifier) and the database service returning the result (Prover). Of course, the Verifier doesn&#39;t always have to send the query; it can be any client, such as a smart contract, a dapp frontend, or a laptop . This setup is crucial for applications with limited compute or storage but still requires a security guarantee that data analytics are correctly executed and the data remains unaltered. The Prover handles heavy computations, while the Verifier is lightweight, suitable for client devices or smart contracts with limited resources.&lt;/p&gt; &#xA;&lt;p&gt;A key architectural feature is the concept of a commitment, or digest. To ensure data integrity, the Verifier maintains this commitment to detect any tampering. Think of it as a digital fingerprint—a lightweight digest representing the data in the table.&lt;/p&gt; &#xA;&lt;h3&gt;Data Ingestion&lt;/h3&gt; &#xA;&lt;p&gt;The initial interaction between the Verifier and the Prover involves data ingestion. In this process, when a service or client submits data for database inclusion, it first passes through the Verifier. Here, the Verifier generates (or updates) a commitment containing sufficient information to safeguard against tampering throughout the protocol. Once this commitment is established, the Verifier forwards the data to the database for storage, while retaining the commitment for future reference.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/docs/DataIngestionDiagram.png&#34; alt=&#34;Data Ingestion Diagram&#34; width=&#34;50%&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Query Request&lt;/h3&gt; &#xA;&lt;p&gt;The second interaction involves query requests, where the Verifier seeks data analytics on Prover-held data. When a service, client, or Verifier initiates a query request, it sends the request to the Prover. Here, the Prover parses the query, computes the result, and generates a proof, sent alongside the result to the Verifier, which is maintaining the commitment. The Verifier, armed with the proof and commitment, can verify the Prover&#39;s result against the query request.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/spaceandtimelabs/sxt-proof-of-sql/main/docs/QueryRequestDiagram.png&#34; alt=&#34;Query Request Diagram&#34; width=&#34;50%&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Proof of SQL is licensed under the Decentralized Open Software License 1.0. Please see the &lt;a href=&#34;https://github.com/spaceandtimelabs/sxt-proof-of-sql/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
</feed>