<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-24T01:28:09Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>piotrostr/listen</title>
    <updated>2025-01-24T01:28:09Z</updated>
    <id>tag:github.com,2025-01-24:/piotrostr/listen</id>
    <link href="https://github.com/piotrostr/listen" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Solana Swiss Army Knife&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/piotrostr/listen/main/frontend/public/listen-more.png&#34; width=&#34;35%&#34;&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://docs.listen-rs.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-API-blue.svg?sanitize=true&#34;&gt;&lt;/a&gt; &amp;nbsp; &lt;a href=&#34;https://github.com/piotrostr/listen&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/piotrostr/listen?style=social&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/built_with-Rust-dca282.svg?logo=rust&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;code&gt;listen&lt;/code&gt; is a Solana Swiss-Knife toolkit for algorithmic trading &lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîç Real-time transaction monitoring&lt;/li&gt; &#xA; &lt;li&gt;üí± Multi-DEX swap execution (Pump.fun, Jupiter V6 API or Raydium)&lt;/li&gt; &#xA; &lt;li&gt;üöÄ Blazingly fast transactions thanks to Jito MEV bundles&lt;/li&gt; &#xA; &lt;li&gt;üìä Price tracking and metrics&lt;/li&gt; &#xA; &lt;li&gt;üß∞ Token management utilities&lt;/li&gt; &#xA; &lt;li&gt;üìà Performance monitoring with Prometheus integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And more!&lt;/p&gt; &#xA;&lt;p&gt;It works plug&#39;n&#39;play with &lt;a href=&#34;https://github.com/0xPlaygrounds/rig&#34;&gt;$arc rig framework&lt;/a&gt; framework allowing AI Agents interact with the Solana blockchain, see example: &lt;a href=&#34;https://github.com/piotrostr/listen/raw/main/src/agent.rs&#34;&gt;src/agent.rs&lt;/a&gt; and the output &lt;a href=&#34;https://github.com/piotrostr/listen/raw/main/example.png&#34;&gt;image&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For complete rundown of features, check out the CLI output of &lt;code&gt;cargo run&lt;/code&gt; or the &lt;a href=&#34;https://docs.listen-rs.com/&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;To play around with listen-rs, you can use the UI&lt;/p&gt; &#xA;&lt;p&gt;Fill in the &lt;code&gt;.env.example&lt;/code&gt; and &lt;code&gt;./dashboard/.env.example&lt;/code&gt;, copy over to &lt;code&gt;.env&lt;/code&gt; and &lt;code&gt;./dashboard/.env.example&lt;/code&gt;, then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then access the dashboard over &lt;code&gt;http://localhost:4173&lt;/code&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] listen-rs is undergoing rapid iterations, some things might not work and there could be breaking changes&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;System Dependencies&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Rust (with nightly toolchain)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;protoc&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;build-essential&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pkg-config&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;libssl-dev&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configuration&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Copy &lt;code&gt;.env.example&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Set up &lt;code&gt;auth.json&lt;/code&gt; for JITO authentication (optional, gRPC HTTP/2.0 searcher client)&lt;/li&gt; &#xA;   &lt;li&gt;Populate &lt;code&gt;fund.json&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Both keypairs are in &lt;code&gt;solana-keygen&lt;/code&gt; format, array of 64 bytes, 32 bytes private key and 32 bytes public key.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install dependencies&#xA;sudo apt install protoc build-essential pkg-config libssl-dev&#xA;&#xA;# Build&#xA;cargo build --release&#xA;&#xA;# Run services&#xA;./run-systemd-services.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Transaction Monitoring&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo run -- listen \&#xA;  --worker-count [COUNT] \&#xA;  --buffer-size [SIZE]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Token Swapping&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo run -- swap \&#xA;  --input-mint sol \&#xA;  --output-mint EPjFWdd5AufqSSqeM2qN1xzybapC8G4wEGGkZwyTDt1v \&#xA;  --amount 10000000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Default configuration is set for mainnet with small transactions. Ensure proper configuration for testnet usage and carefully review code before execution.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Metrics and Monitoring&lt;/h2&gt; &#xA;&lt;p&gt;Listen includes built-in metrics exposed at &lt;code&gt;localhost:3030/metrics&lt;/code&gt;. To visualize:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Start Prometheus:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;prometheus --config=prometheus.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Access metrics at &lt;code&gt;localhost:3030/metrics&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Grafana should show something like this&lt;/p&gt; &#xA;&lt;p&gt;&lt;img width=&#34;910&#34; alt=&#34;image&#34; src=&#34;https://github.com/piotrostr/listen/assets/63755291/95668158-9f7d-4cd2-be84-7c2b893d3f5c&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Swap Profiling&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;stackcollapse.pl&lt;/code&gt; can be installed through&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;gh repo clone brendangregg/FlameGraph &amp;amp;&amp;amp; \&#xA;  sudo cp FlameGraph/stackcollapse.pl /usr/local/bin &amp;amp;&amp;amp; \&#xA;  sudo cp FlameGraph/flamegraph.pl /usr/local/bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Profile swap performance using DTrace to produce a flamegraph:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./hack/profile-swap.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;1210&#34; alt=&#34;image&#34; src=&#34;https://github.com/piotrostr/listen/assets/63755291/699405b7-adf0-448b-89c1-ba71152dc72b&#34;&gt;</summary>
  </entry>
  <entry>
    <title>deepseek-ai/awesome-deepseek-integration</title>
    <updated>2025-01-24T01:28:09Z</updated>
    <id>tag:github.com,2025-01-24:/deepseek-ai/awesome-deepseek-integration</id>
    <link href="https://github.com/deepseek-ai/awesome-deepseek-integration" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;1000px&#34; alt=&#34;Awesome DeepSeek Integrations&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/Awesome%20DeepSeek%20Integrations.png&#34;&gt; &lt;/p&gt; &#xA; &lt;h1&gt;Awesome DeepSeek Integrations &lt;img src=&#34;https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/h1&gt; &#xA; &lt;p&gt;Integrate the DeepSeek API into popular softwares. Access &lt;a href=&#34;https://platform.deepseek.com/&#34;&gt;DeepSeek Open Platform&lt;/a&gt; to get an API key.&lt;/p&gt; &#xA; &lt;p&gt;English/&lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/README_cn.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Applications&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/13600976/224d547a-6fbc-47c8-859f-aa14813e2b0f&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/chatbox/README.md&#34;&gt;Chatbox&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Chatbox is a desktop client for multiple cutting-edge LLM models, available on Windows, Mac and Linux &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/bb65404c-f867-42d8-ae2b-281fe953ab54&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/chatgpt_next_web/README.md&#34;&gt; ChatGPT-Next-Web &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; ChatGPT Next Web is a cross-platform ChatGPT web UI, with GPT3, GPT4 &amp;amp; Gemini Pro support &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/liubai/assets/liubai-logo.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/liubai/README.md&#34;&gt;Liubai&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Liubai allows DeepSeek to have arms and legs to manipulate your notes, tasks, calendars, and to-do lists just on WeChat! &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/1ac9791b-87f7-41d9-9282-a70698344e1d&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/pal/README.md&#34;&gt; Pal - AI Chat Client&lt;br&gt;(iOS, ipadOS) &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Pal is a customized chat playground on iOS &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://www.librechat.ai/librechat.svg?sanitize=true&#34; alt=&#34;LibreChat&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://www.librechat.ai/docs/configuration/librechat_yaml/ai_endpoints/deepseek&#34;&gt;LibreChat&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; LibreChat is a customizable open-source app that seamlessly integrates DeepSeek for enhanced AI interactions &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rss-translator/RSS-Translator/main/core/static/favicon.ico&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/rss_translator/README.md&#34;&gt; RSS Translator &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Translate RSS feeds into your language! &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ysnows/enconvo_media/main/logo.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/enconvo/README.md&#34;&gt; Enconvo &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Enconvo is the Launcher of the AI era, the entry point for all AI functions, and a thoughtful intelligent assistant.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/kangfenmao/cherry-studio/raw/main/src/renderer/src/assets/images/logo.png?raw=true&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34; style=&#34;border-radius: 10px&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/cherrystudio/README.md&#34;&gt;Cherry Studio&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A powerful desktop AI assistant for producer&lt;/td&gt; &#xA;  &lt;/tr&gt;  &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;img src=&#34;https://tomemo.top/images/logo.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/tomemo/README.md&#34;&gt; ToMemo (iOS, ipadOS) &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; A phrasebook + clipboard history + keyboard iOS app with integrated AI macromodeling for quick output use in the keyboard.&lt;/td&gt; &#xA;  &lt;/tr&gt;  &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/buxuku/video-subtitle-master/refs/heads/main/resources/icon.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/buxuku/video-subtitle-master&#34;&gt;Video Subtitle Master&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt; Batch generate subtitles for videos, with the ability to translate subtitles into other languages. This is a client-side tool that supports both Mac and Windows platforms and integrates with multiple translation services such as Baidu, Volcengine, DeepLx, OpenAI, DeepSeek, and Ollama.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/UnknownEnergy/chatgpt-api/raw/master/dist/assets/chatworm-72x72.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/UnknownEnergy/chatgpt-api/raw/master/README.md&#34;&gt;Chatworm&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Chatworm is a webapp for multiple cutting-edge LLM models, open-source and also available on Android &lt;/td&gt; &#xA;  &lt;/tr&gt;  &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tisfeng/ImageBed/main/uPic/icon_512x512@2x.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/tisfeng/Easydict&#34;&gt;Easydict&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt; Easydict is a concise and easy-to-use translation dictionary macOS App that allows you to easily and elegantly look up words or translate text. Supports calling large language model APIs for translation.&lt;/td&gt; &#xA;  &lt;/tr&gt;  &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt; &lt;img src=&#34;https://www.raycast.com/favicon-production.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/raycast/README.md&#34;&gt;Raycast&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raycast.com/?via=ViGeng&#34;&gt;Raycast&lt;/a&gt; is a productivity tool for macOS that lets you control your tools with a few keystrokes. It supports various extensions including DeepSeek AI.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/193405629?s=200&amp;amp;v=4&#34; alt=&#34;PHP Client&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-php/deepseek-php-client/raw/master/README.md&#34;&gt;PHP Client&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Deepseek PHP Client is a robust and community-driven PHP client library for seamless integration with the Deepseek API &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://avatars.githubusercontent.com/u/958072?s=200&amp;amp;v=4&#34; alt=&#34;Laravel Integration&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-php/deepseek-laravel/raw/master/README.md&#34;&gt;Laravel Integration&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Laravel wrapper for Deepseek PHP client, to seamless deepseek API integration with laravel applications.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/zotero/assets/zotero-icon.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/zotero/README_cn.md&#34;&gt;Zotero&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://www.zotero.org&#34;&gt;Zotero&lt;/a&gt; is a free, easy-to-use tool to help you collect, organize, annotate, cite, and share research.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/Siyuan/assets/image-20250122162731-7wkftbw.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/Siyuan/README.md&#34;&gt;SiYuan&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; SiYuan is a privacy-first personal knowledge management system that supports complete offline usage, as well as end-to-end encrypted data sync.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;RAG framework&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/33142505/77093e84-9f7c-4716-9168-bac962fa1372&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/ragflow/README.md&#34;&gt; RAGFlow &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; An open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted data. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;IM Application Plugins&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/InternLM/HuixiangDou/releases/download/v0.1.0rc1/huixiangdou.jpg&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/huixiangdou/README_cn.md&#34;&gt;HuixiangDou&lt;br&gt;(wechat,lark)&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Domain knowledge assistant in personal WeChat and Feishu, focusing on answering questions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/RockChinQ/QChatGPT/raw/master/res/logo.png?raw=true&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/RockChinQ/QChatGPT&#34;&gt;QChatGPT&lt;br&gt;ÔºàQQÔºâ&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; A QQ chatbot with high stability, plugin support, and real-time networking &lt;/td&gt;  &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Browser Extensions&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/9d3f42b8-fcd0-47ab-8b06-1dd0554dd80e&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/immersive_translate/README.md&#34;&gt; Immersive Translate &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Immersive Translate is a bilingual webpage translation plugin &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/8a301619-a3de-489b-81fd-69aaa7c1c561&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/chatgpt_box/README.md&#34;&gt; ChatGPT Box &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; ChatGPT Box is a ChatGPT integration in browser, completely for free &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/c3d9d100-247a-41cc-97c1-10b01ed25e70&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/hcfy/README.md&#34;&gt; hcfy (ÂàíËØçÁøªËØë) &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; hcfy (ÂàíËØçÁøªËØë) is a web browser extension to integrate multiple translation services &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://static.eudic.net/web/trans/en_trans.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/Lulu%20Translate/README.md&#34;&gt; Lulu Translate &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; The plugin provides mouse selection translation, paragraph-by-paragraph comparison translation, and PDF document translation functionalities. It can utilize various translation engines, such as DeepSeek AI, Bing, GPT, Google, etc. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;VS Code Extensions&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/e4d082de-6f64-44b9-beaa-0de55d70cfab&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/continue/README.md&#34;&gt; Continue &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Continue is an open-source autopilot in IDE &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/cline/assets/favicon.png?raw=true&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/cline/README.md&#34;&gt; Cline &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Meet Cline, an AI assistant that can use your CLI aNd Editor. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;neovim Extensions&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/d66dfc62-8e69-4b00-8549-d0158e48e2e0&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/avante.nvim/README.md&#34;&gt; avante.nvim &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; avante.nvim is an open-source autopilot in IDE &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/d66dfc62-8e69-4b00-8549-d0158e48e2e0&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/llm.nvim/README.md&#34;&gt; llm.nvim &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; A free large language model(LLM) plugin that allows you to interact with LLM in Neovim. Supports any LLM, such as Deepseek, GPT, GLM, Kimi or local LLMs (such as ollama). &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/d66dfc62-8e69-4b00-8549-d0158e48e2e0&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/awesome-deepseek-integration/main/docs/codecompanion.nvim/README.md&#34;&gt; codecompanion.nvim &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; AI-powered coding, seamlessly in Neovim. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;JetBrains Extensions&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://plugins.jetbrains.com/files/21520/412905/icon/pluginIcon.svg?sanitize=true&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://ide.unitmesh.cc/quick-start&#34;&gt; AutoDev &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt;‚ÄçAutoDev is an open-source AI coding assistant in JetBrain&#39;s IDE &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://plugins.jetbrains.com/files/21410/561595/icon/pluginIcon.svg?sanitize=true&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://plugins.jetbrains.com/plugin/21410-onegai-copilot&#34;&gt; Onegai Copilot &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt;Onegai Copilot is an AI coding assistant in JetBrain&#39;s IDE &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/e4d082de-6f64-44b9-beaa-0de55d70cfab&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/raw/main/docs/continue/README.md&#34;&gt; Continue &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Continue is an open-source autopilot in IDE &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://raw.githubusercontent.com/a18792721831/studyplugin/535b9cab69da0f97b42dcaebb00bb0d4ed15c8a6/translate/src/main/resources/META-INF/pluginIcon.svg?sanitize=true&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://plugins.jetbrains.com/plugin/18336-chinese-english-translate&#34;&gt;Chinese-English Translate&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Chinese-English Translate is a multiple translation services in JetBrain&#39;s IDE &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://plugins.jetbrains.com/files/24851/659002/icon/pluginIcon.svg?sanitize=true&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://plugins.jetbrains.com/plugin/24851-ai-git-commit&#34;&gt;AI Git Commit&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; This plugin uses AI to automatically generate commit messages based on the changes in your code. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Cursor&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://global.discourse-cdn.com/flex020/uploads/cursor1/original/2X/a/a4f78589d63edd61a2843306f8e11bad9590f0ca.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://www.cursor.com/&#34;&gt; Cursor &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt;‚ÄçThe AI Code Editor&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Others&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/assets/59196087/c1e47b01-1766-4f7e-bfe6-ab3cb3991c30&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/deepseek-ai/awesome-deepseek-integration/tree/main/docs/siri_deepseek_shortcut&#34;&gt; siri_deepseek_shortcut &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Siri equiped with the DeepSeek API &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://github.com/n8n-io/n8n/raw/master/assets/n8n-logo.png?raw=true&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/rubickecho/n8n-deepseek&#34;&gt; n8n-nodes-deepseek &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; An N8N community node that supports direct integration with the DeepSeek API into workflows. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;img src=&#34;https://framerusercontent.com/images/8rF2JOaZ8l9AvM4H6ezliw44aI.png&#34; alt=&#34;Icon&#34; width=&#34;64&#34; height=&#34;auto&#34;&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt; LiteLLM &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format. Supports DeepSeek AI with cost tracking as well. &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt;</summary>
  </entry>
  <entry>
    <title>deepseek-ai/DeepSeek-LLM</title>
    <updated>2025-01-24T01:28:09Z</updated>
    <id>tag:github.com,2025-01-24:/deepseek-ai/DeepSeek-LLM</id>
    <link href="https://github.com/deepseek-ai/DeepSeek-LLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepSeek LLM: Let there be answers&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/images/logo.svg?sanitize=true&#34; width=&#34;60%&#34; alt=&#34;DeepSeek LLM&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.deepseek.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Homepage&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/images/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://chat.deepseek.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Chat&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%96%2520Chat-DeepSeek%2520LLM-536af5?color=536af5&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/deepseek-ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Hugging Face&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/Tc7c45Zzu5&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/images/qr.jpeg&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Wechat&#34; src=&#34;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/deepseek_ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/LICENSE-CODE&#34;&gt; &lt;img alt=&#34;Code License&#34; src=&#34;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/LICENSE-MODEL&#34;&gt; &lt;img alt=&#34;Model License&#34; src=&#34;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/#2-model-downloads&#34;&gt;Model Download&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/#5-quick-start&#34;&gt;Quick Start&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/#3-evaluation-results&#34;&gt;Evaluation Results&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/#8-license&#34;&gt;License&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/#9-citation&#34;&gt;Citation&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.02954&#34;&gt;&lt;b&gt;Paper Link&lt;/b&gt;üëÅÔ∏è&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Introducing DeepSeek LLM, an advanced language model comprising 67 billion parameters. It has been trained from scratch on a vast dataset of 2 trillion tokens in both English and Chinese. In order to foster research, we have made DeepSeek LLM 7B/67B Base and DeepSeek LLM 7B/67B Chat open source for the research community.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/images/llm_radar.png&#34; alt=&#34;result&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Superior General Capabilities:&lt;/strong&gt; DeepSeek LLM 67B Base outperforms Llama2 70B Base in areas such as reasoning, coding, math, and Chinese comprehension.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Proficient in Coding and Math:&lt;/strong&gt; DeepSeek LLM 67B Chat exhibits outstanding performance in coding (HumanEval Pass@1: 73.78) and mathematics (GSM8K 0-shot: 84.1, Math 0-shot: 32.6). It also demonstrates remarkable generalization abilities, as evidenced by its exceptional score of 65 on the Hungarian National High School Exam.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Mastery in Chinese Language:&lt;/strong&gt; Based on our evaluation, DeepSeek LLM 67B Chat surpasses GPT-3.5 in Chinese.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;2. Model Downloads&lt;/h2&gt; &#xA;&lt;p&gt;We release the DeepSeek LLM 7B/67B, including both base and chat models, to the public. To support a broader and more diverse range of research within both academic and commercial communities, we are providing access to the intermediate checkpoints of the base model from its training process. Please &lt;strong&gt;note&lt;/strong&gt; that the use of this model is subject to the terms outlined in &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/#8-license&#34;&gt;License section&lt;/a&gt;. Commercial usage is permitted under these terms.&lt;/p&gt; &#xA;&lt;h3&gt;Huggingface&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sequence Length&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepSeek LLM 7B Base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4096&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ü§ó &lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-llm-7b-base&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepSeek LLM 7B Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4096&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ü§ó &lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepSeek LLM 67B Base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4096&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ü§ó &lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-llm-67b-base&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepSeek LLM 67B Chat&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4096&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ü§ó &lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Intermediate Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;We host the intermediate checkpoints of DeepSeek LLM 7B/67B on AWS S3 (Simple Storage Service). These files can be downloaded using the AWS Command Line Interface (CLI).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# using AWS CLI&#xA;&#xA;# DeepSeek-LLM-7B-Base&#xA;aws s3 cp s3://deepseek-ai/DeepSeek-LLM/DeepSeek-LLM-7B-Base &amp;lt;local_path&amp;gt; --recursive --request-payer&#xA;&#xA;# DeepSeek-LLM-67B-Base&#xA;aws s3 cp s3://deepseek-ai/DeepSeek-LLM/DeepSeek-LLM-67B-Base &amp;lt;local_path&amp;gt; --recursive  --request-payer&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Evaluation Results&lt;/h2&gt; &#xA;&lt;h3&gt;Base Model&lt;/h3&gt; &#xA;&lt;p&gt;We evaluate our models and some baseline models on a series of representative benchmarks, both in English and Chinese. More results can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/evaluation&#34;&gt;&lt;code&gt;evaluation&lt;/code&gt;&lt;/a&gt; folder. In this part, the evaluation results we report are based on the internal, non-open-source hai-llm evaluation framework. Please note that there may be slight discrepancies when using the converted HuggingFace models.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Hella&lt;br&gt;Swag&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Trivia&lt;br&gt;QA&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GSM8K&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Human&lt;br&gt;Eval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;BBH&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CEval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CMMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Chinese&lt;br&gt;QA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5-shot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA-2&lt;br&gt;-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;14.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA-2&lt;br&gt;-70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepSeek LLM&lt;br&gt;7B Base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DeepSeek LLM&lt;br&gt;67B Base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;84.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;71.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;66.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; ChineseQA is an in-house benchmark, inspired by TriviaQA.&lt;/p&gt; &#xA;&lt;h3&gt;Chat Model&lt;/h3&gt; &#xA;&lt;h4&gt;Never Seen Before Exam&lt;/h4&gt; &#xA;&lt;p&gt;To address data contamination and tuning for specific testsets, we have designed fresh problem sets to assess the capabilities of open-source LLM models. &lt;strong&gt;The evaluation results indicate that DeepSeek LLM 67B Chat performs exceptionally well on never-before-seen exams.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hungarian National High-School Exam:&lt;/strong&gt; In line with Grok-1, we have evaluated the model&#39;s mathematical capabilities using the &lt;a href=&#34;https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam&#34;&gt;Hungarian National High School Exam&lt;/a&gt;. This exam comprises 33 problems, and the model&#39;s scores are determined through human annotation. We follow the scoring metric in the &lt;a href=&#34;https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam/blob/main/test/solutions.pdf&#34;&gt;solution.pdf&lt;/a&gt; to evaluate all models.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/images/mathexam.png&#34; alt=&#34;result&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Remark:&lt;/strong&gt; We have rectified an error from our initial evaluation. In this revised version, we have omitted the lowest scores for questions 16, 17, 18, as well as for the aforementioned image. Evaluation details are &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/tree/HEAD/evaluation/hungarian_national_hs_solutions&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Instruction Following Evaluation:&lt;/strong&gt; On Nov 15th, 2023, Google released an &lt;a href=&#34;https://arxiv.org/pdf/2311.07911.pdf&#34;&gt;instruction following evaluation dataset&lt;/a&gt;. They identified 25 types of verifiable instructions and constructed around 500 prompts, with each prompt containing one or more verifiable instructions. We use the prompt-level loose metric to evaluate all models. Here, we used the first version released by Google for the evaluation. For the Google revised test set evaluation results, please refer to the number in our paper.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/images/if_eval.png&#34; alt=&#34;result&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;LeetCode Weekly Contest:&lt;/strong&gt; To assess the coding proficiency of the model, we have utilized problems from the &lt;a href=&#34;https://leetcode.com/contest/&#34;&gt;LeetCode Weekly Contest&lt;/a&gt; (Weekly Contest 351-372, Bi-Weekly Contest 108-117, from July 2023 to Nov 2023). We have obtained these problems by crawling data from LeetCode, which consists of 126 problems with over 20 test cases for each. The evaluation metric employed is akin to that of HumanEval. In this regard, if a model&#39;s outputs successfully pass all test cases, the model is considered to have effectively solved the problem. The model&#39;s coding capabilities are depicted in the Figure below, where the y-axis represents the pass@1 score on in-domain human evaluation testing, and the x-axis represents the pass@1 score on out-domain LeetCode Weekly Contest problems.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/images/leetcode.png&#34; alt=&#34;result&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;The specific questions and test cases will be released soon. Stay tuned!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Standard Benchmark&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;TriviaQA&lt;/th&gt; &#xA;   &lt;th&gt;MMLU&lt;/th&gt; &#xA;   &lt;th&gt;GSM8K&lt;/th&gt; &#xA;   &lt;th&gt;HumanEval&lt;/th&gt; &#xA;   &lt;th&gt;BBH&lt;/th&gt; &#xA;   &lt;th&gt;C-Eval&lt;/th&gt; &#xA;   &lt;th&gt;CMMLU&lt;/th&gt; &#xA;   &lt;th&gt;ChineseQA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek LLM 7B Base&lt;/td&gt; &#xA;   &lt;td&gt;59.7&lt;/td&gt; &#xA;   &lt;td&gt;48.2&lt;/td&gt; &#xA;   &lt;td&gt;17.4&lt;/td&gt; &#xA;   &lt;td&gt;26.2&lt;/td&gt; &#xA;   &lt;td&gt;39.5&lt;/td&gt; &#xA;   &lt;td&gt;45.0&lt;/td&gt; &#xA;   &lt;td&gt;47.2&lt;/td&gt; &#xA;   &lt;td&gt;78.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek LLM 67B Base&lt;/td&gt; &#xA;   &lt;td&gt;78.9&lt;/td&gt; &#xA;   &lt;td&gt;71.3&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;42.7&lt;/td&gt; &#xA;   &lt;td&gt;68.7&lt;/td&gt; &#xA;   &lt;td&gt;66.1&lt;/td&gt; &#xA;   &lt;td&gt;70.8&lt;/td&gt; &#xA;   &lt;td&gt;87.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek LLM 7B Chat&lt;/td&gt; &#xA;   &lt;td&gt;57.9&lt;/td&gt; &#xA;   &lt;td&gt;49.4&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;48.2&lt;/td&gt; &#xA;   &lt;td&gt;42.3&lt;/td&gt; &#xA;   &lt;td&gt;47.0&lt;/td&gt; &#xA;   &lt;td&gt;49.7&lt;/td&gt; &#xA;   &lt;td&gt;75.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek LLM 67B Chat&lt;/td&gt; &#xA;   &lt;td&gt;81.5&lt;/td&gt; &#xA;   &lt;td&gt;71.1&lt;/td&gt; &#xA;   &lt;td&gt;84.1&lt;/td&gt; &#xA;   &lt;td&gt;73.8&lt;/td&gt; &#xA;   &lt;td&gt;71.7&lt;/td&gt; &#xA;   &lt;td&gt;65.2&lt;/td&gt; &#xA;   &lt;td&gt;67.8&lt;/td&gt; &#xA;   &lt;td&gt;85.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We evaluate chat models with 0-shot for MMLU, GSM8K, C-Eval, and CMMLU. More evaluation results can be found &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/evaluation/more_results.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Revisit Multi-Choice Question Benchmarks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Based on our experimental observations, we have discovered that enhancing benchmark performance using multi-choice (MC) questions, such as MMLU, CMMLU, and C-Eval, is a relatively straightforward task. By incorporating multi-choice questions from Chinese exams, we have achieved exceptional results, as depicted in the table below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;MMLU&lt;/th&gt; &#xA;   &lt;th&gt;C-Eval&lt;/th&gt; &#xA;   &lt;th&gt;CMMLU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek LLM 7B Chat&lt;/td&gt; &#xA;   &lt;td&gt;49.4&lt;/td&gt; &#xA;   &lt;td&gt;47.0&lt;/td&gt; &#xA;   &lt;td&gt;49.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek LLM 7B Chat + MC&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;   &lt;td&gt;71.3&lt;/td&gt; &#xA;   &lt;td&gt;73.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; +MC represents the addition of 20 million Chinese multiple-choice questions collected from the web. It is important to note that we conducted deduplication for the C-Eval validation set and CMMLU test set to prevent data contamination. This addition not only improves Chinese multiple-choice benchmarks but also enhances English benchmarks. However, we observed that it does not enhance the model&#39;s knowledge performance on other evaluations that do not utilize the multiple-choice style in the 7B setting. As a result, &lt;strong&gt;we made the decision to not incorporate MC data in the pre-training or fine-tuning process&lt;/strong&gt;, as it would lead to overfitting on benchmarks.&lt;/p&gt; &#xA;&lt;h2&gt;4. Pre-Training Details&lt;/h2&gt; &#xA;&lt;h3&gt;Data&lt;/h3&gt; &#xA;&lt;p&gt;Our primary goal is to holistically enhance the dataset&#39;s richness and variety. To achieve this, we&#39;ve implemented multiple methods and established a distributed, frequent-checkpointing batch processing system, named &#34;cc_cleaner&#34;, to bolster our data pipeline.&lt;/p&gt; &#xA;&lt;p&gt;Our minimal viable solution departs from RefinedWeb + CCNet. We greatly appreciate their selfless dedication to the research of AGI.&lt;/p&gt; &#xA;&lt;p&gt;We have also significantly incorporated deterministic randomization into our data pipeline. This approach enables us to continuously enhance our data throughout the lengthy and unpredictable training process.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Data Composition&lt;/strong&gt;: Our training data comprises a diverse mix of Internet text, math, code, books, and self-collected data respecting robots.txt. In addition to the diverse content, we place a high priority on personal privacy and copyright protection. All content containing personal information or subject to copyright restrictions has been removed from our dataset.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dataset Pruning&lt;/strong&gt;: Our system employs heuristic rules and models to refine our training data. Our filtering process removes low-quality web data while preserving precious low-resource knowledge. It aims to improve overall corpus quality and remove harmful or toxic content.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Deduplication&lt;/strong&gt;: Our advanced deduplication system, using MinhashLSH, strictly removes duplicates both at document and string levels. This rigorous deduplication process ensures exceptional data uniqueness and integrity, especially crucial in large-scale datasets.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pre-Training&lt;/h3&gt; &#xA;&lt;p&gt;DeepSeek LM models use the same architecture as LLaMA, an auto-regressive transformer decoder model. The 7B model uses Multi-Head attention (MHA) while the 67B model uses Grouped-Query Attention (GQA).&lt;/p&gt; &#xA;&lt;p&gt;We pre-trained DeepSeek language models on a vast dataset of 2 trillion tokens, with a sequence length of 4096 and AdamW optimizer. The 7B model&#39;s training involved a batch size of 2304 and a learning rate of 4.2e-4 and the 67B model was trained with a batch size of 4608 and a learning rate of 3.2e-4. We employ a multi-step learning rate schedule in our training process. The learning rate begins with 2000 warmup steps, and then it is stepped to 31.6% of the maximum at 1.6 trillion tokens and 10% of the maximum at 1.8 trillion tokens.&lt;/p&gt; &#xA;&lt;p&gt;We release the training loss curve and several benchmark metrics curves, as detailed below.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/images/pretrain_loss.png&#34; alt=&#34;result&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-LLM/main/images/pretrain_metric.png&#34; alt=&#34;result&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;5. Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference with Huggingface&#39;s Transformers&lt;/h3&gt; &#xA;&lt;p&gt;You can directly employ &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface&#39;s Transformers&lt;/a&gt; for model inference.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text Completion&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig&#xA;&#xA;model_name = &#34;deepseek-ai/deepseek-llm-67b-base&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=&#34;auto&#34;)&#xA;model.generation_config = GenerationConfig.from_pretrained(model_name)&#xA;model.generation_config.pad_token_id = model.generation_config.eos_token_id&#xA;&#xA;text = &#34;An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is&#34;&#xA;inputs = tokenizer(text, return_tensors=&#34;pt&#34;)&#xA;outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)&#xA;&#xA;result = tokenizer.decode(outputs[0], skip_special_tokens=True)&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Chat Completion&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig&#xA;&#xA;model_name = &#34;deepseek-ai/deepseek-llm-67b-chat&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=&#34;auto&#34;)&#xA;model.generation_config = GenerationConfig.from_pretrained(model_name)&#xA;model.generation_config.pad_token_id = model.generation_config.eos_token_id&#xA;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Who are you?&#34;}&#xA;]&#xA;input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=&#34;pt&#34;)&#xA;outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)&#xA;&#xA;result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Avoiding the use of the provided function &lt;code&gt;apply_chat_template&lt;/code&gt;, you can also interact with our model following the sample template. Note that &lt;code&gt;messages&lt;/code&gt; should be replaced by your input.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;User: {messages[0][&#39;content&#39;]}&#xA;&#xA;Assistant: {messages[1][&#39;content&#39;]}&amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt;User: {messages[2][&#39;content&#39;]}&#xA;&#xA;Assistant:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; By default (&lt;code&gt;add_special_tokens=True&lt;/code&gt;), our tokenizer automatically adds a &lt;code&gt;bos_token&lt;/code&gt; (&lt;code&gt;&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;&lt;/code&gt;) before the input text. Additionally, since the system prompt is not compatible with this version of our models, we DO NOT RECOMMEND including the system prompt in your input.&lt;/p&gt; &#xA;&lt;h3&gt;Inference with vLLM&lt;/h3&gt; &#xA;&lt;p&gt;You can also employ &lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt; for high-throughput inference.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text Completion&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from vllm import LLM, SamplingParams&#xA;&#xA;tp_size = 4 # Tensor Parallelism&#xA;sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=100)&#xA;model_name = &#34;deepseek-ai/deepseek-llm-67b-base&#34;&#xA;llm = LLM(model=model_name, trust_remote_code=True, gpu_memory_utilization=0.9, tensor_parallel_size=tp_size)&#xA;&#xA;prompts = [&#xA;    &#34;If everyone in a country loves one another,&#34;,&#xA;    &#34;The research should also focus on the technologies&#34;,&#xA;    &#34;To determine if the label is correct, we need to&#34;&#xA;]&#xA;outputs = llm.generate(prompts, sampling_params)&#xA;&#xA;generated_text = [output.outputs[0].text for output in outputs]&#xA;print(generated_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Chat Completion&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer&#xA;from vllm import LLM, SamplingParams&#xA;&#xA;tp_size = 4 # Tensor Parallelism&#xA;sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=100)&#xA;model_name = &#34;deepseek-ai/deepseek-llm-67b-chat&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;llm = LLM(model=model_name, trust_remote_code=True, gpu_memory_utilization=0.9, tensor_parallel_size=tp_size)&#xA;&#xA;messages_list = [&#xA;    [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Who are you?&#34;}],&#xA;    [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What can you do?&#34;}],&#xA;    [{&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Explain Transformer briefly.&#34;}],&#xA;]&#xA;# Avoid adding bos_token repeatedly&#xA;prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]&#xA;&#xA;sampling_params.stop = [tokenizer.eos_token]&#xA;outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)&#xA;&#xA;generated_text = [output.outputs[0].text for output in outputs]&#xA;print(generated_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;6. FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Could You Provide the tokenizer.model File for Model Quantization?&lt;/h3&gt; &#xA;&lt;p&gt;DeepSeek LLM utilizes the &lt;a href=&#34;https://huggingface.co/docs/tokenizers/index&#34;&gt;HuggingFace Tokenizer&lt;/a&gt; to implement the Byte-level BPE algorithm, with specially designed pre-tokenizers to ensure optimal performance. Currently, there is no direct way to convert the tokenizer into a SentencePiece tokenizer. We are contributing to the open-source quantization methods facilitate the usage of HuggingFace Tokenizer.&lt;/p&gt; &#xA;&lt;h4&gt;GGUF(llama.cpp)&lt;/h4&gt; &#xA;&lt;p&gt;We have submitted a &lt;a href=&#34;https://github.com/ggerganov/llama.cpp/pull/4070&#34;&gt;PR&lt;/a&gt; to the popular quantization repository &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; to fully support all HuggingFace pre-tokenizers, including ours.&lt;/p&gt; &#xA;&lt;p&gt;While waiting for the PR to be merged, you can generate your GGUF model using the following steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/DOGEwbx/llama.cpp.git&#xA;cd llama.cpp&#xA;git checkout regex_gpt2_preprocess&#xA;# set up the environment according to README&#xA;make&#xA;python3 -m pip install -r requirements.txt&#xA;# generate GGUF model&#xA;python convert-hf-to-gguf.py &amp;lt;MODEL_PATH&amp;gt; --outfile &amp;lt;GGUF_PATH&amp;gt; --model-name deepseekllm&#xA;# use q4_0 quantization as an example&#xA;./quantize &amp;lt;GGUF_PATH&amp;gt; &amp;lt;OUTPUT_PATH&amp;gt; q4_0&#xA;./main -m &amp;lt;OUTPUT_PATH&amp;gt; -n 128 -p &amp;lt;PROMPT&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;GPTQ(exllamav2)&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;UPDATE:&lt;/code&gt;&lt;a href=&#34;https://github.com/turboderp/exllamav2&#34;&gt;exllamav2&lt;/a&gt; has been able to support HuggingFace Tokenizer. Please pull the latest version and try out.&lt;/p&gt; &#xA;&lt;h3&gt;GPU Memory Usage&lt;/h3&gt; &#xA;&lt;p&gt;We profile the peak memory usage of inference for 7B and 67B models at different batch size and sequence length settings.&lt;/p&gt; &#xA;&lt;p&gt;For DeepSeek LLM 7B, we utilize &lt;strong&gt;1 NVIDIA A100-PCIE-40GB GPU&lt;/strong&gt; for inference.&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;thead&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;th rowspan=&#34;2&#34;&gt;Batch Size&lt;/th&gt;&#xA;   &lt;th colspan=&#34;5&#34;&gt;Sequence Length&lt;/th&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;th&gt;256&lt;/th&gt;&#xA;   &lt;th&gt;512&lt;/th&gt;&#xA;   &lt;th&gt;1024&lt;/th&gt;&#xA;   &lt;th&gt;2048&lt;/th&gt;&#xA;   &lt;th&gt;4096&lt;/th&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/thead&gt;&#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;1&lt;/td&gt;&#xA;   &lt;td&gt;13.29 GB&lt;/td&gt;&#xA;   &lt;td&gt;13.63 GB&lt;/td&gt;&#xA;   &lt;td&gt;14.47 GB&lt;/td&gt;&#xA;   &lt;td&gt;16.37 GB&lt;/td&gt;&#xA;   &lt;td&gt;21.25 GB&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;2&lt;/td&gt;&#xA;   &lt;td&gt;13.63 GB&lt;/td&gt;&#xA;   &lt;td&gt;14.39 GB&lt;/td&gt;&#xA;   &lt;td&gt;15.98 GB&lt;/td&gt;&#xA;   &lt;td&gt;19.82 GB&lt;/td&gt;&#xA;   &lt;td&gt;29.59 GB&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;4&lt;/td&gt;&#xA;   &lt;td&gt;14.47 GB&lt;/td&gt;&#xA;   &lt;td&gt;15.82 GB&lt;/td&gt;&#xA;   &lt;td&gt;19.04 GB&lt;/td&gt;&#xA;   &lt;td&gt;26.65 GB&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;8&lt;/td&gt;&#xA;   &lt;td&gt;15.99 GB&lt;/td&gt;&#xA;   &lt;td&gt;18.71 GB&lt;/td&gt;&#xA;   &lt;td&gt;25.14 GB&lt;/td&gt;&#xA;   &lt;td&gt;35.19 GB&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;16&lt;/td&gt;&#xA;   &lt;td&gt;19.06 GB&lt;/td&gt;&#xA;   &lt;td&gt;24.52 GB&lt;/td&gt;&#xA;   &lt;td&gt;37.28 GB&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;For DeepSeek LLM 67B, we utilize &lt;strong&gt;8 NVIDIA A100-PCIE-40GB GPUs&lt;/strong&gt; for inference.&lt;/p&gt; &#xA;&lt;table&gt;&#xA; &lt;thead&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;th rowspan=&#34;2&#34;&gt;Batch Size&lt;/th&gt;&#xA;   &lt;th colspan=&#34;5&#34;&gt;Sequence Length&lt;/th&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;th&gt;256&lt;/th&gt;&#xA;   &lt;th&gt;512&lt;/th&gt;&#xA;   &lt;th&gt;1024&lt;/th&gt;&#xA;   &lt;th&gt;2048&lt;/th&gt;&#xA;   &lt;th&gt;4096&lt;/th&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/thead&gt;&#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;1&lt;/td&gt;&#xA;   &lt;td&gt;16.92 GB&lt;/td&gt;&#xA;   &lt;td&gt;17.11 GB&lt;/td&gt;&#xA;   &lt;td&gt;17.66 GB&lt;/td&gt;&#xA;   &lt;td&gt;20.01 GB&lt;/td&gt;&#xA;   &lt;td&gt;33.23 GB&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;2&lt;/td&gt;&#xA;   &lt;td&gt;17.04 GB&lt;/td&gt;&#xA;   &lt;td&gt;17.28 GB&lt;/td&gt;&#xA;   &lt;td&gt;18.55 GB&lt;/td&gt;&#xA;   &lt;td&gt;25.27 GB&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;4&lt;/td&gt;&#xA;   &lt;td&gt;17.20 GB&lt;/td&gt;&#xA;   &lt;td&gt;17.80 GB&lt;/td&gt;&#xA;   &lt;td&gt;21.28 GB&lt;/td&gt;&#xA;   &lt;td&gt;33.71 GB&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;8&lt;/td&gt;&#xA;   &lt;td&gt;17.59 GB&lt;/td&gt;&#xA;   &lt;td&gt;19.25 GB&lt;/td&gt;&#xA;   &lt;td&gt;25.69 GB&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;16&lt;/td&gt;&#xA;   &lt;td&gt;18.17 GB&lt;/td&gt;&#xA;   &lt;td&gt;21.69 GB&lt;/td&gt;&#xA;   &lt;td&gt;34.54 GB&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;   &lt;td&gt;OOM&lt;/td&gt;&#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;7. Limitation&lt;/h2&gt; &#xA;&lt;p&gt;While DeepSeek LLMs have demonstrated impressive capabilities, they are not without their limitations. Here are some potential drawbacks of such models:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Over-reliance on training data: These models are trained on vast amounts of text data, which can introduce biases present in the data. They may inadvertently generate biased or discriminatory responses, reflecting the biases prevalent in the training data.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Hallucination: The model sometimes generates responses or outputs that may sound plausible but are factually incorrect or unsupported. This can occur when the model relies heavily on the statistical patterns it has learned from the training data, even if those patterns do not align with real-world knowledge or facts.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Repetition: The model may exhibit repetition in their generated responses. This repetition can manifest in various ways, such as repeating certain phrases or sentences, generating redundant information, or producing repetitive structures in the generated text. This issue can make the output of LLMs less diverse and less engaging for users.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;8. License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is licensed under &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-CODE&#34;&gt;the MIT License&lt;/a&gt;. The use of DeepSeek LLM Base/Chat models is subject to &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-MODEL&#34;&gt;the Model License&lt;/a&gt;. DeepSeek LLM series (including Base and Chat) supports commercial use.&lt;/p&gt; &#xA;&lt;h2&gt;9. Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{deepseek-llm,&#xA;  author = {DeepSeek-AI},&#xA;  title = {DeepSeek LLM: Scaling Open-Source Language Models with Longtermism},&#xA;  journal = {arXiv preprint arXiv:2401.02954},&#xA;  year = {2024},&#xA;  url = {https://github.com/deepseek-ai/DeepSeek-LLM}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;10. Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href=&#34;mailto:service@deepseek.com&#34;&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>