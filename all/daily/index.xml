<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-20T01:23:55Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>aishwaryanr/awesome-generative-ai-guide</title>
    <updated>2024-04-20T01:23:55Z</updated>
    <id>tag:github.com,2024-04-20:/aishwaryanr/awesome-generative-ai-guide</id>
    <link href="https://github.com/aishwaryanr/awesome-generative-ai-guide" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A one stop repository for generative AI research updates, interview resources, notebooks and much more!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;span&gt;‚≠ê&lt;/span&gt; &lt;span&gt;üîñ&lt;/span&gt; awesome-generative-ai-guide&lt;/h1&gt; &#xA;&lt;p&gt;Generative AI is experiencing rapid growth, and this repository serves as a comprehensive hub for updates on generative AI research, interview materials, notebooks, and more!&lt;/p&gt; &#xA;&lt;p&gt;Explore the following resources:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide?tab=readme-ov-file#star-best-genai-papers-list-january-2024&#34;&gt;Monthly Best GenAI Papers List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide?tab=readme-ov-file#computer-interview-prep&#34;&gt;GenAI Interview Resources&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide?tab=readme-ov-file#ongoing-applied-llms-mastery-2024&#34;&gt;Applied LLMs Mastery 2024 (created by Aishwarya Naresh Reganti) course material&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide?tab=readme-ov-file#book-list-of-free-genai-courses&#34;&gt;List of all GenAI-related free courses (over 70 listed)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide?tab=readme-ov-file#notebook-code-notebooks&#34;&gt;List of code repositories/notebooks for developing generative AI applications&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We&#39;ll be updating this repository regularly, so keep an eye out for the latest additions!&lt;/p&gt; &#xA;&lt;p&gt;Happy Learning!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;span&gt;üîà&lt;/span&gt; Announcements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Applied LLMs Mastery full course content has been released!!! (&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024&#34;&gt;Click Here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;5-day roadmap to learn LLM foundations out now! (&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/resources/genai_roadmap.md&#34;&gt;Click Here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;60 Common GenAI Interview Questions out now! (&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/interview_prep/60_gen_ai_questions.md&#34;&gt;Click Here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ICLR 2024 paper summaries (&lt;a href=&#34;https://areganti.notion.site/06f0d4fe46a94d62bff2ae001cfec22c?v=d501ca62e4b745768385d698f173ae14&#34;&gt;Click Here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;List of free GenAI courses (&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide#book-list-of-free-genai-courses&#34;&gt;Click Here&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Generative AI resources and roadmaps &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/resources/RAG_roadmap.md&#34;&gt;3-day RAG roadmap&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/resources/genai_roadmap.md&#34;&gt;5-day LLM foundations roadmap&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/resources/agents_roadmap.md&#34;&gt;5-day LLM agents roadmap&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/resources/agents_101_guide.md&#34;&gt;Agents 101 guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚≠ê&lt;/span&gt; Best GenAI Papers List (March 2024)&lt;/h2&gt; &#xA;&lt;p&gt;*Updated at the end of every month&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Summary&lt;/th&gt; &#xA;   &lt;th&gt;Topics&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;29 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.20327&#34;&gt;Gecko: Versatile Text Embeddings Distilled from Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Gecko introduces a novel approach for creating compact and efficient text embeddings by distilling knowledge from large language models into a retriever. Utilizing a two-step distillation process that generates diverse, synthetic paired data, Gecko achieves superior retrieval performance. With a focus on compactness, it outperforms larger models and higher-dimensional embeddings on the Massive Text Embedding Benchmark (MTEB), demonstrating its efficacy and potential in improving information retrieval tasks.&lt;/td&gt; &#xA;   &lt;td&gt;LLM Embeddings&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;28 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://x.ai/blog/grok-1.5&#34;&gt;Grok-1.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Grok 1.5 offers enhanced reasoning capabilities and a context length of 128,000 tokens. It showcases significant advancements in coding, math-related tasks, and long context understanding. With improvements in MATH, GSM8K, and HumanEval benchmarks, Grok-1.5 offers expanded memory capacity and exceptional retrieval capabilities. Built on a custom distributed training framework, it promises efficiency and reliability for large-scale language model research&lt;/td&gt; &#xA;   &lt;td&gt;Foundational LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;28 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.19270&#34;&gt;Don&#39;t Use Your Data All at Once: sDPO&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;sDPO introduces a novel method in the realm of language model training, focusing on the strategic use of preference datasets in a stepwise manner. This technique enhances model alignment with human preferences by employing parts of the dataset progressively, leading to more precise reference models and outperforming other popular LLMs in terms of performance, even those with more parameters.&lt;/td&gt; &#xA;   &lt;td&gt;Instruction Tuning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;28 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ai21.com/blog/announcing-jamba&#34;&gt;Jamba: AI21&#39;s SSM-Transformer Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AI21 labs announced Jamba novel SSM-Transformer model offering a 256K context window, aiming to balance the SSM model&#39;s efficiency with the Transformer&#39;s capability. It shows significant performance improvements across various benchmarks. Jamba is open-source under Apache 2.0, available on Hugging Face, and soon on NVIDIA&#39;s API catalog, marking a significant advancement in hybrid model architecture‚Äã&lt;/td&gt; &#xA;   &lt;td&gt;Foundational LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;28 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.19154&#34;&gt;STaR-GATE: Teaching Language Models to Ask Clarifying Questions&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper presents STaR-GATE, a novel approach for enhancing language models&#39; interaction skills by training them to ask clarifying questions. By employing a strategic teacher-student learning framework, STaR-GATE aims to improve the models&#39; ability to clarify ambiguities in user queries, thereby enhancing communication effectiveness and accuracy in understanding and responding to complex requests&lt;/td&gt; &#xA;   &lt;td&gt;Prompt Engineering&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;27 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.18802v1&#34;&gt;Long-form factuality in large language models &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper tackles the challenge of factuality in LLM-generated content on open-ended topics. It introduces LongFact, a set of prompts for evaluating long-form factuality, and proposes the Search-Augmented Factuality Evaluator (SAFE) method. SAFE assesses the accuracy of facts in LLM responses through a multi-step reasoning process, comparing supported facts against Google Search results. The findings indicate LLMs&#39; potential for superhuman factuality assessment, offering a cost-effective alternative to human annotation.&lt;/td&gt; &#xA;   &lt;td&gt;LLM Factuality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;27 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.18814v1&#34;&gt;Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mini-Gemini presents a framework to enhance multi-modal Vision Language Models (VLMs) by improving visual tokens, constructing high-quality datasets, and guiding VLM-based generation for better performance. It uses an additional visual encoder for high-resolution refinement without increasing visual token count, aiming to enhance image understanding, reasoning, and simultaneous generation capabilities of VLMs. Mini-Gemini has shown leading performance in zero-shot benchmarks, surpassing developed private models.&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;27 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm&#34;&gt;DBRX&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A state-of-the-art open large language model surpassing established models like GPT-3.5 and competing with Gemini 1.0 Pro. DBRX excels in programming and general LLM capabilities, featuring a fine-grained mixture-of-experts architecture for enhanced training and inference efficiency. It&#39;s 40% the size of Grok-1, offering faster inference and reduced compute requirements. The model is available on Hugging Face, emphasizing Databricks&#39; commitment to open models and enabling customers to pretrain DBRX-class models with their infrastructure&lt;/td&gt; &#xA;   &lt;td&gt;Foundational LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;25 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.16971&#34;&gt;AIOS: LLM Agent Operating System &lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AIOS is designed as an LLM agent operating system to optimize resource allocation, enable concurrent execution, and provide access control. It embeds LLMs into operating systems, presenting an &#34;OS with soul&#34; toward AGI. The system improves the performance and efficiency of LLM agents, offering a pioneering platform for the AIOS ecosystem development.&lt;/td&gt; &#xA;   &lt;td&gt;Agents&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;22 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.12373&#34;&gt;RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The paper introduces RankPrompt, a novel prompting method aimed at improving the reasoning capabilities of Large Language Models like ChatGPT and GPT-4. Unlike existing solutions requiring human annotations or failing in inconsistent scenarios, RankPrompt enables LLMs to self-rank their responses by comparing diverse outputs. Experiments across 11 reasoning tasks demonstrate significant performance enhancements, with up to a 13% improvement. Moreover, RankPrompt aligns with human judgments 74% of the time in open-ended evaluations and exhibits robustness to response variations. This method proves effective in eliciting high-quality feedback from LLMs, offering promising avenues for advancing reasoning abilities.&lt;/td&gt; &#xA;   &lt;td&gt;Prompt Engineering&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;22 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.13248&#34;&gt;Mora: Enabling Generalist Video Generation via A Multi-Agent Framework&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mora proposes a new multi-agent framework to address the gap in generalist video generation capabilities, aiming to match the performance of the pioneering model Sora. It leverages multiple visual AI agents to achieve text-to-video generation, image-to-video conversion, video extension, editing, connection, and digital world simulation, demonstrating close performance to Sora across various tasks but with a noticeable gap when assessed holistically.&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;22 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.15042&#34;&gt;LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This study introduces LLM2LLM, a data enhancement strategy utilizing a teacher-student LLM framework for improving performance in tasks with limited data. It involves fine-tuning a student LLM on initial seed data, identifying errors, and generating new data based on these errors using a teacher LLM. This iterative process significantly boosts LLM performance in low-data regimes across various datasets, demonstrating substantial improvements over traditional fine-tuning and other augmentation methods.&lt;/td&gt; &#xA;   &lt;td&gt;Data Augmentation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;21 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.14403.pdf&#34;&gt;Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The paper introduces a novel adaptive QA framework. It dynamically selects the most appropriate strategy for handling queries of varying complexities, from simple to sophisticated, by integrating retrieval-augmented LLMs with a complexity-level classifier. This approach aims to balance efficiency and accuracy in response generation across different query types, showing improvements over existing models and adaptive retrieval methods&lt;/td&gt; &#xA;   &lt;td&gt;RAG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;20 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.13793&#34;&gt;Evaluating Frontier Models for Dangerous Capabilities&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper pioneers &#34;dangerous capability&#34; evaluations, focusing on areas like persuasion, cyber-security, self-proliferation, and self-reasoning, using Gemini 1.0 models. While no strong dangerous capabilities were found, early warning signs were identified. The study aims to advance the science of evaluating such capabilities in AI models, preparing for future advancements.&lt;/td&gt; &#xA;   &lt;td&gt;LLM Attacks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;19 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.13187&#34;&gt;Evolutionary Optimization of Model Merging Recipes&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper presents an new approach for automating the creation of powerful foundation models by merging diverse open-source models. It optimizes beyond individual model weights, facilitating cross-domain merging and achieving state-of-the-art performance, notably in Japanese language tasks. This approach introduces a new paradigm for automated model composition, offering efficient alternatives for foundation model development.&lt;/td&gt; &#xA;   &lt;td&gt;Model Merging&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;19 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.12881v1&#34;&gt;Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper addresses the challenge of integrating agent abilities into Large Language Models for improved performance in NLP tasks. It identifies key observations regarding the entanglement of agent training data, varying learning speeds of LLMs, and side-effects of existing approaches. Introducing Agent-FLAN, a method for Fine-tuning LANguage models for Agents, the paper proposes a novel approach to address these challenges. By carefully redesigning the training corpus and incorporating negative samples, Agent-FLAN enables significant performance improvements, outperforming prior works by 3.5% across multiple evaluation datasets. Moreover, it mitigates hallucination issues and enhances LLMs&#39; agent capabilities, even with scaled model sizes, while slightly improving their general capability.&lt;/td&gt; &#xA;   &lt;td&gt;Agents, Hallucination&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;18 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://zorazrw.github.io/files/WhatAreToolsAnyway.pdf&#34;&gt;What Are Tools Anyway? A Survey from the Language Model Perspective&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper dives into the role of tools in enhancing the performance of language models for text generation tasks. It addresses the ambiguity surrounding the term &#34;tool&#34; and explores how tools aid LMs. Through a systematic review, the paper defines tools as external programs utilized by LMs and examines different tooling scenarios and approaches. Empirical studies assess the efficiency of various tooling methods by measuring compute requirements and performance gains across benchmarks. The survey also identifies challenges and potential avenues for future research in LM tooling.&lt;/td&gt; &#xA;   &lt;td&gt;Agents, Tools, Survey&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;17 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://x.ai/blog/grok/model-card&#34;&gt;Grok-1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Grok-1 is an autoregressive Transformer-based model designed for next-token prediction, fine-tuned with feedback from Grok-0 models and humans. Released in November 2023, it boasts a context length of 8,192 tokens and is geared towards various NLP tasks like question answering and coding assistance. However, while Grok-1 excels in information processing, human review is essential to ensure accuracy as it lacks independent web-search capabilities. Despite access to external sources, the model may still hallucinate. Trained on data up to Q3 2023 from the internet and AI Tutors, its performance was evaluated on reasoning tasks and foreign math questions, with ongoing testing involving early adopters for further refinement.&lt;/td&gt; &#xA;   &lt;td&gt;Foundational LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;15 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.10131&#34;&gt;RAFT: Adapting Language Model to Domain Specific RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper introduces Retrieval Augmented FineTuning (RAFT), a training approach aimed at enhancing the ability of Large Language Models to answer questions in domain-specific settings. RAFT leverages retrieval augmented fine-tuning to enable the model to effectively incorporate new knowledge into its reasoning process. By training the model to disregard irrelevant documents (distractor documents) and cite relevant sequences from retrieved documents, RAFT improves the model&#39;s ability to provide accurate and coherent responses. Experimental results across various datasets demonstrate the effectiveness of RAFT in domain-specific Retrieval Augmented Generation, offering a valuable post-training recipe for enhancing pre-trained LLMs in domain-specific contexts.&lt;/td&gt; &#xA;   &lt;td&gt;RAG, Fine-Tuning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.09539&#34;&gt;Logits of API-Protected LLMs Leak Proprietary Information&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper reveals that even with restricted API access to proprietary Large Language Models, significant proprietary information can be inferred from a small number of API queries. By exploiting a softmax bottleneck present in most modern LLMs, the research demonstrates the ability to unveil hidden aspects of the model architecture and obtain full-vocabulary outputs. This includes efficiently discovering hidden model sizes, identifying different model updates, and estimating output layer parameters. Empirical investigations on OpenAI&#39;s gpt-3.5-turbo reveal its embedding size to be approximately 4,096. The paper concludes by discussing potential measures for LLM providers to mitigate such attacks and suggests viewing these capabilities as opportunities for enhanced transparency and accountability rather than vulnerabilities.&lt;/td&gt; &#xA;   &lt;td&gt;LLM Attacks, Privacy&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.09629&#34;&gt;Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper introduces Quiet-STaR, a method aimed at enabling language models to learn to generate rationales to explain future text, thereby improving their predictive abilities. Building upon the Self-Taught Reasoner (STaR) framework, Quiet-STaR allows LMs to infer unstated rationales in arbitrary text. Key challenges addressed include computational costs, LM&#39;s initial unfamiliarity with generating internal thoughts, and predicting beyond individual tokens. The proposed method involves tokenwise parallel sampling, learnable tokens for indicating thought boundaries, and extended teacher-forcing techniques. Quiet-STaR leads to significant improvements in LM performance on tasks like GSM8K and CommonsenseQA without requiring fine-tuning, marking a step towards more general and scalable reasoning capabilities in LMs.&lt;/td&gt; &#xA;   &lt;td&gt;Prompt Engineering&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.09611&#34;&gt;MM1: Methods, Analysis &amp;amp; Insights from Multimodal LLM Pre-training&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper explores the development of high-performing Multimodal Large Language Models (MLLMs) and investigates the significance of various architecture components and data choices. Through meticulous ablations of the image encoder, vision language connector, and pre-training data options, several crucial design insights are uncovered. For instance, the careful integration of image-caption, interleaved image-text, and text-only data is shown to be essential for achieving state-of-the-art few-shot results across multiple benchmarks. Additionally, the impact of image resolution and token count in the image encoder is highlighted, while the vision-language connector design is found to be comparatively less critical. Scaling up the proposed approach results in MM1, a family of multimodal models with up to 30B parameters, including dense models and mixture-of-experts variants. MM1 achieves state-of-the-art pre-training metrics and competitive performance on various multimodal benchmarks, benefiting from enhanced in-context learning and multi-image reasoning capabilities enabled by large-scale pre-training.&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;13 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.08319&#34;&gt;Knowledge Conflicts for LLMs: A Survey&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This survey dives into the intricacies of knowledge conflicts encountered by large language models, focusing on the blending of contextual and parametric knowledge. It identifies three main categories of conflicts: context-memory, inter-context, and intra-memory conflicts, which can significantly impact LLM trustworthiness and performance, particularly in real-world scenarios with noise and misinformation. Through categorization, exploration of causes, observation of LLM behaviors, and review of existing solutions, the survey aims to provide insights into strategies for enhancing LLM robustness, serving as a valuable resource for advancing research in this domain.&lt;/td&gt; &#xA;   &lt;td&gt;LLM Robustness&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07508&#34;&gt;MoAI: Mixture of All Intelligence for Large Language and Vision Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MoAI introduces an innovative approach to combine the strengths of large language and vision models with specialized computer vision models for tasks like segmentation and OCR. By leveraging auxiliary visual information and blending it with language features through a unique modular design, MoAI achieves superior performance in various zero-shot visual language tasks, particularly in real-world scene understanding, without increasing model size or requiring additional visual instruction datasets.&lt;/td&gt; &#xA;   &lt;td&gt;Multimodal LLMs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;12 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.07816&#34;&gt;Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper explores methods for efficiently training Large Language Models to excel in multiple specialized domains such as coding, math reasoning, and world knowledge. Introducing Branch-Train-MiX (BTX), the approach starts with a seed model and branches to train experts in parallel, reducing communication costs. After training, BTX combines the experts&#39; feedforward parameters into Mixture-of-Expert (MoE) layers, followed by an MoE-finetuning stage to learn token-level routing. BTX encompasses two special cases: Branch-Train-Merge, which lacks the MoE finetuning stage, and sparse upcycling, which skips asynchronous training. Results demonstrate that BTX offers the best accuracy-efficiency tradeoff compared to alternative methods.&lt;/td&gt; &#xA;   &lt;td&gt;MoEs, Foundational LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;11 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.06634&#34;&gt;Stealing Part of a Production Language Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper presents the first model-stealing attack capable of extracting precise information from black-box production language models like OpenAI&#39;s ChatGPT or Google&#39;s PaLM-2. By leveraging typical API access, the attack can recover the embedding projection layer of a transformer model, including symmetries. Remarkably, the attack achieves this for under $20 USD, revealing hidden dimensions of 1024 and 2048 for OpenAI&#39;s Ada and Babbage models, respectively. Additionally, the exact hidden dimension size of the gpt-3.5-turbo model is recovered, with an estimated cost of under $2,000 in queries to retrieve the entire projection matrix. The paper concludes with discussions on potential defenses and mitigations, as well as implications for future work that could extend the attack.&lt;/td&gt; &#xA;   &lt;td&gt;LLM Attacks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;8 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aishwaryanr/awesome-generative-ai-guide/main/nan&#34;&gt;RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in Long-Horizon Generation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper introduces Retrieval Augmented Thoughts (RAT), a method aimed at enhancing large language models&#39; reasoning and generation abilities in long-horizon generation tasks while reducing hallucination. RAT iteratively revises a chain of thoughts by incorporating relevant retrieved information at each step. Applied to GPT-3.5, GPT-4, and CodeLLaMA-7b, RAT significantly improves performance across various tasks, with average rating score increases of 13.63% in code generation, 16.96% in mathematical reasoning, 19.2% in creative writing, and 42.78% in embodied task planning.&lt;/td&gt; &#xA;   &lt;td&gt;RAG, Prompt Engineering&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.05313&#34;&gt;Common 7B Language Models Already Possess Strong Math Capabilities&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This research reveals that smaller, 7B-sized language models, specifically LLaMA-2, already exhibit strong mathematical abilities, challenging previous assumptions that such capabilities require very large models or extensive math-focused pre-training. By leveraging synthetic data and scaling strategies, the study significantly improves the model&#39;s math-solving accuracy, surpassing previous benchmarks and demonstrating that with appropriate training, even relatively small models can achieve remarkable math performance.&lt;/td&gt; &#xA;   &lt;td&gt;Domain Specific LLMs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.03853&#34;&gt;ShortGPT: Layers in Large Language Models are More Redundant Than You Expect&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper introduces ShortGPT, which demonstrates a high degree of redundancy across the layers of large language models. By evaluating the necessity of each layer through a metric called Block Influence (BI), the authors propose a straightforward pruning method. Their approach, which simplifies the model by removing redundant layers, shows significant improvements in efficiency without compromising on the model&#39;s performance, marking a step forward in optimizing LLM architectures.&lt;/td&gt; &#xA;   &lt;td&gt;Smaller LLMs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.04121&#34;&gt;Can Large Language Models Reason and Plan?&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper questions the ability of large language models to perform self-critique and correct their erroneous guesses, a capability humans occasionally demonstrate. This inquiry underscores the distinct nature of human cognitive processes compared to the computational mechanisms of LLMs, challenging the assumption of equivalent reasoning and self-correction abilities between the two.&lt;/td&gt; &#xA;   &lt;td&gt;Prompt Engineering&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.03507&#34;&gt;GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This paper proposes a novel training strategy called GaLore. This approach aims to reduce the memory requirements of training large language models by implementing gradient low-rank projection, significantly cutting down the memory used by optimizer states without sacrificing performance. It allows for the efficient training of large models on consumer-grade GPUs, marking a significant advancement in the accessibility of AI model training.&lt;/td&gt; &#xA;   &lt;td&gt;Memory Optimization&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.03101&#34;&gt;KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The work introduces KnowAgent, a novel approach designed to enhance large language models&#39; planning capabilities by incorporating explicit action knowledge. This integration aims to address the inadequacies in current models that lack built-in action knowledge, leading to planning hallucination. KnowAgent uses an action knowledge base and a self-learning strategy to guide planning trajectories, resulting in more accurate and efficient problem-solving across various domains.&lt;/td&gt; &#xA;   &lt;td&gt;Agents&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4 March 2024&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/aishwaryanr/awesome-generative-ai-guide/main/nan&#34;&gt;The Claude 3 Model Family: Opus, Sonnet, Haiku&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This technical report from Claude introduces Claude 3, a new family of large multimodal models designed to address various needs within the AI landscape. Claude 3 comprises three distinct offerings: Opus, Sonnet, and Haiku, each tailored to different requirements in terms of capability, speed, and cost-effectiveness. All models feature vision capabilities for image data processing. Across benchmark evaluations, the Claude 3 family demonstrates robust performance, setting new standards in reasoning, math, and coding tasks. Claude 3 Opus achieves state-of-the-art results on several evaluations, while Haiku performs comparably to Claude 2 on text-based tasks, and Sonnet and Opus significantly surpass it. Moreover, these models exhibit enhanced fluency in non-English languages, enhancing their versatility for a global audience. The report also includes an in-depth analysis of evaluations, focusing on core capabilities, safety considerations, societal impacts, and adherence to Responsible Scaling Policy.&lt;/td&gt; &#xA;   &lt;td&gt;Foundational LLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;span&gt;üéì&lt;/span&gt; Courses&lt;/h2&gt; &#xA;&lt;h4&gt;[Ongoing] Applied LLMs Mastery 2024&lt;/h4&gt; &#xA;&lt;p&gt;Join 1000+ students on this 10-week adventure as we delve into the application of LLMs across a variety of use cases&lt;/p&gt; &#xA;&lt;h4&gt;&lt;a href=&#34;https://areganti.notion.site/Applied-LLMs-Mastery-2024-562ddaa27791463e9a1286199325045c&#34;&gt;Link&lt;/a&gt; to the course website&lt;/h4&gt; &#xA;&lt;h5&gt;[Feb 2024] Registrations are still open &lt;a href=&#34;https://forms.gle/353sQMRvS951jDYu7&#34;&gt;click here&lt;/a&gt; to register&lt;/h5&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 1 [Jan 15 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week1_part1_foundations.md&#34;&gt;Practical Introduction to LLMs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Applied LLM Foundations&lt;/li&gt; &#xA; &lt;li&gt;Real World LLM Use Cases&lt;/li&gt; &#xA; &lt;li&gt;Domain and Task Adaptation Methods&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 2 [Jan 22 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week2_prompting.md&#34;&gt;Prompting and Prompt Engineering&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basic Prompting Principles&lt;/li&gt; &#xA; &lt;li&gt;Types of Prompting&lt;/li&gt; &#xA; &lt;li&gt;Applications, Risks and Advanced Prompting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 3 [Jan 29 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week3_finetuning_llms.md&#34;&gt;LLM Fine-tuning&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basics of Fine-Tuning&lt;/li&gt; &#xA; &lt;li&gt;Types of Fine-Tuning&lt;/li&gt; &#xA; &lt;li&gt;Fine-Tuning Challenges&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 4 [Feb 5 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week4_RAG.md&#34;&gt;RAG (Retrieval-Augmented Generation)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Understanding the concept of RAG in LLMs&lt;/li&gt; &#xA; &lt;li&gt;Key components of RAG&lt;/li&gt; &#xA; &lt;li&gt;Advanced RAG Methods&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 5 [ Feb 12 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week5_tools_for_LLM_apps.md&#34;&gt;Tools for building LLM Apps&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fine-tuning Tools&lt;/li&gt; &#xA; &lt;li&gt;RAG Tools&lt;/li&gt; &#xA; &lt;li&gt;Tools for observability, prompting, serving, vector search etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 6 [Feb 19 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week6_llm_evaluation.md&#34;&gt;Evaluation Techniques&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Types of Evaluation&lt;/li&gt; &#xA; &lt;li&gt;Common Evaluation Benchmarks&lt;/li&gt; &#xA; &lt;li&gt;Common Metrics&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 7 [Feb 26 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week7_build_llm_app.md&#34;&gt;Building Your Own LLM Application&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Components of LLM application&lt;/li&gt; &#xA; &lt;li&gt;Build your own LLM App end to end&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 8 [March 4 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week8_advanced_features.md&#34;&gt;Advanced Features and Deployment&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLM lifecycle and LLMOps&lt;/li&gt; &#xA; &lt;li&gt;LLM Monitoring and Observability&lt;/li&gt; &#xA; &lt;li&gt;Deployment strategies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 9 [March 11 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week9_challenges_with_llms.md&#34;&gt;Challenges with LLMs&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Scaling Challenges&lt;/li&gt; &#xA; &lt;li&gt;Behavioral Challenges&lt;/li&gt; &#xA; &lt;li&gt;Future directions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 10 [March 18 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week10_research_trends.md&#34;&gt;Emerging Research Trends&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Smaller and more performant models&lt;/li&gt; &#xA; &lt;li&gt;Multimodal models&lt;/li&gt; &#xA; &lt;li&gt;LLM Alignment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üóìÔ∏è&lt;em&gt;Week 11 &lt;em&gt;Bonus&lt;/em&gt; [March 25 2024]&lt;/em&gt;&lt;strong&gt;: &lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/free_courses/Applied_LLMs_Mastery_2024/week11_foundations.md&#34;&gt;Foundations&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generative Models Foundations&lt;/li&gt; &#xA; &lt;li&gt;Self-Attention and Transformers&lt;/li&gt; &#xA; &lt;li&gt;Neural Networks for Language&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h4&gt;&lt;span&gt;üìñ&lt;/span&gt; List of Free GenAI Courses&lt;/h4&gt; &#xA;&lt;h5&gt;LLM Basics and Foundations&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://rycolab.io/classes/llm-s23/&#34;&gt;Large Language Models&lt;/a&gt; by ETH Zurich&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/fall22/cos597G/&#34;&gt;Understanding Large Language Models&lt;/a&gt; by Princeton&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/learn/nlp-course/chapter1/1&#34;&gt;Transformers course&lt;/a&gt; by Huggingface&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/learn/nlp-course/chapter1/1&#34;&gt;NLP course&lt;/a&gt; by Huggingface&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://stanford-cs324.github.io/winter2022/&#34;&gt;CS324 - Large Language Models&lt;/a&gt; by Stanford&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.coursera.org/learn/generative-ai-with-llms&#34;&gt;Generative AI with Large Language Models&lt;/a&gt; by Coursera&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.coursera.org/learn/introduction-to-generative-ai&#34;&gt;Introduction to Generative AI&lt;/a&gt; by Coursera&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/paths/118/course_templates/556&#34;&gt;Generative AI Fundamentals&lt;/a&gt; by Google Cloud&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/paths/118/course_templates/539&#34;&gt;Introduction to Large Language Models&lt;/a&gt; by Google Cloud&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/paths/118/course_templates/536&#34;&gt;Introduction to Generative AI&lt;/a&gt; by Google Cloud&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.datacamp.com/courses/generative-ai-concepts&#34;&gt;Generative AI Concepts&lt;/a&gt; by DataCamp (Daniel Tedesco Data Lead @ Google)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xu5_kka-suc&#34;&gt;1 Hour Introduction to LLM (Large Language Models)&lt;/a&gt; by WeCloudData&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=W0c7jQezTDw&amp;amp;list=PLTPXxbhUt-YWjMCDahwdVye8HW69p5NYS&#34;&gt;LLM Foundation Models from the Ground Up | Primer&lt;/a&gt; by Databricks&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://courses.nvidia.com/courses/course-v1:DLI+S-FX-07+V1/&#34;&gt;Generative AI Explained&lt;/a&gt; by Nvidia&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/538&#34;&gt;Transformer Models and BERT Model&lt;/a&gt; by Google Cloud&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://explore.skillbuilder.aws/learn/public/learning_plan/view/1909/generative-ai-learning-plan-for-decision-makers&#34;&gt;Generative AI Learning Plan for Decision Makers&lt;/a&gt; by AWS&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.cloudskillsboost.google/course_templates/554&#34;&gt;Introduction to Responsible AI&lt;/a&gt; by Google Cloud&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/training/modules/fundamentals-generative-ai/&#34;&gt;Fundamentals of Generative AI&lt;/a&gt; by Microsoft Azure&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-122979-leestott&#34;&gt;Generative AI for Beginners&lt;/a&gt; by Microsoft&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.udemy.com/course/chatgpt-for-beginners-the-ultimate-use-cases-for-everyone/&#34;&gt;ChatGPT for Beginners: The Ultimate Use Cases for Everyone&lt;/a&gt; by Udemy&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zjkBMFhNj_g&#34;&gt;[1hr Talk] Intro to Large Language Models&lt;/a&gt; by Andrej Karpathy&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5&gt;Building LLM Applications&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.udacity.com/course/building-real-world-applications-with-large-language-models--cd13455&#34;&gt;LLMOps: Building Real-World Applications With Large Language Models&lt;/a&gt; by Udacity&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://fullstackdeeplearning.com/llm-bootcamp/&#34;&gt;Full Stack LLM Bootcamp&lt;/a&gt; by FSDL&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/generative-ai-for-beginners/tree/main&#34;&gt;Generative AI for beginners&lt;/a&gt; by Microsoft&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.edx.org/learn/computer-science/databricks-large-language-models-application-through-production&#34;&gt;Large Language Models: Application through Production&lt;/a&gt; by Databricks&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=oYm66fHqHUM&amp;amp;list=PLhr1KZpdzukf-xb0lmiU3G89GJXaDbAIF&#34;&gt;Generative AI Foundations&lt;/a&gt; by AWS&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ajWheP8ZD70&amp;amp;list=PLmQAMKHKeLZ-iTT-E2kK9uePrJ1Xua9VL&#34;&gt;Introduction to Generative AI Community Course&lt;/a&gt; by ineuron&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://docs.cohere.com/docs/llmu&#34;&gt;LLM University&lt;/a&gt; by Cohere&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://lightning.ai/pages/llm-learning-lab/&#34;&gt;LLM Learning Lab&lt;/a&gt; by Lightning AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/functions-tools-agents-langchain&#34;&gt;Functions, Tools and Agents with LangChain&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/login?redirect_course=langchain&amp;amp;callbackUrl=https%3A%2F%2Flearn.deeplearning.ai%2Fcourses%2Flangchain&#34;&gt;LangChain for LLM Application Development&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/llmops&#34;&gt;LLMOps&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/automated-testing-llmops&#34;&gt;Automated Testing for LLMOps&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://courses.nvidia.com/courses/course-v1:DLI+S-FX-15+V1/&#34;&gt;Building RAG Agents with LLMs&lt;/a&gt; by Nvidia&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://explore.skillbuilder.aws/learn/course/external/view/elearning/17904/building-generative-ai-applications-using-amazon-bedrock-aws-digital-training&#34;&gt;Building Generative AI Applications Using Amazon Bedrock&lt;/a&gt; by AWS&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/courses/efficiently-serving-llms/lesson/1/introduction&#34;&gt;Efficiently Serving LLMs&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/&#34;&gt;Building Systems with the ChatGPT API&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/serverless-llm-apps-amazon-bedrock/&#34;&gt;Serverless LLM apps with Amazon Bedrock&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/building-applications-vector-databases/&#34;&gt;Building Applications with Vector Databases&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/automated-testing-llmops/&#34;&gt;Automated Testing for LLMOps&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/llmops/&#34;&gt;LLMOps&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/build-llm-apps-with-langchain-js/&#34;&gt;Build LLM Apps with LangChain.js&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/advanced-retrieval-for-ai/&#34;&gt;Advanced Retrieval for AI with Chroma&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.coursera.org/learn/llmops-azure&#34;&gt;Operationalizing LLMs on Azure&lt;/a&gt; by Coursera&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=mEsleV16qdo&#34;&gt;Generative AI Full Course ‚Äì Gemini Pro, OpenAI, Llama, Langchain, Pinecone, Vector Databases &amp;amp; More&lt;/a&gt; by freeCodeCamp.org&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.activeloop.ai/courses/llms&#34;&gt;Training &amp;amp; Fine-Tuning LLMs for Production&lt;/a&gt; by Activeloop&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5&gt;Prompt Engineering, RAG and Fine-Tuning&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/redirect?event=video_description&amp;amp;redir_token=QUFFLUhqbVhnQW8xNDdhSU9IUDVLXzFhV2N0UkNRMkZrQXxBQ3Jtc0traUxHMzZJcGJQYjlyckYxaGxYVWlsOFNGUFlFVEdhNzdjTWpPUlQ2TF9XczRqNkxMVGpJTnd5YmYzV0prQ0IwZURNcHhIZ3h1Z051VTl5MXBBLUN0dkM0NHRkQTFua1Jpc0VCRFJUb0ZQZG95b0JqMA&amp;amp;q=https%3A%2F%2Flearn.activeloop.ai%2Fcourses%2Flangchain&amp;amp;v=gKUTDC13jys&#34;&gt;LangChain &amp;amp; Vector Databases in Production&lt;/a&gt; by Activeloop&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/reinforcement-learning-from-human-feedback&#34;&gt;Reinforcement Learning from Human Feedback&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/building-applications-vector-databases&#34;&gt;Building Applications with Vector Databases&lt;/a&gt; by DeepLearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/finetuning-large-language-models&#34;&gt;Finetuning Large Language Models&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://learn.deeplearning.ai/langchain-chat-with-your-data/&#34;&gt;LangChain: Chat with Your Data&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/chatgpt-building-system&#34;&gt;Building Systems with the ChatGPT API&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2/&#34;&gt;Prompt Engineering with Llama 2&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/building-applications-vector-databases&#34;&gt;Building Applications with Vector Databases&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction&#34;&gt;ChatGPT Prompt Engineering for Developers&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=CeDS1yvw9E4&#34;&gt;Advanced RAG Orchestration series&lt;/a&gt; by LlamaIndex&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.coursera.org/specializations/prompt-engineering&#34;&gt;Prompt Engineering Specialization&lt;/a&gt; by Coursera&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://courses.nvidia.com/courses/course-v1:NVIDIA+S-FX-16+v1/&#34;&gt;Augment your LLM Using Retrieval Augmented Generation&lt;/a&gt; by Nvidia&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/knowledge-graphs-rag/&#34;&gt;Knowledge Graphs for RAG&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/open-source-models-hugging-face/&#34;&gt;Open Source Models with Hugging Face&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications/&#34;&gt;Vector Databases: from Embeddings to Applications&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/google-cloud-vertex-ai/&#34;&gt;Understanding and Applying Text Embeddings&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex/&#34;&gt;JavaScript RAG Web Apps with LlamaIndex&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/&#34;&gt;Quantization Fundamentals with Hugging Face&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/preprocessing-unstructured-data-for-llm-applications/&#34;&gt;Preprocessing Unstructured Data for LLM Applications&lt;/a&gt; by Deeplearning.AI&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://learn.activeloop.ai/courses/rag&#34;&gt;Retrieval Augmented Generation for Production with LangChain &amp;amp; LlamaIndex&lt;/a&gt; by Activeloop&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5&gt;Evaluation&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.deeplearning.ai/building-evaluating-advanced-rag&#34;&gt;Building and Evaluating Advanced RAG Applications&lt;/a&gt; by DeepLearning.AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.deeplearning.ai/evaluating-debugging-generative-ai&#34;&gt;Evaluating and Debugging Generative AI Models Using Weights and Biases&lt;/a&gt; by Deeplearning.AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/quality-safety-llm-applications/&#34;&gt;Quality and Safety for LLM Applications&lt;/a&gt; by Deeplearning.AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/?utm_campaign=giskard-launch&amp;amp;utm_medium=headband&amp;amp;utm_source=dlai-homepage&#34;&gt;Red Teaming LLM Applications&lt;/a&gt; by Deeplearning.AI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h5&gt;Multimodal&lt;/h5&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.deeplearning.ai/short-courses/how-diffusion-models-work/&#34;&gt;How Diffusion Models Work&lt;/a&gt; by DeepLearning.AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5wdCev86RYE&#34;&gt;How to Use Midjourney, AI Art and ChatGPT to Create an Amazing Website&lt;/a&gt; by Brad Hussey&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scrimba.com/learn/buildaiapps&#34;&gt;Build AI Apps with ChatGPT, DALL-E and GPT-4&lt;/a&gt; by Scrimba&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL-Fhd_vrvisNM7pbbevXKAbT_Xmub37fA&#34;&gt;11-777: Multimodal Machine Learning&lt;/a&gt; by Carnegie Mellon University&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Miscellaneous&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/avoiding-ai-harm&#34;&gt;Avoiding AI Harm&lt;/a&gt; by Coursera&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coursera.org/learn/developing-ai-policy&#34;&gt;Developing AI Policy&lt;/a&gt; by Coursera&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìé&lt;/span&gt; Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://areganti.notion.site/06f0d4fe46a94d62bff2ae001cfec22c?v=d501ca62e4b745768385d698f173ae14&#34;&gt;ICLR 2024 Paper Summaries&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;span&gt;üíª&lt;/span&gt; Interview Prep&lt;/h2&gt; &#xA;&lt;h4&gt;Topic wise Questions:&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aishwaryanr/awesome-generative-ai-guide/raw/main/interview_prep/60_gen_ai_questions.md&#34;&gt;Common GenAI Interview Questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prompting and Prompt Engineering&lt;/li&gt; &#xA; &lt;li&gt;Model Fine-Tuning&lt;/li&gt; &#xA; &lt;li&gt;Model Evaluation&lt;/li&gt; &#xA; &lt;li&gt;MLOps for GenAI&lt;/li&gt; &#xA; &lt;li&gt;Generative Models Foundations&lt;/li&gt; &#xA; &lt;li&gt;Latest Research Trends&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;GenAI System Design (Coming Soon):&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Designing an LLM-Powered Search Engine&lt;/li&gt; &#xA; &lt;li&gt;Building a Customer Support Chatbot&lt;/li&gt; &#xA; &lt;li&gt;Building a system for natural language interaction with your data.&lt;/li&gt; &#xA; &lt;li&gt;Building an AI Co-pilot&lt;/li&gt; &#xA; &lt;li&gt;Designing a Custom Chatbot for Q/A on Multimodal Data (Text, Images, Tables, CSV Files)&lt;/li&gt; &#xA; &lt;li&gt;Building an Automated Product Description and Image Generation System for E-commerce&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìì&lt;/span&gt; Code Notebooks&lt;/h2&gt; &#xA;&lt;h4&gt;RAG Tutorials&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aws-samples/amazon-bedrock-workshop&#34;&gt;AWS Bedrock Workshop Tutorials&lt;/a&gt; by Amazon Web Services&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gkamradt/langchain-tutorials&#34;&gt;Langchain Tutorials&lt;/a&gt; by gkamradt&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ray-project/llm-applications/tree/main&#34;&gt;LLM Applications for production&lt;/a&gt; by ray-project&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama/tree/main/examples&#34;&gt;LLM tutorials&lt;/a&gt; by Ollama&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mallahyari/llm-hub&#34;&gt;LLM Hub&lt;/a&gt; by mallahyari&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Fine-Tuning Tutorials&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ashishpatel26/LLM-Finetuning&#34;&gt;LLM Fine-tuning tutorials&lt;/a&gt; by ashishpatel26&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/peft/tree/main/examples&#34;&gt;PEFT&lt;/a&gt; example notebooks by Huggingface&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://levelup.gitconnected.com/14-free-large-language-models-fine-tuning-notebooks-532055717cb7&#34;&gt;Free LLM Fine-Tuning Notebooks&lt;/a&gt; by Youssef Hosni&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;span&gt;‚úí&lt;/span&gt; Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you want to add to the repository or find any issues, please feel free to raise a PR and ensure correct placement within the relevant section or category.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;span&gt;üìå&lt;/span&gt; Cite Us&lt;/h2&gt; &#xA;&lt;p&gt;To cite this guide, use the below format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{areganti_generative_ai_guide,&#xA;author = {Reganti, Aishwarya Naresh},&#xA;journal = {https://github.com/aishwaryanr/awesome-generative-ai-resources},&#xA;month = {01},&#xA;title = {{Generative AI Guide}},&#xA;year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;[MIT License]&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pointfreeco/swift-composable-architecture</title>
    <updated>2024-04-20T01:23:55Z</updated>
    <id>tag:github.com,2024-04-20:/pointfreeco/swift-composable-architecture</id>
    <link href="https://github.com/pointfreeco/swift-composable-architecture" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A library for building applications in a consistent and understandable way, with composition, testing, and ergonomics in mind.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Composable Architecture&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pointfreeco/swift-composable-architecture/actions?query=workflow%3ACI&#34;&gt;&lt;img src=&#34;https://github.com/pointfreeco/swift-composable-architecture/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.pointfree.co/slack-invite&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-chat-informational.svg?label=Slack&amp;amp;logo=slack&#34; alt=&#34;Slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://swiftpackageindex.com/pointfreeco/swift-composable-architecture&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-composable-architecture%2Fbadge%3Ftype%3Dswift-versions&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://swiftpackageindex.com/pointfreeco/swift-composable-architecture&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fpointfreeco%2Fswift-composable-architecture%2Fbadge%3Ftype%3Dplatforms&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Composable Architecture (TCA, for short) is a library for building applications in a consistent and understandable way, with composition, testing, and ergonomics in mind. It can be used in SwiftUI, UIKit, and more, and on any Apple platform (iOS, macOS, tvOS, and watchOS).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/#what-is-the-composable-architecture&#34;&gt;What is the Composable Architecture?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/#learn-more&#34;&gt;Learn more&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/#examples&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/#basic-usage&#34;&gt;Basic usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/#documentation&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/#translations&#34;&gt;Translations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What is the Composable Architecture?&lt;/h2&gt; &#xA;&lt;p&gt;This library provides a few core tools that can be used to build applications of varying purpose and complexity. It provides compelling stories that you can follow to solve many problems you encounter day-to-day when building applications, such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;State management&lt;/strong&gt; &lt;br&gt; How to manage the state of your application using simple value types, and share state across many screens so that mutations in one screen can be immediately observed in another screen.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Composition&lt;/strong&gt; &lt;br&gt; How to break down large features into smaller components that can be extracted to their own, isolated modules and be easily glued back together to form the feature.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Side effects&lt;/strong&gt; &lt;br&gt; How to let certain parts of the application talk to the outside world in the most testable and understandable way possible.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt; &lt;br&gt; How to not only test a feature built in the architecture, but also write integration tests for features that have been composed of many parts, and write end-to-end tests to understand how side effects influence your application. This allows you to make strong guarantees that your business logic is running in the way you expect.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Ergonomics&lt;/strong&gt; &lt;br&gt; How to accomplish all of the above in a simple API with as few concepts and moving parts as possible.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Learn More&lt;/h2&gt; &#xA;&lt;p&gt;The Composable Architecture was designed over the course of many episodes on &lt;a href=&#34;https://www.pointfree.co&#34;&gt;Point-Free&lt;/a&gt;, a video series exploring functional programming and the Swift language, hosted by &lt;a href=&#34;https://twitter.com/mbrandonw&#34;&gt;Brandon Williams&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/stephencelis&#34;&gt;Stephen Celis&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can watch all of the episodes &lt;a href=&#34;https://www.pointfree.co/collections/composable-architecture&#34;&gt;here&lt;/a&gt;, as well as a dedicated, &lt;a href=&#34;https://www.pointfree.co/collections/tours/composable-architecture-1-0&#34;&gt;multipart tour&lt;/a&gt; of the architecture from scratch.&lt;/p&gt; &#xA;&lt;a href=&#34;https://www.pointfree.co/collections/tours/composable-architecture-1-0&#34;&gt; &lt;img alt=&#34;video poster image&#34; src=&#34;https://d3rccdn33rt8ze.cloudfront.net/episodes/0243.jpeg&#34; width=&#34;600&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples&#34;&gt;&lt;img src=&#34;https://d3rccdn33rt8ze.cloudfront.net/composable-architecture/demos.png&#34; alt=&#34;Screen shots of example applications&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo comes with &lt;em&gt;lots&lt;/em&gt; of examples to demonstrate how to solve common and complex problems with the Composable Architecture. Check out &lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples&#34;&gt;this&lt;/a&gt; directory to see them all, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples/CaseStudies&#34;&gt;Case Studies&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Getting started&lt;/li&gt; &#xA;   &lt;li&gt;Effects&lt;/li&gt; &#xA;   &lt;li&gt;Navigation&lt;/li&gt; &#xA;   &lt;li&gt;Higher-order reducers&lt;/li&gt; &#xA;   &lt;li&gt;Reusable components&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pointfreeco/composable-core-location/tree/main/Examples/LocationManager&#34;&gt;Location manager&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pointfreeco/composable-core-motion/tree/main/Examples/MotionManager&#34;&gt;Motion manager&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples/Search&#34;&gt;Search&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples/SpeechRecognition&#34;&gt;Speech Recognition&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples/SyncUps&#34;&gt;SyncUps app&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples/TicTacToe&#34;&gt;Tic-Tac-Toe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples/Todos&#34;&gt;Todos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples/VoiceMemos&#34;&gt;Voice memos&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Looking for something more substantial? Check out the source code for &lt;a href=&#34;https://github.com/pointfreeco/isowords&#34;&gt;isowords&lt;/a&gt;, an iOS word search game built in SwiftUI and the Composable Architecture.&lt;/p&gt; &#xA;&lt;h2&gt;Basic Usage&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Note] For a step-by-step interactive tutorial, be sure to check out &lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/tutorials/meetcomposablearchitecture&#34;&gt;Meet the Composable Architecture&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To build a feature using the Composable Architecture you define some types and values that model your domain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;State&lt;/strong&gt;: A type that describes the data your feature needs to perform its logic and render its UI.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt;: A type that represents all of the actions that can happen in your feature, such as user actions, notifications, event sources and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reducer&lt;/strong&gt;: A function that describes how to evolve the current state of the app to the next state given an action. The reducer is also responsible for returning any effects that should be run, such as API requests, which can be done by returning an &lt;code&gt;Effect&lt;/code&gt; value.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Store&lt;/strong&gt;: The runtime that actually drives your feature. You send all user actions to the store so that the store can run the reducer and effects, and you can observe state changes in the store so that you can update UI.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The benefits of doing this are that you will instantly unlock testability of your feature, and you will be able to break large, complex features into smaller domains that can be glued together.&lt;/p&gt; &#xA;&lt;p&gt;As a basic example, consider a UI that shows a number along with &#34;+&#34; and &#34;‚àí&#34; buttons that increment and decrement the number. To make things interesting, suppose there is also a button that when tapped makes an API request to fetch a random fact about that number and displays it in the view.&lt;/p&gt; &#xA;&lt;p&gt;To implement this feature we create a new type that will house the domain and behavior of the feature, and it will be annotated with the &lt;code&gt;@Reducer&lt;/code&gt; macro:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import ComposableArchitecture&#xA;&#xA;@Reducer&#xA;struct Feature {&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In here we need to define a type for the feature&#39;s state, which consists of an integer for the current count, as well as an optional string that represents the fact being presented:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;@Reducer&#xA;struct Feature {&#xA;  @ObservableState&#xA;  struct State: Equatable {&#xA;    var count = 0&#xA;    var numberFact: String?&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Note] We&#39;ve applied the &lt;code&gt;@ObservableState&lt;/code&gt; macro to &lt;code&gt;State&lt;/code&gt; in order to take advantage of the observation tools in the library.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We also need to define a type for the feature&#39;s actions. There are the obvious actions, such as tapping the decrement button, increment button, or fact button. But there are also some slightly non-obvious ones, such as the action that occurs when we receive a response from the fact API request:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;@Reducer&#xA;struct Feature {&#xA;  @ObservableState&#xA;  struct State: Equatable { /* ... */ }&#xA;  enum Action {&#xA;    case decrementButtonTapped&#xA;    case incrementButtonTapped&#xA;    case numberFactButtonTapped&#xA;    case numberFactResponse(String)&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then we implement the &lt;code&gt;body&lt;/code&gt; property, which is responsible for composing the actual logic and behavior for the feature. In it we can use the &lt;code&gt;Reduce&lt;/code&gt; reducer to describe how to change the current state to the next state, and what effects need to be executed. Some actions don&#39;t need to execute effects, and they can return &lt;code&gt;.none&lt;/code&gt; to represent that:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;@Reducer&#xA;struct Feature {&#xA;  @ObservableState&#xA;  struct State: Equatable { /* ... */ }&#xA;  enum Action { /* ... */ }&#xA;&#xA;  var body: some Reducer&amp;lt;State, Action&amp;gt; {&#xA;    Reduce { state, action in&#xA;      switch action {&#xA;      case .decrementButtonTapped:&#xA;        state.count -= 1&#xA;        return .none&#xA;&#xA;      case .incrementButtonTapped:&#xA;        state.count += 1&#xA;        return .none&#xA;&#xA;      case .numberFactButtonTapped:&#xA;        return .run { [count = state.count] send in&#xA;          let (data, _) = try await URLSession.shared.data(&#xA;            from: URL(string: &#34;http://numbersapi.com/\(count)/trivia&#34;)!&#xA;          )&#xA;          await send(&#xA;            .numberFactResponse(String(decoding: data, as: UTF8.self))&#xA;          )&#xA;        }&#xA;&#xA;      case let .numberFactResponse(fact):&#xA;        state.numberFact = fact&#xA;        return .none&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then finally we define the view that displays the feature. It holds onto a &lt;code&gt;StoreOf&amp;lt;Feature&amp;gt;&lt;/code&gt; so that it can observe all changes to the state and re-render, and we can send all user actions to the store so that state changes:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;struct FeatureView: View {&#xA;  let store: StoreOf&amp;lt;Feature&amp;gt;&#xA;&#xA;  var body: some View {&#xA;    Form {&#xA;      Section {&#xA;        Text(&#34;\(store.count)&#34;)&#xA;        Button(&#34;Decrement&#34;) { store.send(.decrementButtonTapped) }&#xA;        Button(&#34;Increment&#34;) { store.send(.incrementButtonTapped) }&#xA;      }&#xA;&#xA;      Section {&#xA;        Button(&#34;Number fact&#34;) { store.send(.numberFactButtonTapped) }&#xA;      }&#xA;      &#xA;      if let fact = store.numberFact {&#xA;        Text(fact)&#xA;      }&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is also straightforward to have a UIKit controller driven off of this store. You can observe state changes in the store in &lt;code&gt;viewDidLoad&lt;/code&gt;, and then populate the UI components with data from the store. The code is a bit longer than the SwiftUI version, so we have collapsed it here:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to expand!&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;class FeatureViewController: UIViewController {&#xA;  let store: StoreOf&amp;lt;Feature&amp;gt;&#xA;&#xA;  init(store: StoreOf&amp;lt;Feature&amp;gt;) {&#xA;    self.store = store&#xA;    super.init(nibName: nil, bundle: nil)&#xA;  }&#xA;&#xA;  required init?(coder: NSCoder) {&#xA;    fatalError(&#34;init(coder:) has not been implemented&#34;)&#xA;  }&#xA;&#xA;  override func viewDidLoad() {&#xA;    super.viewDidLoad()&#xA;&#xA;    let countLabel = UILabel()&#xA;    let decrementButton = UIButton()&#xA;    let incrementButton = UIButton()&#xA;    let factLabel = UILabel()&#xA;    &#xA;    // Omitted: Add subviews and set up constraints...&#xA;    &#xA;    observe { [weak self] in&#xA;      guard let self &#xA;      else { return }&#xA;      &#xA;      countLabel.text = &#34;\(self.store.text)&#34;&#xA;      factLabel.text = self.store.numberFact&#xA;    }&#xA;  }&#xA;&#xA;  @objc private func incrementButtonTapped() {&#xA;    self.store.send(.incrementButtonTapped)&#xA;  }&#xA;  @objc private func decrementButtonTapped() {&#xA;    self.store.send(.decrementButtonTapped)&#xA;  }&#xA;  @objc private func factButtonTapped() {&#xA;    self.store.send(.numberFactButtonTapped)&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Once we are ready to display this view, for example in the app&#39;s entry point, we can construct a store. This can be done by specifying the initial state to start the application in, as well as the reducer that will power the application:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import ComposableArchitecture&#xA;&#xA;@main&#xA;struct MyApp: App {&#xA;  var body: some Scene {&#xA;    WindowGroup {&#xA;      FeatureView(&#xA;        store: Store(initialState: Feature.State()) {&#xA;          Feature()&#xA;        }&#xA;      )&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And that is enough to get something on the screen to play around with. It&#39;s definitely a few more steps than if you were to do this in a vanilla SwiftUI way, but there are a few benefits. It gives us a consistent manner to apply state mutations, instead of scattering logic in some observable objects and in various action closures of UI components. It also gives us a concise way of expressing side effects. And we can immediately test this logic, including the effects, without doing much additional work.&lt;/p&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Note] For more in-depth information on testing, see the dedicated &lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/testing&#34;&gt;testing&lt;/a&gt; article.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To test use a &lt;code&gt;TestStore&lt;/code&gt;, which can be created with the same information as the &lt;code&gt;Store&lt;/code&gt;, but it does extra work to allow you to assert how your feature evolves as actions are sent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;@MainActor&#xA;func testFeature() async {&#xA;  let store = TestStore(initialState: Feature.State()) {&#xA;    Feature()&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the test store is created we can use it to make an assertion of an entire user flow of steps. Each step of the way we need to prove that state changed how we expect. For example, we can simulate the user flow of tapping on the increment and decrement buttons:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;// Test that tapping on the increment/decrement buttons changes the count&#xA;await store.send(.incrementButtonTapped) {&#xA;  $0.count = 1&#xA;}&#xA;await store.send(.decrementButtonTapped) {&#xA;  $0.count = 0&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Further, if a step causes an effect to be executed, which feeds data back into the store, we must assert on that. For example, if we simulate the user tapping on the fact button we expect to receive a fact response back with the fact, which then causes the &lt;code&gt;numberFact&lt;/code&gt; state to be populated:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;await store.send(.numberFactButtonTapped)&#xA;&#xA;await store.receive(\.numberFactResponse) {&#xA;  $0.numberFact = ???&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, how do we know what fact is going to be sent back to us?&lt;/p&gt; &#xA;&lt;p&gt;Currently our reducer is using an effect that reaches out into the real world to hit an API server, and that means we have no way to control its behavior. We are at the whims of our internet connectivity and the availability of the API server in order to write this test.&lt;/p&gt; &#xA;&lt;p&gt;It would be better for this dependency to be passed to the reducer so that we can use a live dependency when running the application on a device, but use a mocked dependency for tests. We can do this by adding a property to the &lt;code&gt;Feature&lt;/code&gt; reducer:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;@Reducer&#xA;struct Feature {&#xA;  let numberFact: (Int) async throws -&amp;gt; String&#xA;  // ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then we can use it in the &lt;code&gt;reduce&lt;/code&gt; implementation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;case .numberFactButtonTapped:&#xA;  return .run { [count = state.count] send in &#xA;    let fact = try await self.numberFact(count)&#xA;    await send(.numberFactResponse(fact))&#xA;  }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And in the entry point of the application we can provide a version of the dependency that actually interacts with the real world API server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;@main&#xA;struct MyApp: App {&#xA;  var body: some Scene {&#xA;    WindowGroup {&#xA;      FeatureView(&#xA;        store: Store(initialState: Feature.State()) {&#xA;          Feature(&#xA;            numberFact: { number in&#xA;              let (data, _) = try await URLSession.shared.data(&#xA;                from: URL(string: &#34;http://numbersapi.com/\(number)&#34;)!&#xA;              )&#xA;              return String(decoding: data, as: UTF8.self)&#xA;            }&#xA;          )&#xA;        }&#xA;      )&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;But in tests we can use a mock dependency that immediately returns a deterministic, predictable fact:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;@MainActor&#xA;func testFeature() async {&#xA;  let store = TestStore(initialState: Feature.State()) {&#xA;    Feature(numberFact: { &#34;\($0) is a good number Brent&#34; })&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With that little bit of upfront work we can finish the test by simulating the user tapping on the fact button, and thenreceiving the response from the dependency to present the fact:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;await store.send(.numberFactButtonTapped)&#xA;&#xA;await store.receive(\.numberFactResponse) {&#xA;  $0.numberFact = &#34;0 is a good number Brent&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can also improve the ergonomics of using the &lt;code&gt;numberFact&lt;/code&gt; dependency in our application. Over time the application may evolve into many features, and some of those features may also want access to &lt;code&gt;numberFact&lt;/code&gt;, and explicitly passing it through all layers can get annoying. There is a process you can follow to ‚Äúregister‚Äù dependencies with the library, making them instantly available to any layer in the application.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Note] For more in-depth information on dependency management, see the dedicated &lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/dependencymanagement&#34;&gt;dependencies&lt;/a&gt; article.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;We can start by wrapping the number fact functionality in a new type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;struct NumberFactClient {&#xA;  var fetch: (Int) async throws -&amp;gt; String&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then registering that type with the dependency management system by conforming the client to the &lt;code&gt;DependencyKey&lt;/code&gt; protocol, which requires you to specify the live value to use when running the application in simulators or devices:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;extension NumberFactClient: DependencyKey {&#xA;  static let liveValue = Self(&#xA;    fetch: { number in&#xA;      let (data, _) = try await URLSession.shared&#xA;        .data(from: URL(string: &#34;http://numbersapi.com/\(number)&#34;)!&#xA;      )&#xA;      return String(decoding: data, as: UTF8.self)&#xA;    }&#xA;  )&#xA;}&#xA;&#xA;extension DependencyValues {&#xA;  var numberFact: NumberFactClient {&#xA;    get { self[NumberFactClient.self] }&#xA;    set { self[NumberFactClient.self] = newValue }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;With that little bit of upfront work done you can instantly start making use of the dependency in any feature by using the &lt;code&gt;@Dependency&lt;/code&gt; property wrapper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-diff&#34;&gt; @Reducer&#xA; struct Feature {&#xA;-  let numberFact: (Int) async throws -&amp;gt; String&#xA;+  @Dependency(\.numberFact) var numberFact&#xA;   &#xA;   ‚Ä¶&#xA;&#xA;-  try await self.numberFact(count)&#xA;+  try await self.numberFact.fetch(count)&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This code works exactly as it did before, but you no longer have to explicitly pass the dependency when constructing the feature&#39;s reducer. When running the app in previews, the simulator or on a device, the live dependency will be provided to the reducer, and in tests the test dependency will be provided.&lt;/p&gt; &#xA;&lt;p&gt;This means the entry point to the application no longer needs to construct dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;@main&#xA;struct MyApp: App {&#xA;  var body: some Scene {&#xA;    WindowGroup {&#xA;      FeatureView(&#xA;        store: Store(initialState: Feature.State()) {&#xA;          Feature()&#xA;        }&#xA;      )&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And the test store can be constructed without specifying any dependencies, but you can still override any dependency you need to for the purpose of the test:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;let store = TestStore(initialState: Feature.State()) {&#xA;  Feature()&#xA;} withDependencies: {&#xA;  $0.numberFact.fetch = { &#34;\($0) is a good number Brent&#34; }&#xA;}&#xA;&#xA;// ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That is the basics of building and testing a feature in the Composable Architecture. There are &lt;em&gt;a lot&lt;/em&gt; more things to be explored, such as composition, modularity, adaptability, and complex effects. The &lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples&#34;&gt;Examples&lt;/a&gt; directory has a bunch of projects to explore to see more advanced usages.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The documentation for releases and &lt;code&gt;main&lt;/code&gt; are available here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/&#34;&gt;&lt;code&gt;main&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.9.0/documentation/composablearchitecture/&#34;&gt;1.9.0&lt;/a&gt; (&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/migratingto1.9&#34;&gt;migration guide&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Other versions &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.8.0/documentation/composablearchitecture/&#34;&gt;1.8.0&lt;/a&gt; (&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/migratingto1.8&#34;&gt;migration guide&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.7.0/documentation/composablearchitecture/&#34;&gt;1.7.0&lt;/a&gt; (&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/migratingto1.7&#34;&gt;migration guide&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.6.0/documentation/composablearchitecture/&#34;&gt;1.6.0&lt;/a&gt; (&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/migratingto1.6&#34;&gt;migration guide&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.5.0/documentation/composablearchitecture/&#34;&gt;1.5.0&lt;/a&gt; (&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/migratingto1.5&#34;&gt;migration guide&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.4.0/documentation/composablearchitecture/&#34;&gt;1.4.0&lt;/a&gt; (&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/migratingto1.4&#34;&gt;migration guide&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.3.0/documentation/composablearchitecture/&#34;&gt;1.3.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.2.0/documentation/composablearchitecture/&#34;&gt;1.2.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.1.0/documentation/composablearchitecture/&#34;&gt;1.1.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/1.0.0/documentation/composablearchitecture/&#34;&gt;1.0.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/0.59.0/documentation/composablearchitecture/&#34;&gt;0.59.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/0.58.0/documentation/composablearchitecture/&#34;&gt;0.58.0&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/0.57.0/documentation/composablearchitecture/&#34;&gt;0.57.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;There are a number of articles in the documentation that you may find helpful as you become more comfortable with the library:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/gettingstarted&#34;&gt;Getting started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/dependencymanagement&#34;&gt;Dependencies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/testing&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/navigation&#34;&gt;Navigation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/swiftconcurrency&#34;&gt;Concurrency&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pointfreeco.github.io/swift-composable-architecture/main/documentation/composablearchitecture/bindings&#34;&gt;Bindings&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;If you want to discuss the Composable Architecture or have a question about how to use it to solve a particular problem, there are a number of places you can discuss with fellow &lt;a href=&#34;http://www.pointfree.co&#34;&gt;Point-Free&lt;/a&gt; enthusiasts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For long-form discussions, we recommend the &lt;a href=&#34;https://github.com/pointfreeco/swift-composable-architecture/discussions&#34;&gt;discussions&lt;/a&gt; tab of this repo.&lt;/li&gt; &#xA; &lt;li&gt;For casual chat, we recommend the &lt;a href=&#34;http://pointfree.co/slack-invite&#34;&gt;Point-Free Community slack&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;You can add ComposableArchitecture to an Xcode project by adding it as a package dependency.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;From the &lt;strong&gt;File&lt;/strong&gt; menu, select &lt;strong&gt;Add Package Dependencies...&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Enter &#34;&lt;a href=&#34;https://github.com/pointfreeco/swift-composable-architecture&#34;&gt;https://github.com/pointfreeco/swift-composable-architecture&lt;/a&gt;&#34; into the package repository URL text field&lt;/li&gt; &#xA; &lt;li&gt;Depending on how your project is structured: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If you have a single application target that needs access to the library, then add &lt;strong&gt;ComposableArchitecture&lt;/strong&gt; directly to your application.&lt;/li&gt; &#xA;   &lt;li&gt;If you want to use this library from multiple Xcode targets, or mix Xcode targets and SPM targets, you must create a shared framework that depends on &lt;strong&gt;ComposableArchitecture&lt;/strong&gt; and then depend on that framework in all of your targets. For an example of this, check out the &lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/Examples/TicTacToe&#34;&gt;Tic-Tac-Toe&lt;/a&gt; demo application, which splits lots of features into modules and consumes the static library in this fashion using the &lt;strong&gt;tic-tac-toe&lt;/strong&gt; Swift package.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Companion libraries&lt;/h2&gt; &#xA;&lt;p&gt;The Composable Architecture is built with extensibility in mind, and there are a number of community-supported libraries available to enhance your applications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ryu0118/swift-composable-architecture-extras&#34;&gt;Composable Architecture Extras&lt;/a&gt;: A companion library to the Composable Architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mentalflux/tca-composer&#34;&gt;TCAComposer&lt;/a&gt;: A macro framework for generating boiler-plate code in the Composable Architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/johnpatrickmorgan/TCACoordinators&#34;&gt;TCACoordinators&lt;/a&gt;: The coordinator pattern in the Composable Architecture.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute a library, please &lt;a href=&#34;https://github.com/pointfreeco/swift-composable-architecture/edit/main/README.md&#34;&gt;open a PR&lt;/a&gt; with a link to it!&lt;/p&gt; &#xA;&lt;h2&gt;Translations&lt;/h2&gt; &#xA;&lt;p&gt;The following translations of this README have been contributed by members of the community:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/NorhanBoghdadi/1b98d55c02b683ddef7e05c2ebcccd47&#34;&gt;Arabic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/nikitamounier/0e93eb832cf389db12f9a69da030a2dc&#34;&gt;French&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/akashsoni01/b358ee0b3b747167964ef6946123c88d&#34;&gt;Hindi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/wendyliga/792ea9ac5cc887f59de70a9e39cc7343&#34;&gt;Indonesian&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/Bellaposa/5114e6d4d55fdb1388e8186886d48958&#34;&gt;Italian&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/Achoo-kr/2d0712deb77f78b3379551ac7baea3e4&#34;&gt;Japanese&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/Achoo-kr/5d8936d12e71028fcc4a7c5e078ca038&#34;&gt;Korean&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/MarcelStarczyk/6b6153051f46912a665c32199f0d1d54&#34;&gt;Polish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/SevioCorrea/2bbf337cd084a58c89f2f7f370626dc8&#34;&gt;Portuguese&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/artyom-ivanov/ed0417fd1f008f0492d3431c033175df&#34;&gt;Russian&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/sh3l6orrr/10c8f7c634a892a9c37214f3211242ad&#34;&gt;Simplified Chinese&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/pitt500/f5e32fccb575ce112ffea2827c7bf942&#34;&gt;Spanish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gist.github.com/barabashd/33b64676195ce41f4bb73c327ea512a8&#34;&gt;Ukrainian&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute a translation, please &lt;a href=&#34;https://github.com/pointfreeco/swift-composable-architecture/edit/main/README.md&#34;&gt;open a PR&lt;/a&gt; with a link to a &lt;a href=&#34;https://gist.github.com&#34;&gt;Gist&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;How does the Composable Architecture compare to Elm, Redux, and others?&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Expand to see answer&lt;/summary&gt; The Composable Architecture (TCA) is built on a foundation of ideas popularized by the Elm Architecture (TEA) and Redux, but made to feel at home in the Swift language and on Apple&#39;s platforms. &#xA;   &lt;p&gt;In some ways TCA is a little more opinionated than the other libraries. For example, Redux is not prescriptive with how one executes side effects, but TCA requires all side effects to be modeled in the &lt;code&gt;Effect&lt;/code&gt; type and returned from the reducer.&lt;/p&gt; &#xA;   &lt;p&gt;In other ways TCA is a little more lax than the other libraries. For example, Elm controls what kinds of effects can be created via the &lt;code&gt;Cmd&lt;/code&gt; type, but TCA allows an escape hatch to any kind of effect since &lt;code&gt;Effect&lt;/code&gt; wraps around an async operation.&lt;/p&gt; &#xA;   &lt;p&gt;And then there are certain things that TCA prioritizes highly that are not points of focus for Redux, Elm, or most other libraries. For example, composition is very important aspect of TCA, which is the process of breaking down large features into smaller units that can be glued together. This is accomplished with reducer builders and operators like &lt;code&gt;Scope&lt;/code&gt;, and it aids in handling complex features as well as modularization for a better-isolated code base and improved compile times.&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Credits and thanks&lt;/h2&gt; &#xA;&lt;p&gt;The following people gave feedback on the library at its early stages and helped make the library what it is today:&lt;/p&gt; &#xA;&lt;p&gt;Paul Colton, Kaan Dedeoglu, Matt Diephouse, Josef Dole≈æal, Eimantas, Matthew Johnson, George Kaimakas, Nikita Leonov, Christopher Liscio, Jeffrey Macko, Alejandro Martinez, Shai Mishali, Willis Plummer, Simon-Pierre Roy, Justin Price, Sven A. Schmidt, Kyle Sherman, Petr ≈†√≠ma, Jasdev Singh, Maxim Smirnov, Ryan Stone, Daniel Hollis Tavares, and all of the &lt;a href=&#34;https://www.pointfree.co&#34;&gt;Point-Free&lt;/a&gt; subscribers üòÅ.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://twitter.com/liscio&#34;&gt;Chris Liscio&lt;/a&gt; who helped us work through many strange SwiftUI quirks and helped refine the final API.&lt;/p&gt; &#xA;&lt;p&gt;And thanks to &lt;a href=&#34;https://github.com/freak4pc&#34;&gt;Shai Mishali&lt;/a&gt; and the &lt;a href=&#34;https://github.com/CombineCommunity/CombineExt/&#34;&gt;CombineCommunity&lt;/a&gt; project, from which we took their implementation of &lt;code&gt;Publishers.Create&lt;/code&gt;, which we use in &lt;code&gt;Effect&lt;/code&gt; to help bridge delegate and callback-based APIs, making it much easier to interface with 3rd party frameworks.&lt;/p&gt; &#xA;&lt;h2&gt;Other libraries&lt;/h2&gt; &#xA;&lt;p&gt;The Composable Architecture was built on a foundation of ideas started by other libraries, in particular &lt;a href=&#34;https://elm-lang.org&#34;&gt;Elm&lt;/a&gt; and &lt;a href=&#34;https://redux.js.org/&#34;&gt;Redux&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;There are also many architecture libraries in the Swift and iOS community. Each one of these has their own set of priorities and trade-offs that differ from the Composable Architecture.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/uber/RIBs&#34;&gt;RIBs&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ReactiveCocoa/Loop&#34;&gt;Loop&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ReSwift/ReSwift&#34;&gt;ReSwift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/square/workflow&#34;&gt;Workflow&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ReactorKit/ReactorKit&#34;&gt;ReactorKit&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/NoTests/RxFeedback.swift&#34;&gt;RxFeedback&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/spotify/mobius.swift&#34;&gt;Mobius.swift&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;And more&lt;/summary&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://github.com/FluxorOrg/Fluxor&#34;&gt;Fluxor&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://github.com/RPallas92/PromisedArchitectureKit&#34;&gt;PromisedArchitectureKit&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This library is released under the MIT license. See &lt;a href=&#34;https://raw.githubusercontent.com/pointfreeco/swift-composable-architecture/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>amalshaji/portr</title>
    <updated>2024-04-20T01:23:55Z</updated>
    <id>tag:github.com,2024-04-20:/amalshaji/portr</id>
    <link href="https://github.com/amalshaji/portr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open source ngrok alternative designed for teams&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/amalshaji/portr/main/docs/src/assets/logo.svg?sanitize=true&#34; height=&#34;75px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;GitHub License&#34; src=&#34;https://img.shields.io/github/license/amalshaji/portr&#34;&gt; &#xA; &lt;img alt=&#34;GitHub Release&#34; src=&#34;https://img.shields.io/github/v/release/amalshaji/portr&#34;&gt; &#xA; &lt;a href=&#34;https://portr.dev&#34; target=&#34;_blank&#34;&gt;&lt;img alt=&#34;Documentation&#34; src=&#34;https://img.shields.io/badge/Documentation-portr.dev-0096FF&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Portr is a tunnel solution that allows you to expose local http, tcp or websocket connections to the public internet. It utilizes SSH remote port forwarding under the hood to securely tunnel connections.&lt;/p&gt; &#xA;&lt;p&gt;Portr is primarily designed for small teams looking to expose development servers on a public URL. It is not recommended for use alongside production servers.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Portr is currently in beta. Expect bugs and anticipate breaking changes.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üéâ Easily tunnel http, tcp or websocket connections.&lt;/li&gt; &#xA; &lt;li&gt;üëæ Admin dashboard for team/user management. &lt;a href=&#34;https://youtu.be/P37la8DjrzA&#34;&gt;Watch video&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;üö® Portr inspector for inspecting and replaying requests. &lt;a href=&#34;https://youtu.be/hhbte2JI3qk&#34;&gt;Watch video&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://portr.dev/server/&#34;&gt;Server setup guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://portr.dev/client/installation/&#34;&gt;Client installation guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please read through &lt;a href=&#34;https://raw.githubusercontent.com/amalshaji/portr/main/.github/contributing.md&#34;&gt;our contributing guide&lt;/a&gt; and set up your &lt;a href=&#34;https://portr.dev/local-development/admin/&#34;&gt;development environment&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the GNU Affero General Public License v3.0 (AGPL-3.0). See the &lt;a href=&#34;https://raw.githubusercontent.com/amalshaji/portr/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for the full license text.&lt;/p&gt;</summary>
  </entry>
</feed>