<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-11T01:31:06Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>FranxYao/chain-of-thought-hub</title>
    <updated>2023-06-11T01:31:06Z</updated>
    <id>tag:github.com,2023-06-11:/FranxYao/chain-of-thought-hub</id>
    <link href="https://github.com/FranxYao/chain-of-thought-hub" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Benchmarking large language models&#39; complex reasoning ability with chain-of-thought prompting&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chain-of-Thought Hub: Measuring LLMs&#39; Reasoning Performance&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/resources/title.png&#34; alt=&#34;Title&#34;&gt; &#34;A fantasy graph illustrating a chain of stars in a dark night with blue sky, digital art, super resolution&#34;. Midjourney V5&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;By &lt;a href=&#34;https://franxyao.github.io/&#34;&gt;Yao Fu&lt;/a&gt;, &lt;a href=&#34;https://github.com/Leonard907&#34;&gt;Litu Ou&lt;/a&gt;, &lt;a href=&#34;https://github.com/Spehhhhh&#34;&gt;Mingyu Chen&lt;/a&gt;, &lt;a href=&#34;https://github.com/Yuhao-Wan&#34;&gt;Yuhao Wan&lt;/a&gt;, &lt;a href=&#34;https://haopeng-nlp.github.io/&#34;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&#34;https://allenai.org/team/tushark&#34;&gt;Tushar Khot&lt;/a&gt;, &lt;a href=&#34;https://wenhuchen.github.io/&#34;&gt;Wenhu Chen&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;From University of Edinburgh, University of Washington, Allen Institute for AI, University of Waterloo&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2305.17306&#34;&gt;paper&lt;/a&gt;] [&lt;a href=&#34;https://yaofu.notion.site/Towards-Complex-Reasoning-the-Polaris-of-Large-Language-Models-c2b4a51355b44764975f88e6a42d4e75&#34;&gt;blog&lt;/a&gt;] [&lt;a href=&#34;https://twitter.com/Francis_YAO_/status/1663472109299937280&#34;&gt;twitter&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;Recently, there are a lot of progress in LLMs. Many claim that a small model less than 10B can achieve comparable performance to GPT-3.5. Really?&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In a casual conversation, the distinction between GPT-3.5 and GPT-4 can be subtle. The difference comes out when &lt;strong&gt;*the complexity of the task reaches a sufficient threshold*&lt;/strong&gt; — GPT-4 is more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5. -- &lt;em&gt;GPT-4 release blog&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The key differentiator is whether a model can do &lt;strong&gt;complex tasks&lt;/strong&gt;, like the old saying: &#34;chit-chat is cheap, show me the reasoning.&#34; This is why we compile a list of complex reasoning tasks including math (GSM8K), science (MATH, TheoremQA), symbolic (BBH), knowledge (MMLU, C-Eval), coding (HumanEval), factual (SummEdits) to measure the models&#39; performance on challenging tasks.&lt;/p&gt; &#xA;&lt;p&gt;More importantly, we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications. When this comes, chain-of-thought prompt engineering will be the next-generation system calls and shell scripts.&lt;/p&gt; &#xA;&lt;p&gt;The resutls and scripts from Chain-of-thought Hub is being used and referred by leading industrial and academic organizations in the space of large language models.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Call for contribution]&lt;/strong&gt;: would love to invite community members to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Send a PR to fill in a missing number in the table&lt;/li&gt; &#xA; &lt;li&gt;Raise an issue to suggest a new task that can clearly differentiate models&#39; performance&lt;/li&gt; &#xA; &lt;li&gt;Raise an issue to suggest a new model that can be added to the table&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;[UPDATE 20230609]&lt;/strong&gt;: Add &lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/MMLU/readme.md&#34;&gt;evaluation scripts&lt;/a&gt; on MMLU for LLaMA and Falcon&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[UPDATE 20230601]&lt;/strong&gt;: Add SummEdits&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[UPDATE 20230527]&lt;/strong&gt;: Add TheoremQA, add Vicuna, Alpaca, InstructCodeT5.&lt;/p&gt; &#xA;&lt;h2&gt;Leaderboard&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Param.&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;GSM8K&lt;/th&gt; &#xA;   &lt;th&gt;MATH&lt;/th&gt; &#xA;   &lt;th&gt;MMLU&lt;/th&gt; &#xA;   &lt;th&gt;BBH&lt;/th&gt; &#xA;   &lt;th&gt;HumanEval&lt;/th&gt; &#xA;   &lt;th&gt;C-Eval&lt;/th&gt; &#xA;   &lt;th&gt;TheoremQA&lt;/th&gt; &#xA;   &lt;th&gt;SummEdits&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-4&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;92.0&lt;/td&gt; &#xA;   &lt;td&gt;42.5&lt;/td&gt; &#xA;   &lt;td&gt;86.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;67.0&lt;/td&gt; &#xA;   &lt;td&gt;68.7*&lt;/td&gt; &#xA;   &lt;td&gt;43.4&lt;/td&gt; &#xA;   &lt;td&gt;82.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;claude-v1.3&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;81.8*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;75.6*&lt;/td&gt; &#xA;   &lt;td&gt;67.3*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;54.2*&lt;/td&gt; &#xA;   &lt;td&gt;24.9&lt;/td&gt; &#xA;   &lt;td&gt;59.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaLM-2-Unicorn&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;80.7&lt;/td&gt; &#xA;   &lt;td&gt;34.3&lt;/td&gt; &#xA;   &lt;td&gt;78.3&lt;/td&gt; &#xA;   &lt;td&gt;78.1&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;31.8&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaLM-2-bison&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;69.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;gpt-3.5-turbo&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;74.9*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;67.3*&lt;/td&gt; &#xA;   &lt;td&gt;70.1*&lt;/td&gt; &#xA;   &lt;td&gt;48.1&lt;/td&gt; &#xA;   &lt;td&gt;54.4*&lt;/td&gt; &#xA;   &lt;td&gt;30.2&lt;/td&gt; &#xA;   &lt;td&gt;71.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;claude-instant&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;70.8*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;61.3*&lt;/td&gt; &#xA;   &lt;td&gt;66.9*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;45.9*&lt;/td&gt; &#xA;   &lt;td&gt;23.6&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;text-davinci-003&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;RLHF&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;64.6&lt;/td&gt; &#xA;   &lt;td&gt;70.7&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;22.8&lt;/td&gt; &#xA;   &lt;td&gt;70.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;code-davinci-002&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;66.6&lt;/td&gt; &#xA;   &lt;td&gt;19.1&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;   &lt;td&gt;73.7&lt;/td&gt; &#xA;   &lt;td&gt;47.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;text-davinci-002&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;SIFT&lt;/td&gt; &#xA;   &lt;td&gt;55.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;60.0&lt;/td&gt; &#xA;   &lt;td&gt;67.2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;16.6&lt;/td&gt; &#xA;   &lt;td&gt;60.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Minerva&lt;/td&gt; &#xA;   &lt;td&gt;540B&lt;/td&gt; &#xA;   &lt;td&gt;SIFT&lt;/td&gt; &#xA;   &lt;td&gt;58.8&lt;/td&gt; &#xA;   &lt;td&gt;33.6&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flan-PaLM&lt;/td&gt; &#xA;   &lt;td&gt;540B&lt;/td&gt; &#xA;   &lt;td&gt;SIFT&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;70.9&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flan-U-PaLM&lt;/td&gt; &#xA;   &lt;td&gt;540B&lt;/td&gt; &#xA;   &lt;td&gt;SIFT&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;69.8&lt;/td&gt; &#xA;   &lt;td&gt;64.9&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaLM&lt;/td&gt; &#xA;   &lt;td&gt;540B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;56.9&lt;/td&gt; &#xA;   &lt;td&gt;8.8&lt;/td&gt; &#xA;   &lt;td&gt;62.9&lt;/td&gt; &#xA;   &lt;td&gt;62.0&lt;/td&gt; &#xA;   &lt;td&gt;26.2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;50.9&lt;/td&gt; &#xA;   &lt;td&gt;10.6&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;23.7&lt;/td&gt; &#xA;   &lt;td&gt;38.8*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PaLM&lt;/td&gt; &#xA;   &lt;td&gt;64B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;52.4&lt;/td&gt; &#xA;   &lt;td&gt;4.4&lt;/td&gt; &#xA;   &lt;td&gt;49.0&lt;/td&gt; &#xA;   &lt;td&gt;42.3&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon&lt;/td&gt; &#xA;   &lt;td&gt;40B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;49.0*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;35.6&lt;/td&gt; &#xA;   &lt;td&gt;7.1&lt;/td&gt; &#xA;   &lt;td&gt;57.8&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;21.7&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InstructCodeT5+&lt;/td&gt; &#xA;   &lt;td&gt;16B&lt;/td&gt; &#xA;   &lt;td&gt;SIFT&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;35.0&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;11.6&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarCoder&lt;/td&gt; &#xA;   &lt;td&gt;15B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;8.4&lt;/td&gt; &#xA;   &lt;td&gt;15.1&lt;/td&gt; &#xA;   &lt;td&gt;33.9&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;33.6&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;12.2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;SIFT&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;12.9&lt;/td&gt; &#xA;   &lt;td&gt;56.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;17.8&lt;/td&gt; &#xA;   &lt;td&gt;3.9&lt;/td&gt; &#xA;   &lt;td&gt;46.9&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;15.8&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flan-T5&lt;/td&gt; &#xA;   &lt;td&gt;11B&lt;/td&gt; &#xA;   &lt;td&gt;SIFT&lt;/td&gt; &#xA;   &lt;td&gt;16.1*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;48.6&lt;/td&gt; &#xA;   &lt;td&gt;41.4&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alpaca&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;SIFT&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;13.5&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;11.0&lt;/td&gt; &#xA;   &lt;td&gt;2.9&lt;/td&gt; &#xA;   &lt;td&gt;35.1&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;10.5&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flan-T5&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;SIFT&lt;/td&gt; &#xA;   &lt;td&gt;13.5*&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;45.5&lt;/td&gt; &#xA;   &lt;td&gt;35.2&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Base means the pretrained checkpoint. SIFT means the checkpoint after supervised instruction finetuning. RLHF means the checkpoint after Reinforcement Learning from Human Feedback. Numbers marked with an asterisk * are from our own run, otherwise from multiple sources which we explain below. All methods are measured in accuracy, the higher the better.&lt;/p&gt; &#xA;&lt;h3&gt;What&#39;s different than other important evaluation?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://crfm.stanford.edu/helm/latest/&#34;&gt;HeLM&lt;/a&gt; uses answer-only prompting, we use chain-of-thought promoting&lt;/li&gt; &#xA; &lt;li&gt;HeLM evaluates everything. We only focus on complex reasoning, the key differentiator of LLMs&#39; capability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34;&gt;Open LLM Leaderboard&lt;/a&gt; evaluates open-sourced language models. We consider most leading models. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Currently, the performance of LLaMA 65B on Open LLM Leaderboard is just 48.8, which is significantly lower than the 63.4 reported in the paper. This casts &lt;a href=&#34;https://twitter.com/karpathy/status/1662209158748442625&#34;&gt;doubts&lt;/a&gt; on the comparison between LLaMA and Falcon.&lt;/li&gt; &#xA;   &lt;li&gt;In our &lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/MMLU/readme.md&#34;&gt;reproduction&lt;/a&gt;, we got 61.4 using the MMLU official prompt + greedy decoding + fp16. Our results favors the original LLaMA number and cast doublts on the results of Open LLM Leaderboard.&lt;/li&gt; &#xA;   &lt;li&gt;Our &lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/MMLU/run_mmlu_llama.py&#34;&gt;evaluation script&lt;/a&gt; is rather straightforward, most parameters are default, no fancy prompt engineering. We encourage the community to try out our scripts and reproduce our results.&lt;/li&gt; &#xA;   &lt;li&gt;According to &lt;a href=&#34;https://twitter.com/natolambert/status/1667249342456160257?s=20&#34;&gt;Nathan Lambert&lt;/a&gt;, HuggingFace is currently redoing the backend of Open LLM Leaderboard, and the results may change (Jun 10 2023).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://lmsys.org/blog/2023-05-03-arena/&#34;&gt;Chatbot Arena&lt;/a&gt; evaluates chatbot models, which is more user-oriented at deployment. Our evaluation is more developer-oriented, and we consider on not only chatbots but also base models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How the models are ranked&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If we know model scale, we rank it by scale.&lt;/li&gt; &#xA; &lt;li&gt;If we do not know model scale, we rank it by GSM8K, the classical benchmark measuring chain-of-thought math reasoning performance. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This is definitely not the only metric, but a good interpretation is &#34;how good the model can do math while maintaining other generic abilities&#34; -- which is also very hard.&lt;/li&gt; &#xA;   &lt;li&gt;GPT-4 is already pretrained on GSM8k training split, others may not. So for GPT-4, its perf. on GSM8k is in-distribution generalization, while for others are ood. generalization. Yet even for in-dist. FlanT5 is also trained on GSM8k, still shows perf. difference.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Generally it is very hard to rigiously compare model perf. due to multiple factors (whether trained on the corresponding training split, whether trained on code, whether optimize prompt .etc). View our results as approximate reference.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Source of numbers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT-4 from its &lt;a href=&#34;https://openai.com/research/gpt-4&#34;&gt;website&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2303.12712&#34;&gt;Bubeck et al Mar 2023&lt;/a&gt;. Note that the version that Bubeck uses is GPT-4 Early which is supposedly to be more powerful than GPT-4 Launch (OpenAI paid a lot of alignment tax to make GPT-4 safer).&lt;/li&gt; &#xA; &lt;li&gt;*-davinci-00* and *PaLM are from the &lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;Flan-PaLM&lt;/a&gt; paper appendix. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;code-davinci-002 is the base model of GPT-3.5 family but unfortunately it can no longer be accessed.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;LLaMA from &lt;a href=&#34;https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/&#34;&gt;LLaMA&lt;/a&gt; paper. &lt;del&gt;Note that the prompt of LLaMA used in these tasks are not released so reproduction may have varied numbers, see &lt;a href=&#34;https://twitter.com/karpathy/status/1662209158748442625&#34;&gt;this twitter thread&lt;/a&gt; for more discussions.&lt;/del&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;del&gt;We are doing our own implementation of LLaMA on MMLU and BBH. Stay tuned.&lt;/del&gt;&lt;/li&gt; &#xA;   &lt;li&gt;We have reproduced LLaMA on MMLU using the official MMLU prompts and default HuggingFace Transformers &lt;code&gt;generate()&lt;/code&gt; function, and our results matches the official numbers very well. See &lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/MMLU/readme.md&#34;&gt;here&lt;/a&gt; for more details.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Falcon on MMLU is from our own script &lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/MMLU/readme.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;PaLM-2 from &lt;a href=&#34;https://ai.google/static/documents/palm2techreport.pdf&#34;&gt;their tech report&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Claude is from our own test script, see below about how to run it.&lt;/li&gt; &#xA; &lt;li&gt;The HumanEval results for LLaMA models, PaLM and StartCoder are from &lt;a href=&#34;https://huggingface.co/blog/starcoder&#34;&gt;HuggingFace report&lt;/a&gt;. Code-davinci-002&#39;s performance on HumanEval is from &lt;a href=&#34;https://arxiv.org/pdf/2305.07922.pdf&#34;&gt;CodeT5+ paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;C-Eval is from their &lt;a href=&#34;https://cevalbenchmark.com/static/leaderboard.html&#34;&gt;website&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TheoremQA is from their &lt;a href=&#34;https://github.com/wenhuchen/TheoremQA&#34;&gt;github&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SummEdits is from their &lt;a href=&#34;https://github.com/salesforce/factualNLG/tree/master&#34;&gt;github&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2305.14540&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Current results&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPT-4 clearly outperforms all other models on GSM8K and MMLU.&lt;/li&gt; &#xA; &lt;li&gt;**&lt;strong&gt;The 65B LLaMA is very close to text/code-davinci-002, which means that based on it, if SFT and RLHF are done correctly, it is very likely that we could reproduce ChatGPT based on the 65B LLaMA&lt;/strong&gt;**&lt;/li&gt; &#xA; &lt;li&gt;Claude is the only model family that is comparable to GPT family.&lt;/li&gt; &#xA; &lt;li&gt;On GSM8K, gpt-3.5-turbo improves over text-davinci-003. This confirms OpenAI&#39;s Jan 30 2023 release notes &#34;improved mathematical capabilities.&#34;&lt;/li&gt; &#xA; &lt;li&gt;On MMLU, gpt-3.5-turbo is slightly better than text-davinci-003. But this level of margin is NOT SIGNIFICANT&lt;/li&gt; &#xA; &lt;li&gt;Also remember that gpt-3.5-turbo is 10 times cheaper than text-davinci-003&lt;/li&gt; &#xA; &lt;li&gt;Also be careful that GPT-4/ 3.5&#39;s performance on GSM8K is not true few-shot -- in &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4.pdf&#34;&gt;GPT-4 report&lt;/a&gt; they said that they mixed a portion of GSM8K training set to train the model&lt;/li&gt; &#xA; &lt;li&gt;LLaMA performance on MMLU is from their paper and probably not CoT but AO. Generally on MMLU, AO is better than CoT but just slightly better. So the LLaMA numbers on MMLU might be slightly overestimated.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Visualization&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/resources/ranking.png&#34; alt=&#34;Title&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;There is a clear gap between open-source and close.&lt;/li&gt; &#xA; &lt;li&gt;Most top models are after RLHF.&lt;/li&gt; &#xA; &lt;li&gt;LLaMA 65B is very close to code-davinc-002.&lt;/li&gt; &#xA; &lt;li&gt;Existing results strongly suggest that if RLHF is done right on LLaMA, it may be close to ChatGPT-3.5.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;More about the tasks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.11903&#34;&gt;GSM8K&lt;/a&gt;: 8k elementary school math. -- Performance improvements on this dataset directly translate to daily math abilities when interacting with LLMs&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;MMLU&lt;/a&gt;: 15k problems under 57 subjects, high school and college knowledge&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.14858&#34;&gt;MATH&lt;/a&gt; (Hard!): 12k problems within 7 categories, very hard math and natural science. All current models struggle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.09261&#34;&gt;BBH&lt;/a&gt;: 6.5k problems within 23 subsets, symbolic and text reasoning&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/human-eval&#34;&gt;HumanEval&lt;/a&gt;: a classical handwritten dataset of 164 Python problems for evaluating coding capability.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cevalbenchmark.com/&#34;&gt;C-Eval&lt;/a&gt;: a collection of 13k multi-choice questions spanning 52 disciplines of knowledge test in Chinese.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wenhuchen/TheoremQA&#34;&gt;TheoremQA&lt;/a&gt; (Hard!): 800 QA pairs covering 350+ theorems spanning across Math, EE&amp;amp;CS, Physics and Finance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/salesforce/factualNLG&#34;&gt;SummEdits&lt;/a&gt;: 6.3k factual consistency reasoning problems within 10 domains.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;h3&gt;MMLU&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd MMLU&#xA;mkdir outputs&#xA;API_KEY=&amp;lt;your_api_key&amp;gt;&#xA;# GPT-3.5-Turbo&#xA;python run_mmlu_gpt_3.5_turbo.py --api_key=${API_KEY}&#xA;# Claude-v1.3&#xA;python run_mmlu_claude.py --api_key=${API_KEY} --engine=claude-v1.3&#xA;&#xA;# LLaMA&#xA;LLAMA_CKPT_DIR=&amp;lt;path to model checkpoints&amp;gt;&#xA;PARAM_SIZE=65 # 7, 13, 33, 65&#xA;MODEL_TYPE=llama # [&#34;llama&#34;, &#34;falcon&#34;] &#xA;python run_mmlu_open_source.py --ckpt_dir ${LLAMA_CKPT_DIR} --param_size ${PARAM_SIZE} --model_type ${MODEL_TYPE}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GSM8k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd gsm8k &#xA;mkdir outputs&#xA;&#xA;# run gpt-3.5&#xA;# codex_gsm8k_complex.ipynb         -- code-davinci-002 + complex prompt&#xA;# gpt3.5turbo_gsm8k_complex.ipynb   -- gpt-3.5-turbo + complex prompt&#xA;&#xA;# run claude&#xA;python run_gsm8k_claude.py\&#xA;  --anthropic_key=${API_KEY}\&#xA;  --prompt_file=lib_prompt/prompt_original.txt\&#xA;  --engine=claude-v1.3\&#xA;  --output_file=outputs/gsm8k_claude_v1.3_original_test.txt&#xA;&#xA;# run FlanT5&#xA;# flan_t5_11b_gsm8k.ipynb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;BBH&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd BBH&#xA;mkdir outputs&#xA;# then run jupyter notebook to see an example penguins dataset&#xA;cd penguins&#xA;# gpt3.5trubo_penguins_original.ipynb&#xA;&#xA;# Or run the script for all datasets&#xA;API_KEY=&amp;lt;your_api_key&amp;gt;&#xA;TASK=&amp;lt;all | multiple_choice | free_form&amp;gt;&#xA;python run_bbh_gpt_3.5_turbo.py --api_key=${API_KEY} --task=${TASK} # task=all by default&#xA;python run_bbh_claude_v1.3.py --api_key=${API_KEY} --model_index=claude-v1.3 --task=${TASK} # task=all by default&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The sensibility of model performance is very high. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Unfortunately, it is a nature of LLMs. We are currently taking efforts to standardize the prompts (see initial progress &lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/spl/markdown.md&#34;&gt;here&lt;/a&gt;) and will update more on it.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;What are the prompts used in the &lt;em&gt;complexity-based prompting&lt;/em&gt; paper? &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See &lt;code&gt;research/complexity_based_prompting/&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;I want to try some open-sourced model &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See &lt;code&gt;gsm8k/flan_t5_11b_gsm8k.ipynb&lt;/code&gt; for a place to start&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;There are some prompts that have wrong answer &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yes, but we keep it as they are used in the original papers&lt;/li&gt; &#xA;   &lt;li&gt;Generally the model can be robust under prompt perturbation: even if sometimes there are errors in the prompt, as long as the format of the prompt is about the corresponding task, the model tend to only look at the format, ignore the prompt error, and make its own prediction.&lt;/li&gt; &#xA;   &lt;li&gt;See &lt;a href=&#34;https://arxiv.org/abs/2202.12837&#34;&gt;https://arxiv.org/abs/2202.12837&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2212.10001&#34;&gt;https://arxiv.org/abs/2212.10001&lt;/a&gt; about more analysis how the model can ignore errors in the prompt&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;I want to know more about building LLMs for reasoning tasks&lt;/h2&gt; &#xA;&lt;p&gt;A detailed roadmap is discussed in &lt;a href=&#34;https://yaofu.notion.site/Towards-Complex-Reasoning-the-Polaris-of-Large-Language-Models-c2b4a51355b44764975f88e6a42d4e75&#34;&gt;our previous blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Generally, the recipe for building models of strong reasoning is the same as generic LLMs: pretraining, finetuning, reinforcement learning. Here we list some very important papers that should be considered:&lt;/p&gt; &#xA;&lt;h3&gt;Pretraining/ Continue Training&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lewkowycz et. al. 2022. Minerva: &lt;a href=&#34;https://arxiv.org/abs/2206.14858&#34;&gt;Solving Quantitative Reasoning Problems with Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Taylor et. al. 2022. &lt;a href=&#34;https://arxiv.org/abs/2211.09085&#34;&gt;Galactica: A Large Language Model for Science&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Finetuning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chung et. al. 2022. &lt;a href=&#34;https://arxiv.org/abs/2210.11416&#34;&gt;Scaling Instruction-Finetuned Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Li et. al. 2022. &lt;a href=&#34;https://arxiv.org/abs/2203.07814&#34;&gt;Competition-Level Code Generation with AlphaCode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fu et. al. 2023. &lt;a href=&#34;https://arxiv.org/abs/2301.12726&#34;&gt;Specializing Smaller Language Models towards Multi-Step Reasoning&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Reinforcement Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Uesato et. al. 2022. &lt;a href=&#34;https://arxiv.org/abs/2211.14275&#34;&gt;Solving math word problems with process- and outcome-based feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Le et. al. 2022. &lt;a href=&#34;https://arxiv.org/abs/2207.01780&#34;&gt;CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Lightman et. al. 2023. &lt;a href=&#34;https://openai.com/research/improving-mathematical-reasoning-with-process-supervision&#34;&gt;Let’s Verify Step by Step&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Under Development&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/spl/readme.md&#34;&gt;CotHub Standard Prompt Library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/resources/todo.md&#34;&gt;TODOs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/resources/literature.md&#34;&gt;Literature&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FranxYao/chain-of-thought-hub/main/resources/detailed_results.md&#34;&gt;Detailed Results&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>iv-org/invidious</title>
    <updated>2023-06-11T01:31:06Z</updated>
    <id>tag:github.com,2023-06-11:/iv-org/invidious</id>
    <link href="https://github.com/iv-org/invidious" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Invidious is an alternative front-end to YouTube&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/assets/invidious-colored-vector.svg?sanitize=true&#34; width=&#34;192&#34; height=&#34;192&#34; alt=&#34;Invidious logo&#34;&gt; &#xA; &lt;h1&gt;Invidious&lt;/h1&gt; &#xA; &lt;a href=&#34;https://www.gnu.org/licenses/agpl-3.0.en.html&#34;&gt; &lt;img alt=&#34;License: AGPLv3&#34; src=&#34;https://shields.io/badge/License-AGPL%20v3-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/iv-org/invidious/actions&#34;&gt; &lt;img alt=&#34;Build Status&#34; src=&#34;https://github.com/iv-org/invidious/workflows/Invidious%20CI/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/iv-org/invidious/commits/master&#34;&gt; &lt;img alt=&#34;GitHub commits&#34; src=&#34;https://img.shields.io/github/commit-activity/y/iv-org/invidious?color=red&amp;amp;label=commits&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/iv-org/invidious/issues&#34;&gt; &lt;img alt=&#34;GitHub issues&#34; src=&#34;https://img.shields.io/github/issues/iv-org/invidious?color=important&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/iv-org/invidious/pulls&#34;&gt; &lt;img alt=&#34;GitHub pull requests&#34; src=&#34;https://img.shields.io/github/issues-pr/iv-org/invidious?color=blueviolet&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://hosted.weblate.org/engage/invidious/&#34;&gt; &lt;img alt=&#34;Translation Status&#34; src=&#34;https://hosted.weblate.org/widgets/invidious/-/translations/svg-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/humanetech-community/awesome-humane-tech&#34;&gt; &lt;img alt=&#34;Awesome Humane Tech&#34; src=&#34;https://raw.githubusercontent.com/humanetech-community/awesome-humane-tech/main/humane-tech-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;h3&gt;An open source alternative front-end to YouTube&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://invidious.io/&#34;&gt;Website&lt;/a&gt; &amp;nbsp;•&amp;nbsp; &lt;a href=&#34;https://instances.invidious.io/&#34;&gt;Instances list&lt;/a&gt; &amp;nbsp;•&amp;nbsp; &lt;a href=&#34;https://docs.invidious.io/faq/&#34;&gt;FAQ&lt;/a&gt; &amp;nbsp;•&amp;nbsp; &lt;a href=&#34;https://docs.invidious.io/&#34;&gt;Documentation&lt;/a&gt; &amp;nbsp;•&amp;nbsp; &lt;a href=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/#contribute&#34;&gt;Contribute&lt;/a&gt; &amp;nbsp;•&amp;nbsp; &lt;a href=&#34;https://invidious.io/donate/&#34;&gt;Donate&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h5&gt;Chat with us:&lt;/h5&gt; &#xA; &lt;a href=&#34;https://matrix.to/#/#invidious:matrix.org&#34;&gt; &lt;img alt=&#34;Matrix&#34; src=&#34;https://img.shields.io/matrix/invidious:matrix.org?label=Matrix&amp;amp;color=darkgreen&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://web.libera.chat/?channel=#invidious&#34;&gt; &lt;img alt=&#34;Libera.chat (IRC)&#34; src=&#34;https://img.shields.io/badge/IRC%20%28Libera.chat%29-%23invidious-darkgreen&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a rel=&#34;me&#34; href=&#34;https://social.tchncs.de/@invidious&#34;&gt; &lt;img alt=&#34;Fediverse: @invidious@social.tchncs.de&#34; src=&#34;https://img.shields.io/badge/Fediverse-%40invidious%40social.tchncs.de-darkgreen&#34;&gt; &lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;a href=&#34;https://invidious.io/contact/&#34;&gt; &lt;img alt=&#34;E-mail&#34; src=&#34;https://img.shields.io/badge/E%2d%2dmail-darkgreen&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Player&lt;/th&gt; &#xA;   &lt;th&gt;Preferences&lt;/th&gt; &#xA;   &lt;th&gt;Subscriptions&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/01_player.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/02_preferences.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/03_subscriptions.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/04_description.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/05_preferences.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/screenshots/06_subscriptions.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;User features&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lightweight&lt;/li&gt; &#xA; &lt;li&gt;No ads&lt;/li&gt; &#xA; &lt;li&gt;No tracking&lt;/li&gt; &#xA; &lt;li&gt;No JavaScript required&lt;/li&gt; &#xA; &lt;li&gt;Light/Dark themes&lt;/li&gt; &#xA; &lt;li&gt;Customizable homepage&lt;/li&gt; &#xA; &lt;li&gt;Subscriptions independent from Google&lt;/li&gt; &#xA; &lt;li&gt;Notifications for all subscribed channels&lt;/li&gt; &#xA; &lt;li&gt;Audio-only mode (with background play on mobile)&lt;/li&gt; &#xA; &lt;li&gt;Support for Reddit comments&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/locales/&#34;&gt;Available in many languages&lt;/a&gt;, thanks to &lt;a href=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/#contribute&#34;&gt;our translators&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Data import/export&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Import subscriptions from YouTube, NewPipe and Freetube&lt;/li&gt; &#xA; &lt;li&gt;Import watch history from NewPipe&lt;/li&gt; &#xA; &lt;li&gt;Export subscriptions to NewPipe and Freetube&lt;/li&gt; &#xA; &lt;li&gt;Import/Export Invidious user data&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Technical features&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Embedded video support&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.invidious.io/api/&#34;&gt;Developer API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Does not use official YouTube APIs&lt;/li&gt; &#xA; &lt;li&gt;No Contributor License Agreement (CLA)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using invidious:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://instances.invidious.io&#34;&gt;Select a public instance from the list&lt;/a&gt; and start watching videos right now!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hosting invidious:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.invidious.io/installation/&#34;&gt;Follow the installation instructions&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The full documentation can be accessed online at &lt;a href=&#34;https://docs.invidious.io/&#34;&gt;https://docs.invidious.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The documentation&#39;s source code is available in this repository: &lt;a href=&#34;https://github.com/iv-org/documentation&#34;&gt;https://github.com/iv-org/documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Extensions&lt;/h3&gt; &#xA;&lt;p&gt;We highly recommend the use of &lt;a href=&#34;https://github.com/SimonBrazell/privacy-redirect#get&#34;&gt;Privacy Redirect&lt;/a&gt;, a browser extension that automatically redirects Youtube URLs to any Invidious instance and replaces embedded youtube videos on other websites with invidious.&lt;/p&gt; &#xA;&lt;p&gt;The documentation contains a list of browser extensions that we recommended to use along with Invidious.&lt;/p&gt; &#xA;&lt;p&gt;You can read more here: &lt;a href=&#34;https://docs.invidious.io/applications/&#34;&gt;https://docs.invidious.io/applications/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;h3&gt;Code&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork it ( &lt;a href=&#34;https://github.com/iv-org/invidious/fork&#34;&gt;https://github.com/iv-org/invidious/fork&lt;/a&gt; ).&lt;/li&gt; &#xA; &lt;li&gt;Create your feature branch (&lt;code&gt;git checkout -b my-new-feature&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Stage your files (&lt;code&gt;git add .&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Commit your changes (&lt;code&gt;git commit -am &#39;Add some feature&#39;&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Push to the branch (&lt;code&gt;git push origin my-new-feature&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Create a new pull request ( &lt;a href=&#34;https://github.com/iv-org/invidious/compare&#34;&gt;https://github.com/iv-org/invidious/compare&lt;/a&gt; ).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Translations&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://weblate.org&#34;&gt;Weblate&lt;/a&gt; to manage Invidious translations.&lt;/p&gt; &#xA;&lt;p&gt;You can suggest new translations and/or correction here: &lt;a href=&#34;https://hosted.weblate.org/engage/invidious/&#34;&gt;https://hosted.weblate.org/engage/invidious/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Creating an account is not required, but recommended, especially if you want to contribute regularly. Weblate also allows you to log-in with major SSO providers like Github, Gitlab, BitBucket, Google, ...&lt;/p&gt; &#xA;&lt;h2&gt;Projects using Invidious&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/FreeTubeApp/FreeTube&#34;&gt;FreeTube&lt;/a&gt;: A libre software YouTube app for privacy.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sr.ht/~cadence/tube/&#34;&gt;CloudTube&lt;/a&gt;: A JavaScript-rich alternate YouTube player.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gitlab.com/Cha_de_L/peertubeify&#34;&gt;PeerTubeify&lt;/a&gt;: On YouTube, displays a link to the same video on PeerTube, if it exists.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deep-gaurav/MusicPiped&#34;&gt;MusicPiped&lt;/a&gt;: A material design music player that streams music from YouTube.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/stephane-r/holoplay-wa&#34;&gt;HoloPlay&lt;/a&gt;: Progressive Web App connecting on Invidious API&#39;s with search, playlists and favorites.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/WatchTubeTeam/WatchTube&#34;&gt;WatchTube&lt;/a&gt;: Powerful YouTube client for Apple Watch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yattee/yattee&#34;&gt;Yattee&lt;/a&gt;: Alternative YouTube frontend for iPhone, iPad, Mac and Apple TV.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://codeberg.org/777/TubiTui&#34;&gt;TubiTui&lt;/a&gt;: A lightweight, libre, TUI-based YouTube client.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pystardust/ytfzf&#34;&gt;Ytfzf&lt;/a&gt;: A posix script to find and watch youtube videos from the terminal. (Without API)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/iBicha/playlet&#34;&gt;Playlet&lt;/a&gt;: Unofficial Youtube client for Roku TV&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lamarios/clipious&#34;&gt;Clipious&lt;/a&gt;: Unofficial Invidious client for Android&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Liability&lt;/h2&gt; &#xA;&lt;p&gt;We take no responsibility for the use of our tool, or external instances provided by third parties. We strongly recommend you abide by the valid official regulations in your country. Furthermore, we refuse liability for any inappropriate use of Invidious, such as illegal downloading. This tool is provided to you in the spirit of free, open software.&lt;/p&gt; &#xA;&lt;p&gt;You may view the LICENSE in which this software is provided to you &lt;a href=&#34;https://raw.githubusercontent.com/iv-org/invidious/master/LICENSE&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;ol start=&#34;16&#34;&gt; &#xA;  &lt;li&gt;Limitation of Liability.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
  <entry>
    <title>reactive-python/reactpy</title>
    <updated>2023-06-11T01:31:06Z</updated>
    <id>tag:github.com,2023-06-11:/reactive-python/reactpy</id>
    <link href="https://github.com/reactive-python/reactpy" rel="alternate"></link>
    <summary type="html">&lt;p&gt;It&#39;s React, but in Python&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://raw.githubusercontent.com/reactive-python/reactpy/main/branding/svg/reactpy-logo-square.svg?sanitize=true&#34; align=&#34;left&#34; height=&#34;45&#34;&gt; ReactPy&lt;/h1&gt; &#xA;&lt;p&gt; &lt;a href=&#34;https://github.com/reactive-python/reactpy/actions&#34;&gt; &lt;img src=&#34;https://github.com/reactive-python/reactpy/workflows/test/badge.svg?event=push&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/reactpy/&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/reactpy.svg?label=PyPI&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/reactive-python/reactpy/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-MIT-purple.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://reactpy.dev/&#34;&gt; &lt;img src=&#34;https://img.shields.io/website?down_message=offline&amp;amp;label=Docs&amp;amp;logo=read-the-docs&amp;amp;logoColor=white&amp;amp;up_message=online&amp;amp;url=https%3A%2F%2Freactpy.dev%2Fdocs%2Findex.html&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/uNb5P4hA9X&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/1111078259854168116?label=Discord&amp;amp;logo=discord&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://reactpy.dev/&#34;&gt;ReactPy&lt;/a&gt; is a library for building user interfaces in Python without Javascript. ReactPy interfaces are made from components that look and behave similar to those found in &lt;a href=&#34;https://reactjs.org/&#34;&gt;ReactJS&lt;/a&gt;. Designed with simplicity in mind, ReactPy can be used by those without web development experience while also being powerful enough to grow with your ambitions.&lt;/p&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;2&#34; style=&#34;text-align: center&#34;&gt;Supported Backends&lt;/th&gt; &#xA;  &lt;/tr&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th style=&#34;text-align: center&#34;&gt;Built-in&lt;/th&gt; &#xA;   &lt;th style=&#34;text-align: center&#34;&gt;External&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://reactpy.dev/docs/guides/getting-started/installing-reactpy.html#officially-supported-servers&#34;&gt; Flask, FastAPI, Sanic, Tornado &lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://github.com/reactive-python/reactpy-django&#34;&gt;Django&lt;/a&gt;, &lt;a href=&#34;https://github.com/reactive-python/reactpy-jupyter&#34;&gt;Jupyter&lt;/a&gt;, &lt;a href=&#34;https://github.com/idom-team/idom-dash&#34;&gt;Plotly-Dash&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;At a Glance&lt;/h1&gt; &#xA;&lt;p&gt;To get a rough idea of how to write apps in ReactPy, take a look at this tiny &lt;em&gt;Hello World&lt;/em&gt; application.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from reactpy import component, html, run&#xA;&#xA;@component&#xA;def hello_world():&#xA;    return html.h1(&#34;Hello, World!&#34;)&#xA;&#xA;run(hello_world)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;p&gt;Follow the links below to find out more about this project.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mybinder.org/v2/gh/reactive-python/reactpy-jupyter/main?urlpath=lab/tree/notebooks/introduction.ipynb&#34;&gt;Try ReactPy (Jupyter Notebook)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://reactpy.dev/&#34;&gt;Documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reactive-python/reactpy/discussions&#34;&gt;GitHub Discussions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/uNb5P4hA9X&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://reactpy.dev/docs/about/contributor-guide.html&#34;&gt;Contributor Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reactive-python/reactpy/raw/main/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>