<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-23T01:30:39Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ykdojo/kaguya</title>
    <updated>2023-06-23T01:30:39Z</updated>
    <id>tag:github.com,2023-06-23:/ykdojo/kaguya</id>
    <link href="https://github.com/ykdojo/kaguya" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A ChatGPT plugin that allows you to load and edit your local files in a controlled way, as well as run any Python, JavaScript, and bash script.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Kaguya&lt;/h1&gt; &#xA;&lt;p&gt;Kaguya is a ChatGPT plugin that allows you to load and edit your local files in a controlled way, as well as run any Python, JavaScript, and bash script. This makes it a powerful tool for developers, enabling them to interact with their file system and run scripts directly from ChatGPT.&lt;/p&gt; &#xA;&lt;h2&gt;API Endpoints&lt;/h2&gt; &#xA;&lt;p&gt;The project provides several API endpoints that allow you to interact with the file system. The API is described in the &lt;code&gt;openapi.yaml&lt;/code&gt; file. Here is a brief overview:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/executeCommand&lt;/code&gt;: Execute a shell command.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GET /api/listFilesInDirectory&lt;/code&gt;: List files and directories in the specified directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;GET /api/readFile&lt;/code&gt;: Read the content of a file in the user&#39;s directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/update&lt;/code&gt;: Update a file in the user&#39;s directory by performing a search-and-replace operation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/updateWholeFile&lt;/code&gt;: Replace the entire content of a file in the user&#39;s directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/createFile&lt;/code&gt;: Create a new file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/deleteFile&lt;/code&gt;: Delete a file in the user&#39;s directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/renameFile&lt;/code&gt;: Rename a file in the user&#39;s directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/appendToFile&lt;/code&gt;: Append content to the end of an existing file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/createDirectory&lt;/code&gt;: Create a new directory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/deleteDirectory&lt;/code&gt;: Delete a directory and its contents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;POST /api/readMultipleFiles&lt;/code&gt;: Read the content of multiple files.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running the Project&lt;/h2&gt; &#xA;&lt;p&gt;You can run the project using Docker. Simply execute the &lt;code&gt;docker.sh&lt;/code&gt; script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After running the script, you can interact with Kaguya through ChatGPT using the localhost port.&lt;/p&gt; &#xA;&lt;h2&gt;More About Kaguya&lt;/h2&gt; &#xA;&lt;p&gt;You can check out a demo of Kaguya in action on Twitter: &lt;a href=&#34;https://twitter.com/ykdojo/status/1645846044843077635&#34;&gt;Demo Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can check out a second demo of Kaguya in action on Twitter: &lt;a href=&#34;https://twitter.com/ykdojo/status/1670848611532562433&#34;&gt;Second Demo Video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Discord&lt;/h2&gt; &#xA;&lt;p&gt;Join our Discord server &lt;a href=&#34;https://discord.com/invite/nNtVfKddDD&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>linux-china/chatgpt-spring-boot-starter</title>
    <updated>2023-06-23T01:30:39Z</updated>
    <id>tag:github.com,2023-06-23:/linux-china/chatgpt-spring-boot-starter</id>
    <link href="https://github.com/linux-china/chatgpt-spring-boot-starter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Spring Boot ChatGPT Starter&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT Spring Boot Starter&lt;/h1&gt; &#xA;&lt;p&gt;Spring Boot ChatGPT starter with ChatGPT chat and functions support.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Base on Spring Boot 3.0+&lt;/li&gt; &#xA; &lt;li&gt;Async with Spring Webflux&lt;/li&gt; &#xA; &lt;li&gt;Support ChatGPT Chat Stream&lt;/li&gt; &#xA; &lt;li&gt;Support ChatGPT functions&lt;/li&gt; &#xA; &lt;li&gt;No third-party library: base on Spring 6 HTTP interface&lt;/li&gt; &#xA; &lt;li&gt;GraalVM native image support&lt;/li&gt; &#xA; &lt;li&gt;Azure OpenAI support&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Get Started&lt;/h1&gt; &#xA;&lt;h3&gt;Add dependency&lt;/h3&gt; &#xA;&lt;p&gt;Add &lt;code&gt;chatgpt-spring-boot-starter&lt;/code&gt; dependency in your pom.xml.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&#xA;&amp;lt;dependency&amp;gt;&#xA;    &amp;lt;groupId&amp;gt;org.mvnsearch&amp;lt;/groupId&amp;gt;&#xA;    &amp;lt;artifactId&amp;gt;chatgpt-spring-boot-starter&amp;lt;/artifactId&amp;gt;&#xA;    &amp;lt;version&amp;gt;0.3.0&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Add configuration&lt;/h3&gt; &#xA;&lt;p&gt;Add &lt;code&gt;openai.api.key&lt;/code&gt; in &lt;code&gt;application.properties&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;# OpenAI API Token, or you can set environment variable OPENAI_API_KEY&#xA;openai.api.key=sk-xxxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use Azure OpenAI, you can add &lt;code&gt;openai.api.url&lt;/code&gt; in &lt;code&gt;application.properties&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-properties&#34;&gt;openai.api.key=1138xxxx9037&#xA;openai.api.url=https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2023-05-15&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Call ChatGPT Service&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;&#xA;@RestController&#xA;public class ChatRobotController {&#xA;    @Autowired&#xA;    private ChatGPTService chatGPTService;&#xA;&#xA;    @PostMapping(&#34;/chat&#34;)&#xA;    public Mono&amp;lt;String&amp;gt; chat(@RequestBody String content) {&#xA;        return chatGPTService.chat(ChatCompletionRequest.of(content))&#xA;                .map(ChatCompletionResponse::getReplyText);&#xA;    }&#xA;&#xA;    @GetMapping(&#34;/stream-chat&#34;)&#xA;    public Flux&amp;lt;String&amp;gt; streamChat(@RequestParam String content) {&#xA;        return chatGPTService.stream(ChatCompletionRequest.of(content))&#xA;                .map(ChatCompletionResponse::getReplyText);&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;ChatGPT Interface&lt;/h1&gt; &#xA;&lt;p&gt;ChatGPT service interface is almost like &lt;a href=&#34;https://docs.spring.io/spring-framework/reference/integration/rest-clients.html#rest-http-interface&#34;&gt;Spring 6 HTTP Interface&lt;/a&gt;. You can declare a ChatGPT service interface with &lt;code&gt;@ChatGPTExchange&lt;/code&gt; annotation, and declare completion methods with &lt;code&gt;@ChatCompletion&lt;/code&gt; annotation, then you just call service interface directly.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;&#xA;@GPTExchange&#xA;public interface GPTHelloService {&#xA;&#xA;    @ChatCompletion(&#34;You are a language translator, please translate the below text to Chinese.\n&#34;)&#xA;    Mono&amp;lt;String&amp;gt; translateIntoChinese(String text);&#xA;&#xA;    @ChatCompletion(&#34;You are a language translator, please translate the below text from {0} to {1}.\n {2}&#34;)&#xA;    Mono&amp;lt;String&amp;gt; translate(String sourceLanguage, String targetLanguage, String text);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create ChatGPT interface service bean:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    @Bean&#xA;    public GPTHelloService gptHelloService(ChatGPTServiceProxyFactory proxyFactory) {&#xA;        return proxyFactory.createClient(GPTHelloService.class);&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;ChatGPT functions&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a Spring Bean with &lt;code&gt;@Component&lt;/code&gt; and implement &lt;code&gt;GPTFunctionsStub&lt;/code&gt; interface. Annotate GPT functions with &lt;code&gt;@GPTFunction&lt;/code&gt; annotation, and annotate function parameters with &lt;code&gt;@Parameter&lt;/code&gt; annotation. &lt;code&gt;@Nonnull&lt;/code&gt; means that the parameter is required.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;&#xA;import jakarta.annotation.Nonnull;&#xA;&#xA;@Component&#xA;public class GPTFunctions implements GPTFunctionsStub {&#xA;&#xA;    public record SendEmailRequest(&#xA;            @Nonnull @Parameter(&#34;Recipients of email&#34;) List&amp;lt;String&amp;gt; recipients,&#xA;            @Nonnull @Parameter(&#34;Subject of email&#34;) String subject,&#xA;            @Parameter(&#34;Content of email&#34;) String content) {&#xA;    }&#xA;&#xA;    @GPTFunction(name = &#34;send_email&#34;, value = &#34;Send email to receiver&#34;)&#xA;    public String sendEmail(SendEmailRequest request) {&#xA;        System.out.println(&#34;Recipients: &#34; + String.join(&#34;,&#34;, request.recipients));&#xA;        System.out.println(&#34;Subject: &#34; + request.subject);&#xA;        System.out.println(&#34;Content:\n&#34; + request.content);&#xA;        return &#34;Email sent to &#34; + String.join(&#34;,&#34;, request.recipients) + &#34; successfully!&#34;;&#xA;    }&#xA;&#xA;    public record SQLQueryRequest(&#xA;            @Parameter(required = true, value = &#34;SQL to query&#34;) String sql) {&#xA;    }&#xA;&#xA;    @GPTFunction(name = &#34;execute_sql_query&#34;, value = &#34;Execute SQL query and return the result set&#34;)&#xA;    public String executeSQLQuery(SQLQueryRequest request) {&#xA;        System.out.println(&#34;Execute SQL: &#34; + request.sql);&#xA;        return &#34;id, name, salary\n1,Jackie,8000\n2,Libing,78000\n3,Sam,7500&#34;;&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Call GPT function by &lt;code&gt;response.getReplyCombinedText()&lt;/code&gt; or &lt;code&gt;chatMessage.getFunctionCall().getFunctionStub().call()&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public class ChatGPTServiceImplTest {&#xA;    @Test&#xA;    public void testChatWithFunctions() throws Exception {&#xA;        final String prompt = &#34;Hi Jackie, could you write an email to Libing(libing.chen@gmail.com) and Sam(linux_china@hotmail.com) and invite them to join Mike&#39;s birthday party at 4 pm tomorrow? Thanks!&#34;;&#xA;        final ChatCompletionRequest request = ChatCompletionRequest.functions(prompt, List.of(&#34;send_email&#34;));&#xA;        final ChatCompletionResponse response = chatGPTService.chat(request).block();&#xA;        // display reply combined text with function call&#xA;        System.out.println(response.getReplyCombinedText());&#xA;        // call function manually&#xA;        for (ChatMessage chatMessage : response.getReply()) {&#xA;            final FunctionCall functionCall = chatMessage.getFunctionCall();&#xA;            if (functionCall != null) {&#xA;                final Object result = functionCall.getFunctionStub().call();&#xA;                System.out.println(result);&#xA;            }&#xA;        }&#xA;    }&#xA;&#xA;    @Test&#xA;    public void testExecuteSQLQuery() {&#xA;        String context = &#34;You are SQL developer. Write SQL according to requirements, and execute it in MySQL database.&#34;;&#xA;        final String prompt = &#34;Query all employees whose salary is greater than the average.&#34;;&#xA;        final ChatCompletionRequest request = ChatCompletionRequest.functions(prompt, List.of(&#34;execute_sql_query&#34;));&#xA;        // add prompt context as system message&#xA;        request.addMessage(ChatMessage.systemMessage(context));&#xA;        final ChatCompletionResponse response = chatGPTService.chat(request).block();&#xA;        System.out.println(response.getReplyCombinedText());&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;code&gt;@GPTExchange&lt;/code&gt; and &lt;code&gt;@ChatCompletion&lt;/code&gt; has functions built-in, so you just need to fill functions parameters.&lt;/p&gt; &#xA;&lt;h3&gt;ChatGPT Functions use cases:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Structure Output: such as SQL, JSON, CSV, YAML etc., then delegate functions to process them.&lt;/li&gt; &#xA; &lt;li&gt;Commands: such as send_email, post on Twitter.&lt;/li&gt; &#xA; &lt;li&gt;DevOps: such as generate K8S yaml file, then call K8S functions to deploy it.&lt;/li&gt; &#xA; &lt;li&gt;Search Matching: bind search with functions, such as search for a book, then call function to show it.&lt;/li&gt; &#xA; &lt;li&gt;Spam detection: email spam, advertisement spam etc&lt;/li&gt; &#xA; &lt;li&gt;PipeLine: you can think function as a node in pipeline. After process by function, and you can pass it to ChatGPT again.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to have a simple test for ChatGPT functions, you can install &lt;a href=&#34;https://plugins.jetbrains.com/plugin/21671-chatgpt-with-markdown&#34;&gt;ChatGPT with Markdown JetBrains IDE Plugin&lt;/a&gt;, and take a look at &lt;a href=&#34;https://raw.githubusercontent.com/linux-china/chatgpt-spring-boot-starter/main/chat.gpt&#34;&gt;chat.gpt file&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ&lt;/h1&gt; &#xA;&lt;h3&gt;OpenAI REST API proxy&lt;/h3&gt; &#xA;&lt;p&gt;Please refer &lt;a href=&#34;https://raw.githubusercontent.com/linux-china/chatgpt-spring-boot-starter/main/src/test/java/org/mvnsearch/chatgpt/demo/OpenAIProxyController.java&#34;&gt;OpenAIProxyController&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;&#xA;@RestController&#xA;public class OpenAIProxyController {&#xA;    @Autowired&#xA;    private OpenAIChatAPI openAIChatAPI;&#xA;&#xA;    @PostMapping(&#34;/v1/chat/completions&#34;)&#xA;    public Publisher&amp;lt;ChatCompletionResponse&amp;gt; completions(@RequestBody ChatCompletionRequest request) {&#xA;        return openAIChatAPI.proxy(request);&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Of course, you can use standard URL &lt;code&gt;http://localhost:8080/v1/chat/completions&lt;/code&gt; to call Azure OpenAI API.&lt;/p&gt; &#xA;&lt;h3&gt;Prompt templates&lt;/h3&gt; &#xA;&lt;p&gt;How to manage prompts in Java? Now my suggestion is to adopt properties file format, and use MessageFormat to format. Please take a look at &lt;a href=&#34;https://raw.githubusercontent.com/linux-china/chatgpt-spring-boot-starter/main/src/test/java/org/mvnsearch/chatgpt/demo/service/PromptManager.java&#34;&gt;PromptManager&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;OpenAI chat API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.spring.io/spring-boot/docs/current/reference/html/&#34;&gt;Spring Boot 3.0+&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.spring.io/spring-framework/reference/web/webflux.html&#34;&gt;Spring Boot Webflux&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.spring.io/spring-framework/reference/integration/rest-clients.html#rest-http-interface&#34;&gt;Spring 6 HTTP interface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/cd/E23095_01/Platform.93/ATGProgGuide/html/s0204propertiesfileformat01.html&#34;&gt;Properties File Format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/text/MessageFormat.html&#34;&gt;MessageFormat JavaDoc&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>BradyFU/Awesome-Multimodal-Large-Language-Models</title>
    <updated>2023-06-23T01:30:39Z</updated>
    <id>tag:github.com,2023-06-23:/BradyFU/Awesome-Multimodal-Large-Language-Models</id>
    <link href="https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Latest Papers and Datasets on Multimodal Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome-Multimodal-Large-Language-Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;font size=&#34;6&#34;&gt;&lt;/font&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA; &lt;font size=&#34;6&#34;&gt;&lt;big&gt;&lt;b&gt; &lt;a href=&#34;https://awesome.re&#34;&gt;&lt;img src=&#34;https://awesome.re/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt; &lt;/b&gt;&lt;/big&gt;&lt;/font&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸ”¥ðŸ”¥ðŸ”¥ A curated list of &lt;b&gt;Multimodal Large Language Models (MLLM)&lt;/b&gt;, including &lt;b&gt;datasets&lt;/b&gt;, &lt;b&gt;multimodal instruction tuning&lt;/b&gt;, &lt;b&gt;multimodal in-context learning&lt;/b&gt;, &lt;b&gt;multimodal chain-of-thought&lt;/b&gt;, &lt;b&gt;llm-aided visual reasoning&lt;/b&gt;, &lt;b&gt;foundation models&lt;/b&gt;, and &lt;b&gt;others&lt;/b&gt;.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ”¥ðŸ”¥ðŸ”¥ This list will be updated in real time.&lt;/p&gt; &#xA;&lt;p&gt;ðŸ”¥ðŸ”¥ðŸ”¥ A survey paper on MLLM is preparing and will be released soon!&lt;/p&gt; &#xA;&lt;p&gt;Welcome to join our WeChat group of MLLM communication!&lt;/p&gt; &#xA;&lt;p&gt;Please add WeChat ID (xjtupanda) to join the group.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;font size=&#34;5&#34;&gt;&lt;/font&gt;&lt;/p&gt;&#xA;&lt;center&gt;&#xA; &lt;font size=&#34;5&#34;&gt;&lt;b&gt; Table of Contents &lt;/b&gt; &lt;/font&gt;&#xA;&lt;/center&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#awesome-papers&#34;&gt;Awesome Papers&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#multimodal-instruction-tuning&#34;&gt;Multimodal Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#multimodal-in-context-learning&#34;&gt;Multimodal In-Context Learning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#multimodal-chain-of-thought&#34;&gt;Multimodal Chain-of-Thought&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#llm-aided-visual-reasoning&#34;&gt;LLM-Aided Visual Reasoning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#foundation-models&#34;&gt;Foundation Models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#others&#34;&gt;Others&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#awesome-datasets&#34;&gt;Awesome Datasets&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#datasets-of-pre-training-for-alignment&#34;&gt;Datasets of Pre-Training for Alignment&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#datasets-of-multimodal-instruction-tuning&#34;&gt;Datasets of Multimodal Instruction Tuning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#datasets-of-in-context-learning&#34;&gt;Datasets of In-Context Learning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#datasets-of-multimodal-chain-of-thought&#34;&gt;Datasets of Multimodal Chain-of-Thought&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/BradyFU/Awesome-Multimodal-Large-Language-Models/main/#others-1&#34;&gt;Others&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Awesome Papers&lt;/h1&gt; &#xA;&lt;h2&gt;Multimodal Instruction Tuning&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Title&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Venue&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenLAMM/LAMM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2306.09093.pdf&#34;&gt;&lt;strong&gt;Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lyuchenyang/Macaw-LLM&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenLAMM/LAMM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2306.06687.pdf&#34;&gt;&lt;strong&gt;LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenLAMM/LAMM&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/openlamm/LAMM&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mbzuai-oryx/Video-ChatGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2306.05424.pdf&#34;&gt;&lt;strong&gt;Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://www.ival-mbzuai.com/video-chatgpt&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Luodian/Otter.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2306.05425.pdf&#34;&gt;&lt;strong&gt;MIMIC-IT: Multi-Modal In-Context Instruction Tuning&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://otter.cliangyu.com/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.04387.pdf&#34;&gt;&lt;strong&gt;M&lt;sup&gt;3&lt;/sup&gt;IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/DAMO-NLP-SG/Video-LLaMA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2306.02858.pdf&#34;&gt;&lt;strong&gt;Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DAMO-NLP-SG/Video-LLaMA&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/DAMO-NLP-SG/Video-LLaMA&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/LLaVA-Med.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2306.00890.pdf&#34;&gt;&lt;strong&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/LLaVA-Med&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/StevenGrove/GPT4Tools.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.18752.pdf&#34;&gt;&lt;strong&gt;GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/StevenGrove/GPT4Tools&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/stevengrove/GPT4Tools&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yxuansu/PandaGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.16355.pdf&#34;&gt;&lt;strong&gt;PandaGPT: One Model To Instruction-Follow Them All&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yxuansu/PandaGPT&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/GMFTBY/PandaGPT&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/joez17/ChatBridge.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.16103.pdf&#34;&gt;&lt;strong&gt;ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/joez17/ChatBridge&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/luogen1996/LaVIN.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.15023.pdf&#34;&gt;&lt;strong&gt;Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/luogen1996/LaVIN&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OptimalScale/DetGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.14167.pdf&#34;&gt;&lt;strong&gt;DetGPT: Detect What You Need via Reasoning&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OptimalScale/DetGPT&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://d3c431c0c77b1d9010.gradio.live/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/VisionLLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.11175.pdf&#34;&gt;&lt;strong&gt;VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenGVLab/VisionLLM&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://igpt.opengvlab.com/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/YuanGongND/ltu.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.10790.pdf&#34;&gt;&lt;strong&gt;Listen, Think, and Understand&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/YuanGongND/ltu&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/YuanGongND/ltu&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/THUDM/VisualGLM-6B.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;strong&gt;VisualGLM-6B&lt;/strong&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/THUDM/VisualGLM-6B&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/xiaoman-zhang/PMC-VQA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.10415.pdf&#34;&gt;&lt;strong&gt;PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/xiaoman-zhang/PMC-VQA&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.06500.pdf&#34;&gt;&lt;strong&gt;InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-11&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/instructblip&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/OpenGVLab/Ask-Anything.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.06355.pdf&#34;&gt;&lt;strong&gt;VideoChat: Chat-Centric Video Understanding&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenGVLab/Ask-Anything&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://ask.opengvlab.com/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/open-mmlab/Multimodal-GPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.04790.pdf&#34;&gt;&lt;strong&gt;MultiModal-GPT: A Vision and Language Model for Dialogue with Humans&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/open-mmlab/Multimodal-GPT&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://mmgpt.openmmlab.org.cn/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/phellonchen/X-LLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.04160.pdf&#34;&gt;&lt;strong&gt;X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-07&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/phellonchen/X-LLM&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/YunxinLi/LingCloud.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.03701.pdf&#34;&gt;&lt;strong&gt;LMEye: An Interactive Perception Network for Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-05&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/YunxinLi/LingCloud&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ZrrSkywalker/LLaMA-Adapter.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.15010.pdf&#34;&gt;&lt;strong&gt;LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04-28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZrrSkywalker/LLaMA-Adapter&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;http://llama-adapter.opengvlab.com/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/X-PLUG/mPLUG-Owl.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.14178.pdf&#34;&gt;&lt;strong&gt;mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04-27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/X-PLUG/mPLUG-Owl&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/MAGAer13/mPLUG-Owl&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Vision-CAIR/MiniGPT-4.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.10592.pdf&#34;&gt;&lt;strong&gt;MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04-20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/haotian-liu/LLaVA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.08485.pdf&#34;&gt;&lt;strong&gt;Visual Instruction Tuning&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04-17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;GitHub&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://llava.hliu.cc/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ZrrSkywalker/LLaMA-Adapter.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.16199.pdf&#34;&gt;&lt;strong&gt;LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZrrSkywalker/LLaMA-Adapter&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/csuhan/LLaMA-Adapter&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/VT-NLP/MultiInstruct.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2212.10773.pdf&#34;&gt;&lt;strong&gt;MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACL&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-12-21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/VT-NLP/MultiInstruct&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Multimodal In-Context Learning&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Title&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Venue&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Luodian/Otter.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2306.05425.pdf&#34;&gt;&lt;strong&gt;MIMIC-IT: Multi-Modal In-Context Instruction Tuning&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://otter.cliangyu.com/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/lupantech/chameleon-llm.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.09842.pdf&#34;&gt;&lt;strong&gt;Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04-19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lupantech/chameleon-llm&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://chameleon-llm.github.io/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.17580.pdf&#34;&gt;&lt;strong&gt;HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/JARVIS&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/microsoft/HuggingGPT&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/MM-REACT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.11381.pdf&#34;&gt;&lt;strong&gt;MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/MM-REACT&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/microsoft-cognitive-service/mm-react&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/MILVLG/prophet.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.01903.pdf&#34;&gt;&lt;strong&gt;Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CVPR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/MILVLG/prophet&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/allenai/visprog.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf&#34;&gt;&lt;strong&gt;Visual Programming: Compositional visual reasoning without training&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CVPR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-11-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/allenai/visprog&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/PICa.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://ojs.aaai.org/index.php/AAAI/article/download/20215/19974&#34;&gt;&lt;strong&gt;An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;AAAI&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-06-28&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/PICa&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/mlfoundations/open_flamingo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2204.14198.pdf&#34;&gt;&lt;strong&gt;Flamingo: a Visual Language Model for Few-Shot Learning&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NeurIPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-04-29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mlfoundations/open_flamingo&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/dhansmair/flamingo-mini-cap&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2106.13884.pdf&#34;&gt;&lt;strong&gt;Multimodal Few-Shot Learning with Frozen Language Models&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NeurIPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2021-06-25&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Multimodal Chain-of-Thought&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Title&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Venue&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/EmbodiedGPT/EmbodiedGPT_Pytorch.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.15021.pdf&#34;&gt;&lt;strong&gt;EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.13903.pdf&#34;&gt;&lt;strong&gt;Letâ€™s Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ttengwang/Caption-Anything.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.02677.pdf&#34;&gt;&lt;strong&gt;Caption Anything: Interactive Image Description with Diverse Multimodal Controls&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ttengwang/Caption-Anything&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/TencentARC/Caption-Anything&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.02317.pdf&#34;&gt;&lt;strong&gt;Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/dannyrose30/VCOT&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/lupantech/chameleon-llm.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.09842.pdf&#34;&gt;&lt;strong&gt;Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04-19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lupantech/chameleon-llm&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://chameleon-llm.github.io/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.07919.pdf&#34;&gt;&lt;strong&gt;Chain of Thought Prompt Tuning in Vision Language Models&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04-16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/MM-REACT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.11381.pdf&#34;&gt;&lt;strong&gt;MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/MM-REACT&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/microsoft-cognitive-service/mm-react&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/TaskMatrix.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.04671.pdf&#34;&gt;&lt;strong&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/TaskMatrix&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/microsoft/visual_chatgpt&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/amazon-science/mm-cot.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2302.00923.pdf&#34;&gt;&lt;strong&gt;Multimodal Chain-of-Thought Reasoning in Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-02-02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/amazon-science/mm-cot&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/allenai/visprog.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf&#34;&gt;&lt;strong&gt;Visual Programming: Compositional visual reasoning without training&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CVPR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-11-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/allenai/visprog&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/lupantech/ScienceQA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf&#34;&gt;&lt;strong&gt;Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NeurIPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-09-20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lupantech/ScienceQA&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;LLM-Aided Visual Reasoning&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Title&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Venue&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.11732.pdf&#34;&gt;&lt;strong&gt;Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/StevenGrove/GPT4Tools.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.18752.pdf&#34;&gt;&lt;strong&gt;GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/StevenGrove/GPT4Tools&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://c60eb7e9400930f31b.gradio.live/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/weixi-feng/LayoutGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.15393.pdf&#34;&gt;&lt;strong&gt;LayoutGPT: Compositional Visual Planning and Generation with Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/weixi-feng/LayoutGPT&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Hxyou/IdealGPT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.14985.pdf&#34;&gt;&lt;strong&gt;IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Hxyou/IdealGPT&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/matrix-alpha/Accountable-Textual-Visual-Chat.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.05983.pdf&#34;&gt;&lt;strong&gt;Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-10&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/matrix-alpha/Accountable-Textual-Visual-Chat&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ttengwang/Caption-Anything.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.02677.pdf&#34;&gt;&lt;strong&gt;Caption Anything: Interactive Image Description with Diverse Multimodal Controls&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ttengwang/Caption-Anything&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/TencentARC/Caption-Anything&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/lupantech/chameleon-llm.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2304.09842.pdf&#34;&gt;&lt;strong&gt;Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-04-19&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lupantech/chameleon-llm&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://chameleon-llm.github.io/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/JARVIS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.17580.pdf&#34;&gt;&lt;strong&gt;HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/JARVIS&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/microsoft/HuggingGPT&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/MM-REACT.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.11381.pdf&#34;&gt;&lt;strong&gt;MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/MM-REACT&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/microsoft-cognitive-service/mm-react&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/cvlab-columbia/viper.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.08128.pdf&#34;&gt;&lt;strong&gt;ViperGPT: Visual Inference via Python Execution for Reasoning&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-14&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/cvlab-columbia/viper&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Vision-CAIR/ChatCaptioner.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.06594.pdf&#34;&gt;&lt;strong&gt;ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-12&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Vision-CAIR/ChatCaptioner&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/TaskMatrix.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.04671.pdf&#34;&gt;&lt;strong&gt;Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-08&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/TaskMatrix&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/microsoft/visual_chatgpt&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ZrrSkywalker/CaFo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.02151.pdf&#34;&gt;&lt;strong&gt;Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CVPR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-03&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ZrrSkywalker/CaFo&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yangyangyang127/PointCLIP_V2.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2211.11682.pdf&#34;&gt;&lt;strong&gt;PointCLIP V2: Adapting CLIP for Powerful 3D Open-world Learning&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CVPR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-11-21&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yangyangyang127/PointCLIP_V2&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/allenai/visprog.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2023/papers/Gupta_Visual_Programming_Compositional_Visual_Reasoning_Without_Training_CVPR_2023_paper.pdf&#34;&gt;&lt;strong&gt;Visual Programming: Compositional visual reasoning without training&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CVPR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-11-18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/allenai/visprog&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/google-research/google-research.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2204.00598.pdf&#34;&gt;&lt;strong&gt;Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-04-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/socraticmodels&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Foundation Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Title&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Venue&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/VPGTrans/VPGTrans.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.01278.pdf&#34;&gt;&lt;strong&gt;Transfer Visual Prompt Generator across LLMs&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-02&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/VPGTrans/VPGTrans&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://3fc7715dbc44234a7f.gradio.live/&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.08774.pdf&#34;&gt;&lt;strong&gt;GPT-4 Technical Report&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-15&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.03378.pdf&#34;&gt;&lt;strong&gt;PaLM-E: An Embodied Multimodal Language Model&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://palm-e.github.io/#demo&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/NVlabs/prismer.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2303.02506.pdf&#34;&gt;&lt;strong&gt;Prismer: A Vision-Language Model with An Ensemble of Experts&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-03-04&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/NVlabs/prismer&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/lorenmt/prismer&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/microsoft/unilm.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2302.14045.pdf&#34;&gt;&lt;strong&gt;Language Is Not All You Need: Aligning Perception with Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-02-27&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/unilm&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/salesforce/LAVIS.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2301.12597.pdf&#34;&gt;&lt;strong&gt;BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-01-30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip2&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://colab.research.google.com/github/salesforce/LAVIS/blob/main/examples/blip2_instructed_generation.ipynb&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/vimalabs/VIMA.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2210.03094.pdf&#34;&gt;&lt;strong&gt;VIMA: General Robot Manipulation with Multimodal Prompts&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ICML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-10-06&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/vimalabs/VIMA&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Local Demo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/MineDojo/MineDojo.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2206.08853.pdf&#34;&gt;&lt;strong&gt;MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;NeurIPS&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2022-06-17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/MineDojo/MineDojo&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Others&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Title&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Venue&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Date&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Code&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.00693.pdf&#34;&gt;&lt;strong&gt;Can Large Pre-trained Models Help Vision Models on Perception Tasks?&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-06-01&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yuhangzang/ContextDET.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.18279.pdf&#34;&gt;&lt;strong&gt;Contextual Object Detection with Multimodal Large Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yuhangzang/ContextDET&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/yuhangzang/ContextDet-Demo&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/kohjingyu/gill.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.17216.pdf&#34;&gt;&lt;strong&gt;Generating Images with Multimodal Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/kohjingyu/gill&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/yunqing-me/AttackVLM.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.16934.pdf&#34;&gt;&lt;strong&gt;On Evaluating Adversarial Robustness of Large Vision-Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-26&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/yunqing-me/AttackVLM&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/RUCAIBox/POPE.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2305.10355.pdf&#34;&gt;&lt;strong&gt;Evaluating Object Hallucination in Large Vision-Language Models&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;arXiv&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-05-17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/RUCAIBox/POPE&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/kohjingyu/fromage.svg?style=social&amp;amp;label=Star&#34; alt=&#34;Star&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/pdf/2301.13823.pdf&#34;&gt;&lt;strong&gt;Grounding Language Models to Images for Multimodal Inputs and Outputs&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ICML&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023-01-31&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/kohjingyu/fromage&#34;&gt;Github&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/spaces/jykoh/fromage&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Awesome Datasets&lt;/h1&gt; &#xA;&lt;h2&gt;Datasets of Pre-Training for Alignment&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Modalities&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;MS-COCO&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1405.0312.pdf&#34;&gt;Microsoft COCO: Common Objects in Context&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;SBU Captions&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf&#34;&gt;Im2Text: Describing Images Using 1 Million Captioned Photographs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Conceptual Captions&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://aclanthology.org/P18-1238.pdf&#34;&gt;Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;LAION-400M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2111.02114.pdf&#34;&gt;LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;VG Captions&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://link.springer.com/content/pdf/10.1007/s11263-016-0981-7.pdf&#34;&gt;Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Flickr30k&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_iccv_2015/papers/Plummer_Flickr30k_Entities_Collecting_ICCV_2015_paper.pdf&#34;&gt;Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AI-Caps&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1711.06475.pdf&#34;&gt;AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Wukong Captions&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/file/a90b9a09a6ee43d6631cf42e225d73b4-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Youku-mPLUG&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.04362.pdf&#34;&gt;Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;MSR-VTT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2016/papers/Xu_MSR-VTT_A_Large_CVPR_2016_paper.pdf&#34;&gt;MSR-VTT: A Large Video Description Dataset for Bridging Video and Language&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Webvid10M&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2104.00650.pdf&#34;&gt;Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;WavCaps&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.17395.pdf&#34;&gt;WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Caption&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Audio-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AISHELL-1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1709.05522.pdf&#34;&gt;AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ASR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Audio-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;AISHELL-2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/1808.10583.pdf&#34;&gt;AISHELL-2: Transforming Mandarin ASR Research Into Industrial Scale&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ASR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Audio-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;VSDial-CN&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.04160.pdf&#34;&gt;X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ASR&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Image-Audio-Text&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Datasets of Multimodal Instruction Tuning&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Macaw-LLM&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.09093.pdf&#34;&gt;Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lyuchenyang/Macaw-LLM/tree/main/data&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A large-scale multi-modal instruction dataset in terms of multi-turn dialogue&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;LAMM-Dataset&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.06687.pdf&#34;&gt;LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenLAMM/LAMM#lamm-dataset&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A comprehensive multi-modal instruction tuning dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Video-ChatGPT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.05424.pdf&#34;&gt;Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT#video-instruction-dataset-open_file_folder&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100K high-quality video instruction dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;MIMIC-IT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.05425.pdf&#34;&gt;MIMIC-IT: Multi-Modal In-Context Instruction Tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multimodal in-context instruction tuning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;M&lt;sup&gt;3&lt;/sup&gt;IT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.04387.pdf&#34;&gt;M&lt;sup&gt;3&lt;/sup&gt;IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/MMInstruction/M3IT&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Large-scale, broad-coverage multimodal instruction tuning dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;LLaVA-Med&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.00890.pdf&#34;&gt;LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/LLaVA-Med#llava-med-dataset&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A large-scale, broad-coverage biomedical instruction-following dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;GPT4Tools&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.18752.pdf&#34;&gt;GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/StevenGrove/GPT4Tools#dataset&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Tool-related instruction datasets&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;MULTIS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.16103.pdf&#34;&gt;ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://iva-chatbridge.github.io/&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multimodal instruction tuning dataset covering 16 multimodal tasks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;DetGPT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.14167.pdf&#34;&gt;DetGPT: Detect What You Need via Reasoning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OptimalScale/DetGPT/tree/main/dataset&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Instruction-tuning dataset with 5000 images and around 30000 query-answer pairs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;PMC-VQA&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.10415.pdf&#34;&gt;PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://xiaoman-zhang.github.io/PMC-VQA/&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Large-scale medical visual question-answering dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;VideoChat&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.06355.pdf&#34;&gt;VideoChat: Chat-Centric Video Understanding&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenGVLab/InternVideo/tree/main/Data/instruction_data&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Video-centric multimodal instruction dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;X-LLM&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.04160.pdf&#34;&gt;X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/phellonchen/X-LLM&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Chinese multimodal instruction dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;LMEye&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.03701.pdf&#34;&gt;LMEye: An Interactive Perception Network for Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/YunxinLi/Multimodal_Insturction_Data_V2&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A multi-modal instruction-tuning dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;cc-sbu-align&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.10592.pdf&#34;&gt;MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/Vision-CAIR/cc_sbu_align&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multimodal aligned dataset for improving model&#39;s usability and generation&#39;s fluency&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;LLaVA-Instruct-150K&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.08485.pdf&#34;&gt;Visual Instruction Tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multimodal instruction-following data generated by GPT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;MultiInstruct&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2212.10773.pdf&#34;&gt;MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/VT-NLP/MultiInstruct&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The first multimodal instruction tuning benchmark dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Datasets of In-Context Learning&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;MIMIC-IT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.05425.pdf&#34;&gt;MIMIC-IT: Multi-Modal In-Context Instruction Tuning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/Luodian/Otter&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multimodal in-context instruction dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Datasets of Multimodal Chain-of-Thought&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;EgoCOT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.15021.pdf&#34;&gt;EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Large-scale embodied planning dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;VIP&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.13903.pdf&#34;&gt;Letâ€™s Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;&#34;&gt;Coming soon&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;An inference-time dataset that can be used to evaluate VideoCOT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;ScienceQA&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf&#34;&gt;Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/lupantech/ScienceQA#ghost-download-the-dataset&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Large-scale multi-choice dataset, featuring multimodal science questions and diverse domains&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Others&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Paper&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Link&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;IMAD&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.10512.pdf&#34;&gt;IMAD: IMage-Augmented multi-modal Dialogue&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/VityaVitalich/IMAD&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Multimodal dialogue dataset&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;LAMM-Benchmark&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.06687.pdf&#34;&gt;LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenLAMM/LAMM#lamm-benchmark&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A benchmark for evaluating the quantitative performance of MLLMs on various2D/3D vision tasks&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;OwlEval&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.14178.pdf&#34;&gt;mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/X-PLUG/mPLUG-Owl/tree/main/OwlEval&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Dataset for evaluation on multiple capabilities&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Video-ChatGPT&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.05424.pdf&#34;&gt;Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT#quantitative-evaluation-bar_chart&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A quantitative evaluation framework for video-based dialogue models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;LVLM-eHub&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.09265.pdf&#34;&gt;LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/OpenGVLab/Multi-Modality-Arena&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;An evaluation platform for MLLMs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;M3Exam&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.05179.pdf&#34;&gt;M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/DAMO-NLP-SG/M3Exam&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A multilingual, multimodal, multilevel benchmark for evaluating MLLM&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;CLEVR-ATVC&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.05983.pdf&#34;&gt;Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1TqBzkyqxOSg1hgCXF8JjpYIAuRV-uVft&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A synthetic multimodal fine-tuning dataset for learning to reject instructions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;Fruit-ATVC&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.05983.pdf&#34;&gt;Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/drive/folders/1Saaia2rRRb1nz5sKdmpzYdS4jHiMDaP0&#34;&gt;Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;A manually pictured multimodal fine-tuning dataset for learning to reject instructions&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt;</summary>
  </entry>
</feed>