<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-24T01:31:55Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>scottpetrovic/mesh2motion-app</title>
    <updated>2025-08-24T01:31:55Z</updated>
    <id>tag:github.com,2025-08-24:/scottpetrovic/mesh2motion-app</id>
    <link href="https://github.com/scottpetrovic/mesh2motion-app" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Import a 3D Model and automatically assign and export animations&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/scottpetrovic/mesh2motion-app/main/mesh2motion.svg?sanitize=true&#34; alt=&#34;Mesh2Motion Logo&#34; width=&#34;400&#34; /&gt; &#xA;&lt;p&gt;Import a 3D Model and automatically assign and export animations with Mesh2Motion. This is kind of similar to a web application like Mixamo, but I would like it to be more flexible so it can support other model and skeleton types. Hopefully the open source nature means it can be expanded on and evolve more than than the closed tools have.&lt;/p&gt; &#xA;&lt;p&gt;The marketing site that explains features and release notes: &lt;a href=&#34;https://mesh2motion.org/&#34;&gt;https://mesh2motion.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Try it live: &lt;a href=&#34;https://app.mesh2motion.org/&#34;&gt;https://app.mesh2motion.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/scottpetrovic/mesh2motion-app/main/readme.png&#34; alt=&#34;Screenshot&#34; /&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;There are instructions built into the web application, but this is the general flow of how to use it:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Import a 3d model of your choosing (currently only supports GLB/GLTF format)&lt;/li&gt; &#xA; &lt;li&gt;Pick what type of skeleton that the 3d model will use&lt;/li&gt; &#xA; &lt;li&gt;Modify the skeleton to fit inside of the model (optionally test the results)&lt;/li&gt; &#xA; &lt;li&gt;Test out various animations to see the results.&lt;/li&gt; &#xA; &lt;li&gt;Select which animations you want to use, then export (currently only GLB/GLTF supported format)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Building and running locally&lt;/h2&gt; &#xA;&lt;p&gt;The main dependency you need is Node.js. I am using 18.15, but other versions probably work fine too. Open you command line tool to the directory this readme is in. Run ths following commands to start the web server.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Creating a production build for the web&lt;/h2&gt; &#xA;&lt;p&gt;We mostly just have typescript for this project, which web browsers cannot just read, so we need to do a build step to get everything ready for deploying. This project uses Vite for the web server and builder. See the vite.config.js for more info. This command will create a &#34;dist&#34; folder with all the files to serve to the web:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running in Docker&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t want to modify your local file system, you can alternitvely build and run the project from Docker. Make sure you have Docker and Docker Compose installed. Navigate your command line tool to this directory where your Dockerfile is at. Make sure Docker is actually started and running before you run this command.&lt;/p&gt; &#xA;&lt;p&gt;Execute the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To try it out, visit &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running and creating video previews&lt;/h2&gt; &#xA;&lt;p&gt;There is separate tool in the web app where you can generate video previews for each animation. It isn&#39;t too hard to run, but it has a separate README file that explains how that works. It is more of an internal tool, so I didn&#39;t want to muddy up this page too much.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/scottpetrovic/mesh2motion-app/main/src/preview-generator/README.md&#34;&gt;Preview Generator Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Animator Guide&lt;/h2&gt; &#xA;&lt;p&gt;Are you an animator who wants to help build out animations for this tool? This is by far my weakest skill, which is why I have been avoiding it. In the &lt;strong&gt;static &amp;gt; blender&lt;/strong&gt; folder, you can see all the source Blender files where I have been working. There are a couple of model files where I just have the model, and other files that actually contain the animations. These are the files we can build animations into.&lt;/p&gt; &#xA;&lt;p&gt;ü¶ä fox.blend (animations for the quadruped character)&lt;/p&gt; &#xA;&lt;p&gt;ü´° human.blend (animations for the humanoid character)&lt;/p&gt; &#xA;&lt;p&gt;üê¶‚Äç‚¨õ bird.blend (animations for the bird character)&lt;/p&gt; &#xA;&lt;p&gt;When new animations are added, I export everything to GLB and save the file in the &lt;strong&gt;static &amp;gt; animation&lt;/strong&gt; folder. Just overwrite the file that correlates. For the human animations, use the &#34;addon&#34; GLB file since these are being appended to the Quaternius ones. The Mixamo one is unused right now. Just a reference if I were to later add some type of support.&lt;/p&gt; &#xA;&lt;p&gt;If you come up with anything, just get me the source .blend file and let me know what you changed. I can export it out to GLB and rebuild the animation previews.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute to the animation fund&lt;/h2&gt; &#xA;&lt;p&gt;I don&#39;t expect to be receiving money for working on this, but I am also not the best animator. If people want to see better, and more, animations made, add to the fund. I can pay for an animator to help build out the animation library better. Or, if you know an animator that wants to help with this, send them my way! I am just a dude working on this during nights and weekends.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/scottpetrovic/mesh2motion-app/main/venmo.png&#34; alt=&#34;Venmo Animator Fund&#34; width=&#34;400&#34; /&gt;</summary>
  </entry>
  <entry>
    <title>google/highway</title>
    <updated>2025-08-24T01:31:55Z</updated>
    <id>tag:github.com,2025-08-24:/google/highway</id>
    <link href="https://github.com/google/highway" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Performance-portable, length-agnostic SIMD with runtime dispatch&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Efficient and performance-portable vector software&lt;/h1&gt; &#xA;&lt;p&gt;Highway is a C++ library that provides portable SIMD/vector intrinsics.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://google.github.io/highway/en/master/&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Previously licensed under Apache 2, now dual-licensed as Apache 2 / BSD-3.&lt;/p&gt; &#xA;&lt;h2&gt;Why&lt;/h2&gt; &#xA;&lt;p&gt;We are passionate about high-performance software. We see major untapped potential in CPUs (servers, mobile, desktops). Highway is for engineers who want to reliably and economically push the boundaries of what is possible in software.&lt;/p&gt; &#xA;&lt;h2&gt;How&lt;/h2&gt; &#xA;&lt;p&gt;CPUs provide SIMD/vector instructions that apply the same operation to multiple data items. This can reduce energy usage e.g. &lt;em&gt;fivefold&lt;/em&gt; because fewer instructions are executed. We also often see &lt;em&gt;5-10x&lt;/em&gt; speedups.&lt;/p&gt; &#xA;&lt;p&gt;Highway makes SIMD/vector programming practical and workable according to these guiding principles:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Does what you expect&lt;/strong&gt;: Highway is a C++ library with carefully-chosen functions that map well to CPU instructions without extensive compiler transformations. The resulting code is more predictable and robust to code changes/compiler updates than autovectorization.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Works on widely-used platforms&lt;/strong&gt;: Highway supports five architectures; the same application code can target various instruction sets, including those with &#39;scalable&#39; vectors (size unknown at compile time). Highway only requires C++11 and supports four families of compilers. If you would like to use Highway on other platforms, please raise an issue.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Flexible to deploy&lt;/strong&gt;: Applications using Highway can run on heterogeneous clouds or client devices, choosing the best available instruction set at runtime. Alternatively, developers may choose to target a single instruction set without any runtime overhead. In both cases, the application code is the same except for swapping &lt;code&gt;HWY_STATIC_DISPATCH&lt;/code&gt; with &lt;code&gt;HWY_DYNAMIC_DISPATCH&lt;/code&gt; plus one line of code. See also @kfjahnke&#39;s &lt;a href=&#34;https://github.com/kfjahnke/zimt/raw/multi_isa/examples/multi_isa_example/multi_simd_isa.md&#34;&gt;introduction to dispatching&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Suitable for a variety of domains&lt;/strong&gt;: Highway provides an extensive set of operations, used for image processing (floating-point), compression, video analysis, linear algebra, cryptography, sorting and random generation. We recognise that new use-cases may require additional ops and are happy to add them where it makes sense (e.g. no performance cliffs on some architectures). If you would like to discuss, please file an issue.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rewards data-parallel design&lt;/strong&gt;: Highway provides tools such as Gather, MaskedLoad, and FixedTag to enable speedups for legacy data structures. However, the biggest gains are unlocked by designing algorithms and data structures for scalable vectors. Helpful techniques include batching, structure-of-array layouts, and aligned/padded allocations.&lt;/p&gt; &#xA;&lt;p&gt;We recommend these resources for getting started:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=R57biOOhnJM&#34;&gt;SIMD programming with Highway talk&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://const.me/articles/simd/simd.pdf&#34;&gt;SIMD for C++ Developers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.algorithmica.org/hpc/&#34;&gt;Algorithms for Modern Hardware&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://agner.org/optimize/optimizing_cpp.pdf&#34;&gt;Optimizing software in C++&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stackoverflow.blog/2020/07/08/improving-performance-with-simd-intrinsics-in-three-use-cases/&#34;&gt;Improving performance with SIMD intrinsics in three use cases&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Online demos using Compiler Explorer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gcc.godbolt.org/z/KM3ben7ET&#34;&gt;multiple targets with dynamic dispatch&lt;/a&gt; (more complicated, but flexible and uses best available SIMD)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gcc.godbolt.org/z/rGnjMevKG&#34;&gt;single target using -m flags&lt;/a&gt; (simpler, but requires/only uses the instruction set enabled by compiler flags)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We observe that Highway is referenced in the following open source projects, found via sourcegraph.com. Most are GitHub repositories. If you would like to add your project or link to it directly, feel free to raise an issue or contact us via the below email.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Audio: &lt;a href=&#34;https://github.com/google/zimtohrli&#34;&gt;Zimtohrli perceptual metric&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Browsers: Chromium (+Vivaldi), Firefox (+floorp / foxhound / librewolf / Waterfox)&lt;/li&gt; &#xA; &lt;li&gt;Computational biology: &lt;a href=&#34;https://github.com/bnprks/BPCells&#34;&gt;RNA analysis&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Computer graphics: &lt;a href=&#34;https://github.com/rools/voxl&#34;&gt;Sparse voxel renderer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cryptography: google/distributed_point_functions, google/shell-encryption&lt;/li&gt; &#xA; &lt;li&gt;Data structures: bkille/BitLib&lt;/li&gt; &#xA; &lt;li&gt;Image codecs: eustas/2im, &lt;a href=&#34;https://github.com/GrokImageCompression/grok&#34;&gt;Grok JPEG 2000&lt;/a&gt;, &lt;a href=&#34;https://github.com/libjxl/libjxl&#34;&gt;JPEG XL&lt;/a&gt;, &lt;a href=&#34;https://github.com/osamu620/JPEGenc&#34;&gt;JPEGenc&lt;/a&gt;, &lt;a href=&#34;https://github.com/google/jpegli&#34;&gt;Jpegli&lt;/a&gt;, OpenHTJ2K&lt;/li&gt; &#xA; &lt;li&gt;Image processing: cloudinary/ssimulacra2, m-ab-s/media-autobuild_suite, &lt;a href=&#34;https://github.com/libvips/libvips&#34;&gt;libvips&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Image viewers: AlienCowEatCake/ImageViewer, diffractor/diffractor, mirillis/jpegxl-wic, &lt;a href=&#34;https://bitbucket.org/kfj/pv/&#34;&gt;Lux panorama/image viewer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Information retrieval: &lt;a href=&#34;https://github.com/iresearch-toolkit/iresearch&#34;&gt;iresearch database index&lt;/a&gt;, michaeljclark/zvec, &lt;a href=&#34;https://github.com/varchar-io/nebula&#34;&gt;nebula interactive analytics / OLAP&lt;/a&gt;, &lt;a href=&#34;https://github.com/google-research/google-research/tree/7a269cb2ce0ae1db591fe11b62cbc0be7d72532a/scann&#34;&gt;ScaNN Scalable Nearest Neighbors&lt;/a&gt;, &lt;a href=&#34;https://github.com/1yefuwang1/vectorlite/&#34;&gt;vectorlite vector search&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Machine learning: &lt;a href=&#34;https://github.com/google/gemma.cpp&#34;&gt;gemma.cpp&lt;/a&gt;, Tensorflow, Numpy, zpye/SimpleInfer&lt;/li&gt; &#xA; &lt;li&gt;Robotics: &lt;a href=&#34;https://github.com/RobotLocomotion/drake&#34;&gt;MIT Model-Based Design and Verification&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Other&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.mnm-team.org/pub/Fopras/rock23/&#34;&gt;Evaluation of C++ SIMD Libraries&lt;/a&gt;: &#34;Highway excelled with a strong performance across multiple SIMD extensions [..]. Thus, Highway may currently be the most suitable SIMD library for many software projects.&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kfjahnke/zimt&#34;&gt;zimt&lt;/a&gt;: C++11 template library to process n-dimensional arrays with multi-threaded SIMD code&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google/highway/tree/master/hwy/contrib/sort&#34;&gt;vectorized Quicksort&lt;/a&gt; (&lt;a href=&#34;https://arxiv.org/abs/2205.05982&#34;&gt;paper&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;d like to get Highway, in addition to cloning from this GitHub repository or using it as a Git submodule, you can also find it in the following package managers or repositories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;alpinelinux&lt;/li&gt; &#xA; &lt;li&gt;conan-io&lt;/li&gt; &#xA; &lt;li&gt;conda-forge&lt;/li&gt; &#xA; &lt;li&gt;DragonFlyBSD,&lt;/li&gt; &#xA; &lt;li&gt;fd00/yacp&lt;/li&gt; &#xA; &lt;li&gt;freebsd&lt;/li&gt; &#xA; &lt;li&gt;getsolus/packages&lt;/li&gt; &#xA; &lt;li&gt;ghostbsd&lt;/li&gt; &#xA; &lt;li&gt;microsoft/vcpkg&lt;/li&gt; &#xA; &lt;li&gt;MidnightBSD&lt;/li&gt; &#xA; &lt;li&gt;MSYS2&lt;/li&gt; &#xA; &lt;li&gt;NetBSD&lt;/li&gt; &#xA; &lt;li&gt;openSUSE&lt;/li&gt; &#xA; &lt;li&gt;opnsense&lt;/li&gt; &#xA; &lt;li&gt;Xilinx/Vitis_Libraries&lt;/li&gt; &#xA; &lt;li&gt;xmake-io/xmake-repo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See also the list at &lt;a href=&#34;https://repology.org/project/highway-simd-library/versions&#34;&gt;https://repology.org/project/highway-simd-library/versions&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;h2&gt;Current status&lt;/h2&gt; &#xA;&lt;h3&gt;Targets&lt;/h3&gt; &#xA;&lt;p&gt;Highway supports 24 targets, listed in alphabetical order of platform:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Any: &lt;code&gt;EMU128&lt;/code&gt;, &lt;code&gt;SCALAR&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Armv7+: &lt;code&gt;NEON_WITHOUT_AES&lt;/code&gt;, &lt;code&gt;NEON&lt;/code&gt;, &lt;code&gt;NEON_BF16&lt;/code&gt;, &lt;code&gt;SVE&lt;/code&gt;, &lt;code&gt;SVE2&lt;/code&gt;, &lt;code&gt;SVE_256&lt;/code&gt;, &lt;code&gt;SVE2_128&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;IBM Z: &lt;code&gt;Z14&lt;/code&gt;, &lt;code&gt;Z15&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;POWER: &lt;code&gt;PPC8&lt;/code&gt; (v2.07), &lt;code&gt;PPC9&lt;/code&gt; (v3.0), &lt;code&gt;PPC10&lt;/code&gt; (v3.1B, not yet supported due to compiler bugs, see #1207; also requires QEMU 7.2);&lt;/li&gt; &#xA; &lt;li&gt;RISC-V: &lt;code&gt;RVV&lt;/code&gt; (1.0);&lt;/li&gt; &#xA; &lt;li&gt;WebAssembly: &lt;code&gt;WASM&lt;/code&gt;, &lt;code&gt;WASM_EMU256&lt;/code&gt; (a 2x unrolled version of wasm128, enabled if &lt;code&gt;HWY_WANT_WASM2&lt;/code&gt; is defined. This will remain supported until it is potentially superseded by a future version of WASM.);&lt;/li&gt; &#xA; &lt;li&gt;x86: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;SSE2&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SSSE3&lt;/code&gt; (~Intel Core)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;SSE4&lt;/code&gt; (~Nehalem, also includes AES + CLMUL).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX2&lt;/code&gt; (~Haswell, also includes BMI2 + F16 + FMA)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX3&lt;/code&gt; (~Skylake, AVX-512F/BW/CD/DQ/VL)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX3_DL&lt;/code&gt; (~Icelake, includes &lt;code&gt;BitAlg&lt;/code&gt; + &lt;code&gt;CLMUL&lt;/code&gt; + &lt;code&gt;GFNI&lt;/code&gt; + &lt;code&gt;VAES&lt;/code&gt; + &lt;code&gt;VBMI&lt;/code&gt; + &lt;code&gt;VBMI2&lt;/code&gt; + &lt;code&gt;VNNI&lt;/code&gt; + &lt;code&gt;VPOPCNT&lt;/code&gt;),&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX3_ZEN4&lt;/code&gt; (AVX3_DL plus BF16, optimized for AMD Zen4; requires opt-in by defining &lt;code&gt;HWY_WANT_AVX3_ZEN4&lt;/code&gt; if compiling for static dispatch, but enabled by default for runtime dispatch),&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;AVX3_SPR&lt;/code&gt; (~Sapphire Rapids, includes AVX-512FP16)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our policy is that unless otherwise specified, targets will remain supported as long as they can be (cross-)compiled with currently supported Clang or GCC, and tested using QEMU. If the target can be compiled with LLVM trunk and tested using our version of QEMU without extra flags, then it is eligible for inclusion in our continuous testing infrastructure. Otherwise, the target will be manually tested before releases with selected versions/configurations of Clang and GCC.&lt;/p&gt; &#xA;&lt;p&gt;SVE was initially tested using farm_sve (see acknowledgments).&lt;/p&gt; &#xA;&lt;h3&gt;Versioning&lt;/h3&gt; &#xA;&lt;p&gt;Highway releases aim to follow the semver.org system (MAJOR.MINOR.PATCH), incrementing MINOR after backward-compatible additions and PATCH after backward-compatible fixes. We recommend using releases (rather than the Git tip) because they are tested more extensively, see below.&lt;/p&gt; &#xA;&lt;p&gt;The current version 1.0 signals an increased focus on backwards compatibility. Applications using documented functionality will remain compatible with future updates that have the same major version number.&lt;/p&gt; &#xA;&lt;h3&gt;Testing&lt;/h3&gt; &#xA;&lt;p&gt;Continuous integration tests build with a recent version of Clang (running on native x86, or QEMU for RISC-V and Arm) and MSVC 2019 (v19.28, running on native x86).&lt;/p&gt; &#xA;&lt;p&gt;Before releases, we also test on x86 with Clang and GCC, and Armv7/8 via GCC cross-compile. See the &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/release_testing_process.md&#34;&gt;testing process&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h3&gt;Related modules&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;contrib&lt;/code&gt; directory contains SIMD-related utilities: an image class with aligned rows, a math library (16 functions already implemented, mostly trigonometry), and functions for computing dot products and sorting.&lt;/p&gt; &#xA;&lt;h3&gt;Other libraries&lt;/h3&gt; &#xA;&lt;p&gt;If you only require x86 support, you may also use Agner Fog&#39;s &lt;a href=&#34;https://github.com/vectorclass&#34;&gt;VCL vector class library&lt;/a&gt;. It includes many functions including a complete math library.&lt;/p&gt; &#xA;&lt;p&gt;If you have existing code using x86/NEON intrinsics, you may be interested in &lt;a href=&#34;https://github.com/simd-everywhere/simde&#34;&gt;SIMDe&lt;/a&gt;, which emulates those intrinsics using other platforms&#39; intrinsics or autovectorization.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;This project uses CMake to generate and build. In a Debian-based system you can install it via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install cmake&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Highway&#39;s unit tests use &lt;a href=&#34;https://github.com/google/googletest&#34;&gt;googletest&lt;/a&gt;. By default, Highway&#39;s CMake downloads this dependency at configuration time. You can avoid this by setting the &lt;code&gt;HWY_SYSTEM_GTEST&lt;/code&gt; CMake variable to ON and installing gtest separately:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt install libgtest-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can define &lt;code&gt;HWY_TEST_STANDALONE=1&lt;/code&gt; and remove all occurrences of &lt;code&gt;gtest_main&lt;/code&gt; in each BUILD file, then tests avoid the dependency on GUnit.&lt;/p&gt; &#xA;&lt;p&gt;Running cross-compiled tests requires support from the OS, which on Debian is provided by the &lt;code&gt;qemu-user-binfmt&lt;/code&gt; package.&lt;/p&gt; &#xA;&lt;p&gt;To build Highway as a shared or static library (depending on BUILD_SHARED_LIBS), the standard CMake workflow can be used:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p build &amp;amp;&amp;amp; cd build&#xA;cmake ..&#xA;make -j &amp;amp;&amp;amp; make test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can run &lt;code&gt;run_tests.sh&lt;/code&gt; (&lt;code&gt;run_tests.bat&lt;/code&gt; on Windows).&lt;/p&gt; &#xA;&lt;p&gt;Bazel is also supported for building, but it is not as widely used/tested.&lt;/p&gt; &#xA;&lt;p&gt;When building for Armv7, a limitation of current compilers requires you to add &lt;code&gt;-DHWY_CMAKE_ARM7:BOOL=ON&lt;/code&gt; to the CMake command line; see #834 and #1032. We understand that work is underway to remove this limitation.&lt;/p&gt; &#xA;&lt;p&gt;Building on 32-bit x86 is not officially supported, and AVX2/3 are disabled by default there. Note that johnplatts has successfully built and run the Highway tests on 32-bit x86, including AVX2/3, on GCC 7/8 and Clang 8/11/12. On Ubuntu 22.04, Clang 11 and 12, but not later versions, require extra compiler flags &lt;code&gt;-m32 -isystem /usr/i686-linux-gnu/include&lt;/code&gt;. Clang 10 and earlier require the above plus &lt;code&gt;-isystem /usr/i686-linux-gnu/include/c++/12/i686-linux-gnu&lt;/code&gt;. See #1279.&lt;/p&gt; &#xA;&lt;h2&gt;Building highway - Using vcpkg&lt;/h2&gt; &#xA;&lt;p&gt;highway is now available in &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;vcpkg&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vcpkg install highway&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The highway port in vcpkg is kept up to date by Microsoft team members and community contributors. If the version is out of date, please &lt;a href=&#34;https://github.com/Microsoft/vcpkg&#34;&gt;create an issue or pull request&lt;/a&gt; on the vcpkg repository.&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;benchmark&lt;/code&gt; inside examples/ as a starting point.&lt;/p&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/quick_reference.md&#34;&gt;quick-reference page&lt;/a&gt; briefly lists all operations and their parameters, and the &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/instruction_matrix.pdf&#34;&gt;instruction_matrix&lt;/a&gt; indicates the number of instructions per operation.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/faq.md&#34;&gt;FAQ&lt;/a&gt; answers questions about portability, API design and where to find more information.&lt;/p&gt; &#xA;&lt;p&gt;We recommend using full SIMD vectors whenever possible for maximum performance portability. To obtain them, pass a &lt;code&gt;ScalableTag&amp;lt;float&amp;gt;&lt;/code&gt; (or equivalently &lt;code&gt;HWY_FULL(float)&lt;/code&gt;) tag to functions such as &lt;code&gt;Zero/Set/Load&lt;/code&gt;. There are two alternatives for use-cases requiring an upper bound on the lanes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For up to &lt;code&gt;N&lt;/code&gt; lanes, specify &lt;code&gt;CappedTag&amp;lt;T, N&amp;gt;&lt;/code&gt; or the equivalent &lt;code&gt;HWY_CAPPED(T, N)&lt;/code&gt;. The actual number of lanes will be &lt;code&gt;N&lt;/code&gt; rounded down to the nearest power of two, such as 4 if &lt;code&gt;N&lt;/code&gt; is 5, or 8 if &lt;code&gt;N&lt;/code&gt; is 8. This is useful for data structures such as a narrow matrix. A loop is still required because vectors may actually have fewer than &lt;code&gt;N&lt;/code&gt; lanes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For exactly a power of two &lt;code&gt;N&lt;/code&gt; lanes, specify &lt;code&gt;FixedTag&amp;lt;T, N&amp;gt;&lt;/code&gt;. The largest supported &lt;code&gt;N&lt;/code&gt; depends on the target, but is guaranteed to be at least &lt;code&gt;16/sizeof(T)&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Due to ADL restrictions, user code calling Highway ops must either:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reside inside &lt;code&gt;namespace hwy { namespace HWY_NAMESPACE {&lt;/code&gt;; or&lt;/li&gt; &#xA; &lt;li&gt;prefix each op with an alias such as &lt;code&gt;namespace hn = hwy::HWY_NAMESPACE; hn::Add()&lt;/code&gt;; or&lt;/li&gt; &#xA; &lt;li&gt;add using-declarations for each op used: &lt;code&gt;using hwy::HWY_NAMESPACE::Add;&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, each function that calls Highway ops (such as &lt;code&gt;Load&lt;/code&gt;) must either be prefixed with &lt;code&gt;HWY_ATTR&lt;/code&gt;, OR reside between &lt;code&gt;HWY_BEFORE_NAMESPACE()&lt;/code&gt; and &lt;code&gt;HWY_AFTER_NAMESPACE()&lt;/code&gt;. Lambda functions currently require &lt;code&gt;HWY_ATTR&lt;/code&gt; before their opening brace.&lt;/p&gt; &#xA;&lt;p&gt;Do not use namespace-scope nor &lt;code&gt;static&lt;/code&gt; initializers for SIMD vectors because this can cause SIGILL when using runtime dispatch and the compiler chooses an initializer compiled for a target not supported by the current CPU. Instead, constants initialized via &lt;code&gt;Set&lt;/code&gt; should generally be local (const) variables.&lt;/p&gt; &#xA;&lt;p&gt;The entry points into code using Highway differ slightly depending on whether they use static or dynamic dispatch. In both cases, we recommend that the top-level function receives one or more pointers to arrays, rather than target-specific vector types.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;For static dispatch, &lt;code&gt;HWY_TARGET&lt;/code&gt; will be the best available target among &lt;code&gt;HWY_BASELINE_TARGETS&lt;/code&gt;, i.e. those allowed for use by the compiler (see &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/quick_reference.md&#34;&gt;quick-reference&lt;/a&gt;). Functions inside &lt;code&gt;HWY_NAMESPACE&lt;/code&gt; can be called using &lt;code&gt;HWY_STATIC_DISPATCH(func)(args)&lt;/code&gt; within the same module they are defined in. You can call the function from other modules by wrapping it in a regular function and declaring the regular function in a header.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For dynamic dispatch, a table of function pointers is generated via the &lt;code&gt;HWY_EXPORT&lt;/code&gt; macro that is used by &lt;code&gt;HWY_DYNAMIC_DISPATCH(func)(args)&lt;/code&gt; to call the best function pointer for the current CPU&#39;s supported targets. A module is automatically compiled for each target in &lt;code&gt;HWY_TARGETS&lt;/code&gt; (see &lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/quick_reference.md&#34;&gt;quick-reference&lt;/a&gt;) if &lt;code&gt;HWY_TARGET_INCLUDE&lt;/code&gt; is defined and &lt;code&gt;foreach_target.h&lt;/code&gt; is included. Note that the first invocation of &lt;code&gt;HWY_DYNAMIC_DISPATCH&lt;/code&gt;, or each call to the pointer returned by the first invocation of &lt;code&gt;HWY_DYNAMIC_POINTER&lt;/code&gt;, involves some CPU detection overhead. You can prevent this by calling the following before any invocation of &lt;code&gt;HWY_DYNAMIC_*&lt;/code&gt;: &lt;code&gt;hwy::GetChosenTarget().Update(hwy::SupportedTargets());&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See also a separate &lt;a href=&#34;https://github.com/kfjahnke/zimt/raw/multi_isa/examples/multi_isa_example/multi_simd_isa.md&#34;&gt;introduction to dynamic dispatch&lt;/a&gt; by @kfjahnke.&lt;/p&gt; &#xA;&lt;p&gt;When using dynamic dispatch, &lt;code&gt;foreach_target.h&lt;/code&gt; is included from translation units (.cc files), not headers. Headers containing vector code shared between several translation units require a special include guard, for example the following taken from &lt;code&gt;examples/skeleton-inl.h&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#if defined(HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_) == defined(HWY_TARGET_TOGGLE)&#xA;#ifdef HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_&#xA;#undef HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_&#xA;#else&#xA;#define HIGHWAY_HWY_EXAMPLES_SKELETON_INL_H_&#xA;#endif&#xA;&#xA;#include &#34;hwy/highway.h&#34;&#xA;// Your vector code&#xA;#endif&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By convention, we name such headers &lt;code&gt;-inl.h&lt;/code&gt; because their contents (often function templates) are usually inlined.&lt;/p&gt; &#xA;&lt;h2&gt;Compiler flags&lt;/h2&gt; &#xA;&lt;p&gt;Applications should be compiled with optimizations enabled. Without inlining SIMD code may slow down by factors of 10 to 100. For clang and GCC, &lt;code&gt;-O2&lt;/code&gt; is generally sufficient.&lt;/p&gt; &#xA;&lt;p&gt;For MSVC, we recommend compiling with &lt;code&gt;/Gv&lt;/code&gt; to allow non-inlined functions to pass vector arguments in registers. If intending to use the AVX2 target together with half-width vectors (e.g. for &lt;code&gt;PromoteTo&lt;/code&gt;), it is also important to compile with &lt;code&gt;/arch:AVX2&lt;/code&gt;. This seems to be the only way to reliably generate VEX-encoded SSE instructions on MSVC. Sometimes MSVC generates VEX-encoded SSE instructions, if they are mixed with AVX, but not always, see &lt;a href=&#34;https://developercommunity.visualstudio.com/t/10618264&#34;&gt;DevCom-10618264&lt;/a&gt;. Otherwise, mixing VEX-encoded AVX2 instructions and non-VEX SSE may cause severe performance degradation. Unfortunately, with &lt;code&gt;/arch:AVX2&lt;/code&gt; option, the resulting binary will then require AVX2. Note that no such flag is needed for clang and GCC because they support target-specific attributes, which we use to ensure proper VEX code generation for AVX2 targets.&lt;/p&gt; &#xA;&lt;h2&gt;Strip-mining loops&lt;/h2&gt; &#xA;&lt;p&gt;When vectorizing a loop, an important question is whether and how to deal with a number of iterations (&#39;trip count&#39;, denoted &lt;code&gt;count&lt;/code&gt;) that does not evenly divide the vector size &lt;code&gt;N = Lanes(d)&lt;/code&gt;. For example, it may be necessary to avoid writing past the end of an array.&lt;/p&gt; &#xA;&lt;p&gt;In this section, let &lt;code&gt;T&lt;/code&gt; denote the element type and &lt;code&gt;d = ScalableTag&amp;lt;T&amp;gt;&lt;/code&gt;. Assume the loop body is given as a function &lt;code&gt;template&amp;lt;bool partial, class D&amp;gt; void LoopBody(D d, size_t index, size_t max_n)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&#34;Strip-mining&#34; is a technique for vectorizing a loop by transforming it into an outer loop and inner loop, such that the number of iterations in the inner loop matches the vector width. Then, the inner loop is replaced with vector operations.&lt;/p&gt; &#xA;&lt;p&gt;Highway offers several strategies for loop vectorization:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure all inputs/outputs are padded. Then the (outer) loop is simply&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for (size_t i = 0; i &amp;lt; count; i += N) LoopBody&amp;lt;false&amp;gt;(d, i, 0);&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here, the template parameter and second function argument are not needed.&lt;/p&gt; &lt;p&gt;This is the preferred option, unless &lt;code&gt;N&lt;/code&gt; is in the thousands and vector operations are pipelined with long latencies. This was the case for supercomputers in the 90s, but nowadays ALUs are cheap and we see most implementations split vectors into 1, 2 or 4 parts, so there is little cost to processing entire vectors even if we do not need all their lanes. Indeed this avoids the (potentially large) cost of predication or partial loads/stores on older targets, and does not duplicate code.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Process whole vectors and include previously processed elements in the last vector:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;for (size_t i = 0; i &amp;lt; count; i += N) LoopBody&amp;lt;false&amp;gt;(d, HWY_MIN(i, count - N), 0);&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is the second preferred option provided that &lt;code&gt;count &amp;gt;= N&lt;/code&gt; and &lt;code&gt;LoopBody&lt;/code&gt; is idempotent. Some elements might be processed twice, but a single code path and full vectorization is usually worth it. Even if &lt;code&gt;count &amp;lt; N&lt;/code&gt;, it usually makes sense to pad inputs/outputs up to &lt;code&gt;N&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Use the &lt;code&gt;Transform*&lt;/code&gt; functions in hwy/contrib/algo/transform-inl.h. This takes care of the loop and remainder handling and you simply define a generic lambda function (C++14) or functor which receives the current vector from the input/output array, plus optionally vectors from up to two extra input arrays, and returns the value to write to the input/output array.&lt;/p&gt; &lt;p&gt;Here is an example implementing the BLAS function SAXPY (&lt;code&gt;alpha * x + y&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Transform1(d, x, n, y, [](auto d, const auto v, const auto v1) HWY_ATTR {&#xA;  return MulAdd(Set(d, alpha), v, v1);&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Process whole vectors as above, followed by a scalar loop:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;size_t i = 0;&#xA;for (; i + N &amp;lt;= count; i += N) LoopBody&amp;lt;false&amp;gt;(d, i, 0);&#xA;for (; i &amp;lt; count; ++i) LoopBody&amp;lt;false&amp;gt;(CappedTag&amp;lt;T, 1&amp;gt;(), i, 0);&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The template parameter and second function arguments are again not needed.&lt;/p&gt; &lt;p&gt;This avoids duplicating code, and is reasonable if &lt;code&gt;count&lt;/code&gt; is large. If &lt;code&gt;count&lt;/code&gt; is small, the second loop may be slower than the next option.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Process whole vectors as above, followed by a single call to a modified &lt;code&gt;LoopBody&lt;/code&gt; with masking:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;size_t i = 0;&#xA;for (; i + N &amp;lt;= count; i += N) {&#xA;  LoopBody&amp;lt;false&amp;gt;(d, i, 0);&#xA;}&#xA;if (i &amp;lt; count) {&#xA;  LoopBody&amp;lt;true&amp;gt;(d, i, count - i);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the template parameter and third function argument can be used inside &lt;code&gt;LoopBody&lt;/code&gt; to non-atomically &#39;blend&#39; the first &lt;code&gt;num_remaining&lt;/code&gt; lanes of &lt;code&gt;v&lt;/code&gt; with the previous contents of memory at subsequent locations: &lt;code&gt;BlendedStore(v, FirstN(d, num_remaining), d, pointer);&lt;/code&gt;. Similarly, &lt;code&gt;MaskedLoad(FirstN(d, num_remaining), d, pointer)&lt;/code&gt; loads the first &lt;code&gt;num_remaining&lt;/code&gt; elements and returns zero in other lanes.&lt;/p&gt; &lt;p&gt;This is a good default when it is infeasible to ensure vectors are padded, but is only safe &lt;code&gt;#if !HWY_MEM_OPS_MIGHT_FAULT&lt;/code&gt;! In contrast to the scalar loop, only a single final iteration is needed. The increased code size from two loop bodies is expected to be worthwhile because it avoids the cost of masking in all but the final iteration.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Additional resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/highway_intro.pdf&#34;&gt;Highway introduction (slides)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/instruction_matrix.pdf&#34;&gt;Overview of instructions per operation on different architectures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/design_philosophy.md&#34;&gt;Design philosophy and comparison&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/google/highway/master/g3doc/impl_details.md&#34;&gt;Implementation details&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We have used &lt;a href=&#34;https://gitlab.inria.fr/bramas/farm-sve&#34;&gt;farm-sve&lt;/a&gt; by Berenger Bramas; it has proved useful for checking the SVE port on an x86 development machine.&lt;/p&gt; &#xA;&lt;p&gt;This is not an officially supported Google product. Contact: &lt;a href=&#34;mailto:janwas@google.com&#34;&gt;janwas@google.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVIDIA-NeMo/RL</title>
    <updated>2025-08-24T01:31:55Z</updated>
    <id>tag:github.com,2025-08-24:/NVIDIA-NeMo/RL</id>
    <link href="https://github.com/NVIDIA-NeMo/RL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scalable toolkit for efficient model reinforcement&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Nemo RL: A Scalable and Efficient Post-Training Library&lt;/h1&gt; &#xA;&lt;h2&gt;üì£ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[7/25/2025] &lt;a href=&#34;https://github.com/NVIDIA-NeMo/RL/releases/tag/v0.3.0&#34;&gt;Release v0.3.0!&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üìù &lt;a href=&#34;https://nvidia-nemo.github.io/blog/2025/07/21/nemo-rl-v0.3/&#34;&gt;v0.3.0 Blog Post&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;üìä View the release run metrics on &lt;a href=&#34;https://colab.research.google.com/drive/15kpesCV1m_C5UQFStssTEjaN2RsBMeZ0?usp=sharing&#34;&gt;Google Colab&lt;/a&gt; to get a head start on your experimentation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;[5/14/2025] &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/grpo-deepscaler.md&#34;&gt;Reproduce DeepscaleR with NeMo RL!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[5/14/2025] &lt;a href=&#34;https://github.com/NVIDIA-NeMo/RL/releases/tag/v0.2.1&#34;&gt;Release v0.2.1!&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üìä View the release run metrics on &lt;a href=&#34;https://colab.research.google.com/drive/1o14sO0gj_Tl_ZXGsoYip3C0r5ofkU1Ey?usp=sharing&#34;&gt;Google Colab&lt;/a&gt; to get a head start on your experimentation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;!-- markdown all in one --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#nemo-rl-a-scalable-and-efficient-post-training-library&#34;&gt;Nemo RL: A Scalable and Efficient Post-Training Library&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#-news&#34;&gt;üì£ News&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#features&#34;&gt;Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#training-backends&#34;&gt;Training Backends&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo&#34;&gt;GRPO&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-single-node&#34;&gt;GRPO Single Node&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-multi-node&#34;&gt;GRPO Multi-node&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-qwen25-32b&#34;&gt;GRPO Qwen2.5-32B&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#grpo-multi-turn&#34;&gt;GRPO Multi-Turn&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#supervised-fine-tuning-sft&#34;&gt;Supervised Fine-Tuning (SFT)&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#sft-single-node&#34;&gt;SFT Single Node&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#sft-multi-node&#34;&gt;SFT Multi-node&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo&#34;&gt;DPO&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo-single-node&#34;&gt;DPO Single Node&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#dpo-multi-node&#34;&gt;DPO Multi-node&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm&#34;&gt;RM&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm-single-node&#34;&gt;RM Single Node&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#rm-multi-node&#34;&gt;RM Multi-node&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#convert-model-format-optional&#34;&gt;Convert Model Format (Optional)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#run-evaluation&#34;&gt;Run Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#set-up-clusters&#34;&gt;Set Up Clusters&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#tips-and-tricks&#34;&gt;Tips and Tricks&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/#licenses&#34;&gt;Licenses&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Nemo RL&lt;/strong&gt; is a scalable and efficient post-training library designed for models ranging from 1 GPU to thousands, and from tiny to over 100 billion parameters.&lt;/p&gt; &#xA;&lt;p&gt;What you can expect:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Seamless integration with Hugging Face&lt;/strong&gt; for ease of use, allowing users to leverage a wide range of pre-trained models and tools.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-performance implementation with Megatron Core&lt;/strong&gt;, supporting various parallelism techniques for large models (&amp;gt;100B) and large context lengths.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Efficient resource management using Ray&lt;/strong&gt;, enabling scalable and flexible deployment across different hardware configurations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt; with a modular design that allows easy integration and customization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comprehensive documentation&lt;/strong&gt; that is both detailed and user-friendly, with practical examples.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;‚úÖ &lt;em&gt;Available now&lt;/em&gt; | üîú &lt;em&gt;Coming in v0.4&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Fast Generation&lt;/strong&gt; - vLLM backend for optimized inference.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;HuggingFace Integration&lt;/strong&gt; - Works with 1-70B models (Qwen, Llama).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Distributed Training&lt;/strong&gt; - Fully Sharded Data Parallel (FSDP2) support and Ray-based infrastructure.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Environment Support&lt;/strong&gt; - Support for multi-environment training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Learning Algorithms&lt;/strong&gt; - GRPO (Group Relative Policy Optimization), SFT (Supervised Fine-Tuning), and DPO (Direct Preference Optimization).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Multi-Turn RL&lt;/strong&gt; - Multi-turn generation and training for RL with tool use, games, etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Large Model Support&lt;/strong&gt; - Native PyTorch support for models up to 70B parameters.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Advanced Parallelism&lt;/strong&gt; - PyTorch native FSDP2, TP, CP, and SP for efficient training.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;(even) Larger Model Support with Long(er) Sequences&lt;/strong&gt; - Advanced parallelisms with Megatron Core (TP/PP/CP/SP/EP).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Worker Isolation&lt;/strong&gt; - Process isolation between RL Actors (no worries about global state).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Environment Isolation&lt;/strong&gt; - Dependency isolation between components.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Megatron Inference&lt;/strong&gt; - (static) Megatron Inference for day-0 support for new megatron models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;MoE Models&lt;/strong&gt; - Support for DeepseekV3 and Qwen-3 MoE models&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Sequence Packing&lt;/strong&gt; - Sequence packing in both DTensor and MCore for huge training perf gains&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîú &lt;strong&gt;Improved Native Performance&lt;/strong&gt; - Improve training time for Native Pytorch Models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üîú &lt;strong&gt;Megatron Inference&lt;/strong&gt; - (dynamic) Megatron Inference for fast day-0 support for new megatron models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;Clone &lt;strong&gt;NeMo RL&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone git@github.com:NVIDIA-NeMo/RL.git nemo-rl&#xA;cd nemo-rl&#xA;&#xA;# If you are using the Megatron backend, download the pinned versions of Megatron-LM and NeMo submodules &#xA;# by running (This is not necessary if you are using the pure Pytorch/DTensor path):&#xA;git submodule update --init --recursive&#xA;&#xA;# Different branches of the repo can have different pinned versions of these third-party submodules. Ensure&#xA;# submodules are automatically updated after switching branches or pulling updates by configuring git with:&#xA;# git config submodule.recurse true&#xA;&#xA;# **NOTE**: this setting will not download **new** or remove **old** submodules with the branch&#39;s changes.&#xA;# You will have to run the full `git submodule update --init --recursive` command in these situations.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are using the Megatron backend on bare-metal (outside of a container), you may need to install the cudnn headers as well. Here is how you can check as well as install them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Check if you have libcudnn installed&#xA;dpkg -l | grep cudnn.*cuda&#xA;&#xA;# Find the version you need here: https://developer.nvidia.com/cudnn-downloads?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;Distribution=Ubuntu&amp;amp;target_version=20.04&amp;amp;target_type=deb_network&#xA;# As an example, these are the &#34;Linux Ubuntu 20.04 x86_64&#34; instructions&#xA;wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb&#xA;sudo dpkg -i cuda-keyring_1.1-1_all.deb&#xA;sudo apt-get update&#xA;sudo apt-get install cudnn-cuda-12&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For faster setup and environment isolation, we use &lt;a href=&#34;https://docs.astral.sh/uv/&#34;&gt;uv&lt;/a&gt;. Follow &lt;a href=&#34;https://docs.astral.sh/uv/getting-started/installation/&#34;&gt;these instructions&lt;/a&gt; to install uv.&lt;/p&gt; &#xA;&lt;p&gt;Then, initialize NeMo RL project virtual environment via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Please do not use &lt;code&gt;-p/--python&lt;/code&gt; and instead allow &lt;code&gt;uv venv&lt;/code&gt; to read it from &lt;code&gt;.python-version&lt;/code&gt;. This ensures that the version of python used is always what we prescribe.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If working outside a container, it can help to build &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;flash-attn&lt;/a&gt; and warm the uv cache before your first run.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;bash tools/build-flash-attn-in-uv-cache.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] On the first install, &lt;code&gt;flash-attn&lt;/code&gt; can take a while to install (~45min with 48 CPU hyperthreads). After it is built once, it is cached in your uv&#39;s cache dir making subsequent installs much quicker.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] The NeMo RL Dockerfile will warm the uv cache with flash-attn. See &lt;a href=&#34;https://docs.nvidia.com/nemo/rl/latest/docker.html&#34;&gt;https://docs.nvidia.com/nemo/rl/latest/docker.html&lt;/a&gt; for instructions if you are looking for the NeMo RL container.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If sucessful, you should see &lt;code&gt;‚úÖ flash-attn successfully added to uv cache&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Use &lt;code&gt;uv run&lt;/code&gt; to launch all commands. It handles pip installing implicitly and ensures your environment is up to date with our lock file.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;It is not recommended to activate the &lt;code&gt;venv&lt;/code&gt;, and you should use &lt;code&gt;uv run &amp;lt;command&amp;gt;&lt;/code&gt; instead to execute scripts within the managed environment. This ensures consistent environment usage across different shells and sessions. Example: &lt;code&gt;uv run python examples/run_grpo_math.py&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ensure you have the necessary CUDA drivers and PyTorch installed compatible with your hardware.&lt;/li&gt; &#xA;  &lt;li&gt;If you update your environment in &lt;code&gt;pyproject.toml&lt;/code&gt;, it is necessary to force a rebuild of the virtual environments by setting &lt;code&gt;NRL_FORCE_REBUILD_VENVS=true&lt;/code&gt; next time you launch a run.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Reminder&lt;/strong&gt;: Don&#39;t forget to set your &lt;code&gt;HF_HOME&lt;/code&gt;, &lt;code&gt;WANDB_API_KEY&lt;/code&gt;, and &lt;code&gt;HF_DATASETS_CACHE&lt;/code&gt; (if needed). You&#39;ll need to do a &lt;code&gt;huggingface-cli login&lt;/code&gt; as well for Llama models.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Training Backends&lt;/h2&gt; &#xA;&lt;p&gt;NeMo RL supports multiple training backends to accommodate different model sizes and hardware configurations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;DTensor (FSDP2)&lt;/strong&gt; - PyTorch&#39;s next-generation distributed training with improved memory efficiency&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Megatron&lt;/strong&gt; - NVIDIA&#39;s high-performance training framework for scaling to large models (&amp;gt;100B parameters)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The training backend is automatically determined based on your YAML configuration settings. For detailed information on backend selection, configuration, and examples, see the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/training-backends.md&#34;&gt;Training Backends documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;GRPO&lt;/h2&gt; &#xA;&lt;p&gt;We have a reference GRPO experiment config set up trained for math benchmarks using the &lt;a href=&#34;https://huggingface.co/datasets/nvidia/OpenMathInstruct-2&#34;&gt;OpenInstructMath2&lt;/a&gt; dataset.&lt;/p&gt; &#xA;&lt;p&gt;You can read about the details of the GRPO implementation &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/grpo.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;GRPO Single Node&lt;/h3&gt; &#xA;&lt;p&gt;To run GRPO on a single GPU for &lt;code&gt;Qwen/Qwen2.5-1.5B&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run the GRPO math example using a 1B parameter model&#xA;uv run python examples/run_grpo_math.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this uses the configuration in &lt;code&gt;examples/configs/grpo_math_1B.yaml&lt;/code&gt;. You can customize parameters with command-line overrides. For example, to run on 8 GPUs,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run the GRPO math example using a 1B parameter model using 8 GPUs&#xA;uv run python examples/run_grpo_math.py \&#xA;  cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can override any of the parameters listed in the yaml configuration file. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_grpo_math.py \&#xA;  policy.model_name=&#34;meta-llama/Llama-3.2-1B-Instruct&#34; \&#xA;  checkpointing.checkpoint_dir=&#34;results/llama1b_math&#34; \&#xA;  logger.wandb_enabled=True \&#xA;  logger.wandb.name=&#34;grpo-llama1b_math&#34; \&#xA;  logger.num_val_samples_to_print=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The default configuration uses the DTensor training backend. We also provide a config &lt;code&gt;examples/configs/grpo_math_1B_megatron.yaml&lt;/code&gt; which is set up to use the Megatron backend out of the box.&lt;/p&gt; &#xA;&lt;p&gt;To train using this config on a single GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run a GRPO math example on 1 GPU using the Megatron backend&#xA;uv run python examples/run_grpo_math.py \&#xA;  --config examples/configs/grpo_math_1B_megatron.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional details on supported backends and how to configure the training backend to suit your setup, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/training-backends.md&#34;&gt;Training Backends documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;GRPO Multi-node&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;NUM_ACTOR_NODES=2&#xA;&#xA;# grpo_math_8b uses Llama-3.1-8B-Instruct model&#xA;COMMAND=&#34;uv run ./examples/run_grpo_math.py --config examples/configs/grpo_math_8B.yaml cluster.num_nodes=2 checkpointing.checkpoint_dir=&#39;results/llama8b_2nodes&#39; logger.wandb_enabled=True logger.wandb.name=&#39;grpo-llama8b_math&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The required &lt;code&gt;CONTAINER&lt;/code&gt; can be built by following the instructions in the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/docker.md&#34;&gt;Docker documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;GRPO Qwen2.5-32B&lt;/h4&gt; &#xA;&lt;p&gt;This section outlines how to run GRPO for Qwen2.5-32B with a 16k sequence length.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;NUM_ACTOR_NODES=32&#xA;&#xA;# Download Qwen before the job starts to avoid spending time downloading during the training loop&#xA;HF_HOME=/path/to/hf_home huggingface-cli download Qwen/Qwen2.5-32B&#xA;&#xA;# Ensure HF_HOME is included in your MOUNTS&#xA;HF_HOME=/path/to/hf_home \&#xA;COMMAND=&#34;uv run ./examples/run_grpo_math.py --config examples/configs/grpo_math_8B.yaml policy.model_name=&#39;Qwen/Qwen2.5-32B&#39; policy.generation.vllm_cfg.tensor_parallel_size=4 policy.max_total_sequence_length=16384 cluster.num_nodes=${NUM_ACTOR_NODES} policy.dtensor_cfg.enabled=True policy.dtensor_cfg.tensor_parallel_size=8 policy.dtensor_cfg.sequence_parallel=True policy.dtensor_cfg.activation_checkpointing=True checkpointing.checkpoint_dir=&#39;results/qwen2.5-32b&#39; logger.wandb_enabled=True logger.wandb.name=&#39;qwen2.5-32b&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;GRPO Multi-Turn&lt;/h4&gt; &#xA;&lt;p&gt;We also support multi-turn generation and training (tool use, games, etc.). Reference example for training to play a Sliding Puzzle Game:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_grpo_sliding_puzzle.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Supervised Fine-Tuning (SFT)&lt;/h2&gt; &#xA;&lt;p&gt;We provide an example SFT experiment using the &lt;a href=&#34;https://rajpurkar.github.io/SQuAD-explorer/&#34;&gt;SQuAD dataset&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SFT Single Node&lt;/h3&gt; &#xA;&lt;p&gt;The default SFT configuration is set to run on a single GPU. To start the experiment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_sft.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This fine-tunes the &lt;code&gt;Llama3.2-1B&lt;/code&gt; model on the SQuAD dataset using a 1 GPU.&lt;/p&gt; &#xA;&lt;p&gt;To use multiple GPUs on a single node, you can modify the cluster configuration. This adjustment will also let you potentially increase the model and batch size:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_sft.py \&#xA;  policy.model_name=&#34;meta-llama/Meta-Llama-3-8B&#34; \&#xA;  policy.train_global_batch_size=128 \&#xA;  sft.val_global_batch_size=128 \&#xA;  cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;code&gt;examples/configs/sft.yaml&lt;/code&gt; for a full list of parameters that can be overridden.&lt;/p&gt; &#xA;&lt;h3&gt;SFT Multi-node&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;NUM_ACTOR_NODES=2&#xA;&#xA;COMMAND=&#34;uv run ./examples/run_sft.py --config examples/configs/sft.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 checkpointing.checkpoint_dir=&#39;results/sft_llama8b_2nodes&#39; logger.wandb_enabled=True logger.wandb.name=&#39;sft-llama8b&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;DPO&lt;/h2&gt; &#xA;&lt;p&gt;We provide a sample DPO experiment that uses the &lt;a href=&#34;https://huggingface.co/datasets/nvidia/HelpSteer3&#34;&gt;HelpSteer3 dataset&lt;/a&gt; for preference-based training.&lt;/p&gt; &#xA;&lt;h3&gt;DPO Single Node&lt;/h3&gt; &#xA;&lt;p&gt;The default DPO experiment is configured to run on a single GPU. To launch the experiment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_dpo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This trains &lt;code&gt;Llama3.2-1B-Instruct&lt;/code&gt; on one GPU.&lt;/p&gt; &#xA;&lt;p&gt;If you have access to more GPUs, you can update the experiment accordingly. To run on 8 GPUs, we update the cluster configuration and switch to an 8B Llama3.1 Instruct model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_dpo.py \&#xA;  policy.model_name=&#34;meta-llama/Llama-3.1-8B-Instruct&#34; \&#xA;  policy.train_global_batch_size=256 \&#xA;  cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Any of the DPO parameters can be customized from the command line. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_dpo.py \&#xA;  dpo.sft_loss_weight=0.1 \&#xA;  dpo.preference_average_log_probs=True \&#xA;  checkpointing.checkpoint_dir=&#34;results/llama_dpo_sft&#34; \&#xA;  logger.wandb_enabled=True \&#xA;  logger.wandb.name=&#34;llama-dpo-sft&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;code&gt;examples/configs/dpo.yaml&lt;/code&gt; for a full list of parameters that can be overridden. For an in-depth explanation of how to add your own DPO dataset, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/dpo.md&#34;&gt;DPO documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;DPO Multi-node&lt;/h3&gt; &#xA;&lt;p&gt;For distributed DPO training across multiple nodes, modify the following script for your use case:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;## number of nodes to use for your job&#xA;NUM_ACTOR_NODES=2&#xA;&#xA;COMMAND=&#34;uv run ./examples/run_dpo.py --config examples/configs/dpo.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 dpo.val_global_batch_size=32 checkpointing.checkpoint_dir=&#39;results/dpo_llama81_2nodes&#39; logger.wandb_enabled=True logger.wandb.name=&#39;dpo-llama1b&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;RM&lt;/h2&gt; &#xA;&lt;p&gt;We provide a sample RM experiment that uses the &lt;a href=&#34;https://huggingface.co/datasets/nvidia/HelpSteer3&#34;&gt;HelpSteer3 dataset&lt;/a&gt; for preference-based training.&lt;/p&gt; &#xA;&lt;h3&gt;RM Single Node&lt;/h3&gt; &#xA;&lt;p&gt;The default RM experiment is configured to run on a single GPU. To launch the experiment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_rm.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This trains a RM based on &lt;code&gt;meta-llama/Llama-3.2-1B-Instruct&lt;/code&gt; on one GPU.&lt;/p&gt; &#xA;&lt;p&gt;If you have access to more GPUs, you can update the experiment accordingly. To run on 8 GPUs, we update the cluster configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_rm.py cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/rm.md&#34;&gt;RM documentation&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h3&gt;RM Multi-node&lt;/h3&gt; &#xA;&lt;p&gt;For distributed RM training across multiple nodes, modify the following script for your use case:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Run from the root of NeMo RL repo&#xA;## number of nodes to use for your job&#xA;NUM_ACTOR_NODES=2&#xA;&#xA;COMMAND=&#34;uv run ./examples/run_rm.py --config examples/configs/rm.yaml cluster.num_nodes=2 cluster.gpus_per_node=8 checkpointing.checkpoint_dir=&#39;results/rm_llama1b_2nodes&#39; logger.wandb_enabled=True logger.wandb.name=&#39;rm-llama1b-2nodes&#39;&#34; \&#xA;CONTAINER=YOUR_CONTAINER \&#xA;MOUNTS=&#34;$PWD:$PWD&#34; \&#xA;sbatch \&#xA;    --nodes=${NUM_ACTOR_NODES} \&#xA;    --account=YOUR_ACCOUNT \&#xA;    --job-name=YOUR_JOBNAME \&#xA;    --partition=YOUR_PARTITION \&#xA;    --time=4:0:0 \&#xA;    --gres=gpu:8 \&#xA;    ray.sub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We provide evaluation tools to assess model capabilities.&lt;/p&gt; &#xA;&lt;h3&gt;Convert Model Format (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;If you have trained a model and saved the checkpoint in the Pytorch DCP format, you first need to convert it to the Hugging Face format before running evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Example for a GRPO checkpoint at step 170&#xA;uv run python examples/converters/convert_dcp_to_hf.py \&#xA;    --config results/grpo/step_170/config.yaml \&#xA;    --dcp-ckpt-path results/grpo/step_170/policy/weights/ \&#xA;    --hf-ckpt-path results/grpo/hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have a model saved in Megatron format, you can use the following command to convert it to Hugging Face format prior to running evaluation. This script requires mcore, so make sure to launch with the mcore extra:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Example for a GRPO checkpoint at step 170&#xA;uv run --extra mcore python examples/converters/convert_megatron_to_hf.py \&#xA;    --config results/grpo/step_170/config.yaml \&#xA;    --megatron-ckpt-path results/grpo/step_170/policy/weights/iter_0000000 \&#xA;    --hf-ckpt-path results/grpo/hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Adjust the paths according to your training output directory structure.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For an in-depth explanation of checkpointing, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/design-docs/checkpointing.md&#34;&gt;Checkpointing documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Run evaluation script with converted model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;uv run python examples/run_eval.py generation.model_name=$PWD/results/grpo/hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run evaluation script with custom settings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# Example: Evaluation of DeepScaleR-1.5B-Preview on MATH-500 using 8 GPUs&#xA;#          Pass@1 accuracy averaged over 16 samples for each problem&#xA;uv run python examples/run_eval.py \&#xA;    --config examples/configs/evals/math_eval.yaml \&#xA;    generation.model_name=agentica-org/DeepScaleR-1.5B-Preview \&#xA;    generation.temperature=0.6 \&#xA;    generation.top_p=0.95 \&#xA;    generation.vllm_cfg.max_model_len=32768 \&#xA;    data.dataset_name=math500 \&#xA;    eval.num_tests_per_prompt=16 \&#xA;    cluster.gpus_per_node=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Evaluation results may vary slightly due to various factors, such as sampling parameters, random seed, inference engine version, and inference engine settings.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Refer to &lt;code&gt;examples/configs/evals/eval.yaml&lt;/code&gt; for a full list of parameters that can be overridden. For an in-depth explanation of evaluation, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/guides/eval.md&#34;&gt;Evaluation documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Set Up Clusters&lt;/h2&gt; &#xA;&lt;p&gt;For detailed instructions on how to set up and launch NeMo RL on Slurm or Kubernetes clusters, please refer to the dedicated &lt;a href=&#34;https://raw.githubusercontent.com/NVIDIA-NeMo/RL/main/docs/cluster.md&#34;&gt;Cluster Start&lt;/a&gt; documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Tips and Tricks&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;If you forget to initialize the NeMo and Megatron submodules when cloning the NeMo-RL repository, you may run into an error like this:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ModuleNotFoundError: No module named &#39;megatron&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you see this error, there is likely an issue with your virtual environments. To fix this, first intialize the submodules:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and then force a rebuild of the virtual environments by setting &lt;code&gt;NRL_FORCE_REBUILD_VENVS=true&lt;/code&gt; next time you launch a run:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;NRL_FORCE_REBUILD_VENVS=true uv run examples/run_grpo.py ...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use NeMo RL in your research, please cite it using the following BibTeX entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{nemo-rl,&#xA;title = {NeMo RL: A Scalable and Efficient Post-Training Library},&#xA;howpublished = {\url{https://github.com/NVIDIA-NeMo/RL}},&#xA;year = {2025},&#xA;note = {GitHub repository},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to NeMo RL! Please see our &lt;a href=&#34;https://github.com/NVIDIA-NeMo/RL/raw/main/CONTRIBUTING.md&#34;&gt;Contributing Guidelines&lt;/a&gt; for more information on how to get involved.&lt;/p&gt; &#xA;&lt;h2&gt;Licenses&lt;/h2&gt; &#xA;&lt;p&gt;NVIDIA NeMo RL is licensed under the &lt;a href=&#34;https://github.com/NVIDIA-NeMo/RL/raw/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>