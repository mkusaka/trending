<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-07-08T01:32:37Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>focus-creative-games/hybridclr</title>
    <updated>2022-07-08T01:32:37Z</updated>
    <id>tag:github.com,2022-07-08:/focus-creative-games/hybridclr</id>
    <link href="https://github.com/focus-creative-games/hybridclr" rel="alternate"></link>
    <summary type="html">&lt;p&gt;HybridCLR是一个特性完整、零成本、高性能、低内存的近乎完美的Unity全平台原生c#热更方案。 HybridCLR is a fully featured, zero-cost, high-performance, low-memory solution for Unity&#39;s all-platform native c# hotfix&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;HybridCLR&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/focus-creative-games/HybridCLR/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;http://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;HybridCLR是一个&lt;strong&gt;特性完整、零成本、高性能、低内存&lt;/strong&gt;的&lt;strong&gt;近乎完美&lt;/strong&gt;的Unity全平台原生c#热更方案。&lt;/p&gt; &#xA;&lt;p&gt;HybridCLR扩充了il2cpp的代码，使它由纯&lt;a href=&#34;https://en.wikipedia.org/wiki/Ahead-of-time_compilation&#34;&gt;AOT&lt;/a&gt; runtime变成‘AOT+Interpreter’ 混合runtime，进而原生支持动态加载assembly，使得基于il2cpp backend打包的游戏不仅能在Android平台，也能在IOS、Consoles等限制了JIT的平台上高效地以&lt;strong&gt;AOT+interpreter&lt;/strong&gt;混合模式执行。从底层彻底支持了热更新。&lt;/p&gt; &#xA;&lt;p&gt;HybridCLR&lt;strong&gt;开创性地实现了 &lt;code&gt;differential hybrid dll&lt;/code&gt; 技术&lt;/strong&gt;====。即可以对AOT dll任意增删改，HybridCLR会智能地让变化或者新增的类和函数以interpreter模式运行，但未改动的类和函数以AOT方式运行，让热更新的游戏逻辑的运行性能基本达到原生AOT的水平。&lt;/p&gt; &#xA;&lt;h2&gt;特性&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;特性完整。 近乎完整实现了&lt;a href=&#34;https://www.ecma-international.org/publications-and-standards/standards/ecma-335/&#34;&gt;ECMA-335规范&lt;/a&gt;，除了 下文中&#34;限制和注意事项&#34; 之外的特性都支持。&lt;/li&gt; &#xA; &lt;li&gt;零学习和使用成本。 HybridCLR将纯AOT runtime增强为完整的runtime，使得热更新代码与AOT代码无缝工作。脚本类与AOT类在同一个运行时内，可以随意写继承、反射、多线程(volatile、ThreadStatic、Task、async)之类的代码。不需要额外写任何特殊代码、没有代码生成，也没有什么特殊限制。&lt;/li&gt; &#xA; &lt;li&gt;执行高效。实现了一个极其高效的寄存器解释器，所有指标都大幅优于其他热更新方案。&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/performance/benchmark/#%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E6%8A%A5%E5%91%8A&#34;&gt;性能测试报告&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;内存高效。 热更新脚本中定义的类跟普通c#类占用一样的内存空间，远优于其他热更新方案。&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/performance/benchmark/#%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E6%8A%A5%E5%91%8A&#34;&gt;内存占用报告&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;原生支持hotfix修复AOT部分代码。几乎不增加任何开发和运行开销。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;开创性地实现了 &lt;code&gt;differential hybrid dll&lt;/code&gt; 技术&lt;/strong&gt;。即可以将某个热更新dll先AOT形式打包，后面可以对该dll任意增删改，HybridCLR会智能地让变化或者新增的类和函数以interpreter模式运行，但未改动的类和函数以AOT方式运行。这意味着热更新的游戏逻辑的运行性能将接近原生AOT的水平。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;工作原理&lt;/h2&gt; &#xA;&lt;p&gt;HybridCLR从mono的&lt;a href=&#34;https://developpaper.com/new-net-interpreter-mono-has-arrived/&#34;&gt;hybrid mode execution&lt;/a&gt;技术中得到启发，为unity的il2cpp之类的AOT runtime额外提供了interpreter模块，将它们由纯AOT运行时改造为&#34;AOT + Interpreter&#34;混合运行方式。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/focus-creative-games/hybridclr/main/docs/images/architecture.png&#34; alt=&#34;icon&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;更具体地说，HybridCLR做了以下几点工作：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;实现了一个高效的元数据(dll)解析库&lt;/li&gt; &#xA; &lt;li&gt;改造了元数据管理模块，实现了元数据的动态注册&lt;/li&gt; &#xA; &lt;li&gt;实现了一个IL指令集到自定义的寄存器指令集的compiler&lt;/li&gt; &#xA; &lt;li&gt;实现了一个高效的寄存器解释器&lt;/li&gt; &#xA; &lt;li&gt;额外提供大量的instinct函数，提升解释器性能&lt;/li&gt; &#xA; &lt;li&gt;提供hotfix AOT的支持&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;与其他流行的c#热更新方案的区别&lt;/h2&gt; &#xA;&lt;h3&gt;本质比较&lt;/h3&gt; &#xA;&lt;p&gt;HybridCLR是原生的c#热更新方案。通俗地说，il2cpp相当于mono的aot模块，HybridCLR相当于mono的interpreter模块，两者合一成为完整mono。HybridCLR使得il2cpp变成一个全功能的runtime，原生（即通过System.Reflection.Assembly.Load）支持动态加载dll，从而支持ios平台的热更新。&lt;/p&gt; &#xA;&lt;p&gt;正因为HybridCLR是原生runtime级别实现，热更新部分的类型与主工程AOT部分类型是完全等价并且无缝统一的。可以随意调用、继承、反射、多线程，不需要生成代码或者写适配器。&lt;/p&gt; &#xA;&lt;p&gt;其他热更新方案则是独立vm，与il2cpp的关系本质上相当于mono中嵌入lua的关系。因此类型系统不统一，为了让热更新类型能够继承AOT部分类型，需要写适配器，并且解释器中的类型不能为主工程的类型系统所识别。特性不完整、开发麻烦、运行效率低下。&lt;/p&gt; &#xA;&lt;h3&gt;实际使用体验或者特性比较&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;HybridCLR学习和使用成本几乎为零。HybridCLR让il2cpp变成全功能的runtime，学习和使用成本几乎为零，几乎零侵入性。而其他方案则有大量的坑和需要规避的规则，学习和使用成本，需要对原项目作大量改造。&lt;/li&gt; &#xA; &lt;li&gt;HybridCLR可以使用所有c#的特性。而其他方案往往有大量的限制。&lt;/li&gt; &#xA; &lt;li&gt;HybridCLR中可以直接支持使用和继承主工程中的类型。其他方案要写适配器或者生成代码。&lt;/li&gt; &#xA; &lt;li&gt;HybridCLR中热更新部分元数据与AOT元数据无缝统一。像反射代码能够正常工作的，AOT部分也可以通过标准Reflection接口创建出热更新对象。其他方案做不到。&lt;/li&gt; &#xA; &lt;li&gt;HybridCLR对多线程支持良好。像多线程、ThreadStatic、async等等特性都是HybridCLR直接支持，其他方案除了async特性外均难以支持。&lt;/li&gt; &#xA; &lt;li&gt;HybridCLR中Unity工作流与原生几乎完全相同。HybridCLR中热更新MonoBehaviour可以直接挂载在热更新资源上，并且正确工作。其他方案不行。&lt;/li&gt; &#xA; &lt;li&gt;HybridCLR兼容性极高。各种第三方库只要在il2cpp下能工作，在HybridCLR下也能正常工作。其他方案往往要大量魔改源码。&lt;/li&gt; &#xA; &lt;li&gt;HybridCLR内存效率极高。HybridCLR中热更新类型与主工程的AOT类型完全等价，占用一样多的空间。其他方案的同等类型则是假类型，不仅不能被runtime识别，还多占了数倍空间。&lt;/li&gt; &#xA; &lt;li&gt;HybridCLR执行效率高。HybridCLR中热更新部分与主工程AOT部分交互属于il2cpp内部交互，效率极高。而其他方案则是独立虚拟机与il2cpp之间的效率，不仅交互麻烦还效率低下。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;文档&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://focus-creative-games.github.io/&#34;&gt;文档站&lt;/a&gt;，&lt;strong&gt;推荐使用&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/faq/&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/performance/limit/&#34;&gt;限制和注意事项&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/focus-creative-games/HybridCLR_trial&#34;&gt;示例项目&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zhihu.com/column/c_1489549396035870720&#34;&gt;知乎专栏&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/donate/&#34;&gt;==&amp;gt;致谢名单&amp;lt;==&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;稳定性状况&lt;/h2&gt; &#xA;&lt;p&gt;=== &lt;strong&gt;庆祝于 2021.6.7 第一款使用HybridCLR的android和iOS双端休闲游戏正式上线&lt;/strong&gt; ===，7月份还有几款中重游戏上线或者对外测试。&lt;/p&gt; &#xA;&lt;p&gt;技术评估上目前稳定性处于Beta版本。由于HybridCLR技术原理的先进性，bug本质上不多，稳定得非常快。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;目前PC、Android、iOS 已跑通所有单元测试，可稳定体验使用。&lt;/li&gt; &#xA; &lt;li&gt;测试了游戏常用库和框架的兼容性，兼容性良好。只要能在il2cpp backend下工作的库都可以在HybridCLR下正常工作。甚至那些与il2cpp因为AOT问题不兼容的库，现在因为HybridCLR对il2cpp的能力扩充，反而可以正常运行了。具体参见 &lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/performance/compatible/&#34;&gt;兼容性报告&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;已经有几十个大中型游戏项目较完整地接入HybridCLR，并且其中一些在紧锣密鼓作上线前测试。具体参见收集的一些 &lt;a href=&#34;https://focus-creative-games.github.io/HybridCLR/ref_project/&#34;&gt;完整接入的商业项目列表&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;支持与联系&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;开发交流，欢迎加QQ群或邮件联系 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;QQ群：651188171 HybridCLR技术交流群 &lt;strong&gt;(官方主群)&lt;/strong&gt;。可以反馈bug，但&lt;strong&gt;不要在群里咨询基础使用问题&lt;/strong&gt;。&lt;/li&gt; &#xA;   &lt;li&gt;QQ群：428404198 HybridCLR使用疑难咨询群 &lt;strong&gt;(新手群)&lt;/strong&gt;。新手使用过程中遇到问题，都可以在群里咨询。&lt;/li&gt; &#xA;   &lt;li&gt;邮箱：taojingjian#gmail.com&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;RoadMap&lt;/h2&gt; &#xA;&lt;p&gt;HybridCLR虽然与il2cpp相关，但绝大多数核心代码独立于il2cpp，很容易移植（预计一个月）到其他不支持AOT+Interpreter的CLR平台。无论unity如何版本变迁，哪怕废弃了il2cpp改用.net 6+，HybridCLR会持续跟进，稳定地提供跨平台的CLR热更新服务，直至某天.net官方直接支持AOT+Interpreter，则HybridCLR完成其历史使命。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持Unity 2019、2020和2021系列版本 (2022.6 -)&lt;/li&gt; &#xA; &lt;li&gt;支持32位 (2022.6 - 2022.7)&lt;/li&gt; &#xA; &lt;li&gt;指令优化，编译后指令数减少到原来1/4-1/2，基础指令和大多数对象模型指令有100%-300%的性能提升。 (2022.7 -)&lt;/li&gt; &#xA; &lt;li&gt;支持增量式gc (2022.8 -)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;license&lt;/h2&gt; &#xA;&lt;p&gt;HybridCLR is licensed under the &lt;a href=&#34;https://github.com/focus-creative-games/HybridCLR/raw/main/LICENSE&#34;&gt;MIT&lt;/a&gt; license&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>babysor/MockingBird</title>
    <updated>2022-07-08T01:32:37Z</updated>
    <id>tag:github.com,2022-07-08:/babysor/MockingBird</id>
    <link href="https://github.com/babysor/MockingBird" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🚀AI拟声: 5秒内克隆您的声音并生成任意语音内容 Clone a voice in 5 seconds to generate arbitrary speech in real-time&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg&#34; alt=&#34;mockingbird&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://choosealicense.com/licenses/mit/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?style=flat&#34; alt=&#34;MIT License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/babysor/MockingBird/main/README-CN.md&#34;&gt;中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;🌍 &lt;strong&gt;Chinese&lt;/strong&gt; supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.&lt;/p&gt; &#xA;&lt;p&gt;🤩 &lt;strong&gt;PyTorch&lt;/strong&gt; worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060&lt;/p&gt; &#xA;&lt;p&gt;🌍 &lt;strong&gt;Windows + Linux&lt;/strong&gt; run in both Windows OS and linux OS (even in M1 MACOS)&lt;/p&gt; &#xA;&lt;p&gt;🤩 &lt;strong&gt;Easy &amp;amp; Awesome&lt;/strong&gt; effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder&lt;/p&gt; &#xA;&lt;p&gt;🌍 &lt;strong&gt;Webserver Ready&lt;/strong&gt; to serve your result with remote calling&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV17Q4y1B7mY/&#34;&gt;DEMO VIDEO&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h3&gt;Ongoing Works(Helps Needed)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Major upgrade on GUI/Client and unifying web and toolbox [X] Init framework &lt;code&gt;./mkgui&lt;/code&gt; and &lt;a href=&#34;https://vaj2fgg8yn.feishu.cn/docs/doccnvotLWylBub8VJIjKzoEaee&#34;&gt;tech design&lt;/a&gt; [X] Add demo part of Voice Cloning and Conversion [X] Add preprocessing and training for Voice Conversion [ ] Add preprocessing and training for Encoder/Synthesizer/Vocoder&lt;/li&gt; &#xA; &lt;li&gt;Major upgrade on model backend based on ESPnet2(not yet started)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install Requirements&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Follow the original repo to test if you got all environment ready. **Python 3.7 or higher ** is needed to run the toolbox.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you get an &lt;code&gt;ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 )&lt;/code&gt; This error is probably due to a low version of python, try using 3.9 and it will install successfully&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://ffmpeg.org/download.html#get-packages&#34;&gt;ffmpeg&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; to install the remaining necessary packages.&lt;/li&gt; &#xA; &lt;li&gt;Install webrtcvad &lt;code&gt;pip install webrtcvad-wheels&lt;/code&gt;(If you need)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note that we are using the pretrained encoder/vocoder but synthesizer, since the original model is incompatible with the Chinese sympols. It means the demo_cli is not working at this moment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;2. Prepare your models&lt;/h3&gt; &#xA;&lt;p&gt;You can either train your models or use existing ones:&lt;/p&gt; &#xA;&lt;h4&gt;2.1 Train encoder with your dataset (Optional)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Preprocess with the audios and the mel spectrograms: &lt;code&gt;python encoder_preprocess.py &amp;lt;datasets_root&amp;gt;&lt;/code&gt; Allowing parameter &lt;code&gt;--dataset {dataset}&lt;/code&gt; to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the encoder: &lt;code&gt;python encoder_train.py my_run &amp;lt;datasets_root&amp;gt;/SV2TTS/encoder&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For training, the encoder uses visdom. You can disable it with &lt;code&gt;--no_visdom&lt;/code&gt;, but it&#39;s nice to have. Run &#34;visdom&#34; in a separate CLI/process to start your visdom server.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2.2 Train synthesizer with your dataset&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Download dataset and unzip: make sure you can access all .wav in folder&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Preprocess with the audios and the mel spectrograms: &lt;code&gt;python pre.py &amp;lt;datasets_root&amp;gt;&lt;/code&gt; Allowing parameter &lt;code&gt;--dataset {dataset}&lt;/code&gt; to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the synthesizer: &lt;code&gt;python synthesizer_train.py mandarin &amp;lt;datasets_root&amp;gt;/SV2TTS/synthesizer&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Go to next step when you see attention line show and loss meet your need in training folder &lt;em&gt;synthesizer/saved_models/&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2.3 Use pretrained model of synthesizer&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Thanks to the community, some models will be shared:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;author&lt;/th&gt; &#xA;   &lt;th&gt;Download link&lt;/th&gt; &#xA;   &lt;th&gt;Preview Video&lt;/th&gt; &#xA;   &lt;th&gt;Info&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@author&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&#34;&gt;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g&#34;&gt;Baidu&lt;/a&gt; 4j5d&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;75k steps trained by multiple datasets&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@author&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&#34;&gt;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&lt;/a&gt; &lt;a href=&#34;https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw&#34;&gt;Baidu&lt;/a&gt; code：om7f&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;25k steps trained by multiple datasets, only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@FawenYo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing&#34;&gt;https://drive.google.com/file/d/1H-YGOUHpmqKxJ9FRc6vAjPuqQki24UbC/view?usp=sharing&lt;/a&gt; &lt;a href=&#34;https://u.teknik.io/AYxWf.pt&#34;&gt;https://u.teknik.io/AYxWf.pt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3&#34;&gt;input&lt;/a&gt; &lt;a href=&#34;https://github.com/babysor/MockingBird/wiki/audio/export.wav&#34;&gt;output&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;200k steps with local accent of Taiwan, only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;@miven&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ&#34;&gt;https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ&lt;/a&gt; code: 2021 &lt;a href=&#34;https://www.aliyundrive.com/s/AwPsbo8mcSP&#34;&gt;https://www.aliyundrive.com/s/AwPsbo8mcSP&lt;/a&gt; code: z2m0&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1uh411B7AD/&#34;&gt;https://www.bilibili.com/video/BV1uh411B7AD/&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;only works under version 0.0.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;2.4 Train vocoder (Optional)&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;note: vocoder has little difference in effect, so you may not need to train a new one.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Preprocess the data: &lt;code&gt;python vocoder_preprocess.py &amp;lt;datasets_root&amp;gt; -m &amp;lt;synthesizer_model_path&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; replace with your dataset root，&lt;code&gt;&amp;lt;synthesizer_model_path&amp;gt;&lt;/code&gt;replace with directory of your best trained models of sythensizer, e.g. &lt;em&gt;sythensizer\saved_mode\xxx&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the wavernn vocoder: &lt;code&gt;python vocoder_train.py mandarin &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Train the hifigan vocoder &lt;code&gt;python vocoder_train.py mandarin &amp;lt;datasets_root&amp;gt; hifigan&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. Launch&lt;/h3&gt; &#xA;&lt;h4&gt;3.1 Using the web server&lt;/h4&gt; &#xA;&lt;p&gt;You can then try to run:&lt;code&gt;python web.py&lt;/code&gt; and open it in browser, default as &lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.2 Using the Toolbox&lt;/h4&gt; &#xA;&lt;p&gt;You can then try the toolbox: &lt;code&gt;python demo_toolbox.py -d &amp;lt;datasets_root&amp;gt;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.3 Using the command line&lt;/h4&gt; &#xA;&lt;p&gt;You can then try the command: &lt;code&gt;python gen_voice.py &amp;lt;text_file.txt&amp;gt; your_wav_file.wav&lt;/code&gt; you may need to install cn2an by &#34;pip install cn2an&#34; for better digital number result.&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;This repository is forked from &lt;a href=&#34;https://github.com/CorentinJ/Real-Time-Voice-Cloning&#34;&gt;Real-Time-Voice-Cloning&lt;/a&gt; which only support English.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;   &lt;th&gt;Designation&lt;/th&gt; &#xA;   &lt;th&gt;Title&lt;/th&gt; &#xA;   &lt;th&gt;Implementation source&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/1803.09017&#34;&gt;1803.09017&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GlobalStyleToken (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;2010.05646&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;HiFi-GAN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.02297&#34;&gt;2106.02297&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fre-GAN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Fre-GAN: Adversarial Frequency-consistent Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1806.04558.pdf&#34;&gt;&lt;strong&gt;1806.04558&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;SV2TTS&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1802.08435.pdf&#34;&gt;1802.08435&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WaveRNN (vocoder)&lt;/td&gt; &#xA;   &lt;td&gt;Efficient Neural Audio Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.10135.pdf&#34;&gt;1703.10135&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron (synthesizer)&lt;/td&gt; &#xA;   &lt;td&gt;Tacotron: Towards End-to-End Speech Synthesis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/fatchord/WaveRNN&#34;&gt;fatchord/WaveRNN&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1710.10467.pdf&#34;&gt;1710.10467&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GE2E (encoder)&lt;/td&gt; &#xA;   &lt;td&gt;Generalized End-To-End Loss for Speaker Verification&lt;/td&gt; &#xA;   &lt;td&gt;This repo&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;F Q&amp;amp;A&lt;/h2&gt; &#xA;&lt;h4&gt;1.Where can I download the dataset?&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Original Source&lt;/th&gt; &#xA;   &lt;th&gt;Alternative Sources&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;aidatatang_200zh&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.openslr.org/62/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;magicdata&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://www.openslr.org/68/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing&#34;&gt;Google Drive (Dev set)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;aishell3&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/93/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;data_aishell&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.openslr.org/33/&#34;&gt;OpenSLR&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;After unzip aidatatang_200zh, you need to unzip all the files under &lt;code&gt;aidatatang_200zh\corpus\train&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;2.What is&lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt;?&lt;/h4&gt; &#xA;&lt;p&gt;If the dataset path is &lt;code&gt;D:\data\aidatatang_200zh&lt;/code&gt;,then &lt;code&gt;&amp;lt;datasets_root&amp;gt;&lt;/code&gt; is&lt;code&gt;D:\data&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;3.Not enough VRAM&lt;/h4&gt; &#xA;&lt;p&gt;Train the synthesizer：adjust the batch_size in &lt;code&gt;synthesizer/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;tts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule&#xA;                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)&#xA;                (2,  2e-4,  80_000,  12),   #&#xA;                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames&#xA;                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)&#xA;                (2,  1e-5, 640_000,  12)],  # lr = learning rate&#xA;//After&#xA;tts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule&#xA;                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)&#xA;                (2,  2e-4,  80_000,  8),   #&#xA;                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames&#xA;                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)&#xA;                (2,  1e-5, 640_000,  8)],  # lr = learning rate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train Vocoder-Preprocess the data：adjust the batch_size in &lt;code&gt;synthesizer/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;### Data Preprocessing&#xA;        max_mel_frames = 900,&#xA;        rescale = True,&#xA;        rescaling_max = 0.9,&#xA;        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.&#xA;//After&#xA;### Data Preprocessing&#xA;        max_mel_frames = 900,&#xA;        rescale = True,&#xA;        rescaling_max = 0.9,&#xA;        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Train Vocoder-Train the vocoder：adjust the batch_size in &lt;code&gt;vocoder/wavernn/hparams.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;//Before&#xA;# Training&#xA;voc_batch_size = 100&#xA;voc_lr = 1e-4&#xA;voc_gen_at_checkpoint = 5&#xA;voc_pad = 2&#xA;&#xA;//After&#xA;# Training&#xA;voc_batch_size = 6&#xA;voc_lr = 1e-4&#xA;voc_gen_at_checkpoint = 5&#xA;voc_pad =2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;4.If it happens &lt;code&gt;RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to issue &lt;a href=&#34;https://github.com/babysor/MockingBird/issues/37&#34;&gt;#37&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;5. How to improve CPU and GPU occupancy rate?&lt;/h4&gt; &#xA;&lt;p&gt;Adjust the batch_size as appropriate to improve&lt;/p&gt; &#xA;&lt;h4&gt;6. What if it happens &lt;code&gt;the page file is too small to complete the operation&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;p&gt;Please refer to this &lt;a href=&#34;https://www.youtube.com/watch?v=Oh6dga-Oy10&amp;amp;ab_channel=CodeProf&#34;&gt;video&lt;/a&gt; and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.&lt;/p&gt; &#xA;&lt;h4&gt;7. When should I stop during training?&lt;/h4&gt; &#xA;&lt;p&gt;FYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps. &lt;img src=&#34;https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png&#34; alt=&#34;attention_step_20500_sample_1&#34;&gt; &lt;img src=&#34;https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png&#34; alt=&#34;step-135500-mel-spectrogram_sample_1&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>asdfugil/haxx</title>
    <updated>2022-07-08T01:32:37Z</updated>
    <id>tag:github.com,2022-07-08:/asdfugil/haxx</id>
    <link href="https://github.com/asdfugil/haxx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Untethered + Unsandboxed code execution haxx as root on iOS 14 - iOS 14.8.1.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;haxx&lt;/h1&gt; &#xA;&lt;p&gt;Untethered + Unsandboxed code execution haxx as root on iOS 14 - iOS 14.8.1.&lt;/p&gt; &#xA;&lt;p&gt;Based on &lt;a href=&#34;https://github.com/zhuowei/CoreTrustDemo&#34;&gt;CoreTrustDemo&lt;/a&gt;, also please note that certificates are not copyrightable.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Note: requires macOS + existing jailbreak&lt;/p&gt; &#xA;&lt;h3&gt;Get up and running&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;On your mac import dev_certificate.p12 into the keychain, and the password is &lt;code&gt;password&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Modify haxx.c to include your own code (if you need it).&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;make&lt;/code&gt; to build&lt;/li&gt; &#xA; &lt;li&gt;On the device, Copy &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd&lt;/code&gt; to &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd.back&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Then replace &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd&lt;/code&gt; with &lt;code&gt;/usr/bin/fileproviderctl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create the &lt;code&gt;/private/var/haxx&lt;/code&gt; directory, mode should be 0777&lt;/li&gt; &#xA; &lt;li&gt;Copy &lt;code&gt;fileproviderctl_internal&lt;/code&gt; and &lt;code&gt;haxx&lt;/code&gt; generated from the build to &lt;code&gt;/usr/local/bin&lt;/code&gt; on the device, mode should be 0755.&lt;/li&gt; &#xA; &lt;li&gt;Profit.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Fixing fileproviderctl&lt;/h3&gt; &#xA;&lt;p&gt;After doing the above steps, &lt;code&gt;fileproviderctl&lt;/code&gt; will be broken, to fix it do the following steps&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Grab a copy of &lt;code&gt;/usr/bin/fileproviderctl&lt;/code&gt; on your device to your mac&lt;/li&gt; &#xA; &lt;li&gt;Patch the binary with GNU sed: &lt;code&gt;gsed -i &#39;s|/usr/local/bin/fileproviderctl_internal|/usr/local/bin/fileproviderctl_XXXXXXXX|g&#39; fileproviderctl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Resign it: &lt;code&gt;codesign -s &#34;Worth Doing Badly iPhone OS Application Signing&#34; --preserve-metadata=entitlements --force fileproviderctl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Put the fixed binary back onto your device.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Removal&lt;/h3&gt; &#xA;&lt;p&gt;To remove the installation, do the following steps&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Copy &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd&lt;/code&gt; to &lt;code&gt;/usr/bin/fileproviderctl&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Move &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd.back&lt;/code&gt; to &lt;code&gt;/System/Library/PrivateFrameworks/CoreAnalytics.framework/Support/analyticsd&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Delete &lt;code&gt;/var/haxx&lt;/code&gt;, &lt;code&gt;/usr/local/bin/fileproviderctl_internal&lt;/code&gt; as well as &lt;code&gt;/usr/local/bin/haxx&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>