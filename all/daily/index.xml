<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-21T01:29:54Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dzoba/gptrpg</title>
    <updated>2023-04-21T01:29:54Z</updated>
    <id>tag:github.com,2023-04-21:/dzoba/gptrpg</id>
    <link href="https://github.com/dzoba/gptrpg" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A demo of an GPT-based agent existing in an RPG-like environment&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;em&gt;LOOKING FOR GPT-4 API ACCESS. IF YOU CAN HELP PLEASE GET IN TOUCH. @ChrisDzoba on Twitter&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;GPTRPG&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/dzoba/gptrpg/main/map.png&#34; alt=&#34;map of the game&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains two things:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A simple RPG-like environment for an LLM-enabled AI Agent to exist in&lt;/li&gt; &#xA; &lt;li&gt;A simple AI Agent connected to the OpenAI API to exist in that environment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It is intended as a proof of concept.&lt;/p&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;GPTRPG is intended to be run locally. To run:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure you have updated the &lt;code&gt;agent/env.json&lt;/code&gt; file with your OpenAI API key.&lt;/li&gt; &#xA; &lt;li&gt;Only tested with node 16.19.0&lt;/li&gt; &#xA; &lt;li&gt;In the &lt;code&gt;gptrpg&lt;/code&gt; directory run &lt;code&gt;npm install&lt;/code&gt; to install dependencies for all projects.&lt;/li&gt; &#xA; &lt;li&gt;Then run &lt;code&gt;npm start&lt;/code&gt; in the root directory. This will start the agent and the front-end. The front-end will be available at &lt;code&gt;http://localhost:3000&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;The Environment&lt;/h2&gt; &#xA;&lt;p&gt;Code for the environment lives in the &lt;code&gt;ui-admin&lt;/code&gt; directory. It is a React project.&lt;/p&gt; &#xA;&lt;p&gt;The environment was made with &lt;a href=&#34;https://www.mapeditor.org/&#34;&gt;Tiled&lt;/a&gt; map editor. The files live in &lt;code&gt;ui-admin/src/assets&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The environment is rendered with &lt;a href=&#34;https://phaser.io/&#34;&gt;Phaser&lt;/a&gt; and the &lt;a href=&#34;https://annoraaq.github.io/grid-engine/&#34;&gt;Grid Engine Plugin&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;The environment consists of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The character (agent)&lt;/li&gt; &#xA; &lt;li&gt;Impassable tiles&lt;/li&gt; &#xA; &lt;li&gt;A plant layer with &#34;plantable&#34; tiles, and plants (not currently in use by agent). Player can plant food on plantable tiles with S key and harvest food with D key.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;The Agent&lt;/h2&gt; &#xA;&lt;p&gt;Code for the agent lives in the &lt;code&gt;agent&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;The agent is a simple AI agent that uses the OpenAI API to make decisions. It communicates with the front-end via a websocket.&lt;/p&gt; &#xA;&lt;p&gt;The agent is provided with a list of possible actions, the state of its surroundings, and its internal state (currently only sleepiness is measured).&lt;/p&gt; &#xA;&lt;h2&gt;Upcoming features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi agent support&lt;/li&gt; &#xA; &lt;li&gt;More agent actions (drink, eat, plant food, harvest food, write poetry, etc.)&lt;/li&gt; &#xA; &lt;li&gt;More agent states (hunger, thirst, etc.)&lt;/li&gt; &#xA; &lt;li&gt;Agent memory&lt;/li&gt; &#xA; &lt;li&gt;Agent goals&lt;/li&gt; &#xA; &lt;li&gt;Agent inventory&lt;/li&gt; &#xA; &lt;li&gt;Deployment to web&lt;/li&gt; &#xA; &lt;li&gt;Human controlled character&lt;/li&gt; &#xA; &lt;li&gt;UI enhancements (agent state, human interactions, etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;p&gt;Currently, GPTRPG runs with the &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; API.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;LOOKING FOR GPT-4 API ACCESS. IF YOU CAN HELP PLEASE GET IN TOUCH. @ChrisDzoba on Twitter&lt;/em&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/dinov2</title>
    <updated>2023-04-21T01:29:54Z</updated>
    <id>tag:github.com,2023-04-21:/facebookresearch/dinov2</id>
    <link href="https://github.com/facebookresearch/dinov2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch code and models for the DINOv2 self-supervised learning method.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, FAIR&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Patrick Labatut, Armand Joulin, Piotr Bojanowski&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://ai.facebook.com/blog/dino-v2-computer-vision-self-supervised-learning/&#34;&gt;&lt;code&gt;Blog&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://dinov2.metademolab.com&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/#citing-dinov2&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;PyTorch implementation and pretrained models for DINOv2. For details, see the paper: &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.07193&#34;&gt;DINOv2: Learning Robust Visual Features without Supervision&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/60359573/230078733-5faffa19-e6ce-4c55-9200-62dd76f8236a.mp4&#34;&gt;https://user-images.githubusercontent.com/60359573/230078733-5faffa19-e6ce-4c55-9200-62dd76f8236a.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Visualization of the three first principal components of the patch features of all frames, mapped to RGB values. &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Pretrained models&lt;/h2&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;# of&lt;br&gt;params&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;k-NN&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;linear&lt;/th&gt; &#xA;   &lt;th&gt;download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;21 M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;79.0%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86 M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;82.1%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;300 M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;1,100 M&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;83.5%&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth&#34;&gt;backbone only&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pretrained models via PyTorch Hub&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install PyTorch (the only required dependency for loading the model). Installing PyTorch with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;p&gt;A corresponding &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/MODEL_CARD.md&#34;&gt;model card&lt;/a&gt; is included in the repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;dinov2_vits14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vits14&#39;)&#xA;dinov2_vitb14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitb14&#39;)&#xA;dinov2_vitl14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitl14&#39;)&#xA;dinov2_vitg14 = torch.hub.load(&#39;facebookresearch/dinov2&#39;, &#39;dinov2_vitg14&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The training and evaluation code requires PyTorch 2.0 and &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xFormers&lt;/a&gt; 0.0.18 as well as a number of other 3rd party packages. Note that the code has only been tested with the specified versions and also expects a Linux environment. To setup all the required dependencies for training and evaluation, please follow the instructions below:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html&#34;&gt;conda&lt;/a&gt;&lt;/em&gt; &lt;strong&gt;(Recommended)&lt;/strong&gt; - Clone the repository and then create and activate a &lt;code&gt;dinov2&lt;/code&gt; conda environment using the provided environment definition:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f conda.yaml&#xA;conda activate dinov2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;a href=&#34;https://pip.pypa.io/en/stable/getting-started/&#34;&gt;pip&lt;/a&gt;&lt;/em&gt; - Clone the repository and then use the provided &lt;code&gt;requirements.txt&lt;/code&gt; to install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;h3&gt;ImageNet-1k&lt;/h3&gt; &#xA;&lt;p&gt;The root directory of the dataset should hold the following contents:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/test/ILSVRC2012_test_00000001.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/test/[..]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/test/ILSVRC2012_test_00100000.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/train/n01440764/n01440764_10026.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/train/[...]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/train/n15075141/n15075141_9993.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/val/n01440764/ILSVRC2012_val_00000293.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/val/[...]&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/val/n15075141/ILSVRC2012_val_00049174.JPEG&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&amp;lt;root&amp;gt;/labels.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;ImageNet-22k&lt;/h3&gt; &#xA;&lt;p&gt;Please adapt the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/dinov2/data/datasets/image_net_22k.py&#34;&gt;dataset class&lt;/a&gt; to match your local setup.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;span&gt;⚠&lt;/span&gt; To execute the commands provided in the next sections for training and evaluation, the &lt;code&gt;dinov2&lt;/code&gt; package should be included in the Python module search path, i.e. simply prefix the command to run with &lt;code&gt;PYTHONPATH=.&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Fast setup: training DINOv2 ViT-L/16 on ImageNet-1k&lt;/h3&gt; &#xA;&lt;p&gt;Run DINOv2 training on 4 A100-80GB nodes (32 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/train/train.py \&#xA;    --nodes 4 \&#xA;    --config-file dinov2/configs/train/vitl16_short.yaml \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \&#xA;    train.dataset_path=ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training time is approximately 1 day and the resulting checkpoint should reach 81.6% on k-NN eval and 82.9% on linear eval.&lt;/p&gt; &#xA;&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; &#xA;&lt;h3&gt;Long setup: training DINOv2 ViT-L/14 on ImageNet-22k&lt;/h3&gt; &#xA;&lt;p&gt;Run DINOv2 training on 12 A100-80GB nodes (96 GPUs) in a SLURM cluster environment with submitit:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/train/train.py \&#xA;    --nodes 12 \&#xA;    --config-file dinov2/configs/train/vitl14.yaml \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt; \&#xA;    train.dataset_path=ImageNet22k:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training time is approximately 3.3 days and the resulting checkpoint should reach 82.0% on k-NN eval and 84.5% on linear eval.&lt;/p&gt; &#xA;&lt;p&gt;The training code saves the weights of the teacher in the &lt;code&gt;eval&lt;/code&gt; folder every 12500 iterations for evaluation.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;The training code regularly saves the teacher weights. In order to evaluate the model, run the following evaluation on a single node:&lt;/p&gt; &#xA;&lt;h3&gt;k-NN classification on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/knn.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/knn \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logistic regression classification on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/log_regression.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/logreg \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Linear classification with data augmentation on ImageNet-1k&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/linear.py \&#xA;    --config-file &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/config.yaml \&#xA;    --pretrained-weights &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/teacher_checkpoint.pth \&#xA;    --output-dir &amp;lt;PATH/TO/OUTPUT/DIR&amp;gt;/eval/training_24999/linear \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We release the weights from evaluating the different models:&lt;/p&gt; &#xA;&lt;table style=&#34;margin: auto&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;ImageNet&lt;br&gt;top-1&lt;/th&gt; &#xA;   &lt;th&gt;linear evaluation&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-S/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;81.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-B/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;84.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-L/14 distilled&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ViT-g/14&lt;/td&gt; &#xA;   &lt;td align=&#34;right&#34;&gt;86.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_linear_head.pth&#34;&gt;linear head weights&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;The performance of the provided pretrained model weights can be evaluated as follows on ImageNet-1k:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python dinov2/run/eval/linear.py \&#xA;    --config-file dinov2/configs/eval/vitg14_pretrain.yaml \&#xA;    --pretrained-weights https://dl.fbaipublicfiles.com/dinov2/dinov2_vitg14/dinov2_vitg14_pretrain.pth \&#xA;    --train-dataset ImageNet:split=TRAIN:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt; \&#xA;    --val-dataset ImageNet:split=VAL:root=&amp;lt;PATH/TO/DATASET&amp;gt;:extra=&amp;lt;PATH/TO/DATASET&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;DINOv2 code and model weights are released under the CC-BY-NC 4.0 license. See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for additional details.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CONTRIBUTING.md&#34;&gt;contributing&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/dinov2/main/CODE_OF_CONDUCT.md&#34;&gt;code of conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing DINOv2&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;🦖&lt;/span&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{oquab2023dinov2,&#xA;  title={DINOv2: Learning Robust Visual Features without Supervision},&#xA;  author={Oquab, Maxime and Darcet, Timothée and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},&#xA;  journal={arXiv:2304.07193},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>pashpashpash/vault-ai</title>
    <updated>2023-04-21T01:29:54Z</updated>
    <id>tag:github.com,2023-04-21:/pashpashpash/vault-ai</id>
    <link href="https://github.com/pashpashpash/vault-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OP Vault ChatGPT: Give ChatGPT long-term memory using the OP Stack (OpenAI + Pinecone Vector Database). Upload your own custom knowledge base files (PDF, txt, etc) using a simple React frontend.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OP Vault&lt;/h1&gt; &#xA;&lt;p&gt;OP Vault uses the OP Stack (OpenAI + Pinecone Vector Database) to enable users to upload their own custom knowledgebase files and ask questions about their contents.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://vault.pash.city&#34;&gt;vault.pash.city&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;512&#34; alt=&#34;Screen Shot 2023-04-09 at 1 53 33 AM&#34; src=&#34;https://raw.githubusercontent.com/pashpashpash/vault-ai/master/static/img/common/vault_library.png&#34;&gt; &#xA;&lt;p&gt;With quick setup, you can launch your own version of this Golang server along with a user-friendly React frontend that allows users to ask OpenAI questions about the specific knowledge base provided. The primary focus is on human-readable content like books, letters, and other documents, making it a practical and valuable tool for knowledge extraction and question-answering. You can upload an entire library&#39;s worth of books and documents and recieve pointed answers along with the name of the file and specific section within the file that the answer is based on!&lt;/p&gt; &#xA;&lt;img width=&#34;1498&#34; alt=&#34;Screen Shot 2023-04-17 at 6 23 00 PM&#34; src=&#34;https://user-images.githubusercontent.com/20898225/232645187-fff56d2b-f654-4c92-b061-4670734b2764.png&#34;&gt; &#xA;&lt;h2&gt;What can you do with OP Vault?&lt;/h2&gt; &#xA;&lt;p&gt;With The Vault, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Upload a variety of popular document types via a simple react frontend to create a custom knowledge base&lt;/li&gt; &#xA; &lt;li&gt;Retrieve accurate and relevant answers based on the content of your uploaded documents&lt;/li&gt; &#xA; &lt;li&gt;See the filenames and specific context snippets that inform the answer&lt;/li&gt; &#xA; &lt;li&gt;Explore the power of the OP Stack (OpenAI + Pinecone Vector Database) in a user-friendly interface&lt;/li&gt; &#xA; &lt;li&gt;Load entire libraries&#39; worth of books into The Vault&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Manual Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;node: v19&lt;/li&gt; &#xA; &lt;li&gt;go: v1.18.9 darwin/arm64&lt;/li&gt; &#xA; &lt;li&gt;poppler&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Install manual dependencies&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install go:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Follow the go docs &lt;a href=&#34;https://go.dev/doc/install&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install node v19&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;I recommend &lt;a href=&#34;https://medium.com/@iam_vinojan/how-to-install-node-js-and-npm-using-node-version-manager-nvm-143165b16ce1&#34;&gt;installing nvm and using it to install node v19&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install poppler&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;sudo apt-get install -y poppler-utils&lt;/code&gt; on Ubuntu, or &lt;code&gt;brew install poppler&lt;/code&gt; on Mac&lt;/p&gt; &#xA;&lt;h3&gt;Set up your API keys and endpoints in the &lt;code&gt;secret&lt;/code&gt; folder&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Create a new file &lt;code&gt;secret/openai_api_key&lt;/code&gt; and paste your &lt;a href=&#34;https://platform.openai.com/docs/api-reference/authentication&#34;&gt;OpenAI API key&lt;/a&gt; into it:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;echo &#34;your_openai_api_key_here&#34; &amp;gt; secret/openai_api_key&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create a new file &lt;code&gt;secret/pinecone_api_key&lt;/code&gt; and paste your &lt;a href=&#34;https://docs.pinecone.io/docs/quickstart#2-get-and-verify-your-pinecone-api-key&#34;&gt;Pinecone API key&lt;/a&gt; into it:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;echo &#34;your_pinecone_api_key_here&#34; &amp;gt; secret/pinecone_api_key&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;When setting up your pinecone index, use a vector size of &lt;code&gt;1536&lt;/code&gt; and keep all the default settings the same.&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Create a new file &lt;code&gt;secret/pinecone_api_endpoint&lt;/code&gt; and paste your &lt;a href=&#34;https://app.pinecone.io/organizations/&#34;&gt;Pinecone API endpoint&lt;/a&gt; into it:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;echo &#34;https://example-50709b5.svc.asia-southeast1-gcp.pinecone.io&#34; &amp;gt; secret/pinecone_api_endpoint&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Running the development environment&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install javascript package dependencies:&lt;/p&gt; &lt;p&gt;&lt;code&gt;npm install&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the golang webserver (default port &lt;code&gt;:8100&lt;/code&gt;):&lt;/p&gt; &lt;p&gt;&lt;code&gt;npm start&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In another terminal window, run webpack to compile the js code and create a bundle.js file:&lt;/p&gt; &lt;p&gt;&lt;code&gt;npm run dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Visit the local version of the site at &lt;a href=&#34;http://localhost:8100&#34;&gt;http://localhost:8100&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Screenshots:&lt;/h2&gt; &#xA;&lt;p&gt;In the example screenshots, I uploaded a couple of books by Plato and some letters by Alexander Hamilton, showcasing the ability of OP Vault to answer questions based on the uploaded content.&lt;/p&gt; &#xA;&lt;h3&gt;Uploading files&lt;/h3&gt; &#xA;&lt;img width=&#34;1483&#34; alt=&#34;Screen Shot 2023-04-17 at 6 16 40 PM&#34; src=&#34;https://user-images.githubusercontent.com/20898225/232645162-e89dc752-ad69-40d3-9eda-8c9075ddeeda.png&#34;&gt; &#xA;&lt;img width=&#34;1509&#34; alt=&#34;Screen Shot 2023-04-17 at 6 17 29 PM&#34; src=&#34;https://user-images.githubusercontent.com/20898225/232645171-b8eb56d5-8797-4970-b163-e17ed76b5b97.png&#34;&gt; &#xA;&lt;h3&gt;Asking questions&lt;/h3&gt; &#xA;&lt;img width=&#34;1498&#34; alt=&#34;Screen Shot 2023-04-17 at 6 20 25 PM&#34; src=&#34;https://user-images.githubusercontent.com/20898225/232645180-f41b3ebc-e050-4df5-bd47-b2819f480081.png&#34;&gt; &#xA;&lt;img width=&#34;1500&#34; alt=&#34;Screen Shot 2023-04-17 at 6 20 58 PM&#34; src=&#34;https://user-images.githubusercontent.com/20898225/232645183-e28bc0fa-3545-48f3-9374-29529c513fe2.png&#34;&gt; &#xA;&lt;img width=&#34;1498&#34; alt=&#34;Screen Shot 2023-04-17 at 6 23 00 PM&#34; src=&#34;https://user-images.githubusercontent.com/20898225/232645187-fff56d2b-f654-4c92-b061-4670734b2764.png&#34;&gt; &#xA;&lt;h2&gt;Under the hood&lt;/h2&gt; &#xA;&lt;p&gt;The golang server uses POST APIs to process incoming uploads and respond to questions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;/upload&lt;/code&gt; for uploading files&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;/api/question&lt;/code&gt; for answering questions&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;All api endpoints are declared in the &lt;a href=&#34;https://github.com/pashpashpash/vault-ai/raw/master/vault-web-server/main.go#L79-L80&#34;&gt;vault-web-server/main.go&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h3&gt;Uploading files and processing them into embeddings&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/pashpashpash/vault-ai/raw/master/vault-web-server/postapi/fileupload.go#L29&#34;&gt;vault-web-server/postapi/fileupload.go&lt;/a&gt; file contains the &lt;code&gt;UploadHandler&lt;/code&gt; logic for handling incoming uploads on the backend. The UploadHandler function in the postapi package is responsible for handling file uploads (with a maximum total upload size of 300 MB) and processing them into embeddings to store in Pinecone. It accepts PDF and plain text files, extracts text from them, and divides the content into chunks. Using OpenAI API, it obtains embeddings for each chunk and upserts (inserts or updates) the embeddings into Pinecone. The function returns a JSON response containing information about the uploaded files and their processing status.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Limit the size of the request body to MAX_TOTAL_UPLOAD_SIZE (300 MB).&lt;/li&gt; &#xA; &lt;li&gt;Parse the incoming multipart form data with a maximum allowed size of 300 MB.&lt;/li&gt; &#xA; &lt;li&gt;Initialize response data with fields for successful and failed file uploads.&lt;/li&gt; &#xA; &lt;li&gt;Iterate over the uploaded files, and for each file: a. Check if the file size is within the allowed limit (MAX_FILE_SIZE, 300 MB). b. Read the file into memory. c. If the file is a PDF, extract the text from it; otherwise, read the contents as plain text. d. Divide the file contents into chunks. e. Use OpenAI API to obtain embeddings for each chunk. f. Upsert (insert or update) the embeddings into Pinecone. g. Update the response data with information about successful and failed uploads.&lt;/li&gt; &#xA; &lt;li&gt;Return a JSON response containing information about the uploaded files and their processing status.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Storing embeddings into Pinecone db&lt;/h3&gt; &#xA;&lt;p&gt;After getting OpenAI embeddings for each chunk of an uploaded file, the server stores all of the embeddings, along with metadata associated for each embedding in Pinecone DB. The metadata for each embedding is created in the &lt;a href=&#34;https://github.com/pashpashpash/vault-ai/raw/master/vault-web-server/postapi/pinecone.go#L22&#34;&gt;upsertEmbeddingsToPinecone&lt;/a&gt; function, with the following keys and values:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;file_name&lt;/code&gt;: The name of the file from which the text chunk was extracted.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;start&lt;/code&gt;: The starting character position of the text chunk in the original file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;end&lt;/code&gt;: The ending character position of the text chunk in the original file.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;title&lt;/code&gt;: The title of the chunk, which is also the file name in this case.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;text&lt;/code&gt;: The text of the chunk.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This metadata is useful for providing context to the embeddings and is used to display additional information about the matched embeddings when retrieving results from the Pinecone database.&lt;/p&gt; &#xA;&lt;h3&gt;Answering questions&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;QuestionHandler&lt;/code&gt; function in &lt;a href=&#34;https://github.com/pashpashpash/vault-ai/raw/master/vault-web-server/postapi/questions.go#L24&#34;&gt;vault-web-server/postapi/questions.go&lt;/a&gt; is responsible for handling all incoming questions. When a question is entered on the frontend and the user presses &#34;search&#34; (or enter), the server uses the OpenAI embeddings API once again to get an embedding for the question (a.k.a. query vector). This query vector is used to query Pinecone db to get the most relevant context for the question. Finally, a prompt is built by packing the most relevant context + the question in a prompt string that adheres to OpenAI token limits (the go tiktoken library is used to estimate token count).&lt;/p&gt; &#xA;&lt;h3&gt;Frontend info&lt;/h3&gt; &#xA;&lt;p&gt;The frontend is built using &lt;code&gt;React.js&lt;/code&gt; and &lt;code&gt;less&lt;/code&gt; for styling.&lt;/p&gt; &#xA;&lt;h3&gt;Generative question-answering with long-term memory&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d like to read more about this topic, I recommend this post from the pinecone blog:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/openai-gen-qa/&#34;&gt;https://www.pinecone.io/learn/openai-gen-qa/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I hope you enjoy it (:&lt;/p&gt; &#xA;&lt;h2&gt;Uploading larger files&lt;/h2&gt; &#xA;&lt;p&gt;I currently have the max individual file size set to 3MB. If you want to increase this limit, edit the &lt;code&gt;MAX_FILE_SIZE&lt;/code&gt; and &lt;code&gt;MAX_TOTAL_UPLOAD_SIZE&lt;/code&gt; constants in &lt;a href=&#34;https://github.com/pashpashpash/vault-ai/raw/master/vault-web-server/postapi/fileupload.go#L26-L27&#34;&gt;fileupload.go&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>