<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-25T01:29:49Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>DjangoPeng/openai-quickstart</title>
    <updated>2023-07-25T01:29:49Z</updated>
    <id>tag:github.com,2023-07-25:/DjangoPeng/openai-quickstart</id>
    <link href="https://github.com/DjangoPeng/openai-quickstart" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A comprehensive guide to understanding and implementing large language models with hands-on examples using LangChain for AIGC applications.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI Quickstart&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; English | &lt;a href=&#34;https://raw.githubusercontent.com/DjangoPeng/openai-quickstart/main/README-CN.md&#34;&gt;中文&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;This project is designed as a one-stop learning resource for anyone interested in large language models and their application in Artificial Intelligence Governance and Control (AIGC) scenarios. By providing theoretical foundations, development basics, and hands-on examples, this project offers comprehensive guidance on these cutting-edge topics.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Theory and Development Basics of Large Language Models&lt;/strong&gt;: Deep dive into the inner workings of large language models like GPT-4, including their architecture, training methods, applications, and more.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;AIGC Application Development with LangChain&lt;/strong&gt;: Hands-on examples and tutorials using LangChain to develop AIGC applications, demonstrating the practical application of large language models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;You can start by cloning this repository to your local machine:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/DjangoPeng/openai-quickstart.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then navigate to the directory and follow the individual module instructions to get started.&lt;/p&gt; &#xA;&lt;h2&gt;Schedule&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Course Materials&lt;/th&gt; &#xA;   &lt;th&gt;Events&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mon Jul 12 &lt;strong&gt;Week 1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fundamentals of Large Models: Evolution of Theory and Technology &lt;br&gt; - An Initial Exploration of Large Models: Origin and Development &lt;br&gt; - Warm-up: Decoding Attention Mechanism &lt;br&gt; - Milestone of Transformation: The Rise of Transformer &lt;br&gt; - Taking Different Paths: The Choices of GPT and Bert&lt;/td&gt; &#xA;   &lt;td&gt;Suggested Readings:&lt;br&gt;- &lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34;&gt;Attention Mechanism: Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://arxiv.org/abs/1904.02874&#34;&gt;An Attentive Survey of Attention Models&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;Transformer: Attention is All you Need&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://raw.githubusercontent.com/DjangoPeng/openai-quickstart/main/docs/homework_01.md&#34;&gt;Homework&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Thu Jul 16&lt;/td&gt; &#xA;   &lt;td&gt;The GPT Model Family: From Start to Present &lt;br&gt; - From GPT-1 to GPT-3.5: The Evolution &lt;br&gt; - ChatGPT: Where It Wins &lt;br&gt; - GPT-4: A New Beginning &lt;br&gt;Prompt Learning &lt;br&gt; - Chain-of-Thought (CoT): The Pioneering Work &lt;br&gt; - Self-Consistency: Multi-path Reasoning &lt;br&gt; - Tree-of-Thoughts (ToT): Continuing the Story&lt;/td&gt; &#xA;   &lt;td&gt;Suggested Readings:&lt;br&gt;- &lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;GPT-1: Improving Language Understanding by Generative Pre-training&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;GPT-2: Language Models are Unsupervised Multitask Learners&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;GPT-3: Language Models are Few-Shot Learners&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;Additional Readings:&lt;br&gt;- &lt;a href=&#34;https://www.semianalysis.com/p/gpt-4-architecture-infrastructure&#34;&gt;GPT-4: Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://arxiv.org/abs/2303.10130&#34;&gt;GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://arxiv.org/abs/2303.12712&#34;&gt;Sparks of Artificial General Intelligence: Early experiments with GPT-4&lt;/a&gt;&lt;br&gt;&lt;br&gt;&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://raw.githubusercontent.com/DjangoPeng/openai-quickstart/main/docs/homework_02.md&#34;&gt;Homework&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Tue Jul 19 &lt;strong&gt;Week 2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fundamentals of Large Model Development: OpenAI Embedding &lt;br&gt; - The Eve of General Artificial Intelligence &lt;br&gt; - &#34;Three Worlds&#34; and &#34;Turing Test&#34; &lt;br&gt; - Computer Data Representation &lt;br&gt; - Representation Learning and Embedding &lt;br&gt; Embeddings Dev 101 &lt;br&gt; - Course Project: GitHub openai-quickstart &lt;br&gt; - Getting Started with OpenAI Embeddings&lt;/td&gt; &#xA;   &lt;td&gt;Suggested Readings:&lt;br&gt;- &lt;a href=&#34;https://arxiv.org/abs/1206.5538&#34;&gt;Representation Learning: A Review and New Perspectives&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://arxiv.org/abs/1301.3781&#34;&gt;Word2Vec: Efficient Estimation of Word Representations in Vector Space&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://nlp.stanford.edu/pubs/glove.pdf&#34;&gt;GloVe: Global Vectors for Word Representation&lt;/a&gt;&lt;br&gt;&lt;br&gt;Additional Readings:&lt;br&gt;&lt;br&gt;- &lt;a href=&#34;http://www.aclweb.org/anthology/Q15-1016&#34;&gt;Improving Distributional Similarity with Lessons Learned from Word Embeddings&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;http://www.aclweb.org/anthology/D15-1036&#34;&gt;Evaluation methods for unsupervised word embeddings&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;[&lt;a href=&#34;https://raw.githubusercontent.com/DjangoPeng/openai-quickstart/main/docs/homework_03.md&#34;&gt;Homework&lt;/a&gt;]&lt;br&gt;Code:&lt;br&gt;[&lt;a href=&#34;https://raw.githubusercontent.com/DjangoPeng/openai-quickstart/main/openai_api/embedding.ipynb&#34;&gt;embedding&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Sat Jul 23&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI Large Model Development and Application Practice &lt;br&gt; - OpenAI Large Model Development Guide &lt;br&gt; - Overview of OpenAI Language Models &lt;br&gt; - OpenAI GPT-4, GPT-3.5, GPT-3, Moderation &lt;br&gt; - OpenAI Token Billing and Calculation &lt;br&gt;OpenAI API Introduction and Practice &lt;br&gt; - OpenAI Models API &lt;br&gt; - OpenAI Completions API &lt;br&gt; - OpenAI Chat Completions API &lt;br&gt; - Completions vs Chat Completions &lt;br&gt;OpenAI Large Model Application Practice &lt;br&gt; - Initial Exploration of Text Completion &lt;br&gt; - Initial Exploration of Chatbots&lt;/td&gt; &#xA;   &lt;td&gt;Suggested Readings:&lt;br&gt;&lt;br&gt;- &lt;a href=&#34;https://platform.openai.com/docs/models&#34;&gt;OpenAI Models&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/completions-api&#34;&gt;OpenAI Completions API&lt;/a&gt;&lt;br&gt;- &lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/chat-completions-api&#34;&gt;OpenAI Chat Completions API&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Code:&lt;br&gt;[&lt;a href=&#34;https://raw.githubusercontent.com/DjangoPeng/openai-quickstart/main/openai_api/models.ipynb&#34;&gt;models&lt;/a&gt;] &lt;br&gt;[&lt;a href=&#34;https://raw.githubusercontent.com/DjangoPeng/openai-quickstart/main/openai_api/count_tokens_with_tiktoken.ipynb&#34;&gt;tiktoken&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are what make the open source community such an amazing place to learn, inspire, and create. Any contributions you make are greatly appreciated. If you have any suggestions or feature requests, please open an issue first to discuss what you would like to change.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the terms of the Apache-2.0 License . See the &lt;a href=&#34;https://raw.githubusercontent.com/DjangoPeng/openai-quickstart/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;Django Peng - &lt;a href=&#34;mailto:pjt73651@email.com&#34;&gt;pjt73651@email.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Project Link: &lt;a href=&#34;https://github.com/DjangoPeng/openai-quickstart&#34;&gt;https://github.com/DjangoPeng/openai-quickstart&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>omerbt/TokenFlow</title>
    <updated>2023-07-25T01:29:49Z</updated>
    <id>tag:github.com,2023-07-25:/omerbt/TokenFlow</id>
    <link href="https://github.com/omerbt/TokenFlow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official Pytorch Implementation for &#34;TokenFlow: Consistent Diffusion Features for Consistent Video Editing&#34; presenting &#34;TokenFlow&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TokenFlow: Consistent Diffusion Features for Consistent Video Editing&lt;/h1&gt; &#xA;&lt;h2&gt;[&lt;a href=&#34;https://diffusion-tokenflow.github.io/&#34; target=&#34;_blank&#34;&gt;Project Page&lt;/a&gt;]&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-TokenFlow-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/PyTorch-%3E=1.10.0-Red?logo=pytorch&#34; alt=&#34;Pytorch&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/omerbt/TokenFlow/assets/52277000/06080ac3-e1fc-42df-852f-c6c49bfa16eb&#34;&gt;https://github.com/omerbt/TokenFlow/assets/52277000/06080ac3-e1fc-42df-852f-c6c49bfa16eb&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;TokenFlow&lt;/strong&gt; is a framework that enables consistent video editing, using a pre-trained text-to-image diffusion model, without any further training or finetuning.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The generative AI revolution has been recently expanded to videos. Nevertheless, current state-of-the-art video mod- els are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial lay- out and dynamics of the input video. Our method is based on our key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in con- junction with any off-the-shelf text-to-image editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more see the &lt;a href=&#34;https://diffusion-tokenflow.github.io&#34;&gt;project webpage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;CODE IS COMING SOON!&lt;/h2&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>Narasimha1997/fake-sms</title>
    <updated>2023-07-25T01:29:49Z</updated>
    <id>tag:github.com,2023-07-25:/Narasimha1997/fake-sms</id>
    <link href="https://github.com/Narasimha1997/fake-sms" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A simple command line tool using which you can skip phone number based SMS verification by using a temporary phone number that acts like a proxy.&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Fake-SMS&lt;/h2&gt; &#xA;&lt;p&gt;A simple command line tool using which you can skip phone number based SMS verification by using a temporary phone number that acts like a proxy.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Latest update : The tool no longer uses upmasked.com, as the service went down. We are using another provider which provides more phone numbers across more countries. Make sure you pull the main branch before compiling.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Features:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Written in Go-1.15 (with modules support enabled)&lt;/li&gt; &#xA; &lt;li&gt;Provides an interactive CLI, which is easier to use.&lt;/li&gt; &#xA; &lt;li&gt;Provides a local file based DB to save and manage a list of fake phone numbers to help you remember and reuse.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Requirements:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go programming language - 1.15+&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;To build:&lt;/h3&gt; &#xA;&lt;p&gt;The build process is simple, it is just like building any other Go module. Follow the steps below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export GOBIN=$PWD/bin&#xA;go install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will build the binary and place it in &lt;code&gt;bin/&lt;/code&gt;. You can also consider using the pre-built binary which is available under &lt;code&gt;bin/&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Steps to use:&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Register a number in local DB: You can register a number by selecting one of the available numbers as shown below.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Narasimha1997/fake-sms/main/gifs/add.gif&#34; alt=&#34;register-number&#34;&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Get the messages from any registered number: You can select a number which was saved in step-1 and view its messages as a list. The tool will also save the dump as json in the format &lt;code&gt;${PWD}/selected-phone-number.json&lt;/code&gt;. As shown below:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Narasimha1997/fake-sms/main/gifs/messages.gif&#34; alt=&#34;get-messages&#34;&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Optionally, you can choose to delete the rembered numbers or list them.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Acknowledgements&lt;/h4&gt; &#xA;&lt;p&gt;The similar tool is also available in pure shell script. &lt;a href=&#34;https://github.com/sdushantha/tmpsms&#34;&gt;Check this out.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Contributing&lt;/h4&gt; &#xA;&lt;p&gt;The tool is very simple and I don&#39;t think there is any major feature missing. But I would welcome any kind of suggestion, enhancements or a bug-fix from the community. Please open an issue to discuss or directly make a PR!!&lt;/p&gt;</summary>
  </entry>
</feed>