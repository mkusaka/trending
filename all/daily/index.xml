<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-14T01:30:10Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>eknkc/ssr-benchmark</title>
    <updated>2024-04-14T01:30:10Z</updated>
    <id>tag:github.com,2024-04-14:/eknkc/ssr-benchmark</id>
    <link href="https://github.com/eknkc/ssr-benchmark" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Benchmarking JS web framework SSR performance&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;SSR Framework Benchmark&lt;/h1&gt; &#xA;&lt;p&gt;This is an experiment in response to &lt;a href=&#34;https://twitter.com/thdxr/status/1777782835249553517&#34;&gt;https://twitter.com/thdxr/status/1777782835249553517&lt;/a&gt; where it is stated that Next.JS is a lot slower on server side rendering compared to Vanilla React.&lt;/p&gt; &#xA;&lt;p&gt;This is not a comprehensive or scientific test. Just wanted to compare each in a setup a little complex than just printing &lt;code&gt;hello world&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Frameworks&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;(index)&lt;/th&gt; &#xA;   &lt;th&gt;name&lt;/th&gt; &#xA;   &lt;th&gt;ops/sec&lt;/th&gt; &#xA;   &lt;th&gt;average (ms)&lt;/th&gt; &#xA;   &lt;th&gt;samples&lt;/th&gt; &#xA;   &lt;th&gt;body (kb)&lt;/th&gt; &#xA;   &lt;th&gt;duplication&lt;/th&gt; &#xA;   &lt;th&gt;relative to react&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;&#39;react&#39;&lt;/td&gt; &#xA;   &lt;td&gt;779&lt;/td&gt; &#xA;   &lt;td&gt;&#39;1.283&#39;&lt;/td&gt; &#xA;   &lt;td&gt;7793&lt;/td&gt; &#xA;   &lt;td&gt;&#39;97.28&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x1.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;&#39;sveltekit&#39;&lt;/td&gt; &#xA;   &lt;td&gt;607&lt;/td&gt; &#xA;   &lt;td&gt;&#39;1.647&#39;&lt;/td&gt; &#xA;   &lt;td&gt;6073&lt;/td&gt; &#xA;   &lt;td&gt;&#39;184.46&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x2.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;1.28 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;&#39;remix&#39;&lt;/td&gt; &#xA;   &lt;td&gt;462&lt;/td&gt; &#xA;   &lt;td&gt;&#39;2.163&#39;&lt;/td&gt; &#xA;   &lt;td&gt;4624&lt;/td&gt; &#xA;   &lt;td&gt;&#39;189.10&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x2.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;1.69 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&#39;nuxt&#39;&lt;/td&gt; &#xA;   &lt;td&gt;198&lt;/td&gt; &#xA;   &lt;td&gt;&#39;5.040&#39;&lt;/td&gt; &#xA;   &lt;td&gt;1985&lt;/td&gt; &#xA;   &lt;td&gt;&#39;201.05&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x2.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;3.93 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&#39;next-pages&#39;&lt;/td&gt; &#xA;   &lt;td&gt;114&lt;/td&gt; &#xA;   &lt;td&gt;&#39;8.766&#39;&lt;/td&gt; &#xA;   &lt;td&gt;1141&lt;/td&gt; &#xA;   &lt;td&gt;&#39;187.67&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x2.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;6.83 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;&#39;astro&#39;&lt;/td&gt; &#xA;   &lt;td&gt;105&lt;/td&gt; &#xA;   &lt;td&gt;&#39;9.447&#39;&lt;/td&gt; &#xA;   &lt;td&gt;1059&lt;/td&gt; &#xA;   &lt;td&gt;&#39;99.91&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x1.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;7.42 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&#39;mfng&#39;&lt;/td&gt; &#xA;   &lt;td&gt;68&lt;/td&gt; &#xA;   &lt;td&gt;&#39;14.609&#39;&lt;/td&gt; &#xA;   &lt;td&gt;685&lt;/td&gt; &#xA;   &lt;td&gt;&#39;317.31&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x2.50&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;11.46 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&#39;next&#39;&lt;/td&gt; &#xA;   &lt;td&gt;53&lt;/td&gt; &#xA;   &lt;td&gt;&#39;18.578&#39;&lt;/td&gt; &#xA;   &lt;td&gt;539&lt;/td&gt; &#xA;   &lt;td&gt;&#39;284.64&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x2.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;14.70 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;react&lt;/strong&gt; is here only as a baseline renderer to compare framework performance with.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Renderers&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;(index)&lt;/th&gt; &#xA;   &lt;th&gt;name&lt;/th&gt; &#xA;   &lt;th&gt;ops/sec&lt;/th&gt; &#xA;   &lt;th&gt;average (ms)&lt;/th&gt; &#xA;   &lt;th&gt;samples&lt;/th&gt; &#xA;   &lt;th&gt;body (kb)&lt;/th&gt; &#xA;   &lt;th&gt;duplication&lt;/th&gt; &#xA;   &lt;th&gt;relative to marko&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;   &lt;td&gt;&#39;marko&#39;&lt;/td&gt; &#xA;   &lt;td&gt;6678&lt;/td&gt; &#xA;   &lt;td&gt;&#39;0.150&#39;&lt;/td&gt; &#xA;   &lt;td&gt;66784&lt;/td&gt; &#xA;   &lt;td&gt;&#39;96.74&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x1.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;&#39;kita&#39;&lt;/td&gt; &#xA;   &lt;td&gt;3151&lt;/td&gt; &#xA;   &lt;td&gt;&#39;0.317&#39;&lt;/td&gt; &#xA;   &lt;td&gt;31518&lt;/td&gt; &#xA;   &lt;td&gt;&#39;97.34&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x1.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;2.12 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;&#39;hono&#39;&lt;/td&gt; &#xA;   &lt;td&gt;948&lt;/td&gt; &#xA;   &lt;td&gt;&#39;1.054&#39;&lt;/td&gt; &#xA;   &lt;td&gt;9486&lt;/td&gt; &#xA;   &lt;td&gt;&#39;97.15&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x1.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;7.04 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;&#39;react&#39;&lt;/td&gt; &#xA;   &lt;td&gt;775&lt;/td&gt; &#xA;   &lt;td&gt;&#39;1.289&#39;&lt;/td&gt; &#xA;   &lt;td&gt;7760&lt;/td&gt; &#xA;   &lt;td&gt;&#39;97.28&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x1.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;8.62 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;&#39;solid&#39;&lt;/td&gt; &#xA;   &lt;td&gt;616&lt;/td&gt; &#xA;   &lt;td&gt;&#39;1.622&#39;&lt;/td&gt; &#xA;   &lt;td&gt;6167&lt;/td&gt; &#xA;   &lt;td&gt;&#39;215.93&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x2.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;10.84 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5&lt;/td&gt; &#xA;   &lt;td&gt;&#39;vue&#39;&lt;/td&gt; &#xA;   &lt;td&gt;289&lt;/td&gt; &#xA;   &lt;td&gt;&#39;3.458&#39;&lt;/td&gt; &#xA;   &lt;td&gt;2892&lt;/td&gt; &#xA;   &lt;td&gt;&#39;96.72&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;x1.00&#39;&lt;/td&gt; &#xA;   &lt;td&gt;&#39;23.11 x slower&#39;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;solid&lt;/strong&gt; is here but it also carries hydration data for client side hydration, it is more comparable to frameworks in that way.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;body&lt;/strong&gt; is the response body length in kb&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;duplication&lt;/strong&gt; is the data duplication factor. 2x means each rendered data item has been observed twice in the response. It is required for hydration to work.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Table has been updated thanks to &lt;a href=&#34;https://github.com/kiliman&#34;&gt;kiliman&lt;/a&gt;. Remix now uses &lt;a href=&#34;https://remix.run/docs/en/main/utils/defer&#34;&gt;defer&lt;/a&gt; yielding much better results.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;mfng&lt;/strong&gt; is a minimal RSC implementation. Important to see its results compared to Next as they both reflect the RSC rendering performance.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Test Environment&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Only SSR. We do not even build the client bundles for most of the modules.&lt;/li&gt; &#xA; &lt;li&gt;Next.JS route cache is disabled using &lt;code&gt;const dynamic = &#39;force-dynamic&#39;&lt;/code&gt;. (Otherwise we would be benchmarking a static http server because there is no dynamic code like accessing cookies.)&lt;/li&gt; &#xA; &lt;li&gt;Instead of going through the http server, the benchmark code creates mock http requests and responses. This ensures that we do not pay for tcp overhead.&lt;/li&gt; &#xA; &lt;li&gt;Tests ran on Node.JS &lt;code&gt;v20.6.1&lt;/code&gt; on my Macbook Pro M1 Pro&lt;/li&gt; &#xA; &lt;li&gt;Each framework renders a table of 1000 rows, each containing two uuid columns.&lt;/li&gt; &#xA; &lt;li&gt;The table data is emulated as async and requires Suspense on react, solid and vue. On Next it is loaded in an async RSC component. On Remix it is loaded in a route &lt;code&gt;loader&lt;/code&gt; function.&lt;/li&gt; &#xA; &lt;li&gt;Streaming rendering used on solid, react and vue.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ npm install&#xA;$ npm run build&#xA;$ npm start&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>PabloMK7/citra</title>
    <updated>2024-04-14T01:30:10Z</updated>
    <id>tag:github.com,2024-04-14:/PabloMK7/citra</id>
    <link href="https://github.com/PabloMK7/citra" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Nintendo 3DS Emulator&lt;/p&gt;&lt;hr&gt;&lt;p&gt;√±&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Lightning-AI/litgpt</title>
    <updated>2024-04-14T01:30:10Z</updated>
    <id>tag:github.com,2024-04-14:/Lightning-AI/litgpt</id>
    <link href="https://github.com/Lightning-AI/litgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Pretrain, finetune, deploy 20+ LLMs on your own data. Uses state-of-the-art techniques: flash attention, FSDP, 4-bit, LoRA, and more.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://pl-public-data.s3.amazonaws.com/assets_lightning/LitStableLM_Badge.png&#34; alt=&#34;LitGPT&#34; width=&#34;128&#34;&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;h1&gt;‚ö° LitGPT&lt;/h1&gt; &#xA; &lt;p&gt;&lt;strong&gt;Pretrain, finetune, deploy 20+ LLMs on your own data&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;Uses the latest state-of-the-art techniques:&lt;/p&gt; &#xA; &lt;p&gt;‚úÖ fp4/8/16/32 &amp;nbsp; &amp;nbsp; ‚úÖ LoRA, QLoRA, Adapter (v1, v2) &amp;nbsp; &amp;nbsp; ‚úÖ flash attention &amp;nbsp; &amp;nbsp; ‚úÖ FSDP &amp;nbsp; &amp;nbsp; ‚úÖ 1-1000+ GPUs/TPUs&lt;/p&gt; &#xA; &lt;hr&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/pytorch-lightning&#34; alt=&#34;PyPI - Python Version&#34;&gt; &lt;img src=&#34;https://github.com/lightning-AI/lit-stablelm/actions/workflows/cpu-tests.yml/badge.svg?sanitize=true&#34; alt=&#34;cpu-tests&#34;&gt; &lt;a href=&#34;https://github.com/Lightning-AI/lit-stablelm/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/VptPCZkGNa&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1077906959069626439?style=plastic&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://lightning.ai/&#34;&gt;Lightning.ai&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#install-litgpt&#34;&gt;Install&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#get-started&#34;&gt;Get started&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#use-an-llm&#34;&gt;Use LLMs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#finetune-an-llm&#34;&gt;Finetune, pretrain LLMs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#choose-from-20-llms&#34;&gt;Models&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#state-of-the-art-features&#34;&gt;Features&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#training-recipes&#34;&gt;Training recipes (YAML)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/#litgpt-design-principles&#34;&gt;Design principles&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Install LitGPT&lt;/h2&gt; &#xA;&lt;p&gt;Install LitGPT with all dependencies (including CLI, quantization, tokenizers for all models, etc.):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#39;litgpt[all]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Advanced install options&lt;/summary&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;p&gt;Install from source:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Lightning-AI/litgpt&#xA;cd litgpt&#xA;pip install -e &#39;.[all]&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Get started&lt;/h1&gt; &#xA;&lt;p&gt;LitGPT is a command-line tool to use, pretrain, finetune and deploy LLMs.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Use an LLM&lt;/h3&gt; &#xA;&lt;p&gt;Here&#39;s an example showing how to use the Mistral 7B LLM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1) Download a pretrained model&#xA;litgpt download --repo_id mistralai/Mistral-7B-Instruct-v0.2&#xA;&#xA;# 2) Chat with the model&#xA;litgpt chat \&#xA;  --checkpoint_dir checkpoints/mistralai/Mistral-7B-Instruct-v0.2&#xA;&#xA;&amp;gt;&amp;gt; Prompt: What do Llamas eat?&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information, refer to the &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/download_model_weights.md&#34;&gt;download&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/inference.md&#34;&gt;inference&lt;/a&gt; tutorials.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Finetune an LLM&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune.md&#34;&gt;Finetune&lt;/a&gt; a model to specialize it on your own custom dataset:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1) Download a pretrained model&#xA;litgpt download --repo_id microsoft/phi-2&#xA;&#xA;# 2) Finetune the model&#xA;curl -L https://huggingface.co/datasets/medalpaca/medical_meadow_health_advice/raw/main/medical_meadow_health_advice.json -o my_custom_dataset.json&#xA;&#xA;litgpt finetune lora \&#xA;  --checkpoint_dir checkpoints/microsoft/phi-2 \&#xA;  --data JSON \&#xA;  --data.json_path my_custom_dataset.json \&#xA;  --val_split_fraction 0.1 \&#xA;  --out_dir out/phi-2-lora&#xA;&#xA;# 3) Chat with the model&#xA;litgpt chat \&#xA;  --checkpoint_dir out/phi-2-lora/final&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pretrain an LLM&lt;/h3&gt; &#xA;&lt;p&gt;Train an LLM from scratch on your own data via pretraining:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p custom_texts&#xA;curl https://www.gutenberg.org/cache/epub/24440/pg24440.txt --output custom_texts/book1.txt&#xA;curl https://www.gutenberg.org/cache/epub/26393/pg26393.txt --output custom_texts/book2.txt&#xA;&#xA;# 1) Download a tokenizer&#xA;litgpt download \&#xA;  --repo_id EleutherAI/pythia-160m \&#xA;  --tokenizer_only True&#xA;&#xA;# 2) Pretrain the model&#xA;litgpt pretrain \&#xA;  --model_name pythia-160m \&#xA;  --tokenizer_dir checkpoints/EleutherAI/pythia-160m \&#xA;  --data TextFiles \&#xA;  --data.train_data_path &#34;custom_texts/&#34; \&#xA;  --train.max_tokens 10_000_000 \&#xA;  --out_dir out/custom-model&#xA;&#xA;# 3) Chat with the model&#xA;litgpt chat \&#xA;  --checkpoint_dir out/custom-model/final&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Continue pretraining an LLM&lt;/h3&gt; &#xA;&lt;p&gt;This is another way of finetuning that specialize an already pretrained model by training on custom data:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;mkdir -p custom_texts&#xA;curl https://www.gutenberg.org/cache/epub/24440/pg24440.txt --output custom_texts/book1.txt&#xA;curl https://www.gutenberg.org/cache/epub/26393/pg26393.txt --output custom_texts/book2.txt&#xA;&#xA;# 1) Download a pretrained model&#xA;litgpt download --repo_id EleutherAI/pythia-160m&#xA;&#xA;# 2) Continue pretraining the model&#xA;litgpt pretrain \&#xA;  --model_name pythia-160m \&#xA;  --initial_checkpoint_dir checkpoints/EleutherAI/pythia-160m \&#xA;  --data TextFiles \&#xA;  --data.train_data_path &#34;custom_texts/&#34; \&#xA;  --train.max_tokens 10_000_000 \&#xA;  --out_dir out/custom-model&#xA;&#xA;# 3) Chat with the model&#xA;litgpt chat \&#xA;  --checkpoint_dir out/custom-model/final&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/0_to_litgpt.md&#34;&gt;Read the full docs&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Choose from 20+ LLMs&lt;/h1&gt; &#xA;&lt;p&gt;Use, Finetune, pretrain, deploy over 20+ LLMs (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/download_model_weights.md&#34;&gt;full list&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Model size&lt;/th&gt; &#xA;   &lt;th&gt;Author&lt;/th&gt; &#xA;   &lt;th&gt;Reference&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeGemma&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.google.dev/gemma/docs/codegemma&#34;&gt;Google Team, Google Deepmind&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Code Llama&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 34B, 70B&lt;/td&gt; &#xA;   &lt;td&gt;Meta AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.12950&#34;&gt;Rozi√®re et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dolly&lt;/td&gt; &#xA;   &lt;td&gt;3B, 7B, 12B&lt;/td&gt; &#xA;   &lt;td&gt;Databricks&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm&#34;&gt;Conover et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Falcon&lt;/td&gt; &#xA;   &lt;td&gt;7B, 40B, 180B&lt;/td&gt; &#xA;   &lt;td&gt;TII UAE&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://falconllm.tii.ae&#34;&gt;TII 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FreeWilly2 (Stable Beluga 2)&lt;/td&gt; &#xA;   &lt;td&gt;70B&lt;/td&gt; &#xA;   &lt;td&gt;Stability AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://stability.ai/blog/stable-beluga-large-instruction-fine-tuned-models&#34;&gt;Stability AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Function Calling Llama 2&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;Trelis&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Trelis/Llama-2-7b-chat-hf-function-calling-v2&#34;&gt;Trelis et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;2B, 7B&lt;/td&gt; &#xA;   &lt;td&gt;Google&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf&#34;&gt;Google Team, Google Deepmind&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 2&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 70B&lt;/td&gt; &#xA;   &lt;td&gt;Meta AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Touvron et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LongChat&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B&lt;/td&gt; &#xA;   &lt;td&gt;LMSYS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lmsys.org/blog/2023-06-29-longchat/&#34;&gt;LongChat Team 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;Mistral AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://mistral.ai/&#34;&gt;Mistral website&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nous-Hermes&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 70B&lt;/td&gt; &#xA;   &lt;td&gt;NousResearch&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/NousResearch&#34;&gt;Org page&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenLLaMA&lt;/td&gt; &#xA;   &lt;td&gt;3B, 7B, 13B&lt;/td&gt; &#xA;   &lt;td&gt;OpenLM Research&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/openlm-research/open_llama&#34;&gt;Geng &amp;amp; Liu 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi&lt;/td&gt; &#xA;   &lt;td&gt;1.3B, 2.7B&lt;/td&gt; &#xA;   &lt;td&gt;Microsoft Research&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.05463&#34;&gt;Li et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Platypus&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 70B&lt;/td&gt; &#xA;   &lt;td&gt;Lee et al.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2308.07317&#34;&gt;Lee, Hunter, and Ruiz 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia&lt;/td&gt; &#xA;   &lt;td&gt;{14,31,70,160,410}M, {1,1.4,2.8,6.9,12}B&lt;/td&gt; &#xA;   &lt;td&gt;EleutherAI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.01373&#34;&gt;Biderman et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RedPajama-INCITE&lt;/td&gt; &#xA;   &lt;td&gt;3B, 7B&lt;/td&gt; &#xA;   &lt;td&gt;Together&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://together.ai/blog/redpajama-models-v1&#34;&gt;Together 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StableCode&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;Stability AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://stability.ai/blog/stablecode-llm-generative-ai-coding&#34;&gt;Stability AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StableLM&lt;/td&gt; &#xA;   &lt;td&gt;3B, 7B&lt;/td&gt; &#xA;   &lt;td&gt;Stability AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Stability-AI/StableLM&#34;&gt;Stability AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StableLM Zephyr&lt;/td&gt; &#xA;   &lt;td&gt;3B&lt;/td&gt; &#xA;   &lt;td&gt;Stability AI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://stability.ai/blog/stablecode-llm-generative-ai-coding&#34;&gt;Stability AI 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TinyLlama&lt;/td&gt; &#xA;   &lt;td&gt;1.1B&lt;/td&gt; &#xA;   &lt;td&gt;Zhang et al.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jzhang38/TinyLlama&#34;&gt;Zhang et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 33B&lt;/td&gt; &#xA;   &lt;td&gt;LMSYS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lmsys.org/blog/2023-03-30-vicuna/&#34;&gt;Li et al. 2023&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;State-of-the-art features&lt;/h2&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;State-of-the-art optimizations: Flash Attention v2, multi-GPU support via fully-sharded data parallelism, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/oom.md#do-sharding-across-multiple-gpus&#34;&gt;optional CPU offloading&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/extensions/xla&#34;&gt;TPU and XLA support&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain.md&#34;&gt;Pretrain&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune.md&#34;&gt;finetune&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/inference.md&#34;&gt;deploy&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Reduce compute requirements with low-precision settings: FP16, BF16, and FP16/FP32 mixed.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Lower memory requirements with &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/quantize.md&#34;&gt;quantization&lt;/a&gt;: 4-bit floats, 8-bit integers, and double quantization.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub&#34;&gt;Configuration files&lt;/a&gt; for great out-of-the-box performance.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Parameter-efficient finetuning: &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_lora.md&#34;&gt;LoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_lora.md&#34;&gt;QLoRA&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_adapter.md&#34;&gt;Adapter&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune_adapter.md&#34;&gt;Adapter v2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/convert_lit_models.md&#34;&gt;Exporting&lt;/a&gt; to other popular model weight formats.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Many popular datasets for &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain.md&#34;&gt;pretraining&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/prepare_dataset.md&#34;&gt;finetuning&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/prepare_dataset.md#preparing-custom-datasets-for-instruction-finetuning&#34;&gt;support for custom datasets&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Readable and easy-to-modify code to experiment with the latest research ideas.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Training recipes&lt;/h1&gt; &#xA;&lt;p&gt;LitGPT comes with validated recipes (YAML configs) to train models under different conditions.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve generated these recipes based on the parameters we found to perform the best for different training conditions.&lt;/p&gt; &#xA;&lt;h3&gt;Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;litgpt finetune lora \&#xA;  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Browse all training recipes &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What is a config&lt;/h3&gt; &#xA;&lt;p&gt;Configs let you customize training for all granular parameters like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# The path to the base model&#39;s checkpoint directory to load for finetuning. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: checkpoints/stabilityai/stablelm-base-alpha-3b)&#xA;checkpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf&#xA;&#xA;# Directory in which to save checkpoints and logs. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: out/lora)&#xA;out_dir: out/finetune/qlora-llama2-7b&#xA;&#xA;# The precision to use for finetuning. Possible choices: &#34;bf16-true&#34;, &#34;bf16-mixed&#34;, &#34;32-true&#34;. (type: Optional[str], default: null)&#xA;precision: bf16-true&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Example: LoRA finetuning config&lt;/summary&gt; &#xA; &lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# The path to the base model&#39;s checkpoint directory to load for finetuning. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: checkpoints/stabilityai/stablelm-base-alpha-3b)&#xA;checkpoint_dir: checkpoints/meta-llama/Llama-2-7b-hf&#xA;&#xA;# Directory in which to save checkpoints and logs. (type: &amp;lt;class &#39;Path&#39;&amp;gt;, default: out/lora)&#xA;out_dir: out/finetune/qlora-llama2-7b&#xA;&#xA;# The precision to use for finetuning. Possible choices: &#34;bf16-true&#34;, &#34;bf16-mixed&#34;, &#34;32-true&#34;. (type: Optional[str], default: null)&#xA;precision: bf16-true&#xA;&#xA;# If set, quantize the model with this algorithm. See ``tutorials/quantize.md`` for more information. (type: Optional[Literal[&#39;nf4&#39;, &#39;nf4-dq&#39;, &#39;fp4&#39;, &#39;fp4-dq&#39;, &#39;int8-training&#39;]], default: null)&#xA;quantize: bnb.nf4&#xA;&#xA;# How many devices/GPUs to use. (type: Union[int, str], default: 1)&#xA;devices: 1&#xA;&#xA;# The LoRA rank. (type: int, default: 8)&#xA;lora_r: 32&#xA;&#xA;# The LoRA alpha. (type: int, default: 16)&#xA;lora_alpha: 16&#xA;&#xA;# The LoRA dropout value. (type: float, default: 0.05)&#xA;lora_dropout: 0.05&#xA;&#xA;# Whether to apply LoRA to the query weights in attention. (type: bool, default: True)&#xA;lora_query: true&#xA;&#xA;# Whether to apply LoRA to the key weights in attention. (type: bool, default: False)&#xA;lora_key: false&#xA;&#xA;# Whether to apply LoRA to the value weights in attention. (type: bool, default: True)&#xA;lora_value: true&#xA;&#xA;# Whether to apply LoRA to the output projection in the attention block. (type: bool, default: False)&#xA;lora_projection: false&#xA;&#xA;# Whether to apply LoRA to the weights of the MLP in the attention block. (type: bool, default: False)&#xA;lora_mlp: false&#xA;&#xA;# Whether to apply LoRA to output head in GPT. (type: bool, default: False)&#xA;lora_head: false&#xA;&#xA;# Data-related arguments. If not provided, the default is ``litgpt.data.Alpaca``.&#xA;data:&#xA;  class_path: litgpt.data.Alpaca2k&#xA;  init_args:&#xA;    mask_prompt: false&#xA;    val_split_fraction: 0.05&#xA;    prompt_style: alpaca&#xA;    ignore_index: -100&#xA;    seed: 42&#xA;    num_workers: 4&#xA;    download_dir: data/alpaca2k&#xA;&#xA;# Training-related arguments. See ``litgpt.args.TrainArgs`` for details&#xA;train:&#xA;&#xA;  # Number of optimizer steps between saving checkpoints (type: Optional[int], default: 1000)&#xA;  save_interval: 200&#xA;&#xA;  # Number of iterations between logging calls (type: int, default: 1)&#xA;  log_interval: 1&#xA;&#xA;  # Number of samples between optimizer steps across data-parallel ranks (type: int, default: 128)&#xA;  global_batch_size: 8&#xA;&#xA;  # Number of samples per data-parallel rank (type: int, default: 4)&#xA;  micro_batch_size: 2&#xA;&#xA;  # Number of iterations with learning rate warmup active (type: int, default: 100)&#xA;  lr_warmup_steps: 10&#xA;&#xA;  # Number of epochs to train on (type: Optional[int], default: 5)&#xA;  epochs: 4&#xA;&#xA;  # Total number of tokens to train on (type: Optional[int], default: null)&#xA;  max_tokens:&#xA;&#xA;  # Limits the number of optimizer steps to run (type: Optional[int], default: null)&#xA;  max_steps:&#xA;&#xA;  # Limits the length of samples (type: Optional[int], default: null)&#xA;  max_seq_length: 512&#xA;&#xA;  # Whether to tie the embedding weights with the language modeling head weights (type: Optional[bool], default: null)&#xA;  tie_embeddings:&#xA;&#xA;  #   (type: float, default: 0.0003)&#xA;  learning_rate: 0.0002&#xA;&#xA;  #   (type: float, default: 0.02)&#xA;  weight_decay: 0.0&#xA;&#xA;  #   (type: float, default: 0.9)&#xA;  beta1: 0.9&#xA;&#xA;  #   (type: float, default: 0.95)&#xA;  beta2: 0.95&#xA;&#xA;  #   (type: Optional[float], default: null)&#xA;  max_norm:&#xA;&#xA;  #   (type: float, default: 6e-05)&#xA;  min_lr: 6.0e-05&#xA;&#xA;# Evaluation-related arguments. See ``litgpt.args.EvalArgs`` for details&#xA;eval:&#xA;&#xA;  # Number of optimizer steps between evaluation calls (type: int, default: 100)&#xA;  interval: 100&#xA;&#xA;  # Number of tokens to generate (type: Optional[int], default: 100)&#xA;  max_new_tokens: 100&#xA;&#xA;  # Number of iterations (type: int, default: 100)&#xA;  max_iters: 100&#xA;&#xA;# The name of the logger to send metrics to. (type: Literal[&#39;wandb&#39;, &#39;tensorboard&#39;, &#39;csv&#39;], default: csv)&#xA;logger_name: csv&#xA;&#xA;# The random seed to use for reproducibility. (type: int, default: 1337)&#xA;seed: 1337&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Override config params via CLI&lt;/h3&gt; &#xA;&lt;p&gt;Override any parameter in the CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;litgpt finetune lora \&#xA;  --config https://raw.githubusercontent.com/Lightning-AI/litgpt/main/config_hub/finetune/llama-2-7b/lora.yaml \&#xA;  --lora_r 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Get involved!&lt;/h2&gt; &#xA;&lt;p&gt;We appreciate your feedback and contributions. If you have feature requests, questions, or want to contribute code or config files, please don&#39;t hesitate to use the &lt;a href=&#34;https://github.com/Lightning-AI/litgpt/issues&#34;&gt;GitHub Issue&lt;/a&gt; tracker.&lt;/p&gt; &#xA;&lt;p&gt;We welcome all individual contributors, regardless of their level of experience or hardware. Your contributions are valuable, and we are excited to see what you can accomplish in this collaborative and supportive environment.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Unsure about contributing? Check out our &lt;a href=&#34;https://lightning.ai/pages/community/tutorial/how-to-contribute-to-litgpt/&#34;&gt;How to Contribute to LitGPT&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you have general questions about building with LitGPT, please &lt;a href=&#34;https://discord.gg/VptPCZkGNa&#34;&gt;join our Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Tutorials, how-to guides, and docs&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] We recommend starting with the &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/0_to_litgpt.md&#34;&gt;Zero to LitGPT: Getting Started with Pretraining, Finetuning, and Using LLMs&lt;/a&gt;&lt;/strong&gt; if you are looking to get started with using LitGPT.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Tutorials and in-depth feature documentation can be found below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finetuning, incl. LoRA, QLoRA, and Adapters (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/finetune.md&#34;&gt;tutorials/finetune.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Pretraining (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/pretrain.md&#34;&gt;tutorials/pretrain.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Model evaluation (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/evaluation.md&#34;&gt;tutorials/evaluation.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Supported and custom datasets (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/prepare_dataset.md&#34;&gt;tutorials/prepare_dataset.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Quantization (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/quantize.md&#34;&gt;tutorials/quantize.md&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Tips for dealing with out-of-memory (OOM) errors (&lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/tutorials/oom.md&#34;&gt;tutorials/oom.md&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;XLA&lt;/h2&gt; &#xA;&lt;p&gt;Lightning AI has partnered with Google to add first-class support for &lt;a href=&#34;https://cloud.google.com/tpu&#34;&gt;Cloud TPUs&lt;/a&gt; in &lt;a href=&#34;https://github.com/Lightning-AI/lightning&#34;&gt;Lightning&#39;s frameworks&lt;/a&gt; and LitGPT, helping democratize AI for millions of developers and researchers worldwide.&lt;/p&gt; &#xA;&lt;p&gt;Using TPUs with Lightning is as straightforward as changing one line of code.&lt;/p&gt; &#xA;&lt;p&gt;We provide scripts fully optimized for TPUs in the &lt;a href=&#34;https://raw.githubusercontent.com/Lightning-AI/litgpt/main/extensions/xla&#34;&gt;XLA directory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This implementation extends on &lt;a href=&#34;https://github.com/lightning-AI/lit-llama&#34;&gt;Lit-LLaMA&lt;/a&gt; and &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;, and it&#39;s &lt;strong&gt;powered by &lt;a href=&#34;https://lightning.ai/docs/fabric/stable/&#34;&gt;Lightning Fabric&lt;/a&gt; ‚ö°&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy&#34;&gt;@karpathy&lt;/a&gt; for &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI&#34;&gt;@EleutherAI&lt;/a&gt; for &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX&lt;/a&gt; and the &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;Evaluation Harness&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TimDettmers&#34;&gt;@TimDettmers&lt;/a&gt; for &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft&#34;&gt;@Microsoft&lt;/a&gt; for &lt;a href=&#34;https://github.com/microsoft/LoRA&#34;&gt;LoRA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tridao&#34;&gt;@tridao&lt;/a&gt; for &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;Flash Attention 2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Community showcase&lt;/h2&gt; &#xA;&lt;p&gt;Check out the projects below that use and build on LitGPT. If you have a project you&#39;d like to add to this section, please don&#39;t hesitate to open a pull request.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üèÜ NeurIPS 2023 Large Language Model Efficiency Challenge: 1 LLM + 1 GPU + 1 Day&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The LitGPT repository was the official starter kit for the &lt;a href=&#34;https://llm-efficiency-challenge.github.io&#34;&gt;NeurIPS 2023 LLM Efficiency Challenge&lt;/a&gt;, which is a competition focused on finetuning an existing non-instruction tuned LLM for 24 hours on a single GPU.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ü¶ô TinyLlama: An Open-Source Small Language Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;LitGPT powered the &lt;a href=&#34;https://github.com/jzhang38/TinyLlama&#34;&gt;TinyLlama project&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2401.02385&#34;&gt;TinyLlama: An Open-Source Small Language Model&lt;/a&gt; research paper.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üç™ MicroLlama: MicroLlama-300M&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/keeeeenw/MicroLlama&#34;&gt;MicroLlama&lt;/a&gt; is a 300M Llama model pretrained on 50B tokens powered by TinyLlama and LitGPT.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use LitGPT in your research, please cite the following work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{litgpt-2023,&#xA;  author       = {Lightning AI},&#xA;  title        = {LitGPT},&#xA;  howpublished = {\url{https://github.com/Lightning-AI/litgpt}},&#xA;  year         = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;LitGPT is released under the &lt;a href=&#34;https://github.com/Lightning-AI/litgpt/raw/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt; license.&lt;/p&gt;</summary>
  </entry>
</feed>