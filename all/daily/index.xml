<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-07-18T01:30:06Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Kyome22/RunCat365</title>
    <updated>2025-07-18T01:30:06Z</updated>
    <id>tag:github.com,2025-07-18:/Kyome22/RunCat365</id>
    <link href="https://github.com/Kyome22/RunCat365" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A cute running cat animation on your windows taskbar.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RunCat 365&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;A cute running cat animation on your Windows Taskbar.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!CAUTION]&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;This project is for Windows, so we do not accept inquiries about macOS version.&lt;/li&gt; &#xA;  &lt;li&gt;We do not accept issues or pull requests in languages other than English.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Kyome22/RunCat365/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/Kyome22/RunCat365&#34; alt=&#34;Github issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Kyome22/RunCat365/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/Kyome22/RunCat365&#34; alt=&#34;Github forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Kyome22/RunCat365/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Kyome22/RunCat365&#34; alt=&#34;Github stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Kyome22/RunCat365/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/languages/top/Kyome22/RunCat365&#34; alt=&#34;Top language&#34;&gt;&lt;/a&gt; &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/Kyome22/RunCat365&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Kyome22/RunCat365/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/Kyome22/RunCat365&#34; alt=&#34;Github license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;C#&lt;/code&gt; &lt;code&gt;.NET 9.0&lt;/code&gt; &lt;code&gt;Visual Studio&lt;/code&gt; &lt;code&gt;RunCat&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Demo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Kyome22/RunCat365/main/docs/images/demo.gif&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;a href=&#34;https://github.com/Kyome22/RunCat365/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=Kyome22/RunCat365&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>langchain-ai/open_deep_research</title>
    <updated>2025-07-18T01:30:06Z</updated>
    <id>tag:github.com,2025-07-18:/langchain-ai/open_deep_research</id>
    <link href="https://github.com/langchain-ai/open_deep_research" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open Deep Research&lt;/h1&gt; &#xA;&lt;img width=&#34;1388&#34; height=&#34;298&#34; alt=&#34;full_diagram&#34; src=&#34;https://github.com/user-attachments/assets/12a2371b-8be2-4219-9b48-90503eb43c69&#34;&gt; &#xA;&lt;p&gt;Deep research has broken out as one of the most popular agent applications. This is a simple, configurable, fully open source deep research agent that works across many model providers, search tools, and MCP servers.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read more in our &lt;a href=&#34;https://blog.langchain.com/open-deep-research/&#34;&gt;blog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;See our &lt;a href=&#34;https://www.youtube.com/watch?v=agGiWUpxkhg&#34;&gt;video&lt;/a&gt; for a quick overview&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üöÄ Quickstart&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository and activate a virtual environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/langchain-ai/open_deep_research.git&#xA;cd open_deep_research&#xA;uv venv&#xA;source .venv/bin/activate  # On Windows: .venv\Scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;uv pip install -r pyproject.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Set up your &lt;code&gt;.env&lt;/code&gt; file to customize the environment variables (for model selection, search tools, and other configuration settings):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Launch the assistant with the LangGraph server locally to open LangGraph Studio in your browser:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install dependencies and start the LangGraph server&#xA;uvx --refresh --from &#34;langgraph-cli[inmem]&#34; --with-editable . --python 3.11 langgraph dev --allow-blocking&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use this to open the Studio UI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;- üöÄ API: http://127.0.0.1:2024&#xA;- üé® Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024&#xA;- üìö API Docs: http://127.0.0.1:2024/docs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img width=&#34;817&#34; height=&#34;666&#34; alt=&#34;Screenshot 2025-07-13 at 11 21 12‚ÄØPM&#34; src=&#34;https://github.com/user-attachments/assets/052f2ed3-c664-4a4f-8ec2-074349dcaa3f&#34;&gt; &#xA;&lt;p&gt;Ask a question in the &lt;code&gt;messages&lt;/code&gt; input field and click &lt;code&gt;Submit&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Configurations&lt;/h3&gt; &#xA;&lt;p&gt;Open Deep Research offers extensive configuration options to customize the research process and model behavior. All configurations can be set via the web UI, environment variables, or by modifying the configuration directly.&lt;/p&gt; &#xA;&lt;h4&gt;General Settings&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max Structured Output Retries&lt;/strong&gt; (default: 3): Maximum number of retries for structured output calls from models when parsing fails&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Allow Clarification&lt;/strong&gt; (default: true): Whether to allow the researcher to ask clarifying questions before starting research&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max Concurrent Research Units&lt;/strong&gt; (default: 5): Maximum number of research units to run concurrently using sub-agents. Higher values enable faster research but may hit rate limits&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Research Configuration&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Search API&lt;/strong&gt; (default: Tavily): Choose from Tavily (works with all models), OpenAI Native Web Search, Anthropic Native Web Search, or None&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max Researcher Iterations&lt;/strong&gt; (default: 3): Number of times the Research Supervisor will reflect on research and ask follow-up questions&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max React Tool Calls&lt;/strong&gt; (default: 5): Maximum number of tool calling iterations in a single researcher step&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Models&lt;/h4&gt; &#xA;&lt;p&gt;Open Deep Research uses multiple specialized models for different research tasks:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Summarization Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1-nano&lt;/code&gt;): Summarizes research results from search APIs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Research Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1&lt;/code&gt;): Conducts research and analysis&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Compression Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1-mini&lt;/code&gt;): Compresses research findings from sub-agents&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Final Report Model&lt;/strong&gt; (default: &lt;code&gt;openai:gpt-4.1&lt;/code&gt;): Writes the final comprehensive report&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All models are configured using &lt;a href=&#34;https://python.langchain.com/docs/how_to/chat_models_universal_init/&#34;&gt;init_chat_model() API&lt;/a&gt; which supports providers like OpenAI, Anthropic, Google Vertex AI, and others.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important Model Requirements:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Structured Outputs&lt;/strong&gt;: All models must support structured outputs. Check support &lt;a href=&#34;https://python.langchain.com/docs/integrations/chat/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Search API Compatibility&lt;/strong&gt;: Research and Compression models must support your selected search API:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Anthropic search requires Anthropic models with web search capability&lt;/li&gt; &#xA;   &lt;li&gt;OpenAI search requires OpenAI models with web search capability&lt;/li&gt; &#xA;   &lt;li&gt;Tavily works with all models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Tool Calling&lt;/strong&gt;: All models must support tool calling functionality&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Special Configurations&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For OpenRouter: Follow &lt;a href=&#34;https://github.com/langchain-ai/open_deep_research/issues/75#issuecomment-2811472408&#34;&gt;this guide&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;For local models via Ollama: See &lt;a href=&#34;https://github.com/langchain-ai/open_deep_research/issues/65#issuecomment-2743586318&#34;&gt;setup instructions&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Example MCP (Model Context Protocol) Servers&lt;/h4&gt; &#xA;&lt;p&gt;Open Deep Research supports MCP servers to extend research capabilities.&lt;/p&gt; &#xA;&lt;h4&gt;Local MCP Servers&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Filesystem MCP Server&lt;/strong&gt; provides secure file system operations with robust access control:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read, write, and manage files and directories&lt;/li&gt; &#xA; &lt;li&gt;Perform operations like reading file contents, creating directories, moving files, and searching&lt;/li&gt; &#xA; &lt;li&gt;Restrict operations to predefined directories for security&lt;/li&gt; &#xA; &lt;li&gt;Support for both command-line configuration and dynamic MCP roots&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Example usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mcp-server-filesystem /path/to/allowed/dir1 /path/to/allowed/dir2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Remote MCP Servers&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;Remote MCP servers&lt;/strong&gt; enable distributed agent coordination and support streamable HTTP requests. Unlike local servers, they can be multi-tenant and require more complex authentication.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Arcade MCP Server Example&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;url&#34;: &#34;https://api.arcade.dev/v1/mcps/ms_0ujssxh0cECutqzMgbtXSGnjorm&#34;,&#xA;  &#34;tools&#34;: [&#34;Search_SearchHotels&#34;, &#34;Search_SearchOneWayFlights&#34;, &#34;Search_SearchRoundtripFlights&#34;]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Remote servers can be configured as authenticated or unauthenticated and support JWT-based authentication through OAuth endpoints.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;A comprehensive batch evaluation system designed for detailed analysis and comparative studies.&lt;/p&gt; &#xA;&lt;h4&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-dimensional Scoring&lt;/strong&gt;: Specialized evaluators with 0-1 scale ratings&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Dataset-driven Evaluation&lt;/strong&gt;: Batch processing across multiple test cases&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;strong&gt;Usage:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run comprehensive evaluation on LangSmith datasets&#xA;python tests/run_evaluate.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;&lt;strong&gt;Key Files:&lt;/strong&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;tests/run_evaluate.py&lt;/code&gt;: Main evaluation script&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tests/evaluators.py&lt;/code&gt;: Specialized evaluator functions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;tests/prompts.py&lt;/code&gt;: Evaluation prompts for each dimension&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Deployments and Usages&lt;/h3&gt; &#xA;&lt;h4&gt;LangGraph Studio&lt;/h4&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/langchain-ai/open_deep_research/main/#-quickstart&#34;&gt;quickstart&lt;/a&gt; to start LangGraph server locally and test the agent out on LangGraph Studio.&lt;/p&gt; &#xA;&lt;h4&gt;Hosted deployment&lt;/h4&gt; &#xA;&lt;p&gt;You can easily deploy to &lt;a href=&#34;https://langchain-ai.github.io/langgraph/concepts/#deployment-options&#34;&gt;LangGraph Platform&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Open Agent Platform&lt;/h4&gt; &#xA;&lt;p&gt;Open Agent Platform (OAP) is a UI from which non-technical users can build and configure their own agents. OAP is great for allowing users to configure the Deep Researcher with different MCP tools and search APIs that are best suited to their needs and the problems that they want to solve.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve deployed Open Deep Research to our public demo instance of OAP. All you need to do is add your API Keys, and you can test out the Deep Researcher for yourself! Try it out &lt;a href=&#34;https://oap.langchain.com&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also deploy your own instance of OAP, and make your own custom agents (like Deep Researcher) available on it to your users.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.oap.langchain.com/quickstart&#34;&gt;Deploy Open Agent Platform&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.oap.langchain.com/setup/agents&#34;&gt;Add Deep Researcher to OAP&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Updates üî•&lt;/h3&gt; &#xA;&lt;h3&gt;Legacy Implementations üèõÔ∏è&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;src/legacy/&lt;/code&gt; folder contains two earlier implementations that provide alternative approaches to automated research:&lt;/p&gt; &#xA;&lt;h4&gt;1. Workflow Implementation (&lt;code&gt;legacy/graph.py&lt;/code&gt;)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Plan-and-Execute&lt;/strong&gt;: Structured workflow with human-in-the-loop planning&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sequential Processing&lt;/strong&gt;: Creates sections one by one with reflection&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interactive Control&lt;/strong&gt;: Allows feedback and approval of report plans&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quality Focused&lt;/strong&gt;: Emphasizes accuracy through iterative refinement&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;2. Multi-Agent Implementation (&lt;code&gt;legacy/multi_agent.py&lt;/code&gt;)&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supervisor-Researcher Architecture&lt;/strong&gt;: Coordinated multi-agent system&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parallel Processing&lt;/strong&gt;: Multiple researchers work simultaneously&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speed Optimized&lt;/strong&gt;: Faster report generation through concurrency&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;MCP Support&lt;/strong&gt;: Extensive Model Context Protocol integration&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;code&gt;src/legacy/legacy.md&lt;/code&gt; for detailed documentation, configuration options, and usage examples for both legacy implementations.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>WasmEdge/WasmEdge</title>
    <updated>2025-07-18T01:30:06Z</updated>
    <id>tag:github.com,2025-07-18:/WasmEdge/WasmEdge</id>
    <link href="https://github.com/WasmEdge/WasmEdge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime for cloud native, edge, and decentralized applications. It powers serverless apps, embedded functions, microservices, smart contracts, and IoT devices.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;right&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/README-zh.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/README-zh-TW.md&#34;&gt;Ê≠£È´î‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/README-ja.md&#34;&gt;Êó•Êú¨Ë™û„ÅßË™≠„ÇÄ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/docs/wasmedge-runtime-logo.png&#34; alt=&#34;WasmEdge Logo&#34;&gt;&lt;/p&gt; &#xA; &lt;h1&gt;&lt;a href=&#34;https://llamaedge.com/docs/user-guide/llm/get-started-with-llamaedge&#34;&gt;ü§© WasmEdge is the easiest and fastest way to run LLMs on your own devices. ü§©&lt;/a&gt;&lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/2481&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/2481&#34; alt=&#34;WasmEdge%2FWasmEdge | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;WasmEdge is a lightweight, high-performance, and extensible WebAssembly runtime. It is &lt;a href=&#34;https://ieeexplore.ieee.org/document/9214403&#34;&gt;the fastest Wasm VM&lt;/a&gt;. WasmEdge is an official sandbox project hosted by the &lt;a href=&#34;https://www.cncf.io/&#34;&gt;CNCF&lt;/a&gt;. &lt;a href=&#34;https://github.com/LlamaEdge/LlamaEdge&#34;&gt;LlamaEdge&lt;/a&gt; is an application framework built on top of WasmEdge to run GenAI models (e.g., &lt;a href=&#34;https://llamaedge.com/docs/user-guide/llm/get-started-with-llamaedge&#34;&gt;LLM&lt;/a&gt;, &lt;a href=&#34;https://llamaedge.com/docs/user-guide/speech-to-text/quick-start-whisper&#34;&gt;speech-to-text&lt;/a&gt;, &lt;a href=&#34;https://llamaedge.com/docs/user-guide/text-to-image/quick-start-sd&#34;&gt;text-to-image&lt;/a&gt;, and &lt;a href=&#34;https://github.com/LlamaEdge/whisper-api-server&#34;&gt;TTS&lt;/a&gt;) across GPUs on servers, personal computers, and edge devices. Additional &lt;a href=&#34;https://wasmedge.org/docs/start/usage/use-cases/&#34;&gt;use cases&lt;/a&gt; include microservices on the edge cloud, serverless SaaS APIs, embedded functions, smart contracts, and smart devices.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/WasmEdge/WasmEdge/actions/workflows/build.yml?query=event%3Apush++branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/WasmEdge/WasmEdge/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/WasmEdge/WasmEdge&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/WasmEdge/WasmEdge/branch/master/graph/badge.svg?sanitize=true&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/WasmEdge/WasmEdge/actions/workflows/codeql-analysis.yml?query=event%3Apush++branch%3Amaster&#34;&gt;&lt;img src=&#34;https://github.com/WasmEdge/WasmEdge/actions/workflows/codeql-analysis.yml/badge.svg?sanitize=true&#34; alt=&#34;CodeQL&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge?ref=badge_shield&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge.svg?type=shield&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bestpractices.coreinfrastructure.org/projects/5059&#34;&gt;&lt;img src=&#34;https://bestpractices.coreinfrastructure.org/projects/5059/badge&#34; alt=&#34;CII Best Practices&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Quick start guides&lt;/h1&gt; &#xA;&lt;p&gt;üöÄ &lt;a href=&#34;https://wasmedge.org/docs/start/install&#34;&gt;Install&lt;/a&gt; WasmEdge &lt;br&gt; üë∑üèª‚Äç‚ôÇÔ∏è &lt;a href=&#34;https://wasmedge.org/docs/category/build-wasmedge-from-source&#34;&gt;Build&lt;/a&gt; and &lt;a href=&#34;https://wasmedge.org/docs/contribute/&#34;&gt;contribute to&lt;/a&gt; WasmEdge &lt;br&gt; ‚å®Ô∏è &lt;a href=&#34;https://wasmedge.org/docs/category/running-with-wasmedge&#34;&gt;Run&lt;/a&gt; a standalone Wasm program or a &lt;a href=&#34;https://wasmedge.org/docs/category/develop-wasm-apps-in-javascript&#34;&gt;JavaScript program&lt;/a&gt; from CLI or &lt;a href=&#34;https://wasmedge.org/docs/start/getting-started/quick_start_docker&#34;&gt;Docker&lt;/a&gt; &lt;br&gt; ü§ñ &lt;a href=&#34;https://llamaedge.com/docs/user-guide/llm/get-started-with-llamaedge&#34;&gt;Chat&lt;/a&gt; with an open source LLM via &lt;a href=&#34;https://github.com/LlamaEdge/LlamaEdge&#34;&gt;LlamaEdge&lt;/a&gt; &lt;br&gt; üîå Embed a Wasm function in your &lt;a href=&#34;https://wasmedge.org/docs/category/go-sdk-for-embedding-wasmedge&#34;&gt;Go&lt;/a&gt;, &lt;a href=&#34;https://wasmedge.org/docs/category/rust-sdk-for-embedding-wasmedge&#34;&gt;Rust&lt;/a&gt;, or &lt;a href=&#34;https://wasmedge.org/docs/category/c-sdk-for-embedding-wasmedge&#34;&gt;C&lt;/a&gt; app &lt;br&gt; üõ† Manage and orchestrate Wasm runtimes using &lt;a href=&#34;https://wasmedge.org/docs/category/deploy-wasmedge-apps-in-kubernetes&#34;&gt;Kubernetes&lt;/a&gt;, &lt;a href=&#34;https://wasmedge.org/docs/embed/use-case/yomo&#34;&gt;data streaming frameworks&lt;/a&gt;, and &lt;a href=&#34;https://medium.com/ethereum-on-steroids/running-ethereum-smart-contracts-in-a-substrate-blockchain-56fbc27fc95a&#34;&gt;blockchains&lt;/a&gt; &lt;br&gt; üìö &lt;strong&gt;&lt;a href=&#34;https://wasmedge.org/docs/&#34;&gt;Check out our official documentation&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;The WasmEdge Runtime provides a well-defined execution sandbox for its contained WebAssembly bytecode program. The runtime offers isolation and protection for operating system resources (e.g., file system, sockets, environment variables, processes) and memory space. The most important use case for WasmEdge is to safely execute user-defined or community-contributed code as plug-ins in a software product (e.g., SaaS, software-defined vehicles, edge nodes, or even blockchain nodes). It enables third-party developers, vendors, suppliers, and community members to extend and customize the software product. &lt;strong&gt;&lt;a href=&#34;https://wasmedge.org/docs/contribute/users&#34;&gt;Learn more here&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.07115&#34;&gt;A Lightweight Design for High-performance Serverless Computing&lt;/a&gt;, published on IEEE Software, Jan 2021. &lt;a href=&#34;https://arxiv.org/abs/2010.07115&#34;&gt;https://arxiv.org/abs/2010.07115&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.infoq.com/articles/arm-vs-x86-cloud-performance/&#34;&gt;Performance Analysis for Arm vs. x86 CPUs in the Cloud&lt;/a&gt;, published on infoQ.com, Jan 2021. &lt;a href=&#34;https://www.infoq.com/articles/arm-vs-x86-cloud-performance/&#34;&gt;https://www.infoq.com/articles/arm-vs-x86-cloud-performance/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.suborbital.dev/suborbital-wasmedge&#34;&gt;WasmEdge is the fastest WebAssembly Runtime in Suborbital Reactr test suite&lt;/a&gt;, Dec 2021&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;WasmEdge can run standard WebAssembly bytecode programs compiled from C/C++, Rust, Swift, AssemblyScript, or Kotlin source code. It &lt;a href=&#34;https://wasmedge.org/docs/category/develop-wasm-apps-in-javascript&#34;&gt;runs JavaScript&lt;/a&gt;, including 3rd party ES6, CJS, and NPM modules, in a secure, fast, lightweight, portable, and containerized sandbox. It also supports mixing of those languages (e.g., to &lt;a href=&#34;https://wasmedge.org/docs/develop/javascript/rust&#34;&gt;use Rust to implement a JavaScript API&lt;/a&gt;), the &lt;a href=&#34;https://wasmedge.org/docs/develop/javascript/networking#fetch-client&#34;&gt;Fetch&lt;/a&gt; API, and &lt;a href=&#34;https://wasmedge.org/docs/develop/javascript/ssr&#34;&gt;Server-side Rendering (SSR)&lt;/a&gt; functions on edge servers.&lt;/p&gt; &#xA;&lt;p&gt;WasmEdge supports &lt;a href=&#34;https://wasmedge.org/docs/start/wasmedge/extensions/proposals&#34;&gt;all standard WebAssembly features and many proposed extensions&lt;/a&gt;. It also supports a number of extensions tailored for cloud-native and edge computing uses (e.g., the &lt;a href=&#34;https://wasmedge.org/docs/category/socket-networking&#34;&gt;WasmEdge network sockets&lt;/a&gt;,&lt;a href=&#34;https://wasmedge.org/docs/category/database-drivers&#34;&gt;Postgres and MySQL-based database driver&lt;/a&gt;, and the &lt;a href=&#34;https://wasmedge.org/docs/category/ai-inference&#34;&gt;WasmEdge AI extension&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Learn more about &lt;a href=&#34;https://wasmedge.org/docs/start/wasmedge/features&#34;&gt;technical highlights&lt;/a&gt; of WasmEdge.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Integrations and management&lt;/h2&gt; &#xA;&lt;p&gt;WasmEdge and its contained wasm program can be started from the &lt;a href=&#34;https://wasmedge.org/docs/category/running-with-wasmedge&#34;&gt;CLI&lt;/a&gt; as a new process, or from an existing process. If started from an existing process (e.g., from a running &lt;a href=&#34;https://wasmedge.org/docs/category/go-sdk-for-embedding-wasmedge&#34;&gt;Go&lt;/a&gt; or &lt;a href=&#34;https://wasmedge.org/docs/category/rust-sdk-for-embedding-wasmedge&#34;&gt;Rust&lt;/a&gt; program), WasmEdge will simply run inside the process as a function. Currently, WasmEdge is not yet thread-safe. In order to use WasmEdge in your own application or cloud-native frameworks, please refer to the guides below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wasmedge.org/docs/embed/overview&#34;&gt;Embed WasmEdge into a host application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wasmedge.org/docs/category/deploy-wasmedge-apps-in-kubernetes&#34;&gt;Orchestrate and manage WasmEdge instances using container tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wasmedge.org/docs/develop/rust/dapr&#34;&gt;Run a WasmEdge app as a Dapr microservice&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the community! Please check out our:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/docs/CONTRIBUTING.md&#34;&gt;Contributing Guide&lt;/a&gt; for how to get started&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/docs/GOVERNANCE.md&#34;&gt;Governance documentation&lt;/a&gt; for project decision-making processes&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/docs/CODE_OF_CONDUCT.md&#34;&gt;Code of Conduct&lt;/a&gt; for community standards&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want to become a maintainer? See our &lt;a href=&#34;https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/CONTRIBUTION_LADDER.md&#34;&gt;Contributor Ladder&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://github.com/WasmEdge/WasmEdge/raw/master/docs/ROADMAP.md&#34;&gt;project roadmap&lt;/a&gt; to see the upcoming features and plans for WasmEdge.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, feel free to open a GitHub issue on a related project or to join the following channels:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mailing list: Send an email to &lt;a href=&#34;https://groups.google.com/g/wasmedge/&#34;&gt;WasmEdge@googlegroups.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Discord: Join the &lt;a href=&#34;https://discord.gg/h4KDyB8XTt&#34;&gt;WasmEdge Discord server&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;Slack: Join the #WasmEdge channel on the &lt;a href=&#34;https://slack.cncf.io/&#34;&gt;CNCF Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;X (formerly Twitter): Follow @realwasmedge on &lt;a href=&#34;https://x.com/realwasmedge&#34;&gt;X&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Adopters&lt;/h2&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://wasmedge.org/docs/contribute/users/&#34;&gt;list of Adopters&lt;/a&gt; who are using WasmEdge in their projects.&lt;/p&gt; &#xA;&lt;h2&gt;Community Meeting&lt;/h2&gt; &#xA;&lt;p&gt;We host a monthly community meeting to showcase new features, demo new use cases, and a Q&amp;amp;A part. Everyone is welcome!&lt;/p&gt; &#xA;&lt;p&gt;Time: The first Tuesday of each month at 11PM Hong Kong Time/ 7AM PST.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/document/d/1iFlVl7R97Lze4RDykzElJGDjjWYDlkI8Rhf8g4dQ5Rk/edit#&#34;&gt;Public meeting agenda/notes&lt;/a&gt; | &lt;a href=&#34;https://us06web.zoom.us/j/82221747919?pwd=3MORhaxDk15rACk7mNDvyz9KtaEbWy.1&#34;&gt;Zoom link&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge?ref=badge_large&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2FWasmEdge%2FWasmEdge.svg?type=large&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>