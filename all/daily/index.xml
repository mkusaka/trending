<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-27T01:24:58Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>premAI-io/state-of-open-source-ai</title>
    <updated>2023-10-27T01:24:58Z</updated>
    <id>tag:github.com,2023-10-27:/premAI-io/state-of-open-source-ai</id>
    <link href="https://github.com/premAI-io/state-of-open-source-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Clarity in the current fast-paced mess of Open Source innovation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;üìò The State of Open Source AI (2023 Edition)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://book.premai.io/state-of-open-source-ai&#34;&gt;&lt;img src=&#34;https://static.premai.io/book/marketing/github--book.jpg&#34; alt=&#34;banner&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Clarity in the current fast-paced mess of Open Source innovation.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the source repository for &lt;a href=&#34;https://book.premai.io/state-of-open-source-ai&#34;&gt;The State of Open Source AI&lt;/a&gt; ebook, a comprehensive guide exploring everything from model evaluations to deployment, and a great FOMO cure.&lt;/p&gt; &#xA;&lt;p&gt;Want to discuss any topics covered in the book? We have a &lt;a href=&#34;https://discord.gg/kpKk6vYVAn&#34;&gt;dedicated channel (&lt;code&gt;#book&lt;/code&gt;) on our Discord server&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;You can help keep the book up-to-date! Contributions, issues, and comments are welcome! See the &lt;a href=&#34;https://book.premai.io/state-of-open-source-ai/#contributing&#34;&gt;Contributing Guide&lt;/a&gt; for more information on how.&lt;/p&gt; &#xA;&lt;h2&gt;Licence&lt;/h2&gt; &#xA;&lt;p&gt;This book is released under &lt;a href=&#34;https://raw.githubusercontent.com/premAI-io/state-of-open-source-ai/main/LICENCE&#34;&gt;CC-BY-4.0 (text) and Apache-2.0 (code)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Citation: &lt;a href=&#34;https://raw.githubusercontent.com/premAI-io/state-of-open-source-ai/main/references.bib#L1&#34;&gt;BibTeX&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/kpKk6vYVAn&#34;&gt;Join the Open Source AI Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/premai_io&#34;&gt;Follow us on Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>AI-Citizen/SolidGPT</title>
    <updated>2023-10-27T01:24:58Z</updated>
    <id>tag:github.com,2023-10-27:/AI-Citizen/SolidGPT</id>
    <link href="https://github.com/AI-Citizen/SolidGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat everything with your code repository, ask repository level code questions, and discuss your requirements. AI Scan and learning your code repository, provide you code repository level answerüß± üß±&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/AI-Citizen/SolidGPT/assets/39673228/347a6be2-93d6-42e9-99e2-f8b7b1ea96de&#34; alt=&#34;IMG_4502&#34;&gt;&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;üß± SolidGPT-Technology Business Boosting Framework&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AI-Citizen/SolidGPT/main/docs/Introduction_CN.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ÊñáÊ°£-‰∏≠ÊñáÁâà-blue.svg&#34; alt=&#34;CN doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/SolidGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/SolidGPT?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;üöÄ&amp;nbsp;What‚Äôs this&lt;/h1&gt; &#xA;&lt;p&gt;Chat everything with your code repository, ask repository level code questions, and discuss your requirements. AI scans and learns from your code to seek coding advice, develop coding plans, and generate a product requirement documents using the information in the repository.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Chat with Your Code Repository (Beta) Published&lt;/strong&gt;&lt;/em&gt; Chat with everything with your code repository, ask repository level code questions, and discuss your requirements. &lt;img width=&#34;1506&#34; alt=&#34;Screen Shot 2023-10-18 at 11 10 47 PM&#34; src=&#34;https://github.com/AI-Citizen/SolidGPT/assets/39673228/3164a280-755a-4f05-8848-ec61c570a420&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üî•üî• &lt;a href=&#34;https://calm-flower-017281610.3.azurestaticapps.net/&#34;&gt;Click to try official host SolidGPT&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;If you like our work, please give us a üåü star. Your support serves as a great encouragement for us. Thank you! üòä&lt;/p&gt; &#xA;&lt;h1&gt;üèÅ&amp;nbsp;Quick Start&lt;/h1&gt; &#xA;&lt;h2&gt;üß± &lt;strong&gt;Prerequisite&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python3.8 or above&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openai.com/blog/openai-api&#34;&gt;OpenAI api key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.npmjs.com/downloading-and-installing-node-js-and-npm&#34;&gt;Install npm&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîß &lt;strong&gt;Setup&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/AI-Citizen/SolidGPT.git&#xA;cd SolidGPT &#xA;pip3 install -r requirements.txt #installing the env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set the project root folder as python path &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Linux/Mac&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;export PYTHONPATH=$PYTHONPATH:$(git rev-parse --show-toplevel)/&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Windows&lt;/p&gt; &lt;p&gt;Replace path\to\directory with the path of the project root directory&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;set PYTHONPATH=path\to\directory&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Start Server&lt;/h3&gt; cd into the project root folder(SolidGPT) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Linux/Mac/WSL2&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sh StartServer.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Windows&lt;/p&gt; &lt;p&gt;Install the &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/install&#34;&gt;WSL2&lt;/a&gt; and start the server from WSL2&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;wsl --install&#xA;wsl2&#xA;sh StartServer.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or install directly on Windows [Not Recommended]&lt;/p&gt; &lt;p&gt;Note: redis server needs to be installed before running below commands: &lt;a href=&#34;https://github.com/microsoftarchive/redis/releases&#34;&gt;https://github.com/microsoftarchive/redis/releases&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uvicorn solidgpt.src.api.api:app --reload&#xA;&#xA;celery -A solidgpt.src.api.celery_tasks worker --loglevel=info -P eventlet&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Docker&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker buildx build -t solidgptlocalhost .&#xA;docker run -p 8000:8000 solidgptlocalhost&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;h3&gt;Start UI portal&lt;/h3&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;You&#39;ll need to install npm, and we recommend using version 9.8.1 or higher.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# From the project root folder&#xA;cd solidportal/panel  &#xA;npm i &amp;amp;&amp;amp; npm start&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üè† Introduction&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;SolidGPT first learns from your repository in the &lt;code&gt;Onboard Project&lt;/code&gt; phase.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;After this, choose Generate PRD or Get Tech Solution for customized solutions based on the onboarded project.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìñ Onboarding your project&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Choose &lt;code&gt;Onboard Project&lt;/code&gt; from the top left dropdown.&lt;/li&gt; &#xA; &lt;li&gt;Enter your OpenAI API key.&lt;/li&gt; &#xA; &lt;li&gt;Upload your project folder.ÔºàAll files will be save in your localstorage &lt;code&gt;SolidGPT/localstorage/...&lt;/code&gt;Ôºâ&lt;/li&gt; &#xA; &lt;li&gt;‚ùóÔ∏èNote: After completing the Onboard Project, an Onboard ID will be generated. If you remain in the same browser session, it will be automatically applied to subsequent actions. Alternatively, you can save it and manually input it in the future to bypass onboarding.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üßÆ Get Technical Solution&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Choose &lt;code&gt;Get Tech Solution&lt;/code&gt; from the top left dropdown.&lt;/li&gt; &#xA; &lt;li&gt;Enter your OpenAI API key.&lt;/li&gt; &#xA; &lt;li&gt;Input your problem/Requirement.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: We currently support Python, JavaScript, and TypeScript projects. Support for more languages is on the way.&lt;/p&gt; &#xA;&lt;h2&gt;üìÅ Generate Product Requirement Document&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Choose &lt;code&gt;Generate RPD&lt;/code&gt; from the top left dropdown.&lt;/li&gt; &#xA; &lt;li&gt;Input your requirement (suggest short and clear)&lt;/li&gt; &#xA; &lt;li&gt;Input additional info or your project, SolidGPT will both use a summary from the repository and additional info you provide (optional)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;üì∫ Demo(v0.2.5)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/AI-Citizen/SolidGPT/assets/39673228/8ef57ba1-093e-4cc5-a07d-45b5c2dea850&#34; alt=&#34;copy_FD8819CE-0A56-4E9C-A018-FA90700E7605&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üñáÔ∏è Document&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/READMEv1.md&#34;&gt;Explore SolidGPT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/solidportal.md&#34;&gt;Solid Portal&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/infrastructure.md&#34;&gt;Solid GPT Infrastructure&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/customagentskill.md&#34;&gt;Deeply Customize Agent Skill&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/embeddingprivatedata.md&#34;&gt;Embedding with private data&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/gptfinetuning.md&#34;&gt;Fine-tuning with GPT3.5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/raw/main/docs/llama2finetuning.md&#34;&gt;Fine-tuning with Llama2&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions or feedback about our project, please don&#39;t hesitate to reach out to us. We greatly appreciate your suggestions!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Email: &lt;a href=&#34;mailto:aict@ai-citi.com&#34;&gt;aict@ai-citi.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GitHub Issues: For more technical inquiries, you can also create a new issue in our &lt;a href=&#34;https://github.com/AI-Citizen/SolidGPT/issues&#34;&gt;GitHub repository&lt;/a&gt;. We will respond to all questions within 2-3 business days.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>SUDO-AI-3D/zero123plus</title>
    <updated>2023-10-27T01:24:58Z</updated>
    <id>tag:github.com,2023-10-27:/SUDO-AI-3D/zero123plus</id>
    <link href="https://github.com/SUDO-AI-3D/zero123plus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code repository for Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Zero123++: A Single Image to Consistent Multi-view Diffusion Base Model&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SUDO-AI-3D/zero123plus/main/resources/teaser-low.jpg&#34; alt=&#34;Teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.15110&#34;&gt;[Report]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/sudo-ai/zero123plus-demo-space&#34;&gt;[Official Demo]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/ysharma/Zero123PlusDemo&#34;&gt;[Demo by @yvrjsharma]&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1_5ECnTOosRuAsm2tUp0zvBG0DppL-F3V?usp=sharing&#34;&gt;[Google Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;You will need &lt;code&gt;torch&lt;/code&gt; (recommended &lt;code&gt;2.0&lt;/code&gt; or higher), &lt;code&gt;diffusers&lt;/code&gt; (recommended &lt;code&gt;0.20.2&lt;/code&gt;), and &lt;code&gt;transformers&lt;/code&gt; to start. If you are using &lt;code&gt;torch&lt;/code&gt; &lt;code&gt;1.x&lt;/code&gt;, it is recommended to install &lt;code&gt;xformers&lt;/code&gt; to compute attentions in the model efficiently. The code also runs on older versions of &lt;code&gt;diffusers&lt;/code&gt;, but you may see a decrease in model performance.&lt;/p&gt; &#xA;&lt;p&gt;And you are all set! We provide a custom pipeline for &lt;code&gt;diffusers&lt;/code&gt;, so no extra code is required.&lt;/p&gt; &#xA;&lt;p&gt;To generate multi-view images from a single input image, you can run the following code (also see &lt;a href=&#34;https://raw.githubusercontent.com/SUDO-AI-3D/zero123plus/main/examples/img_to_mv.py&#34;&gt;examples/img_to_mv.py&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import requests&#xA;from PIL import Image&#xA;from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler&#xA;&#xA;# Load the pipeline&#xA;pipeline = DiffusionPipeline.from_pretrained(&#xA;    &#34;sudo-ai/zero123plus-v1.1&#34;, custom_pipeline=&#34;sudo-ai/zero123plus-pipeline&#34;,&#xA;    torch_dtype=torch.float16&#xA;)&#xA;&#xA;# Feel free to tune the scheduler!&#xA;# `timestep_spacing` parameter is not supported in older versions of `diffusers`&#xA;# so there may be performance degradations&#xA;# We recommend using `diffusers==0.20.2`&#xA;pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(&#xA;    pipeline.scheduler.config, timestep_spacing=&#39;trailing&#39;&#xA;)&#xA;pipeline.to(&#39;cuda:0&#39;)&#xA;&#xA;# Download an example image.&#xA;cond = Image.open(requests.get(&#34;https://d.skis.ltd/nrp/sample-data/lysol.png&#34;, stream=True).raw)&#xA;&#xA;# Run the pipeline!&#xA;result = pipeline(cond, num_inference_steps=75).images[0]&#xA;# for general real and synthetic images of general objects&#xA;# usually it is enough to have around 28 inference steps&#xA;# for images with delicate details like faces (real or anime)&#xA;# you may need 75-100 steps for the details to construct&#xA;&#xA;result.show()&#xA;result.save(&#34;output.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above example requires ~5GB VRAM to run. The input image needs to be square, and the recommended image resolution is &lt;code&gt;&amp;gt;=320x320&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By default, Zero123++ generates opaque images with a gray background (the &lt;code&gt;zero&lt;/code&gt; for Stable Diffusion VAE). You may run an extra background removal pass like &lt;code&gt;rembg&lt;/code&gt; to remove the gray background.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# !pip install rembg&#xA;import rembg&#xA;result = rembg.remove(result)&#xA;result.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the depth ControlNet, you can use the following example (also see &lt;a href=&#34;https://raw.githubusercontent.com/SUDO-AI-3D/zero123plus/main/examples/depth_controlnet.py&#34;&gt;examples/depth_controlnet.py&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import requests&#xA;from PIL import Image&#xA;from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler, ControlNetModel&#xA;&#xA;# Load the pipeline&#xA;pipeline = DiffusionPipeline.from_pretrained(&#xA;    &#34;sudo-ai/zero123plus-v1.1&#34;, custom_pipeline=&#34;sudo-ai/zero123plus-pipeline&#34;,&#xA;    torch_dtype=torch.float16&#xA;)&#xA;pipeline.add_controlnet(ControlNetModel.from_pretrained(&#xA;    &#34;sudo-ai/controlnet-zp11-depth-v1&#34;, torch_dtype=torch.float16&#xA;), conditioning_scale=0.75)&#xA;# Feel free to tune the scheduler&#xA;pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(&#xA;    pipeline.scheduler.config, timestep_spacing=&#39;trailing&#39;&#xA;)&#xA;pipeline.to(&#39;cuda:0&#39;)&#xA;# Run the pipeline&#xA;cond = Image.open(requests.get(&#34;https://d.skis.ltd/nrp/sample-data/0_cond.png&#34;, stream=True).raw)&#xA;depth = Image.open(requests.get(&#34;https://d.skis.ltd/nrp/sample-data/0_depth.png&#34;, stream=True).raw)&#xA;result = pipeline(cond, depth_image=depth, num_inference_steps=36).images[0]&#xA;result.show()&#xA;result.save(&#34;output.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This example requires ~5.7GB VRAM to run.&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;The models are available at &lt;a href=&#34;https://huggingface.co/sudo-ai&#34;&gt;https://huggingface.co/sudo-ai&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;sudo-ai/zero123plus-v1.1&lt;/code&gt;, base Zero123++ model release (v1.1).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sudo-ai/controlnet-zp11-depth-v1&lt;/code&gt; depth ControlNet checkpoint release (v1) for Zero123++ (v1.1).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The source code for the diffusers custom pipeline is available in the &lt;a href=&#34;https://raw.githubusercontent.com/SUDO-AI-3D/zero123plus/main/diffusers-support&#34;&gt;diffusers-support&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h2&gt;Camera Poses&lt;/h2&gt; &#xA;&lt;p&gt;Output views are a fixed set of camera poses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Azimuth (relative to input view): &lt;code&gt;30, 90, 150, 210, 270, 330&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Elevation (absolute): &lt;code&gt;30, -20, 30, -20, 30, -20&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running Demo Locally&lt;/h2&gt; &#xA;&lt;p&gt;You will need to install extra dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run &lt;code&gt;streamlit run app.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Gradio Demo, you can run &lt;code&gt;python gradio_app.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found Zero123++ helpful, please cite our report:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{shi2023zero123plus,&#xA;      title={Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model}, &#xA;      author={Ruoxi Shi and Hansheng Chen and Zhuoyang Zhang and Minghua Liu and Chao Xu and Xinyue Wei and Linghao Chen and Chong Zeng and Hao Su},&#xA;      year={2023},&#xA;      eprint={2310.15110},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>