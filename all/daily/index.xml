<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-18T01:26:35Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>likejazz/llama3.np</title>
    <updated>2024-05-18T01:26:35Z</updated>
    <id>tag:github.com,2024-05-18:/likejazz/llama3.np</id>
    <link href="https://github.com/likejazz/llama3.np" rel="alternate"></link>
    <summary type="html">&lt;p&gt;llama3.np is pure NumPy implementation for Llama 3 model.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama3.np&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;llama3.np&lt;/code&gt; is pure NumPy implementation for Llama 3 model. For an accurate implementation, I ran the &lt;a href=&#34;https://github.com/karpathy/llama2.c?tab=readme-ov-file#models&#34;&gt;stories15M model&lt;/a&gt; trained by Andrej Karpathy.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For a detailed explanation in English, see &lt;a href=&#34;https://docs.likejazz.com/llama3.np/&#34;&gt;Llama 3 implemented in pure NumPy&lt;/a&gt;. &lt;strong&gt;[English Version]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;한글로 작성된 상세한 설명은 &lt;a href=&#34;https://docs.likejazz.com/llama3.np-ko/&#34;&gt;NumPy로 구현하는 라마 3 모델&lt;/a&gt;을 참고하세요. &lt;strong&gt;[Korean Version]&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ python llama3.py &#34;I have a dream&#34;&#xA;&#34;&#34;&#34;&#xA;I have a dream. He dream of a big, beautiful garden full of flower and tree. He dream of playing with hi friend and eating yummy snack.&#xA;One day, he wa walking in the garden when he saw&#xA;&#xA;Token count: 50, elapsed: 1.53s, 33 tokens/s&#xA;&#34;&#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citing llama3.np&lt;/h2&gt; &#xA;&lt;p&gt;If you use or discuss &lt;code&gt;llama3.np&lt;/code&gt; in your academic research, please cite the project to help spread awareness:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{llama3.np,&#xA;  title = {llama3.np: pure NumPy implementation for Llama 3 model},&#xA;  author = {Sang Park}, &#xA;  howpublished = {\url{https://github.com/likejazz/llama3.np}},&#xA;  note = {llama3.np, MIT License}&#xA;  year = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;p&gt;Thank you to the creators of the following libraries and tools and their contributors:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; - @karpathy&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hscspring/llama.np&#34;&gt;llama.np&lt;/a&gt; - @hscspring&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers/raw/main/src/transformers/models/llama/modeling_llama.py&#34;&gt;modeling_llama.py&lt;/a&gt; - Hugging Face&#39;s Transformers&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;I got a lot of information from the articles below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://42dot.ai/blog/178&#34;&gt;42dot LLM 1.3B&lt;/a&gt; - 42dot&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/@vi.ai_/exploring-and-building-the-llama-3-architecture-a-deep-dive-into-components-coding-and-43d4097cfbbb&#34;&gt;Exploring and building the LLaMA 3 Architecture : A Deep Dive into Components, Coding, and Inference Techniques&lt;/a&gt; - @vi.ai_&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.eleuther.ai/rotary-embeddings/&#34;&gt;Rotary Embeddings: A Relative Revolution&lt;/a&gt; - EleutherAI&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/&#34;&gt;Mastering LLM Techniques: Inference Optimization&lt;/a&gt; - NVIDIA&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenGVLab/InternVL</title>
    <updated>2024-05-18T01:26:35Z</updated>
    <id>tag:github.com,2024-05-18:/OpenGVLab/InternVL</id>
    <link href="https://github.com/OpenGVLab/InternVL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[CVPR 2024 Oral] InternVL Family: A Pioneering Open-Source Alternative to GPT-4V. 接近GPT-4V表现的可商用开源多模态对话模型&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img width=&#34;60&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/InternVL/assets/47669167/7037290e-f474-4d11-b90f-1d8316087bf8&#34;&gt; InternVL Family: Closing the Gap to Commercial Multimodal Models with Open-Source Suites —— A Pioneering Open-Source Alternative to GPT-4V&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/BLOG.md&#34;&gt;[Update Blog]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.14238&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.16821&#34;&gt;[InternVL 1.5 Technical Report]&lt;/a&gt; &lt;a href=&#34;https://internvl.opengvlab.com/&#34;&gt;[Chat Demo]&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/OpenGVLab/InternVL&#34;&gt;[HuggingFace Demo]&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/#quick-start-with-huggingface&#34;&gt;[Quick Start]&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/675877376&#34;&gt;[中文解读]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/9803&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/9803&#34; alt=&#34;OpenGVLab%2FInternVL | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;News🚀🚀🚀&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/05/13&lt;/code&gt;: 🔥 InternVL can now be used as the &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-14B-224px&#34;&gt;text encoder&lt;/a&gt; for diffusion models to support multilingual generation natively in over 110 languages worldwide. See &lt;a href=&#34;https://github.com/mulanai/MuLan&#34;&gt;MuLan&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/04/28&lt;/code&gt;: We release the INT8 version of InternVL-Chat-V1-5, see &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-Int8&#34;&gt;HF link&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/04/28&lt;/code&gt;: We achieve the SOTA performance (75.74) on the Infographics VQA benchmark, see &lt;a href=&#34;https://rrc.cvc.uab.es/?ch=17&amp;amp;com=evaluation&amp;amp;task=3&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/04/18&lt;/code&gt;: InternVL-Chat-V1.5 has been released at &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5&#34;&gt;HF link&lt;/a&gt;, approaching the performance of GPT-4V and Gemini Pro on various benchmarks like MMMU, DocVQA, ChartQA, MathVista, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/02/27&lt;/code&gt;: InternVL is accepted by CVPR 2024! 🎉&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/02/24&lt;/code&gt;: InternVL-Chat models have been included in the &lt;a href=&#34;https://github.com/open-compass/VLMEvalKit&#34;&gt;VLMEvalKit&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/02/21&lt;/code&gt;: &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus&#34;&gt;InternVL-Chat-V1.2-Plus&lt;/a&gt; achieves SOTA performance on MathVista (59.9), MMBench (83.8), and MMVP (58.7). See our &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/BLOG.md&#34;&gt;blog&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/02/12&lt;/code&gt;: InternVL-Chat-V1.2 has been released. It achieves 51.6 on MMMU val and 82.3 on MMBench test. For more details, please refer to our &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/BLOG.md&#34;&gt;blog&lt;/a&gt;, &lt;a href=&#34;https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets&#34;&gt;SFT data&lt;/a&gt; or try our &lt;a href=&#34;https://internvl.opengvlab.com/&#34;&gt;demo&lt;/a&gt;. The model is now available on &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2&#34;&gt;HuggingFace&lt;/a&gt;, and both training/evaluation data and scripts are open-sourced.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/02/04&lt;/code&gt;: &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1&#34;&gt;InternVL-Chat-V1.1&lt;/a&gt; achieves 44.67% on &lt;a href=&#34;https://github.com/tsb0601/MMVP&#34;&gt;MMVP&lt;/a&gt;, higher than GPT-4V!&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/01/27&lt;/code&gt;: We release 448 resolution model, achieving 76.6 on MMBench dev, see &lt;a href=&#34;https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#-evaluation-chinese-models&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/01/24&lt;/code&gt;: InternVL-Chat-V1.1 is released, it supports Chinese and has stronger OCR capability, see &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1&#34;&gt;here&lt;/a&gt; or try our &lt;a href=&#34;https://internvl.opengvlab.com/&#34;&gt;demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;2024/01/16&lt;/code&gt;: We release our &lt;a href=&#34;https://github.com/OpenGVLab/InternVL-MMDetSeg&#34;&gt;customized mmcv/mmsegmentation/mmdetection code&lt;/a&gt;, integrated with DeepSpeed, which can be used for training large-scale object detection and semantic segmentation models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How to install InternVL? &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/INSTALLATION.md&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to fine-tune InternVL? &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/internvl_chat/README.md&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to evaluate InternVL-Chat-V1-5? &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/document/how_to_evaluate_internvl_chat_1_5.md&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to evaluate InternVL-Chat-V1-5 using VLMEvalKit? (Recommend) &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/document/how_to_evaluate_internvl_chat_1_5_using_vlmevalkit.md&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to deploy a local demo? &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/document/how_to_deploy_a_local_demo.md&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;How to run InternVL 1.5-8bit with Nvidia V100 GPU? &lt;a href=&#34;https://github.com/OpenGVLab/InternVL/issues/144&#34;&gt;[link]&lt;/a&gt; &lt;a href=&#34;https://zhuanlan.zhihu.com/p/697188143&#34;&gt;[中文教程]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Inference Acceleration by LMDeploy &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/#inference-acceleration-by-lmdeploy&#34;&gt;[link]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Compared with SOTA VLLMs&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;500&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/InternVL/assets/23737120/38e8a632-229c-4b20-b7e1-77299dfc6cee&#34;&gt;&lt;/p&gt; &#xA;&lt;img width=&#34;1229&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/InternVL/assets/23737120/e9065a58-86fa-47ef-be9a-eb734532e73f&#34;&gt; &#xA;&lt;img width=&#34;1229&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/InternVL/assets/23737120/2b4f2978-36ea-4065-841d-3651c58955ed&#34;&gt; &#xA;&lt;h2&gt;What is InternVL?&lt;/h2&gt; &#xA;&lt;p&gt;InternVL scales up the ViT to &lt;em&gt;&lt;strong&gt;6B parameters&lt;/strong&gt;&lt;/em&gt; and aligns it with LLM.&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vision Large Language Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;   &lt;th&gt;Note&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL−Chat−V1.5-Int8&lt;/td&gt; &#xA;   &lt;td&gt;2024.04.28&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-Int8&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The INT8 version of InternVL-Chat-V1-5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL−Chat−V1.5&lt;/td&gt; &#xA;   &lt;td&gt;2024.04.18&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;support 4K image; super strong OCR; Approaching the performance of GPT-4V and Gemini Pro on various benchmarks like MMMU, DocVQA, ChartQA, MathVista, etc. (🔥new)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL−Chat−V1.2−Plus&lt;/td&gt; &#xA;   &lt;td&gt;2024.02.21&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;more SFT data and stronger&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL−Chat−V1.2&lt;/td&gt; &#xA;   &lt;td&gt;2024.02.11&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;scaling up LLM to 34B&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL−Chat−V1.1&lt;/td&gt; &#xA;   &lt;td&gt;2024.01.24&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;support Chinese and stronger OCR&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL−Chat−19B−448px&lt;/td&gt; &#xA;   &lt;td&gt;2024.02.03&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B-448px&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;448 resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL−Chat−19B&lt;/td&gt; &#xA;   &lt;td&gt;2023.12.25&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English multimodal dialogue&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL−Chat−13B&lt;/td&gt; &#xA;   &lt;td&gt;2023.12.25&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-7B&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;English multimodal dialogue&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vision-Language Foundation Model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Date&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;   &lt;th&gt;Note&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternViT−6B−448px−V1.5&lt;/td&gt; &#xA;   &lt;td&gt;2024.04.20&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;support dynamic resolution, super strong OCR (🔥new)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternViT−6B−448px−V1.2&lt;/td&gt; &#xA;   &lt;td&gt;2024.02.11&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;448 resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternViT−6B−448px−V1.0&lt;/td&gt; &#xA;   &lt;td&gt;2024.01.30&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-0&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;448 resolution&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternViT−6B−224px&lt;/td&gt; &#xA;   &lt;td&gt;2023.12.22&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternViT-6B-224px&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;vision foundation model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;InternVL−14B−224px&lt;/td&gt; &#xA;   &lt;td&gt;2023.12.22&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-14B-224px&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;vision-language foundation model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;What can InternVL do?&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Visual Perception (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Linear-Probe Image Classification &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/classification#-evaluation&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ViT-22B uses the private JFT-3B dataset.&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;#param&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-ReaL&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-V2&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-A&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-R&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-Sketch&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-G&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1.8B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;86.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;63.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;66.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;DINOv2-g&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1.1B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;86.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;78.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;75.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;78.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;62.5&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;EVA-01-CLIP-g&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1.1B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;86.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;70.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;63.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;MAWS-ViT-6.5B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;6.5B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B*&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;21.7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;90.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;−&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternViT-6B (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;5.9B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;88.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;90.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;79.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;89.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;69.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Semantic Segmentation &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/segmentation#-evaluation&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;decoder&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;#param (train/total)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;crop size&lt;/th&gt; &#xA;      &lt;th&gt;mIoU&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-G (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.3M / 1.8B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;512&lt;/td&gt; &#xA;      &lt;td&gt;39.3&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.9M / 21.7B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;34.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternViT-6B (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;Linear&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.5M / 5.9B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;47.2 (+12.6)&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.8B / 22.5B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;52.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternViT-6B (frozen)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;0.4B / 6.3B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;54.9 (+2.2)&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;22.5B / 22.5B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;55.3&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternViT-6B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;UperNet&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;6.3B / 6.3B&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;504&lt;/td&gt; &#xA;      &lt;td&gt;58.9 (+3.6)&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Zero-Shot Image Classification &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#imagenet-variants-and-objectnet&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-A&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-R&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-V2&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-Sketch&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;ObjectNet&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-G&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;80.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;69.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;92.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;73.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;68.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;73.0&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;EVA-02-CLIP-E+&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;82.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;82.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;75.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;71.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;79.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViT-22B*&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;85.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;90.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;80.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;−&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;87.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;73.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;80.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Multilingual Zero-Shot Image Classification &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#multilingual-imagenet-1k&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EN: English, ZH: Chinese, JP: Japanese, Ar: Arabic, IT: Italian&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (EN)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (ZH)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (JP)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (AR)&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IN-1K (IT)&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;Taiyi-CLIP-ViT-H&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;54.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;WuKong-ViT-L-G&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;57.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;CN-CLIP-ViT-H&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;59.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;AltCLIP-ViT-L&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;74.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;59.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;EVA-02-CLIP-E+&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;82.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;41.2&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-XLM-R-H&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;77.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;55.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;53.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;37.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;56.8&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;83.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;64.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;61.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;44.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;65.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Zero-Shot Video Classification [see details]&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;#frame&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;K400&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;K600&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;K700&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-G&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;65.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;66.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;59.2&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;EVA-02-CLIP-E+&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;69.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;69.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;63.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;71.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;71.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;65.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;ViCLIP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;75.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;73.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;66.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;79.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;78.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;71.5&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Cross-Modal Retrieval (click to expand)&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;English Zero-Shot Image-Text Retrieval &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#flickr30k--coco&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;tbody&gt;&#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td rowspan=&#34;3&#34; align=&#34;left&#34;&gt;&lt;b&gt;model&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;6&#34; align=&#34;center&#34;&gt;&lt;b&gt;Flickr30K&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;6&#34; align=&#34;center&#34;&gt;&lt;b&gt;COCO&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;avg&lt;/b&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;image-to-text&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;text-to-image&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;image-to-text&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;text-to-image&lt;/b&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;OpenCLIP-G&lt;/td&gt; &#xA;      &lt;td&gt;92.9&lt;/td&gt; &#xA;      &lt;td&gt;99.3&lt;/td&gt; &#xA;      &lt;td&gt;99.8&lt;/td&gt; &#xA;      &lt;td&gt;79.5&lt;/td&gt; &#xA;      &lt;td&gt;95.0&lt;/td&gt; &#xA;      &lt;td&gt;97.1&lt;/td&gt; &#xA;      &lt;td&gt;67.3&lt;/td&gt; &#xA;      &lt;td&gt;86.9&lt;/td&gt; &#xA;      &lt;td&gt;92.6&lt;/td&gt; &#xA;      &lt;td&gt;51.4&lt;/td&gt; &#xA;      &lt;td&gt;74.9&lt;/td&gt; &#xA;      &lt;td&gt;83.0&lt;/td&gt; &#xA;      &lt;td&gt;85.0&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;EVA-02-CLIP-E+&lt;/td&gt; &#xA;      &lt;td&gt;93.9&lt;/td&gt; &#xA;      &lt;td&gt;99.4&lt;/td&gt; &#xA;      &lt;td&gt;99.8&lt;/td&gt; &#xA;      &lt;td&gt;78.8&lt;/td&gt; &#xA;      &lt;td&gt;94.2&lt;/td&gt; &#xA;      &lt;td&gt;96.8&lt;/td&gt; &#xA;      &lt;td&gt;68.8&lt;/td&gt; &#xA;      &lt;td&gt;87.8&lt;/td&gt; &#xA;      &lt;td&gt;92.8&lt;/td&gt; &#xA;      &lt;td&gt;51.1&lt;/td&gt; &#xA;      &lt;td&gt;75.0&lt;/td&gt; &#xA;      &lt;td&gt;82.7&lt;/td&gt; &#xA;      &lt;td&gt;85.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;EVA-CLIP-8B&lt;/td&gt; &#xA;      &lt;td&gt;95.6&lt;/td&gt; &#xA;      &lt;td&gt;99.6&lt;/td&gt; &#xA;      &lt;td&gt;99.9&lt;/td&gt; &#xA;      &lt;td&gt;80.8&lt;/td&gt; &#xA;      &lt;td&gt;95.5&lt;/td&gt; &#xA;      &lt;td&gt;97.6&lt;/td&gt; &#xA;      &lt;td&gt;70.3&lt;/td&gt; &#xA;      &lt;td&gt;89.3&lt;/td&gt; &#xA;      &lt;td&gt;93.9&lt;/td&gt; &#xA;      &lt;td&gt;53.0&lt;/td&gt; &#xA;      &lt;td&gt;76.0&lt;/td&gt; &#xA;      &lt;td&gt;83.4&lt;/td&gt; &#xA;      &lt;td&gt;86.2&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td&gt;94.7&lt;/td&gt; &#xA;      &lt;td&gt;99.6&lt;/td&gt; &#xA;      &lt;td&gt;99.9&lt;/td&gt; &#xA;      &lt;td&gt;81.7&lt;/td&gt; &#xA;      &lt;td&gt;96.0&lt;/td&gt; &#xA;      &lt;td&gt;98.2&lt;/td&gt; &#xA;      &lt;td&gt;70.6&lt;/td&gt; &#xA;      &lt;td&gt;89.0&lt;/td&gt; &#xA;      &lt;td&gt;93.5&lt;/td&gt; &#xA;      &lt;td&gt;54.1&lt;/td&gt; &#xA;      &lt;td&gt;77.3&lt;/td&gt; &#xA;      &lt;td&gt;84.6&lt;/td&gt; &#xA;      &lt;td&gt;86.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;InternVL-G (ours)&lt;/td&gt; &#xA;      &lt;td&gt;95.7&lt;/td&gt; &#xA;      &lt;td&gt;99.7&lt;/td&gt; &#xA;      &lt;td&gt;99.9&lt;/td&gt; &#xA;      &lt;td&gt;85.0&lt;/td&gt; &#xA;      &lt;td&gt;97.0&lt;/td&gt; &#xA;      &lt;td&gt;98.6&lt;/td&gt; &#xA;      &lt;td&gt;74.9&lt;/td&gt; &#xA;      &lt;td&gt;91.3&lt;/td&gt; &#xA;      &lt;td&gt;95.2&lt;/td&gt; &#xA;      &lt;td&gt;58.6&lt;/td&gt; &#xA;      &lt;td&gt;81.3&lt;/td&gt; &#xA;      &lt;td&gt;88.0&lt;/td&gt; &#xA;      &lt;td&gt;88.8&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt;&#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Chinese Zero-Shot Image-Text Retrieval &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#flickr30k-cn--coco-cn&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;tbody&gt;&#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td rowspan=&#34;3&#34; align=&#34;left&#34;&gt;&lt;b&gt;model&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;6&#34; align=&#34;center&#34;&gt;&lt;b&gt;Flickr30K-CN&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;6&#34; align=&#34;center&#34;&gt;&lt;b&gt;COCO-CN&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;avg&lt;/b&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;image-to-text&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;text-to-image&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;image-to-text&lt;/b&gt;&lt;/td&gt; &#xA;      &lt;td colspan=&#34;3&#34; align=&#34;center&#34;&gt;&lt;b&gt;text-to-image&lt;/b&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;      &lt;td&gt;R@1&lt;/td&gt; &#xA;      &lt;td&gt;R@5&lt;/td&gt; &#xA;      &lt;td&gt;R@10&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;CN-CLIP-ViT-H&lt;/td&gt; &#xA;      &lt;td&gt;81.6&lt;/td&gt; &#xA;      &lt;td&gt;97.5&lt;/td&gt; &#xA;      &lt;td&gt;98.8&lt;/td&gt; &#xA;      &lt;td&gt;71.2&lt;/td&gt; &#xA;      &lt;td&gt;91.4&lt;/td&gt; &#xA;      &lt;td&gt;95.5&lt;/td&gt; &#xA;      &lt;td&gt;63.0&lt;/td&gt; &#xA;      &lt;td&gt;86.6&lt;/td&gt; &#xA;      &lt;td&gt;92.9&lt;/td&gt; &#xA;      &lt;td&gt;69.2&lt;/td&gt; &#xA;      &lt;td&gt;89.9&lt;/td&gt; &#xA;      &lt;td&gt;96.1&lt;/td&gt; &#xA;      &lt;td&gt;86.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;OpenCLIP-XLM-R-H&lt;/td&gt; &#xA;      &lt;td&gt;86.1&lt;/td&gt; &#xA;      &lt;td&gt;97.5&lt;/td&gt; &#xA;      &lt;td&gt;99.2&lt;/td&gt; &#xA;      &lt;td&gt;71.0&lt;/td&gt; &#xA;      &lt;td&gt;90.5&lt;/td&gt; &#xA;      &lt;td&gt;94.9&lt;/td&gt; &#xA;      &lt;td&gt;70.0&lt;/td&gt; &#xA;      &lt;td&gt;91.5&lt;/td&gt; &#xA;      &lt;td&gt;97.0&lt;/td&gt; &#xA;      &lt;td&gt;66.1&lt;/td&gt; &#xA;      &lt;td&gt;90.8&lt;/td&gt; &#xA;      &lt;td&gt;96.0&lt;/td&gt; &#xA;      &lt;td&gt;87.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td&gt;90.3&lt;/td&gt; &#xA;      &lt;td&gt;98.8&lt;/td&gt; &#xA;      &lt;td&gt;99.7&lt;/td&gt; &#xA;      &lt;td&gt;75.1&lt;/td&gt; &#xA;      &lt;td&gt;92.9&lt;/td&gt; &#xA;      &lt;td&gt;96.4&lt;/td&gt; &#xA;      &lt;td&gt;68.8&lt;/td&gt; &#xA;      &lt;td&gt;92.0&lt;/td&gt; &#xA;      &lt;td&gt;96.7&lt;/td&gt; &#xA;      &lt;td&gt;68.9&lt;/td&gt; &#xA;      &lt;td&gt;91.9&lt;/td&gt; &#xA;      &lt;td&gt;96.5&lt;/td&gt; &#xA;      &lt;td&gt;89.0&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr align=&#34;center&#34;&gt; &#xA;      &lt;td align=&#34;left&#34;&gt;InternVL-G (ours)&lt;/td&gt; &#xA;      &lt;td&gt;92.9&lt;/td&gt; &#xA;      &lt;td&gt;99.4&lt;/td&gt; &#xA;      &lt;td&gt;99.8&lt;/td&gt; &#xA;      &lt;td&gt;77.7&lt;/td&gt; &#xA;      &lt;td&gt;94.8&lt;/td&gt; &#xA;      &lt;td&gt;97.3&lt;/td&gt; &#xA;      &lt;td&gt;71.4&lt;/td&gt; &#xA;      &lt;td&gt;93.9&lt;/td&gt; &#xA;      &lt;td&gt;97.7&lt;/td&gt; &#xA;      &lt;td&gt;73.8&lt;/td&gt; &#xA;      &lt;td&gt;94.4&lt;/td&gt; &#xA;      &lt;td&gt;98.1&lt;/td&gt; &#xA;      &lt;td&gt;90.9&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt;&#xA;   &lt;/table&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Multilingual Zero-Shot Image-Text Retrieval on XTD &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/clip_benchmark#xtd&#34;&gt;[see details]&lt;/a&gt;&lt;/p&gt; &#xA;   &lt;table&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th&gt;method&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;EN&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;ES&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;FR&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;ZH&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;IT&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;KO&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;RU&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;JP&lt;/th&gt; &#xA;      &lt;th align=&#34;center&#34;&gt;average&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;AltCLIP&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;92.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.4&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;91.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;91.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;93.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;OpenCLIP-XLM-R-H&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;97.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;90.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;93.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-C (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;97.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.0&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;92.2&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;93.3&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;InternVL-G (ours)&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;98.6&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;97.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.5&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.7&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.9&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;95.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;94.8&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.1&lt;/td&gt; &#xA;      &lt;td align=&#34;center&#34;&gt;96.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Multimodal Dialogue (see &#34;Compared with SOTA VLLMs&#34;)&lt;/summary&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Quick Start with Huggingface&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;using InternViT-6B (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, CLIPImageProcessor&#xA;&#xA;model = AutoModel.from_pretrained(&#xA;    &#39;OpenGVLab/InternViT-6B-224px&#39;,&#xA;    torch_dtype=torch.bfloat16,&#xA;    low_cpu_mem_usage=True,&#xA;    trust_remote_code=True).cuda().eval()&#xA;&#xA;image = Image.open(&#39;./examples/image1.jpg&#39;).convert(&#39;RGB&#39;)&#xA;&#xA;image_processor = CLIPImageProcessor.from_pretrained(&#39;OpenGVLab/InternViT-6B-224px&#39;)&#xA;&#xA;pixel_values = image_processor(images=image, return_tensors=&#39;pt&#39;).pixel_values&#xA;pixel_values = pixel_values.to(torch.bfloat16).cuda()&#xA;&#xA;outputs = model(pixel_values)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;using InternVL-C(ontrastive) and InternVL-G(enerative) (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, CLIPImageProcessor&#xA;from transformers import AutoTokenizer&#xA;&#xA;&#xA;model = AutoModel.from_pretrained(&#xA;    &#39;OpenGVLab/InternVL-14B-224px&#39;,&#xA;    torch_dtype=torch.bfloat16,&#xA;    low_cpu_mem_usage=True,&#xA;    trust_remote_code=True).cuda().eval()&#xA;&#xA;image_processor = CLIPImageProcessor.from_pretrained(&#39;OpenGVLab/InternVL-14B-224px&#39;)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#xA;    &#39;OpenGVLab/InternVL-14B-224px&#39;, use_fast=False, add_eos_token=True)&#xA;tokenizer.pad_token_id = 0  # set pad_token_id to 0&#xA;&#xA;images = [&#xA;    Image.open(&#39;./examples/image1.jpg&#39;).convert(&#39;RGB&#39;),&#xA;    Image.open(&#39;./examples/image2.jpg&#39;).convert(&#39;RGB&#39;),&#xA;    Image.open(&#39;./examples/image3.jpg&#39;).convert(&#39;RGB&#39;)&#xA;]&#xA;prefix = &#39;summarize:&#39;&#xA;texts = [&#xA;    prefix + &#39;a photo of a red panda&#39;,  # English&#xA;    prefix + &#39;一张熊猫的照片&#39;,  # Chinese&#xA;    prefix + &#39;二匹の猫の写真&#39;  # Japanese&#xA;]&#xA;&#xA;pixel_values = image_processor(images=images, return_tensors=&#39;pt&#39;).pixel_values&#xA;pixel_values = pixel_values.to(torch.bfloat16).cuda()&#xA;input_ids = tokenizer(texts, return_tensors=&#39;pt&#39;, max_length=80,&#xA;                      truncation=True, padding=&#39;max_length&#39;).input_ids.cuda()&#xA;&#xA;# InternVL-C&#xA;logits_per_image, logits_per_text = model(&#xA;    image=pixel_values, text=input_ids, mode=&#39;InternVL-C&#39;)&#xA;probs = logits_per_image.softmax(dim=-1)&#xA;# tensor([[9.9609e-01, 5.2185e-03, 6.0070e-08],&#xA;#         [2.2949e-02, 9.7656e-01, 5.9903e-06],&#xA;#         [3.2932e-06, 7.4863e-05, 1.0000e+00]], device=&#39;cuda:0&#39;,&#xA;#        dtype=torch.bfloat16, grad_fn=&amp;lt;SoftmaxBackward0&amp;gt;)&#xA;&#xA;# InternVL-G&#xA;logits_per_image, logits_per_text = model(&#xA;    image=pixel_values, text=input_ids, mode=&#39;InternVL-G&#39;)&#xA;probs = logits_per_image.softmax(dim=-1)&#xA;# tensor([[9.9609e-01, 3.1738e-03, 3.6322e-08],&#xA;#         [8.6060e-03, 9.9219e-01, 2.8759e-06],&#xA;#         [1.7583e-06, 3.1233e-05, 1.0000e+00]], device=&#39;cuda:0&#39;,&#xA;#        dtype=torch.bfloat16, grad_fn=&amp;lt;SoftmaxBackward0&amp;gt;)&#xA;&#xA;# please set add_eos_token to False for generation&#xA;tokenizer.add_eos_token = False&#xA;image = Image.open(&#39;./examples/image1.jpg&#39;).convert(&#39;RGB&#39;)&#xA;pixel_values = image_processor(images=image, return_tensors=&#39;pt&#39;).pixel_values&#xA;pixel_values = pixel_values.to(torch.bfloat16).cuda()&#xA;&#xA;tokenized = tokenizer(&#34;English caption:&#34;, return_tensors=&#39;pt&#39;)&#xA;pred = model.generate(&#xA;    pixel_values=pixel_values,&#xA;    input_ids=tokenized.input_ids.cuda(),&#xA;    attention_mask=tokenized.attention_mask.cuda(),&#xA;    num_beams=5,&#xA;    min_new_tokens=8,&#xA;)&#xA;caption = tokenizer.decode(pred[0].cpu(), skip_special_tokens=True).strip()&#xA;# English caption: a red panda sitting on top of a wooden platform&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;using InternVL-Chat (click to expand)&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModel&#xA;import torch&#xA;import torchvision.transforms as T&#xA;from PIL import Image&#xA;&#xA;from torchvision.transforms.functional import InterpolationMode&#xA;&#xA;&#xA;IMAGENET_MEAN = (0.485, 0.456, 0.406)&#xA;IMAGENET_STD = (0.229, 0.224, 0.225)&#xA;&#xA;&#xA;def build_transform(input_size):&#xA;    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD&#xA;    transform = T.Compose([&#xA;        T.Lambda(lambda img: img.convert(&#39;RGB&#39;) if img.mode != &#39;RGB&#39; else img),&#xA;        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),&#xA;        T.ToTensor(),&#xA;        T.Normalize(mean=MEAN, std=STD)&#xA;    ])&#xA;    return transform&#xA;&#xA;&#xA;def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):&#xA;    best_ratio_diff = float(&#39;inf&#39;)&#xA;    best_ratio = (1, 1)&#xA;    area = width * height&#xA;    for ratio in target_ratios:&#xA;        target_aspect_ratio = ratio[0] / ratio[1]&#xA;        ratio_diff = abs(aspect_ratio - target_aspect_ratio)&#xA;        if ratio_diff &amp;lt; best_ratio_diff:&#xA;            best_ratio_diff = ratio_diff&#xA;            best_ratio = ratio&#xA;        elif ratio_diff == best_ratio_diff:&#xA;            if area &amp;gt; 0.5 * image_size * image_size * ratio[0] * ratio[1]:&#xA;                best_ratio = ratio&#xA;    return best_ratio&#xA;&#xA;&#xA;def dynamic_preprocess(image, min_num=1, max_num=6, image_size=448, use_thumbnail=False):&#xA;    orig_width, orig_height = image.size&#xA;    aspect_ratio = orig_width / orig_height&#xA;&#xA;    # calculate the existing image aspect ratio&#xA;    target_ratios = set(&#xA;        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if&#xA;        i * j &amp;lt;= max_num and i * j &amp;gt;= min_num)&#xA;    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])&#xA;&#xA;    # find the closest aspect ratio to the target&#xA;    target_aspect_ratio = find_closest_aspect_ratio(&#xA;        aspect_ratio, target_ratios, orig_width, orig_height, image_size)&#xA;&#xA;    # calculate the target width and height&#xA;    target_width = image_size * target_aspect_ratio[0]&#xA;    target_height = image_size * target_aspect_ratio[1]&#xA;    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]&#xA;&#xA;    # resize the image&#xA;    resized_img = image.resize((target_width, target_height))&#xA;    processed_images = []&#xA;    for i in range(blocks):&#xA;        box = (&#xA;            (i % (target_width // image_size)) * image_size,&#xA;            (i // (target_width // image_size)) * image_size,&#xA;            ((i % (target_width // image_size)) + 1) * image_size,&#xA;            ((i // (target_width // image_size)) + 1) * image_size&#xA;        )&#xA;        # split the image&#xA;        split_img = resized_img.crop(box)&#xA;        processed_images.append(split_img)&#xA;    assert len(processed_images) == blocks&#xA;    if use_thumbnail and len(processed_images) != 1:&#xA;        thumbnail_img = image.resize((image_size, image_size))&#xA;        processed_images.append(thumbnail_img)&#xA;    return processed_images&#xA;&#xA;&#xA;def load_image(image_file, input_size=448, max_num=6):&#xA;    image = Image.open(image_file).convert(&#39;RGB&#39;)&#xA;    transform = build_transform(input_size=input_size)&#xA;    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)&#xA;    pixel_values = [transform(image) for image in images]&#xA;    pixel_values = torch.stack(pixel_values)&#xA;    return pixel_values&#xA;&#xA;&#xA;path = &#34;OpenGVLab/InternVL-Chat-V1-5&#34;&#xA;# If you have an 80G A100 GPU, you can put the entire model on a single GPU.&#xA;model = AutoModel.from_pretrained(&#xA;    path,&#xA;    torch_dtype=torch.bfloat16,&#xA;    low_cpu_mem_usage=True,&#xA;    trust_remote_code=True).eval().cuda()&#xA;# Otherwise, you need to set device_map=&#39;auto&#39; to use multiple GPUs for inference.&#xA;# model = AutoModel.from_pretrained(&#xA;#     path,&#xA;#     torch_dtype=torch.bfloat16,&#xA;#     low_cpu_mem_usage=True,&#xA;#     trust_remote_code=True,&#xA;#     device_map=&#39;auto&#39;).eval()&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)&#xA;# set the max number of tiles in `max_num`&#xA;pixel_values = load_image(&#39;./examples/image1.jpg&#39;, max_num=6).to(torch.bfloat16).cuda()&#xA;&#xA;generation_config = dict(&#xA;    num_beams=1,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;)&#xA;&#xA;# single-round single-image conversation&#xA;question = &#34;请详细描述图片&#34; # Please describe the picture in detail&#xA;response = model.chat(tokenizer, pixel_values, question, generation_config)&#xA;print(question, response)&#xA;&#xA;# multi-round single-image conversation&#xA;question = &#34;请详细描述图片&#34; # Please describe the picture in detail&#xA;response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)&#xA;print(question, response)&#xA;&#xA;question = &#34;请根据图片写一首诗&#34; # Please write a poem according to the picture&#xA;response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)&#xA;print(question, response)&#xA;&#xA;# multi-round multi-image conversation&#xA;pixel_values1 = load_image(&#39;./examples/image1.jpg&#39;, max_num=6).to(torch.bfloat16).cuda()&#xA;pixel_values2 = load_image(&#39;./examples/image2.jpg&#39;, max_num=6).to(torch.bfloat16).cuda()&#xA;pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)&#xA;&#xA;question = &#34;详细描述这两张图片&#34; # Describe the two pictures in detail&#xA;response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)&#xA;print(question, response)&#xA;&#xA;question = &#34;这两张图片的相同点和区别分别是什么&#34; # What are the similarities and differences between these two pictures&#xA;response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)&#xA;print(question, response)&#xA;&#xA;# batch inference (single image per sample)&#xA;pixel_values1 = load_image(&#39;./examples/image1.jpg&#39;, max_num=6).to(torch.bfloat16).cuda()&#xA;pixel_values2 = load_image(&#39;./examples/image2.jpg&#39;, max_num=6).to(torch.bfloat16).cuda()&#xA;image_counts = [pixel_values1.size(0), pixel_values2.size(0)]&#xA;pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)&#xA;&#xA;questions = [&#34;Describe the image in detail.&#34;] * len(image_counts)&#xA;responses = model.batch_chat(tokenizer, pixel_values,&#xA;                             image_counts=image_counts,&#xA;                             questions=questions,&#xA;                             generation_config=generation_config)&#xA;for question, response in zip(questions, responses):&#xA;    print(question)&#xA;    print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Inference Acceleration by LMDeploy&lt;/h2&gt; &#xA;&lt;p&gt;We recommend using &lt;a href=&#34;https://github.com/InternLM/lmdeploy&#34;&gt;LMDeploy&lt;/a&gt;, if InternVL-Chat model inference optimization is required.&lt;/p&gt; &#xA;&lt;p&gt;In the following subsections, we will introduce the usage of LMDeploy with the &lt;a href=&#34;https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5&#34;&gt;InternVL-Chat-V1-5&lt;/a&gt; model as an example.&lt;/p&gt; &#xA;&lt;p&gt;First of all, please setup the inference environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n internvl python=3.10 -y&#xA;conda activate internvl&#xA;&#xA;pip install timm torchvision==0.17.2&#xA;pip install lmdeploy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;LMDeploy pypi package depends on CUDA 12.x by default. For a CUDA 11.x environment, please refer to the &lt;a href=&#34;https://lmdeploy.readthedocs.io/en/latest/get_started.html#installation&#34;&gt;installation guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Offline Inference Pipeline&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lmdeploy import pipeline&#xA;from lmdeploy.vl import load_image&#xA;pipe = pipeline(&#39;OpenGVLab/InternVL-Chat-V1-5&#39;)&#xA;image = load_image(&#39;examples/image2.jpg&#39;)&#xA;response = pipe((&#39;describe this image&#39;, image))&#xA;print(response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more on using the VLM pipeline, including multi-image inference or multi-turn chat, please overview &lt;a href=&#34;https://lmdeploy.readthedocs.io/en/latest/inference/vl_pipeline.html&#34;&gt;this&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h3&gt;Online Inference Service&lt;/h3&gt; &#xA;&lt;p&gt;LMDeploy supports one-click packaging of the VLM model into an OpenAI service, providing seamless integration with the OpenAI API.&lt;/p&gt; &#xA;&lt;p&gt;The service can be launched by one command as below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;lmdeploy serve api_server OpenGVLab/InternVL-Chat-V1-5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The arguments of &lt;code&gt;api_server&lt;/code&gt; can be viewed through the command &lt;code&gt;lmdeploy serve api_server -h&lt;/code&gt;, for instance, &lt;code&gt;--tp&lt;/code&gt; to set tensor parallelism, &lt;code&gt;--session-len&lt;/code&gt; to specify the max length of the context window, &lt;code&gt;--cache-max-entry-count&lt;/code&gt; to adjust the GPU mem ratio for k/v cache etc.&lt;/p&gt; &#xA;&lt;p&gt;For more details, including service startup with docker, RESTful API information, and openai integration methods, please refer to &lt;a href=&#34;https://lmdeploy.readthedocs.io/en/latest/serving/api_server_vl.html&#34;&gt;this&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/OpenGVLab/InternVL/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;. Parts of this project contain code and models from other sources, which are subject to their respective licenses.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful in your research, please consider cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{chen2023internvl,&#xA;  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},&#xA;  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},&#xA;  journal={arXiv preprint arXiv:2312.14238},&#xA;  year={2023}&#xA;}&#xA;&#xA;@article{chen2024far,&#xA;  title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},&#xA;  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},&#xA;  journal={arXiv preprint arXiv:2404.16821},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;InternVL is built with reference to the code of the following projects: &lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;OpenAI CLIP&lt;/a&gt;, &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;Open CLIP&lt;/a&gt;, &lt;a href=&#34;https://github.com/LAION-AI/CLIP_benchmark&#34;&gt;CLIP Benchmark&lt;/a&gt;, &lt;a href=&#34;https://github.com/baaivision/EVA/tree/master&#34;&gt;EVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/OpenGVLab/InternImage&#34;&gt;InternImage&lt;/a&gt;, &lt;a href=&#34;https://github.com/czczup/ViT-Adapter&#34;&gt;ViT-Adapter&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmsegmentation&#34;&gt;MMSegmentation&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;, &lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34;&gt;DINOv2&lt;/a&gt;, &lt;a href=&#34;https://github.com/salesforce/LAVIS/tree/main/projects/blip2&#34;&gt;BLIP-2&lt;/a&gt;, &lt;a href=&#34;https://github.com/QwenLM/Qwen-VL/tree/master/eval_mm&#34;&gt;Qwen-VL&lt;/a&gt;, and &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA-1.5&lt;/a&gt;. Thanks for their awesome work!&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If you want to join our WeChat group, please scan the following QR Code to add our assistant as a Wechat friend:&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img width=&#34;300&#34; alt=&#34;image&#34; src=&#34;https://github.com/OpenGVLab/DragGAN/assets/26198430/e3f0807f-956a-474e-8fd2-1f7c22d73997&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Tencent/HunyuanDiT</title>
    <updated>2024-05-18T01:26:35Z</updated>
    <id>tag:github.com,2024-05-18:/Tencent/HunyuanDiT</id>
    <link href="https://github.com/Tencent/HunyuanDiT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/asset/logo.png&#34; height=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/Tencent/HunyuanDiT&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Hunyuan-DiT Code&amp;amp;message=Github&amp;amp;color=blue&amp;amp;logo=github-pages&#34;&gt;&lt;/a&gt;   &#xA; &lt;a href=&#34;https://dit.hunyuan.tencent.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Project%20Page&amp;amp;message=Github&amp;amp;color=blue&amp;amp;logo=github-pages&#34;&gt;&lt;/a&gt;   &#xA; &lt;a href=&#34;https://arxiv.org/abs/2405.08748&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Tech Report&amp;amp;message=Arxiv:HunYuan-DiT&amp;amp;color=red&amp;amp;logo=arxiv&#34;&gt;&lt;/a&gt;   &#xA; &lt;a href=&#34;https://arxiv.org/abs/2403.08857&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Paper&amp;amp;message=Arxiv:DialogGen&amp;amp;color=red&amp;amp;logo=arxiv&#34;&gt;&lt;/a&gt;   &#xA; &lt;a href=&#34;https://huggingface.co/Tencent-Hunyuan/HunyuanDiT&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Hunyuan-DiT&amp;amp;message=HuggingFace&amp;amp;color=yellow&#34;&gt;&lt;/a&gt;   &#xA; &lt;a href=&#34;https://hunyuan.tencent.com/bot/chat&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Hunyuan Bot&amp;amp;message=Web&amp;amp;color=green&#34;&gt;&lt;/a&gt;   &#xA; &lt;a href=&#34;https://huggingface.co/spaces/Tencent-Hunyuan/HunyuanDiT&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Hunyuan-DiT Demo&amp;amp;message=HuggingFace&amp;amp;color=yellow&#34;&gt;&lt;/a&gt;   &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This repo contains PyTorch model definitions, pre-trained weights and inference/sampling code for our paper exploring Hunyuan-DiT. You can find more visualizations on our &lt;a href=&#34;https://dit.hunyuan.tencent.com/&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2405.08748&#34;&gt;&lt;strong&gt;Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.08857&#34;&gt;&lt;strong&gt;DialogGen:Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation&lt;/strong&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;🔥🔥🔥 Tencent Hunyuan Bot&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to &lt;a href=&#34;https://hunyuan.tencent.com/bot/chat&#34;&gt;Tencent Hunyuan Bot&lt;/a&gt;, where you can explore our innovative products! Just input the suggested prompts below or any other &lt;strong&gt;imaginative prompts containing drawing-related keywords&lt;/strong&gt; to activate the Hunyuan text-to-image generation feature. Unleash your creativity and create any picture you desire, &lt;strong&gt;all for free!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use simple prompts similar to natural language text&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;画一只穿着西装的猪&lt;/p&gt; &#xA; &lt;p&gt;draw a pig in a suit&lt;/p&gt; &#xA; &lt;p&gt;生成一幅画，赛博朋克风，跑车&lt;/p&gt; &#xA; &lt;p&gt;generate a painting, cyberpunk style, sports car&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;or multi-turn language interactions to create the picture.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;画一个木制的鸟&lt;/p&gt; &#xA; &lt;p&gt;draw a wooden bird&lt;/p&gt; &#xA; &lt;p&gt;变成玻璃的&lt;/p&gt; &#xA; &lt;p&gt;turn into glass&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;📑 Open-source Plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hunyuan-DiT (Text-to-Image Model) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Checkpoints&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Distillation Version (Coming soon ⏩️)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; TensorRT Version (Coming soon ⏩️)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training (Coming later ⏩️)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Centaurusalpha/DialogGen&#34;&gt;DialogGen&lt;/a&gt; (Prompt Enhancement Model) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Web Demo (Gradio)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Cli Demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#hunyuan-dit--a-powerful-multi-resolution-diffusion-transformer-with-fine-grained-chinese-understanding&#34;&gt;Hunyuan-DiT&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#-hunyuan-dit-key-features&#34;&gt;🎉 Hunyuan-DiT Key Features&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#chinese-english-bilingual-dit-architecture&#34;&gt;Chinese-English Bilingual DiT Architecture&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#multi-turn-text2image-generation&#34;&gt;Multi-turn Text2Image Generation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#-comparisons&#34;&gt;📈 Comparisons&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#-visualization&#34;&gt;🎥 Visualization&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#-requirements&#34;&gt;📜 Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#%EF%B8%8F-dependencies-and-installation&#34;&gt;🛠 Dependencies and Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#-download-pretrained-models&#34;&gt;🧱 Download Pretrained Models&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#-inference&#34;&gt;🔑 Inference&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#using-gradio&#34;&gt;Using Gradio&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#using-command-line&#34;&gt;Using Command Line&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#more-configurations&#34;&gt;More Configurations&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/#-bibtex&#34;&gt;🔗 BibTeX&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Abstract&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully designed the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-round multi-modal dialogue with users, generating and refining images according to the context. Through our carefully designed holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.&lt;/p&gt; &#xA;&lt;h2&gt;🎉 &lt;strong&gt;Hunyuan-DiT Key Features&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Chinese-English Bilingual DiT Architecture&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Hunyuan-DiT is a diffusion model in the latent space, as depicted in figure below. Following the Latent Diffusion Model, we use a pre-trained Variational Autoencoder (VAE) to compress the images into low-dimensional latent spaces and train a diffusion model to learn the data distribution with diffusion models. Our diffusion model is parameterized with a transformer. To encode the text prompts, we leverage a combination of pre-trained bilingual (English and Chinese) CLIP and multilingual T5 encoder.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/asset/framework.png&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Multi-turn Text2Image Generation&lt;/h3&gt; &#xA;&lt;p&gt;Understanding natural language instructions and performing multi-turn interaction with users are important for a text-to-image system. It can help build a dynamic and iterative creation process that bring the user’s idea into reality step by step. In this section, we will detail how we empower Hunyuan-DiT with the ability to perform multi-round conversations and image generation. We train MLLM to understand the multi-round user dialogue and output the new text prompt for image generation.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/asset/mllm.png&#34; height=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;📈 Comparisons&lt;/h2&gt; &#xA;&lt;p&gt;In order to comprehensively compare the generation capabilities of HunyuanDiT and other models, we constructed a 4-dimensional test set, including Text-Image Consistency, Excluding AI Artifacts, Subject Clarity, Aesthetic. More than 50 professional evaluators performs the evaluation.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Open Source&lt;/th&gt; &#xA;   &lt;th&gt;Text-Image Consistency (%)&lt;/th&gt; &#xA;   &lt;th&gt;Excluding AI Artifacts (%)&lt;/th&gt; &#xA;   &lt;th&gt;Subject Clarity (%)&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Aesthetics (%)&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Overall (%)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SDXL&lt;/td&gt; &#xA;   &lt;td&gt; ✔ &lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;60.6&lt;/td&gt; &#xA;   &lt;td&gt;91.1&lt;/td&gt; &#xA;   &lt;td&gt;76.3&lt;/td&gt; &#xA;   &lt;td&gt;42.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PixArt-α&lt;/td&gt; &#xA;   &lt;td&gt; ✔&lt;/td&gt; &#xA;   &lt;td&gt;68.3&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;   &lt;td&gt;93.2&lt;/td&gt; &#xA;   &lt;td&gt;77.5&lt;/td&gt; &#xA;   &lt;td&gt;45.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Playground 2.5&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;71.9&lt;/td&gt; &#xA;   &lt;td&gt;70.8&lt;/td&gt; &#xA;   &lt;td&gt;94.9&lt;/td&gt; &#xA;   &lt;td&gt;83.3&lt;/td&gt; &#xA;   &lt;td&gt;54.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SD 3&lt;/td&gt; &#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;69.3&lt;/td&gt; &#xA;   &lt;td&gt;94.6&lt;/td&gt; &#xA;   &lt;td&gt;82.5&lt;/td&gt; &#xA;   &lt;td&gt;56.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MidJourney v6&lt;/td&gt;&#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;73.5&lt;/td&gt; &#xA;   &lt;td&gt;80.2&lt;/td&gt; &#xA;   &lt;td&gt;93.5&lt;/td&gt; &#xA;   &lt;td&gt;87.2&lt;/td&gt; &#xA;   &lt;td&gt;63.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DALL-E 3&lt;/td&gt;&#xA;   &lt;td&gt;✘&lt;/td&gt; &#xA;   &lt;td&gt;83.9&lt;/td&gt; &#xA;   &lt;td&gt;80.3&lt;/td&gt; &#xA;   &lt;td&gt;96.5&lt;/td&gt; &#xA;   &lt;td&gt;89.4&lt;/td&gt; &#xA;   &lt;td&gt;71.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr style=&#34;font-weight: bold; background-color: #f2f2f2;&#34;&gt; &#xA;   &lt;td&gt;Hunyuan-DiT&lt;/td&gt;&#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;74.2&lt;/td&gt; &#xA;   &lt;td&gt;74.3&lt;/td&gt; &#xA;   &lt;td&gt;95.4&lt;/td&gt; &#xA;   &lt;td&gt;86.6&lt;/td&gt; &#xA;   &lt;td&gt;59.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🎥 Visualization&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chinese Elements&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/asset/chinese elements understanding.png&#34; height=&#34;220&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Long Text Input&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/asset/long text understanding.png&#34; height=&#34;310&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-turn Text2Image Generation&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Tencent/tencent.github.io/assets/27557933/94b4dcc3-104d-44e1-8bb2-dc55108763d1&#34;&gt;https://github.com/Tencent/tencent.github.io/assets/27557933/94b4dcc3-104d-44e1-8bb2-dc55108763d1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;📜 Requirements&lt;/h2&gt; &#xA;&lt;p&gt;This repo consists of DialogGen (a prompt enhancement model) and Hunyuan-DiT (a text-to-image model).&lt;/p&gt; &#xA;&lt;p&gt;The following table shows the requirements for running the models (The TensorRT version will be updated soon):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;TensorRT&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPU Memory&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DialogGen + Hunyuan-DiT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✘&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V100/A100&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hunyuan-DiT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;✘&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;V100/A100&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- | DialogGen + Hunyuan-DiT  |    ✔     |     1      |     ?      |   A100    |&#xA;|       Hunyuan-DiT        |    ✔     |     1      |     ?      |   A100    | --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;An NVIDIA GPU with CUDA support is required. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;We have tested V100 and A100 GPUs.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Minimum&lt;/strong&gt;: The minimum GPU memory required is 11GB.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Recommended&lt;/strong&gt;: We recommend using a GPU with 32GB of memory for better generation quality.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Tested operating system: Linux&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛠️ Dependencies and Installation&lt;/h2&gt; &#xA;&lt;p&gt;Begin by cloning the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/tencent/HunyuanDiT&#xA;cd HunyuanDiT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide an &lt;code&gt;environment.yml&lt;/code&gt; file for setting up a Conda environment. Conda&#39;s installation instructions are available &lt;a href=&#34;https://docs.anaconda.com/free/miniconda/index.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1. Prepare conda environment&#xA;conda env create -f environment.yml&#xA;&#xA;# 2. Activate the environment&#xA;conda activate HunyuanDiT&#xA;&#xA;# 3. Install pip dependencies&#xA;python -m pip install -r requirements.txt&#xA;&#xA;# 4. (Optional) Install flash attention v2 for acceleration (requires CUDA 11.6 or above)&#xA;python -m pip install git+https://github.com/Dao-AILab/flash-attention.git@v2.1.2.post3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🧱 Download Pretrained Models&lt;/h2&gt; &#xA;&lt;p&gt;To download the model, first install the huggingface-cli. (Detailed instructions are available &lt;a href=&#34;https://huggingface.co/docs/huggingface_hub/guides/cli&#34;&gt;here&lt;/a&gt;.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m pip install &#34;huggingface_hub[cli]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then download the model using the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create a directory named &#39;ckpts&#39; where the model will be saved, fulfilling the prerequisites for running the demo.&#xA;mkdir ckpts&#xA;# Use the huggingface-cli tool to download the model.&#xA;# The download time may vary from 10 minutes to 1 hour depending on network conditions.&#xA;huggingface-cli download Tencent-Hunyuan/HunyuanDiT --local-dir ./ckpts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note：If an &lt;code&gt;No such file or directory: &#39;ckpts/.huggingface/.gitignore.lock&#39;&lt;/code&gt; like error occurs during the download process, you can ignore the error and retry the command by executing &lt;code&gt;huggingface-cli download Tencent-Hunyuan/HunyuanDiT --local-dir ./ckpts&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;All models will be automatically downloaded. For more information about the model, visit the Hugging Face repository &lt;a href=&#34;https://huggingface.co/Tencent-Hunyuan/HunyuanDiT&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#Params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download URL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;mT5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.6B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/tree/main/t2i/mt5&#34;&gt;mT5&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;350M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/tree/main/t2i/clip_text_encoder&#34;&gt;CLIP&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DialogGen&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/tree/main/dialoggen&#34;&gt;DialogGen&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;sdxl-vae-fp16-fix&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/tree/main/t2i/sdxl-vae-fp16-fix&#34;&gt;sdxl-vae-fp16-fix&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Hunyuan-DiT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.5B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/Tencent-Hunyuan/HunyuanDiT/tree/main/t2i/model&#34;&gt;Hunyuan-DiT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🔑 Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Using Gradio&lt;/h3&gt; &#xA;&lt;p&gt;Make sure you have activated the conda environment before running the following command.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# By default, we start a Chinese UI.&#xA;python app/hydit_app.py&#xA;&#xA;# Using Flash Attention for acceleration.&#xA;python app/hydit_app.py --infer-mode fa&#xA;&#xA;# You can disable the enhancement model if the GPU memory is insufficient.&#xA;# The enhancement will be unavailable until you restart the app without the `--no-enhance` flag. &#xA;python app/hydit_app.py --no-enhance&#xA;&#xA;# Start with English UI&#xA;python app/hydit_app.py --lang en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Using Command Line&lt;/h3&gt; &#xA;&lt;p&gt;We provide 3 modes to quick start:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Prompt Enhancement + Text-to-Image. Torch mode&#xA;python sample_t2i.py --prompt &#34;渔舟唱晚&#34;&#xA;&#xA;# Only Text-to-Image. Torch mode&#xA;python sample_t2i.py --prompt &#34;渔舟唱晚&#34; --no-enhance&#xA;&#xA;# Only Text-to-Image. Flash Attention mode&#xA;python sample_t2i.py --infer-mode fa --prompt &#34;渔舟唱晚&#34;&#xA;&#xA;# Generate an image with other image sizes.&#xA;python sample_t2i.py --prompt &#34;渔舟唱晚&#34; --image-size 1280 768&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More example prompts can be found in &lt;a href=&#34;https://raw.githubusercontent.com/Tencent/HunyuanDiT/main/example_prompts.txt&#34;&gt;example_prompts.txt&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;More Configurations&lt;/h3&gt; &#xA;&lt;p&gt;We list some more useful configurations for easy usage:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Argument&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Default&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--prompt&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;None&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The text prompt for image generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--image-size&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1024 1024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The size of the generated image&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--seed&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The random seed for generating images&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--infer-steps&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The number of steps for sampling&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--negative&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The negative prompt for image generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--infer-mode&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;torch&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The inference mode (torch or fa)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--sampler&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ddpm&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The diffusion sampler (ddpm, ddim, or dpmms)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--no-enhance&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Disable the prompt enhancement model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--model-root&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ckpts&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;The root directory of the model checkpoints&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;--load-key&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ema&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Load the student model or EMA model (ema or module)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;🔗 BibTeX&lt;/h2&gt; &#xA;&lt;p&gt;If you find &lt;a href=&#34;https://arxiv.org/abs/2405.08748&#34;&gt;Hunyuan-DiT&lt;/a&gt; or &lt;a href=&#34;https://arxiv.org/abs/2403.08857&#34;&gt;DialogGen&lt;/a&gt; useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@misc{li2024hunyuandit,&#xA;      title={Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding}, &#xA;      author={Zhimin Li and Jianwei Zhang and Qin Lin and Jiangfeng Xiong and Yanxin Long and Xinchi Deng and Yingfang Zhang and Xingchao Liu and Minbin Huang and Zedong Xiao and Dayou Chen and Jiajun He and Jiahao Li and Wenyue Li and Chen Zhang and Rongwei Quan and Jianxiang Lu and Jiabin Huang and Xiaoyan Yuan and Xiaoxiao Zheng and Yixuan Li and Jihong Zhang and Chao Zhang and Meng Chen and Jie Liu and Zheng Fang and Weiyan Wang and Jinbao Xue and Yangyu Tao and Jianchen Zhu and Kai Liu and Sihuan Lin and Yifu Sun and Yun Li and Dongdong Wang and Mingtao Chen and Zhichao Hu and Xiao Xiao and Yan Chen and Yuhong Liu and Wei Liu and Di Wang and Yong Yang and Jie Jiang and Qinglin Lu},&#xA;      year={2024},&#xA;      eprint={2405.08748},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&#xA;@article{huang2024dialoggen,&#xA;  title={DialogGen: Multi-modal Interactive Dialogue System for Multi-turn Text-to-Image Generation},&#xA;  author={Huang, Minbin and Long, Yanxin and Deng, Xinchi and Chu, Ruihang and Xiong, Jiangfeng and Liang, Xiaodan and Cheng, Hong and Lu, Qinglin and Liu, Wei},&#xA;  journal={arXiv preprint arXiv:2403.08857},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Start History&lt;/h2&gt; &#xA;&lt;a href=&#34;https://star-history.com/#Tencent/HunyuanDiT&amp;amp;Date&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=Tencent/HunyuanDiT&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=Tencent/HunyuanDiT&amp;amp;type=Date&#34;&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=Tencent/HunyuanDiT&amp;amp;type=Date&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt;</summary>
  </entry>
</feed>