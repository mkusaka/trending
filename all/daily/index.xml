<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-22T01:30:15Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>techleadhd/chatgpt-retrieval</title>
    <updated>2023-06-22T01:30:15Z</updated>
    <id>tag:github.com,2023-06-22:/techleadhd/chatgpt-retrieval</id>
    <link href="https://github.com/techleadhd/chatgpt-retrieval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;chatgpt-retrieval&lt;/h1&gt; &#xA;&lt;p&gt;Simple script to use ChatGPT on your own files.&lt;/p&gt; &#xA;&lt;p&gt;Here&#39;s the &lt;a href=&#34;https://youtu.be/9AXP7tCI9PI&#34;&gt;YouTube Video&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;Langchain&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install langchain&#xA;pip install openai&#xA;pip install chromadb&#xA;pip install tiktoken&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Modify &lt;code&gt;constants.py&lt;/code&gt; to use your own &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI API key&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Place your own data into &lt;code&gt;data.txt&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Example usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; python chatgpt.py &#34;what is my dog&#39;s name&#34;&#xA;Your dog&#39;s name is Sunny.&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>jackfrued/Python-Core-50-Courses</title>
    <updated>2023-06-22T01:30:15Z</updated>
    <id>tag:github.com,2023-06-22:/jackfrued/Python-Core-50-Courses</id>
    <link href="https://github.com/jackfrued/Python-Core-50-Courses" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Python语言基础50课&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Python语言基础50课&lt;/h2&gt; &#xA;&lt;p&gt;由于之前发布的 Python 学习项目 &lt;a href=&#34;https://github.com/jackfrued/Python-100-Days&#34;&gt;Python-100-Days&lt;/a&gt; 对初学者来说上手还是有一定难度，所以花了点之间把原来项目中 Python 语言基础部分单独剥离出来，做成了现在这个名为“Python语言基础50课”的项目。现在这个项目用更为简单通俗的方式重写了原来“Python100天”项目中第1天到第15天的部分，&lt;strong&gt;有删减也有补充&lt;/strong&gt;，力求&lt;strong&gt;对初学者更加友好&lt;/strong&gt;，也欢迎大家关注这个持续更新中的项目。国内用户如果访问 GitHub 比较慢的话，也可以关注我的知乎号 &lt;a href=&#34;https://www.zhihu.com/people/jackfrued&#34;&gt;Python-Jack&lt;/a&gt; 上的&lt;a href=&#34;https://zhuanlan.zhihu.com/c_1216656665569013760&#34;&gt;“从零开始学Python”&lt;/a&gt;专栏，两边同步更新。有需要的小伙伴可以关注我在知乎的专栏、文章和回答，当然，也欢迎大家评论、收藏和点赞。如果需要&lt;strong&gt;视频教程&lt;/strong&gt;，可以到“B站”上搜索&lt;a href=&#34;https://www.bilibili.com/video/BV1FT4y1R7sz&#34;&gt;《Python零基础快速上手》&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;最近，国内访问 GitHub 会因为 DNS（域名解析服务）的问题出现&lt;strong&gt;图片无法显示&lt;/strong&gt;的情况，如果你也遇到了这样的问题，可以通过&lt;strong&gt;修改本机的 hosts 文件&lt;/strong&gt;直接对 GitHub 的资源链接进行域名解析来加以解决。使用 macOS 系统的读者可以参考&lt;a href=&#34;https://www.jianshu.com/p/752211238c1b&#34;&gt;《macOS 下三种修改 hosts 文件的方法》&lt;/a&gt;一文来修改 hosts 文件；使用 Windows 系统的读者可以参考&lt;a href=&#34;https://sspai.com/post/43248&#34;&gt;《在 Windows 上如何管理 hosts 文件》&lt;/a&gt;一文来进行操作。我们可以把下面的内容添加到 hosts 文件的末尾，这样就可以解决 GitHub 上图片无法显示的问题。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-INI&#34;&gt;151.101.184.133    assets-cdn.github.com&#xA;151.101.184.133    raw.githubusercontent.com&#xA;151.101.184.133    gist.githubusercontent.com&#xA;151.101.184.133    cloud.githubusercontent.com&#xA;151.101.184.133    camo.githubusercontent.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;视频资源&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1FT4y1R7sz&#34;&gt;《Python零基础教程快速上手》&lt;/a&gt; - Python基础部分的视频，因为随堂录制，有的时候声音会有点小，点赞过3000就为大家重新录制一套声音和画面都更好的精讲版视频。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1dA411w7tu&#34;&gt;《Python零基础数据库可视化教程》&lt;/a&gt;- 数据库部分的视频，随堂录制，数据库部分讲得比较简单，后面还讲了一些做数据可视化系统的内容，我自己对这套视频不是很满意，点赞过2000之后就重新做一套面向数据分析师精讲数据库的视频，重点放在SQL和业务查询知识上，解决数据分析师日常提数问题。&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1QY411F7Vt&#34;&gt;《Scrapy爬虫框架教学》&lt;/a&gt;- 爬虫框架Scrapy教学视频，随堂录制，讲解了一个爬取淘宝商品信息的项目，有一点实用价值，爬虫本身并不是我感兴趣的内容，就将就看看吧。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;文件资源&lt;/h3&gt; &#xA;&lt;p&gt;教程和视频中用到的文件、代码等内容，请统一访问百度网盘获取。&lt;/p&gt; &#xA;&lt;p&gt;链接：&lt;a href=&#34;https://pan.baidu.com/s/1NhWtYcpFzF72cxcsoDoXjQ?pwd=swg1&#34;&gt;https://pan.baidu.com/s/1NhWtYcpFzF72cxcsoDoXjQ?pwd=swg1&lt;/a&gt;，提取码：swg1。&lt;/p&gt; &#xA;&lt;h3&gt;交流大群&lt;/h3&gt; &#xA;&lt;p&gt;下面是我创建的学习交流群，欢迎加入一起学习共同进步。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;骆昊的Python学习群-1，群号：789050736，2000人大群。&lt;/li&gt; &#xA; &lt;li&gt;骆昊的Python学习群-2，群号：837590310，2000人大群。&lt;/li&gt; &#xA; &lt;li&gt;骆昊的Python学习群-3，群号：784430256，1000人大群。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://github.com/jackfrued/mypic/raw/master/20220616120218.JPG&#34; style=&#34;zoom: 75%;&#34;&gt;</summary>
  </entry>
  <entry>
    <title>OpenLMLab/LOMO</title>
    <updated>2023-06-22T01:30:15Z</updated>
    <id>tag:github.com,2023-06-22:/OpenLMLab/LOMO</id>
    <link href="https://github.com/OpenLMLab/LOMO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LOMO: LOw-Memory Optimization&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenLMLab/LOMO/main/README.md&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/OpenLMLab/LOMO/main/README_ZH.md&#34;&gt;&lt;strong&gt;中文&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;LOMO: LOw-Memory Optimization&lt;/h1&gt; &#xA;&lt;p&gt;This is the implementation for &lt;a href=&#34;https://arxiv.org/pdf/2306.09782.pdf&#34;&gt;Full Parameter Fine-Tuning for Large Language Models with Limited Resources&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In this work, we propose a new optimizer, &lt;strong&gt;LO&lt;/strong&gt;w-Memory &lt;strong&gt;O&lt;/strong&gt;ptimization (&lt;strong&gt;LOMO&lt;/strong&gt;), which fuses the gradient computation and the parameter update in one step to reduce memory usage. Our approach enables the full parameter fine-tuning of a 7B model on a single RTX 3090, or a 65B model on a single machine with 8×RTX 3090, each with 24GB memory.&lt;/p&gt; &#xA;&lt;p&gt;LOMO is integrated with &lt;a href=&#34;https://github.com/OpenLMLab/collie&#34;&gt;CoLLiE&lt;/a&gt; library, which supports Collaborative Tuning of Large Language Models in an Efficient Way.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenLMLab/LOMO/main/assets/LOMO.png&#34; alt=&#34;LOMO&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;torch&#xA;deepspeed&#xA;transformers&#xA;peft&#xA;wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The minimum dependency is PyTorch, and others are used to reproduce our paper results.&lt;/p&gt; &#xA;&lt;h2&gt;Run the code&lt;/h2&gt; &#xA;&lt;p&gt;We provide code for fine-tuning Large Language Models (LLMs) using three different approaches: &lt;strong&gt;LOMO&lt;/strong&gt;, &lt;strong&gt;LoRA&lt;/strong&gt;, and &lt;strong&gt;LoRA + LOMO&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;For full parameter fine-tuning using LOMO, the implementation is in &lt;code&gt;src/lomo_trainer.py&lt;/code&gt;, and you can run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;deepspeed --master_port &#34;$port&#34; --include localhost:&#34;$CUDA_VISIBLE_DEVICES&#34; src/train_lomo.py config/args_lomo.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;For LoRA and LoRA + LOMO, the implementation is in &lt;code&gt;src/lomo_lora_trainer.py&lt;/code&gt;, and you can run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;deepspeed --master_port &#34;$port&#34; --include localhost:&#34;$CUDA_VISIBLE_DEVICES&#34; src/train_lomo_lora.py config/args_lomo_lora.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the code, we have included the &lt;code&gt;lora_only&lt;/code&gt; argument in &lt;code&gt;src/arguments.py&lt;/code&gt;, which controls whether to use LoRA only or LoRA + LOMO. Please note that when &lt;code&gt;lora_only&lt;/code&gt; is set to &lt;code&gt;True&lt;/code&gt;, the arguments related to LOMO will not work.&lt;/p&gt; &#xA;&lt;p&gt;Besides, we provide a simple &lt;code&gt;run.sh&lt;/code&gt; script for convenience. You can execute the code using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash run.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For data processing, we currently only provide the six datasets of SuperGLUE mentioned in the paper. If you wish to use new datasets, please modify the &lt;code&gt;Dataset&lt;/code&gt; and &lt;code&gt;DataCollator&lt;/code&gt; accordingly.&lt;/p&gt; &#xA;&lt;p&gt;For evaluation, we currently only provide the &lt;code&gt;eval_step&lt;/code&gt; codes for &lt;a href=&#34;https://github.com/OpenLMLab/LOMO/raw/91cc71387d0a576c000a7dc568543c4ef22401db/src/lomo_trainer.py#L259-L276&#34;&gt;multiple-choice QA&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenLMLab/LOMO/raw/91cc71387d0a576c000a7dc568543c4ef22401db/src/lomo_trainer.py#L278-L297&#34;&gt;generation&lt;/a&gt; tasks. If you have other requirements, please modify the &lt;code&gt;eval_step&lt;/code&gt; code in the &lt;code&gt;LOMOTrainer&lt;/code&gt; or &lt;code&gt;LOMOLoRATrainer&lt;/code&gt; accordingly and provide the necessary &lt;code&gt;compute_metrics&lt;/code&gt; to the trainer.&lt;/p&gt; &#xA;&lt;h2&gt;Reproduce our results&lt;/h2&gt; &#xA;&lt;p&gt;We provide the sampled datasets used in our experiments &lt;a href=&#34;https://drive.google.com/drive/folders/1zV7sXvU7YHKWyS3fYV0yyi7FyTjIpEuO?usp=sharing&#34;&gt;here&lt;/a&gt;. Due to the limited computational resources, we reported the highest results obtained from experiments conducted with the same random seed (&lt;code&gt;42&lt;/code&gt;). We acknolwedge this limitation in our work and plan to conduct repeated experiments in the next version to address it.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Feel free to raise issues if you have any questions.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Implementation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenLMLab/LOMO/main/assets/hook_func.png&#34; alt=&#34;Hook function&#34;&gt; Our implementation relies on injecting hook functions into PyTorch&#39;s backward pass. As depicted in the figure, we register a customized hook function for each parameter. When the gradient of a parameter is computed (prior to writing it to the .grad attribute), its corresponding hook function is invoked. For more information about hook functions and the backward pass of the autograd graph, please refer to &lt;a href=&#34;https://pytorch.org/docs/stable/notes/autograd.html#backward-hooks-execution&#34;&gt;PyTorch&#39;s documentation&lt;/a&gt;. In summary, during the backward pass, we go through a tensor and its grad_fn, write the gradient into the .grad attribute, and then pass to the next tensor.&lt;/p&gt; &#xA;&lt;p&gt;Our customized hook function scans all the parameters, updating a parameter if its .grad attribute is not empty, and then clears and frees the .grad attribute. Since the hook function for a parameter is called before its .grad attribute is set, the .grad attribute of the last parameter in the autograd graph is not ready when the last hook function is invoked. Therefore, we perform an additional scan to update the last parameter.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@inproceedings{Lv2023FullPF,&#xA;  title={Full Parameter Fine-tuning for Large Language Models with Limited Resources},&#xA;  author={Kai Lv and Yuqing Yang and Tengxiao Liu and Qi-jie Gao and Qipeng Guo and Xipeng Qiu},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>