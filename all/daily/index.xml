<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-11T01:21:16Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>haoheliu/AudioLDM2</title>
    <updated>2023-08-11T01:21:16Z</updated>
    <id>tag:github.com,2023-08-11:/haoheliu/AudioLDM2</id>
    <link href="https://github.com/haoheliu/AudioLDM2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Text-to-Audio/Music Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AudioLDM 2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.12503&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2301.12503-brightgreen.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://audioldm.github.io/audioldm2/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/GitHub.io-Audio_Samples-blue?logo=Github&amp;amp;style=flat-square&#34; alt=&#34;githubio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/haoheliu/audioldm2-text2audio-text2music&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo currently support Text-to-Audio Generation (including Music)&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add the text-to-speech checkpoint&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add the text-to-audio checkpoint that does not use FLAN-T5 Cross Attention&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Open-source the AudioLDM 1 &amp;amp; 2 training code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support the generation of longer audio (&amp;gt; 10s)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimizing the inference speed of the model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integration with the Diffusers library&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Web APP&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Prepare running environment&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n audioldm python=3.8; conda activate audioldm&#xA;pip3 install git+https://github.com/haoheliu/AudioLDM2.git&#xA;git clone https://github.com/haoheliu/AudioLDM2; cd AudioLDM2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Start the web application (powered by Gradio)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;A link will be printed out. Click the link to open the browser and play.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Commandline Usage&lt;/h2&gt; &#xA;&lt;p&gt;Prepare running environment&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Optional&#xA;conda create -n audioldm python=3.8; conda activate audioldm&#xA;# Install AudioLDM&#xA;pip3 install git+https://github.com/haoheliu/AudioLDM2.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generate based on a text prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audioldm2 -t &#34;Musical constellations twinkling in the night sky, forming a cosmic melody.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generate based on a list of text&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audioldm2 -tl batch.lst&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Random Seed Matters&lt;/h2&gt; &#xA;&lt;p&gt;Sometimes model may not perform well (sounds wired or low quality) when changing into a different hardware. In this case, please adjust the random seed and find the optimal one for your hardware.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audioldm2 --seed 1234 -t &#34;Musical constellations twinkling in the night sky, forming a cosmic melody.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Pretrained Models&lt;/h2&gt; &#xA;&lt;p&gt;You can choose model checkpoint by setting up &#34;model_name&#34;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;audioldm2 --model_name &#34;audioldm2-full-large-650k&#34; -t &#34;Musical constellations twinkling in the night sky, forming a cosmic melody.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We have three checkpoints you can choose for now:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;audioldm2-full&lt;/strong&gt; (default): This checkpoint can perform both sound effect and music generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;audioldm2-music-665k&lt;/strong&gt;: This checkpoint is specialized on music generation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;audioldm2-full-large-650k&lt;/strong&gt;: This checkpoint is the larger version of audioldm2-full.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Other options&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;  usage: audioldm2 [-h] [-t TEXT] [-tl TEXT_LIST] [-s SAVE_PATH] [--model_name {audioldm2-full,audioldm2-music-665k,audioldm2-full-large-650k}] [-b BATCHSIZE] [--ddim_steps DDIM_STEPS] [-gs GUIDANCE_SCALE]&#xA;                  [-n N_CANDIDATE_GEN_PER_TEXT] [--seed SEED]&#xA;&#xA;  optional arguments:&#xA;    -h, --help            show this help message and exit&#xA;    -t TEXT, --text TEXT  Text prompt to the model for audio generation&#xA;    -tl TEXT_LIST, --text_list TEXT_LIST&#xA;                          A file that contains text prompt to the model for audio generation&#xA;    -s SAVE_PATH, --save_path SAVE_PATH&#xA;                          The path to save model output&#xA;    --model_name {audioldm2-full,audioldm2-music-665k,audioldm2-full-large-650k}&#xA;                          The checkpoint you gonna use&#xA;    -b BATCHSIZE, --batchsize BATCHSIZE&#xA;                          Generate how many samples at the same time&#xA;    --ddim_steps DDIM_STEPS&#xA;                          The sampling step for DDIM&#xA;    -gs GUIDANCE_SCALE, --guidance_scale GUIDANCE_SCALE&#xA;                          Guidance scale (Large =&amp;gt; better quality and relavancy to text; Small =&amp;gt; better diversity)&#xA;    -n N_CANDIDATE_GEN_PER_TEXT, --n_candidate_gen_per_text N_CANDIDATE_GEN_PER_TEXT&#xA;                          Automatic quality control. This number control the number of candidates (e.g., generate three audios and choose the best to show you). A Larger value usually lead to better quality with&#xA;                          heavier computation&#xA;    --seed SEED           Change this value (any integer number) will lead to a different generation result.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Cite this work&lt;/h2&gt; &#xA;&lt;p&gt;If you found this tool useful, please consider citing&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;    AudioLDM 2 paper coming soon&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023audioldm,&#xA;  title={AudioLDM: Text-to-Audio Generation with Latent Diffusion Models},&#xA;  author={Liu, Haohe and Chen, Zehua and Yuan, Yi and Mei, Xinhao and Liu, Xubo and Mandic, Danilo and Wang, Wenwu and Plumbley, Mark D},&#xA;  journal={arXiv preprint arXiv:2301.12503},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>joonspk-research/generative_agents</title>
    <updated>2023-08-11T01:21:16Z</updated>
    <id>tag:github.com,2023-08-11:/joonspk-research/generative_agents</id>
    <link href="https://github.com/joonspk-research/generative_agents" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/joonspk-research/generative_agents/main/cover.png&#34; alt=&#34;Smallville&#34; style=&#34;width: 80%; min-width: 300px; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This repository accompanies our research paper titled &#34;&lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;Generative Agents: Interactive Simulacra of Human Behavior&lt;/a&gt;.&#34; It contains our core simulation module for generative agents—computational agents that simulate believable human behaviors—and their game environment. Below, we document the steps for setting up the simulation environment on your local machine and for replaying the simulation as a demo animation.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Isabella_Rodriguez.png&#34; alt=&#34;Generative Isabella&#34;&gt; Setting Up the Environment&lt;/h2&gt; &#xA;&lt;p&gt;To set up your environment, you will need to generate a &lt;code&gt;utils.py&lt;/code&gt; file that contains your OpenAI API key and download the necessary packages.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1. Generate Utils File&lt;/h3&gt; &#xA;&lt;p&gt;In the &lt;code&gt;reverie/backend_server&lt;/code&gt; folder (where &lt;code&gt;reverie.py&lt;/code&gt; is located), create a new file titled &lt;code&gt;utils.py&lt;/code&gt; and copy and paste the content below into the file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Copy and paste your OpenAI API Key&#xA;openai_api_key = &#34;&amp;lt;Your OpenAI API&amp;gt;&#34;&#xA;# Put your name&#xA;key_owner = &#34;&amp;lt;Name&amp;gt;&#34;&#xA;&#xA;maze_assets_loc = &#34;../../environment/frontend_server/static_dirs/assets&#34;&#xA;env_matrix = f&#34;{maze_assets_loc}/the_ville/matrix&#34;&#xA;env_visuals = f&#34;{maze_assets_loc}/the_ville/visuals&#34;&#xA;&#xA;fs_storage = &#34;../../environment/frontend_server/storage&#34;&#xA;fs_temp_storage = &#34;../../environment/frontend_server/temp_storage&#34;&#xA;&#xA;collision_block_id = &#34;32125&#34;&#xA;&#xA;# Verbose &#xA;debug = True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;&amp;lt;Your OpenAI API&amp;gt;&lt;/code&gt; with your OpenAI API key, and &lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt; with your name.&lt;/p&gt; &#xA;&lt;h3&gt;Step 2. Install requirements.txt&lt;/h3&gt; &#xA;&lt;p&gt;Install everything listed in the &lt;code&gt;requirements.txt&lt;/code&gt; file (I strongly recommend first setting up a virtualenv as usual). A note on Python version: we tested our environment on Python 3.9.12.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Klaus_Mueller.png&#34; alt=&#34;Generative Klaus&#34;&gt; Running a Simulation&lt;/h2&gt; &#xA;&lt;p&gt;To run a new simulation, you will need to concurrently start two servers: the environment server and the agent simulation server.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1. Starting the Environment Server&lt;/h3&gt; &#xA;&lt;p&gt;Again, the environment is implemented as a Django project, and as such, you will need to start the Django server. To do this, first navigate to &lt;code&gt;environment/frontend_server&lt;/code&gt; (this is where &lt;code&gt;manage.py&lt;/code&gt; is located) in your command line. Then run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python manage.py runserver&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, on your favorite browser, go to &lt;a href=&#34;http://localhost:8000/&#34;&gt;http://localhost:8000/&lt;/a&gt;. If you see a message that says, &#34;Your environment server is up and running,&#34; your server is running properly. Ensure that the environment server continues to run while you are running the simulation, so keep this command-line tab open! (Note: I recommend using either Chrome or Safari. Firefox might produce some frontend glitches, although it should not interfere with the actual simulation.)&lt;/p&gt; &#xA;&lt;h3&gt;Step 2. Starting the Simulation Server&lt;/h3&gt; &#xA;&lt;p&gt;Open up another command line (the one you used in Step 1 should still be running the environment server, so leave that as it is). Navigate to &lt;code&gt;reverie/backend_server&lt;/code&gt; and run &lt;code&gt;reverie.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python reverie.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will start the simulation server. A command-line prompt will appear, asking the following: &#34;Enter the name of the forked simulation: &#34;. To start a 3-agent simulation with Isabella Rodriguez, Maria Lopez, and Klaus Mueller, type the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;base_the_ville_isabella_maria_klaus&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The prompt will then ask, &#34;Enter the name of the new simulation: &#34;. Type any name to denote your current simulation (e.g., just &#34;test-simulation&#34; will do for now).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;test-simulation&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Keep the simulator server running. At this stage, it will display the following prompt: &#34;Enter option: &#34;&lt;/p&gt; &#xA;&lt;h3&gt;Step 3. Running and Saving the Simulation&lt;/h3&gt; &#xA;&lt;p&gt;On your browser, navigate to &lt;a href=&#34;http://localhost:8000/simulator_home&#34;&gt;http://localhost:8000/simulator_home&lt;/a&gt;. You should see the map of Smallville, along with a list of active agents on the map. You can move around the map using your keyboard arrows. Please keep this tab open. To run the simulation, type the following command in your simulation server in response to the prompt, &#34;Enter option&#34;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;run &amp;lt;step-count&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you will want to replace &lt;code&gt;&amp;lt;step-count&amp;gt;&lt;/code&gt; above with an integer indicating the number of game steps you want to simulate. For instance, if you want to simulate 100 game steps, you should input &lt;code&gt;run 100&lt;/code&gt;. One game step represents 10 seconds in the game.&lt;/p&gt; &#xA;&lt;p&gt;Your simulation should be running, and you will see the agents moving on the map in your browser. Once the simulation finishes running, the &#34;Enter option&#34; prompt will re-appear. At this point, you can simulate more steps by re-entering the run command with your desired game steps, exit the simulation without saving by typing &lt;code&gt;exit&lt;/code&gt;, or save and exit by typing &lt;code&gt;fin&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The saved simulation can be accessed the next time you run the simulation server by providing the name of your simulation as the forked simulation. This will allow you to restart your simulation from the point where you left off.&lt;/p&gt; &#xA;&lt;h3&gt;Step 4. Replaying a Simulation&lt;/h3&gt; &#xA;&lt;p&gt;You can replay a simulation that you have already run simply by having your environment server running and navigating to the following address in your browser: &lt;code&gt;http://localhost:8000/replay/&amp;lt;simulation-name&amp;gt;/&amp;lt;starting-time-step&amp;gt;&lt;/code&gt;. Please make sure to replace &lt;code&gt;&amp;lt;simulation-name&amp;gt;&lt;/code&gt; with the name of the simulation you want to replay, and &lt;code&gt;&amp;lt;starting-time-step&amp;gt;&lt;/code&gt; with the integer time-step from which you wish to start the replay.&lt;/p&gt; &#xA;&lt;p&gt;For instance, by visiting the following link, you will initiate a pre-simulated example, starting at time-step 1:&lt;br&gt; &lt;a href=&#34;http://localhost:8000/replay/July1_the_ville_isabella_maria_klaus-step-3-20/1/&#34;&gt;http://localhost:8000/replay/July1_the_ville_isabella_maria_klaus-step-3-20/1/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step 5. Demoing a Simulation&lt;/h3&gt; &#xA;&lt;p&gt;You may have noticed that all character sprites in the replay look identical. We would like to clarify that the replay function is primarily intended for debugging purposes and does not prioritize optimizing the size of the simulation folder or the visuals. To properly demonstrate a simulation with appropriate character sprites, you will need to compress the simulation first. To do this, open the &lt;code&gt;compress_sim_storage.py&lt;/code&gt; file located in the &lt;code&gt;reverie&lt;/code&gt; directory using a text editor. Then, execute the &lt;code&gt;compress&lt;/code&gt; function with the name of the target simulation as its input. By doing so, the simulation file will be compressed, making it ready for demonstration.&lt;/p&gt; &#xA;&lt;p&gt;To start the demo, go to the following address on your browser: &lt;code&gt;http://localhost:8000/demo/&amp;lt;simulation-name&amp;gt;/&amp;lt;starting-time-step&amp;gt;/&amp;lt;simulation-speed&amp;gt;&lt;/code&gt;. Note that &lt;code&gt;&amp;lt;simulation-name&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;starting-time-step&amp;gt;&lt;/code&gt; denote the same things as mentioned above. &lt;code&gt;&amp;lt;simulation-speed&amp;gt;&lt;/code&gt; can be set to control the demo speed, where 1 is the slowest, and 5 is the fastest. For instance, visiting the following link will start a pre-simulated example, beginning at time-step 1, with a medium demo speed:&lt;br&gt; &lt;a href=&#34;http://localhost:8000/demo/July1_the_ville_isabella_maria_klaus-step-3-20/1/3/&#34;&gt;http://localhost:8000/demo/July1_the_ville_isabella_maria_klaus-step-3-20/1/3/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Tips&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;ve noticed that OpenAI&#39;s API can hang when it reaches the hourly rate limit. When this happens, you may need to restart your simulation. For now, we recommend saving your simulation often as you progress to ensure that you lose as little of the simulation as possible when you do need to stop and rerun it. Running these simulations, at least as of early 2023, could be somewhat costly, especially when there are many agents in the environment.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Maria_Lopez.png&#34; alt=&#34;Generative Maria&#34;&gt; Simulation Storage Location&lt;/h2&gt; &#xA;&lt;p&gt;All simulations that you save will be located in &lt;code&gt;environment/frontend_server/storage&lt;/code&gt;, and all compressed demos will be located in &lt;code&gt;environment/frontend_server/compressed_storage&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Sam_Moore.png&#34; alt=&#34;Generative Sam&#34;&gt; Customization&lt;/h2&gt; &#xA;&lt;p&gt;There are two ways to optionally customize your simulations.&lt;/p&gt; &#xA;&lt;h3&gt;Author and Load Agent History&lt;/h3&gt; &#xA;&lt;p&gt;First is to initialize agents with unique history at the start of the simulation. To do this, you would want to 1) start your simulation using one of the base simulations, and 2) author and load agent history. More specifically, here are the steps:&lt;/p&gt; &#xA;&lt;h4&gt;Step 1. Starting Up a Base Simulation&lt;/h4&gt; &#xA;&lt;p&gt;There are two base simulations included in the repository: &lt;code&gt;base_the_ville_n25&lt;/code&gt; with 25 agents, and &lt;code&gt;base_the_ville_isabella_maria_klaus&lt;/code&gt; with 3 agents. Load one of the base simulations by following the steps until step 2 above.&lt;/p&gt; &#xA;&lt;h4&gt;Step 2. Loading a History File&lt;/h4&gt; &#xA;&lt;p&gt;Then, when prompted with &#34;Enter option: &#34;, you should load the agent history by responding with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;call -- load history the_ville/&amp;lt;history_file_name&amp;gt;.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you will need to replace &lt;code&gt;&amp;lt;history_file_name&amp;gt;&lt;/code&gt; with the name of an existing history file. There are two history files included in the repo as examples: &lt;code&gt;agent_history_init_n25.csv&lt;/code&gt; for &lt;code&gt;base_the_ville_n25&lt;/code&gt; and &lt;code&gt;agent_history_init_n3.csv&lt;/code&gt; for &lt;code&gt;base_the_ville_isabella_maria_klaus&lt;/code&gt;. These files include semicolon-separated lists of memory records for each of the agents—loading them will insert the memory records into the agents&#39; memory stream.&lt;/p&gt; &#xA;&lt;h4&gt;Step 3. Further Customization&lt;/h4&gt; &#xA;&lt;p&gt;To customize the initialization by authoring your own history file, place your file in the following folder: &lt;code&gt;environment/frontend_server/static_dirs/assets/the_ville&lt;/code&gt;. The column format for your custom history file will have to match the example history files included. Therefore, we recommend starting the process by copying and pasting the ones that are already in the repository.&lt;/p&gt; &#xA;&lt;h3&gt;Create New Base Simulations&lt;/h3&gt; &#xA;&lt;p&gt;For a more involved customization, you will need to author your own base simulation files. The most straightforward approach would be to copy and paste an existing base simulation folder, renaming and editing it according to your requirements. This process will be simpler if you decide to keep the agent names unchanged. However, if you wish to change their names or increase the number of agents that the Smallville map can accommodate, you might need to directly edit the map using the &lt;a href=&#34;https://www.mapeditor.org/&#34;&gt;Tiled&lt;/a&gt; map editor.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Eddy_Lin.png&#34; alt=&#34;Generative Eddy&#34;&gt; Authors and Citation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; Joon Sung Park, Joseph C. O&#39;Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, Michael S. Bernstein&lt;/p&gt; &#xA;&lt;p&gt;Please cite our paper if you use the code or data in this repository.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{Park2023GenerativeAgents,  &#xA;author = {Park, Joon Sung and O&#39;Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},  &#xA;title = {Generative Agents: Interactive Simulacra of Human Behavior},  &#xA;year = {2023},  &#xA;publisher = {Association for Computing Machinery},  &#xA;address = {New York, NY, USA},  &#xA;booktitle = {In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST &#39;23)},  &#xA;keywords = {Human-AI interaction, agents, generative AI, large language models},  &#xA;location = {San Francisco, CA, USA},  &#xA;series = {UIST &#39;23}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;img src=&#34;https://joonsungpark.s3.amazonaws.com:443/static/assets/characters/profile/Wolfgang_Schulz.png&#34; alt=&#34;Generative Wolfgang&#34;&gt; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We encourage you to support the following three amazing artists who have designed the game assets for this project, especially if you are planning to use the assets included here for your own project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Background art: &lt;a href=&#34;https://twitter.com/_PixyMoon_&#34;&gt;PixyMoon (@_PixyMoon _)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Furniture/interior design: &lt;a href=&#34;https://twitter.com/lime_px&#34;&gt;LimeZu (@lime_px)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Character design: &lt;a href=&#34;https://twitter.com/pipohi&#34;&gt;ぴぽ (@pipohi)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition, we thank Lindsay Popowski, Philip Guo, Michael Terry, and the Center for Advanced Study in the Behavioral Sciences (CASBS) community for their insights, discussions, and support. Lastly, all locations featured in Smallville are inspired by real-world locations that Joon has frequented as an undergraduate and graduate student---he thanks everyone there for feeding and supporting him all these years.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/candle</title>
    <updated>2023-08-11T01:21:16Z</updated>
    <id>tag:github.com,2023-08-11:/huggingface/candle</id>
    <link href="https://github.com/huggingface/candle" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Minimalist ML framework for Rust&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;candle&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://crates.io/crates/candle-core&#34;&gt;&lt;img src=&#34;https://img.shields.io/crates/v/candle-core.svg?sanitize=true&#34; alt=&#34;Latest version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.rs/candle-core&#34;&gt;&lt;img src=&#34;https://docs.rs/candle-core/badge.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/crates/l/candle-core.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use. Try our online demos: &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-whisper&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-llama2&#34;&gt;llama2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;let a = Tensor::randn(0f32, 1., (2, 3), &amp;amp;Device::Cpu)?;&#xA;let b = Tensor::randn(0f32, 1., (3, 4), &amp;amp;Device::Cpu)?;&#xA;&#xA;let c = a.matmul(&amp;amp;b)?;&#xA;println!(&#34;{c}&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Check out our examples&lt;/h2&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/&#34;&gt;examples&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/whisper/&#34;&gt;Whisper&lt;/a&gt;: speech recognition model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/llama/&#34;&gt;Llama and Llama-v2&lt;/a&gt;: general LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/falcon/&#34;&gt;Falcon&lt;/a&gt;: general LLM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/bert/&#34;&gt;Bert&lt;/a&gt;: useful for sentence embeddings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/examples/bigcode/&#34;&gt;StarCoder&lt;/a&gt;: LLM specialized to code generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Run them using the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --example whisper --release&#xA;cargo run --example llama --release&#xA;cargo run --example falcon --release&#xA;cargo run --example bert --release&#xA;cargo run --example bigcode --release&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In order to use &lt;strong&gt;CUDA&lt;/strong&gt; add &lt;code&gt;--features cuda&lt;/code&gt; to the example command line.&lt;/p&gt; &#xA;&lt;p&gt;There are also some wasm examples for whisper and &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt;. You can either build them with &lt;code&gt;trunk&lt;/code&gt; or try them online: &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-whisper&#34;&gt;whisper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/lmz/candle-llama2&#34;&gt;llama2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For llama2, run the following command to retrieve the weight files and start a test server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd candle-wasm-examples/llama2-c&#xA;wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/model.bin&#xA;wget https://huggingface.co/spaces/lmz/candle-llama2/resolve/main/tokenizer.json&#xA;trunk serve --release --public-url /candle-llama2/ --port 8081&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And then head over to &lt;a href=&#34;http://localhost:8081/candle-llama2&#34;&gt;http://localhost:8081/candle-llama2&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;!-- ANCHOR: features ---&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple syntax, looks and feels like PyTorch.&lt;/li&gt; &#xA; &lt;li&gt;CPU and Cuda backends, m1, f16, bf16.&lt;/li&gt; &#xA; &lt;li&gt;Serverless (on CPU), small and fast deployments&lt;/li&gt; &#xA; &lt;li&gt;WASM support, run your models in a browser.&lt;/li&gt; &#xA; &lt;li&gt;Model training.&lt;/li&gt; &#xA; &lt;li&gt;Distributed computing using NCCL.&lt;/li&gt; &#xA; &lt;li&gt;Model support out of the box: Llama, Whisper, Falcon, StarCoder...&lt;/li&gt; &#xA; &lt;li&gt;Embed user-defined ops/kernels, such as &lt;a href=&#34;https://github.com/huggingface/candle/raw/89ba005962495f2bfbda286e185e9c3c7f5300a3/candle-flash-attn/src/lib.rs#L152&#34;&gt;flash-attention v2&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- ANCHOR_END: features ---&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;!-- ANCHOR: cheatsheet ---&gt; &#xA;&lt;p&gt;Cheatsheet:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Using PyTorch&lt;/th&gt; &#xA;   &lt;th&gt;Using Candle&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Creation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.Tensor([[1, 2], [3, 4]])&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Tensor::new(&amp;amp;[[1f32, 2.], [3., 4.]], &amp;amp;Device::Cpu)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Creation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.zeros((2, 2))&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Tensor::zeros((2, 2), DType::F32, &amp;amp;Device::Cpu)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Indexing&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor[:, :4]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.i((.., ..4))?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Operations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.view((2, 2))&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.reshape((2, 2))?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Operations&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a.matmul(b)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a.matmul(&amp;amp;b)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arithmetic&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;a + b&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&amp;amp;a + &amp;amp;b&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Device&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to(device=&#34;cuda&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to_device(&amp;amp;Device::Cuda(0))?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dtype&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to(dtype=torch.float16)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;tensor.to_dtype(&amp;amp;DType::F16)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Saving&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;torch.save({&#34;A&#34;: A}, &#34;model.bin&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;candle::safetensors::save(&amp;amp;HashMap::from([(&#34;A&#34;, A)]), &#34;model.safetensors&#34;)?&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Loading&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;weights = torch.load(&#34;model.bin&#34;)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;candle::safetensors::load(&#34;model.safetensors&#34;, &amp;amp;device)&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- ANCHOR_END: cheatsheet ---&gt; &#xA;&lt;h2&gt;Structure&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-core&#34;&gt;candle-core&lt;/a&gt;: Core ops, devices, and &lt;code&gt;Tensor&lt;/code&gt; struct definition&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-nn/&#34;&gt;candle-nn&lt;/a&gt;: Tools to build real models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-examples/&#34;&gt;candle-examples&lt;/a&gt;: Examples of using the library in realistic settings&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-kernels/&#34;&gt;candle-kernels&lt;/a&gt;: CUDA custom kernels&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-datasets/&#34;&gt;candle-datasets&lt;/a&gt;: Datasets and data loaders.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-transformers&#34;&gt;candle-transformers&lt;/a&gt;: transformers-related utilities.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/candle/main/candle-flash-attn&#34;&gt;candle-flash-attn&lt;/a&gt;: Flash attention v2 layer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Why should I use Candle?&lt;/h3&gt; &#xA;&lt;p&gt;Candle&#39;s core goal is to &lt;em&gt;make serverless inference possible&lt;/em&gt;. Full machine learning frameworks like PyTorch are very large, which makes creating instances on a cluster slow. Candle allows deployment of lightweight binaries.&lt;/p&gt; &#xA;&lt;p&gt;Secondly, Candle lets you &lt;em&gt;remove Python&lt;/em&gt; from production workloads. Python overhead can seriously hurt performance, and the &lt;a href=&#34;https://www.backblaze.com/blog/the-python-gil-past-present-and-future/&#34;&gt;GIL&lt;/a&gt; is a notorious source of headaches.&lt;/p&gt; &#xA;&lt;p&gt;Finally, Rust is cool! A lot of the HF ecosystem already has Rust crates, like &lt;a href=&#34;https://github.com/huggingface/safetensors&#34;&gt;safetensors&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;tokenizers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Other ML frameworks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/coreylowman/dfdx&#34;&gt;dfdx&lt;/a&gt; is a formidable crate, with shapes being included in types. This prevents a lot of headaches by getting the compiler to complain about shape mismatches right off the bat. However, we found that some features still require nightly, and writing code can be a bit daunting for non rust experts.&lt;/p&gt; &lt;p&gt;We&#39;re leveraging and contributing to other core crates for the runtime so hopefully both crates can benefit from each other.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/burn-rs/burn&#34;&gt;burn&lt;/a&gt; is a general crate that can leverage multiple backends so you can choose the best engine for your workload.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/LaurentMazare/tch-rs.git&#34;&gt;tch-rs&lt;/a&gt; Bindings to the torch library in Rust. Extremely versatile, but they bring in the entire torch library into the runtime. The main contributor of &lt;code&gt;tch-rs&lt;/code&gt; is also involved in the development of &lt;code&gt;candle&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Missing symbols when compiling with the mkl feature.&lt;/h3&gt; &#xA;&lt;p&gt;If you get some missing symbols when compiling binaries/tests using the mkl features, e.g.:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  = note: /usr/bin/ld: (....o): in function `blas::sgemm&#39;:&#xA;          .../blas-0.22.0/src/lib.rs:1944: undefined reference to `sgemm_&#39; collect2: error: ld returned 1 exit status&#xA;&#xA;  = note: some `extern` functions couldn&#39;t be found; some native libraries may need to be installed or have their path specified&#xA;  = note: use the `-l` flag to specify native libraries to link&#xA;  = note: use the `cargo:rustc-link-lib` directive to specify the native libraries to link with Cargo (see https://doc.rust-lang.org/cargo/reference/build-scripts.html#cargorustc-link-libkindname)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is likely due to a missing linker flag that was needed to enable the mkl library. You can try adding the following at the top of your binary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;extern crate intel_mkl_src;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tracking down errors&lt;/h3&gt; &#xA;&lt;p&gt;You can set &lt;code&gt;RUST_BACKTRACE=1&lt;/code&gt; to be provided with backtraces when a candle error is generated.&lt;/p&gt;</summary>
  </entry>
</feed>