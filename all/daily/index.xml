<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-15T01:21:50Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>lllyasviel/Fooocus</title>
    <updated>2023-08-15T01:21:50Z</updated>
    <id>tag:github.com,2023-08-15:/lllyasviel/Fooocus</id>
    <link href="https://github.com/lllyasviel/Fooocus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Focus on prompting and generating&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Fooocus&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/lllyasviel/misc_files/main/202308/fsm2.png&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p&gt;Fooocus is an image generating software.&lt;/p&gt; &#xA;&lt;p&gt;Fooocus is a rethinking of Stable Diffusion and Midjourney‚Äôs designs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Learned from Stable Diffusion, the software is offline, open source, and free.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Learned from Midjourney, the manual tweaking is not needed, and users only need to focus on the prompts and images.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Fooocus has included and automated &lt;a href=&#34;https://raw.githubusercontent.com/lllyasviel/Fooocus/main/#tech_list&#34;&gt;lots of inner optimizations and quality improvements&lt;/a&gt;. Users can forget all those difficult technical parameters, and just enjoy the interaction between human and computer to &#34;explore new mediums of thought and expanding the imaginative powers of the human species&#34; &lt;code&gt;[1]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Fooocus has simplified the installation. Between pressing &#34;download&#34; and generating the first image, the number of needed mouse clicks is strictly limited to less than 3. Minimal GPU memory requirement is 4GB (Nvidia).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;[1]&lt;/code&gt; David Holz, 2019.&lt;/p&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;You can directly download Fooocus with:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/lllyasviel/Fooocus/releases/download/release/Fooocus_win64_1-1-10.7z&#34;&gt;&amp;gt;&amp;gt;&amp;gt; Click here to download &amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;After you download the file, please uncompress it, and then run the &#34;run.bat&#34;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Fooocus/assets/19834515/c49269c4-c274-4893-b368-047c401cc58c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In the first time you launch the software, it will automatically download models:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;It will download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0_0.9vae.safetensors&#34;&gt;sd_xl_base_1.0_0.9vae.safetensors from here&lt;/a&gt; as the file &#34;Fooocus\models\checkpoints\sd_xl_base_1.0_0.9vae.safetensors&#34;.&lt;/li&gt; &#xA; &lt;li&gt;It will download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0_0.9vae.safetensors&#34;&gt;sd_xl_refiner_1.0_0.9vae.safetensors from here&lt;/a&gt; as the file &#34;Fooocus\models\checkpoints\sd_xl_refiner_1.0_0.9vae.safetensors&#34;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Fooocus/assets/19834515/d386f817-4bd7-490c-ad89-c1e228c23447&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you already have these files, you can copy them to the above locations to speed up installation.&lt;/p&gt; &#xA;&lt;p&gt;Below is a test on a relatively low-end laptop with &lt;strong&gt;16GB System RAM&lt;/strong&gt; and &lt;strong&gt;6GB VRAM&lt;/strong&gt; (Nvidia 3060 laptop). The speed on this machine is about 1.35 seconds per iteration. Pretty impressive ‚Äì nowadays laptops with 3060 are usually at very acceptable price.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Fooocus/assets/19834515/938737a5-b105-4f19-b051-81356cb7c495&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that the minimal requirement is &lt;strong&gt;4GB Nvidia GPU memory (4GB VRAM)&lt;/strong&gt; and &lt;strong&gt;8GB system memory (8GB RAM)&lt;/strong&gt;. This requires using Microsoft‚Äôs Virtual Swap technique, which is automatically enabled by your Windows installation in most cases, so you often do not need to do anything about it. However, if you are not sure, or if you manually turned it off (would anyone really do that?), you can enable it here:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click here to the see the image instruction. &lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Fooocus/assets/19834515/2a06b130-fe9b-4504-94f1-2763be4476e9&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;Please open an issue if you use similar devices but still cannot achieve acceptable performances.&lt;/p&gt; &#xA;&lt;h3&gt;Colab&lt;/h3&gt; &#xA;&lt;p&gt;(Last tested - 2023 Aug 14)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Colab&lt;/th&gt; &#xA;   &lt;th&gt;Info&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/lllyasviel/Fooocus/blob/main/colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fooocus Colab (Official Version)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note that sometimes this Colab will say like &#34;you must restart the runtime in order to use newly installed XX&#34;. This can be safely ignored.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/camenduru&#34;&gt;camenduru&lt;/a&gt;&#39;s codes!&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;p&gt;The command lines are&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Fooocus.git&#xA;cd Fooocus&#xA;conda env create -f environment.yaml&#xA;conda activate fooocus&#xA;pip install -r requirements_versions.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then download the models: download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0_0.9vae.safetensors&#34;&gt;sd_xl_base_1.0_0.9vae.safetensors from here&lt;/a&gt; as the file &#34;Fooocus\models\checkpoints\sd_xl_base_1.0_0.9vae.safetensors&#34;, and download &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0_0.9vae.safetensors&#34;&gt;sd_xl_refiner_1.0_0.9vae.safetensors from here&lt;/a&gt; as the file &#34;Fooocus\models\checkpoints\sd_xl_refiner_1.0_0.9vae.safetensors&#34;. &lt;strong&gt;Or let Fooocus automatically download the models&lt;/strong&gt; using the launcher:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python launch.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or if you want to open a remote port, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python launch.py --listen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Mac/Windows(AMD GPUs)&lt;/h3&gt; &#xA;&lt;p&gt;Coming soon ...&lt;/p&gt; &#xA;&lt;h2&gt;List of &#34;Hidden&#34; Tricks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a name=&#34;tech_list&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below things are already inside the software, and &lt;strong&gt;users do not need to do anything about these&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note that some of these tricks are currently (2023 Aug 11) impossible to reproduce in Automatic1111&#39;s interface or ComfyUI&#39;s node system. You may expect better results from Fooocus than other software even when they use similar models/pipelines.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Native refiner swap inside one single k-sampler. The advantage is that now the refiner model can reuse the base model&#39;s momentum (or ODE&#39;s history parameters) collected from k-sampling to achieve more coherent sampling. In Automatic1111&#39;s high-res fix and ComfyUI&#39;s node system, the base model and refiner use two independent k-samplers, which means the momentum is largely wasted, and the sampling continuity is broken. Fooocus uses its own advanced k-diffusion sampling that ensures seamless, native, and continuous swap in a refiner setup. (Update Aug 13: Actually I discussed this with Automatic1111 several days ago and it seems that the ‚Äúnative refiner swap inside one single k-sampler‚Äù is &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/12371&#34;&gt;merged&lt;/a&gt; into the dev branch of webui. Great!)&lt;/li&gt; &#xA; &lt;li&gt;Negative ADM guidance. Because the highest resolution level of XL Base does not have cross attentions, the positive and negative signals for XL&#39;s highest resolution level cannot receive enough contrasts during the CFG sampling, causing the results look a bit plastic or overly smooth in certain cases. Fortunately, since the XL&#39;s highest resolution level is still conditioned on image aspect ratios (ADM), we can modify the adm on the positive/negative side to compensate for the lack of CFG contrast in the highest resolution level.&lt;/li&gt; &#xA; &lt;li&gt;We implemented a carefully tuned variation of the Section 5.1 of &lt;a href=&#34;https://arxiv.org/pdf/2210.00939.pdf&#34;&gt;&#34;Improving Sample Quality of Diffusion Models Using Self-Attention Guidance&#34;&lt;/a&gt;. The weight is set to very low, but this is Fooocus&#39;s final guarantee to make sure than the XL will never yield overly smooth or plastic appearance. This can almostly eliminate all cases that XL still occasionally produce overly smooth results even with negative ADM guidance.&lt;/li&gt; &#xA; &lt;li&gt;We modified the style templates a bit and added the &#34;cinematic-default&#34;.&lt;/li&gt; &#xA; &lt;li&gt;We tested the &#34;sd_xl_offset_example-lora_1.0.safetensors&#34; and it seems that when the lora weight is below 0.5, the results are always better than XL without lora.&lt;/li&gt; &#xA; &lt;li&gt;The parameters of samplers are carefully tuned.&lt;/li&gt; &#xA; &lt;li&gt;Because XL uses positional encoding for generation resolution, images generated by several fixed resolutions look a bit better than that from arbitrary resolutions (because the positional encoding is not very good at handling int numbers that are unseen during training). This suggests that the resolutions in UI may be hard coded for best results.&lt;/li&gt; &#xA; &lt;li&gt;Separated prompts for two different text encoders seem unnecessary. Separated prompts for base model and refiner may work but the effects are random, and we refrain from implement this.&lt;/li&gt; &#xA; &lt;li&gt;DPM family seems well-suited for XL, since XL sometimes generates overly smooth texture but DPM family sometimes generate overly dense detail in texture. Their joint effect looks neutral and appealing to human perception.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Thanks&lt;/h2&gt; &#xA;&lt;p&gt;The codebase starts from an odd mixture of &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;Automatic1111&lt;/a&gt; and &lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;. (And they both use GPL license.)&lt;/p&gt; &#xA;&lt;h2&gt;Update Log&lt;/h2&gt; &#xA;&lt;p&gt;The log is &lt;a href=&#34;https://raw.githubusercontent.com/lllyasviel/Fooocus/main/update_log.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Alexays/Waybar</title>
    <updated>2023-08-15T01:21:50Z</updated>
    <id>tag:github.com,2023-08-15:/Alexays/Waybar</id>
    <link href="https://github.com/Alexays/Waybar" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Highly customizable Wayland bar for Sway and Wlroots based compositors. ‚úåÔ∏è üéâ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Waybar &lt;a href=&#34;https://raw.githubusercontent.com/Alexays/Waybar/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;Licence&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paypal.me/ARouillard&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Donate-Paypal-2244dd.svg?sanitize=true&#34; alt=&#34;Paypal Donate&#34;&gt;&lt;/a&gt;&lt;br&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alexays/waybar/master/preview-2.png&#34; alt=&#34;Waybar&#34;&gt;&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Highly customizable Wayland bar for Sway and Wlroots based compositors.&lt;br&gt; Available in Arch &lt;a href=&#34;https://www.archlinux.org/packages/extra/x86_64/waybar/&#34;&gt;extra&lt;/a&gt; or &lt;a href=&#34;https://aur.archlinux.org/packages/waybar-git/&#34;&gt;AUR&lt;/a&gt;, &lt;a href=&#34;https://packages.gentoo.org/packages/gui-apps/waybar&#34;&gt;Gentoo&lt;/a&gt;, &lt;a href=&#34;https://build.opensuse.org/package/show/X11:Wayland/waybar&#34;&gt;openSUSE&lt;/a&gt;, and &lt;a href=&#34;https://pkgs.alpinelinux.org/packages?name=waybar&#34;&gt;Alpine Linux&lt;/a&gt;.&lt;br&gt; &lt;em&gt;Waybar &lt;a href=&#34;https://github.com/Alexays/Waybar/wiki/Examples&#34;&gt;examples&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Current features&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Sway (Workspaces, Binding mode, Focused window name)&lt;/li&gt; &#xA; &lt;li&gt;River (Mapping mode, Tags, Focused window name)&lt;/li&gt; &#xA; &lt;li&gt;Hyprland (Focused window name)&lt;/li&gt; &#xA; &lt;li&gt;DWL (Tags) &lt;a href=&#34;https://github.com/djpohly/dwl/wiki/ipc&#34;&gt;requires dwl ipc patch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Tray &lt;a href=&#34;https://github.com/Alexays/Waybar/issues/21&#34;&gt;#21&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Local time&lt;/li&gt; &#xA; &lt;li&gt;Battery&lt;/li&gt; &#xA; &lt;li&gt;UPower&lt;/li&gt; &#xA; &lt;li&gt;Network&lt;/li&gt; &#xA; &lt;li&gt;Bluetooth&lt;/li&gt; &#xA; &lt;li&gt;Pulseaudio&lt;/li&gt; &#xA; &lt;li&gt;Wireplumber&lt;/li&gt; &#xA; &lt;li&gt;Disk&lt;/li&gt; &#xA; &lt;li&gt;Memory&lt;/li&gt; &#xA; &lt;li&gt;Cpu load average&lt;/li&gt; &#xA; &lt;li&gt;Temperature&lt;/li&gt; &#xA; &lt;li&gt;MPD&lt;/li&gt; &#xA; &lt;li&gt;Custom scripts&lt;/li&gt; &#xA; &lt;li&gt;Custom image&lt;/li&gt; &#xA; &lt;li&gt;Multiple output configuration&lt;/li&gt; &#xA; &lt;li&gt;And many more customizations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Configuration and Styling&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Alexays/Waybar/wiki&#34;&gt;See the wiki for more details&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Waybar is available from a number of Linux distributions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://repology.org/project/waybar/versions&#34;&gt;&lt;img src=&#34;https://repology.org/badge/vertical-allrepos/waybar.svg?sanitize=true&#34; alt=&#34;Packaging status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An Ubuntu PPA with more recent versions is available &lt;a href=&#34;https://launchpad.net/~nschloe/+archive/ubuntu/waybar&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Building from source&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/Alexays/Waybar&#xA;$ cd Waybar&#xA;$ meson build&#xA;$ ninja -C build&#xA;$ ./build/waybar&#xA;# If you want to install it&#xA;$ ninja -C build install&#xA;$ waybar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gtkmm3&#xA;jsoncpp&#xA;libsigc++&#xA;fmt&#xA;wayland&#xA;chrono-date&#xA;spdlog&#xA;libgtk-3-dev [gtk-layer-shell]&#xA;gobject-introspection [gtk-layer-shell]&#xA;libgirepository1.0-dev [gtk-layer-shell]&#xA;libpulse [Pulseaudio module]&#xA;libnl [Network module]&#xA;libappindicator-gtk3 [Tray module]&#xA;libdbusmenu-gtk3 [Tray module]&#xA;libmpdclient [MPD module]&#xA;libsndio [sndio module]&#xA;libevdev [KeyboardState module]&#xA;xkbregistry&#xA;upower [UPower battery module]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Build dependencies&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmake&#xA;meson&#xA;scdoc&#xA;wayland-protocols&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Ubuntu, you can install all the relevant dependencies using this command (tested with 19.10 and 20.04):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install \&#xA;  clang-tidy \&#xA;  gobject-introspection \&#xA;  libdbusmenu-gtk3-dev \&#xA;  libevdev-dev \&#xA;  libfmt-dev \&#xA;  libgirepository1.0-dev \&#xA;  libgtk-3-dev \&#xA;  libgtkmm-3.0-dev \&#xA;  libinput-dev \&#xA;  libjsoncpp-dev \&#xA;  libmpdclient-dev \&#xA;  libnl-3-dev \&#xA;  libnl-genl-3-dev \&#xA;  libpulse-dev \&#xA;  libsigc++-2.0-dev \&#xA;  libspdlog-dev \&#xA;  libwayland-dev \&#xA;  scdoc \&#xA;  upower \&#xA;  libxkbregistry-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Contributions welcome!&lt;br&gt; Have fun :)&lt;br&gt; The style guidelines are &lt;a href=&#34;https://google.github.io/styleguide/cppguide.html&#34;&gt;Google&#39;s&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Waybar is licensed under the MIT license. &lt;a href=&#34;https://github.com/Alexays/Waybar/raw/master/LICENSE&#34;&gt;See LICENSE for more information&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/neuralangelo</title>
    <updated>2023-08-15T01:21:50Z</updated>
    <id>tag:github.com,2023-08-15:/NVlabs/neuralangelo</id>
    <link href="https://github.com/NVlabs/neuralangelo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of &#34;Neuralangelo: High-Fidelity Neural Surface Reconstruction&#34; (CVPR 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Neuralangelo&lt;/h1&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://research.nvidia.com/labs/dir/neuralangelo/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2306.03092/&#34;&gt;Paper&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This is the official repo for the implementation of &lt;strong&gt;Neuralangelo: High-Fidelity Neural Surface Reconstruction&lt;/strong&gt;.&lt;br&gt; The code is built upon the Imaginaire library from the Deep Imagination Research Group at NVIDIA.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/neuralangelo/main/assets/teaser.gif&#34;&gt; &#xA;&lt;p&gt;For business inquiries, please submit the &lt;a href=&#34;https://www.nvidia.com/en-us/research/inquiries/&#34;&gt;NVIDIA research licensing form&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;We offer two ways to setup the environment:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;We provide prebuilt Docker images, where&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;docker.io/chenhsuanlin/colmap:3.8&lt;/code&gt; is for running COLMAP and the data preprocessing scripts. This includes the prebuilt COLMAP library (CUDA-supported).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;docker.io/chenhsuanlin/neuralangelo:23.04-py3&lt;/code&gt; is for running the main Neuralangelo pipeline.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;p&gt;The corresponding Dockerfiles can be found in the &lt;code&gt;docker&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The conda environment for Neuralangelo. Install the dependencies and activate the environment &lt;code&gt;neuralangelo&lt;/code&gt; with&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create --file neuralangelo.yaml&#xA;conda activate neuralangelo&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For COLMAP, alternative installation options are also available on the &lt;a href=&#34;https://colmap.github.io/&#34;&gt;COLMAP website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Data preparation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/neuralangelo/main/DATA_PROCESSING.md&#34;&gt;Data Preparation&lt;/a&gt; for step-by-step instructions.&lt;br&gt; We assume known camera poses for each extracted frame from the video. The code uses the same json format as &lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34;&gt;Instant NGP&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run Neuralangelo!&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;EXPERIMENT=toy_example&#xA;GROUP=example_group&#xA;NAME=example_name&#xA;CONFIG=projects/neuralangelo/configs/custom/${EXPERIMENT}.yaml&#xA;GPUS=1  # use &amp;gt;1 for multi-GPU training!&#xA;torchrun --nproc_per_node=${GPUS} train.py \&#xA;    --logdir=logs/${GROUP}/${NAME} \&#xA;    --config=${CONFIG} \&#xA;    --show_pbar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some useful notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This codebase supports logging with &lt;a href=&#34;https://wandb.ai/site&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt;. You should have a W&amp;amp;B account for this. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;--wandb&lt;/code&gt; to the command line argument to enable W&amp;amp;B logging.&lt;/li&gt; &#xA;   &lt;li&gt;Add &lt;code&gt;--wandb_name&lt;/code&gt; to specify the W&amp;amp;B project name.&lt;/li&gt; &#xA;   &lt;li&gt;More detailed control can be found in the &lt;code&gt;init_wandb()&lt;/code&gt; function in &lt;code&gt;imaginaire/trainers/base.py&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Configs can be overridden through the command line (e.g. &lt;code&gt;--optim.params.lr=1e-2&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;--checkpoint={CHECKPOINT_PATH}&lt;/code&gt; to initialize with a certain checkpoint; set &lt;code&gt;--resume=True&lt;/code&gt; to resume training.&lt;/li&gt; &#xA; &lt;li&gt;If appearance embeddings are enabled, make sure &lt;code&gt;data.num_images&lt;/code&gt; is set to the number of training images.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Isosurface extraction&lt;/h2&gt; &#xA;&lt;p&gt;Use the following command to run isosurface mesh extraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CHECKPOINT=logs/${GROUP}/${NAME}/xxx.pt&#xA;OUTPUT_MESH=xxx.ply&#xA;CONFIG=projects/neuralangelo/configs/custom/${EXPERIMENT}.yaml&#xA;RESOLUTION=2048&#xA;BLOCK_RES=128&#xA;GPUS=1  # use &amp;gt;1 for multi-GPU mesh extraction&#xA;torchrun --nproc_per_node=${GPUS} projects/neuralangelo/scripts/extract_mesh.py \&#xA;    --logdir=logs/${GROUP}/${NAME} \&#xA;    --config=${CONFIG} \&#xA;    --checkpoint=${CHECKPOINT} \&#xA;    --output_file=${OUTPUT_MESH} \&#xA;    --resolution=${RESOLUTION} \&#xA;    --block_res=${BLOCK_RES}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;If you find our code useful for your research, please cite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{li2023neuralangelo,&#xA;  title={Neuralangelo: High-Fidelity Neural Surface Reconstruction},&#xA;  author={Li, Zhaoshuo and M\&#34;uller, Thomas and Evans, Alex and Taylor, Russell H and Unberath, Mathias and Liu, Ming-Yu and Lin, Chen-Hsuan},&#xA;  booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>