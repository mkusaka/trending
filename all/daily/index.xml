<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-02T01:25:19Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>heyform/heyform</title>
    <updated>2024-04-02T01:25:19Z</updated>
    <id>tag:github.com,2024-04-02:/heyform/heyform</id>
    <link href="https://github.com/heyform/heyform" rel="alternate"></link>
    <summary type="html">&lt;p&gt;HeyForm is an open-source form builder that allows anyone to create engaging conversational forms for surveys, questionnaires, quizzes, and polls. No coding skills required.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 align=&#34;center&#34;&gt; &lt;img alt=&#34;heyform logo&#34; height=&#34;60&#34; src=&#34;https://raw.githubusercontent.com/heyform/heyform/main/assets/images/logo.svg?sanitize=true&#34;&gt; &lt;/h1&gt; &#xA; &lt;p&gt;HeyForm is an open-source form builder that allows anyone to create engaging conversational forms for surveys, questionnaires, quizzes, and polls. No coding skills required.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://heyform.net&#34;&gt;Website&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://docs.heyform.net&#34;&gt;Documentation&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://heyform.net/blog&#34;&gt;Blog&lt;/a&gt; | &lt;a target=&#34;_blank&#34; href=&#34;https://twitter.com/HeyformHQ&#34;&gt;Twitter&lt;/a&gt; &lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/heyform/heyform/main/assets/images/screenshot.png&#34; alt=&#34;HeyForm&#34;&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;HeyForm simplifies the creation of conversational forms, making it accessible for anyone to gather information or feedback through engaging surveys, quizzes, and polls. We are committed to enhancing HeyForm with regular updates, including bug fixes, new features, and performance improvements.&lt;/p&gt; &#xA;&lt;h3&gt;Build Forms with Ease&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìù &lt;strong&gt;Versatile Inputs&lt;/strong&gt;: From basic text, email, and phone number fields to advanced options like picture choices, date pickers, and file uploads, HeyForm supports a wide array of input types.&lt;/li&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Smart Logic&lt;/strong&gt;: Conditional logic and URL redirections for dynamic, adaptable forms.&lt;/li&gt; &#xA; &lt;li&gt;üîó &lt;strong&gt;Powerful Integrations&lt;/strong&gt;: Connect with webhooks, analytics, marketing platforms, and tools like Zapier and Make.com.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Customize to Your Brand&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üé® &lt;strong&gt;Visual Themes&lt;/strong&gt;: Tailor the look and feel of your forms to match your brand identity with customizable fonts, colors, backgrounds, and more.&lt;/li&gt; &#xA; &lt;li&gt;‚ú® &lt;strong&gt;Advanced Theming&lt;/strong&gt;: Gain greater control with extensive customization options, including custom CSS for deeper personalization.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Analyze and Act on Data&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìä &lt;strong&gt;Insightful Analytics&lt;/strong&gt;: Gain insights with detailed analytics, including drop-off rates and completion rates.&lt;/li&gt; &#xA; &lt;li&gt;üì§ &lt;strong&gt;Data Export&lt;/strong&gt;: Easily export your form results to CSV for further analysis or integration into your systems.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting started with HeyForm&lt;/h2&gt; &#xA;&lt;p&gt;The simplest and most efficient way to dive into HeyForm is through &lt;a href=&#34;https://my.heyform.net&#34;&gt;our official hosted service&lt;/a&gt;. When you choose this cloud version, you&#39;re getting the advantage of high reliability, automatic backups, robust security, and hassle-free maintenance‚Äîall carefully managed by us, the passionate duo behind HeyForm.&lt;/p&gt; &#xA;&lt;p&gt;Choosing our hosted version not only saves a significant amount of time and resources but also supports HeyForm&#39;s development and the open-source community. Get a great service while backing innovation. üíô&lt;/p&gt; &#xA;&lt;h2&gt;Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;‚îî‚îÄ‚îÄ packages&#xA;    ‚îú‚îÄ‚îÄ answer-utils       (form submission utils for server and webapp)&#xA;    ‚îú‚îÄ‚îÄ shared-types-enums (shared types/enums for server and webapp)&#xA;    ‚îú‚îÄ‚îÄ utils              (common utils for server and webapp)&#xA;    ‚îú‚îÄ‚îÄ server             (node server)&#xA;    ‚îî‚îÄ‚îÄ webapp             (react webapp)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Self-hosting&lt;/h2&gt; &#xA;&lt;p&gt;Interested in self-hosting HeyForm on your server? Take a look at the &lt;a href=&#34;https://docs.heyform.net/self-hosting&#34;&gt;self-hosting installation instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Local development&lt;/h2&gt; &#xA;&lt;p&gt;Follow the &lt;a href=&#34;https://docs.heyform.net/local-development&#34;&gt;local installation instructions&lt;/a&gt; to run the project locally.&lt;/p&gt; &#xA;&lt;h2&gt;How to Contribute&lt;/h2&gt; &#xA;&lt;p&gt;You are awesome, let&#39;s build great software together. Head over to the &lt;a href=&#34;https://docs.heyform.net/contribute&#34;&gt;contribute docs&lt;/a&gt; to get started. üí™&lt;/p&gt; &#xA;&lt;h2&gt;Support &amp;amp; Community&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;ll find a lot of resources to help you get started with HeyForm in the &lt;a href=&#34;https://docs.heyform.net&#34;&gt;help center&lt;/a&gt;. However, if you can&#39;t find what you&#39;re looking for there, don&#39;t hesitate to reach out to us:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Have a question? Join the &lt;a href=&#34;https://discord.gg/sgT4v4GSTe&#34;&gt;Discord server&lt;/a&gt; and get instant help.&lt;/li&gt; &#xA; &lt;li&gt;Found a bug? &lt;a href=&#34;https://github.com/heyform/heyform/issues/new/choose&#34;&gt;Create an issue&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;HeyForm is open-source under the GNU Affero General Public License v3.0 (AGPL-3.0), you will find more information about the license and how to comply with it &lt;a href=&#34;https://docs.heyform.net/license&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>wandb/openui</title>
    <updated>2024-04-02T01:25:19Z</updated>
    <id>tag:github.com,2024-04-02:/wandb/openui</id>
    <link href="https://github.com/wandb/openui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenUI let&#39;s you describe UI using your imagination, then see it rendered live.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenUI&lt;/h1&gt; &#xA;&lt;p&gt;Building UI components can be a slog. OpenUI aims to make the process fun, fast, and flexible. It&#39;s also a tool we&#39;re using at &lt;a href=&#34;https://wandb.com&#34;&gt;W&amp;amp;B&lt;/a&gt; to test and prototype our next generation tooling for building powerful applications on top of LLM&#39;s.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/demo.gif&#34; alt=&#34;Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenUI let&#39;s you describe UI using your imagination, then see it rendered live. You can ask for changes and convert HTML to React, Svelte, Web Components, etc. It&#39;s like &lt;a href=&#34;https://v0.dev&#34;&gt;v0&lt;/a&gt; but open source and not as polished &lt;span&gt;üòù&lt;/span&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Live Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://openui.fly.dev&#34;&gt;Try the demo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;You can also run OpenUI locally and use models available to &lt;a href=&#34;https://olama.com&#34;&gt;Ollama&lt;/a&gt;. &lt;a href=&#34;https://ollama.com/download&#34;&gt;Install Ollama&lt;/a&gt; and pull a model like &lt;a href=&#34;https://ollama.com/library/codellama&#34;&gt;CodeLlama&lt;/a&gt;, then assuming you have git and python installed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/wandb/openui&#xA;cd openui/backend&#xA;# You probably want to do this from a virtual environment&#xA;pip install .&#xA;# This must be set to use OpenAI models, find your api key here: https://platform.openai.com/api-keys&#xA;export OPENAI_API_KEY=xxx&#xA;python -m openui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can build and run the docker file from the &lt;code&gt;/backend&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build . -t wandb/openui --load&#xA;docker run -p 7878:7878 -e OPENAI_API_KEY wandb/openui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can goto &lt;a href=&#34;http://localhost:7878&#34;&gt;http://localhost:7878&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;a href=&#34;https://github.com/wandb/openui/raw/main/.devcontainer/devcontainer.json&#34;&gt;dev container&lt;/a&gt; is configured in this repository which is the quickest way to get started.&lt;/p&gt; &#xA;&lt;h3&gt;Codespace&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/wandb/openui/main/assets/codespace.png&#34; alt=&#34;New with options...&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Choose more options when creating a Codespace, then select &lt;strong&gt;New with options...&lt;/strong&gt;. Select the US West region if you want a really fast boot time. You&#39;ll also want to configure your OPENAI_API_KEY secret or just set it to &lt;code&gt;xxx&lt;/code&gt; if you want to try Ollama &lt;em&gt;(you&#39;ll want at least 16GB of Ram)&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once inside the code space you can run the server in one terminal: &lt;code&gt;python -m openui --dev&lt;/code&gt;. Then in a new terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd /workspaces/openui/frontend&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should open another service on port 5173, that&#39;s the service you&#39;ll want to visit. All changes to both the frontend and backend will automatically be reloaded and reflected in your browser.&lt;/p&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;p&gt;See the readmes in the &lt;a href=&#34;https://raw.githubusercontent.com/wandb/openui/main/frontend/README.md&#34;&gt;frontend&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/wandb/openui/main/backend/README.md&#34;&gt;backend&lt;/a&gt; directories.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>TencentARC/BrushNet</title>
    <updated>2024-04-02T01:25:19Z</updated>
    <id>tag:github.com,2024-04-02:/TencentARC/BrushNet</id>
    <link href="https://github.com/TencentARC/BrushNet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The official implementation of paper &#34;BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BrushNet&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the implementation of the paper &#34;BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion&#34;&lt;/p&gt; &#xA;&lt;p&gt;Keywords: Image Inpainting, Diffusion Models, Image Generation&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/juxuan27&#34;&gt;Xuan Ju&lt;/a&gt;&lt;sup&gt;12&lt;/sup&gt;, &lt;a href=&#34;https://alvinliu0.github.io/&#34;&gt;Xian Liu&lt;/a&gt;&lt;sup&gt;12&lt;/sup&gt;, &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;&lt;sup&gt;1*&lt;/sup&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=HzemVzoAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Yuxuan Bian&lt;/a&gt;&lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://www.linkedin.com/in/YingShanProfile/&#34;&gt;Ying Shan&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://cure-lab.github.io/&#34;&gt;Qiang Xu&lt;/a&gt;&lt;sup&gt;2*&lt;/sup&gt;&lt;br&gt; &lt;sup&gt;1&lt;/sup&gt;ARC Lab, Tencent PCG &lt;sup&gt;2&lt;/sup&gt;The Chinese University of Hong Kong &lt;sup&gt;*&lt;/sup&gt;Corresponding Author&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://tencentarc.github.io/BrushNet/&#34;&gt;üåêProject Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2403.06976&#34;&gt;üìúArxiv&lt;/a&gt; | &lt;a href=&#34;https://forms.gle/9TgMZ8tm49UYsZ9s5&#34;&gt;üóÑÔ∏èData&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/file/d/1IkEBWcd2Fui2WHcckap4QFPcCI0gkHBh/view&#34;&gt;üìπVideo&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/TencentARC/BrushNet&#34;&gt;ü§óHugging Face Demo&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üìñ Table of Contents&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#%EF%B8%8F-method-overview&#34;&gt;üõ†Ô∏è Method Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#-getting-started&#34;&gt;üöÄ Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#environment-requirement-&#34;&gt;Environment Requirement üåç&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#data-download-%EF%B8%8F&#34;&gt;Data Download ‚¨áÔ∏è&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#-running-scripts&#34;&gt;üèÉüèº Running Scripts&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#training-&#34;&gt;Training ü§Ø&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#inference-&#34;&gt;Inference üìú&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#evaluation-&#34;&gt;Evaluation üìè&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#-cite-us&#34;&gt;ü§ùüèº Cite Us&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/#-acknowledgement&#34;&gt;üíñ Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release trainig and inference code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release checkpoint (sdv1.5)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release checkpoint (sdxl)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release evaluation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release gradio demo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Method Overview&lt;/h2&gt; &#xA;&lt;p&gt;BrushNet is a diffusion-based text-guided image inpainting model that can be plug-and-play into any pre-trained diffusion model. Our architectural design incorporates two key insights: (1) dividing the masked image features and noisy latent reduces the model&#39;s learning load, and (2) leveraging dense per-pixel control over the entire pre-trained model enhances its suitability for image inpainting tasks. More analysis can be found in the main paper.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/TencentARC/BrushNet/main/examples/brushnet/src/model.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Requirement üåç&lt;/h3&gt; &#xA;&lt;p&gt;BrushNet has been implemented and tested on Pytorch 1.12.1 with python 3.9.&lt;/p&gt; &#xA;&lt;p&gt;Clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/TencentARC/BrushNet.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend you first use &lt;code&gt;conda&lt;/code&gt; to create virtual environment, and install &lt;code&gt;pytorch&lt;/code&gt; following &lt;a href=&#34;https://pytorch.org/&#34;&gt;official instructions&lt;/a&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n diffusers python=3.9 -y&#xA;conda activate diffusers&#xA;python -m pip install --upgrade pip&#xA;pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can install diffusers (implemented in this repo) with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After that, you can install required packages thourgh:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd examples/brushnet/&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Data Download ‚¨áÔ∏è&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can download the BrushData and BrushBench &lt;a href=&#34;https://forms.gle/9TgMZ8tm49UYsZ9s5&#34;&gt;here&lt;/a&gt; (as well as the EditBench we re-processed), which are used for training and testing the BrushNet. By downloading the data, you are agreeing to the terms and conditions of the license. The data structure should be like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;|-- data&#xA;    |-- BrushData&#xA;        |-- 00200.tar&#xA;        |-- 00201.tar&#xA;        |-- ...&#xA;    |-- BrushDench&#xA;        |-- images&#xA;        |-- mapping_file.json&#xA;    |-- EditBench&#xA;        |-- images&#xA;        |-- mapping_file.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Noted: &lt;em&gt;We only provide a part of the BrushData due to the space limit. Please write an email to &lt;a href=&#34;mailto:juxuan.27@gmail.com&#34;&gt;juxuan.27@gmail.com&lt;/a&gt; if you need the full dataset.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Checkpoints&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checkpoints of BrushNet can be downloaded from &lt;a href=&#34;https://drive.google.com/drive/folders/1fqmS1CEOvXCxNWFrsSYd_jHYXxrydh1n?usp=drive_link&#34;&gt;here&lt;/a&gt;. The ckpt folder contains our pretrained checkpoints and pretrinaed Stable Diffusion checkpoint (e.g., realisticVisionV60B1_v51VAE from &lt;a href=&#34;https://civitai.com/&#34;&gt;Civitai&lt;/a&gt;). You can use &lt;code&gt;scripts/convert_original_stable_diffusion_to_diffusers.py&lt;/code&gt; to process other models downloaded from Civitai. The data structure should be like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;|-- data&#xA;    |-- BrushData&#xA;    |-- BrushDench&#xA;    |-- EditBench&#xA;    |-- ckpt&#xA;        |-- realisticVisionV60B1_v51VAE&#xA;            |-- model_index.json&#xA;            |-- vae&#xA;            |-- ...&#xA;        |-- segmentation_mask_brushnet_ckpt&#xA;        |-- random_mask_brushnet_ckpt&#xA;        |-- ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The checkpoint in &lt;code&gt;segmentation_mask_brushnet_ckpt&lt;/code&gt; provides checkpoints trained on BrushData, which has segmentation prior (mask are with the same shape of objects). The &lt;code&gt;random_mask_brushnet_ckpt&lt;/code&gt; provides a more general ckpt for random mask shape.&lt;/p&gt; &#xA;&lt;h2&gt;üèÉüèº Running Scripts&lt;/h2&gt; &#xA;&lt;h3&gt;Training ü§Ø&lt;/h3&gt; &#xA;&lt;p&gt;You can train with segmentation mask using the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch examples/brushnet/train_brushnet.py \&#xA;--pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \&#xA;--output_dir runs/logs/brushnet_segmentationmask \&#xA;--train_data_dir data/BrushData \&#xA;--resolution 512 \&#xA;--learning_rate 1e-5 \&#xA;--train_batch_size 2 \&#xA;--tracker_project_name brushnet \&#xA;--report_to tensorboard \&#xA;--resume_from_checkpoint latest \&#xA;--validation_steps 300&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use custom dataset, you can process your own data to the format of BrushData and revise &lt;code&gt;--train_data_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can train with random mask using the script (by adding &lt;code&gt;--random_mask&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch examples/brushnet/train_brushnet.py \&#xA;--pretrained_model_name_or_path runwayml/stable-diffusion-v1-5 \&#xA;--output_dir runs/logs/brushnet_randommask \&#xA;--train_data_dir data/BrushData \&#xA;--resolution 512 \&#xA;--learning_rate 1e-5 \&#xA;--train_batch_size 2 \&#xA;--tracker_project_name brushnet \&#xA;--report_to tensorboard \&#xA;--resume_from_checkpoint latest \&#xA;--validation_steps 300 \&#xA;--random_mask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference üìú&lt;/h3&gt; &#xA;&lt;p&gt;You can inference with the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/brushnet/test_brushnet.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Since BrushNet is trained on Laion, it can only guarantee the performance on general scenarios. We recommend you train on your own data (e.g., product exhibition, virtual try-on) if you have high-quality industrial application requirements. We would also be appreciate if you would like to contribute your trained model!&lt;/p&gt; &#xA;&lt;p&gt;You can also inference through gradio demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/brushnet/app_brushnet.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation üìè&lt;/h3&gt; &#xA;&lt;p&gt;You can evaluate using the script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/brushnet/evaluate_brushnet.py \&#xA;--brushnet_ckpt_path data/ckpt/segmentation_mask_brushnet_ckpt \&#xA;--image_save_path runs/evaluation_result/BrushBench/brushnet_segmask/inside \&#xA;--mapping_file data/BrushBench/mapping_file.json \&#xA;--base_dir data/BrushBench \&#xA;--mask_key inpainting_mask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;--mask_key&lt;/code&gt; indicates which kind of mask to use, &lt;code&gt;inpainting_mask&lt;/code&gt; for inside inpainting and &lt;code&gt;outpainting_mask&lt;/code&gt; for outside inpainting. The evaluation results (images and metrics) will be saved in &lt;code&gt;--image_save_path&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Noted that you need to ignore the nsfw detector in &lt;code&gt;src/diffusers/pipelines/brushnet/pipeline_brushnet.py#1261&lt;/code&gt; to get the correct evaluation results. Moreover, we find different machine may generate different images, thus providing the results on our machine &lt;a href=&#34;https://drive.google.com/drive/folders/1dK3oIB2UvswlTtnIS1iHfx4s57MevWdZ?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ùüèº Cite Us&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{ju2024brushnet,&#xA;  title={BrushNet: A Plug-and-Play Image Inpainting Model with Decomposed Dual-Branch Diffusion}, &#xA;  author={Xuan Ju and Xian Liu and Xintao Wang and Yuxuan Bian and Ying Shan and Qiang Xu},&#xA;  year={2024},&#xA;  eprint={2403.06976},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üíñ Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;&lt;span id=&#34;acknowledgement&#34;&gt;&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our code is modified based on &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;, thanks to all the contributors!&lt;/p&gt;</summary>
  </entry>
</feed>