<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-13T01:29:02Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ravenscroftj/turbopilot</title>
    <updated>2023-04-13T01:29:02Z</updated>
    <id>tag:github.com,2023-04-13:/ravenscroftj/turbopilot</id>
    <link href="https://github.com/ravenscroftj/turbopilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Turbopilot is an open source large-language-model based code completion engine that runs locally on CPU&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;TurboPilot üöÄ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ravenscroftj/turbopilot/actions/workflows/cmake.yml&#34;&gt;&lt;img src=&#34;https://github.com/ravenscroftj/turbopilot/actions/workflows/cmake.yml/badge.svg?sanitize=true&#34; alt=&#34;CMake&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://fosstodon.org/@jamesravey&#34;&gt;&lt;img src=&#34;https://img.shields.io/mastodon/follow/000117012?domain=https%3A%2F%2Ffosstodon.org%2F&amp;amp;style=social&#34; alt=&#34;Mastodon Follow&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/ravenscroftj/turbopilot&#34; alt=&#34;BSD Licensed&#34;&gt; &lt;img src=&#34;https://img.shields.io/endpoint?url=https://wakapi.nopro.be/api/compat/shields/v1/jamesravey/all_time/project%3Aturbopilot&#34; alt=&#34;Time Spent&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;TurboPilot is a self-hosted &lt;a href=&#34;https://github.com/features/copilot&#34;&gt;copilot&lt;/a&gt; clone which uses the library behind &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;llama.cpp&lt;/a&gt; to run the &lt;a href=&#34;https://github.com/salesforce/CodeGen&#34;&gt;6 Billion Parameter Salesforce Codegen model&lt;/a&gt; in 4GiB of RAM. It is heavily based and inspired by on the &lt;a href=&#34;https://github.com/fauxpilot/fauxpilot&#34;&gt;fauxpilot&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;NB: This is a proof of concept right now rather than a stable tool. Autocompletion is quite slow in this version of the project. Feel free to play with it, but your mileage may vary.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ravenscroftj/turbopilot/main/assets/screenrecording.gif&#34; alt=&#34;a screen recording of turbopilot running through fauxpilot plugin&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;PRs to this project and the corresponding &lt;a href=&#34;https://github.com/ravenscroftj/ggml&#34;&gt;GGML fork&lt;/a&gt; are very welcome.&lt;/p&gt; &#xA;&lt;p&gt;Make a fork, make your changes and then open a &lt;a href=&#34;https://github.com/ravenscroftj/turbopilot/pulls&#34;&gt;PR&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üëã Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to try the project out is to grab the pre-processed models and then run the server in docker.&lt;/p&gt; &#xA;&lt;h3&gt;Getting The Models&lt;/h3&gt; &#xA;&lt;p&gt;You have 2 options for getting the model&lt;/p&gt; &#xA;&lt;h4&gt;Option A: Direct Download - Easy, Quickstart&lt;/h4&gt; &#xA;&lt;p&gt;You can download the pre-converted, pre-quantized models from &lt;a href=&#34;https://drive.google.com/drive/folders/1wFy1Y0pqoK23ZeMWWCp8evxWOJQVdaGh?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;. I&#39;ve made the &lt;code&gt;multi&lt;/code&gt; flavour models with 2B and 6B parameters available - these models are pre-trained on &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;C++&lt;/code&gt;, &lt;code&gt;Go&lt;/code&gt;, &lt;code&gt;Java&lt;/code&gt;, &lt;code&gt;JavaScript&lt;/code&gt;, and &lt;code&gt;Python&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Option B: Convert The Models Yourself - Hard, More Flexible&lt;/h4&gt; &#xA;&lt;p&gt;Follow &lt;a href=&#34;https://github.com/ravenscroftj/turbopilot/wiki/Converting-and-Quantizing-The-Models&#34;&gt;this guide&lt;/a&gt; if you want to experiment with quantizing the models yourself.&lt;/p&gt; &#xA;&lt;h3&gt;‚öôÔ∏è Running TurboPilot Server&lt;/h3&gt; &#xA;&lt;p&gt;Download the &lt;a href=&#34;https://github.com/ravenscroftj/turbopilot/releases&#34;&gt;latest binary&lt;/a&gt; and extract it to the root project folder. If a binary is not provided for your OS or you&#39;d prefer to build it yourself follow the &lt;a href=&#34;https://raw.githubusercontent.com/ravenscroftj/turbopilot/main/BUILD.md&#34;&gt;build instructions&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./codegen-serve -m ./models/codegen-6B-multi-ggml-4bit-quant.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The application should start a server on port &lt;code&gt;18080&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have a multi-core system you can control how many CPUs are used with the &lt;code&gt;-t&lt;/code&gt; option - for example, on my AMD Ryzen 5000 which has 6 cores/12 threads I use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./codegen-serve -t 6 -m ./models/codegen-6B-multi-ggml-4bit-quant.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üì¶ Running From Docker&lt;/h3&gt; &#xA;&lt;p&gt;You can also run Turbopilot from the pre-built docker image supplied &lt;a href=&#34;https://github.com/users/ravenscroftj/packages/container/package/turbopilot%2Fturbopilot&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You will still need to download the models separately, then you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --rm -it \&#xA;  -v ./models:/models \&#xA;  -e THREADS=6 \&#xA;  -e MODEL=&#34;/models/codegen-2B-multi-ggml-4bit-quant.bin&#34; \&#xA;  -p 18080:18080 \&#xA;  ghcr.io/ravenscroftj/turbopilot/turbopilot:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üåê Using the API&lt;/h3&gt; &#xA;&lt;h4&gt;Using the API with FauxPilot Plugin&lt;/h4&gt; &#xA;&lt;p&gt;To use the API from VSCode, I recommend the &lt;a href=&#34;https://github.com/Venthe/vscode-fauxpilot&#34;&gt;vscode-fauxpilot&lt;/a&gt; plugin. Once you install it, you will need to change a few settings in your settings.json file.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open settings (CTRL/CMD + SHIFT + P) and select &lt;code&gt;Preferences: Open User Settings (JSON)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add the following values:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    ... // other settings&#xA;&#xA;    &#34;fauxpilot.enabled&#34;: true,&#xA;    &#34;fauxpilot.server&#34;: &#34;http://localhost:18080/v1/engines&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can enable fauxpilot with &lt;code&gt;CTRL + SHIFT + P&lt;/code&gt; and select &lt;code&gt;Enable Fauxpilot&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The plugin will send API calls to the running &lt;code&gt;codegen-serve&lt;/code&gt; process when you make a keystroke. It will then wait for each request to complete before sending further requests.&lt;/p&gt; &#xA;&lt;h4&gt;Calling the API Directly&lt;/h4&gt; &#xA;&lt;p&gt;You can make requests to &lt;code&gt;http://localhost:18080/v1/engines/codegen/completions&lt;/code&gt; which will behave just like the same Copilot endpoint.&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl --request POST \&#xA;  --url http://localhost:18080/v1/engines/codegen/completions \&#xA;  --header &#39;Content-Type: application/json&#39; \&#xA;  --data &#39;{&#xA; &#34;model&#34;: &#34;codegen&#34;,&#xA; &#34;prompt&#34;: &#34;def main():&#34;,&#xA; &#34;max_tokens&#34;: 100&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Should get you something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA; &#34;choices&#34;: [&#xA;  {&#xA;   &#34;logprobs&#34;: null,&#xA;   &#34;index&#34;: 0,&#xA;   &#34;finish_reason&#34;: &#34;length&#34;,&#xA;   &#34;text&#34;: &#34;\n  \&#34;\&#34;\&#34;Main entry point for this script.\&#34;\&#34;\&#34;\n  logging.getLogger().setLevel(logging.INFO)\n  logging.basicConfig(format=(&#39;%(levelname)s: %(message)s&#39;))\n\n  parser = argparse.ArgumentParser(\n      description=__doc__,\n      formatter_class=argparse.RawDescriptionHelpFormatter,\n      epilog=__doc__)\n  &#34;&#xA;  }&#xA; ],&#xA; &#34;created&#34;: 1681113078,&#xA; &#34;usage&#34;: {&#xA;  &#34;total_tokens&#34;: 105,&#xA;  &#34;prompt_tokens&#34;: 3,&#xA;  &#34;completion_tokens&#34;: 102&#xA; },&#xA; &#34;object&#34;: &#34;text_completion&#34;,&#xA; &#34;model&#34;: &#34;codegen&#34;,&#xA; &#34;id&#34;: &#34;01d7a11b-f87c-4261-8c03-8c78cbe4b067&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üëâ Known Limitations&lt;/h2&gt; &#xA;&lt;p&gt;Again I want to set expectations around this being a proof-of-concept project. With that in mind. Here are some current known limitations.&lt;/p&gt; &#xA;&lt;p&gt;As of &lt;strong&gt;v0.0.1&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The models can be quite slow - especially the 6B ones. It can take ~30-40s to make suggestions across 4 CPU cores.&lt;/li&gt; &#xA; &lt;li&gt;I&#39;ve only tested the system on Ubuntu 22.04. Your mileage may vary on other operating systems. Please let me know if you try it elsewhere. I&#39;m particularly interested in performance on Apple Silicon.&lt;/li&gt; &#xA; &lt;li&gt;Sometimes suggestions get truncated in nonsensical places - e.g. part way through a variable name or string name. This is due to a hard limit on suggestion length.&lt;/li&gt; &#xA; &lt;li&gt;Sometimes the server will run out of memory and crash. This is because it will try to use everything above your current location as context during generation. I&#39;m working on a fix.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üëè Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This project would not have been possible without &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;Georgi Gerganov&#39;s work on GGML and llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;It was completely inspired by &lt;a href=&#34;https://github.com/fauxpilot/fauxpilot&#34;&gt;fauxpilot&lt;/a&gt; which I did experiment with for a little while but wanted to try to make the models work without a GPU&lt;/li&gt; &#xA; &lt;li&gt;The frontend of the project is powered by &lt;a href=&#34;https://github.com/Venthe/vscode-fauxpilot&#34;&gt;Venthe&#39;s vscode-fauxpilot plugin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The project uses the &lt;a href=&#34;https://github.com/salesforce/CodeGen&#34;&gt;Salesforce Codegen&lt;/a&gt; models.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://huggingface.co/moyix&#34;&gt;Moyix&lt;/a&gt; for his work on converting the Salesforce models to run in a GPT-J architecture. Not only does this &lt;a href=&#34;https://gist.github.com/moyix/7896575befbe1b99162ccfec8d135566&#34;&gt;confer some speed benefits&lt;/a&gt; but it also made it much easier for me to port the models to GGML using the &lt;a href=&#34;https://github.com/ggerganov/ggml/tree/master/examples/gpt-j&#34;&gt;existing gpt-j example code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The model server uses &lt;a href=&#34;https://crowcpp.org/master/&#34;&gt;CrowCPP&lt;/a&gt; to serve suggestions.&lt;/li&gt; &#xA; &lt;li&gt;Check out the &lt;a href=&#34;https://arxiv.org/pdf/2203.13474.pdf&#34;&gt;original scientific paper&lt;/a&gt; for CodeGen for more info.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>aress31/burpgpt</title>
    <updated>2023-04-13T01:29:02Z</updated>
    <id>tag:github.com,2023-04-13:/aress31/burpgpt</id>
    <link href="https://github.com/aress31/burpgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Burp Suite extension that integrates OpenAI&#39;s GPT to perform an additional passive scan for discovering highly bespoke vulnerabilities, and enables running traffic-based analysis of any type.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;burpgpt&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/aress31/burpgpt/actions/workflows/gradle-build.yml&#34;&gt;&lt;img src=&#34;https://github.com/aress31/burpgpt/actions/workflows/gradle-build.yml/badge.svg?sanitize=true&#34; alt=&#34;Java CI with Gradle&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;burpgpt&lt;/code&gt; leverages the power of &lt;code&gt;AI&lt;/code&gt; to detect security vulnerabilities that traditional scanners might miss. It sends web traffic to an &lt;code&gt;OpenAI&lt;/code&gt; &lt;code&gt;model&lt;/code&gt; specified by the user, enabling sophisticated analysis within the passive scanner. This extension offers customisable &lt;code&gt;prompts&lt;/code&gt; that enable tailored web traffic analysis to meet the specific needs of each user. Check out the &lt;a href=&#34;https://raw.githubusercontent.com/aress31/burpgpt/main/#example-use-cases&#34;&gt;Example Use Cases&lt;/a&gt; section for inspiration.&lt;/p&gt; &#xA;&lt;p&gt;The extension generates an automated security report that summarises potential security issues based on the user&#39;s &lt;code&gt;prompt&lt;/code&gt; and real-time data from &lt;code&gt;Burp&lt;/code&gt;-issued requests. By leveraging &lt;code&gt;AI&lt;/code&gt; and natural language processing, the extension streamlines the security assessment process and provides security professionals with a higher-level overview of the scanned application or endpoint. This enables them to more easily identify potential security issues and prioritise their analysis, while also covering a larger potential attack surface.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Data traffic is sent to &lt;code&gt;OpenAI&lt;/code&gt; for analysis. If you have concerns about this or are using the extension for security-critical applications, it is important to carefully consider this and review &lt;a href=&#34;https://openai.com/policies/privacy-policy&#34;&gt;OpenAI&#39;s Privacy Policy&lt;/a&gt; for further information.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] While the report is automated, it still requires triaging and post-processing by security professionals, as it may contain false positives.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] The effectiveness of this extension is heavily reliant on the &lt;a href=&#34;https://raw.githubusercontent.com/aress31/burpgpt/main/#prompt-configuration&#34;&gt;quality and precision of the prompts&lt;/a&gt; created by the user for the selected &lt;code&gt;GPT&lt;/code&gt; model. This targeted approach will help ensure the &lt;code&gt;GPT model&lt;/code&gt; generates accurate and valuable results for your security analysis.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adds a &lt;code&gt;passive scan check&lt;/code&gt;, allowing users to submit &lt;code&gt;HTTP&lt;/code&gt; data to an &lt;code&gt;OpenAI&lt;/code&gt;-controlled &lt;code&gt;GPT model&lt;/code&gt; for analysis through a &lt;code&gt;placeholder&lt;/code&gt; system.&lt;/li&gt; &#xA; &lt;li&gt;Leverages the power of &lt;code&gt;OpenAI&#39;s GPT models&lt;/code&gt; to conduct comprehensive traffic analysis, enabling detection of various issues beyond just security vulnerabilities in scanned applications.&lt;/li&gt; &#xA; &lt;li&gt;Enables granular control over the number of &lt;code&gt;GPT tokens&lt;/code&gt; used in the analysis by allowing for precise adjustments of the &lt;code&gt;maximum prompt length&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Offers users multiple &lt;code&gt;OpenAI models&lt;/code&gt; to choose from, allowing them to select the one that best suits their needs.&lt;/li&gt; &#xA; &lt;li&gt;Empowers users to customise &lt;code&gt;prompts&lt;/code&gt; and unleash limitless possibilities for interacting with &lt;code&gt;OpenAI models&lt;/code&gt;. Browse through the &lt;a href=&#34;https://raw.githubusercontent.com/aress31/burpgpt/main/#example-use-cases&#34;&gt;Example Use Cases&lt;/a&gt; for inspiration.&lt;/li&gt; &#xA; &lt;li&gt;Integrates with &lt;code&gt;Burp Suite&lt;/code&gt;, providing all native features for pre- and post-processing, including displaying analysis results directly within the Burp UI for efficient analysis.&lt;/li&gt; &#xA; &lt;li&gt;Provides troubleshooting functionality via the native &lt;code&gt;Burp Event Log&lt;/code&gt;, enabling users to quickly resolve communication issues with the &lt;code&gt;OpenAI API&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;System requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Operating System: Compatible with &lt;code&gt;Linux&lt;/code&gt;, &lt;code&gt;macOS&lt;/code&gt;, and &lt;code&gt;Windows&lt;/code&gt; operating systems.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Java Development Kit (JDK): &lt;code&gt;Version 11&lt;/code&gt; or later.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Burp Suite Professional or Community Edition: &lt;code&gt;Version 2023.3.2&lt;/code&gt; or later.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;[!IMPORTANT] Please note that using any version lower than &lt;code&gt;2023.3.2&lt;/code&gt; may result in a &lt;a href=&#34;https://forum.portswigger.net/thread/montoya-api-nosuchmethoderror-275048be&#34;&gt;java.lang.NoSuchMethodError&lt;/a&gt;. It is crucial to use the specified version or a more recent one to avoid this issue.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Build tool:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gradle: &lt;code&gt;Version 6.9&lt;/code&gt; or later (recommended). The &lt;a href=&#34;https://github.com/aress31/burpgpt/raw/main/lib/build.gradle&#34;&gt;build.gradle&lt;/a&gt; file is provided in the project repository.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Environment variables:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Set up the &lt;code&gt;JAVA_HOME&lt;/code&gt; environment variable to point to the JDK installation directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please ensure that all system requirements, including a compatible version of &lt;code&gt;Burp Suite&lt;/code&gt;, are met before building and running the project. Note that the project&#39;s external dependencies will be automatically managed and installed by &lt;code&gt;Gradle&lt;/code&gt; during the build process. Adhering to the requirements will help avoid potential issues and reduce the need for opening new issues in the project repository.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;1. Compilation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you have &lt;a href=&#34;https://gradle.org/&#34;&gt;Gradle&lt;/a&gt; installed and configured.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the &lt;code&gt;burpgpt&lt;/code&gt; repository:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/aress31/burpgpt&#xA;cd .\burpgpt\&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the standalone &lt;code&gt;jar&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./gradlew shadowJar&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2. Loading the Extension Into &lt;code&gt;Burp Suite&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;To install &lt;code&gt;burpgpt&lt;/code&gt; in &lt;code&gt;Burp Suite&lt;/code&gt;, first go to the &lt;code&gt;Extensions&lt;/code&gt; tab and click on the &lt;code&gt;Add&lt;/code&gt; button. Then, select the &lt;code&gt;burpgpt-all&lt;/code&gt; jar file located in the &lt;code&gt;.\lib\build\libs&lt;/code&gt; folder to load the extension.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;To start using burpgpt, users need to complete the following steps in the Settings panel, which can be accessed from the Burp Suite menu bar:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Enter a valid &lt;code&gt;OpenAI API key&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Select a &lt;code&gt;model&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Define the &lt;code&gt;max prompt size&lt;/code&gt;. This field controls the maximum &lt;code&gt;prompt&lt;/code&gt; length sent to &lt;code&gt;OpenAI&lt;/code&gt; to avoid exceeding the &lt;code&gt;maxTokens&lt;/code&gt; of &lt;code&gt;GPT&lt;/code&gt; models (typically around &lt;code&gt;2048&lt;/code&gt; for &lt;code&gt;GPT-3&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Adjust or create custom prompts according to your requirements.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/11601622/230922492-6434ff25-0f2e-4435-8f4d-b3dd6b7ac9c6.png&#34; alt=&#34;burpgpt UI&#34; width=&#34;75%&#34; height=&#34;75%&#34;&gt; &#xA;&lt;p&gt;Once configured as outlined above, the &lt;code&gt;Burp passive scanner&lt;/code&gt; sends each request to the chosen &lt;code&gt;OpenAI model&lt;/code&gt; via the &lt;code&gt;OpenAI API&lt;/code&gt; for analysis, producing &lt;code&gt;Informational&lt;/code&gt;-level severity findings based on the results.&lt;/p&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/11601622/230796361-2907580f-1993-4cf0-8ac7-f6bae448499d.png&#34; alt=&#34;burpgpt finding&#34; width=&#34;75%&#34; height=&#34;75%&#34;&gt; &#xA;&lt;h2&gt;Prompt Configuration&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;burpgpt&lt;/code&gt; enables users to tailor the &lt;code&gt;prompt&lt;/code&gt; for traffic analysis using a &lt;code&gt;placeholder&lt;/code&gt; system. To include relevant information, we recommend using these &lt;code&gt;placeholders&lt;/code&gt;, which the extension handles directly, allowing dynamic insertion of specific values into the &lt;code&gt;prompt&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Placeholder&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;{REQUEST}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The scanned request.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;{URL}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The URL of the scanned request.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;{METHOD}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The HTTP request method used in the scanned request.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;{REQUEST_HEADERS}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The headers of the scanned request.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;{REQUEST_BODY}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The body of the scanned request.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;{RESPONSE}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The scanned response.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;{RESPONSE_HEADERS}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The headers of the scanned response.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;{RESPONSE_BODY}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The body of the scanned response.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;{IS_TRUNCATED_PROMPT}&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A &lt;code&gt;boolean&lt;/code&gt; value that is programmatically set to &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; to indicate whether the &lt;code&gt;prompt&lt;/code&gt; was truncated to the &lt;code&gt;Maximum Prompt Size&lt;/code&gt; defined in the &lt;code&gt;Settings&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;These &lt;code&gt;placeholders&lt;/code&gt; can be used in the custom &lt;code&gt;prompt&lt;/code&gt; to dynamically generate a request/response analysis &lt;code&gt;prompt&lt;/code&gt; that is specific to the scanned request.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] &amp;gt; &lt;code&gt;Burp Suite&lt;/code&gt; provides the capability to support arbitrary &lt;code&gt;placeholders&lt;/code&gt; through the use of &lt;a href=&#34;https://portswigger.net/support/configuring-burp-suites-session-handling-rules&#34;&gt;Session handling rules&lt;/a&gt; or extensions such as &lt;a href=&#34;https://portswigger.net/bappstore/a0c0cd68ab7c4928b3bf0a9ad48ec8c7&#34;&gt;Custom Parameter Handler&lt;/a&gt;, allowing for even greater customisation of the &lt;code&gt;prompts&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Example Use Cases&lt;/h2&gt; &#xA;&lt;p&gt;The following list of example use cases showcases the bespoke and highly customisable nature of &lt;code&gt;burpgpt&lt;/code&gt;, which enables users to tailor their web traffic analysis to meet their specific needs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Identifying potential vulnerabilities in web applications that use a crypto library affected by a specific CVE:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Analyse the request and response data for potential security vulnerabilities related to the {CRYPTO_LIBRARY_NAME} crypto library affected by CVE-{CVE_NUMBER}:&#xA;&#xA;Web Application URL: {URL}&#xA;Crypto Library Name: {CRYPTO_LIBRARY_NAME}&#xA;CVE Number: CVE-{CVE_NUMBER}&#xA;Request Headers: {REQUEST_HEADERS}&#xA;Response Headers: {RESPONSE_HEADERS}&#xA;Request Body: {REQUEST_BODY}&#xA;Response Body: {RESPONSE_BODY}&#xA;&#xA;Identify any potential vulnerabilities related to the {CRYPTO_LIBRARY_NAME} crypto library affected by CVE-{CVE_NUMBER} in the request and response data and report them.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Scanning for vulnerabilities in web applications that use biometric authentication by analysing request and response data related to the authentication process:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Analyse the request and response data for potential security vulnerabilities related to the biometric authentication process:&#xA;&#xA;Web Application URL: {URL}&#xA;Biometric Authentication Request Headers: {REQUEST_HEADERS}&#xA;Biometric Authentication Response Headers: {RESPONSE_HEADERS}&#xA;Biometric Authentication Request Body: {REQUEST_BODY}&#xA;Biometric Authentication Response Body: {RESPONSE_BODY}&#xA;&#xA;Identify any potential vulnerabilities related to the biometric authentication process in the request and response data and report them.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Analysing the request and response data exchanged between serverless functions for potential security vulnerabilities:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Analyse the request and response data exchanged between serverless functions for potential security vulnerabilities:&#xA;&#xA;Serverless Function A URL: {URL}&#xA;Serverless Function B URL: {URL}&#xA;Serverless Function A Request Headers: {REQUEST_HEADERS}&#xA;Serverless Function B Response Headers: {RESPONSE_HEADERS}&#xA;Serverless Function A Request Body: {REQUEST_BODY}&#xA;Serverless Function B Response Body: {RESPONSE_BODY}&#xA;&#xA;Identify any potential vulnerabilities in the data exchanged between the two serverless functions and report them.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Analysing the request and response data for potential security vulnerabilities specific to a Single-Page Application (SPA) framework:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Analyse the request and response data for potential security vulnerabilities specific to the {SPA_FRAMEWORK_NAME} SPA framework:&#xA;&#xA;Web Application URL: {URL}&#xA;SPA Framework Name: {SPA_FRAMEWORK_NAME}&#xA;Request Headers: {REQUEST_HEADERS}&#xA;Response Headers: {RESPONSE_HEADERS}&#xA;Request Body: {REQUEST_BODY}&#xA;Response Body: {RESPONSE_BODY}&#xA;&#xA;Identify any potential vulnerabilities related to the {SPA_FRAMEWORK_NAME} SPA framework in the request and response data and report them.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add a new field to the &lt;code&gt;Settings&lt;/code&gt; panel that allows users to set the &lt;code&gt;maxTokens&lt;/code&gt; limit for requests, thereby limiting the request size.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add support for connecting to a local instance of the &lt;code&gt;AI model&lt;/code&gt;, allowing users to run and interact with the model on their local machines, potentially improving response times and &lt;strong&gt;data privacy&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Retrieve the precise &lt;code&gt;maxTokens&lt;/code&gt; value for each &lt;code&gt;model&lt;/code&gt; to transmit the maximum allowable data and obtain the most extensive &lt;code&gt;GPT&lt;/code&gt; response possible.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement persistent configuration storage to preserve settings across &lt;code&gt;Burp Suite&lt;/code&gt; restarts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enhance the code for accurate parsing of &lt;code&gt;GPT&lt;/code&gt; responses into the &lt;code&gt;Vulnerability model&lt;/code&gt; for improved reporting.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project Information&lt;/h2&gt; &#xA;&lt;p&gt;The extension is currently under development and we welcome feedback, comments, and contributions to make it even better.&lt;/p&gt; &#xA;&lt;h2&gt;Sponsor üíñ&lt;/h2&gt; &#xA;&lt;p&gt;If this extension has saved you time and hassle during a security assessment, consider showing some love by sponsoring a cup of coffee ‚òï for the developer. It&#39;s the fuel that powers development, after all. Just hit that shiny Sponsor button at the top of the page or &lt;a href=&#34;https://github.com/sponsors/aress31&#34;&gt;click here&lt;/a&gt; to contribute and keep the caffeine flowing. üí∏&lt;/p&gt; &#xA;&lt;h2&gt;Reporting Issues&lt;/h2&gt; &#xA;&lt;p&gt;Did you find a bug? Well, don&#39;t just let it crawl around! Let&#39;s squash it together like a couple of bug whisperers! üêõüí™&lt;/p&gt; &#xA;&lt;p&gt;Please report any issues on the &lt;a href=&#34;https://github.com/aress31/burpgpt/issues&#34;&gt;GitHub issues tracker&lt;/a&gt;. Together, we&#39;ll make this extension as reliable as a cockroach surviving a nuclear apocalypse! üöÄ&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Looking to make a splash with your mad coding skills? üíª&lt;/p&gt; &#xA;&lt;p&gt;Awesome! Contributions are welcome and greatly appreciated. Please submit all PRs on the &lt;a href=&#34;https://github.com/aress31/burpgpt/pulls&#34;&gt;GitHub pull requests tracker&lt;/a&gt;. Together we can make this extension even more amazing! üöÄ&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/aress31/burpgpt/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>reworkd/AgentGPT</title>
    <updated>2023-04-13T01:29:02Z</updated>
    <id>tag:github.com,2023-04-13:/reworkd/AgentGPT</id>
    <link href="https://github.com/reworkd/AgentGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ü§ñ Assemble, configure, and deploy autonomous AI Agents in your browser.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/reworkd/AgentGPT/main/public/banner.png?token=GHSAT0AAAAAAB7JND3U3VGGF3UYYHGYO4RAZBSDJAQ&#34; height=&#34;300&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;em&gt;ü§ñ Assemble, configure, and deploy autonomous AI Agents in your browser. ü§ñ &lt;/em&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Node version&#34; src=&#34;https://img.shields.io/static/v1?label=node&amp;amp;message=%20%3E=16.0.0&amp;amp;logo=node.js&amp;amp;color=2334D058&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://agentgpt.reworkd.ai&#34;&gt;üîó Short link&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href=&#34;https://raw.githubusercontent.com/reworkd/AgentGPT/main/#-getting-started&#34;&gt;ü§ù Contribute&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href=&#34;https://twitter.com/asimdotshrestha/status/1644883727707959296&#34;&gt;üê¶ Twitter&lt;/a&gt; &lt;span&gt;&amp;nbsp;&amp;nbsp;‚Ä¢&amp;nbsp;&amp;nbsp;&lt;/span&gt; &lt;a href=&#34;https://discord.gg/3PccggEG&#34;&gt;üì¢ Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; üíù Support the Advancement of AgentGPT!! üíù &lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Join us in fueling the development of AgentGPT, an open-source project pushing the boundaries of AI autonomy! We&#39;re facing challenges in covering the operational costs üí∏, including in-house API and other infrastructure expenses, which is projected to grow to around $150 USD per day üí≥ü§ï Your sponsorship would drive progress by helping us scale up resources, enhance features and functionality, and continue to iterate on this exciting project! üöÄ &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; By sponsoring this free, open-source project, you not only have the opportunity to have your avatar/logo featured below, but also get the exclusive chance to chat with the founders!üó£Ô∏è &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/sponsors/reworkd-admin&#34;&gt;üëâ Click here&lt;/a&gt; to support the project &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;AgentGPT allows you to configure and deploy Autonomous AI agents. Name your own custom AI and have it embark on any goal imaginable. It will attempt to reach the goal by thinking of tasks to do, executing them, and learning from the results üöÄ.&lt;/p&gt; &#xA;&lt;h2&gt;üéâ Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;This platform is currently in beta, we are currently working on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Long term memory via a vector DB üß†&lt;/li&gt; &#xA; &lt;li&gt;Web browsing capabilities via langchain üåê&lt;/li&gt; &#xA; &lt;li&gt;Interaction with websites and people üë®‚Äçüë©‚Äçüë¶&lt;/li&gt; &#xA; &lt;li&gt;Writing capabilities via a document API üìÑ&lt;/li&gt; &#xA; &lt;li&gt;Saving agent runs üíæ&lt;/li&gt; &#xA; &lt;li&gt;Users and authentication üîê&lt;/li&gt; &#xA; &lt;li&gt;Stripe integration for a lower limit paid version (So we can stop worrying about infra costs) üíµ&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More Coming soon...&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Tech Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;Bootstrapping&lt;/strong&gt;: &lt;a href=&#34;https://create.t3.gg&#34;&gt;create-t3-app&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;Framework&lt;/strong&gt;: &lt;a href=&#34;https://nextjs.org/&#34;&gt;Nextjs 13 + Typescript&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;Auth&lt;/strong&gt;: &lt;a href=&#34;https://next-auth.js.org&#34;&gt;Next-Auth.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;ORM&lt;/strong&gt;: &lt;a href=&#34;https://prisma.io&#34;&gt;Prisma&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;Database&lt;/strong&gt;: &lt;a href=&#34;https://supabase.com/&#34;&gt;Supabase&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;Styling&lt;/strong&gt;: &lt;a href=&#34;https://tailwindcss.com&#34;&gt;TailwindCSS + HeadlessUI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;Typescript Schema Validation&lt;/strong&gt;: &lt;a href=&#34;https://github.com/colinhacks/zod&#34;&gt;Zod&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;End-to-end typesafe API&lt;/strong&gt;: &lt;a href=&#34;https://trpc.io/&#34;&gt;tRPC&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üë®‚ÄçüöÄ Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;üê≥ Docker Setup&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to run AgentGPT locally is by using docker. A convenient setup script is provided to help you get started.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./setup.sh --docker&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üë∑ Local Development Setup&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to develop AgentGPT locally, the easiest way is to use the provided setup script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./setup.sh --local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;üõ†Ô∏è Manual Setup&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üöß You will need &lt;a href=&#34;https://nodejs.org/en/&#34;&gt;Nodejs +18 (LTS recommended)&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork this project:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/reworkd/AgentGPT/fork&#34;&gt;Click here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Clone the repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone git@github.com:YOU_USER/AgentGPT.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install dependencies:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd AgentGPT&#xA;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Create a &lt;strong&gt;.env&lt;/strong&gt; file with the following content:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üöß The environment variables must match the following &lt;a href=&#34;https://github.com/reworkd/AgentGPT/raw/main/src/env/schema.mjs&#34;&gt;schema&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Deployment Environment:&#xA;NODE_ENV=development&#xA;&#xA;# Next Auth config:&#xA;# Generate a secret with `openssl rand -base64 32`&#xA;NEXTAUTH_SECRET=changeme&#xA;NEXTAUTH_URL=http://localhost:3000&#xA;DATABASE_URL=file:./db.sqlite&#xA;&#xA;# Your open api key&#xA;OPENAI_API_KEY=changeme&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Modify prisma schema to use sqlite:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./prisma/useSqlite.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This only needs to be done if you wish to use sqlite.&lt;/p&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Ready ü•≥, now run:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Create database migrations&#xA;npx prisma db push&#xA;&#xA;# Run the project:&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>