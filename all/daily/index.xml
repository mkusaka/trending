<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-29T01:28:48Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>meta-llama/llama-recipes</title>
    <updated>2024-10-29T01:28:48Z</updated>
    <id>tag:github.com,2024-10-29:/meta-llama/llama-recipes</id>
    <link href="https://github.com/meta-llama/llama-recipes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Scripts for fine-tuning Meta Llama with composable FSDP &amp; PEFT methods to cover single/multi-node GPUs. Supports default &amp; custom datasets for applications such as summarization and Q&amp;A. Supporting a number of candid inference solutions such as HF TGI, VLLM for local or cloud deployment. Demo apps to showcase Meta Llama for WhatsApp &amp; Messenger.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Llama Recipes: Examples to get started using the Llama models from Meta&lt;/h1&gt; &#xA;&lt;!-- markdown-link-check-disable --&gt; &#xA;&lt;p&gt;The &#39;llama-recipes&#39; repository is a companion to the &lt;a href=&#34;https://github.com/meta-llama/llama-models&#34;&gt;Meta Llama&lt;/a&gt; models. We support the latest version, &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama3_2/MODEL_CARD_VISION.md&#34;&gt;Llama 3.2 Vision&lt;/a&gt; and &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama3_2/MODEL_CARD.md&#34;&gt;Llama 3.2 Text&lt;/a&gt;, in this repository. This repository contains example scripts and notebooks to get started with the models in a variety of use-cases, including fine-tuning for domain adaptation and building LLM-based applications with Llama and other tools in the LLM ecosystem. The examples here use Llama locally, in the cloud, and on-prem.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Get started with Llama 3.2 with these new recipes:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/meta-llama/llama-recipes/raw/main/recipes/quickstart/finetuning/finetune_vision_model.md&#34;&gt;Finetune Llama 3.2 Vision&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/meta-llama/llama-recipes/raw/main/recipes/quickstart/inference/local_inference/README.md#multimodal-inference&#34;&gt;Multimodal Inference with Llama 3.2 Vision&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/meta-llama/llama-recipes/raw/main/recipes/responsible_ai/llama_guard/llama_guard_text_and_vision_inference.ipynb&#34;&gt;Inference on Llama Guard 1B + Multimodal inference on Llama Guard 11B-Vision&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;!-- markdown-link-check-enable --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Llama 3.2 follows the same prompt template as Llama 3.1, with a new special token &lt;code&gt;&amp;lt;|image|&amp;gt;&lt;/code&gt; representing the input image for the multimodal models.&lt;/p&gt; &#xA; &lt;p&gt;More details on the prompt templates for image reasoning, tool-calling and code interpreter can be found &lt;a href=&#34;https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_2&#34;&gt;on the documentation website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#llama-recipes-examples-to-get-started-using-the-llama-models-from-meta&#34;&gt;Llama Recipes: Examples to get started using the Llama models from Meta&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#table-of-contents&#34;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#getting-started&#34;&gt;Getting Started&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#pytorch-nightlies&#34;&gt;PyTorch Nightlies&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#installing&#34;&gt;Installing&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#install-with-pip&#34;&gt;Install with pip&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#install-with-optional-dependencies&#34;&gt;Install with optional dependencies&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#install-from-source&#34;&gt;Install from source&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#getting-the-llama-models&#34;&gt;Getting the Llama models&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#model-conversion-to-hugging-face&#34;&gt;Model conversion to Hugging Face&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#repository-organization&#34;&gt;Repository Organization&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#recipes&#34;&gt;&lt;code&gt;recipes/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#src&#34;&gt;&lt;code&gt;src/&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#supported-features&#34;&gt;Supported Features&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See deployment for notes on how to deploy the project on a live system.&lt;/p&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;h4&gt;PyTorch Nightlies&lt;/h4&gt; &#xA;&lt;p&gt;If you want to use PyTorch nightlies instead of the stable release, go to &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;this guide&lt;/a&gt; to retrieve the right &lt;code&gt;--extra-index-url URL&lt;/code&gt; parameter for the &lt;code&gt;pip install&lt;/code&gt; commands on your platform.&lt;/p&gt; &#xA;&lt;h3&gt;Installing&lt;/h3&gt; &#xA;&lt;p&gt;Llama-recipes provides a pip distribution for easy install and usage in other projects. Alternatively, it can be installed from source.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Ensure you use the correct CUDA version (from &lt;code&gt;nvidia-smi&lt;/code&gt;) when installing the PyTorch wheels. Here we are using 11.8 as &lt;code&gt;cu118&lt;/code&gt;. H100 GPUs work better with CUDA &amp;gt;12.0&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Install with pip&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama-recipes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Install with optional dependencies&lt;/h4&gt; &#xA;&lt;p&gt;Llama-recipes offers the installation of optional packages. There are three optional dependency groups. To run the unit tests we can install the required dependencies with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama-recipes[tests]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For the vLLM example we need additional requirements that can be installed with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama-recipes[vllm]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use the sensitive topics safety checker install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama-recipes[auditnlg]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some recipes require the presence of langchain. To install the packages follow the recipe description or install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install llama-recipes[langchain]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optional dependencies can also be combined with [option1,option2].&lt;/p&gt; &#xA;&lt;h4&gt;Install from source&lt;/h4&gt; &#xA;&lt;p&gt;To install from source e.g. for development use these commands. We&#39;re using hatchling as our build backend which requires an up-to-date pip as well as setuptools package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:meta-llama/llama-recipes.git&#xA;cd llama-recipes&#xA;pip install -U pip setuptools&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For development and contributing to llama-recipes please install all optional dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:meta-llama/llama-recipes.git&#xA;cd llama-recipes&#xA;pip install -U pip setuptools&#xA;pip install -e .[tests,auditnlg,vllm]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Getting the Llama models&lt;/h3&gt; &#xA;&lt;p&gt;You can find Llama models on Hugging Face hub &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;here&lt;/a&gt;, &lt;strong&gt;where models with &lt;code&gt;hf&lt;/code&gt; in the name are already converted to Hugging Face checkpoints so no further conversion is needed&lt;/strong&gt;. The conversion step below is only for original model weights from Meta that are hosted on Hugging Face model hub as well.&lt;/p&gt; &#xA;&lt;h4&gt;Model conversion to Hugging Face&lt;/h4&gt; &#xA;&lt;p&gt;If you have the model checkpoints downloaded from the Meta website, you can convert it to the Hugging Face format with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## Install Hugging Face Transformers from source&#xA;pip freeze | grep transformers ## verify it is version 4.45.0 or higher&#xA;&#xA;git clone git@github.com:huggingface/transformers.git&#xA;cd transformers&#xA;pip install protobuf&#xA;python src/transformers/models/llama/convert_llama_weights_to_hf.py \&#xA;   --input_dir /path/to/downloaded/llama/weights --model_size 3B --output_dir /output/path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Repository Organization&lt;/h2&gt; &#xA;&lt;p&gt;Most of the code dealing with Llama usage is organized across 2 main folders: &lt;code&gt;recipes/&lt;/code&gt; and &lt;code&gt;src/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;recipes/&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Contains examples organized in folders by topic:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Subfolder&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/recipes/quickstart&#34;&gt;quickstart&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The &#34;Hello World&#34; of using Llama, start here if you are new to using Llama.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/recipes/use_cases&#34;&gt;use_cases&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Scripts showing common applications of Meta Llama3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/recipes/3p_integrations&#34;&gt;3p_integrations&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Partner owned folder showing common applications of Meta Llama3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/recipes/responsible_ai&#34;&gt;responsible_ai&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Scripts to use PurpleLlama for safeguarding model outputs&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/recipes/experimental&#34;&gt;experimental&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Meta Llama implementations of experimental LLM techniques&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;&lt;code&gt;src/&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Contains modules which support the example recipes:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Subfolder&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/src/llama_recipes/configs/&#34;&gt;configs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Contains the configuration files for PEFT methods, FSDP, Datasets, Weights &amp;amp; Biases experiment tracking.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/src/llama_recipes/datasets/&#34;&gt;datasets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Contains individual scripts for each dataset to download and process. Note&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/src/llama_recipes/inference/&#34;&gt;inference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Includes modules for inference for the fine-tuned models.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/src/llama_recipes/model_checkpointing/&#34;&gt;model_checkpointing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Contains FSDP checkpoint handlers.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/src/llama_recipes/policies/&#34;&gt;policies&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Contains FSDP scripts to provide different policies, such as mixed precision, transformer wrapping policy and activation checkpointing along with any precision optimizer (used for running FSDP with pure bf16 mode).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/src/llama_recipes/utils/&#34;&gt;utils&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Utility files for:&lt;br&gt; - &lt;code&gt;train_utils.py&lt;/code&gt; provides training/eval loop and more train utils.&lt;br&gt; - &lt;code&gt;dataset_utils.py&lt;/code&gt; to get preprocessed datasets.&lt;br&gt; - &lt;code&gt;config_utils.py&lt;/code&gt; to override the configs received from CLI.&lt;br&gt; - &lt;code&gt;fsdp_utils.py&lt;/code&gt; provides FSDP wrapping policy for PEFT methods.&lt;br&gt; - &lt;code&gt;memory_utils.py&lt;/code&gt; context manager to track different memory stats in train loop.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Features&lt;/h2&gt; &#xA;&lt;p&gt;The recipes and modules in this repository support the following features:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Feature&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HF support for inference&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HF support for finetuning&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PEFT&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Deferred initialization ( meta init)&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Low CPU mode for multi GPU&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixed precision&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Single node quantization&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Flash attention&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Activation checkpointing FSDP&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hybrid Sharded Data Parallel (HSDP)&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dataset packing &amp;amp; padding&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;BF16 Optimizer (Pure BF16)&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Profiling &amp;amp; MFU tracking&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gradient accumulation&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU offloading&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FSDP checkpoint conversion to HF for inference&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;W&amp;amp;B experiment tracker&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-recipes/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;!-- markdown-link-check-disable --&gt; &#xA;&lt;p&gt;See the License file for Meta Llama 3.2 &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama3_2/LICENSE&#34;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama3_2/USE_POLICY.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the License file for Meta Llama 3.1 &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama3_1/LICENSE&#34;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama3_1/USE_POLICY.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the License file for Meta Llama 3 &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama3/LICENSE&#34;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama3/USE_POLICY.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See the License file for Meta Llama 2 &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama2/LICENSE&#34;&gt;here&lt;/a&gt; and Acceptable Use Policy &lt;a href=&#34;https://github.com/meta-llama/llama-models/raw/main/models/llama2/USE_POLICY.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- markdown-link-check-enable --&gt;</summary>
  </entry>
  <entry>
    <title>ai16z/eliza</title>
    <updated>2024-10-29T01:28:48Z</updated>
    <id>tag:github.com,2024-10-29:/ai16z/eliza</id>
    <link href="https://github.com/ai16z/eliza" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Conversational Agent for Twitter and Discord&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Eliza&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/ai16z/eliza/main/docs/eliza_banner.png&#34; alt=&#34;Eliza Banner&#34; width=&#34;100%&#34;&gt; &#xA;&lt;p&gt;&lt;em&gt;As seen powering &lt;a href=&#34;https://x.com/degenspartanai&#34;&gt;@DegenSpartanAI&lt;/a&gt; and &lt;a href=&#34;https://x.com/pmairca&#34;&gt;@MarcAIndreessen&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Multi-agent simulation framework&lt;/li&gt; &#xA; &lt;li&gt;Add as many unique characters as you want with &lt;a href=&#34;https://github.com/lalalune/characterfile/&#34;&gt;characterfile&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Full-featured Discord and Twitter connectors, with Discord voice channel support&lt;/li&gt; &#xA; &lt;li&gt;Full conversational and document RAG memory&lt;/li&gt; &#xA; &lt;li&gt;Can read links and PDFs, transcribe audio and videos, summarize conversations, and more&lt;/li&gt; &#xA; &lt;li&gt;Highly extensible - create your own actions and clients to extend Eliza&#39;s capabilities&lt;/li&gt; &#xA; &lt;li&gt;Supports open source and local models (default configured with Nous Hermes Llama 3.1B)&lt;/li&gt; &#xA; &lt;li&gt;Supports OpenAI for cloud inference on a light-weight device&lt;/li&gt; &#xA; &lt;li&gt;&#34;Ask Claude&#34; mode for calling Claude on more complex queries&lt;/li&gt; &#xA; &lt;li&gt;100% Typescript&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Install Node.js&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.npmjs.com/downloading-and-installing-node-js-and-npm&#34;&gt;https://docs.npmjs.com/downloading-and-installing-node-js-and-npm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Edit the .env file&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Copy .env.example to .env and fill in the appropriate values&lt;/li&gt; &#xA; &lt;li&gt;Edit the TWITTER environment variables to add your bot&#39;s username and password&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Edit the character file&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Check out the file &lt;code&gt;src/core/defaultCharacter.ts&lt;/code&gt; - you can modify this&lt;/li&gt; &#xA; &lt;li&gt;You can also load characters with the &lt;code&gt;node --loader ts-node/esm src/index.ts --characters=&#34;path/to/your/character.json&#34;&lt;/code&gt; and run multiple bots at the same time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Linux Installation&lt;/h3&gt; &#xA;&lt;p&gt;You might need these&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install --include=optional sharp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run with Llama&lt;/h3&gt; &#xA;&lt;p&gt;You can run Llama 70B or 405B models by setting the &lt;code&gt;XAI_MODEL&lt;/code&gt; environment variable to &lt;code&gt;meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo&lt;/code&gt; or &lt;code&gt;meta-llama/Meta-Llama-3.1-405B-Instruct&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run with Grok&lt;/h3&gt; &#xA;&lt;p&gt;You can run Grok models by setting the &lt;code&gt;XAI_MODEL&lt;/code&gt; environment variable to &lt;code&gt;grok-beta&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run with OpenAI&lt;/h3&gt; &#xA;&lt;p&gt;You can run OpenAI models by setting the &lt;code&gt;XAI_MODEL&lt;/code&gt; environment variable to &lt;code&gt;gpt-4o-mini&lt;/code&gt; or &lt;code&gt;gpt-4o&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Requires Node 20+&lt;/h1&gt; &#xA;&lt;p&gt;If you are getting strange issues when starting up, make sure you&#39;re using Node 20+. Some APIs are not compatible with previous versions. You can check your node version with &lt;code&gt;node -v&lt;/code&gt;. If you need to install a new version of node, we recommend using &lt;a href=&#34;https://github.com/nvm-sh/nvm&#34;&gt;nvm&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Additional Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You may need to install Sharp. If you see an error when starting up, try installing it with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install --include=optional sharp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Environment Setup&lt;/h1&gt; &#xA;&lt;p&gt;You will need to add environment variables to your .env file to connect to various platforms:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Required environment variables&#xA;# Start Discord&#xA;DISCORD_APPLICATION_ID=&#xA;DISCORD_API_TOKEN= # Bot token&#xA;&#xA;# Start Twitter&#xA;TWITTER_USERNAME= # Account username&#xA;TWITTER_PASSWORD= # Account password&#xA;TWITTER_EMAIL= # Account email&#xA;TWITTER_COOKIES= # Account cookies&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Local Setup&lt;/h1&gt; &#xA;&lt;h2&gt;CUDA Setup&lt;/h2&gt; &#xA;&lt;p&gt;If you have an NVIDIA GPU, you can install CUDA to speed up local inference dramatically.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install&#xA;npx --no node-llama-cpp source download --gpu cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure that you&#39;ve installed the CUDA Toolkit, including cuDNN and cuBLAS.&lt;/p&gt; &#xA;&lt;h2&gt;Running locally&lt;/h2&gt; &#xA;&lt;p&gt;Add XAI_MODEL and set it to one of the above options from &lt;a href=&#34;https://raw.githubusercontent.com/ai16z/eliza/main/#run-with-llama&#34;&gt;Run with Llama&lt;/a&gt; - you can leave X_SERVER_URL and XAI_API_KEY blank, it downloads the model from huggingface and queries it locally&lt;/p&gt; &#xA;&lt;h1&gt;Cloud Setup (with OpenAI)&lt;/h1&gt; &#xA;&lt;p&gt;In addition to the environment variables above, you will need to add the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# OpenAI handles the bulk of the work with chat, TTS, image recognition, etc.&#xA;OPENAI_API_KEY=sk-* # OpenAI API key, starting with sk-&#xA;&#xA;# The agent can also ask Claude for help if you have an API key&#xA;ANTHROPIC_API_KEY=&#xA;&#xA;# For Elevenlabs voice generation on Discord voice&#xA;ELEVENLABS_XI_API_KEY= # API key from elevenlabs&#xA;&#xA;# ELEVENLABS SETINGS&#xA;ELEVENLABS_MODEL_ID=eleven_multilingual_v2&#xA;ELEVENLABS_VOICE_ID=21m00Tcm4TlvDq8ikWAM&#xA;ELEVENLABS_VOICE_STABILITY=0.5&#xA;ELEVENLABS_VOICE_SIMILARITY_BOOST=0.9&#xA;ELEVENLABS_VOICE_STYLE=0.66&#xA;ELEVENLABS_VOICE_USE_SPEAKER_BOOST=false&#xA;ELEVENLABS_OPTIMIZE_STREAMING_LATENCY=4&#xA;ELEVENLABS_OUTPUT_FORMAT=pcm_16000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Discord Bot&lt;/h1&gt; &#xA;&lt;p&gt;For help with setting up your Discord Bot, check out here: &lt;a href=&#34;https://discordjs.guide/preparations/setting-up-a-bot-application.html&#34;&gt;https://discordjs.guide/preparations/setting-up-a-bot-application.html&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>blackmatrix7/ios_rule_script</title>
    <updated>2024-10-29T01:28:48Z</updated>
    <id>tag:github.com,2024-10-29:/blackmatrix7/ios_rule_script</id>
    <link href="https://github.com/blackmatrix7/ios_rule_script" rel="alternate"></link>
    <summary type="html">&lt;p&gt;分流规则、重写写规则及脚本。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rules And Scripts&lt;/h1&gt; &#xA;&lt;h2&gt;前言&lt;/h2&gt; &#xA;&lt;p&gt;各平台的分流规则、复写规则及自动化脚本。&lt;/p&gt; &#xA;&lt;h2&gt;特别声明&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;本项目内所有资源文件，禁止任何公众号、自媒体进行任何形式的转载、发布。&lt;/li&gt; &#xA; &lt;li&gt;编写本项目主要目的为学习和研究ES6，无法保证项目内容的合法性、准确性、完整性和有效性。&lt;/li&gt; &#xA; &lt;li&gt;本项目涉及的数据由使用的个人或组织自行填写，本项目不对数据内容负责，包括但不限于数据的真实性、准确性、合法性。使用本项目所造成的一切后果，与本项目的所有贡献者无关，由使用的个人或组织完全承担。&lt;/li&gt; &#xA; &lt;li&gt;本项目中涉及的第三方硬件、软件等，与本项目没有任何直接或间接的关系。本项目仅对部署和使用过程进行客观描述，不代表支持使用任何第三方硬件、软件。使用任何第三方硬件、软件，所造成的一切后果由使用的个人或组织承担，与本项目无关。&lt;/li&gt; &#xA; &lt;li&gt;本项目中所有内容只供学习和研究使用，不得将本项目中任何内容用于违反国家/地区/组织等的法律法规或相关规定的其他用途。&lt;/li&gt; &#xA; &lt;li&gt;所有基于本项目源代码，进行的任何修改，为其他个人或组织的自发行为，与本项目没有任何直接或间接的关系，所造成的一切后果亦与本项目无关。&lt;/li&gt; &#xA; &lt;li&gt;所有直接或间接使用本项目的个人和组织，应24小时内完成学习和研究，并及时删除本项目中的所有内容。如对本项目的功能有需求，应自行开发相关功能。&lt;/li&gt; &#xA; &lt;li&gt;本项目保留随时对免责声明进行补充或更改的权利，直接或间接使用本项目内容的个人或组织，视为接受本项目的特别声明。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;规则&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;我们并不生产规则，我们只是开源规则的搬运工。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;分流规则&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/rule&#34;&gt;https://github.com/blackmatrix7/ios_rule_script/tree/master/rule&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;复写规则&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/rewrite&#34;&gt;https://github.com/blackmatrix7/ios_rule_script/tree/master/rewrite&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;所有规则数据都来自互联网，感谢开源规则项目作者的辛勤付出。&lt;/p&gt; &#xA;&lt;h2&gt;脚本&lt;/h2&gt; &#xA;&lt;h3&gt;脚本说明&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;脚本&lt;/th&gt; &#xA;   &lt;th&gt;介绍&lt;/th&gt; &#xA;   &lt;th&gt;框架&lt;/th&gt; &#xA;   &lt;th&gt;维护状态&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/smzdm&#34;&gt;什么值得买&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;什么值得买任务和去广告&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 2/3&lt;/td&gt; &#xA;   &lt;td&gt;正常&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/tieba&#34;&gt;百度贴吧&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;带重试功能的贴吧签到，提高签到成功率&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 3&lt;/td&gt; &#xA;   &lt;td&gt;正常&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/startup&#34;&gt;开屏去广告&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;通过脚本去除缓存到本地的APP开屏广告&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 3&lt;/td&gt; &#xA;   &lt;td&gt;正常&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/manmanbuy&#34;&gt;慢慢买&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;每日自动签到&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 2&lt;/td&gt; &#xA;   &lt;td&gt;正常&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/dingdong&#34;&gt;叮咚买菜&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;每日自动签到&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 3&lt;/td&gt; &#xA;   &lt;td&gt;正常&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/famijia&#34;&gt;Fa米家&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;每日自动签到&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 2&lt;/td&gt; &#xA;   &lt;td&gt;正常&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/luka&#34;&gt;Luka&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;每日自动签到&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 2&lt;/td&gt; &#xA;   &lt;td&gt;正常&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/zheye&#34;&gt;哲也同学&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;之乎者也&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 3&lt;/td&gt; &#xA;   &lt;td&gt;正常&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/synology&#34;&gt;Synology&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;群晖Download Station资源离线下载&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 3&lt;/td&gt; &#xA;   &lt;td&gt;正常&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/script/applestore&#34;&gt;AppleStore&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AppleStore 商品库存监控&lt;/td&gt; &#xA;   &lt;td&gt;MagicJS 3&lt;/td&gt; &#xA;   &lt;td&gt;暂停&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Quantumult X Gallery&lt;/h4&gt; &#xA;&lt;p&gt;部分脚本已配置为Quantumult X Gallery。&lt;/p&gt; &#xA;&lt;p&gt;地址： &lt;a href=&#34;https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/script/gallery.json&#34;&gt;https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/script/gallery.json&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;BoxJS&lt;/h4&gt; &#xA;&lt;p&gt;感谢 &lt;a href=&#34;https://github.com/chouchoui&#34;&gt;@chouchoui&lt;/a&gt; 为本项目添加BoxJS的订阅。&lt;/p&gt; &#xA;&lt;p&gt;地址：&lt;a href=&#34;https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/script/boxjs.json&#34;&gt;https://raw.githubusercontent.com/blackmatrix7/ios_rule_script/master/script/boxjs.json&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;外部资源&lt;/h3&gt; &#xA;&lt;p&gt;项目中资源来自互联网上其他开源项目（具体以不同目录的说明为准），这里主要进行一些整合和备份。对于此类资源，无法对使用过程中出现的任何问题进行解答，您需要联系原作者。&lt;/p&gt; &#xA;&lt;p&gt;地址：&lt;a href=&#34;https://github.com/blackmatrix7/ios_rule_script/tree/master/external&#34;&gt;https://github.com/blackmatrix7/ios_rule_script/tree/master/external&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;感谢&lt;/h1&gt; &#xA;&lt;p&gt;以下排名不分先后&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/BaileyZyp&#34;&gt;@BaileyZyp&lt;/a&gt; &lt;a href=&#34;https://github.com/Mazeorz&#34;&gt;@Mazeorz&lt;/a&gt; &lt;a href=&#34;https://github.com/LuzMasonj&#34;&gt;@LuzMasonj&lt;/a&gt; &lt;a href=&#34;https://github.com/chouchoui&#34;&gt;@chouchoui&lt;/a&gt; &lt;a href=&#34;https://github.com/ypannnn&#34;&gt;@ypannnn&lt;/a&gt; &lt;a href=&#34;https://github.com/echizenryoma&#34;&gt;@echizenryoma&lt;/a&gt; &lt;a href=&#34;https://github.com/zirawell&#34;&gt;@zirawell&lt;/a&gt; &lt;a href=&#34;https://github.com/urzz&#34;&gt;@urzz&lt;/a&gt; &lt;a href=&#34;https://github.com/ASD-max&#34;&gt;@ASD-max&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>