<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-14T01:29:04Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fchollet/ARC-AGI</title>
    <updated>2024-06-14T01:29:04Z</updated>
    <id>tag:github.com,2024-06-14:/fchollet/ARC-AGI</id>
    <link href="https://github.com/fchollet/ARC-AGI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Abstraction and Reasoning Corpus&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI)&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains the ARC-AGI task data, as well as a browser-based interface for humans to try their hand at solving the tasks manually.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&#34;ARC can be seen as a general artificial intelligence benchmark, as a program synthesis benchmark, or as a psychometric intelligence test. It is targeted at both humans and artificially intelligent systems that aim at emulating a human-like form of general fluid intelligence.&#34;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;A complete description of the dataset, its goals, and its underlying logic, can be found in: &lt;a href=&#34;https://arxiv.org/abs/1911.01547&#34;&gt;On the Measure of Intelligence&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As a reminder, a test-taker is said to solve a task when, upon seeing the task for the first time, they are able to produce the correct output grid for &lt;em&gt;all&lt;/em&gt; test inputs in the task (this includes picking the dimensions of the output grid). For each test input, the test-taker is allowed 3 trials (this holds for all test-takers, either humans or AI).&lt;/p&gt; &#xA;&lt;h2&gt;Task file format&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;data&lt;/code&gt; directory contains two subdirectories:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;data/training&lt;/code&gt;: contains the task files for training (400 tasks). Use these to prototype your algorithm or to train your algorithm to acquire ARC-relevant cognitive priors.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;data/evaluation&lt;/code&gt;: contains the task files for evaluation (400 tasks). Use these to evaluate your final algorithm. To ensure fair evaluation results, do not leak information from the evaluation set into your algorithm (e.g. by looking at the evaluation tasks yourself during development, or by repeatedly modifying an algorithm while using its evaluation score as feedback).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The tasks are stored in JSON format. Each task JSON file contains a dictionary with two fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;train&#34;&lt;/code&gt;: demonstration input/output pairs. It is a list of &#34;pairs&#34; (typically 3 pairs).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;test&#34;&lt;/code&gt;: test input/output pairs. It is a list of &#34;pairs&#34; (typically 1 pair).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A &#34;pair&#34; is a dictionary with two fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;input&#34;&lt;/code&gt;: the input &#34;grid&#34; for the pair.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#34;output&#34;&lt;/code&gt;: the output &#34;grid&#34; for the pair.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;A &#34;grid&#34; is a rectangular matrix (list of lists) of integers between 0 and 9 (inclusive). The smallest possible grid size is 1x1 and the largest is 30x30.&lt;/p&gt; &#xA;&lt;p&gt;When looking at a task, a test-taker has access to inputs &amp;amp; outputs of the demonstration pairs, plus the input(s) of the test pair(s). The goal is to construct the output grid(s) corresponding to the test input grid(s), using 3 trials for each test input. &#34;Constructing the output grid&#34; involves picking the height and width of the output grid, then filling each cell in the grid with a symbol (integer between 0 and 9, which are visualized as colors). Only &lt;em&gt;exact&lt;/em&gt; solutions (all cells match the expected answer) can be said to be correct.&lt;/p&gt; &#xA;&lt;h2&gt;Usage of the testing interface&lt;/h2&gt; &#xA;&lt;p&gt;The testing interface is located at &lt;code&gt;apps/testing_interface.html&lt;/code&gt;. Open it in a web browser (Chrome recommended). It will prompt you to select a task JSON file.&lt;/p&gt; &#xA;&lt;p&gt;After loading a task, you will enter the test space, which looks like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://arc-benchmark.s3.amazonaws.com/figs/arc_test_space.png&#34; alt=&#34;test space&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;On the left, you will see the input/output pairs demonstrating the nature of the task. In the middle, you will see the current test input grid. On the right, you will see the controls you can use to construct the corresponding output grid.&lt;/p&gt; &#xA;&lt;p&gt;You have access to the following tools:&lt;/p&gt; &#xA;&lt;h3&gt;Grid controls&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resize: input a grid size (e.g. &#34;10x20&#34; or &#34;4x4&#34;) and click &#34;Resize&#34;. This preserves existing grid content (in the top left corner).&lt;/li&gt; &#xA; &lt;li&gt;Copy from input: copy the input grid to the output grid. This is useful for tasks where the output consists of some modification of the input.&lt;/li&gt; &#xA; &lt;li&gt;Reset grid: fill the grid with 0s.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Symbol controls&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Edit: select a color (symbol) from the color picking bar, then click on a cell to set its color.&lt;/li&gt; &#xA; &lt;li&gt;Select: click and drag on either the output grid or the input grid to select cells. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;After selecting cells on the output grid, you can select a color from the color picking to set the color of the selected cells. This is useful to draw solid rectangles or lines.&lt;/li&gt; &#xA;   &lt;li&gt;After selecting cells on either the input grid or the output grid, you can press C to copy their content. After copying, you can select a cell on the output grid and press &#34;V&#34; to paste the copied content. You should select the cell in the top left corner of the zone you want to paste into.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Floodfill: click on a cell from the output grid to color all connected cells to the selected color. &#34;Connected cells&#34; are contiguous cells with the same color.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Answer validation&lt;/h3&gt; &#xA;&lt;p&gt;When your output grid is ready, click the green &#34;Submit!&#34; button to check your answer. We do not enforce the 3-trials rule.&lt;/p&gt; &#xA;&lt;p&gt;After you&#39;ve obtained the correct answer for the current test input grid, you can switch to the next test input grid for the task using the &#34;Next test input&#34; button (if there is any available; most tasks only have one test input).&lt;/p&gt; &#xA;&lt;p&gt;When you&#39;re done with a task, use the &#34;load task&#34; button to open a new task.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>datastrato/gravitino</title>
    <updated>2024-06-14T01:29:04Z</updated>
    <id>tag:github.com,2024-06-14:/datastrato/gravitino</id>
    <link href="https://github.com/datastrato/gravitino" rel="alternate"></link>
    <summary type="html">&lt;p&gt;World&#39;s most powerful open data catalog for building a high-performance, geo-distributed and federated metadata lake.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gravitino&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/datastrato/gravitino/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/datastrato/gravitino/actions/workflows/build.yml/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions Build&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/datastrato/gravitino/actions/workflows/integration-test.yml&#34;&gt;&lt;img src=&#34;https://github.com/datastrato/gravitino/actions/workflows/integration-test.yml/badge.svg?sanitize=true&#34; alt=&#34;GitHub Actions Integration Test&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/datastrato/gravitino/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/datastrato/gravitino&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/datastrato/gravitino/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors/datastrato/gravitino&#34; alt=&#34;Contributors&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/datastrato/gravitino/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/datastrato/gravitino&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/datastrato/gravitino/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/datastrato/gravitino&#34; alt=&#34;Open Issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/datastrato/gravitino/commits/main/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/datastrato/gravitino&#34; alt=&#34;Last Committed&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.bestpractices.dev/projects/8358&#34;&gt;&lt;img src=&#34;https://www.bestpractices.dev/projects/8358/badge&#34; alt=&#34;OpenSSF Best Practices&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Gravitino is a high-performance, geo-distributed, and federated metadata lake. It manages the metadata directly in different sources, types, and regions. It also provides users with unified metadata access for data and AI assets.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/datastrato/gravitino/main/docs/assets/gravitino-architecture.png&#34; alt=&#34;Gravitino Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Gravitino aims to provide several key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Single Source of Truth for multi-regional data with geo-distributed architecture support.&lt;/li&gt; &#xA; &lt;li&gt;Unified Data and AI asset management for both users and engines.&lt;/li&gt; &#xA; &lt;li&gt;Security in one place, centralizing the security for different sources.&lt;/li&gt; &#xA; &lt;li&gt;Built-in data management and data access management.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing to Gravitino&lt;/h2&gt; &#xA;&lt;p&gt;Gravitino is open source software available under the Apache 2.0 license. For information on how to contribute to Gravitino please see the &lt;a href=&#34;https://raw.githubusercontent.com/datastrato/gravitino/main/CONTRIBUTING.md&#34;&gt;Contribution guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Online documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the latest Gravitino documentation in the &lt;a href=&#34;https://raw.githubusercontent.com/datastrato/gravitino/main/docs&#34;&gt;doc folder&lt;/a&gt;. This README file only contains basic setup instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Building Gravitino&lt;/h2&gt; &#xA;&lt;p&gt;You can build Gravitino using Gradle. Currently you can build Gravitino on Linux and macOS, Windows isn&#39;t supported.&lt;/p&gt; &#xA;&lt;p&gt;To build Gravitino, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./gradlew clean build -x test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to build a distribution package, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./gradlew compileDistribution -x test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to build a distribution package.&lt;/p&gt; &#xA;&lt;p&gt;Or:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./gradlew assembleDistribution -x test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to build a compressed distribution package.&lt;/p&gt; &#xA;&lt;p&gt;The directory &lt;code&gt;distribution&lt;/code&gt; contains the generated binary distribution package.&lt;/p&gt; &#xA;&lt;p&gt;For the details of building and testing Gravitino, please see &lt;a href=&#34;https://raw.githubusercontent.com/datastrato/gravitino/main/docs/how-to-build.md&#34;&gt;How to build Gravitino&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;h3&gt;Configure and start the Gravitino server&lt;/h3&gt; &#xA;&lt;p&gt;If you already have a binary distribution package, go to the directory of the decompressed package.&lt;/p&gt; &#xA;&lt;p&gt;Before starting the Gravitino server, please configure the Gravitino server configuration file. The configuration file, &lt;code&gt;gravitino.conf&lt;/code&gt;, is in the &lt;code&gt;conf&lt;/code&gt; directory and follows the standard property file format. You can modify the configuration within this file.&lt;/p&gt; &#xA;&lt;p&gt;To start the Gravitino server, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./bin/gravitino.sh start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To stop the Gravitino server, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./bin/gravitino.sh stop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, to run the Gravitino server in frontend, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./bin/gravitino.sh run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And press &lt;code&gt;CTRL+C&lt;/code&gt; to stop the Gravitino server.&lt;/p&gt; &#xA;&lt;h3&gt;Using Trino with Gravitino&lt;/h3&gt; &#xA;&lt;p&gt;Gravitino provides a Trino connector to access the metadata in Gravitino. To use Trino with Gravitino, please follow the &lt;a href=&#34;https://raw.githubusercontent.com/datastrato/gravitino/main/docs/trino-connector/index.md&#34;&gt;trino-gravitino-connector doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Development guide&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastrato/gravitino/main/docs/how-to-build.md&#34;&gt;How to build Gravitino&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastrato/gravitino/main/docs/how-to-test.md&#34;&gt;How to test Gravitino&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datastrato/gravitino/main/docs/publish-docker-images.md&#34;&gt;How to publish Docker images&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Gravitino is under the Apache License Version 2.0, See the &lt;a href=&#34;https://raw.githubusercontent.com/datastrato/gravitino/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for the details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;sub&gt;Apache®, Apache Hadoop®, Apache Hive™, Apache Iceberg™, Apache Kafka®, Apache Spark™, Apache Submarine™, Apache Thrift™ and Apache Zeppelin™ are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries.&lt;/sub&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>IDEA-Research/GroundingDINO</title>
    <updated>2024-06-14T01:29:04Z</updated>
    <id>tag:github.com,2024-06-14:/IDEA-Research/GroundingDINO</id>
    <link href="https://github.com/IDEA-Research/GroundingDINO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of the paper &#34;Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&#34;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/grounding_dino_logo.png&#34; width=&#34;30%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;&lt;span&gt;🦕&lt;/span&gt; Grounding DINO&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-object-detection-on-mscoco?p=grounding-dino-marrying-dino-with-grounded&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/zero-shot-object-detection-on-mscoco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/zero-shot-object-detection-on-odinw?p=grounding-dino-marrying-dino-with-grounded&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/zero-shot-object-detection-on-odinw&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://paperswithcode.com/sota/object-detection-on-coco-minival?p=grounding-dino-marrying-dino-with-grounded&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/object-detection-on-coco-minival&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://paperswithcode.com/sota/object-detection-on-coco?p=grounding-dino-marrying-dino-with-grounded&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/grounding-dino-marrying-dino-with-grounded/object-detection-on-coco&#34; alt=&#34;PWC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/IDEA-Research&#34;&gt;IDEA-CVR, IDEA-Research&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.lsl.zone/&#34;&gt;Shilong Liu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=U_cvvUwAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Zhaoyang Zeng&lt;/a&gt;, &lt;a href=&#34;https://rentainhe.github.io/&#34;&gt;Tianhe Ren&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=ybRe9GcAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Feng Li&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=B8hPxMQAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&#34;https://github.com/yangjie-cv&#34;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=Zd7WmXUAAAAJ&amp;amp;hl=zh-CN&amp;amp;oi=ao&#34;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&#34;https://jwyang.github.io/&#34;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=dxN1_X0AAAAJ&amp;amp;view_op=list_works&amp;amp;sortby=pubdate&#34;&gt;Hang Su&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=axsP38wAAAAJ&#34;&gt;Jun Zhu&lt;/a&gt;, &lt;a href=&#34;https://www.leizhang.org/&#34;&gt;Lei Zhang&lt;/a&gt;&lt;sup&gt;&lt;span&gt;📧&lt;/span&gt;&lt;/sup&gt;.&lt;/p&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;&lt;code&gt;Paper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo&#34;&gt;&lt;code&gt;Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/#black_nib-citation&#34;&gt;&lt;code&gt;BibTex&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;PyTorch implementation and pretrained models for Grounding DINO. For details, see the paper &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&lt;/a&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🔥 &lt;strong&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounding-DINO-1.5-API&#34;&gt;Grounding DINO 1.5&lt;/a&gt;&lt;/strong&gt; is released now, which is IDEA Research&#39;s &lt;strong&gt;Most Capable&lt;/strong&gt; Open-World Object Detection Model!&lt;/li&gt; &#xA; &lt;li&gt;🔥 &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.14159&#34;&gt;Grounded SAM&lt;/a&gt;&lt;/strong&gt; are now supported in Huggingface. For more convenient use, you can refer to &lt;a href=&#34;https://huggingface.co/docs/transformers/model_doc/grounding-dino&#34;&gt;this documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;🌞&lt;/span&gt; Helpful Tutorial&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;🍇&lt;/span&gt; [&lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Read our arXiv Paper&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🍎&lt;/span&gt; [&lt;a href=&#34;https://youtu.be/wxWDt5UiwY8&#34;&gt;Watch our simple introduction video on YouTube&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🌼&lt;/span&gt; &amp;nbsp;[&lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&#34;&gt;Try the Colab Demo&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🌻&lt;/span&gt; [&lt;a href=&#34;https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo&#34;&gt;Try our Official Huggingface Demo&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🍁&lt;/span&gt; [&lt;a href=&#34;https://youtu.be/cMa77r3YrDk&#34;&gt;Watch the Step by Step Tutorial about GroundingDINO by Roboflow AI&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🍄&lt;/span&gt; [&lt;a href=&#34;https://youtu.be/C4NqaRBz_Kw&#34;&gt;GroundingDINO: Automated Dataset Annotation and Evaluation by Roboflow AI&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🌺&lt;/span&gt; [&lt;a href=&#34;https://youtu.be/oEQYStnF2l8&#34;&gt;Accelerate Image Annotation with SAM and GroundingDINO by Roboflow AI&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;💮&lt;/span&gt; [&lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;Autodistill: Train YOLOv8 with ZERO Annotations based on Grounding-DINO and Grounded-SAM by Roboflow AI&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Grounding DINO Methods | &#xA;[![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499) &#xA;[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wxWDt5UiwY8) --&gt; &#xA;&lt;!-- Grounding DINO Demos |&#xA;[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) --&gt; &#xA;&lt;!-- [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/cMa77r3YrDk)&#xA;[![HuggingFace space](https://img.shields.io/badge/🤗-HuggingFace%20Space-cyan.svg)](https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo)&#xA;[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/oEQYStnF2l8)&#xA;[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/C4NqaRBz_Kw) --&gt; &#xA;&lt;h2&gt;&lt;span&gt;✨&lt;/span&gt; Highlight Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Semantic-SAM&#34;&gt;Semantic-SAM: a universal image segmentation model to enable segment and recognize anything at any desired granularity.&lt;/a&gt;,&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OptimalScale/DetGPT&#34;&gt;DetGPT: Detect What You Need via Reasoning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-SAM: Marrying Grounding DINO with Segment Anything&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb&#34;&gt;Grounding DINO with Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_gligen.ipynb&#34;&gt;Grounding DINO with GLIGEN for Controllable Image Editing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/OpenSeeD&#34;&gt;OpenSeeD: A Simple and Strong Openset Segmentation Model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34;&gt;SEEM: Segment Everything Everywhere All at Once&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/X-Decoder/tree/xgpt&#34;&gt;X-GPT: Conversational Visual Agent supported by X-Decoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN: Open-Set Grounded Text-to-Image Generation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA: Large Language and Vision Assistant&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Extensions | [Grounding DINO with Segment Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything); [Grounding DINO with Stable Diffusion](demo/image_editing_with_groundingdino_stablediffusion.ipynb); [Grounding DINO with GLIGEN](demo/image_editing_with_groundingdino_gligen.ipynb)  --&gt; &#xA;&lt;!-- Official PyTorch implementation of [Grounding DINO](https://arxiv.org/abs/2303.05499), a stronger open-set object detector. Code is available now! --&gt; &#xA;&lt;h2&gt;&lt;span&gt;💡&lt;/span&gt; Highlight&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open-Set Detection.&lt;/strong&gt; Detect &lt;strong&gt;everything&lt;/strong&gt; with language!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High Performance.&lt;/strong&gt; COCO zero-shot &lt;strong&gt;52.5 AP&lt;/strong&gt; (training without COCO data!). COCO fine-tune &lt;strong&gt;63.0 AP&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible.&lt;/strong&gt; Collaboration with Stable Diffusion for Image Editting.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;🔥&lt;/span&gt; News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/07/18&lt;/code&gt;&lt;/strong&gt;: We release &lt;a href=&#34;https://github.com/UX-Decoder/Semantic-SAM&#34;&gt;Semantic-SAM&lt;/a&gt;, a universal image segmentation model to enable segment and recognize anything at any desired granularity. &lt;strong&gt;Code&lt;/strong&gt; and &lt;strong&gt;checkpoint&lt;/strong&gt; are available!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/06/17&lt;/code&gt;&lt;/strong&gt;: We provide an example to evaluate Grounding DINO on COCO zero-shot performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/15&lt;/code&gt;&lt;/strong&gt;: Refer to &lt;a href=&#34;https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings&#34;&gt;CV in the Wild Readings&lt;/a&gt; for those who are interested in open-set recognition!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/08&lt;/code&gt;&lt;/strong&gt;: We release &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_gligen.ipynb&#34;&gt;demos&lt;/a&gt; to combine &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt; with &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; for more controllable image editings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/08&lt;/code&gt;&lt;/strong&gt;: We release &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb&#34;&gt;demos&lt;/a&gt; to combine &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt; with &lt;a href=&#34;https://github.com/Stability-AI/StableDiffusion&#34;&gt;Stable Diffusion&lt;/a&gt; for image editings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/04/06&lt;/code&gt;&lt;/strong&gt;: We build a new demo by marrying GroundingDINO with &lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34;&gt;Segment-Anything&lt;/a&gt; named &lt;strong&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt;&lt;/strong&gt; aims to support segmentation in GroundingDINO.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/28&lt;/code&gt;&lt;/strong&gt;: A YouTube &lt;a href=&#34;https://youtu.be/cMa77r3YrDk&#34;&gt;video&lt;/a&gt; about Grounding DINO and basic object detection prompt engineering. [&lt;a href=&#34;https://github.com/SkalskiP&#34;&gt;SkalskiP&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/28&lt;/code&gt;&lt;/strong&gt;: Add a &lt;a href=&#34;https://huggingface.co/spaces/ShilongLiu/Grounding_DINO_demo&#34;&gt;demo&lt;/a&gt; on Hugging Face Space!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/27&lt;/code&gt;&lt;/strong&gt;: Support CPU-only mode. Now the model can run on machines without GPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/25&lt;/code&gt;&lt;/strong&gt;: A &lt;a href=&#34;https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb&#34;&gt;demo&lt;/a&gt; for Grounding DINO is available at Colab. [&lt;a href=&#34;https://github.com/SkalskiP&#34;&gt;SkalskiP&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2023/03/22&lt;/code&gt;&lt;/strong&gt;: Code is available Now!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; Description &lt;/font&gt;&lt;/summary&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Paper&lt;/a&gt; introduction. &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/hero_figure.png&#34; alt=&#34;ODinW&#34; width=&#34;100%&#34;&gt; Marrying &#xA; &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;Grounding DINO&lt;/a&gt; and &#xA; &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; &#xA; &lt;img src=&#34;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/GD_GLIGEN.png&#34; alt=&#34;gd_gligen&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;⭐&lt;/span&gt; Explanations/Tips for Grounding DINO Inputs and Outputs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Grounding DINO accepts an &lt;code&gt;(image, text)&lt;/code&gt; pair as inputs.&lt;/li&gt; &#xA; &lt;li&gt;It outputs &lt;code&gt;900&lt;/code&gt; (by default) object boxes. Each box has similarity scores across all input words. (as shown in Figures below.)&lt;/li&gt; &#xA; &lt;li&gt;We defaultly choose the boxes whose highest similarities are higher than a &lt;code&gt;box_threshold&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;We extract the words whose similarities are higher than the &lt;code&gt;text_threshold&lt;/code&gt; as predicted labels.&lt;/li&gt; &#xA; &lt;li&gt;If you want to obtain objects of specific phrases, like the &lt;code&gt;dogs&lt;/code&gt; in the sentence &lt;code&gt;two dogs with a stick.&lt;/code&gt;, you can select the boxes with highest text similarities with &lt;code&gt;dogs&lt;/code&gt; as final outputs.&lt;/li&gt; &#xA; &lt;li&gt;Note that each word can be split to &lt;strong&gt;more than one&lt;/strong&gt; tokens with different tokenlizers. The number of words in a sentence may not equal to the number of text tokens.&lt;/li&gt; &#xA; &lt;li&gt;We suggest separating different category names with &lt;code&gt;.&lt;/code&gt; for Grounding DINO. &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/model_explan1.PNG&#34; alt=&#34;model_explain1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/model_explan2.PNG&#34; alt=&#34;model_explain2&#34;&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;🏷&lt;/span&gt; TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release inference code and demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release checkpoints.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Grounding DINO with Stable Diffusion and GLIGEN demos.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release training codes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;span&gt;🛠&lt;/span&gt; Install&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;If you have a CUDA environment, please make sure the environment variable &lt;code&gt;CUDA_HOME&lt;/code&gt; is set. It will be compiled under CPU-only mode if no CUDA available.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Please make sure following the installation steps strictly, otherwise the program may produce:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;NameError: name &#39;_C&#39; is not defined&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If this happened, please reinstalled the groundingDINO by reclone the git and do all the installation steps again.&lt;/p&gt; &#xA;&lt;h4&gt;how to check cuda:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo $CUDA_HOME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If it print nothing, then it means you haven&#39;t set up the path/&lt;/p&gt; &#xA;&lt;p&gt;Run this so the environment variable will be set under current shell.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_HOME=/path/to/cuda-11.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Notice the version of cuda should be aligned with your CUDA runtime, for there might exists multiple cuda at the same time.&lt;/p&gt; &#xA;&lt;p&gt;If you want to set the CUDA_HOME permanently, store it using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#39;export CUDA_HOME=/path/to/cuda&#39; &amp;gt;&amp;gt; ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;after that, source the bashrc file and check CUDA_HOME:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source ~/.bashrc&#xA;echo $CUDA_HOME&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this example, /path/to/cuda-11.3 should be replaced with the path where your CUDA toolkit is installed. You can find this by typing &lt;strong&gt;which nvcc&lt;/strong&gt; in your terminal:&lt;/p&gt; &#xA;&lt;p&gt;For instance, if the output is /usr/local/cuda/bin/nvcc, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export CUDA_HOME=/usr/local/cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;1.Clone the GroundingDINO repository from GitHub.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/IDEA-Research/GroundingDINO.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Change the current directory to the GroundingDINO folder.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd GroundingDINO/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the required dependencies in the current directory.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download pre-trained model weights.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir weights&#xA;cd weights&#xA;wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;▶&lt;/span&gt; Demo&lt;/h2&gt; &#xA;&lt;p&gt;Check your GPU ID (only if you&#39;re using a GPU)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvidia-smi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;{GPU ID}&lt;/code&gt;, &lt;code&gt;image_you_want_to_detect.jpg&lt;/code&gt;, and &lt;code&gt;&#34;dir you want to save the output&#34;&lt;/code&gt; with appropriate values in the following command&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES={GPU ID} python demo/inference_on_a_image.py \&#xA;-c groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;-p weights/groundingdino_swint_ogc.pth \&#xA;-i image_you_want_to_detect.jpg \&#xA;-o &#34;dir you want to save the output&#34; \&#xA;-t &#34;chair&#34;&#xA; [--cpu-only] # open it for cpu mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to specify the phrases to detect, here is a demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES={GPU ID} python demo/inference_on_a_image.py \&#xA;-c groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA;-p ./groundingdino_swint_ogc.pth \&#xA;-i .asset/cat_dog.jpeg \&#xA;-o logs/1111 \&#xA;-t &#34;There is a cat and a dog in the image .&#34; \&#xA;--token_spans &#34;[[[9, 10], [11, 14]], [[19, 20], [21, 24]]]&#34;&#xA; [--cpu-only] # open it for cpu mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The token_spans specify the start and end positions of a phrases. For example, the first phrase is &lt;code&gt;[[9, 10], [11, 14]]&lt;/code&gt;. &lt;code&gt;&#34;There is a cat and a dog in the image .&#34;[9:10] = &#39;a&#39;&lt;/code&gt;, &lt;code&gt;&#34;There is a cat and a dog in the image .&#34;[11:14] = &#39;cat&#39;&lt;/code&gt;. Hence it refers to the phrase &lt;code&gt;a cat&lt;/code&gt; . Similarly, the &lt;code&gt;[[19, 20], [21, 24]]&lt;/code&gt; refers to the phrase &lt;code&gt;a dog&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;code&gt;demo/inference_on_a_image.py&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running with Python:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from groundingdino.util.inference import load_model, load_image, predict, annotate&#xA;import cv2&#xA;&#xA;model = load_model(&#34;groundingdino/config/GroundingDINO_SwinT_OGC.py&#34;, &#34;weights/groundingdino_swint_ogc.pth&#34;)&#xA;IMAGE_PATH = &#34;weights/dog-3.jpeg&#34;&#xA;TEXT_PROMPT = &#34;chair . person . dog .&#34;&#xA;BOX_TRESHOLD = 0.35&#xA;TEXT_TRESHOLD = 0.25&#xA;&#xA;image_source, image = load_image(IMAGE_PATH)&#xA;&#xA;boxes, logits, phrases = predict(&#xA;    model=model,&#xA;    image=image,&#xA;    caption=TEXT_PROMPT,&#xA;    box_threshold=BOX_TRESHOLD,&#xA;    text_threshold=TEXT_TRESHOLD&#xA;)&#xA;&#xA;annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)&#xA;cv2.imwrite(&#34;annotated_image.jpg&#34;, annotated_frame)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Web UI&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We also provide a demo code to integrate Grounding DINO with Gradio Web UI. See the file &lt;code&gt;demo/gradio_app.py&lt;/code&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Notebooks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We release &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_gligen.ipynb&#34;&gt;demos&lt;/a&gt; to combine &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt; with &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; for more controllable image editings.&lt;/li&gt; &#xA; &lt;li&gt;We release &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb&#34;&gt;demos&lt;/a&gt; to combine &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;Grounding DINO&lt;/a&gt; with &lt;a href=&#34;https://github.com/Stability-AI/StableDiffusion&#34;&gt;Stable Diffusion&lt;/a&gt; for image editings.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;COCO Zero-shot Evaluations&lt;/h2&gt; &#xA;&lt;p&gt;We provide an example to evaluate Grounding DINO zero-shot performance on COCO. The results should be &lt;strong&gt;48.5&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 \&#xA;python demo/test_ap_on_coco.py \&#xA; -c groundingdino/config/GroundingDINO_SwinT_OGC.py \&#xA; -p weights/groundingdino_swint_ogc.pth \&#xA; --anno_path /path/to/annoataions/ie/instances_val2017.json \&#xA; --image_dir /path/to/imagedir/ie/val2017&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;span&gt;🧳&lt;/span&gt; Checkpoints&lt;/h2&gt; &#xA;&lt;!-- insert a table --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr style=&#34;text-align: right;&#34;&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;name&lt;/th&gt; &#xA;   &lt;th&gt;backbone&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;box AP on COCO&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;Config&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1&lt;/th&gt; &#xA;   &lt;td&gt;GroundingDINO-T&lt;/td&gt; &#xA;   &lt;td&gt;Swin-T&lt;/td&gt; &#xA;   &lt;td&gt;O365,GoldG,Cap4M&lt;/td&gt; &#xA;   &lt;td&gt;48.4 (zero-shot) / 57.2 (fine-tune)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#34;&gt;GitHub link&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swint_ogc.pth&#34;&gt;HF link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/raw/main/groundingdino/config/GroundingDINO_SwinT_OGC.py&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;2&lt;/th&gt; &#xA;   &lt;td&gt;GroundingDINO-B&lt;/td&gt; &#xA;   &lt;td&gt;Swin-B&lt;/td&gt; &#xA;   &lt;td&gt;COCO,O365,GoldG,Cap4M,OpenImage,ODinW-35,RefCOCO&lt;/td&gt; &#xA;   &lt;td&gt;56.7 &lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha2/groundingdino_swinb_cogcoor.pth&#34;&gt;GitHub link&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/ShilongLiu/GroundingDINO/resolve/main/groundingdino_swinb_cogcoor.pth&#34;&gt;HF link&lt;/a&gt; &lt;/td&gt;&#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/raw/main/groundingdino/config/GroundingDINO_SwinB_cfg.py&#34;&gt;link&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;span&gt;🎖&lt;/span&gt; Results&lt;/h2&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; COCO Object Detection Results &lt;/font&gt;&lt;/summary&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/COCO.png&#34; alt=&#34;COCO&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; ODinW Object Detection Results &lt;/font&gt;&lt;/summary&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/ODinW.png&#34; alt=&#34;ODinW&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; Marrying Grounding DINO with &lt;a href=&#34;https://github.com/Stability-AI/StableDiffusion&#34;&gt;Stable Diffusion&lt;/a&gt; for Image Editing &lt;/font&gt;&lt;/summary&gt; See our example &#xA; &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/raw/main/demo/image_editing_with_groundingdino_stablediffusion.ipynb&#34;&gt;notebook&lt;/a&gt; for more details. &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/GD_SD.png&#34; alt=&#34;GD_SD&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;font size=&#34;4&#34;&gt; Marrying Grounding DINO with &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; for more Detailed Image Editing. &lt;/font&gt;&lt;/summary&gt; See our example &#xA; &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/raw/main/demo/image_editing_with_groundingdino_gligen.ipynb&#34;&gt;notebook&lt;/a&gt; for more details. &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/GD_GLIGEN.png&#34; alt=&#34;GD_GLIGEN&#34; width=&#34;100%&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;span&gt;🦕&lt;/span&gt; Model: Grounding DINO&lt;/h2&gt; &#xA;&lt;p&gt;Includes: a text backbone, an image backbone, a feature enhancer, a language-guided query selection, and a cross-modality decoder.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/.asset/arch.png&#34; alt=&#34;arch&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;♥&lt;/span&gt; Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;Our model is related to &lt;a href=&#34;https://github.com/IDEA-Research/DINO&#34;&gt;DINO&lt;/a&gt; and &lt;a href=&#34;https://github.com/microsoft/GLIP&#34;&gt;GLIP&lt;/a&gt;. Thanks for their great work!&lt;/p&gt; &#xA;&lt;p&gt;We also thank great previous work including DETR, Deformable DETR, SMCA, Conditional DETR, Anchor DETR, Dynamic DETR, DAB-DETR, DN-DETR, etc. More related work are available at &lt;a href=&#34;https://github.com/IDEACVR/awesome-detection-transformer&#34;&gt;Awesome Detection Transformer&lt;/a&gt;. A new toolbox &lt;a href=&#34;https://github.com/IDEA-Research/detrex&#34;&gt;detrex&lt;/a&gt; is available as well.&lt;/p&gt; &#xA;&lt;p&gt;Thanks &lt;a href=&#34;https://github.com/Stability-AI/StableDiffusion&#34;&gt;Stable Diffusion&lt;/a&gt; and &lt;a href=&#34;https://github.com/gligen/GLIGEN&#34;&gt;GLIGEN&lt;/a&gt; for their awesome models.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;span&gt;✒&lt;/span&gt; Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{liu2023grounding,&#xA;  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},&#xA;  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},&#xA;  journal={arXiv preprint arXiv:2303.05499},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>