<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-03T01:27:44Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pytorch/torchchat</title>
    <updated>2024-08-03T01:27:44Z</updated>
    <id>tag:github.com,2024-08-03:/pytorch/torchchat</id>
    <link href="https://github.com/pytorch/torchchat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Run PyTorch LLMs locally on servers, desktop and mobile&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chat with LLMs Everywhere&lt;/h1&gt; &#xA;&lt;p&gt;torchchat is a small codebase showcasing the ability to run large language models (LLMs) seamlessly. With torchchat, you can run LLMs using Python, within your own (C/C++) application (desktop or server) and on iOS and Android.&lt;/p&gt; &#xA;&lt;h2&gt;What can you do with torchchat?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#running-via-pytorch--python&#34;&gt;Run models via PyTorch / Python&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#chat&#34;&gt;Chat&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#generate&#34;&gt;Generate&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#browser&#34;&gt;Run chat in the Browser&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#desktopserver-execution&#34;&gt;Run models on desktop/server without python&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#aoti-aot-inductor&#34;&gt;Use AOT Inductor for faster execution&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#running-native-using-our-c-runner&#34;&gt;Running in c++ using the runner&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#mobile-execution&#34;&gt;Run models on mobile&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#deploy-and-run-on-ios&#34;&gt;Deploy and run on iOS&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#deploy-and-run-on-android&#34;&gt;Deploy and run on Android&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#eval&#34;&gt;Evaluate a model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Highlights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Command line interaction with popular LLMs such as Llama 3, Llama 2, Stories, Mistral and more&lt;/li&gt; &#xA; &lt;li&gt;PyTorch-native execution with performance&lt;/li&gt; &#xA; &lt;li&gt;Supports popular hardware and OS &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux (x86)&lt;/li&gt; &#xA;   &lt;li&gt;Mac OS (M1/M2/M3)&lt;/li&gt; &#xA;   &lt;li&gt;Android (Devices that support XNNPACK)&lt;/li&gt; &#xA;   &lt;li&gt;iOS 17+ (iPhone 13 Pro+)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Multiple data types including: float32, float16, bfloat16&lt;/li&gt; &#xA; &lt;li&gt;Multiple quantization schemes&lt;/li&gt; &#xA; &lt;li&gt;Multiple execution modes including: Python (Eager, Compile) or Native (AOT Inductor (AOTI), ExecuTorch)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The following steps require that you have &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;Python 3.10&lt;/a&gt; installed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# get the code&#xA;git clone https://github.com/pytorch/torchchat.git&#xA;cd torchchat&#xA;&#xA;# set up a virtual environment&#xA;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;&#xA;# install dependencies&#xA;./install_requirements.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Commands&lt;/h2&gt; &#xA;&lt;p&gt;The interfaces of torchchat are leveraged through &lt;strong&gt;Python Commands&lt;/strong&gt; and &lt;strong&gt;Native Runners&lt;/strong&gt;. While the Python Commands are enumerable in the --help menu, the latter are explored in their respective sections.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Output&#xA;usage: torchchat [-h] {chat,browser,generate,export,eval,download,list,remove,where,server} ...&#xA;&#xA;positional arguments:&#xA;  {chat,browser,generate,export,eval,download,list,remove,where,server}&#xA;                        The specific command to run&#xA;    chat                Chat interactively with a model via the CLI&#xA;    generate            Generate responses from a model given a prompt&#xA;    browser             Chat interactively with a model in a locally hosted browser&#xA;    export              Export a model artifact to AOT Inductor or ExecuTorch&#xA;    download            Download model artifacts&#xA;    list                List all supported models&#xA;    remove              Remove downloaded model artifacts&#xA;    where               Return directory containing downloaded model artifacts&#xA;    server              [WIP] Starts a locally hosted REST server for model interaction&#xA;    eval                Evaluate a model via lm-eval&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Python Inference&lt;/strong&gt; (chat, generate, browser, server)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;These commands represent different flavors of performing model inference in a Python enviroment.&lt;/li&gt; &#xA; &lt;li&gt;Models are constructed either from CLI args or from loading exported artifacts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Exporting&lt;/strong&gt; (export)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This command generates model artifacts that are consumed by Python Inference or Native Runners.&lt;/li&gt; &#xA; &lt;li&gt;More information is provided in the &lt;a href=&#34;https://github.com/pytorch/torchchat?tab=readme-ov-file#aoti-aot-inductor&#34;&gt;AOT Inductor&lt;/a&gt; and &lt;a href=&#34;https://github.com/pytorch/torchchat?tab=readme-ov-file#export-for-mobile&#34;&gt;ExecuTorch&lt;/a&gt; sections.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Inventory Management&lt;/strong&gt; (download, list, remove, where)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;These commands are used to manage and download models.&lt;/li&gt; &#xA; &lt;li&gt;More information is provided in the &lt;a href=&#34;https://github.com/pytorch/torchchat?tab=readme-ov-file#download-weights&#34;&gt;Download Weights&lt;/a&gt; section.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Evaluation&lt;/strong&gt; (eval)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This command test model fidelity via EleutherAI&#39;s &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm_evaluation_harness&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;More information is provided in the &lt;a href=&#34;https://github.com/pytorch/torchchat?tab=readme-ov-file#eval&#34;&gt;Evaluation&lt;/a&gt; section.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download Weights&lt;/h2&gt; &#xA;&lt;p&gt;Most models use Hugging Face as the distribution channel, so you will need to create a Hugging Face account. Create a Hugging Face user access token &lt;a href=&#34;https://huggingface.co/docs/hub/en/security-tokens&#34;&gt;as documented here&lt;/a&gt; with the &lt;code&gt;write&lt;/code&gt; role.&lt;/p&gt; &#xA;&lt;p&gt;Log into Hugging Face:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once this is done, torchchat will be able to download model artifacts from Hugging Face.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py download llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] This command may prompt you to request access to Llama 3 via Hugging Face, if you do not already have access. Simply follow the prompts and re-run the command when access is granted.*&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Additional Model Inventory Management Commands&lt;/summary&gt; &#xA; &lt;h3&gt;List&lt;/h3&gt; &#xA; &lt;p&gt;This subcommands shows the available models&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Where&lt;/h3&gt; &#xA; &lt;p&gt;This subcommands shows location of a particular model.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;This is useful in scripts when you do not want to hard-code paths&lt;/p&gt; &#xA; &lt;h3&gt;Remove&lt;/h3&gt; &#xA; &lt;p&gt;This subcommands removes the specified model&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py remove llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;More information about these commands can be found by adding the &lt;code&gt;--help&lt;/code&gt; option.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Running via PyTorch / Python&lt;/h2&gt; &#xA;&lt;p&gt;The simplest way to run a model in PyTorch is via &lt;a href=&#34;https://pytorch.org/blog/optimizing-production-pytorch-performance-with-graph-transformations/&#34;&gt;eager execution&lt;/a&gt;. This is the default execution mode for both PyTorch and torchchat. It performs inference without creating exporting artifacts or using a separate runner.&lt;/p&gt; &#xA;&lt;p&gt;The model used for inference can also be configured and tailored to specific needs (compilation, quantization, etc.). See the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/docs/model_customization.md&#34;&gt;customization guide&lt;/a&gt; for the options supported by torchchat.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] For more information about these commands, please refer to the &lt;code&gt;--help&lt;/code&gt; menu.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Chat&lt;/h3&gt; &#xA;&lt;p&gt;This mode allows you to chat with an LLM in an interactive fashion.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py chat llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Generate&lt;/h3&gt; &#xA;&lt;p&gt;This mode generates text based on an input prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py generate llama3.1 --prompt &#34;write me a story about a boy and his bear&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Browser&lt;/h3&gt; &#xA;&lt;p&gt;This mode allows you to chat with the model using a UI in your browser Running the command automatically open a tab in your browser.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamlit run torchchat.py -- browser llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Server&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: This feature is still a work in progress and not all endpoints are working&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;This mode gives a REST API that matches the OpenAI API spec for interacting with a model&lt;/summary&gt; &#xA; &lt;p&gt;To test out the REST API, &lt;strong&gt;you&#39;ll need 2 terminals&lt;/strong&gt;: one to host the server, and one to send the request.&lt;/p&gt; &#xA; &lt;p&gt;In one terminal, start the server&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 torchchat.py server llama3.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;In another terminal, query the server using &lt;code&gt;curl&lt;/code&gt;. Depending on the model configuration, this query might take a few minutes to respond.&lt;/p&gt; &#xA; &lt;p&gt;Setting &lt;code&gt;stream&lt;/code&gt; to &#34;true&#34; in the request emits a response in chunks. Currently, this response is plaintext and will not be formatted to the OpenAI API specification. If &lt;code&gt;stream&lt;/code&gt; is unset or not &#34;true&#34;, then the client will await the full response from the server.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Example Input + Output&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;curl http://127.0.0.1:5000/chat \&#xA;  -H &#34;Content-Type: application/json&#34; \&#xA;  -d &#39;{&#xA;    &#34;model&#34;: &#34;llama3.1&#34;,&#xA;    &#34;stream&#34;: &#34;true&#34;,&#xA;    &#34;messages&#34;: [&#xA;      {&#xA;        &#34;role&#34;: &#34;system&#34;,&#xA;        &#34;content&#34;: &#34;You are a helpful assistant.&#34;&#xA;      },&#xA;      {&#xA;        &#34;role&#34;: &#34;user&#34;,&#xA;        &#34;content&#34;: &#34;Hello!&#34;&#xA;      }&#xA;    ]&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code&gt;{&#34;response&#34;:&#34; I&#39;m a software developer with a passion for building innovative and user-friendly applications. I have experience in developing web and mobile applications using various technologies such as Java, Python, and JavaScript. I&#39;m always looking for new challenges and opportunities to learn and grow as a developer.\n\nIn my free time, I enjoy reading books on computer science and programming, as well as experimenting with new technologies and techniques. I&#39;m also interested in machine learning and artificial intelligence, and I&#39;m always looking for ways to apply these concepts to real-world problems.\n\nI&#39;m excited to be a part of the developer community and to have the opportunity to share my knowledge and experience with others. I&#39;m always happy to help with any questions or problems you may have, and I&#39;m looking forward to learning from you as well.\n\nThank you for visiting my profile! I hope you find my information helpful and interesting. If you have any questions or would like to discuss any topics, please feel free to reach out to me. I&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Desktop/Server Execution&lt;/h2&gt; &#xA;&lt;h3&gt;AOTI (AOT Inductor)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/blog/pytorch2-2/&#34;&gt;AOTI&lt;/a&gt; compiles models before execution for faster inference. The process creates a &lt;a href=&#34;https://en.wikipedia.org/wiki/Shared_library&#34;&gt;DSO&lt;/a&gt; model (represented by a file with extension &lt;code&gt;.so&lt;/code&gt;) that is then loaded for inference. This can be done with both Python and C++ enviroments.&lt;/p&gt; &#xA;&lt;p&gt;The following example exports and executes the Llama3.1 8B Instruct model. The first command compiles and performs the actual export.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py export llama3.1 --output-dso-path exportedModels/llama3.1.so&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] If your machine has cuda add this flag for performance &lt;code&gt;--quantize config/data/cuda.json&lt;/code&gt; when exporting.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more details on quantization and what settings to use for your use case visit our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/docs/model_customization.md&#34;&gt;customization guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run in a Python Enviroment&lt;/h3&gt; &#xA;&lt;p&gt;To run in a python enviroment, use the generate subcommand like before, but include the dso file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py generate llama3.1 --dso-path exportedModels/llama3.1.so --prompt &#34;Hello my name is&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Depending on which accelerator is used to generate the .dso file, the command may need the device specified: &lt;code&gt;--device (cuda | cpu)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Run using our C++ Runner&lt;/h3&gt; &#xA;&lt;p&gt;To run in a C++ enviroment, we need to build the runner binary.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/build_native.sh aoti&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the compiled executable, with the exported DSO from earlier.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake-out/aoti_run exportedModels/llama3.1.so -z `python3 torchchat.py where llama3.1`/tokenizer.model -l 3 -i &#34;Once upon a time&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Depending on which accelerator is used to generate the .dso file, the runner may need the device specified: &lt;code&gt;-d (CUDA | CPU)&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Mobile Execution&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/executorch&#34;&gt;ExecuTorch&lt;/a&gt; enables you to optimize your model for execution on a mobile or embedded device.&lt;/p&gt; &#xA;&lt;h3&gt;Set Up ExecuTorch&lt;/h3&gt; &#xA;&lt;p&gt;Before running any commands in torchchat that require ExecuTorch, you must first install ExecuTorch.&lt;/p&gt; &#xA;&lt;p&gt;To install ExecuTorch, run the following commands. This will download the ExecuTorch repo to ./et-build/src and install various ExecuTorch libraries to ./et-build/install.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The following commands should be run from the torchchat root directory.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code&gt;export TORCHCHAT_ROOT=${PWD}&#xA;./scripts/install_et.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Export for mobile&lt;/h3&gt; &#xA;&lt;p&gt;Similar to AOTI, to deploy onto device, we first export the PTE artifact, then we load the artifact for inference.&lt;/p&gt; &#xA;&lt;p&gt;The following example uses the Llama3.1 8B Instruct model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Export&#xA;python3 torchchat.py export llama3.1 --quantize config/data/mobile.json --output-pte-path llama3.1.pte&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] We use &lt;code&gt;--quantize config/data/mobile.json&lt;/code&gt; to quantize the llama3.1 model to reduce model size and improve performance for on-device use cases.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For more details on quantization and what settings to use for your use case visit our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/docs/model_customization.md&#34;&gt;customization guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Deploy and run on Desktop&lt;/h3&gt; &#xA;&lt;p&gt;While ExecuTorch does not focus on desktop inference, it is capable of doing so. This is handy for testing out PTE models without sending them to a physical device.&lt;/p&gt; &#xA;&lt;p&gt;Specifically there are 2 ways of doing so: Pure Python and via a Runner&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deploying via Python&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;# Execute&#xA;python3 torchchat.py generate llama3.1 --device cpu --pte-path llama3.1.pte --prompt &#34;Hello my name is&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deploying via a Runner&lt;/summary&gt; &#xA; &lt;p&gt;Build the runner&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scripts/build_native.sh et&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Execute using the runner&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cmake-out/et_run llama3.1.pte -z `python3 torchchat.py where llama3.1`/tokenizer.model -l 3 -i &#34;Once upon a time&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Deploy and run on iOS&lt;/h3&gt; &#xA;&lt;p&gt;The following assumes you&#39;ve completed the steps for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#set-up-executorch&#34;&gt;Setting up ExecuTorch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Deploying with Xcode&lt;/summary&gt; &#xA; &lt;h4&gt;Requirements&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://apps.apple.com/us/app/xcode/id497799835?mt=12/&#34;&gt;Xcode&lt;/a&gt; 15.0 or later&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://cmake.org/download/&#34;&gt;Cmake&lt;/a&gt; 3.19 or later &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Download and open the macOS &lt;code&gt;.dmg&lt;/code&gt; installer and move the Cmake app to &lt;code&gt;/Applications&lt;/code&gt; folder.&lt;/li&gt; &#xA;    &lt;li&gt;Install Cmake command line tools: &lt;code&gt;sudo /Applications/CMake.app/Contents/bin/cmake-gui --install&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;A development provisioning profile with the &lt;a href=&#34;https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_kernel_increased-memory-limit&#34;&gt;&lt;code&gt;increased-memory-limit&lt;/code&gt;&lt;/a&gt; entitlement.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Steps&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Open the Xcode project:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;open et-build/src/executorch/examples/demo-apps/apple_ios/LLaMA/LLaMA.xcodeproj&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;blockquote&gt; &#xA;    &lt;p&gt;Note: If you&#39;re running into any issues related to package dependencies, close Xcode, clean some of the caches and/or the build products, and open the Xcode project again:&lt;/p&gt; &#xA;    &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rm -rf \&#xA;  ~/Library/org.swift.swiftpm \&#xA;  ~/Library/Caches/org.swift.swiftpm \&#xA;  ~/Library/Caches/com.apple.dt.Xcode \&#xA;  ~/Library/Developer/Xcode/DerivedData&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;/blockquote&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Click the Play button to launch the app in the Simulator.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;To run on a device, ensure you have it set up for development and a provisioning profile with the &lt;code&gt;increased-memory-limit&lt;/code&gt; entitlement. Update the app&#39;s bundle identifier to match your provisioning profile with the required capability.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;After successfully launching the app, copy the exported ExecuTorch model (&lt;code&gt;.pte&lt;/code&gt;) and tokenizer (&lt;code&gt;.model&lt;/code&gt;) files to the iLLaMA folder. You can find the model file called &lt;code&gt;llama3.1.pte&lt;/code&gt; in the current &lt;code&gt;torchchat&lt;/code&gt; directory and the tokenizer file at &lt;code&gt;$(python3 torchchat.py where llama3.1)/tokenizer.model&lt;/code&gt; path.&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;For the Simulator:&lt;/strong&gt; Drag and drop both files onto the Simulator window and save them in the &lt;code&gt;On My iPhone &amp;gt; iLLaMA&lt;/code&gt; folder.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;For a device:&lt;/strong&gt; Open a separate Finder window, navigate to the Files tab, drag and drop both files into the iLLaMA folder, and wait for the copying to finish.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Follow the app&#39;s UI guidelines to select the model and tokenizer files from the local filesystem and issue a prompt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;em&gt;Click the image below to see it in action!&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://pytorch.org/executorch/main/_static/img/llama_ios_app.mp4&#34;&gt; &lt;img src=&#34;https://pytorch.org/executorch/main/_static/img/llama_ios_app.png&#34; width=&#34;600&#34; alt=&#34;iOS app running a LlaMA model&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Deploy and run on Android&lt;/h3&gt; &#xA;&lt;p&gt;The following assumes you&#39;ve completed the steps for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/#set-up-executorch&#34;&gt;Setting up ExecuTorch&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Approach 1 (Recommended): Android Studio&lt;/summary&gt; &#xA; &lt;h4&gt;Requirements&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Android Studio&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://developer.android.com/build/jdks&#34;&gt;Java 17&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://developer.android.com/about/versions/14/setup-sdk&#34;&gt;Android SDK 34&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://developer.android.com/tools/adb&#34;&gt;adb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Steps&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;Download the AAR file, which contains the Java library and corresponding JNI library, to build and run the app.&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://ossci-android.s3.amazonaws.com/executorch/main/executorch-llama-tiktoken-rc3-0719.aar&#34;&gt;executorch-llama-tiktoken-rc3-0719.aar&lt;/a&gt; (SHASUM: c3e5d2a97708f033c2b1839a89f12f737e3bbbef)&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Rename the downloaded AAR file to &lt;code&gt;executorch.aar&lt;/code&gt; and move the file to &lt;code&gt;android/torchchat/app/libs/&lt;/code&gt;. You may need to create directory &lt;code&gt;android/torchchat/app/libs/&lt;/code&gt; if it does not exist.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Push the model and tokenizer file to your device. You can find the model file called &lt;code&gt;llama3.1.pte&lt;/code&gt; in the current &lt;code&gt;torchchat&lt;/code&gt; directory and the tokenizer file at &lt;code&gt;$(python3 torchchat.py where llama3.1)/tokenizer.model&lt;/code&gt; path.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;adb shell mkdir -p /data/local/tmp/llama&#xA;adb push &amp;lt;model.pte&amp;gt; /data/local/tmp/llama&#xA;adb push &amp;lt;tokenizer.model or tokenizer.bin&amp;gt; /data/local/tmp/llama&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Use Android Studio to open the torchchat app skeleton, located at &lt;code&gt;android/torchchat&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Click the Play button (^R) to launch it to emulator/device.&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;We recommend using a device with at least 12GB RAM and 20GB storage.&lt;/li&gt; &#xA;    &lt;li&gt;If using an emulated device, refer to &lt;a href=&#34;https://stackoverflow.com/questions/45517553/cant-change-the-ram-size-in-avd-manager-android-studio&#34;&gt;this post&lt;/a&gt; on how to set the RAM.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;Follow the app&#39;s UI guidelines to pick the model and tokenizer files from the local filesystem. Then issue a prompt.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The AAR file listed in Step 1 has the tiktoken tokenizer, which is used for Llama 3. To tweak or use a custom tokenizer and runtime, modify the ExecuTorch code and use &lt;a href=&#34;https://github.com/pytorch/executorch/raw/main/build/build_android_llm_demo.sh&#34;&gt;this script&lt;/a&gt; to build the AAR library. For convenience, we also provide an AAR for sentencepiece tokenizer (e.g. Llama 2): &lt;a href=&#34;https://ossci-android.s3.amazonaws.com/executorch/main/executorch-llama-bpe-rc3-0719.aar&#34;&gt;executorch-llama-bpe-rc3-0719.aar&lt;/a&gt; (SHASUM: d5fe81d9a4700c36b50ae322e6bf34882134edb0)&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://pytorch.org/executorch/main/_static/img/android_llama_app.png&#34; width=&#34;600&#34; alt=&#34;Android app running a LlaMA model&#34;&gt; &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Approach 2: E2E Script&lt;/summary&gt; &#xA; &lt;p&gt;Alternatively, you can run &lt;code&gt;scripts/android_example.sh&lt;/code&gt; which sets up Java, Android SDK Manager, Android SDK, Android emulator (if no physical device is found), builds the app, and launches it for you. It can be used if you don&#39;t have a GUI.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;export TORCHCHAT_ROOT=$(pwd)&#xA;export USE_TIKTOKEN=ON # Set this only for tiktoken tokenizer&#xA;sh scripts/android_example.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Eval&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: This feature is still a work in progress and not all features are working&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Uses the lm_eval library to evaluate model accuracy on a variety of tasks. Defaults to wikitext and can be manually controlled using the tasks and limit args. See &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/docs/evaluation.md&#34;&gt;Evaluation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Eager mode:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py eval llama3.1 --dtype fp32 --limit 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To test the perplexity for a lowered or quantized model, pass it in the same way you would to generate:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 torchchat.py eval llama3.1 --pte-path llama3.1.pte --limit 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;The following models are supported by torchchat and have associated aliases.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Mobile Friendly&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct&#34;&gt;meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt; . Alias to &lt;code&gt;llama3.1&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B&#34;&gt;meta-llama/Meta-Llama-3.1-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;llama3.1-base&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct&#34;&gt;meta-llama/Meta-Llama-3-8B-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt; . Alias to &lt;code&gt;llama3&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;meta-llama/Meta-Llama-3-8B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;llama3-base&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;meta-llama/Llama-2-7b-chat-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;llama2&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&#34;&gt;meta-llama/Llama-2-13b-chat-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;llama2-13b-chat&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-70b-chat-hf&#34;&gt;meta-llama/Llama-2-70b-chat-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;llama2-70b-chat&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;meta-llama/Llama-2-7b-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;llama2-base&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/CodeLlama-7b-Python-hf&#34;&gt;meta-llama/CodeLlama-7b-Python-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for Python and &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;codellama&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/CodeLlama-34b-Python-hf&#34;&gt;meta-llama/CodeLlama-34b-Python-hf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for Python and &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;codellama-34b&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-v0.1&#34;&gt;mistralai/Mistral-7B-v0.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;mistral-7b-v01-base&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1&#34;&gt;mistralai/Mistral-7B-Instruct-v0.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;mistral-7b-v01-instruct&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2&#34;&gt;mistralai/Mistral-7B-Instruct-v0.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Tuned for &lt;code&gt;chat&lt;/code&gt;. Alias to &lt;code&gt;mistral&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/karpathy/tinyllamas/tree/main&#34;&gt;tinyllamas/stories15M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Toy model for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;stories15M&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/karpathy/tinyllamas/tree/main&#34;&gt;tinyllamas/stories42M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Toy model for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;stories42M&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/karpathy/tinyllamas/tree/main&#34;&gt;tinyllamas/stories110M&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Toy model for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;stories110M&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openlm-research/open_llama_7b&#34;&gt;openlm-research/open_llama_7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;td&gt;Best for &lt;code&gt;generate&lt;/code&gt;. Alias to &lt;code&gt;open-llama&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;While we describe how to use torchchat using the popular llama3 model, you can perform the example commands with any of these models.&lt;/p&gt; &#xA;&lt;h2&gt;Design Principles&lt;/h2&gt; &#xA;&lt;p&gt;torchchat embodies PyTorch’s design philosophy &lt;a href=&#34;https://pytorch.org/docs/stable/community/design.html&#34;&gt;details&lt;/a&gt;, especially &#34;usability over everything else&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;Native PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;torchchat is a native-PyTorch library. While we provide integrations with the surrounding ecosystem (eg: Hugging Face models, etc), all of the core functionality is written in PyTorch.&lt;/p&gt; &#xA;&lt;h3&gt;Simplicity and Extensibility&lt;/h3&gt; &#xA;&lt;p&gt;torchchat is designed to be easy to understand, use and extend.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Composition over implementation inheritance - layers of inheritance for code re-use makes the code hard to read and extend&lt;/li&gt; &#xA; &lt;li&gt;No training frameworks - explicitly outlining the training logic makes it easy to extend for custom use cases&lt;/li&gt; &#xA; &lt;li&gt;Code duplication is preferred over unnecessary abstractions&lt;/li&gt; &#xA; &lt;li&gt;Modular building blocks over monolithic components&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Correctness&lt;/h3&gt; &#xA;&lt;p&gt;torchchat provides well-tested components with a high-bar on correctness. We provide&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extensive unit-tests to ensure things operate as they should&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We really value our community and the contributions made by our wonderful users. We&#39;ll use this section to call out some of these contributions! If you&#39;d like to help out as well, please see the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;A section of commonly encountered setup errors/exceptions. If this section doesn&#39;t contain your situation, check the GitHub &lt;a href=&#34;https://github.com/pytorch/torchchat/issues&#34;&gt;issues&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Model Access&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Access to model is restricted and you are not in the authorized list&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Some models require an additional step to access. Follow the link provided in the error to get access.&lt;/p&gt; &#xA;&lt;h3&gt;Installing ExecuTorch&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Failed Building Wheel&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If &lt;code&gt;./scripts/install_et.sh&lt;/code&gt; fails with an error like &lt;code&gt;Building wheel for executorch (pyproject.toml) did not run successfully&lt;/code&gt; It&#39;s possible that it&#39;s linking to an older version of pytorch installed some other way like via homebrew. You can break the link by uninstalling other versions such as &lt;code&gt;brew uninstall pytorch&lt;/code&gt; Note: You may break something that depends on this, so be aware.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;CERTIFICATE_VERIFY_FAILED&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Run &lt;code&gt;pip install --upgrade certifi&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Filing Issues&lt;/h2&gt; &#xA;&lt;p&gt;If you encounter bugs or difficulty using torchchat, please file an GitHub &lt;a href=&#34;https://github.com/pytorch/torchchat/issues&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please include the exact command you ran and the output of that command. Also, run this script and include the output saved to &lt;code&gt;system_info.txt&lt;/code&gt; so that we can better debug your issue.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;(echo &#34;Operating System Information&#34;; uname -a; echo &#34;&#34;; cat /etc/os-release; echo &#34;&#34;; echo &#34;Python Version&#34;; python --version || python3 --version; echo &#34;&#34;; echo &#34;PIP Version&#34;; pip --version || pip3 --version; echo &#34;&#34;; echo &#34;Installed Packages&#34;; pip freeze || pip3 freeze; echo &#34;&#34;; echo &#34;PyTorch Version&#34;; python -c &#34;import torch; print(torch.__version__)&#34; || python3 -c &#34;import torch; print(torch.__version__)&#34;; echo &#34;&#34;; echo &#34;Collection Complete&#34;) &amp;gt; system_info.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The torchchat Repository Content is provided without any guarantees about performance or compatibility. In particular, torchchat makes available model architectures written in Python for PyTorch that may not perform in the same manner or meet the same standards as the original versions of those models. When using the torchchat Repository Content, including any model architectures, you are solely responsible for determining the appropriateness of using or redistributing the torchchat Repository Content and assume any risks associated with your use of the torchchat Repository Content or any models, outputs, or results, both alone and in combination with any other technologies. Additionally, you may have other legal obligations that govern your use of other content, such as the terms of service for third-party models, weights, data, or other technologies, and you are solely responsible for complying with all such obligations.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Thank you to the community for all the awesome libraries and tools you&#39;ve built around local LLM inference.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Georgi Gerganov and his &lt;a href=&#34;https://github.com/ggerganov/ggml&#34;&gt;GGML&lt;/a&gt; project shining a spotlight on community-based enablement and inspiring so many other projects.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Andrej Karpathy and his &lt;a href=&#34;https://github.com/karpathy/llama2.c&#34;&gt;llama2.c&lt;/a&gt; project. So many great (and simple!) ideas in llama2.c that we have directly adopted (both ideas and code) from his repo. You can never go wrong by following Andrej&#39;s work.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Michael Gschwind, Bert Maher, Scott Wolchok, Bin Bao, Chen Yang, Huamin Li and Mu-Chu Li who built the first version of nanogpt (&lt;code&gt;DSOGPT&lt;/code&gt;) with AOT Inductor proving that AOTI can be used to build efficient LLMs, and DSOs are a viable distribution format for models. &lt;a href=&#34;https://github.com/karpathy/nanoGPT&#34;&gt;nanoGPT&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bert Maher and his &lt;a href=&#34;https://github.com/bertmaher/llama2.so&#34;&gt;llama2.so&lt;/a&gt;, which built on Andrej&#39;s llama2.c and on DSOGPT to close the loop on Llama models with AOTInductor.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Christian Puhrsch, Horace He, Joe Isaacson and many more for their many contributions in Accelerating GenAI models in the &lt;em&gt;&#34;Anything, Fast!&#34;&lt;/em&gt; pytorch.org blogs, and, in particular, Horace He for &lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast&#34;&gt;GPT, Fast!&lt;/a&gt;, which we have directly adopted (both ideas and code) from his repo.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;torchchat is released under the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchchat/main/LICENSE&#34;&gt;BSD 3 license&lt;/a&gt;. (Additional code in this distribution is covered by the MIT and Apache Open Source licenses.) However you may have other legal obligations that govern your use of content, such as the terms of service for third-party models.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>getredash/redash</title>
    <updated>2024-08-03T01:27:44Z</updated>
    <id>tag:github.com,2024-08-03:/getredash/redash</id>
    <link href="https://github.com/getredash/redash" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make Your Company Data Driven. Connect to any data source, easily visualize, dashboard and share your data.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img title=&#34;Redash&#34; src=&#34;https://redash.io/assets/images/logo.png&#34; width=&#34;200px&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://redash.io/help/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-redash.io/help-brightgreen.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/getredash/redash/actions&#34;&gt;&lt;img src=&#34;https://github.com/getredash/redash/actions/workflows/ci.yml/badge.svg?sanitize=true&#34; alt=&#34;GitHub Build&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Redash is designed to enable anyone, regardless of the level of technical sophistication, to harness the power of data big and small. SQL users leverage Redash to explore, query, visualize, and share data from any data sources. Their work in turn enables anybody in their organization to use the data. Every day, millions of users at thousands of organizations around the world use Redash to develop insights and make data-driven decisions.&lt;/p&gt; &#xA;&lt;p&gt;Redash features:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Browser-based&lt;/strong&gt;: Everything in your browser, with a shareable URL.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ease-of-use&lt;/strong&gt;: Become immediately productive with data without the need to master complex software.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Query editor&lt;/strong&gt;: Quickly compose SQL and NoSQL queries with a schema browser and auto-complete.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Visualization and dashboards&lt;/strong&gt;: Create &lt;a href=&#34;https://redash.io/help/user-guide/visualizations/visualization-types&#34;&gt;beautiful visualizations&lt;/a&gt; with drag and drop, and combine them into a single dashboard.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sharing&lt;/strong&gt;: Collaborate easily by sharing visualizations and their associated queries, enabling peer review of reports and queries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Schedule refreshes&lt;/strong&gt;: Automatically update your charts and dashboards at regular intervals you define.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Alerts&lt;/strong&gt;: Define conditions and be alerted instantly when your data changes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;REST API&lt;/strong&gt;: Everything that can be done in the UI is also available through REST API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Broad support for data sources&lt;/strong&gt;: Extensible data source API with native support for a long list of common databases and platforms.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/getredash/website/8e820cd02c73a8ddf4f946a9d293c54fd3fb08b9/website/_assets/images/redash-anim.gif&#34; width=&#34;80%&#34;&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://redash.io/help/open-source/setup&#34;&gt;Setting up Redash instance&lt;/a&gt; (includes links to ready-made AWS/GCE images).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://redash.io/help/&#34;&gt;Documentation&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Supported Data Sources&lt;/h2&gt; &#xA;&lt;p&gt;Redash supports more than 35 SQL and NoSQL &lt;a href=&#34;https://redash.io/help/data-sources/supported-data-sources&#34;&gt;data sources&lt;/a&gt;. It can also be extended to support more. Below is a list of built-in sources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amazon Athena&lt;/li&gt; &#xA; &lt;li&gt;Amazon CloudWatch / Insights&lt;/li&gt; &#xA; &lt;li&gt;Amazon DynamoDB&lt;/li&gt; &#xA; &lt;li&gt;Amazon Redshift&lt;/li&gt; &#xA; &lt;li&gt;ArangoDB&lt;/li&gt; &#xA; &lt;li&gt;Axibase Time Series Database&lt;/li&gt; &#xA; &lt;li&gt;Apache Cassandra&lt;/li&gt; &#xA; &lt;li&gt;ClickHouse&lt;/li&gt; &#xA; &lt;li&gt;CockroachDB&lt;/li&gt; &#xA; &lt;li&gt;Couchbase&lt;/li&gt; &#xA; &lt;li&gt;CSV&lt;/li&gt; &#xA; &lt;li&gt;Databricks&lt;/li&gt; &#xA; &lt;li&gt;DB2 by IBM&lt;/li&gt; &#xA; &lt;li&gt;Dgraph&lt;/li&gt; &#xA; &lt;li&gt;Apache Drill&lt;/li&gt; &#xA; &lt;li&gt;Apache Druid&lt;/li&gt; &#xA; &lt;li&gt;e6data&lt;/li&gt; &#xA; &lt;li&gt;Eccenca Corporate Memory&lt;/li&gt; &#xA; &lt;li&gt;Elasticsearch&lt;/li&gt; &#xA; &lt;li&gt;Exasol&lt;/li&gt; &#xA; &lt;li&gt;Microsoft Excel&lt;/li&gt; &#xA; &lt;li&gt;Firebolt&lt;/li&gt; &#xA; &lt;li&gt;Databend&lt;/li&gt; &#xA; &lt;li&gt;Google Analytics&lt;/li&gt; &#xA; &lt;li&gt;Google BigQuery&lt;/li&gt; &#xA; &lt;li&gt;Google Spreadsheets&lt;/li&gt; &#xA; &lt;li&gt;Graphite&lt;/li&gt; &#xA; &lt;li&gt;Greenplum&lt;/li&gt; &#xA; &lt;li&gt;Apache Hive&lt;/li&gt; &#xA; &lt;li&gt;Apache Impala&lt;/li&gt; &#xA; &lt;li&gt;InfluxDB&lt;/li&gt; &#xA; &lt;li&gt;InfluxDBv2&lt;/li&gt; &#xA; &lt;li&gt;IBM Netezza Performance Server&lt;/li&gt; &#xA; &lt;li&gt;JIRA (JQL)&lt;/li&gt; &#xA; &lt;li&gt;JSON&lt;/li&gt; &#xA; &lt;li&gt;Apache Kylin&lt;/li&gt; &#xA; &lt;li&gt;OmniSciDB (Formerly MapD)&lt;/li&gt; &#xA; &lt;li&gt;MariaDB&lt;/li&gt; &#xA; &lt;li&gt;MemSQL&lt;/li&gt; &#xA; &lt;li&gt;Microsoft Azure Data Warehouse / Synapse&lt;/li&gt; &#xA; &lt;li&gt;Microsoft Azure SQL Database&lt;/li&gt; &#xA; &lt;li&gt;Microsoft Azure Data Explorer / Kusto&lt;/li&gt; &#xA; &lt;li&gt;Microsoft SQL Server&lt;/li&gt; &#xA; &lt;li&gt;MongoDB&lt;/li&gt; &#xA; &lt;li&gt;MySQL&lt;/li&gt; &#xA; &lt;li&gt;Oracle&lt;/li&gt; &#xA; &lt;li&gt;Apache Phoenix&lt;/li&gt; &#xA; &lt;li&gt;Apache Pinot&lt;/li&gt; &#xA; &lt;li&gt;PostgreSQL&lt;/li&gt; &#xA; &lt;li&gt;Presto&lt;/li&gt; &#xA; &lt;li&gt;Prometheus&lt;/li&gt; &#xA; &lt;li&gt;Python&lt;/li&gt; &#xA; &lt;li&gt;Qubole&lt;/li&gt; &#xA; &lt;li&gt;Rockset&lt;/li&gt; &#xA; &lt;li&gt;RisingWave&lt;/li&gt; &#xA; &lt;li&gt;Salesforce&lt;/li&gt; &#xA; &lt;li&gt;ScyllaDB&lt;/li&gt; &#xA; &lt;li&gt;Shell Scripts&lt;/li&gt; &#xA; &lt;li&gt;Snowflake&lt;/li&gt; &#xA; &lt;li&gt;SPARQL&lt;/li&gt; &#xA; &lt;li&gt;SQLite&lt;/li&gt; &#xA; &lt;li&gt;TiDB&lt;/li&gt; &#xA; &lt;li&gt;Tinybird&lt;/li&gt; &#xA; &lt;li&gt;TreasureData&lt;/li&gt; &#xA; &lt;li&gt;Trino&lt;/li&gt; &#xA; &lt;li&gt;Uptycs&lt;/li&gt; &#xA; &lt;li&gt;Vertica&lt;/li&gt; &#xA; &lt;li&gt;Yandex AppMetrrica&lt;/li&gt; &#xA; &lt;li&gt;Yandex Metrica&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Help&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Issues: &lt;a href=&#34;https://github.com/getredash/redash/issues&#34;&gt;https://github.com/getredash/redash/issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Discussion Forum: &lt;a href=&#34;https://github.com/getredash/redash/discussions/&#34;&gt;https://github.com/getredash/redash/discussions/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Development Discussion: &lt;a href=&#34;https://discord.gg/tN5MdmfGBp&#34;&gt;https://discord.gg/tN5MdmfGBp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Reporting Bugs and Contributing Code&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Want to report a bug or request a feature? Please open &lt;a href=&#34;https://github.com/getredash/redash/issues/new&#34;&gt;an issue&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Want to help us build &lt;strong&gt;&lt;em&gt;Redash&lt;/em&gt;&lt;/strong&gt;? Fork the project, edit in a &lt;a href=&#34;https://github.com/getredash/redash/wiki/Local-development-setup&#34;&gt;dev environment&lt;/a&gt; and make a pull request. We need all the help we can get!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;Please email &lt;a href=&#34;mailto:security@redash.io&#34;&gt;security@redash.io&lt;/a&gt; to report any security vulnerabilities. We will acknowledge receipt of your vulnerability and strive to send you regular updates about our progress. If you&#39;re curious about the status of your disclosure please feel free to email us again. If you want to encrypt your disclosure email, you can use &lt;a href=&#34;https://keybase.io/arikfr/key.asc&#34;&gt;this PGP key&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;BSD-2-Clause.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>projectdiscovery/nuclei</title>
    <updated>2024-08-03T01:27:44Z</updated>
    <id>tag:github.com,2024-08-03:/projectdiscovery/nuclei</id>
    <link href="https://github.com/projectdiscovery/nuclei" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast and customizable vulnerability scanner based on simple YAML based DSL.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;br&gt; &lt;a href=&#34;https://nuclei.projectdiscovery.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/projectdiscovery/nuclei/dev/static/nuclei-logo.png&#34; width=&#34;200px&#34; alt=&#34;Nuclei&#34;&gt;&lt;/a&gt; &lt;/h1&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt;Fast and customisable vulnerability scanner based on simple YAML based DSL.&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/go-mod/go-version/projectdiscovery/nuclei&#34;&gt; &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/downloads/projectdiscovery/nuclei/total&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/projectdiscovery/nuclei/graphs/contributors&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/contributors-anon/projectdiscovery/nuclei&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/projectdiscovery/nuclei/releases/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/projectdiscovery/nuclei&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/projectdiscovery/nuclei/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-raw/projectdiscovery/nuclei&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/projectdiscovery/nuclei/discussions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/discussions/projectdiscovery/nuclei&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://discord.gg/projectdiscovery&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/695645237418131507.svg?logo=discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/pdnuclei&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/pdnuclei.svg?logo=twitter&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/projectdiscovery/nuclei/dev/#how-it-works&#34;&gt;How&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/projectdiscovery/nuclei/dev/#install-nuclei&#34;&gt;Install&lt;/a&gt; • &lt;a href=&#34;https://docs.projectdiscovery.io/tools/nuclei/&#34;&gt;Documentation&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/projectdiscovery/nuclei/dev/#credits&#34;&gt;Credits&lt;/a&gt; • &lt;a href=&#34;https://docs.projectdiscovery.io/tools/nuclei/faq&#34;&gt;FAQs&lt;/a&gt; • &lt;a href=&#34;https://discord.gg/projectdiscovery&#34;&gt;Join Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/raw/main/README.md&#34;&gt;English&lt;/a&gt; • &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/raw/main/README_CN.md&#34;&gt;中文&lt;/a&gt; • &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/raw/main/README_KR.md&#34;&gt;Korean&lt;/a&gt; • &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/raw/main/README_ID.md&#34;&gt;Indonesia&lt;/a&gt; • &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/raw/main/README_ES.md&#34;&gt;Spanish&lt;/a&gt; • &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/raw/main/README_JP.md&#34;&gt;日本語&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Nuclei is used to send requests across targets based on a template, leading to zero false positives and providing fast scanning on a large number of hosts. Nuclei offers scanning for a variety of protocols, including TCP, DNS, HTTP, SSL, File, Whois, Websocket, Headless, Code etc. With powerful and flexible templating, Nuclei can be used to model all kinds of security checks.&lt;/p&gt; &#xA;&lt;p&gt;We have a &lt;a href=&#34;https://github.com/projectdiscovery/nuclei-templates&#34;&gt;dedicated repository&lt;/a&gt; that houses various type of vulnerability templates contributed by &lt;strong&gt;more than 300&lt;/strong&gt; security researchers and engineers.&lt;/p&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/projectdiscovery/nuclei/dev/static/nuclei-flow.jpg&#34; alt=&#34;nuclei-flow&#34; width=&#34;700px&#34;&gt; &lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;span&gt;❗&lt;/span&gt; &lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;This project is in active development&lt;/strong&gt;. Expect breaking changes with releases. Review the release changelog before updating.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;This project was primarily built to be used as a standalone CLI tool. &lt;strong&gt;Running nuclei as a service may pose security risks.&lt;/strong&gt; It&#39;s recommended to use with caution and additional security measures.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Install Nuclei&lt;/h1&gt; &#xA;&lt;p&gt;Nuclei requires &lt;strong&gt;go1.21&lt;/strong&gt; to install successfully. Run the following command to install the latest version -&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;go install -v github.com/projectdiscovery/nuclei/v3/cmd/nuclei@latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Brew&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;brew install nuclei&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Docker&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker pull projectdiscovery/nuclei:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;More installation &lt;a href=&#34;https://docs.projectdiscovery.io/tools/nuclei/install&#34;&gt;methods can be found here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;h3&gt;Nuclei Templates&lt;/h3&gt; &lt;p&gt;Nuclei has built-in support for automatic template download/update as default since version &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/releases/tag/v2.5.2&#34;&gt;v2.5.2&lt;/a&gt;. &lt;a href=&#34;https://github.com/projectdiscovery/nuclei-templates&#34;&gt;&lt;strong&gt;Nuclei-Templates&lt;/strong&gt;&lt;/a&gt; project provides a community-contributed list of ready-to-use templates that is constantly updated.&lt;/p&gt; &lt;p&gt;You may still use the &lt;code&gt;update-templates&lt;/code&gt; flag to update the nuclei templates at any time; You can write your own checks for your individual workflow and needs following Nuclei&#39;s &lt;a href=&#34;https://docs.projectdiscovery.io/templates/&#34;&gt;templating guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The YAML DSL reference syntax is available &lt;a href=&#34;https://raw.githubusercontent.com/projectdiscovery/nuclei/dev/SYNTAX-REFERENCE.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;nuclei -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will display help for the tool. Here are all the switches it supports.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Nuclei is a fast, template based vulnerability scanner focusing&#xA;on extensive configurability, massive extensibility and ease of use.&#xA;&#xA;Usage:&#xA;  ./nuclei [flags]&#xA;&#xA;Flags:&#xA;TARGET:&#xA;   -u, -target string[]          target URLs/hosts to scan&#xA;   -l, -list string              path to file containing a list of target URLs/hosts to scan (one per line)&#xA;   -eh, -exclude-hosts string[]  hosts to exclude to scan from the input list (ip, cidr, hostname)&#xA;   -resume string                resume scan using resume.cfg (clustering will be disabled)&#xA;   -sa, -scan-all-ips            scan all the IP&#39;s associated with dns record&#xA;   -iv, -ip-version string[]     IP version to scan of hostname (4,6) - (default 4)&#xA;&#xA;TARGET-FORMAT:&#xA;   -im, -input-mode string        mode of input file (list, burp, jsonl, yaml, openapi, swagger) (default &#34;list&#34;)&#xA;   -ro, -required-only            use only required fields in input format when generating requests&#xA;   -sfv, -skip-format-validation  skip format validation (like missing vars) when parsing input file&#xA;&#xA;TEMPLATES:&#xA;   -nt, -new-templates                    run only new templates added in latest nuclei-templates release&#xA;   -ntv, -new-templates-version string[]  run new templates added in specific version&#xA;   -as, -automatic-scan                   automatic web scan using wappalyzer technology detection to tags mapping&#xA;   -t, -templates string[]                list of template or template directory to run (comma-separated, file)&#xA;   -turl, -template-url string[]          template url or list containing template urls to run (comma-separated, file)&#xA;   -w, -workflows string[]                list of workflow or workflow directory to run (comma-separated, file)&#xA;   -wurl, -workflow-url string[]          workflow url or list containing workflow urls to run (comma-separated, file)&#xA;   -validate                              validate the passed templates to nuclei&#xA;   -nss, -no-strict-syntax                disable strict syntax check on templates&#xA;   -td, -template-display                 displays the templates content&#xA;   -tl                                    list all available templates&#xA;   -tgl                                   list all available tags&#xA;   -sign                                  signs the templates with the private key defined in NUCLEI_SIGNATURE_PRIVATE_KEY env variable&#xA;   -code                                  enable loading code protocol-based templates&#xA;   -dut, -disable-unsigned-templates      disable running unsigned templates or templates with mismatched signature&#xA;&#xA;FILTERING:&#xA;   -a, -author string[]               templates to run based on authors (comma-separated, file)&#xA;   -tags string[]                     templates to run based on tags (comma-separated, file)&#xA;   -etags, -exclude-tags string[]     templates to exclude based on tags (comma-separated, file)&#xA;   -itags, -include-tags string[]     tags to be executed even if they are excluded either by default or configuration&#xA;   -id, -template-id string[]         templates to run based on template ids (comma-separated, file, allow-wildcard)&#xA;   -eid, -exclude-id string[]         templates to exclude based on template ids (comma-separated, file)&#xA;   -it, -include-templates string[]   path to template file or directory to be executed even if they are excluded either by default or configuration&#xA;   -et, -exclude-templates string[]   path to template file or directory to exclude (comma-separated, file)&#xA;   -em, -exclude-matchers string[]    template matchers to exclude in result&#xA;   -s, -severity value[]              templates to run based on severity. Possible values: info, low, medium, high, critical, unknown&#xA;   -es, -exclude-severity value[]     templates to exclude based on severity. Possible values: info, low, medium, high, critical, unknown&#xA;   -pt, -type value[]                 templates to run based on protocol type. Possible values: dns, file, http, headless, tcp, workflow, ssl, websocket, whois, code, javascript&#xA;   -ept, -exclude-type value[]        templates to exclude based on protocol type. Possible values: dns, file, http, headless, tcp, workflow, ssl, websocket, whois, code, javascript&#xA;   -tc, -template-condition string[]  templates to run based on expression condition&#xA;&#xA;OUTPUT:&#xA;   -o, -output string            output file to write found issues/vulnerabilities&#xA;   -sresp, -store-resp           store all request/response passed through nuclei to output directory&#xA;   -srd, -store-resp-dir string  store all request/response passed through nuclei to custom directory (default &#34;output&#34;)&#xA;   -silent                       display findings only&#xA;   -nc, -no-color                disable output content coloring (ANSI escape codes)&#xA;   -j, -jsonl                    write output in JSONL(ines) format&#xA;   -irr, -include-rr -omit-raw   include request/response pairs in the JSON, JSONL, and Markdown outputs (for findings only) [DEPRECATED use -omit-raw] (default true)&#xA;   -or, -omit-raw                omit request/response pairs in the JSON, JSONL, and Markdown outputs (for findings only)&#xA;   -ot, -omit-template           omit encoded template in the JSON, JSONL output&#xA;   -nm, -no-meta                 disable printing result metadata in cli output&#xA;   -ts, -timestamp               enables printing timestamp in cli output&#xA;   -rdb, -report-db string       nuclei reporting database (always use this to persist report data)&#xA;   -ms, -matcher-status          display match failure status&#xA;   -me, -markdown-export string  directory to export results in markdown format&#xA;   -se, -sarif-export string     file to export results in SARIF format&#xA;   -je, -json-export string      file to export results in JSON format&#xA;   -jle, -jsonl-export string    file to export results in JSONL(ine) format&#xA;&#xA;CONFIGURATIONS:&#xA;   -config string                        path to the nuclei configuration file&#xA;   -tp, -profile string                  template profile config file to run&#xA;   -tpl, -profile-list                   list community template profiles&#xA;   -fr, -follow-redirects                enable following redirects for http templates&#xA;   -fhr, -follow-host-redirects          follow redirects on the same host&#xA;   -mr, -max-redirects int               max number of redirects to follow for http templates (default 10)&#xA;   -dr, -disable-redirects               disable redirects for http templates&#xA;   -rc, -report-config string            nuclei reporting module configuration file&#xA;   -H, -header string[]                  custom header/cookie to include in all http request in header:value format (cli, file)&#xA;   -V, -var value                        custom vars in key=value format&#xA;   -r, -resolvers string                 file containing resolver list for nuclei&#xA;   -sr, -system-resolvers                use system DNS resolving as error fallback&#xA;   -dc, -disable-clustering              disable clustering of requests&#xA;   -passive                              enable passive HTTP response processing mode&#xA;   -fh2, -force-http2                    force http2 connection on requests&#xA;   -ev, -env-vars                        enable environment variables to be used in template&#xA;   -cc, -client-cert string              client certificate file (PEM-encoded) used for authenticating against scanned hosts&#xA;   -ck, -client-key string               client key file (PEM-encoded) used for authenticating against scanned hosts&#xA;   -ca, -client-ca string                client certificate authority file (PEM-encoded) used for authenticating against scanned hosts&#xA;   -sml, -show-match-line                show match lines for file templates, works with extractors only&#xA;   -ztls                                 use ztls library with autofallback to standard one for tls13 [Deprecated] autofallback to ztls is enabled by default&#xA;   -sni string                           tls sni hostname to use (default: input domain name)&#xA;   -dka, -dialer-keep-alive value        keep-alive duration for network requests.&#xA;   -lfa, -allow-local-file-access        allows file (payload) access anywhere on the system&#xA;   -lna, -restrict-local-network-access  blocks connections to the local / private network&#xA;   -i, -interface string                 network interface to use for network scan&#xA;   -at, -attack-type string              type of payload combinations to perform (batteringram,pitchfork,clusterbomb)&#xA;   -sip, -source-ip string               source ip address to use for network scan&#xA;   -rsr, -response-size-read int         max response size to read in bytes&#xA;   -rss, -response-size-save int         max response size to read in bytes (default 1048576)&#xA;   -reset                                reset removes all nuclei configuration and data files (including nuclei-templates)&#xA;   -tlsi, -tls-impersonate               enable experimental client hello (ja3) tls randomization&#xA;   -hae, -http-api-endpoint string       experimental http api endpoint&#xA;&#xA;INTERACTSH:&#xA;   -iserver, -interactsh-server string  interactsh server url for self-hosted instance (default: oast.pro,oast.live,oast.site,oast.online,oast.fun,oast.me)&#xA;   -itoken, -interactsh-token string    authentication token for self-hosted interactsh server&#xA;   -interactions-cache-size int         number of requests to keep in the interactions cache (default 5000)&#xA;   -interactions-eviction int           number of seconds to wait before evicting requests from cache (default 60)&#xA;   -interactions-poll-duration int      number of seconds to wait before each interaction poll request (default 5)&#xA;   -interactions-cooldown-period int    extra time for interaction polling before exiting (default 5)&#xA;   -ni, -no-interactsh                  disable interactsh server for OAST testing, exclude OAST based templates&#xA;&#xA;FUZZING:&#xA;   -ft, -fuzzing-type string     overrides fuzzing type set in template (replace, prefix, postfix, infix)&#xA;   -fm, -fuzzing-mode string     overrides fuzzing mode set in template (multiple, single)&#xA;   -fuzz                         enable loading fuzzing templates (Deprecated: use -dast instead)&#xA;   -dast                         enable / run dast (fuzz) nuclei templates&#xA;   -dfp, -display-fuzz-points    display fuzz points in the output for debugging&#xA;   -fuzz-param-frequency int     frequency of uninteresting parameters for fuzzing before skipping (default 10)&#xA;   -fa, -fuzz-aggression string  fuzzing aggression level controls payload count for fuzz (low, medium, high) (default &#34;low&#34;)&#xA;&#xA;UNCOVER:&#xA;   -uc, -uncover                  enable uncover engine&#xA;   -uq, -uncover-query string[]   uncover search query&#xA;   -ue, -uncover-engine string[]  uncover search engine (shodan,censys,fofa,shodan-idb,quake,hunter,zoomeye,netlas,criminalip,publicwww,hunterhow,google) (default shodan)&#xA;   -uf, -uncover-field string     uncover fields to return (ip,port,host) (default &#34;ip:port&#34;)&#xA;   -ul, -uncover-limit int        uncover results to return (default 100)&#xA;   -ur, -uncover-ratelimit int    override ratelimit of engines with unknown ratelimit (default 60 req/min) (default 60)&#xA;&#xA;RATE-LIMIT:&#xA;   -rl, -rate-limit int               maximum number of requests to send per second (default 150)&#xA;   -rld, -rate-limit-duration value   maximum number of requests to send per second (default 1s)&#xA;   -rlm, -rate-limit-minute int       maximum number of requests to send per minute (DEPRECATED)&#xA;   -bs, -bulk-size int                maximum number of hosts to be analyzed in parallel per template (default 25)&#xA;   -c, -concurrency int               maximum number of templates to be executed in parallel (default 25)&#xA;   -hbs, -headless-bulk-size int      maximum number of headless hosts to be analyzed in parallel per template (default 10)&#xA;   -headc, -headless-concurrency int  maximum number of headless templates to be executed in parallel (default 10)&#xA;   -jsc, -js-concurrency int          maximum number of javascript runtimes to be executed in parallel (default 120)&#xA;   -pc, -payload-concurrency int      max payload concurrency for each template (default 25)&#xA;   -prc, -probe-concurrency int       http probe concurrency with httpx (default 50)&#xA;&#xA;OPTIMIZATIONS:&#xA;   -timeout int                     time to wait in seconds before timeout (default 10)&#xA;   -retries int                     number of times to retry a failed request (default 1)&#xA;   -ldp, -leave-default-ports       leave default HTTP/HTTPS ports (eg. host:80,host:443)&#xA;   -mhe, -max-host-error int        max errors for a host before skipping from scan (default 30)&#xA;   -te, -track-error string[]       adds given error to max-host-error watchlist (standard, file)&#xA;   -nmhe, -no-mhe                   disable skipping host from scan based on errors&#xA;   -project                         use a project folder to avoid sending same request multiple times&#xA;   -project-path string             set a specific project path (default &#34;/tmp&#34;)&#xA;   -spm, -stop-at-first-match       stop processing HTTP requests after the first match (may break template/workflow logic)&#xA;   -stream                          stream mode - start elaborating without sorting the input&#xA;   -ss, -scan-strategy value        strategy to use while scanning(auto/host-spray/template-spray) (default auto)&#xA;   -irt, -input-read-timeout value  timeout on input read (default 3m0s)&#xA;   -nh, -no-httpx                   disable httpx probing for non-url input&#xA;   -no-stdin                        disable stdin processing&#xA;&#xA;HEADLESS:&#xA;   -headless                        enable templates that require headless browser support (root user on Linux will disable sandbox)&#xA;   -page-timeout int                seconds to wait for each page in headless mode (default 20)&#xA;   -sb, -show-browser               show the browser on the screen when running templates with headless mode&#xA;   -ho, -headless-options string[]  start headless chrome with additional options&#xA;   -sc, -system-chrome              use local installed Chrome browser instead of nuclei installed&#xA;   -lha, -list-headless-action      list available headless actions&#xA;&#xA;DEBUG:&#xA;   -debug                    show all requests and responses&#xA;   -dreq, -debug-req         show all sent requests&#xA;   -dresp, -debug-resp       show all received responses&#xA;   -p, -proxy string[]       list of http/socks5 proxy to use (comma separated or file input)&#xA;   -pi, -proxy-internal      proxy all internal requests&#xA;   -ldf, -list-dsl-function  list all supported DSL function signatures&#xA;   -tlog, -trace-log string  file to write sent requests trace log&#xA;   -elog, -error-log string  file to write sent requests error log&#xA;   -version                  show nuclei version&#xA;   -hm, -hang-monitor        enable nuclei hang monitoring&#xA;   -v, -verbose              show verbose output&#xA;   -profile-mem string       optional nuclei memory profile dump file&#xA;   -vv                       display templates loaded for scan&#xA;   -svd, -show-var-dump      show variables dump for debugging&#xA;   -ep, -enable-pprof        enable pprof debugging server&#xA;   -tv, -templates-version   shows the version of the installed nuclei-templates&#xA;   -hc, -health-check        run diagnostic check up&#xA;&#xA;UPDATE:&#xA;   -up, -update                      update nuclei engine to the latest released version&#xA;   -ut, -update-templates            update nuclei-templates to latest released version&#xA;   -ud, -update-template-dir string  custom directory to install / update nuclei-templates&#xA;   -duc, -disable-update-check       disable automatic nuclei/templates update check&#xA;&#xA;STATISTICS:&#xA;   -stats                    display statistics about the running scan&#xA;   -sj, -stats-json          display statistics in JSONL(ines) format&#xA;   -si, -stats-interval int  number of seconds to wait between showing a statistics update (default 5)&#xA;   -mp, -metrics-port int    port to expose nuclei metrics on (default 9092)&#xA;&#xA;CLOUD:&#xA;   -auth                      configure projectdiscovery cloud (pdcp) api key (default true)&#xA;   -cup, -cloud-upload        upload scan results to pdcp dashboard&#xA;   -sid, -scan-id string      upload scan results to existing scan id (optional)&#xA;   -sname, -scan-name string  scan name to set (optional)&#xA;&#xA;AUTHENTICATION:&#xA;   -sf, -secret-file string[]  path to config file containing secrets for nuclei authenticated scan&#xA;   -ps, -prefetch-secrets      prefetch secrets from the secrets file&#xA;&#xA;&#xA;EXAMPLES:&#xA;Run nuclei on single host:&#xA;   $ nuclei -target example.com&#xA;&#xA;Run nuclei with specific template directories:&#xA;   $ nuclei -target example.com -t http/cves/ -t ssl&#xA;&#xA;Run nuclei against a list of hosts:&#xA;   $ nuclei -list hosts.txt&#xA;&#xA;Run nuclei with a JSON output:&#xA;   $ nuclei -target example.com -json-export output.json&#xA;&#xA;Run nuclei with sorted Markdown outputs (with environment variables):&#xA;   $ MARKDOWN_EXPORT_SORT_MODE=template nuclei -target example.com -markdown-export nuclei_report/&#xA;&#xA;Additional documentation is available at: https://docs.nuclei.sh/getting-started/running&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running Nuclei&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://docs.projectdiscovery.io/tools/nuclei/running&#34;&gt;https://docs.projectdiscovery.io/tools/nuclei/running&lt;/a&gt; for details on running Nuclei&lt;/p&gt; &#xA;&lt;h3&gt;Using Nuclei From Go Code&lt;/h3&gt; &#xA;&lt;p&gt;Complete guide of using Nuclei as Library/SDK is available at &lt;a href=&#34;https://pkg.go.dev/github.com/projectdiscovery/nuclei/v3/lib#section-readme&#34;&gt;godoc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Resources&lt;/h3&gt; &#xA;&lt;p&gt;You can access the main documentation for Nuclei at &lt;a href=&#34;https://docs.projectdiscovery.io/tools/nuclei/&#34;&gt;https://docs.projectdiscovery.io/tools/nuclei/&lt;/a&gt;, and learn more about Nuclei in the cloud with &lt;a href=&#34;https://cloud.projectdiscovery.io&#34;&gt;ProjectDiscovery Cloud Platform&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://docs.projectdiscovery.io/tools/nuclei/resources&#34;&gt;https://docs.projectdiscovery.io/tools/nuclei/resources&lt;/a&gt; for more resources and videos about Nuclei!&lt;/p&gt; &#xA;&lt;h3&gt;Credits&lt;/h3&gt; &#xA;&lt;p&gt;Thanks to all the amazing &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/graphs/contributors&#34;&gt;community contributors for sending PRs&lt;/a&gt; and keeping this project updated. &lt;span&gt;❤️&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you have an idea or some kind of improvement, you are welcome to contribute and participate in the Project, feel free to send your PR.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=projectdiscovery/nuclei&amp;amp;max=500&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Do also check out the below similar open-source projects that may fit in your workflow:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ffuf/ffuf&#34;&gt;FFuF&lt;/a&gt;, &lt;a href=&#34;https://github.com/ameenmaali/qsfuzz&#34;&gt;Qsfuzz&lt;/a&gt;, &lt;a href=&#34;https://github.com/proabiral/inception&#34;&gt;Inception&lt;/a&gt;, &lt;a href=&#34;https://github.com/hannob/snallygaster&#34;&gt;Snallygaster&lt;/a&gt;, &lt;a href=&#34;https://github.com/Static-Flow/gofingerprint&#34;&gt;Gofingerprint&lt;/a&gt;, &lt;a href=&#34;https://github.com/1N3/Sn1per/tree/master/templates&#34;&gt;Sn1per&lt;/a&gt;, &lt;a href=&#34;https://github.com/google/tsunami-security-scanner&#34;&gt;Google tsunami&lt;/a&gt;, &lt;a href=&#34;https://github.com/jaeles-project/jaeles&#34;&gt;Jaeles&lt;/a&gt;, &lt;a href=&#34;https://github.com/michelin/ChopChop&#34;&gt;ChopChop&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;Nuclei is distributed under &lt;a href=&#34;https://github.com/projectdiscovery/nuclei/raw/main/LICENSE.md&#34;&gt;MIT License&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1 align=&#34;left&#34;&gt; &lt;a href=&#34;https://discord.gg/projectdiscovery&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/projectdiscovery/nuclei/dev/static/Join-Discord.png&#34; width=&#34;380&#34; alt=&#34;Join Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.projectdiscovery.io&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/projectdiscovery/nuclei/dev/static/check-nuclei-documentation.png&#34; width=&#34;380&#34; alt=&#34;Check Nuclei Documentation&#34;&gt;&lt;/a&gt; &lt;/h1&gt;</summary>
  </entry>
</feed>