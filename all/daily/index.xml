<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-28T01:22:37Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenDevin/OpenDevin</title>
    <updated>2024-03-28T01:22:37Z</updated>
    <id>tag:github.com,2024-03-28:/OpenDevin/OpenDevin</id>
    <link href="https://github.com/OpenDevin/OpenDevin" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üêö OpenDevin: Code Less, Make More&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;OpenDevin Logo&#34; src=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/logo.png&#34; width=&#34;150&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;OpenDevin: Code Less, Make More&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-green&#34; alt=&#34;License&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenDevin/OpenDevin/assets/38853559/5b1092cc-3554-4357-a279-c2a2e9b352ad&#34;&gt;demo-video.webm&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Mission üéØ&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to OpenDevin, an open-source project aiming to replicate &lt;a href=&#34;https://www.cognition-labs.com/introducing-devin&#34;&gt;Devin&lt;/a&gt;, an autonomous AI software engineer who is capable of executing complex engineering tasks and collaborating actively with users on software development projects. This project aspires to replicate, enhance, and innovate upon Devin through the power of the open-source community.&lt;/p&gt; &#xA;&lt;h2&gt;Work in Progress&lt;/h2&gt; &#xA;&lt;p&gt;OpenDevin is still a work in progress. But you can run the alpha version to see things working end-to-end.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt; &amp;gt;= 3.10&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.npmjs.com/downloading-and-installing-node-js-and-npm&#34;&gt;NodeJS&lt;/a&gt; &amp;gt;= 14.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;First, make sure Docker is running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker ps # this should exit successfully&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then pull our latest image &lt;a href=&#34;https://github.com/opendevin/OpenDevin/pkgs/container/sandbox&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull ghcr.io/opendevin/sandbox:v0.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then start the backend:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#34;...&#34;&#xA;export WORKSPACE_DIR=&#34;/path/to/your/project&#34;&#xA;python -m pip install -r requirements.txt&#xA;uvicorn opendevin.server.listen:app --port 3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then in a second terminal:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd frontend&#xA;npm install&#xA;npm run start -- --port 3001&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll see OpenDevin running at localhost:3001&lt;/p&gt; &#xA;&lt;h3&gt;Picking a Model&lt;/h3&gt; &#xA;&lt;p&gt;We use LiteLLM, so you can run OpenDevin with any foundation model, including OpenAI, Claude, and Gemini. LiteLLM has a &lt;a href=&#34;https://docs.litellm.ai/docs/providers&#34;&gt;full list of providers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To change the model, set the &lt;code&gt;LLM_MODEL&lt;/code&gt; and &lt;code&gt;LLM_API_KEY&lt;/code&gt; environment variables.&lt;/p&gt; &#xA;&lt;p&gt;For example, to run Claude:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LLM_API_KEY=&#34;your-api-key&#34;&#xA;export LLM_MODEL=&#34;claude-3-opus-20240229&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also set the base URL for local/custom models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LLM_BASE_URL=&#34;https://localhost:3000&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And you can customize which embeddings are used for the vector database storage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export LLM_EMBEDDING_MODEL=&#34;llama2&#34; # can be &#34;llama2&#34;, &#34;openai&#34;, &#34;azureopenai&#34;, or &#34;local&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running on the Command Line&lt;/h3&gt; &#xA;&lt;p&gt;You can run OpenDevin from your command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;PYTHONPATH=`pwd` python opendevin/main.py -d ./workspace/ -i 100 -t &#34;Write a bash script that prints &#39;hello world&#39;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ü§î What is &lt;a href=&#34;https://www.cognition-labs.com/introducing-devin&#34;&gt;Devin&lt;/a&gt;?&lt;/h2&gt; &#xA;&lt;p&gt;Devin represents a cutting-edge autonomous agent designed to navigate the complexities of software engineering. It leverages a combination of tools such as a shell, code editor, and web browser, showcasing the untapped potential of LLMs in software development. Our goal is to explore and expand upon Devin&#39;s capabilities, identifying both its strengths and areas for improvement, to guide the progress of open code models.&lt;/p&gt; &#xA;&lt;h2&gt;üêö Why OpenDevin?&lt;/h2&gt; &#xA;&lt;p&gt;The OpenDevin project is born out of a desire to replicate, enhance, and innovate beyond the original Devin model. By engaging the open-source community, we aim to tackle the challenges faced by Code LLMs in practical scenarios, producing works that significantly contribute to the community and pave the way for future advancements.&lt;/p&gt; &#xA;&lt;h2&gt;‚≠êÔ∏è Research Strategy&lt;/h2&gt; &#xA;&lt;p&gt;Achieving full replication of production-grade applications with LLMs is a complex endeavor. Our strategy involves:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Core Technical Research:&lt;/strong&gt; Focusing on foundational research to understand and improve the technical aspects of code generation and handling.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Specialist Abilities:&lt;/strong&gt; Enhancing the effectiveness of core components through data curation, training methods, and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Task Planning:&lt;/strong&gt; Developing capabilities for bug detection, codebase management, and optimization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Evaluation:&lt;/strong&gt; Establishing comprehensive evaluation metrics to better understand and improve our models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üõ† Technology Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Sandboxing Environment:&lt;/strong&gt; Ensuring safe execution of code using technologies like Docker and Kubernetes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frontend Interface:&lt;/strong&gt; Developing user-friendly interfaces for monitoring progress and interacting with Devin, potentially leveraging frameworks like React or creating a VSCode plugin for a more integrated experience.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Next Steps&lt;/h2&gt; &#xA;&lt;p&gt;An MVP demo is urgent for us. Here are the most important things to do:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;UI: a chat interface, a shell demonstrating commands, a browser, etc.&lt;/li&gt; &#xA; &lt;li&gt;Architecture: an agent framework with a stable backend, which can read, write and run simple commands&lt;/li&gt; &#xA; &lt;li&gt;Agent: capable of generating bash scripts, running tests, etc.&lt;/li&gt; &#xA; &lt;li&gt;Evaluation: a minimal evaluation pipeline that is consistent with Devin&#39;s evaluation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After finishing building the MVP, we will move towards research in different topics, including foundation models, specialist capabilities, evaluation, agent studies, etc.&lt;/p&gt; &#xA;&lt;h2&gt;How to Contribute&lt;/h2&gt; &#xA;&lt;p&gt;OpenDevin is a community-driven project, and we welcome contributions from everyone. Whether you&#39;re a developer, a researcher, or simply enthusiastic about advancing the field of software engineering with AI, there are many ways to get involved:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code Contributions:&lt;/strong&gt; Help us develop the core functionalities, frontend interface, or sandboxing solutions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Research and Evaluation:&lt;/strong&gt; Contribute to our understanding of LLMs in software engineering, participate in evaluating the models, or suggest improvements.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feedback and Testing:&lt;/strong&gt; Use the OpenDevin toolset, report bugs, suggest features, or provide feedback on usability.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For details, please check &lt;a href=&#34;https://raw.githubusercontent.com/OpenDevin/OpenDevin/main/CONTRIBUTING.md&#34;&gt;this document&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Join Us&lt;/h2&gt; &#xA;&lt;p&gt;We use Slack to discuss. Feel free to fill in the &lt;a href=&#34;https://forms.gle/758d5p6Ve8r2nxxq6&#34;&gt;form&lt;/a&gt; if you would like to join the Slack organization of OpenDevin. We will reach out shortly if we feel you are a good fit to the current team!&lt;/p&gt; &#xA;&lt;p&gt;Stay updated on OpenDevin&#39;s progress, share your ideas, and collaborate with fellow enthusiasts and experts. Together, we can make significant strides towards simplifying software engineering tasks and creating more efficient, powerful tools for developers everywhere.&lt;/p&gt; &#xA;&lt;p&gt;üêö &lt;strong&gt;Code less, make more with OpenDevin.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#OpenDevin/OpenDevin&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=OpenDevin/OpenDevin&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>jasonppy/VoiceCraft</title>
    <updated>2024-03-28T01:22:37Z</updated>
    <id>tag:github.com,2024-03-28:/jasonppy/VoiceCraft</id>
    <link href="https://github.com/jasonppy/VoiceCraft" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Zero-Shot Speech Editing and Text-to-Speech in the Wild&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://jasonppy.github.io/VoiceCraft_web&#34;&gt;Demo&lt;/a&gt; &lt;a href=&#34;https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;TL;DR&lt;/h3&gt; &#xA;&lt;p&gt;VoiceCraft is a token infilling neural codec language model, that achieves state-of-the-art performance on both &lt;strong&gt;speech editing&lt;/strong&gt; and &lt;strong&gt;zero-shot text-to-speech (TTS)&lt;/strong&gt; on in-the-wild data including audiobooks, internet videos, and podcasts.&lt;/p&gt; &#xA;&lt;p&gt;To clone or edit an unseen voice, VoiceCraft needs only a few seconds of reference.&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;p&gt;The TODOs left will be completed by the end of March 2024.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Codebase upload&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Environment setup&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference demo for speech editing and TTS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training guidance&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Upload the RealEdit dataset and training manifest&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Upload model weights (encodec weights are up)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Environment setup&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n voicecraft python=3.9.16&#xA;conda activate voicecraft&#xA;&#xA;pip install torch==2.0.1 # this assumes your system is compatible with CUDA 11.7, otherwise checkout https://pytorch.org/get-started/previous-versions/#v201&#xA;apt-get install ffmpeg # if you don&#39;t already have ffmpeg installed&#xA;pip install -e git+https://github.com/facebookresearch/audiocraft.git@c5157b5bf14bf83449c17ea1eeb66c19fb4bc7f0#egg=audiocraft&#xA;apt-get install espeak-ng # backend for the phonemizer installed below&#xA;pip install tensorboard==2.16.2&#xA;pip install phonemizer==3.2.1&#xA;pip install torchaudio==2.0.2&#xA;pip install datasets==2.16.0&#xA;pip install torchmetrics==0.11.1&#xA;# install MFA for getting forced-alignment, this could take a few minutes&#xA;conda install -c conda-forge montreal-forced-aligner=2.2.17 openfst=1.8.2 kaldi=5.5.1068&#xA;# conda install pocl # above gives an warning for installing pocl, not sure if really need this&#xA;&#xA;# to run ipynb&#xA;conda install -n voicecraft ipykernel --update-deps --force-reinstall&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have encountered version issues when running things, checkout &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/environment.yml&#34;&gt;environment.yml&lt;/a&gt; for exact matching.&lt;/p&gt; &#xA;&lt;h2&gt;Inference Examples&lt;/h2&gt; &#xA;&lt;p&gt;Checkout &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/inference_speech_editing.ipynb&#34;&gt;&lt;code&gt;inference_speech_editing.ipynb&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/inference_tts.ipynb&#34;&gt;&lt;code&gt;inference_tts.ipynb&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To train an VoiceCraft model, you need to prepare the following parts:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;utterances and their transcripts&lt;/li&gt; &#xA; &lt;li&gt;encode the utterances into codes using e.g. Encodec&lt;/li&gt; &#xA; &lt;li&gt;convert transcripts into phoneme sequence, and a phoneme set (we named it vocab.txt)&lt;/li&gt; &#xA; &lt;li&gt;manifest (i.e. metadata)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Step 1,2,3 are handled in &lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/data/phonemize_encodec_encode_hf.py&#34;&gt;./data/phonemize_encodec_encode_hf.py&lt;/a&gt;, where&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Gigaspeech is downloaded through HuggingFace. Note that you need to sign an agreement in order to download the dataset (it needs your auth token)&lt;/li&gt; &#xA; &lt;li&gt;phoneme sequence and encodec codes are also extracted using the script.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;An example run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate voicecraft&#xA;export CUDA_VISIBLE_DEVICES=0&#xA;cd ./data&#xA;python phonemize_encodec_encode_hf.py \&#xA;--dataset_size xs \&#xA;--download_to path/to/store_huggingface_downloads \&#xA;--save_dir path/to/store_extracted_codes_and_phonemes \&#xA;--encodec_model_path path/to/encodec_model \&#xA;--mega_batch_size 120 \&#xA;--batch_size 32 \&#xA;--max_len 30000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where encodec_model_path is avaliable &lt;a href=&#34;https://huggingface.co/pyp1/VoiceCraft&#34;&gt;here&lt;/a&gt;. This model is trained on Gigaspeech XL, it has 56M parameters, 4 codebooks, each codebook has 2048 codes. Details are described in our &lt;a href=&#34;https://jasonppy.github.io/assets/pdfs/VoiceCraft.pdf&#34;&gt;paper&lt;/a&gt;. If you encounter OOM during extraction, try decrease the batch_size and/or max_len. The extracted codes, phonemes, and vocab.txt will be stored at &lt;code&gt;path/to/store_extracted_codes_and_phonemes/${dataset_size}/{encodec_16khz_4codebooks,phonemes,vocab.txt}&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As for manifest, please download train.txt and validation.txt from &lt;a href=&#34;https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main&#34;&gt;here&lt;/a&gt;, and put them under &lt;code&gt;path/to/store_extracted_codes_and_phonemes/manifest/&lt;/code&gt;. Please also download vocab.txt from &lt;a href=&#34;https://huggingface.co/datasets/pyp1/VoiceCraft_RealEdit/tree/main&#34;&gt;here&lt;/a&gt; if you want to use our pretrained VoiceCraft model (so that the phoneme-to-token matching is the same).&lt;/p&gt; &#xA;&lt;p&gt;Now, you are good to start training!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda activate voicecraft&#xA;cd ./z_scripts&#xA;bash e830M.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The codebase is under CC BY-NC-SA 4.0 (&lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/LICENSE-CODE&#34;&gt;LICENSE-CODE&lt;/a&gt;), and the model weights are under Coqui Public Model License 1.0.0 (&lt;a href=&#34;https://raw.githubusercontent.com/jasonppy/VoiceCraft/master/LICENSE-MODEL&#34;&gt;LICENSE-MODEL&lt;/a&gt;). Note that we use some of the code from other repository that are under different licenses: &lt;code&gt;./models/codebooks_patterns.py&lt;/code&gt; is under MIT license; &lt;code&gt;./models/modules&lt;/code&gt;, &lt;code&gt;./steps/optim.py&lt;/code&gt;, &lt;code&gt;data/tokenizer.py&lt;/code&gt; are under Apache License, Version 2.0; the phonemizer we used is under GNU 3.0 License. For drop-in replacement of the phonemizer (i.e. text to IPA phoneme mapping), try &lt;a href=&#34;https://github.com/roedoejet/g2p&#34;&gt;g2p&lt;/a&gt; (MIT License) or &lt;a href=&#34;https://github.com/NeuralVox/OpenPhonemizer&#34;&gt;OpenPhonemizer&lt;/a&gt; (BSD-3-Clause Clear), although these are not tested.&lt;/p&gt; &#xA;&lt;!-- How to use g2p to convert english text into IPA phoneme sequence&#xA;first install it with `pip install g2p`&#xA;```python&#xA;from g2p import make_g2p&#xA;transducer = make_g2p(&#39;eng&#39;, &#39;eng-ipa&#39;)&#xA;transducer(&#34;hello&#34;).output_string &#xA;# it will output: &#39;h ålo ä&#39;&#xA;``` --&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;We thank Feiteng for his &lt;a href=&#34;https://github.com/lifeiteng/vall-e&#34;&gt;VALL-E reproduction&lt;/a&gt;, and we thank audiocraft team for open-sourcing &lt;a href=&#34;https://github.com/facebookresearch/audiocraft&#34;&gt;encodec&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{peng2024voicecraft,&#xA;  author    = {Peng, Puyuan and Huang, Po-Yao and Li, Daniel and Mohamed, Abdelrahman and Harwath, David},&#xA;  title     = {VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild},&#xA;  journal   = {arXiv},&#xA;  year      = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Any organization or individual is prohibited from using any technology mentioned in this paper to generate or edit someone&#39;s speech without his/her consent, including but not limited to government leaders, political figures, and celebrities. If you do not comply with this item, you could be in violation of copyright laws.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>IDEA-Research/T-Rex</title>
    <updated>2024-03-28T01:22:37Z</updated>
    <id>tag:github.com,2024-03-28:/IDEA-Research/T-Rex</id>
    <link href="https://github.com/IDEA-Research/T-Rex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2/head.jpg&#34; width=&#34;900&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; A picture speaks volumes, as do the words that frame it.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://img.shields.io/badge/T--Rex-2-2&#34; alt=&#34;Static Badge&#34;&gt; &lt;a href=&#34;https://arxiv.org/pdf/2403.14610.pdf&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arxiv_2403.14610-blue%3Flog%3Darxiv&#34; alt=&#34;arXiv preprint&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://deepdataspace.com/home&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/homepage-visit-blue&#34; alt=&#34;Homepage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hits.seeyoufarm.com&#34;&gt;&lt;img src=&#34;https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FMountchicken%2FT-Rex&amp;amp;count_bg=%2379C83D&amp;amp;title_bg=%23DF9B9B&amp;amp;icon=iconify.svg&amp;amp;icon_color=%23FFF9F9&amp;amp;title=VISITORS&amp;amp;edge_flat=false&#34; alt=&#34;Hits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://deepdataspace.com/playground/ivp&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Try_Demo!-blue?logo=chainguard&amp;amp;logoColor=green&#34; alt=&#34;Static Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Mountchicken/T-Rex2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20demo-Gradio-ff7c00&#34; alt=&#34;Gradio demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- Add demo video from youtube --&gt; &#xA;&lt;h1&gt;Introduction Video üé•&lt;/h1&gt; &#xA;&lt;p&gt;Turn on the music if possible üéß&lt;/p&gt; &#xA;&lt;!-- Add a video here --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Mountchicken/Union14M/assets/65173622/60be19f5-88e4-478e-b1a3-af62b8d6d177&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2/video_cover.jpg&#34; alt=&#34;Video Name&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;News üì∞&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-03-27&lt;/strong&gt;: Online Gradio demo hosted at &lt;a href=&#34;https://huggingface.co/spaces/Mountchicken/T-Rex2&#34;&gt;https://huggingface.co/spaces/Mountchicken/T-Rex2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2024-03-26&lt;/strong&gt;: New Gradio demo availableü§ó! Get our API but don&#39;t know how to use it? We are now providing a local Gradio demo that provides GUI interface to support your usage. Check here: &lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#4-local-gradio-demo-with-api%F0%9F%8E%A8&#34;&gt;Gradio APP&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contents üìú&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#introduction-video-&#34;&gt;Introduction Video üé•&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#news-&#34;&gt;News üì∞&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#contents-&#34;&gt;Contents üìú&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#1-introduction-&#34;&gt;1. Introduction üìö&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#what-can-t-rex-do-&#34;&gt;What Can T-Rex Do üìù&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#2-try-demo-&#34;&gt;2. Try Demo üéÆ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#3-api-usage-examples&#34;&gt;3. API Usage Examplesüìö&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#setup&#34;&gt;Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#interactive-visual-prompt-api&#34;&gt;Interactive Visual Prompt API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#generic-visual-prompt-api&#34;&gt;Generic Visual Prompt API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#customize-visual-prompt-embedding-api&#34;&gt;Customize Visual Prompt Embedding API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#embedding-inference-api&#34;&gt;Embedding Inference API&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#4-local-gradio-demo-with-api&#34;&gt;4. Local Gradio Demo with APIüé®&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#41-setup&#34;&gt;4.1. Setup&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#42-run-the-gradio-demo&#34;&gt;4.2. Run the Gradio Demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#43-basic-operations&#34;&gt;4.3. Basic Operations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#5-related-works&#34;&gt;5. Related Works&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/#bibtex-&#34;&gt;BibTeX üìö&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;1. Introduction üìö&lt;/h1&gt; &#xA;&lt;p&gt;Object detection, the ability to locate and identify objects within an image, is a cornerstone of computer vision, pivotal to applications ranging from autonomous driving to content moderation. A notable limitation of traditional object detection models is their closed-set nature. These models are trained on a predetermined set of categories, confining their ability to recognize only those specific categories. The training process itself is arduous, demanding expert knowledge, extensive datasets, and intricate model tuning to achieve desirable accuracy. Moreover, the introduction of a novel object category, exacerbates these challenges, necessitating the entire process to be repeated.&lt;/p&gt; &#xA;&lt;p&gt;T-Rex2 addresses these limitations by integrating both text and visual prompts in one model, thereby harnessing the strengths of both modalities. The synergy of text and visual prompts equips T-Rex2 with robust zero-shot capabilities, making it a versatile tool in the ever-changing landscape of object detection.&lt;/p&gt; &#xA;&lt;!-- insert image in the middle --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2/method.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;What Can T-Rex Do üìù&lt;/h2&gt; &#xA;&lt;p&gt;T-Rex2 is well-suited for a variety of real-world applications, including but not limited to: agriculture, industry, livstock and wild animals monitoring, biology, medicine, OCR, retail, electronics, transportation, logistics, and more. T-Rex2 mainly supports three major workflows including interactive visual prompt workflow, generic visual prompt workflow and text prompt workflow. It can cover most of the application scenarios that require object detection&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Mountchicken/Union14M/assets/65173622/c3585d49-208c-4ba4-9954-fd1572d299dc&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2/video_cover2.png&#34; alt=&#34;Video Name&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;2. Try Demo üéÆ&lt;/h1&gt; &#xA;&lt;p&gt;We are now opening online demo for T-Rex2. &lt;a href=&#34;https://deepdataspace.com/playground/ivp&#34;&gt;Check our demo here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2/demo.jpg&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;3. API Usage Examplesüìö&lt;/h1&gt; &#xA;&lt;p&gt;We are now opening free API access to T-Rex2. For educators, students, and researchers, we offer an API with extensive usage times to support your educational and research endeavors. Please send a request to this email address (&lt;a href=&#34;mailto:weiliu@idea.edu.cn&#34;&gt;weiliu@idea.edu.cn&lt;/a&gt;) and attach your usage purpose as well as your institution.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cloudapi-sdk.deepdataspace.com/dds_cloudapi_sdk/tasks/trex_interactive.html&#34;&gt;Full API documentation can be found here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;p&gt;Install the API package and acquire the API token from the email.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/IDEA-Research/T-Rex.git&#xA;cd T-Rex&#xA;pip install dds-cloudapi-sdk==0.1.1&#xA;pip install -v -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Interactive Visual Prompt API&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;In interactive visual prompt workflow, users can provide visual prompts in boxes or points format on a given image to specify the object to be detected.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python demo_examples/interactive_inference.py --token &amp;lt;your_token&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You are supposed get the following visualization results at &lt;code&gt;demo_vis/&lt;/code&gt; &#xA;    &lt;div align=&#34;center&#34;&gt; &#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2/interactive_0.jpg&#34; width=&#34;400&#34;&gt; &#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2/interactive_1.jpg&#34; height=&#34;285&#34;&gt; &#xA;    &lt;/div&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Generic Visual Prompt API&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;In generic visual prompt workflow, users can provide visual prompts on one reference image and detect on the other image.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python demo_examples/generic_inference.py --token &amp;lt;your_token&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You are supposed get the following visualization results at &lt;code&gt;demo_vis/&lt;/code&gt; &#xA;    &lt;div align=&#34;center&#34;&gt; &#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2_api_examples/generic_prompt1.jpg&#34; width=&#34;280&#34;&gt; + &#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2_api_examples/generic_prompt2.jpg&#34; width=&#34;280&#34;&gt; = &#xA;     &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2/generic.jpg&#34; width=&#34;280&#34;&gt; &#xA;    &lt;/div&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Customize Visual Prompt Embedding API&lt;/h2&gt; &#xA;&lt;p&gt;In this workflow, you cam customize a visual embedding for a object category using multiple images. With this embedding, you can detect on any images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python demo_examples/customize_embedding.py --token &amp;lt;your_token&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You are supposed to get a download link for this visual prompt embedding in &lt;code&gt;safetensors&lt;/code&gt; format. Save it and let&#39;s use it for &lt;code&gt;embedding_inference&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Embedding Inference API&lt;/h2&gt; &#xA;&lt;p&gt;With the visual prompt embeddings generated from the previous API. You can use it detect on any images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;  python demo_examples/embedding_inference.py --token &amp;lt;your_token&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;4. Local Gradio Demo with APIüé®&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/IDEA-Research/T-Rex/trex2/assets/trex2/gradio.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;4.1. Setup&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install T-Rex2 API if you haven&#39;t done so&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;- install gradio and other dependencies&#xA;```bash&#xA;# install gradio and other dependencies&#xA;pip install gradio==4.22.0&#xA;pip install gradio-image-prompter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4.2. Run the Gradio Demo&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python gradio_demo.py --trex2_api_token &amp;lt;your_token&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4.3. Basic Operations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Draw Box&lt;/strong&gt;: Draw a box on the image to specify the object to be detected. Drag the left mouse button to draw a box.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Draw Point&lt;/strong&gt;: Draw a point on the image to specify the object to be detected. Click the left mouse button to draw a point.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Interacive Visual Prompt&lt;/strong&gt;: Provide visual prompts in boxes or points format on a given image to specify the object to be detected. The Input Target Image and Interactive Visual Prompt Image should be the same&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generic Visual Prompt&lt;/strong&gt;: Provide visual prompts on multiple reference images and detect on the other image.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;5. Related Works&lt;/h1&gt; &#xA;&lt;p&gt;&lt;span&gt;üî•&lt;/span&gt; We release the &lt;a href=&#34;https://github.com/UX-Decoder/DINOv&#34;&gt;training and inference code&lt;/a&gt; and &lt;a href=&#34;http://semantic-sam.xyzou.net:6099/&#34;&gt;demo link&lt;/a&gt; of &lt;a href=&#34;https://arxiv.org/pdf/2311.13601.pdf&#34;&gt;DINOv&lt;/a&gt;, which can handle in-context &lt;strong&gt;visual prompts&lt;/strong&gt; for open-set and referring detection &amp;amp; segmentation. Check it out!&lt;/p&gt; &#xA;&lt;h1&gt;BibTeX üìö&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{jiang2024trex2,&#xA;      title={T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy}, &#xA;      author={Qing Jiang and Feng Li and Zhaoyang Zeng and Tianhe Ren and Shilong Liu and Lei Zhang},&#xA;      year={2024},&#xA;      eprint={2403.14610},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>