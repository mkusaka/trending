<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-30T01:29:11Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>livestorejs/livestore</title>
    <updated>2025-05-30T01:29:11Z</updated>
    <id>tag:github.com,2025-05-30:/livestorejs/livestore</id>
    <link href="https://github.com/livestorejs/livestore" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LiveStore is a next-generation state management framework based on reactive SQLite and built-in sync engine.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://share.cleanshot.com/njfQBDqB+&#34; alt=&#34;LiveStore Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What LiveStore does&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üè∞ Provide a powerful data foundation for your app.&lt;/li&gt; &#xA; &lt;li&gt;‚ö° Reactive query layer with full SQLite support.&lt;/li&gt; &#xA; &lt;li&gt;üîå Adapters for most platforms (web, mobile, server/edge, desktop).&lt;/li&gt; &#xA; &lt;li&gt;üìê Flexible data modeling and schema management.&lt;/li&gt; &#xA; &lt;li&gt;üìµ Support true offline-first workflows.&lt;/li&gt; &#xA; &lt;li&gt;üí• Custom merge conflict resolution.&lt;/li&gt; &#xA; &lt;li&gt;üîÑ Sync with a &lt;a href=&#34;https://docs.livestore.dev/reference/syncing/sync-provider/cloudflare/&#34;&gt;supported provider&lt;/a&gt; or roll your own.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/getting-started/react-web/&#34;&gt;React Web&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/getting-started/expo/&#34;&gt;Expo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/getting-started/node/&#34;&gt;Node&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/getting-started/vue/&#34;&gt;Vue&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How LiveStore works&lt;/h2&gt; &#xA;&lt;p&gt;LiveStore is a fully-featured, client-centric data layer (replacing libraries like Redux, MobX, etc.) with a reactive embedded SQLite database powered by real-time sync (via event-sourcing).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://share.cleanshot.com/j1h8Z1P5+&#34; alt=&#34;How LiveStore works&#34;&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Instant, reactive queries to your local SQLite database (via built-in query builder or raw SQL).&lt;/li&gt; &#xA; &lt;li&gt;Data changes are commited to the store, applied instantly and synced across clients.&lt;/li&gt; &#xA; &lt;li&gt;Change events are persisted locally and synced across clients (and across tabs).&lt;/li&gt; &#xA; &lt;li&gt;Events are instantly applied to the local database via materializers.&lt;/li&gt; &#xA; &lt;li&gt;Query results are reactively and synchronously updated in the next render.&lt;/li&gt; &#xA; &lt;li&gt;The LiveStore sync backend propagates changes to all connected clients.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you‚Äôd like to learn more about how LiveStore works under the hood, feel free to check out our in-depth guides in the &lt;a href=&#34;https://docs.livestore.dev/evaluation/how-livestore-works/&#34;&gt;documentation&lt;/a&gt; and dive into topics like:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/reference/concepts/&#34;&gt;Concepts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/evaluation/event-sourcing/&#34;&gt;Event Sourcing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/evaluation/design-decisions/&#34;&gt;Design Decisions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/evaluation/performance/&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/data-modeling/&#34;&gt;Date Modeling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.livestore.dev/evaluation/technology-comparison/&#34;&gt;Technology comparison&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Livestore is licensed under the &lt;a href=&#34;https://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fastapi/fastapi</title>
    <updated>2025-05-30T01:29:11Z</updated>
    <id>tag:github.com,2025-05-30:/fastapi/fastapi</id>
    <link href="https://github.com/fastapi/fastapi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FastAPI framework, high performance, easy to learn, fast to code, ready for production&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://fastapi.tiangolo.com&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png&#34; alt=&#34;FastAPI&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;em&gt;FastAPI framework, high performance, easy to learn, fast to code, ready for production&lt;/em&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&amp;amp;branch=master&#34; alt=&#34;Test&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg?sanitize=true&#34; alt=&#34;Coverage&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/fastapi&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/fastapi?color=%2334D058&amp;amp;label=pypi%20package&#34; alt=&#34;Package version&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/fastapi&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/fastapi.svg?color=%2334D058&#34; alt=&#34;Supported Python versions&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: &lt;a href=&#34;https://fastapi.tiangolo.com&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://fastapi.tiangolo.com&#34;&gt;https://fastapi.tiangolo.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Source Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/fastapi/fastapi&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/fastapi/fastapi&#34;&gt;https://github.com/fastapi/fastapi&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;FastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.&lt;/p&gt; &#xA;&lt;p&gt;The key features are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Very high performance, on par with &lt;strong&gt;NodeJS&lt;/strong&gt; and &lt;strong&gt;Go&lt;/strong&gt; (thanks to Starlette and Pydantic). &lt;a href=&#34;https://raw.githubusercontent.com/fastapi/fastapi/master/#performance&#34;&gt;One of the fastest Python frameworks available&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast to code&lt;/strong&gt;: Increase the speed to develop features by about 200% to 300%. *&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fewer bugs&lt;/strong&gt;: Reduce about 40% of human (developer) induced errors. *&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intuitive&lt;/strong&gt;: Great editor support. &lt;abbr title=&#34;also known as auto-complete, autocompletion, IntelliSense&#34;&gt;Completion&lt;/abbr&gt; everywhere. Less time debugging.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt;: Designed to be easy to use and learn. Less time reading docs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Short&lt;/strong&gt;: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Robust&lt;/strong&gt;: Get production-ready code. With automatic interactive documentation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Standards-based&lt;/strong&gt;: Based on (and fully compatible with) the open standards for APIs: &lt;a href=&#34;https://github.com/OAI/OpenAPI-Specification&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;OpenAPI&lt;/a&gt; (previously known as Swagger) and &lt;a href=&#34;https://json-schema.org/&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;JSON Schema&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;small&gt;* estimation based on tests on an internal development team, building production applications.&lt;/small&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;!-- sponsors --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://blockbee.io?ref=fastapi&#34; target=&#34;_blank&#34; title=&#34;BlockBee Cryptocurrency Payment Gateway&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/blockbee.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://platform.sh/try-it-now/?utm_source=fastapi-signup&amp;amp;utm_medium=banner&amp;amp;utm_campaign=FastAPI-signup-June-2023&#34; target=&#34;_blank&#34; title=&#34;Build, run and scale your apps on a modern, reliable, and secure PaaS.&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/platform-sh.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.porter.run&#34; target=&#34;_blank&#34; title=&#34;Deploy FastAPI on AWS with a few clicks&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/porter.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/scalar/scalar/?utm_source=fastapi&amp;amp;utm_medium=website&amp;amp;utm_campaign=main-badge&#34; target=&#34;_blank&#34; title=&#34;Scalar: Beautiful Open-Source API References from Swagger/OpenAPI files&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/scalar.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.propelauth.com/?utm_source=fastapi&amp;amp;utm_campaign=1223&amp;amp;utm_medium=mainbadge&#34; target=&#34;_blank&#34; title=&#34;Auth, user management and more for your B2B product&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/propelauth.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zuplo.link/fastapi-gh&#34; target=&#34;_blank&#34; title=&#34;Zuplo: Deploy, Secure, Document, and Monetize your FastAPI&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/zuplo.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://liblab.com?utm_source=fastapi&#34; target=&#34;_blank&#34; title=&#34;liblab - Generate SDKs from FastAPI&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/liblab.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.render.com/deploy-fastapi?utm_source=deploydoc&amp;amp;utm_medium=referral&amp;amp;utm_campaign=fastapi&#34; target=&#34;_blank&#34; title=&#34;Deploy &amp;amp; scale any full-stack web app on Render. Focus on building apps, not infra.&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/render.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.coderabbit.ai/?utm_source=fastapi&amp;amp;utm_medium=badge&amp;amp;utm_campaign=fastapi&#34; target=&#34;_blank&#34; title=&#34;Cut Code Review Time &amp;amp; Bugs in Half with CodeRabbit&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/coderabbit.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://subtotal.com/?utm_source=fastapi&amp;amp;utm_medium=sponsorship&amp;amp;utm_campaign=open-source&#34; target=&#34;_blank&#34; title=&#34;The Gold Standard in Retail Account Linking&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/subtotal.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://databento.com/&#34; target=&#34;_blank&#34; title=&#34;Pay as you go for market data&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/databento.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://speakeasy.com?utm_source=fastapi+repo&amp;amp;utm_medium=github+sponsorship&#34; target=&#34;_blank&#34; title=&#34;SDKs for your API | Speakeasy&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/speakeasy.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.svix.com/&#34; target=&#34;_blank&#34; title=&#34;Svix - Webhooks as a service&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/svix.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.stainlessapi.com/?utm_source=fastapi&amp;amp;utm_medium=referral&#34; target=&#34;_blank&#34; title=&#34;Stainless | Generate best-in-class SDKs&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/stainless.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.permit.io/blog/implement-authorization-in-fastapi?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=fastapi&#34; target=&#34;_blank&#34; title=&#34;Fine-Grained Authorization for FastAPI&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/permit.png&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.interviewpal.com/?utm_source=fastapi&amp;amp;utm_medium=open-source&amp;amp;utm_campaign=dev-hiring&#34; target=&#34;_blank&#34; title=&#34;InterviewPal - AI Interview Coach for Engineers and Devs&#34;&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/sponsors/interviewpal.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- /sponsors --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fastapi.tiangolo.com/fastapi-people/#sponsors&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;Other sponsors&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Opinions&lt;/h2&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;[...] I&#39;m using &lt;strong&gt;FastAPI&lt;/strong&gt; a ton these days. [...] I&#39;m actually planning to use it for all of my team&#39;s &lt;strong&gt;ML services at Microsoft&lt;/strong&gt;. Some of them are getting integrated into the core &lt;strong&gt;Windows&lt;/strong&gt; product and some &lt;strong&gt;Office&lt;/strong&gt; products.&lt;/em&gt;&#34;&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: right; margin-right: 10%;&#34;&gt;&#xA; Kabir Khan - &#xA; &lt;strong&gt;Microsoft&lt;/strong&gt; &#xA; &lt;a href=&#34;https://github.com/fastapi/fastapi/pull/26&#34; target=&#34;_blank&#34;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;We adopted the &lt;strong&gt;FastAPI&lt;/strong&gt; library to spawn a &lt;strong&gt;REST&lt;/strong&gt; server that can be queried to obtain &lt;strong&gt;predictions&lt;/strong&gt;. [for Ludwig]&lt;/em&gt;&#34;&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: right; margin-right: 10%;&#34;&gt;&#xA; Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - &#xA; &lt;strong&gt;Uber&lt;/strong&gt; &#xA; &lt;a href=&#34;https://eng.uber.com/ludwig-v0-2/&#34; target=&#34;_blank&#34;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;&lt;strong&gt;Netflix&lt;/strong&gt; is pleased to announce the open-source release of our &lt;strong&gt;crisis management&lt;/strong&gt; orchestration framework: &lt;strong&gt;Dispatch&lt;/strong&gt;! [built with &lt;strong&gt;FastAPI&lt;/strong&gt;]&lt;/em&gt;&#34;&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: right; margin-right: 10%;&#34;&gt;&#xA; Kevin Glisson, Marc Vilanova, Forest Monsen - &#xA; &lt;strong&gt;Netflix&lt;/strong&gt; &#xA; &lt;a href=&#34;https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072&#34; target=&#34;_blank&#34;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;I‚Äôm over the moon excited about &lt;strong&gt;FastAPI&lt;/strong&gt;. It‚Äôs so fun!&lt;/em&gt;&#34;&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: right; margin-right: 10%;&#34;&gt;&#xA; Brian Okken - &#xA; &lt;strong&gt;&lt;a href=&#34;https://pythonbytes.fm/episodes/show/123/time-to-right-the-py-wrongs?time_in_sec=855&#34; target=&#34;_blank&#34;&gt;Python Bytes&lt;/a&gt; podcast host&lt;/strong&gt; &#xA; &lt;a href=&#34;https://twitter.com/brianokken/status/1112220079972728832&#34; target=&#34;_blank&#34;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;Honestly, what you&#39;ve built looks super solid and polished. In many ways, it&#39;s what I wanted &lt;strong&gt;Hug&lt;/strong&gt; to be - it&#39;s really inspiring to see someone build that.&lt;/em&gt;&#34;&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: right; margin-right: 10%;&#34;&gt;&#xA; Timothy Crosley - &#xA; &lt;strong&gt;&lt;a href=&#34;https://github.com/hugapi/hug&#34; target=&#34;_blank&#34;&gt;Hug&lt;/a&gt; creator&lt;/strong&gt; &#xA; &lt;a href=&#34;https://news.ycombinator.com/item?id=19455465&#34; target=&#34;_blank&#34;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;If you&#39;re looking to learn one &lt;strong&gt;modern framework&lt;/strong&gt; for building REST APIs, check out &lt;strong&gt;FastAPI&lt;/strong&gt; [...] It&#39;s fast, easy to use and easy to learn [...]&lt;/em&gt;&#34;&lt;/p&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;We&#39;ve switched over to &lt;strong&gt;FastAPI&lt;/strong&gt; for our &lt;strong&gt;APIs&lt;/strong&gt; [...] I think you&#39;ll like it [...]&lt;/em&gt;&#34;&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: right; margin-right: 10%;&#34;&gt;&#xA; Ines Montani - Matthew Honnibal - &#xA; &lt;strong&gt;&lt;a href=&#34;https://explosion.ai&#34; target=&#34;_blank&#34;&gt;Explosion AI&lt;/a&gt; founders - &lt;a href=&#34;https://spacy.io&#34; target=&#34;_blank&#34;&gt;spaCy&lt;/a&gt; creators&lt;/strong&gt; &#xA; &lt;a href=&#34;https://twitter.com/_inesmontani/status/1144173225322143744&#34; target=&#34;_blank&#34;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt; - &#xA; &lt;a href=&#34;https://twitter.com/honnibal/status/1144031421859655680&#34; target=&#34;_blank&#34;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&#34;&lt;em&gt;If anyone is looking to build a production Python API, I would highly recommend &lt;strong&gt;FastAPI&lt;/strong&gt;. It is &lt;strong&gt;beautifully designed&lt;/strong&gt;, &lt;strong&gt;simple to use&lt;/strong&gt; and &lt;strong&gt;highly scalable&lt;/strong&gt;, it has become a &lt;strong&gt;key component&lt;/strong&gt; in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer.&lt;/em&gt;&#34;&lt;/p&gt; &#xA;&lt;div style=&#34;text-align: right; margin-right: 10%;&#34;&gt;&#xA; Deon Pillsbury - &#xA; &lt;strong&gt;Cisco&lt;/strong&gt; &#xA; &lt;a href=&#34;https://www.linkedin.com/posts/deonpillsbury_cisco-cx-python-activity-6963242628536487936-trAp/&#34; target=&#34;_blank&#34;&gt;&lt;small&gt;(ref)&lt;/small&gt;&lt;/a&gt;&#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Typer&lt;/strong&gt;, the FastAPI of CLIs&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://typer.tiangolo.com&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg?sanitize=true&#34; style=&#34;width: 20%;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are building a &lt;abbr title=&#34;Command Line Interface&#34;&gt;CLI&lt;/abbr&gt; app to be used in the terminal instead of a web API, check out &lt;a href=&#34;https://typer.tiangolo.com/&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Typer&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Typer&lt;/strong&gt; is FastAPI&#39;s little sibling. And it&#39;s intended to be the &lt;strong&gt;FastAPI of CLIs&lt;/strong&gt;. ‚å®Ô∏è üöÄ&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;FastAPI stands on the shoulders of giants:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.starlette.io/&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;Starlette&lt;/a&gt; for the web parts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.pydantic.dev/&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;Pydantic&lt;/a&gt; for the data parts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Create and activate a &lt;a href=&#34;https://fastapi.tiangolo.com/virtual-environments/&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;virtual environment&lt;/a&gt; and then install FastAPI:&lt;/p&gt; &#xA;&lt;div class=&#34;termy&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ pip install &#34;fastapi[standard]&#34;&#xA;&#xA;---&amp;gt; 100%&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Make sure you put &lt;code&gt;&#34;fastapi[standard]&#34;&lt;/code&gt; in quotes to ensure it works in all terminals.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;h3&gt;Create it&lt;/h3&gt; &#xA;&lt;p&gt;Create a file &lt;code&gt;main.py&lt;/code&gt; with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from typing import Union&#xA;&#xA;from fastapi import FastAPI&#xA;&#xA;app = FastAPI()&#xA;&#xA;&#xA;@app.get(&#34;/&#34;)&#xA;def read_root():&#xA;    return {&#34;Hello&#34;: &#34;World&#34;}&#xA;&#xA;&#xA;@app.get(&#34;/items/{item_id}&#34;)&#xA;def read_item(item_id: int, q: Union[str, None] = None):&#xA;    return {&#34;item_id&#34;: item_id, &#34;q&#34;: q}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;Or use &lt;code&gt;async def&lt;/code&gt;...&lt;/summary&gt; &#xA; &lt;p&gt;If your code uses &lt;code&gt;async&lt;/code&gt; / &lt;code&gt;await&lt;/code&gt;, use &lt;code&gt;async def&lt;/code&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from typing import Union&#xA;&#xA;from fastapi import FastAPI&#xA;&#xA;app = FastAPI()&#xA;&#xA;&#xA;@app.get(&#34;/&#34;)&#xA;async def read_root():&#xA;    return {&#34;Hello&#34;: &#34;World&#34;}&#xA;&#xA;&#xA;@app.get(&#34;/items/{item_id}&#34;)&#xA;async def read_item(item_id: int, q: Union[str, None] = None):&#xA;    return {&#34;item_id&#34;: item_id, &#34;q&#34;: q}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;p&gt;If you don&#39;t know, check the &lt;em&gt;&#34;In a hurry?&#34;&lt;/em&gt; section about &lt;a href=&#34;https://fastapi.tiangolo.com/async/#in-a-hurry&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;async&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt; in the docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Run it&lt;/h3&gt; &#xA;&lt;p&gt;Run the server with:&lt;/p&gt; &#xA;&lt;div class=&#34;termy&#34;&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ fastapi dev main.py&#xA;&#xA; ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI CLI - Development mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ&#xA; ‚îÇ                                                     ‚îÇ&#xA; ‚îÇ  Serving at: http://127.0.0.1:8000                  ‚îÇ&#xA; ‚îÇ                                                     ‚îÇ&#xA; ‚îÇ  API docs: http://127.0.0.1:8000/docs               ‚îÇ&#xA; ‚îÇ                                                     ‚îÇ&#xA; ‚îÇ  Running in development mode, for production use:   ‚îÇ&#xA; ‚îÇ                                                     ‚îÇ&#xA; ‚îÇ  fastapi run                                        ‚îÇ&#xA; ‚îÇ                                                     ‚îÇ&#xA; ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ&#xA;&#xA;INFO:     Will watch for changes in these directories: [&#39;/home/user/code/awesomeapp&#39;]&#xA;INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)&#xA;INFO:     Started reloader process [2248755] using WatchFiles&#xA;INFO:     Started server process [2248757]&#xA;INFO:     Waiting for application startup.&#xA;INFO:     Application startup complete.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/div&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;About the command &lt;code&gt;fastapi dev main.py&lt;/code&gt;...&lt;/summary&gt; &#xA; &lt;p&gt;The command &lt;code&gt;fastapi dev&lt;/code&gt; reads your &lt;code&gt;main.py&lt;/code&gt; file, detects the &lt;strong&gt;FastAPI&lt;/strong&gt; app in it, and starts a server using &lt;a href=&#34;https://www.uvicorn.org&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;Uvicorn&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;By default, &lt;code&gt;fastapi dev&lt;/code&gt; will start with auto-reload enabled for local development.&lt;/p&gt; &#xA; &lt;p&gt;You can read more about it in the &lt;a href=&#34;https://fastapi.tiangolo.com/fastapi-cli/&#34; target=&#34;_blank&#34;&gt;FastAPI CLI docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Check it&lt;/h3&gt; &#xA;&lt;p&gt;Open your browser at &lt;a href=&#34;http://127.0.0.1:8000/items/5?q=somequery&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;http://127.0.0.1:8000/items/5?q=somequery&#34;&gt;http://127.0.0.1:8000/items/5?q=somequery&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will see the JSON response as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-JSON&#34;&gt;{&#34;item_id&#34;: 5, &#34;q&#34;: &#34;somequery&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You already created an API that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Receives HTTP requests in the &lt;em&gt;paths&lt;/em&gt; &lt;code&gt;/&lt;/code&gt; and &lt;code&gt;/items/{item_id}&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Both &lt;em&gt;paths&lt;/em&gt; take &lt;code&gt;GET&lt;/code&gt; &lt;em&gt;operations&lt;/em&gt; (also known as HTTP &lt;em&gt;methods&lt;/em&gt;).&lt;/li&gt; &#xA; &lt;li&gt;The &lt;em&gt;path&lt;/em&gt; &lt;code&gt;/items/{item_id}&lt;/code&gt; has a &lt;em&gt;path parameter&lt;/em&gt; &lt;code&gt;item_id&lt;/code&gt; that should be an &lt;code&gt;int&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;em&gt;path&lt;/em&gt; &lt;code&gt;/items/{item_id}&lt;/code&gt; has an optional &lt;code&gt;str&lt;/code&gt; &lt;em&gt;query parameter&lt;/em&gt; &lt;code&gt;q&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Interactive API docs&lt;/h3&gt; &#xA;&lt;p&gt;Now go to &lt;a href=&#34;http://127.0.0.1:8000/docs&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;http://127.0.0.1:8000/docs&#34;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will see the automatic interactive API documentation (provided by &lt;a href=&#34;https://github.com/swagger-api/swagger-ui&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;Swagger UI&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png&#34; alt=&#34;Swagger UI&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Alternative API docs&lt;/h3&gt; &#xA;&lt;p&gt;And now, go to &lt;a href=&#34;http://127.0.0.1:8000/redoc&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;http://127.0.0.1:8000/redoc&#34;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You will see the alternative automatic documentation (provided by &lt;a href=&#34;https://github.com/Rebilly/ReDoc&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;ReDoc&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png&#34; alt=&#34;ReDoc&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example upgrade&lt;/h2&gt; &#xA;&lt;p&gt;Now modify the file &lt;code&gt;main.py&lt;/code&gt; to receive a body from a &lt;code&gt;PUT&lt;/code&gt; request.&lt;/p&gt; &#xA;&lt;p&gt;Declare the body using standard Python types, thanks to Pydantic.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;from typing import Union&#xA;&#xA;from fastapi import FastAPI&#xA;from pydantic import BaseModel&#xA;&#xA;app = FastAPI()&#xA;&#xA;&#xA;class Item(BaseModel):&#xA;    name: str&#xA;    price: float&#xA;    is_offer: Union[bool, None] = None&#xA;&#xA;&#xA;@app.get(&#34;/&#34;)&#xA;def read_root():&#xA;    return {&#34;Hello&#34;: &#34;World&#34;}&#xA;&#xA;&#xA;@app.get(&#34;/items/{item_id}&#34;)&#xA;def read_item(item_id: int, q: Union[str, None] = None):&#xA;    return {&#34;item_id&#34;: item_id, &#34;q&#34;: q}&#xA;&#xA;&#xA;@app.put(&#34;/items/{item_id}&#34;)&#xA;def update_item(item_id: int, item: Item):&#xA;    return {&#34;item_name&#34;: item.name, &#34;item_id&#34;: item_id}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;fastapi dev&lt;/code&gt; server should reload automatically.&lt;/p&gt; &#xA;&lt;h3&gt;Interactive API docs upgrade&lt;/h3&gt; &#xA;&lt;p&gt;Now go to &lt;a href=&#34;http://127.0.0.1:8000/docs&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;http://127.0.0.1:8000/docs&#34;&gt;http://127.0.0.1:8000/docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The interactive API documentation will be automatically updated, including the new body:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png&#34; alt=&#34;Swagger UI&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Click on the button &#34;Try it out&#34;, it allows you to fill the parameters and directly interact with the API:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png&#34; alt=&#34;Swagger UI interaction&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Then click on the &#34;Execute&#34; button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png&#34; alt=&#34;Swagger UI interaction&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Alternative API docs upgrade&lt;/h3&gt; &#xA;&lt;p&gt;And now, go to &lt;a href=&#34;http://127.0.0.1:8000/redoc&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;&lt;/a&gt;&lt;a href=&#34;http://127.0.0.1:8000/redoc&#34;&gt;http://127.0.0.1:8000/redoc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The alternative documentation will also reflect the new query parameter and body:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png&#34; alt=&#34;ReDoc&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Recap&lt;/h3&gt; &#xA;&lt;p&gt;In summary, you declare &lt;strong&gt;once&lt;/strong&gt; the types of parameters, body, etc. as function parameters.&lt;/p&gt; &#xA;&lt;p&gt;You do that with standard modern Python types.&lt;/p&gt; &#xA;&lt;p&gt;You don&#39;t have to learn a new syntax, the methods or classes of a specific library, etc.&lt;/p&gt; &#xA;&lt;p&gt;Just standard &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For example, for an &lt;code&gt;int&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;item_id: int&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or for a more complex &lt;code&gt;Item&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;item: Item&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...and with that single declaration you get:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Editor support, including: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Completion.&lt;/li&gt; &#xA;   &lt;li&gt;Type checks.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Validation of data: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Automatic and clear errors when the data is invalid.&lt;/li&gt; &#xA;   &lt;li&gt;Validation even for deeply nested JSON objects.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;abbr title=&#34;also known as: serialization, parsing, marshalling&#34;&gt;Conversion&lt;/abbr&gt; of input data: coming from the network to Python data and types. Reading from: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;JSON.&lt;/li&gt; &#xA;   &lt;li&gt;Path parameters.&lt;/li&gt; &#xA;   &lt;li&gt;Query parameters.&lt;/li&gt; &#xA;   &lt;li&gt;Cookies.&lt;/li&gt; &#xA;   &lt;li&gt;Headers.&lt;/li&gt; &#xA;   &lt;li&gt;Forms.&lt;/li&gt; &#xA;   &lt;li&gt;Files.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;abbr title=&#34;also known as: serialization, parsing, marshalling&#34;&gt;Conversion&lt;/abbr&gt; of output data: converting from Python data and types to network data (as JSON): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Convert Python types (&lt;code&gt;str&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt;, &lt;code&gt;float&lt;/code&gt;, &lt;code&gt;bool&lt;/code&gt;, &lt;code&gt;list&lt;/code&gt;, etc).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;datetime&lt;/code&gt; objects.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;UUID&lt;/code&gt; objects.&lt;/li&gt; &#xA;   &lt;li&gt;Database models.&lt;/li&gt; &#xA;   &lt;li&gt;...and many more.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Automatic interactive API documentation, including 2 alternative user interfaces: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Swagger UI.&lt;/li&gt; &#xA;   &lt;li&gt;ReDoc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Coming back to the previous code example, &lt;strong&gt;FastAPI&lt;/strong&gt; will:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Validate that there is an &lt;code&gt;item_id&lt;/code&gt; in the path for &lt;code&gt;GET&lt;/code&gt; and &lt;code&gt;PUT&lt;/code&gt; requests.&lt;/li&gt; &#xA; &lt;li&gt;Validate that the &lt;code&gt;item_id&lt;/code&gt; is of type &lt;code&gt;int&lt;/code&gt; for &lt;code&gt;GET&lt;/code&gt; and &lt;code&gt;PUT&lt;/code&gt; requests. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If it is not, the client will see a useful, clear error.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Check if there is an optional query parameter named &lt;code&gt;q&lt;/code&gt; (as in &lt;code&gt;http://127.0.0.1:8000/items/foo?q=somequery&lt;/code&gt;) for &lt;code&gt;GET&lt;/code&gt; requests. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;As the &lt;code&gt;q&lt;/code&gt; parameter is declared with &lt;code&gt;= None&lt;/code&gt;, it is optional.&lt;/li&gt; &#xA;   &lt;li&gt;Without the &lt;code&gt;None&lt;/code&gt; it would be required (as is the body in the case with &lt;code&gt;PUT&lt;/code&gt;).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;For &lt;code&gt;PUT&lt;/code&gt; requests to &lt;code&gt;/items/{item_id}&lt;/code&gt;, read the body as JSON: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Check that it has a required attribute &lt;code&gt;name&lt;/code&gt; that should be a &lt;code&gt;str&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Check that it has a required attribute &lt;code&gt;price&lt;/code&gt; that has to be a &lt;code&gt;float&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Check that it has an optional attribute &lt;code&gt;is_offer&lt;/code&gt;, that should be a &lt;code&gt;bool&lt;/code&gt;, if present.&lt;/li&gt; &#xA;   &lt;li&gt;All this would also work for deeply nested JSON objects.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Convert from and to JSON automatically.&lt;/li&gt; &#xA; &lt;li&gt;Document everything with OpenAPI, that can be used by: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Interactive documentation systems.&lt;/li&gt; &#xA;   &lt;li&gt;Automatic client code generation systems, for many languages.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Provide 2 interactive documentation web interfaces directly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;We just scratched the surface, but you already get the idea of how it all works.&lt;/p&gt; &#xA;&lt;p&gt;Try changing the line with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;    return {&#34;item_name&#34;: item.name, &#34;item_id&#34;: item_id}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...from:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;        ... &#34;item_name&#34;: item.name ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;        ... &#34;item_price&#34;: item.price ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;...and see how your editor will auto-complete the attributes and know their types:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://fastapi.tiangolo.com/img/vscode-completion.png&#34; alt=&#34;editor support&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;For a more complete example including more features, see the &lt;a href=&#34;https://fastapi.tiangolo.com/tutorial/&#34;&gt;Tutorial - User Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Spoiler alert&lt;/strong&gt;: the tutorial - user guide includes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Declaration of &lt;strong&gt;parameters&lt;/strong&gt; from other different places as: &lt;strong&gt;headers&lt;/strong&gt;, &lt;strong&gt;cookies&lt;/strong&gt;, &lt;strong&gt;form fields&lt;/strong&gt; and &lt;strong&gt;files&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;How to set &lt;strong&gt;validation constraints&lt;/strong&gt; as &lt;code&gt;maximum_length&lt;/code&gt; or &lt;code&gt;regex&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;A very powerful and easy to use &lt;strong&gt;&lt;abbr title=&#34;also known as components, resources, providers, services, injectables&#34;&gt;Dependency Injection&lt;/abbr&gt;&lt;/strong&gt; system.&lt;/li&gt; &#xA; &lt;li&gt;Security and authentication, including support for &lt;strong&gt;OAuth2&lt;/strong&gt; with &lt;strong&gt;JWT tokens&lt;/strong&gt; and &lt;strong&gt;HTTP Basic&lt;/strong&gt; auth.&lt;/li&gt; &#xA; &lt;li&gt;More advanced (but equally easy) techniques for declaring &lt;strong&gt;deeply nested JSON models&lt;/strong&gt; (thanks to Pydantic).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GraphQL&lt;/strong&gt; integration with &lt;a href=&#34;https://strawberry.rocks&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;Strawberry&lt;/a&gt; and other libraries.&lt;/li&gt; &#xA; &lt;li&gt;Many extra features (thanks to Starlette) as: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;WebSockets&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;extremely easy tests based on HTTPX and &lt;code&gt;pytest&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;CORS&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Cookie Sessions&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;li&gt;...and more.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Independent TechEmpower benchmarks show &lt;strong&gt;FastAPI&lt;/strong&gt; applications running under Uvicorn as &lt;a href=&#34;https://www.techempower.com/benchmarks/#section=test&amp;amp;runid=7464e520-0dc2-473d-bd34-dbdfd7e85911&amp;amp;hw=ph&amp;amp;test=query&amp;amp;l=zijzen-7&#34; class=&#34;external-link&#34; target=&#34;_blank&#34;&gt;one of the fastest Python frameworks available&lt;/a&gt;, only below Starlette and Uvicorn themselves (used internally by FastAPI). (*)&lt;/p&gt; &#xA;&lt;p&gt;To understand more about it, see the section &lt;a href=&#34;https://fastapi.tiangolo.com/benchmarks/&#34; class=&#34;internal-link&#34; target=&#34;_blank&#34;&gt;Benchmarks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;p&gt;FastAPI depends on Pydantic and Starlette.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;standard&lt;/code&gt; Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;When you install FastAPI with &lt;code&gt;pip install &#34;fastapi[standard]&#34;&lt;/code&gt; it comes with the &lt;code&gt;standard&lt;/code&gt; group of optional dependencies:&lt;/p&gt; &#xA;&lt;p&gt;Used by Pydantic:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JoshData/python-email-validator&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;email-validator&lt;/code&gt;&lt;/a&gt; - for email validation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Used by Starlette:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.python-httpx.org&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;httpx&lt;/code&gt;&lt;/a&gt; - Required if you want to use the &lt;code&gt;TestClient&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jinja.palletsprojects.com&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;jinja2&lt;/code&gt;&lt;/a&gt; - Required if you want to use the default template configuration.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Kludex/python-multipart&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;python-multipart&lt;/code&gt;&lt;/a&gt; - Required if you want to support form &lt;abbr title=&#34;converting the string that comes from an HTTP request into Python data&#34;&gt;&#34;parsing&#34;&lt;/abbr&gt;, with &lt;code&gt;request.form()&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Used by FastAPI / Starlette:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uvicorn.org&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;uvicorn&lt;/code&gt;&lt;/a&gt; - for the server that loads and serves your application. This includes &lt;code&gt;uvicorn[standard]&lt;/code&gt;, which includes some dependencies (e.g. &lt;code&gt;uvloop&lt;/code&gt;) needed for high performance serving.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;fastapi-cli&lt;/code&gt; - to provide the &lt;code&gt;fastapi&lt;/code&gt; command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Without &lt;code&gt;standard&lt;/code&gt; Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t want to include the &lt;code&gt;standard&lt;/code&gt; optional dependencies, you can install with &lt;code&gt;pip install fastapi&lt;/code&gt; instead of &lt;code&gt;pip install &#34;fastapi[standard]&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Additional Optional Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;There are some additional dependencies you might want to install.&lt;/p&gt; &#xA;&lt;p&gt;Additional optional Pydantic dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.pydantic.dev/latest/usage/pydantic_settings/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;pydantic-settings&lt;/code&gt;&lt;/a&gt; - for settings management.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.pydantic.dev/latest/usage/types/extra_types/extra_types/&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;pydantic-extra-types&lt;/code&gt;&lt;/a&gt; - for extra types to be used with Pydantic.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additional optional FastAPI dependencies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ijl/orjson&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;orjson&lt;/code&gt;&lt;/a&gt; - Required if you want to use &lt;code&gt;ORJSONResponse&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/esnme/ultrajson&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;ujson&lt;/code&gt;&lt;/a&gt; - Required if you want to use &lt;code&gt;UJSONResponse&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the terms of the MIT license.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>KwaiVGI/LivePortrait</title>
    <updated>2025-05-30T01:29:11Z</updated>
    <id>tag:github.com,2025-05-30:/KwaiVGI/LivePortrait</id>
    <link href="https://github.com/KwaiVGI/LivePortrait" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bring portraits to life!&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/cleardusk&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Jianzhu Guo&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1*‚Ä†&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://github.com/Mystery099&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Dingyun Zhang&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1,2*&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://github.com/KwaiVGI&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Xiaoqiang Liu&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://github.com/zzzweakman&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Zhizhou Zhong&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1,3&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://scholar.google.com.hk/citations?user=_8k1ubAAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Yuan Zhang&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1&lt;/sup&gt;‚ÄÉ &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://scholar.google.com/citations?user=P6MraaYAAAAJ&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Pengfei Wan&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1&lt;/sup&gt;‚ÄÉ &#xA; &lt;a href=&#34;https://openreview.net/profile?id=~Di_ZHANG3&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Di Zhang&lt;/strong&gt;&lt;/a&gt;&#xA; &lt;sup&gt; 1&lt;/sup&gt;‚ÄÉ &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;1 &lt;/sup&gt;Kuaishou Technology‚ÄÉ &#xA; &lt;sup&gt;2 &lt;/sup&gt;University of Science and Technology of China‚ÄÉ &#xA; &lt;sup&gt;3 &lt;/sup&gt;Fudan University‚ÄÉ &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;small&gt;&lt;sup&gt;*&lt;/sup&gt; Equal contributions&lt;/small&gt; &#xA; &lt;small&gt;&lt;sup&gt;‚Ä†&lt;/sup&gt; Corresponding author&lt;/small&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;a href=&#39;LICENSE&#39;&gt;&lt;img src=&#39;https://img.shields.io/badge/license-MIT-yellow&#39;&gt;&lt;/a&gt; --&gt; &#xA; &lt;a href=&#34;https://arxiv.org/pdf/2407.03168&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-LivePortrait-red&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://liveportrait.github.io&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-LivePortrait-green&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/spaces/KwaiVGI/liveportrait&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://hellogithub.com/repository/bed652ef02154dd7a434e0720125639e&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=bed652ef02154dd7a434e0720125639e&amp;amp;claim_uid=XyBT2K9QJ7RZhej&amp;amp;theme=small&#34; alt=&#34;FeaturedÔΩúHelloGitHub&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/KwaiVGI/LivePortrait&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/KwaiVGI/LivePortrait&#34;&gt;&lt;/a&gt; &#xA; &lt;br&gt; &#xA; &lt;strong&gt;English&lt;/strong&gt; | &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/readme_zh_cn.md&#34;&gt;&lt;strong&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/strong&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/showcase2.gif&#34; alt=&#34;showcase&#34;&gt; &lt;br&gt; üî• For more results, visit our &lt;a href=&#34;https://liveportrait.github.io/&#34;&gt;&lt;strong&gt;homepage&lt;/strong&gt;&lt;/a&gt; üî• &lt;/p&gt; &#xA;&lt;h2&gt;üî• Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2025/01/01&lt;/code&gt;&lt;/strong&gt;: üê∂ We updated a new version of the Animals model with more data, see &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/changelog/2025-01-01.md&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/10/18&lt;/code&gt;&lt;/strong&gt;: ‚ùó We have updated the versions of the &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;gradio&lt;/code&gt; libraries to avoid security vulnerabilities. Details &lt;a href=&#34;https://github.com/KwaiVGI/LivePortrait/pull/421/files&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/08/29&lt;/code&gt;&lt;/strong&gt;: üì¶ We update the Windows &lt;a href=&#34;https://huggingface.co/cleardusk/LivePortrait-Windows/blob/main/LivePortrait-Windows-v20240829.zip&#34;&gt;one-click installer&lt;/a&gt; and support auto-updates, see &lt;a href=&#34;https://huggingface.co/cleardusk/LivePortrait-Windows#20240829&#34;&gt;changelog&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/08/19&lt;/code&gt;&lt;/strong&gt;: üñºÔ∏è We support &lt;strong&gt;image driven mode&lt;/strong&gt; and &lt;strong&gt;regional control&lt;/strong&gt;. For details, see &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/changelog/2024-08-19.md&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/08/06&lt;/code&gt;&lt;/strong&gt;: üé® We support &lt;strong&gt;precise portrait editing&lt;/strong&gt; in the Gradio interface, inspired by &lt;a href=&#34;https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait&#34;&gt;ComfyUI-AdvancedLivePortrait&lt;/a&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/changelog/2024-08-06.md&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/08/05&lt;/code&gt;&lt;/strong&gt;: üì¶ Windows users can now download the &lt;a href=&#34;https://huggingface.co/cleardusk/LivePortrait-Windows/blob/main/LivePortrait-Windows-v20240806.zip&#34;&gt;one-click installer&lt;/a&gt; for Humans mode and &lt;strong&gt;Animals mode&lt;/strong&gt; now! For details, see &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/changelog/2024-08-05.md&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/08/02&lt;/code&gt;&lt;/strong&gt;: üò∏ We released a version of the &lt;strong&gt;Animals model&lt;/strong&gt;, along with several other updates and improvements. Check out the details &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/changelog/2024-08-02.md&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/07/25&lt;/code&gt;&lt;/strong&gt;: üì¶ Windows users can now download the package from &lt;a href=&#34;https://huggingface.co/cleardusk/LivePortrait-Windows/tree/main&#34;&gt;HuggingFace&lt;/a&gt;. Simply unzip and double-click &lt;code&gt;run_windows.bat&lt;/code&gt; to enjoy!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/07/24&lt;/code&gt;&lt;/strong&gt;: üé® We support pose editing for source portraits in the Gradio interface. We‚Äôve also lowered the default detection threshold to increase recall. &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/changelog/2024-07-24.md&#34;&gt;Have fun&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/07/19&lt;/code&gt;&lt;/strong&gt;: ‚ú® We support üéûÔ∏è &lt;strong&gt;portrait video editing (aka v2v)&lt;/strong&gt;! More to see &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/changelog/2024-07-19.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/07/17&lt;/code&gt;&lt;/strong&gt;: üçé We support macOS with Apple Silicon, modified from &lt;a href=&#34;https://github.com/jeethu&#34;&gt;jeethu&lt;/a&gt;&#39;s PR &lt;a href=&#34;https://github.com/KwaiVGI/LivePortrait/pull/143&#34;&gt;#143&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/07/10&lt;/code&gt;&lt;/strong&gt;: üí™ We support audio and video concatenating, driving video auto-cropping, and template making to protect privacy. More to see &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/changelog/2024-07-10.md&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/07/09&lt;/code&gt;&lt;/strong&gt;: ü§ó We released the &lt;a href=&#34;https://huggingface.co/spaces/KwaiVGI/liveportrait&#34;&gt;HuggingFace Space&lt;/a&gt;, thanks to the HF team and &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;Gradio&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/07/04&lt;/code&gt;&lt;/strong&gt;: üòä We released the initial version of the inference code and models. Continuous updates, stay tuned!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;2024/07/04&lt;/code&gt;&lt;/strong&gt;: üî• We released the &lt;a href=&#34;https://liveportrait.github.io&#34;&gt;homepage&lt;/a&gt; and technical report on &lt;a href=&#34;https://arxiv.org/pdf/2407.03168&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction üìñ&lt;/h2&gt; &#xA;&lt;p&gt;This repo, named &lt;strong&gt;LivePortrait&lt;/strong&gt;, contains the official PyTorch implementation of our paper &lt;a href=&#34;https://arxiv.org/pdf/2407.03168&#34;&gt;LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control&lt;/a&gt;. We are actively updating and improving this repository. If you find any bugs or have suggestions, welcome to raise issues or submit pull requests (PR) üíñ.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started üèÅ&lt;/h2&gt; &#xA;&lt;h3&gt;1. Clone the code and prepare the environment üõ†Ô∏è&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Note] Make sure your system has &lt;a href=&#34;https://git-scm.com/&#34;&gt;&lt;code&gt;git&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://anaconda.org/anaconda/conda&#34;&gt;&lt;code&gt;conda&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;&lt;code&gt;FFmpeg&lt;/code&gt;&lt;/a&gt; installed. For details on FFmpeg installation, see &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/how-to-install-ffmpeg.md&#34;&gt;&lt;strong&gt;how to install FFmpeg&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/KwaiVGI/LivePortrait&#xA;cd LivePortrait&#xA;&#xA;# create env using conda&#xA;conda create -n LivePortrait python=3.10&#xA;conda activate LivePortrait&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For Linux or Windows Users&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/IDEA-Research/X-Pose&#34;&gt;X-Pose&lt;/a&gt; requires your &lt;code&gt;torch&lt;/code&gt; version to be compatible with the CUDA version.&lt;/p&gt; &#xA;&lt;p&gt;Firstly, check your current CUDA version by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;nvcc -V # example versions: 11.1, 11.8, 12.1, etc.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install the corresponding torch version. Here are examples for different CUDA versions. Visit the &lt;a href=&#34;https://pytorch.org/get-started/previous-versions&#34;&gt;PyTorch Official Website&lt;/a&gt; for installation commands if your CUDA version is not listed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# for CUDA 11.1&#xA;pip install torch==1.10.1+cu111 torchvision==0.11.2 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html&#xA;# for CUDA 11.8&#xA;pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu118&#xA;# for CUDA 12.1&#xA;pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121&#xA;# ...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: On Windows systems, some higher versions of CUDA (such as 12.4, 12.6, etc.) may lead to unknown issues. You may consider downgrading CUDA to version 11.8 for stability. See the &lt;a href=&#34;https://github.com/dimitribarbot/sd-webui-live-portrait/raw/main/assets/docs/how-to-install-xpose.md#cuda-toolkit-118&#34;&gt;downgrade guide&lt;/a&gt; by &lt;a href=&#34;https://github.com/dimitribarbot&#34;&gt;@dimitribarbot&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Finally, install the remaining dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For macOS with Apple Silicon Users&lt;/h4&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://github.com/IDEA-Research/X-Pose&#34;&gt;X-Pose&lt;/a&gt; dependency does not support macOS, so you can skip its installation. While Humans mode works as usual, Animals mode is not supported. Use the provided requirements file for macOS with Apple Silicon:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# for macOS with Apple Silicon users&#xA;pip install -r requirements_macOS.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Download pretrained weights üì•&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to download the pretrained weights is from HuggingFace:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# !pip install -U &#34;huggingface_hub[cli]&#34;&#xA;huggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude &#34;*.git*&#34; &#34;README.md&#34; &#34;docs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you cannot access to Huggingface, you can use &lt;a href=&#34;https://hf-mirror.com/&#34;&gt;hf-mirror&lt;/a&gt; to download:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# !pip install -U &#34;huggingface_hub[cli]&#34;&#xA;export HF_ENDPOINT=https://hf-mirror.com&#xA;huggingface-cli download KwaiVGI/LivePortrait --local-dir pretrained_weights --exclude &#34;*.git*&#34; &#34;README.md&#34; &#34;docs&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can download all pretrained weights from &lt;a href=&#34;https://drive.google.com/drive/folders/1UtKgzKjFAOmZkhNK-OYT0caJ_w2XAnib&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1MGctWmNla_vZxDbEp2Dtzw?pwd=z5cn&#34;&gt;Baidu Yun&lt;/a&gt;. Unzip and place them in &lt;code&gt;./pretrained_weights&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Ensuring the directory structure is as or contains &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/directory-structure.md&#34;&gt;&lt;strong&gt;this&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;3. Inference üöÄ&lt;/h3&gt; &#xA;&lt;h4&gt;Fast hands-on (humans) üë§&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Linux and Windows users&#xA;python inference.py&#xA;&#xA;# For macOS users with Apple Silicon (Intel is not tested). NOTE: this maybe 20x slower than RTX 4090&#xA;PYTORCH_ENABLE_MPS_FALLBACK=1 python inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the script runs successfully, you will get an output mp4 file named &lt;code&gt;animations/s6--d0_concat.mp4&lt;/code&gt;. This file includes the following results: driving video, input image or video, and generated result.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/inference.gif&#34; alt=&#34;image&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Or, you can change the input by specifying the &lt;code&gt;-s&lt;/code&gt; and &lt;code&gt;-d&lt;/code&gt; arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# source input is an image&#xA;python inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d0.mp4&#xA;&#xA;# source input is a video ‚ú®&#xA;python inference.py -s assets/examples/source/s13.mp4 -d assets/examples/driving/d0.mp4&#xA;&#xA;# more options to see&#xA;python inference.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Fast hands-on (animals) üê±üê∂&lt;/h4&gt; &#xA;&lt;p&gt;Animals mode is ONLY tested on Linux and Windows with NVIDIA GPU.&lt;/p&gt; &#xA;&lt;p&gt;You need to build an OP named &lt;code&gt;MultiScaleDeformableAttention&lt;/code&gt; first, which is used by &lt;a href=&#34;https://github.com/IDEA-Research/X-Pose&#34;&gt;X-Pose&lt;/a&gt;, a general keypoint detection framework.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd src/utils/dependencies/XPose/models/UniPose/ops&#xA;python setup.py build install&#xA;cd - # equal to cd ../../../../../../../&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference_animals.py -s assets/examples/source/s39.jpg -d assets/examples/driving/wink.pkl --driving_multiplier 1.75 --no_flag_stitching&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the script runs successfully, you will get an output mp4 file named &lt;code&gt;animations/s39--wink_concat.mp4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/inference-animals.gif&#34; alt=&#34;image&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Driving video auto-cropping üì¢üì¢üì¢&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] To use your own driving video, we &lt;strong&gt;recommend&lt;/strong&gt;: ‚¨áÔ∏è&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Crop it to a &lt;strong&gt;1:1&lt;/strong&gt; aspect ratio (e.g., 512x512 or 256x256 pixels), or enable auto-cropping by &lt;code&gt;--flag_crop_driving_video&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Focus on the head area, similar to the example videos.&lt;/li&gt; &#xA;  &lt;li&gt;Minimize shoulder movement.&lt;/li&gt; &#xA;  &lt;li&gt;Make sure the first frame of driving video is a frontal face with &lt;strong&gt;neutral expression&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Below is an auto-cropping case by &lt;code&gt;--flag_crop_driving_video&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d13.mp4 --flag_crop_driving_video&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you find the results of auto-cropping is not well, you can modify the &lt;code&gt;--scale_crop_driving_video&lt;/code&gt;, &lt;code&gt;--vy_ratio_crop_driving_video&lt;/code&gt; options to adjust the scale and offset, or do it manually.&lt;/p&gt; &#xA;&lt;h4&gt;Motion template making&lt;/h4&gt; &#xA;&lt;p&gt;You can also use the auto-generated motion template files ending with &lt;code&gt;.pkl&lt;/code&gt; to speed up inference, and &lt;strong&gt;protect privacy&lt;/strong&gt;, such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py -s assets/examples/source/s9.jpg -d assets/examples/driving/d5.pkl # portrait animation&#xA;python inference.py -s assets/examples/source/s13.mp4 -d assets/examples/driving/d5.pkl # portrait video editing&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Gradio interface ü§ó&lt;/h3&gt; &#xA;&lt;p&gt;We also provide a Gradio &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/gradio-app/gradio&#34;&gt;&lt;/a&gt; interface for a better experience, just run by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Linux and Windows users (and macOS with Intel??)&#xA;python app.py # humans mode&#xA;&#xA;# For macOS with Apple Silicon users, Intel not supported, this maybe 20x slower than RTX 4090&#xA;PYTORCH_ENABLE_MPS_FALLBACK=1 python app.py # humans mode&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide a Gradio interface of animals mode, which is only tested on Linux with NVIDIA GPU:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app_animals.py # animals mode üê±üê∂&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can specify the &lt;code&gt;--server_port&lt;/code&gt;, &lt;code&gt;--share&lt;/code&gt;, &lt;code&gt;--server_name&lt;/code&gt; arguments to satisfy your needs!&lt;/p&gt; &#xA;&lt;p&gt;üöÄ We also provide an acceleration option &lt;code&gt;--flag_do_torch_compile&lt;/code&gt;. The first-time inference triggers an optimization process (about one minute), making subsequent inferences 20-30% faster. Performance gains may vary with different CUDA versions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# enable torch.compile for faster inference&#xA;python app.py --flag_do_torch_compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: This method is not supported on Windows and macOS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Or, try it out effortlessly on &lt;a href=&#34;https://huggingface.co/spaces/KwaiVGI/LivePortrait&#34;&gt;HuggingFace&lt;/a&gt; ü§ó&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5. Inference speed evaluation üöÄüöÄüöÄ&lt;/h3&gt; &#xA;&lt;p&gt;We have also provided a script to evaluate the inference speed of each module:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For NVIDIA GPU&#xA;python speed.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results are &lt;a href=&#34;https://raw.githubusercontent.com/KwaiVGI/LivePortrait/main/assets/docs/speed.md&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community Resources ü§ó&lt;/h2&gt; &#xA;&lt;p&gt;Discover the invaluable resources contributed by our community to enhance your LivePortrait experience.&lt;/p&gt; &#xA;&lt;h3&gt;Community-developed Projects&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Repo&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Author / Links&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/antgroup/ditto-talkinghead&#34;&gt;&lt;strong&gt;ditto-talkinghead&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Real-time audio-driven talking head.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2411.19509&#34;&gt;ArXiv&lt;/a&gt;, &lt;a href=&#34;https://digital-avatar.github.io/ai/Ditto/&#34;&gt;Homepage&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/warmshao/FasterLivePortrait&#34;&gt;&lt;strong&gt;FasterLivePortrait&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Faster real-time version using TensorRT.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/warmshao&#34;&gt;@warmshao&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jhj0517/AdvancedLivePortrait-WebUI&#34;&gt;&lt;strong&gt;AdvancedLivePortrait-WebUI&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Dedicated gradio based WebUI started from &lt;a href=&#34;https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait&#34;&gt;ComfyUI-AdvancedLivePortrait&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jhj0517&#34;&gt;@jhj0517&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jbilcke-hf/FacePoke&#34;&gt;&lt;strong&gt;FacePoke&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A real-time head transformation app, controlled by your mouse!&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/jbilcke-hf&#34;&gt;@jbilcke-hf&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facefusion/facefusion&#34;&gt;&lt;strong&gt;FaceFusion&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FaceFusion 3.0 integregates LivePortrait as &lt;code&gt;expression_restorer&lt;/code&gt; and &lt;code&gt;face_editor&lt;/code&gt; processors.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/henryruhs&#34;&gt;@henryruhs&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dimitribarbot/sd-webui-live-portrait&#34;&gt;&lt;strong&gt;sd-webui-live-portrait&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;WebUI extension of LivePortrait, adding atab to the original Stable Diffusion WebUI to benefit from LivePortrait features.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/dimitribarbot&#34;&gt;@dimitribarbot&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kijai/ComfyUI-LivePortraitKJ&#34;&gt;&lt;strong&gt;ComfyUI-LivePortraitKJ&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A ComfyUI node to use LivePortrait, with MediaPipe as as an alternative to Insightface.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kijai&#34;&gt;@kijai&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PowerHouseMan/ComfyUI-AdvancedLivePortrait&#34;&gt;&lt;strong&gt;ComfyUI-AdvancedLivePortrait&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A faster ComfyUI node with real-time preview that has inspired many other community-developed tools and projects.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/PowerHouseMan&#34;&gt;@PowerHouseMan&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/shadowcz007/comfyui-liveportrait&#34;&gt;&lt;strong&gt;comfyui-liveportrait&lt;/strong&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A ComfyUI node to use LivePortrait, supporting multi-faces, expression interpolation etc, with a &lt;a href=&#34;https://www.bilibili.com/video/BV1JW421R7sP&#34;&gt;tutorial&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/shadowcz007&#34;&gt;@shadowcz007&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Playgrounds, ü§ó HuggingFace Spaces and Others&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/jbilcke-hf/FacePoke&#34;&gt;FacePoke Space&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/fffiloni/expression-editor&#34;&gt;Expression Editor Space&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/fofr/expression-editor&#34;&gt;Expression Editor Replicate&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/demos/face-control&#34;&gt;Face Control Realtime Demo&lt;/a&gt; on FAL&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/fofr/live-portrait&#34;&gt;Replicate Playground&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nuke can use LivePortrait through CompyUI node, details &lt;a href=&#34;https://x.com/bilawalsidhu/status/1837349806475276338&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LivePortrait lives on &lt;a href=&#34;https://poe.com/LivePortrait&#34;&gt;Poe&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Video Tutorials&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/xfzK_6cTs58?si=aYjgypeJBkhc46VL&#34;&gt;Workflow of LivePortrait Video to Video&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@curiousrefuge&#34;&gt;@curiousrefuge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/59Y9ePAXTp0?si=KzEWhklBlporW7D8&#34;&gt;Google Colab tutorial&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@planetai217&#34;&gt;@Planet Ai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/fD0P6UWSu8I?si=Vn5wxUa8qSu1jv4l&#34;&gt;Paper reading&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@TwoMinutePapers&#34;&gt;@TwoMinutePapers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://youtu.be/q0Vf-ZZsbzI?si=nbs3npleH-dVCt28&#34;&gt;ComfyUI Advanced LivePortrait&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@TutoView&#34;&gt;TutoView&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=vsvlbTEqgXQ&#34;&gt;LivePortarit exploration&lt;/a&gt; and &lt;a href=&#34;https://youtu.be/cucaEEDYmsw?si=AtPaDWc5G-a4E8dD&#34;&gt;A deep dive into LivePortrait&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@TheoreticallyMedia&#34;&gt;TheoreticallyMedia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=uyjSTAOY7yI&#34;&gt;LivePortrait hands-on tutorial&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@theAIsearch&#34;&gt;@AI Search&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8-IcDDmiUMM&#34;&gt;ComfyUI tutorial&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@sebastiankamph&#34;&gt;@Sebastian Kamph&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A &lt;a href=&#34;https://www.bilibili.com/video/BV1cf421i7Ly&#34;&gt;tutorial&lt;/a&gt; on BiliBili&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And so MANY amazing contributions from our community, too many to list them all üíñ&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements üíê&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the contributors of &lt;a href=&#34;https://github.com/AliaksandrSiarohin/first-order-model&#34;&gt;FOMM&lt;/a&gt;, &lt;a href=&#34;https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis&#34;&gt;Open Facevid2vid&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVlabs/SPADE&#34;&gt;SPADE&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;InsightFace&lt;/a&gt; and &lt;a href=&#34;https://github.com/IDEA-Research/X-Pose&#34;&gt;X-Pose&lt;/a&gt; repositories, for their open research and contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Ethics Considerations üõ°Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;Portrait animation technologies come with social risks, particularly the potential for misuse in creating deepfakes. To mitigate these risks, it‚Äôs crucial to follow ethical guidelines and adopt responsible usage practices. At present, the synthesized results contain visual artifacts that may help in detecting deepfakes. Please note that we do not assume any legal responsibility for the use of the results generated by this project.&lt;/p&gt; &#xA;&lt;h2&gt;Citation üíñ&lt;/h2&gt; &#xA;&lt;p&gt;If you find LivePortrait useful for your research, welcome to üåü this repo and cite our work using the following BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{guo2024liveportrait,&#xA;  title   = {LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control},&#xA;  author  = {Guo, Jianzhu and Zhang, Dingyun and Liu, Xiaoqiang and Zhong, Zhizhou and Zhang, Yuan and Wan, Pengfei and Zhang, Di},&#xA;  journal = {arXiv preprint arXiv:2407.03168},&#xA;  year    = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Long live in arXiv.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact üìß&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://guojianzhu.com&#34;&gt;&lt;strong&gt;Jianzhu Guo (ÈÉ≠Âª∫Áè†)&lt;/strong&gt;&lt;/a&gt;; &lt;strong&gt;&lt;a href=&#34;mailto:guojianzhu1994@gmail.com&#34;&gt;guojianzhu1994@gmail.com&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
</feed>