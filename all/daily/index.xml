<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-29T01:28:27Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>deepseek-ai/Janus</title>
    <updated>2025-01-29T01:28:27Z</updated>
    <id>tag:github.com,2025-01-29:/deepseek-ai/Janus</id>
    <link href="https://github.com/deepseek-ai/Janus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Janus-Series: Unified Multimodal Understanding and Generation Models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/logo.svg?sanitize=true&#34; width=&#34;60%&#34; alt=&#34;DeepSeek LLM&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;üöÄ Janus-Series: Unified Multimodal Understanding and Generation Models&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.deepseek.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Homepage&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt;  &#xA; &lt;a href=&#34;https://huggingface.co/deepseek-ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Hugging Face&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;a href=&#34;https://discord.gg/Tc7c45Zzu5&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;logoColor=white&amp;color=7289da&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &#xA; &lt;!-- &lt;a href=&#34;images/qr.jpeg&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img alt=&#34;Wechat&#34; src=&#34;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;logoColor=white&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &#xA; &lt;!-- &lt;a href=&#34;https://twitter.com/deepseek_ai&#34; target=&#34;_blank&#34;&gt;&#xA;    &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;logoColor=white&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/LICENSE-CODE&#34;&gt; &lt;img alt=&#34;Code License&#34; src=&#34;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/LICENSE-MODEL&#34;&gt; &lt;img alt=&#34;Model License&#34; src=&#34;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#2-model-download&#34;&gt;&lt;b&gt;üì• Model Download&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#3-quick-start&#34;&gt;&lt;b&gt;‚ö° Quick Start&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#4-license&#34;&gt;&lt;b&gt;üìú License&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#5-citation&#34;&gt;&lt;b&gt;üìñ Citation&lt;/b&gt;&lt;/a&gt; &lt;br&gt; &#xA; &lt;!-- üìÑ Paper Link (&lt;a href=&#34;https://arxiv.org/abs/2410.13848&#34;&gt;&lt;b&gt;Janus&lt;/b&gt;&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2410.13848&#34;&gt;&lt;b&gt;JanusFlow&lt;/b&gt;&lt;/a&gt;) | --&gt; ü§ó Online Demo (&lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B&#34;&gt;&lt;b&gt;Janus-Pro-7B&lt;/b&gt;&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-1.3B&#34;&gt;&lt;b&gt;Janus&lt;/b&gt;&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B&#34;&gt;&lt;b&gt;JanusFlow&lt;/b&gt;&lt;/a&gt;) &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2025.01.27&lt;/strong&gt;: Janus-Pro is released, an advanced version of Janus, improving both multimodal understanding and visual generation significantly. See &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/janus_pro_tech_report.pdf&#34;&gt;paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.11.13&lt;/strong&gt;: JanusFlow is released, a new unified model with rectified flow for image generation. See &lt;a href=&#34;https://arxiv.org/abs/2411.07975&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B&#34;&gt;demo&lt;/a&gt; and &lt;a href=&#34;https://github.com/deepseek-ai/Janus?tab=readme-ov-file#janusflow&#34;&gt;usage&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.10.23&lt;/strong&gt;: Evaluation code for reproducing the multimodal understanding results from the paper has been added to VLMEvalKit. Please refer to &lt;a href=&#34;https://github.com/open-compass/VLMEvalKit/pull/541&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2024.10.20&lt;/strong&gt;: (1) Fix a bug in &lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-1.3B/blob/main/tokenizer_config.json&#34;&gt;tokenizer_config.json&lt;/a&gt;. The previous version caused classifier-free guidance to not function properly, resulting in relatively poor visual generation quality. (2) Release Gradio demo (&lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-1.3B&#34;&gt;online demo&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#gradio-demo&#34;&gt;local&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/janus_pro_tech_report.pdf&#34;&gt;&lt;b&gt;Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Janus-Pro&lt;/strong&gt; is an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/teaser_januspro.png&#34; style=&#34;width:90%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2410.13848&#34;&gt;&lt;b&gt;Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Janus&lt;/strong&gt; is a novel autoregressive framework that unifies multimodal understanding and generation. It addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/teaser.png&#34; style=&#34;width:90%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2411.07975&#34;&gt;&lt;b&gt;JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation&lt;/b&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JanusFlow&lt;/strong&gt; introduces a minimalist architecture that integrates autoregressive language models with rectified flow, a state-of-the-art method in generative modeling. Our key finding demonstrates that rectified flow can be straightforwardly trained within the large language model framework, eliminating the need for complex architectural modifications. Extensive experiments show that JanusFlow achieves comparable or superior performance to specialized models in their respective domains, while significantly outperforming existing unified approaches across standard benchmarks. This work represents a step toward more efficient and versatile vision-language models.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;image&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/images/teaser_janusflow.png&#34; style=&#34;width:90%;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;2. Model Download&lt;/h2&gt; &#xA;&lt;p&gt;We release Janus to the public to support a broader and more diverse range of research within both academic and commercial communities. Please note that the use of this model is subject to the terms outlined in &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/Janus/main/#5-license&#34;&gt;License section&lt;/a&gt;. Commercial usage is permitted under these terms.&lt;/p&gt; &#xA;&lt;h3&gt;Huggingface&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Sequence Length&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-1.3B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-1.3B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;JanusFlow-1.3B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/JanusFlow-1.3B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-Pro-1B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-Pro-1B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Janus-Pro-7B&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/Janus-Pro-7B&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;3. Quick Start&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Janus-Pro&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA; &lt;h4&gt;Multimodal Understanding&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import torch&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;from janus.utils.io import load_pil_images&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-Pro-7B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;&amp;lt;|User|&amp;gt;&#34;,&#xA;        &#34;content&#34;: f&#34;&amp;lt;image_placeholder&amp;gt;\n{question}&#34;,&#xA;        &#34;images&#34;: [image],&#xA;    },&#xA;    {&#34;role&#34;: &#34;&amp;lt;|Assistant|&amp;gt;&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation, images=pil_images, force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# # run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# # run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True,&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Text-to-Image Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import PIL.Image&#xA;import torch&#xA;import numpy as np&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-Pro-7B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;&amp;lt;|User|&amp;gt;&#34;,&#xA;        &#34;content&#34;: &#34;A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair&#34;,&#xA;    },&#xA;    {&#34;role&#34;: &#34;&amp;lt;|Assistant|&amp;gt;&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(&#xA;    conversations=conversation,&#xA;    sft_format=vl_chat_processor.sft_format,&#xA;    system_prompt=&#34;&#34;,&#xA;)&#xA;prompt = sft_format + vl_chat_processor.image_start_tag&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;def generate(&#xA;    mmgpt: MultiModalityCausalLM,&#xA;    vl_chat_processor: VLChatProcessor,&#xA;    prompt: str,&#xA;    temperature: float = 1,&#xA;    parallel_size: int = 16,&#xA;    cfg_weight: float = 5,&#xA;    image_token_num_per_image: int = 576,&#xA;    img_size: int = 384,&#xA;    patch_size: int = 16,&#xA;):&#xA;    input_ids = vl_chat_processor.tokenizer.encode(prompt)&#xA;    input_ids = torch.LongTensor(input_ids)&#xA;&#xA;    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()&#xA;    for i in range(parallel_size*2):&#xA;        tokens[i, :] = input_ids&#xA;        if i % 2 != 0:&#xA;            tokens[i, 1:-1] = vl_chat_processor.pad_id&#xA;&#xA;    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)&#xA;&#xA;    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()&#xA;&#xA;    for i in range(image_token_num_per_image):&#xA;        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)&#xA;        hidden_states = outputs.last_hidden_state&#xA;        &#xA;        logits = mmgpt.gen_head(hidden_states[:, -1, :])&#xA;        logit_cond = logits[0::2, :]&#xA;        logit_uncond = logits[1::2, :]&#xA;        &#xA;        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)&#xA;        probs = torch.softmax(logits / temperature, dim=-1)&#xA;&#xA;        next_token = torch.multinomial(probs, num_samples=1)&#xA;        generated_tokens[:, i] = next_token.squeeze(dim=-1)&#xA;&#xA;        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)&#xA;        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)&#xA;        inputs_embeds = img_embeds.unsqueeze(dim=1)&#xA;&#xA;&#xA;    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])&#xA;    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)&#xA;&#xA;    dec = np.clip((dec + 1) / 2 * 255, 0, 255)&#xA;&#xA;    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)&#xA;    visual_img[:, :, :] = dec&#xA;&#xA;    os.makedirs(&#39;generated_samples&#39;, exist_ok=True)&#xA;    for i in range(parallel_size):&#xA;        save_path = os.path.join(&#39;generated_samples&#39;, &#34;img_{}.jpg&#34;.format(i))&#xA;        PIL.Image.fromarray(visual_img[i]).save(save_path)&#xA;&#xA;&#xA;generate(&#xA;    vl_gpt,&#xA;    vl_chat_processor,&#xA;    prompt,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA; &lt;p&gt;We have deployed online demo in &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-Pro-7B&#34;&gt;Huggingface&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For the local gradio demo, you can run with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install -e .[gradio]&#xA;&#xA;python demo/app_januspro.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Have Fun!&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Janus&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA; &lt;h4&gt;Multimodal Understanding&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import torch&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;from janus.utils.io import load_pil_images&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;&amp;lt;image_placeholder&amp;gt;\nConvert the formula into latex code.&#34;,&#xA;        &#34;images&#34;: [&#34;images/equation.png&#34;],&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation, images=pil_images, force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# # run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# # run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True,&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Text-to-Image Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import PIL.Image&#xA;import torch&#xA;import numpy as np&#xA;from transformers import AutoModelForCausalLM&#xA;from janus.models import MultiModalityCausalLM, VLChatProcessor&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/Janus-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair&#34;,&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(&#xA;    conversations=conversation,&#xA;    sft_format=vl_chat_processor.sft_format,&#xA;    system_prompt=&#34;&#34;,&#xA;)&#xA;prompt = sft_format + vl_chat_processor.image_start_tag&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;def generate(&#xA;    mmgpt: MultiModalityCausalLM,&#xA;    vl_chat_processor: VLChatProcessor,&#xA;    prompt: str,&#xA;    temperature: float = 1,&#xA;    parallel_size: int = 16,&#xA;    cfg_weight: float = 5,&#xA;    image_token_num_per_image: int = 576,&#xA;    img_size: int = 384,&#xA;    patch_size: int = 16,&#xA;):&#xA;    input_ids = vl_chat_processor.tokenizer.encode(prompt)&#xA;    input_ids = torch.LongTensor(input_ids)&#xA;&#xA;    tokens = torch.zeros((parallel_size*2, len(input_ids)), dtype=torch.int).cuda()&#xA;    for i in range(parallel_size*2):&#xA;        tokens[i, :] = input_ids&#xA;        if i % 2 != 0:&#xA;            tokens[i, 1:-1] = vl_chat_processor.pad_id&#xA;&#xA;    inputs_embeds = mmgpt.language_model.get_input_embeddings()(tokens)&#xA;&#xA;    generated_tokens = torch.zeros((parallel_size, image_token_num_per_image), dtype=torch.int).cuda()&#xA;&#xA;    for i in range(image_token_num_per_image):&#xA;        outputs = mmgpt.language_model.model(inputs_embeds=inputs_embeds, use_cache=True, past_key_values=outputs.past_key_values if i != 0 else None)&#xA;        hidden_states = outputs.last_hidden_state&#xA;        &#xA;        logits = mmgpt.gen_head(hidden_states[:, -1, :])&#xA;        logit_cond = logits[0::2, :]&#xA;        logit_uncond = logits[1::2, :]&#xA;        &#xA;        logits = logit_uncond + cfg_weight * (logit_cond-logit_uncond)&#xA;        probs = torch.softmax(logits / temperature, dim=-1)&#xA;&#xA;        next_token = torch.multinomial(probs, num_samples=1)&#xA;        generated_tokens[:, i] = next_token.squeeze(dim=-1)&#xA;&#xA;        next_token = torch.cat([next_token.unsqueeze(dim=1), next_token.unsqueeze(dim=1)], dim=1).view(-1)&#xA;        img_embeds = mmgpt.prepare_gen_img_embeds(next_token)&#xA;        inputs_embeds = img_embeds.unsqueeze(dim=1)&#xA;&#xA;&#xA;    dec = mmgpt.gen_vision_model.decode_code(generated_tokens.to(dtype=torch.int), shape=[parallel_size, 8, img_size//patch_size, img_size//patch_size])&#xA;    dec = dec.to(torch.float32).cpu().numpy().transpose(0, 2, 3, 1)&#xA;&#xA;    dec = np.clip((dec + 1) / 2 * 255, 0, 255)&#xA;&#xA;    visual_img = np.zeros((parallel_size, img_size, img_size, 3), dtype=np.uint8)&#xA;    visual_img[:, :, :] = dec&#xA;&#xA;    os.makedirs(&#39;generated_samples&#39;, exist_ok=True)&#xA;    for i in range(parallel_size):&#xA;        save_path = os.path.join(&#39;generated_samples&#39;, &#34;img_{}.jpg&#34;.format(i))&#xA;        PIL.Image.fromarray(visual_img[i]).save(save_path)&#xA;&#xA;&#xA;generate(&#xA;    vl_gpt,&#xA;    vl_chat_processor,&#xA;    prompt,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA; &lt;p&gt;We have deployed online demo in &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/Janus-1.3B&#34;&gt;Huggingface&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;For the local gradio demo, you can run with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install -e .[gradio]&#xA;&#xA;python demo/app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Have Fun!&lt;/p&gt; &#xA; &lt;h3&gt;FastAPI Demo&lt;/h3&gt; &#xA; &lt;p&gt;It&#39;s easy to run a FastAPI server to host an API server running the same functions as gradio.&lt;/p&gt; &#xA; &lt;p&gt;To start FastAPI server, run the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python demo/fastapi_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;To test the server, you can open another terminal and run:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;python demo/fastapi_client.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;JanusFlow&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h3&gt;Installation&lt;/h3&gt; &#xA; &lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;pip install diffusers[torch]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;ü§ó Huggingface Online Demo&lt;/h3&gt; &#xA; &lt;p&gt;Check out the demo in &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/JanusFlow-1.3B&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA; &lt;h4&gt;Multimodal Understanding&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;import torch&#xA;from janus.janusflow.models import MultiModalityCausalLM, VLChatProcessor&#xA;from janus.utils.io import load_pil_images&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/JanusFlow-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt = MultiModalityCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;&amp;lt;image_placeholder&amp;gt;\nConvert the formula into latex code.&#34;,&#xA;        &#34;images&#34;: [&#34;images/equation.png&#34;],&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation, images=pil_images, force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# # run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# # run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True,&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Text-to-Image Generation&lt;/h4&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;import PIL.Image&#xA;import torch&#xA;import numpy as np&#xA;from janus.janusflow.models import MultiModalityCausalLM, VLChatProcessor&#xA;import torchvision&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/JanusFlow-1.3B&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt = MultiModalityCausalLM.from_pretrained(&#xA;    model_path, trust_remote_code=True&#xA;)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;from diffusers.models import AutoencoderKL&#xA;# remember to use bfloat16 dtype, this vae doesn&#39;t work with fp16&#xA;vae = AutoencoderKL.from_pretrained(&#34;stabilityai/sdxl-vae&#34;)&#xA;vae = vae.to(torch.bfloat16).cuda().eval()&#xA;&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;A stunning princess from kabul in red, white traditional clothing, blue eyes, brown hair&#34;,&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;sft_format = vl_chat_processor.apply_sft_template_for_multi_turn_prompts(&#xA;    conversations=conversation,&#xA;    sft_format=vl_chat_processor.sft_format,&#xA;    system_prompt=&#34;&#34;,&#xA;)&#xA;prompt = sft_format + vl_chat_processor.image_gen_tag&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;def generate(&#xA;    mmgpt: MultiModalityCausalLM,&#xA;    vl_chat_processor: VLChatProcessor,&#xA;    prompt: str,&#xA;    cfg_weight: float = 5.0,&#xA;    num_inference_steps: int = 30,&#xA;    batchsize: int = 5&#xA;):&#xA;    input_ids = vl_chat_processor.tokenizer.encode(prompt)&#xA;    input_ids = torch.LongTensor(input_ids)&#xA;    &#xA;    tokens = torch.stack([input_ids] * 2 * batchsize).cuda()&#xA;    tokens[batchsize:, 1:] = vl_chat_processor.pad_id&#xA;    inputs_embeds = vl_gpt.language_model.get_input_embeddings()(tokens)&#xA;&#xA;    # we remove the last &amp;lt;bog&amp;gt; token and replace it with t_emb later&#xA;    inputs_embeds = inputs_embeds[:, :-1, :] &#xA;    &#xA;    # generate with rectified flow ode&#xA;    # step 1: encode with vision_gen_enc&#xA;    z = torch.randn((batchsize, 4, 48, 48), dtype=torch.bfloat16).cuda()&#xA;    &#xA;    dt = 1.0 / num_inference_steps&#xA;    dt = torch.zeros_like(z).cuda().to(torch.bfloat16) + dt&#xA;    &#xA;    # step 2: run ode&#xA;    attention_mask = torch.ones((2*batchsize, inputs_embeds.shape[1]+577)).to(vl_gpt.device)&#xA;    attention_mask[batchsize:, 1:inputs_embeds.shape[1]] = 0&#xA;    attention_mask = attention_mask.int()&#xA;    for step in range(num_inference_steps):&#xA;        # prepare inputs for the llm&#xA;        z_input = torch.cat([z, z], dim=0) # for cfg&#xA;        t = step / num_inference_steps * 1000.&#xA;        t = torch.tensor([t] * z_input.shape[0]).to(dt)&#xA;        z_enc = vl_gpt.vision_gen_enc_model(z_input, t)&#xA;        z_emb, t_emb, hs = z_enc[0], z_enc[1], z_enc[2]&#xA;        z_emb = z_emb.view(z_emb.shape[0], z_emb.shape[1], -1).permute(0, 2, 1)&#xA;        z_emb = vl_gpt.vision_gen_enc_aligner(z_emb)&#xA;        llm_emb = torch.cat([inputs_embeds, t_emb.unsqueeze(1), z_emb], dim=1)&#xA;&#xA;        # input to the llm&#xA;        # we apply attention mask for CFG: 1 for tokens that are not masked, 0 for tokens that are masked.&#xA;        if step == 0:&#xA;            outputs = vl_gpt.language_model.model(inputs_embeds=llm_emb, &#xA;                                             use_cache=True, &#xA;                                             attention_mask=attention_mask,&#xA;                                             past_key_values=None)&#xA;            past_key_values = []&#xA;            for kv_cache in past_key_values:&#xA;                k, v = kv_cache[0], kv_cache[1]&#xA;                past_key_values.append((k[:, :, :inputs_embeds.shape[1], :], v[:, :, :inputs_embeds.shape[1], :]))&#xA;            past_key_values = tuple(past_key_values)&#xA;        else:&#xA;            outputs = vl_gpt.language_model.model(inputs_embeds=llm_emb, &#xA;                                             use_cache=True, &#xA;                                             attention_mask=attention_mask,&#xA;                                             past_key_values=past_key_values)&#xA;        hidden_states = outputs.last_hidden_state&#xA;        &#xA;        # transform hidden_states back to v&#xA;        hidden_states = vl_gpt.vision_gen_dec_aligner(vl_gpt.vision_gen_dec_aligner_norm(hidden_states[:, -576:, :]))&#xA;        hidden_states = hidden_states.reshape(z_emb.shape[0], 24, 24, 768).permute(0, 3, 1, 2)&#xA;        v = vl_gpt.vision_gen_dec_model(hidden_states, hs, t_emb)&#xA;        v_cond, v_uncond = torch.chunk(v, 2)&#xA;        v = cfg_weight * v_cond - (cfg_weight-1.) * v_uncond&#xA;        z = z + dt * v&#xA;        &#xA;    # step 3: decode with vision_gen_dec and sdxl vae&#xA;    decoded_image = vae.decode(z / vae.config.scaling_factor).sample&#xA;    &#xA;    os.makedirs(&#39;generated_samples&#39;, exist_ok=True)&#xA;    save_path = os.path.join(&#39;generated_samples&#39;, &#34;img.jpg&#34;)&#xA;    torchvision.utils.save_image(decoded_image.clip_(-1.0, 1.0)*0.5+0.5, save_path)&#xA;&#xA;generate(&#xA;    vl_gpt,&#xA;    vl_chat_processor,&#xA;    prompt,&#xA;    cfg_weight=2.0,&#xA;    num_inference_steps=30,&#xA;    batchsize=5&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA; &lt;p&gt;For the local gradio demo, you can run with the following command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;pip install -e .[gradio]&#xA;&#xA;python demo/app_janusflow.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Have Fun!&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;4. License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is licensed under &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-CODE&#34;&gt;the MIT License&lt;/a&gt;. The use of Janus models is subject to &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-MODEL&#34;&gt;DeepSeek Model License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;5. Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{chen2025januspro,&#xA;      title={Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling}, &#xA;      author={Xiaokang Chen and Zhiyu Wu and Xingchao Liu and Zizheng Pan and Wen Liu and Zhenda Xie and Xingkai Yu and Chong Ruan},&#xA;      year={2025},&#xA;}&#xA;&#xA;@article{wu2024janus,&#xA;  title={Janus: Decoupling visual encoding for unified multimodal understanding and generation},&#xA;  author={Wu, Chengyue and Chen, Xiaokang and Wu, Zhiyu and Ma, Yiyang and Liu, Xingchao and Pan, Zizheng and Liu, Wen and Xie, Zhenda and Yu, Xingkai and Ruan, Chong and others},&#xA;  journal={arXiv preprint arXiv:2410.13848},&#xA;  year={2024}&#xA;}&#xA;&#xA;@misc{ma2024janusflow,&#xA;      title={JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation}, &#xA;      author={Yiyang Ma and Xingchao Liu and Xiaokang Chen and Wen Liu and Chengyue Wu and Zhiyu Wu and Zizheng Pan and Zhenda Xie and Haowei Zhang and Xingkai yu and Liang Zhao and Yisong Wang and Jiaying Liu and Chong Ruan},&#xA;      journal={arXiv preprint arXiv:2411.07975},&#xA;      year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;6. Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href=&#34;mailto:service@deepseek.com&#34;&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deepseek-ai/DeepSeek-VL</title>
    <updated>2025-01-29T01:28:27Z</updated>
    <id>tag:github.com,2025-01-29:/deepseek-ai/DeepSeek-VL</id>
    <link href="https://github.com/deepseek-ai/DeepSeek-VL" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepSeek-VL: Towards Real-World Vision-Language Understanding&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/images/logo.svg?sanitize=true&#34; width=&#34;60%&#34; alt=&#34;DeepSeek LLM&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.deepseek.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Homepage&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/images/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Chat&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%96%2520Chat-DeepSeek%2520VL-536af5?color=536af5&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/deepseek-ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Hugging Face&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/Tc7c45Zzu5&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/images/qr.jpeg&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Wechat&#34; src=&#34;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/deepseek_ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/LICENSE-CODE&#34;&gt; &lt;img alt=&#34;Code License&#34; src=&#34;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/LICENSE-MODEL&#34;&gt; &lt;img alt=&#34;Model License&#34; src=&#34;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#3-model-downloads&#34;&gt;&lt;b&gt;üì• Model Download&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#4-quick-start&#34;&gt;&lt;b&gt;‚ö° Quick Start&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#5-license&#34;&gt;&lt;b&gt;üìú License&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#6-citation&#34;&gt;&lt;b&gt;üìñ Citation&lt;/b&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://arxiv.org/abs/2403.05525&#34;&gt;&lt;b&gt;üìÑ Paper Link&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/papers/2403.05525&#34;&gt;&lt;b&gt;ü§ó Huggingface Paper Link&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&#34;&gt;&lt;b&gt;üëÅÔ∏è Demo&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Introducing DeepSeek-VL, an open-source Vision-Language (VL) Model designed for real-world vision and language understanding applications. DeepSeek-VL possesses general multimodal understanding capabilities, capable of processing logical diagrams, web pages, formula recognition, scientific literature, natural images, and embodied intelligence in complex scenarios.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.05525&#34;&gt;DeepSeek-VL: Towards Real-World Vision-Language Understanding&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Haoyu Lu*, Wen Liu*, Bo Zhang**, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, Chong Ruan (*Equal Contribution, **Project Lead)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/deepseek-ai/DeepSeek-VL/raw/main/images/sample.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;2. Release&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚úÖ &lt;b&gt;2024-03-14&lt;/b&gt;: Demo for DeepSeek-VL-7B available on &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/summary&gt; &#xA; &lt;br&gt;Check out the gradio demo of DeepSeek-VL-7B at &#xA; &lt;a href=&#34;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&#34;&gt;https://huggingface.co/spaces/deepseek-ai/DeepSeek-VL-7B&lt;/a&gt;. Experience its capabilities firsthand! &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚úÖ &lt;b&gt;2024-03-13&lt;/b&gt;: Support DeepSeek-VL gradio demo. &lt;/summary&gt;&#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;‚úÖ &lt;b&gt;2024-03-11&lt;/b&gt;: DeepSeek-VL family released, including &lt;code&gt;DeepSeek-VL-7B-base&lt;/code&gt;, &lt;code&gt;DeepSeek-VL-7B-chat&lt;/code&gt;, &lt;code&gt;DeepSeek-VL-1.3B-base&lt;/code&gt;, and &lt;code&gt;DeepSeek-VL-1.3B-chat&lt;/code&gt;.&lt;/summary&gt; &#xA; &lt;br&gt;The release includes a diverse set of models tailored for various applications within the DeepSeek-VL family. The models come in two sizes: 7B and 1.3B parameters, each offering base and chat variants to cater to different needs and integration scenarios. &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;3. Model Downloads&lt;/h2&gt; &#xA;&lt;p&gt;We release the DeepSeek-VL family, including 1.3B-base, 1.3B-chat, 7b-base and 7b-chat models, to the public. To support a broader and more diverse range of research within both academic and commercial communities. Please note that the use of this model is subject to the terms outlined in &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/#5-license&#34;&gt;License section&lt;/a&gt;. Commercial usage is permitted under these terms.&lt;/p&gt; &#xA;&lt;h3&gt;Huggingface&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Sequence Length&lt;/th&gt; &#xA;   &lt;th&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-VL-1.3B-base&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-vl-1.3b-base&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-VL-1.3B-chat&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-vl-1.3b-chat&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-VL-7B-base&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-vl-7b-base&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek-VL-7B-chat&lt;/td&gt; &#xA;   &lt;td&gt;4096&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-vl-7b-chat&#34;&gt;ü§ó Hugging Face&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;4. Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;On the basis of &lt;code&gt;Python &amp;gt;= 3.8&lt;/code&gt; environment, install the necessary dependencies by running the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Simple Inference Example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoModelForCausalLM&#xA;&#xA;from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM&#xA;from deepseek_vl.utils.io import load_pil_images&#xA;&#xA;&#xA;# specify the path to the model&#xA;model_path = &#34;deepseek-ai/deepseek-vl-7b-chat&#34;&#xA;vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)&#xA;tokenizer = vl_chat_processor.tokenizer&#xA;&#xA;vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)&#xA;vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()&#xA;&#xA;## single image conversation example&#xA;conversation = [&#xA;    {&#xA;        &#34;role&#34;: &#34;User&#34;,&#xA;        &#34;content&#34;: &#34;&amp;lt;image_placeholder&amp;gt;Describe each stage of this image.&#34;,&#xA;        &#34;images&#34;: [&#34;./images/training_pipelines.jpg&#34;],&#xA;    },&#xA;    {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;},&#xA;]&#xA;&#xA;## multiple images (or in-context learning) conversation example&#xA;# conversation = [&#xA;#     {&#xA;#         &#34;role&#34;: &#34;User&#34;,&#xA;#         &#34;content&#34;: &#34;&amp;lt;image_placeholder&amp;gt;A dog wearing nothing in the foreground, &#34;&#xA;#                    &#34;&amp;lt;image_placeholder&amp;gt;a dog wearing a santa hat, &#34;&#xA;#                    &#34;&amp;lt;image_placeholder&amp;gt;a dog wearing a wizard outfit, and &#34;&#xA;#                    &#34;&amp;lt;image_placeholder&amp;gt;what&#39;s the dog wearing?&#34;,&#xA;#         &#34;images&#34;: [&#xA;#             &#34;images/dog_a.png&#34;,&#xA;#             &#34;images/dog_b.png&#34;,&#xA;#             &#34;images/dog_c.png&#34;,&#xA;#             &#34;images/dog_d.png&#34;,&#xA;#         ],&#xA;#     },&#xA;#     {&#34;role&#34;: &#34;Assistant&#34;, &#34;content&#34;: &#34;&#34;}&#xA;# ]&#xA;&#xA;# load images and prepare for inputs&#xA;pil_images = load_pil_images(conversation)&#xA;prepare_inputs = vl_chat_processor(&#xA;    conversations=conversation,&#xA;    images=pil_images,&#xA;    force_batchify=True&#xA;).to(vl_gpt.device)&#xA;&#xA;# run image encoder to get the image embeddings&#xA;inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)&#xA;&#xA;# run the model to get the response&#xA;outputs = vl_gpt.language_model.generate(&#xA;    inputs_embeds=inputs_embeds,&#xA;    attention_mask=prepare_inputs.attention_mask,&#xA;    pad_token_id=tokenizer.eos_token_id,&#xA;    bos_token_id=tokenizer.bos_token_id,&#xA;    eos_token_id=tokenizer.eos_token_id,&#xA;    max_new_tokens=512,&#xA;    do_sample=False,&#xA;    use_cache=True&#xA;)&#xA;&#xA;answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)&#xA;print(f&#34;{prepare_inputs[&#39;sft_format&#39;][0]}&#34;, answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Chat&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_chat.py --model_path &#34;deepseek-ai/deepseek-vl-7b-chat&#34;&#xA;&#xA;# or local path&#xA;python cli_chat.py --model_path &#34;local model path&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -e .[gradio]&#xA;&#xA;python deepseek_vl/serve/app_deepseek.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-VL/main/images/gradio_demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Have Fun!&lt;/p&gt; &#xA;&lt;h2&gt;5. License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is licensed under &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-CODE&#34;&gt;the MIT License&lt;/a&gt;. The use of DeepSeek-VL Base/Chat models is subject to &lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/HEAD/LICENSE-MODEL&#34;&gt;DeepSeek Model License&lt;/a&gt;. DeepSeek-VL series (including Base and Chat) supports commercial use.&lt;/p&gt; &#xA;&lt;h2&gt;6. Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lu2024deepseekvl,&#xA;      title={DeepSeek-VL: Towards Real-World Vision-Language Understanding},&#xA;      author={Haoyu Lu and Wen Liu and Bo Zhang and Bingxuan Wang and Kai Dong and Bo Liu and Jingxiang Sun and Tongzheng Ren and Zhuoshu Li and Hao Yang and Yaofeng Sun and Chengqi Deng and Hanwei Xu and Zhenda Xie and Chong Ruan},&#xA;      year={2024},&#xA;      eprint={2403.05525},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;7. Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href=&#34;mailto:service@deepseek.com&#34;&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>deepseek-ai/DeepSeek-Math</title>
    <updated>2025-01-29T01:28:27Z</updated>
    <id>tag:github.com,2025-01-29:/deepseek-ai/DeepSeek-Math</id>
    <link href="https://github.com/deepseek-ai/DeepSeek-Math" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/images/logo.svg?sanitize=true&#34; width=&#34;60%&#34; alt=&#34;DeepSeek LLM&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.deepseek.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Homepage&#34; src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/images/badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://chat.deepseek.com/&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Chat&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%96%2520Chat-DeepSeek%2520LLM-536af5?color=536af5&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/deepseek-ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Hugging Face&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://replicate.com/cjwbw/deepseek-math-7b-base&#34; target=&#34;_parent&#34;&gt;&lt;img src=&#34;https://replicate.com/cjwbw/deepseek-math-7b-base/badge&#34; alt=&#34;Replicate&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://discord.gg/Tc7c45Zzu5&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Discord&#34; src=&#34;https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&amp;amp;logoColor=white&amp;amp;color=7289da&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/images/qr.jpeg&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Wechat&#34; src=&#34;https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/deepseek_ai&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Twitter Follow&#34; src=&#34;https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/LICENSE-CODE&#34;&gt; &lt;img alt=&#34;Code License&#34; src=&#34;https://img.shields.io/badge/Code_License-MIT-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/LICENSE-MODEL&#34;&gt; &lt;img alt=&#34;Model License&#34; src=&#34;https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&amp;amp;color=f5de53&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/#4-model-downloads&#34;&gt;Model Download&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/#2-evaluation-results&#34;&gt;Evaluation Results&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/#5-quick-start&#34;&gt;Quick Start&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/#6-license&#34;&gt;License&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/#7-citation&#34;&gt;Citation&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/pdf/2402.03300.pdf&#34;&gt;&lt;b&gt;Paper Link&lt;/b&gt;üëÅÔ∏è&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;1. Introduction&lt;/h2&gt; &#xA;&lt;p&gt;DeepSeekMath is initialized with &lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-coder-7b-base-v1.5&#34;&gt;DeepSeek-Coder-v1.5 7B&lt;/a&gt; and continues pre-training on math-related tokens sourced from Common Crawl, together with natural language and code data for 500B tokens. DeepSeekMath 7B has achieved an impressive score of &lt;strong&gt;51.7%&lt;/strong&gt; on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. For research purposes, we release &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/#4-model-downloads&#34;&gt;checkpoints&lt;/a&gt; of base, instruct, and RL models to the public.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/images/math.png&#34; alt=&#34;table&#34; width=&#34;70%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;2. Evaluation Results&lt;/h2&gt; &#xA;&lt;h3&gt;DeepSeekMath-Base 7B&lt;/h3&gt; &#xA;&lt;p&gt;We conduct a comprehensive assessment of the mathematical capabilities of DeepSeekMath-Base 7B, focusing on its ability to produce self-contained mathematical solutions without relying on external tools, solve math problems using tools, and conduct formal theorem proving. Beyond mathematics, we also provide a more general profile of the base model, including its performance of natural language understanding, reasoning, and programming skills.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mathematical problem solving with step-by-step reasoning&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/images/base_results_1.png&#34; alt=&#34;table&#34; width=&#34;70%&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Mathematical problem solving with tool use&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/images/base_results_2.png&#34; alt=&#34;table&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Natural Language Understanding, Reasoning, and Code&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/images/base_results_3.png&#34; alt=&#34;table&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The evaluation results from the tables above can be summarized as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Superior Mathematical Reasoning:&lt;/strong&gt; On the competition-level MATH dataset, DeepSeekMath-Base 7B outperforms existing open-source base models by more than 10% in absolute terms through few-shot chain-of-thought prompting, and also surpasses Minerva 540B.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Strong Tool Use Ability:&lt;/strong&gt; Continuing pre-training with DeepSeekCoder-Base-7B-v1.5 enables DeepSeekMath-Base 7B to more effectively solve and prove mathematical problems by writing programs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Comparable Reasoning and Coding Performance:&lt;/strong&gt; DeepSeekMath-Base 7B achieves performance in reasoning and coding that is comparable to that of DeepSeekCoder-Base-7B-v1.5.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;DeepSeekMath-Instruct and -RL 7B&lt;/h3&gt; &#xA;&lt;p&gt;DeepSeekMath-Instruct 7B is a mathematically instructed tuning model derived from DeepSeekMath-Base 7B, while DeepSeekMath-RL 7B is trained on the foundation of DeepSeekMath-Instruct 7B, utilizing our proposed Group Relative Policy Optimization (GRPO) algorithm.&lt;/p&gt; &#xA;&lt;p&gt;We evaluate mathematical performance both without and with tool use, on 4 quantitative reasoning benchmarks in English and Chinese. As shown in Table, DeepSeekMath-Instruct 7B demonstrates strong performance of step-by-step reasoning, and DeepSeekMath-RL 7B approaches an accuracy of 60% on MATH with tool use, surpassing all existing open-source models.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/images/instruct_results.png&#34; alt=&#34;table&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;3. Data Collection&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Step 1: Select &lt;a href=&#34;https://arxiv.org/pdf/2310.06786.pdf&#34;&gt;OpenWebMath&lt;/a&gt;, a collection of high-quality mathematical web texts, as our initial seed corpus for training a FastText model.&lt;/li&gt; &#xA; &lt;li&gt;Step 2: Use the FastText model to retrieve mathematical web pages from the deduplicated Common Crawl database.&lt;/li&gt; &#xA; &lt;li&gt;Step 3: Identify potential math-related domains through statistical analysis.&lt;/li&gt; &#xA; &lt;li&gt;Step 4: Manually annotate URLs within these identified domains that are associated with mathematical content.&lt;/li&gt; &#xA; &lt;li&gt;Step 5: Add web pages linked to these annotated URLs, but not yet collected, to the seed corpus. Jump to step 1 until four iterations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/images/data_pipeline.png&#34; alt=&#34;table&#34; width=&#34;80%&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;After four iterations of data collection, we end up with &lt;strong&gt;35.5M&lt;/strong&gt; mathematical web pages, totaling &lt;strong&gt;120B&lt;/strong&gt; tokens.&lt;/p&gt; &#xA;&lt;h2&gt;4. Model Downloads&lt;/h2&gt; &#xA;&lt;p&gt;We release the DeepSeekMath 7B, including base, instruct and RL models, to the public. To support a broader and more diverse range of research within both academic and commercial communities. Please &lt;strong&gt;note&lt;/strong&gt; that the use of this model is subject to the terms outlined in &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/#6-license&#34;&gt;License section&lt;/a&gt;. Commercial usage is permitted under these terms.&lt;/p&gt; &#xA;&lt;h3&gt;Huggingface&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Sequence Length&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DeepSeekMath-Base 7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4096&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ü§ó &lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-math-7b-base&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DeepSeekMath-Instruct 7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4096&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ü§ó &lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;DeepSeekMath-RL 7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4096&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ü§ó &lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-math-7b-rl&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;5. Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You can directly employ &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Huggingface&#39;s Transformers&lt;/a&gt; for model inference.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text Completion&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig&#xA;&#xA;model_name = &#34;deepseek-ai/deepseek-math-7b-base&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=&#34;auto&#34;)&#xA;model.generation_config = GenerationConfig.from_pretrained(model_name)&#xA;model.generation_config.pad_token_id = model.generation_config.eos_token_id&#xA;&#xA;text = &#34;The integral of x^2 from 0 to 2 is&#34;&#xA;inputs = tokenizer(text, return_tensors=&#34;pt&#34;)&#xA;outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)&#xA;&#xA;result = tokenizer.decode(outputs[0], skip_special_tokens=True)&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Chat Completion&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig&#xA;&#xA;model_name = &#34;deepseek-ai/deepseek-math-7b-instruct&#34;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=&#34;auto&#34;)&#xA;model.generation_config = GenerationConfig.from_pretrained(model_name)&#xA;model.generation_config.pad_token_id = model.generation_config.eos_token_id&#xA;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;what is the integral of x^2 from 0 to 2?\nPlease reason step by step, and put your final answer within \boxed{}.&#34;}&#xA;]&#xA;input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=&#34;pt&#34;)&#xA;outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100)&#xA;&#xA;result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True)&#xA;print(result)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Avoiding the use of the provided function &lt;code&gt;apply_chat_template&lt;/code&gt;, you can also interact with our model following the sample template. Note that &lt;code&gt;messages&lt;/code&gt; should be replaced by your input.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;User: {messages[0][&#39;content&#39;]}&#xA;&#xA;Assistant: {messages[1][&#39;content&#39;]}&amp;lt;ÔΩúend‚ñÅof‚ñÅsentenceÔΩú&amp;gt;User: {messages[2][&#39;content&#39;]}&#xA;&#xA;Assistant:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; By default (&lt;code&gt;add_special_tokens=True&lt;/code&gt;), our tokenizer automatically adds a &lt;code&gt;bos_token&lt;/code&gt; (&lt;code&gt;&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;&lt;/code&gt;) before the input text. Additionally, since the system prompt is not compatible with this version of our models, we DO NOT RECOMMEND including the system prompt in your input.&lt;/p&gt; &#xA;&lt;p&gt;‚ùó‚ùó‚ùó &lt;strong&gt;Please use chain-of-thought prompt to test DeepSeekMath-Instruct and DeepSeekMath-RL:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;English questions: &lt;strong&gt;{question}\nPlease reason step by step, and put your final answer within \boxed{}.&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Chinese questions: &lt;strong&gt;{question}\nËØ∑ÈÄöËøáÈÄêÊ≠•Êé®ÁêÜÊù•Ëß£Á≠îÈóÆÈ¢òÔºåÂπ∂ÊääÊúÄÁªàÁ≠îÊ°àÊîæÁΩÆ‰∫é\boxed{}‰∏≠„ÄÇ&lt;/strong&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;6. License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is licensed under the MIT License. The use of DeepSeekMath models is subject to the Model License. DeepSeekMath supports commercial use.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/LICENSE-CODE&#34;&gt;LICENSE-CODE&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/deepseek-ai/DeepSeek-Math/main/LICENSE-MODEL&#34;&gt;LICENSE-MODEL&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;7. Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{deepseek-math,&#xA;  author = {Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, Daya Guo},&#xA;  title = {DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models},&#xA;  journal = {CoRR},&#xA;  volume = {abs/2402.03300},&#xA;  year = {2024},&#xA;  url = {https://arxiv.org/abs/2402.03300},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;8. Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please raise an issue or contact us at &lt;a href=&#34;mailto:service@deepseek.com&#34;&gt;service@deepseek.com&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>