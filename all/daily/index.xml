<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-14T01:22:20Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>bcdannyboy/CVE-2023-44487</title>
    <updated>2023-10-14T01:22:20Z</updated>
    <id>tag:github.com,2023-10-14:/bcdannyboy/CVE-2023-44487</id>
    <link href="https://github.com/bcdannyboy/CVE-2023-44487" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Basic vulnerability scanning to see if web servers may be vulnerable to CVE-2023-44487&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2023-44487&lt;/h1&gt; &#xA;&lt;p&gt;Basic vulnerability scanning to see if web servers may be vulnerable to CVE-2023-44487&lt;/p&gt; &#xA;&lt;p&gt;This tool checks to see if a website is vulnerable to CVE-2023-44487 completely non-invasively.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The tool checks if a web server accepts HTTP/2 requests without downgrading them&lt;/li&gt; &#xA; &lt;li&gt;If the web server accepts and does not downgrade HTTP/2 requests the tool attempts to open a connection stream and subsequently reset it&lt;/li&gt; &#xA; &lt;li&gt;If the web server accepts the creation and resetting of a connection stream then the server is definitely vulnerable, if it only accepts HTTP/2 requests but the stream connection fails it may be vulnerable if the server-side capabilities are enabled.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;To run,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python3 -m pip install -r requirements.txt&#xA;&#xA;$ python3 cve202344487.py -i input_urls.txt -o output_results.csv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also specify an HTTP proxy to proxy all the requests through with the &lt;code&gt;--proxy&lt;/code&gt; flag&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ python3 cve202344487.py -i input_urls.txt -o output_results.csv --proxy http://proxysite.com:1234&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script outputs a CSV file with the following columns&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Timestamp: a timestamp of the request&lt;/li&gt; &#xA; &lt;li&gt;Source Internal IP: The internal IP address of the host sending the HTTP requessts&lt;/li&gt; &#xA; &lt;li&gt;Source External IP: The external IP address of the host sending the HTTP requests&lt;/li&gt; &#xA; &lt;li&gt;URL: The URL being scanned&lt;/li&gt; &#xA; &lt;li&gt;Vulnerability Status: &#34;VULNERABLE&#34;/&#34;LIKELY&#34;/&#34;POSSIBLE&#34;/&#34;SAFE&#34;/&#34;ERROR&#34;&lt;/li&gt; &#xA; &lt;li&gt;Error/Downgrade Version: The error or the version the HTTP server downgrades the request to&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: &#34;Vulnerable&#34; in this context means that it is confirmed that an attacker can reset the a stream connection without issue, it does not take into account implementation-specific or volume-based detections&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Dockerized&lt;/h1&gt; &#xA;&lt;p&gt;Build&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker build -t py-cve-2023-44487 .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ docker run --rm -v /path/to/urls:/shared py-cve-2023-44487 -i /shared/input_urls.txt -o /shared/output_results.csv&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>THUDM/CogVLM</title>
    <updated>2023-10-14T01:22:20Z</updated>
    <id>tag:github.com,2023-10-14:/THUDM/CogVLM</id>
    <link href="https://github.com/THUDM/CogVLM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;a state-of-the-art-level open visual language model&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CogVLM&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;CogVLM 是一个强大的开源视觉语言模型，利用视觉专家模块深度整合语言编码和视觉编码，在 14 项权威跨模态基准上取得了 SOTA 性能。目前仅支持英文，后续会提供中英双语版本支持，欢迎持续关注！&lt;/p&gt; &#xA;&lt;p&gt;📖 &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/cogvlm-paper.pdf&#34;&gt;Paper（论文）&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;🌐 &lt;a href=&#34;http://36.103.203.44:7861/&#34;&gt;web demo（测试网址）&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM, a powerful open-source visual language foundation model. Different from the popular shallow-align method which maps image features into the input space of language model, &lt;strong&gt;CogVLM bridges the gap between the frozen pretrained language model and image encoder by a trainable visual expert module in the attention and FFN layers&lt;/strong&gt;. CogVLM enables deep fusion of visual language features without sacrificing any performance on NLP tasks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;CogVLM-17B achieves state-of-the-art performance on 10 classic cross-modal benchmarks, including NoCaps, Flicker30k captioning, RefCOCO, RefCOCO+, RefCOCOg, Visual7W, GQA, ScienceQA, VizWiz VQA and TDIUC, and rank the 2nd on VQAv2, OKVQA, TextVQA, COCO captioning, etc., &lt;strong&gt;surpassing or matching PaLI-X 55B&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;We anticipate that the open-sourcing of CogVLM will greatly help the research and industrial application of visual understanding.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/metrics.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;CogVLM is powerful for answering various types of visual questions, including &lt;strong&gt;Detailed Description &amp;amp; Visual Question Answering&lt;/strong&gt;, &lt;strong&gt;Complex Counting&lt;/strong&gt;, &lt;strong&gt;Visual Math Problem Solving&lt;/strong&gt;, &lt;strong&gt;OCR-Free Reasonging&lt;/strong&gt;, &lt;strong&gt;OCR-Free Visual Question Answering&lt;/strong&gt;, &lt;strong&gt;World Knowledge&lt;/strong&gt;, &lt;strong&gt;Referring Expression Comprehension&lt;/strong&gt;, &lt;strong&gt;Programming with Visual Input&lt;/strong&gt;, &lt;strong&gt;Grounding with Caption&lt;/strong&gt;, &lt;strong&gt;Grounding Visual Question Answering&lt;/strong&gt;, etc.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/compare.png&#34; width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;!-- ![compare](assets/compare.png) --&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to expand/collapse more examples&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/chat.png&#34; alt=&#34;Chat Examples&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Method&lt;/h2&gt; &#xA;&lt;p&gt;CogVLM model comprises four fundamental components: a vision transformer (ViT) encoder, an MLP adapter, a pretrained large language model (GPT), and a visual expert module. See &lt;a href=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/cogvlm-paper.pdf&#34;&gt;Paper&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/method.png&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;python -m spacy download en_core_web_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Online Web Demo&lt;/h3&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;http://36.103.203.44:7861/&#34;&gt;web demo&lt;/a&gt; based on &lt;a href=&#34;https://gradio.app&#34;&gt;Gradio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/THUDM/CogVLM/main/assets/web_demo.png&#34; width=&#34;70%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Local Web Demo&lt;/h3&gt; &#xA;&lt;p&gt;We also offer a local web demo based on Gradio. First, install Gradio by running: &lt;code&gt;pip install gradio&lt;/code&gt;. Then download and enter this repository and run &lt;code&gt;web_demo.py&lt;/code&gt;. See the next section for detailed usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python web_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;python web_demo.py --from_pretrained cogvlm-grounding-generalist --version base --english --bf16&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Terminal Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python cli_demo.py --from_pretrained cogvlm-base-224 --version base --english --bf16 --no_prompt&#xA;python cli_demo.py --from_pretrained cogvlm-base-490 --version base --english --bf16 --no_prompt&#xA;python cli_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;python cli_demo.py --from_pretrained cogvlm-grounding-base --version base --english --bf16&#xA;python cli_demo.py --from_pretrained cogvlm-grounding-generalist --version base --english --bf16&#xA;# We also support model parallel inference, which splits model to multiple (2/4/8) GPUs.&#xA;torchrun --standalone --nnodes=1 --nproc-per-node=2 cli_demo.py --from_pretrained cogvlm-chat --version chat --english --bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The program will automatically download the sat model and interact in the command line. You can generate replies by entering instructions and pressing enter. Enter &#39;clear&#39; to clear the conversation history and &#39;stop&#39; to stop the program.&lt;/p&gt; &#xA;&lt;p&gt;Note:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have trouble in accessing huggingface.co, you can add &lt;code&gt;--local_tokenizer /path/to/vicuna-7b-v1.5&lt;/code&gt; to load the tokenizer.&lt;/li&gt; &#xA; &lt;li&gt;If you have trouble in automatically downloading model with 🔨&lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt;, try downloading from 🤖&lt;a href=&#34;https://www.modelscope.cn/models/ZhipuAI/CogVLM/summary&#34;&gt;modelscope&lt;/a&gt; or 🤗&lt;a href=&#34;https://huggingface.co/THUDM/CogVLM&#34;&gt;huggingface&lt;/a&gt; manually.&lt;/li&gt; &#xA; &lt;li&gt;Download model using 🔨&lt;a href=&#34;https://github.com/THUDM/SwissArmyTransformer&#34;&gt;SAT&lt;/a&gt;, the model will be saved to the default location &lt;code&gt;~/.sat_models&lt;/code&gt;. Change the default location by setting the environment variable &lt;code&gt;SAT_HOME&lt;/code&gt;. For example, if you want to save the model to &lt;code&gt;/path/to/my/models&lt;/code&gt;, you can run &lt;code&gt;export SAT_HOME=/path/to/my/models&lt;/code&gt; before running the python command.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The program provides the following hyperparameters to control the generation process:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;usage: cli_demo.py [-h] [--max_length MAX_LENGTH] [--top_p TOP_P] [--top_k TOP_K] [--temperature TEMPERATURE] [--english]&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  --max_length MAX_LENGTH&#xA;                        max length of the total sequence&#xA;  --top_p TOP_P         top p for nucleus sampling&#xA;  --top_k TOP_K         top k for top k sampling&#xA;  --temperature TEMPERATURE&#xA;                        temperature for sampling&#xA;  --english             only output English&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;Start by downloading the &lt;a href=&#34;https://www.kaggle.com/datasets/aadhavvignesh/captcha-images&#34;&gt;Captcha Images dataset&lt;/a&gt;. Once downloaded, extract the contents of the ZIP file.&lt;/p&gt; &#xA;&lt;p&gt;To create a train/validation/test split in the ratio of 80/5/15, execute the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/split_dataset.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Kickstart the fine-tuning process with this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/finetune_(224/490)_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, merge the model to model_parallel_size=1: (replace 4 with your training MP_SIZE)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --standalone --nnodes=1 --nproc-per-node=4 merge_model.py --version base --bf16 --from_pretrained ./checkpoints/merged_lora_(224/490)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To evaluate the performance of your model, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/evaluate_(224/490).sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is recommended to use 490 version. However, if you have limited GPU resources (such as only one node with eight 24GB 3090 cards), you can try 224 version with model parallel. The anticipated result is around 95% accuracy on test set. It is worth noting that the fine-tuning examples only tune limited parameters. If you want to improve performance, you can change trainable parameters in &lt;code&gt;finetune_demo.py&lt;/code&gt; as needed.&lt;/p&gt; &#xA;&lt;h2&gt;Model Quantization&lt;/h2&gt; &#xA;&lt;p&gt;Model quantization is not possible right now, but we are working on it. We will release the quantized model as soon as possible.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository is open source under the Apache-2.0 license, while the use of the CogVLM model weights must comply with the Model License.&lt;/p&gt; &#xA;&lt;h2&gt;Citation &amp;amp; Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, please consider citing the following papers&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the instruction fine-tuning phase of the CogVLM, there are some English image-text data from the &lt;a href=&#34;https://github.com/Vision-CAIR/MiniGPT-4&#34;&gt;MiniGPT-4&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLAVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/FuxiaoLiu/LRV-Instruction&#34;&gt;LRV-Instruction&lt;/a&gt;, &lt;a href=&#34;https://github.com/SALT-NLP/LLaVAR&#34;&gt;LLaVAR&lt;/a&gt; and &lt;a href=&#34;https://github.com/shikras/shikra&#34;&gt;Shikra&lt;/a&gt; projects, as well as many classic cross-modal work datasets. We sincerely thank them for their contributions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fastfire/deepdarkCTI</title>
    <updated>2023-10-14T01:22:20Z</updated>
    <id>tag:github.com,2023-10-14:/fastfire/deepdarkCTI</id>
    <link href="https://github.com/fastfire/deepdarkCTI" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Collection of Cyber Threat Intelligence sources from the deep and dark web&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;deepdarkCTI&lt;/h1&gt; &#xA;&lt;p&gt;Collection of Cyber Threat Intelligence sources from the Deep and Dark Web&lt;/p&gt; &#xA;&lt;p&gt;The aim of this project is to collect the sources, present in the Deep and Dark web, which can be useful in Cyber Threat Intelligence contexts.&lt;/p&gt; &#xA;&lt;p&gt;The contributors of the project (or people active in the field of Cyber Threat Intelligence) have a &lt;strong&gt;Telegram groups&lt;/strong&gt; available to propose new sources to be integrated within the project and to have a place to discuss the tactics and techniques of research and analysis that are used daily. It is possible to request access to the Telegram group by sending a request to &lt;a href=&#34;https://t.me/fastfire83&#34;&gt;https://t.me/fastfire83&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or, if you want to contribute with a donation, you can. We added this possibility because it was requested by some followers of the project. The donations, as well as the use of the same, will be managed in total transparency and will be used exclusively to build resources related to the deepdarkCTI project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.buymeacoffee.com/fastfire&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://cdn.buymeacoffee.com/buttons/default-orange.png&#34; alt=&#34;Buy Me A Coffee&#34; height=&#34;41&#34; width=&#34;174&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;What is Cyber Threat Intelligence?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Cyber Threat Intelligence (CTI) is defined as the collection and analysis of information about threats and adversaries and drawing patterns that provide an ability to make knowledgeable decisions for the preparedness, prevention and response actions against various cyber attacks.&lt;/p&gt; &#xA;&lt;p&gt;CTI involves collecting, researching and analyzing trends and technical developments in the area of cyber threats and if often presented in the form of Indicators of Compromise (IoCs) or threat feeds, provides evidence-base knowledge regarding an organization&#39;s unique threat landscape.&lt;/p&gt; &#xA;&lt;p&gt;In Cyber Threat Intelligence, analysis if performed based on the intent, capability and opportunity triad. With the study of this triad, experts can evaluate and make informed, forward-learning strategic, operational and tactical decisions on existing or emerging threats to the organization.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;There are three types of Threat Intelligence:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Strategic&lt;/em&gt; - provides high-level information regarding cyber security posture, threats and its impact on business.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Tactical&lt;/em&gt; - provides information related to threat actor&#39;s Tactics, Techniques and Procedures (TTPs) used to perform attacks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Operational&lt;/em&gt; - provides information about specific threats against the organization.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Typical sources of intelligence are:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open Source Intelligence (OSINT)&lt;/li&gt; &#xA; &lt;li&gt;Human Intelligence&lt;/li&gt; &#xA; &lt;li&gt;Counter Intelligence&lt;/li&gt; &#xA; &lt;li&gt;Internal Intelligence&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Through this project, which takes into consideration the OSINT sources related to the Deep and Dark Web domain, we aim to monitor the intelligence information present in the following sources:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Telegram channels, groups and chats&lt;/li&gt; &#xA; &lt;li&gt;Discord channels&lt;/li&gt; &#xA; &lt;li&gt;ransomware gangs sites&lt;/li&gt; &#xA; &lt;li&gt;forums related to cyber criminal activities and data leaks&lt;/li&gt; &#xA; &lt;li&gt;markets&lt;/li&gt; &#xA; &lt;li&gt;exploits databases&lt;/li&gt; &#xA; &lt;li&gt;Twitter accounts&lt;/li&gt; &#xA; &lt;li&gt;RaaS (Ransomware As A Service) sites&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition, within the &lt;strong&gt;methods&lt;/strong&gt; file, various techniques for searching and analyzing sources are described.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#fastfire/deepdarkCTI&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=fastfire/deepdarkCTI&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>