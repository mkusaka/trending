<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-11T01:28:10Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>potpie-ai/potpie</title>
    <updated>2025-02-11T01:28:10Z</updated>
    <id>tag:github.com,2025-02-11:/potpie-ai/potpie</id>
    <link href="https://github.com/potpie-ai/potpie" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Prompt-To-Agent : Create custom engineering agents for your codebase&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://potpie.ai?utm_source=github&#34;&gt; &lt;img src=&#34;https://github.com/user-attachments/assets/1a0b9824-833b-4c0a-b56d-ede5623295ca&#34; width=&#34;318px&#34; alt=&#34;Momentum logo&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://trendshift.io/repositories/12918&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/12918&#34; alt=&#34;potpie-ai%2Fpotpie | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://app.potpie.ai&#34; rel=&#34;dofollow&#34;&gt;App&lt;/a&gt; | &lt;a href=&#34;https://docs.potpie.ai&#34; rel=&#34;dofollow&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://docs.potpie.ai/open-source&#34; rel=&#34;dofollow&#34;&gt;API Reference&lt;/a&gt; | &lt;a href=&#34;https://app.potpie.ai/newchat?repo=potpie-ai/potpie&amp;amp;branch=main&#34; rel=&#34;dofollow&#34;&gt;Chat with ü•ß Repo&lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/potpie-ai/potpie/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/potpie-ai/potpie&#34; alt=&#34;Apache 2.0&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/potpie-ai/potpie&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/potpie-ai/potpie&#34; alt=&#34;GitHub Repo stars&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://discord.gg/ryk5CMD5v6&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Join%20our-Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Join our Discord&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://twitter.com/intent/tweet?text=I%2520created%2520custom%2520engineering%2520agents%2520for%2520my%2520codebase%2520in%2520minutes%2520with%2520potpie.ai%2520@potpiedotai%2520!%F0%9F%A5%A7&#34;&gt; &lt;img alt=&#34;tweet&#34; src=&#34;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; &lt;p&gt;Prompt-To-Agent: Create custom engineering agents for your code&lt;/p&gt; &lt;/h1&gt; &#xA;&lt;p&gt;Potpie is an open-source platform that creates AI agents specialized in your codebase, enabling automated code analysis, testing, and development tasks. By building a comprehensive knowledge graph of your code, Potpie&#39;s agents can understand complex relationships and assist with everything from debugging to feature development.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;1506&#34; alt=&#34;Screenshot 2025-01-09 at 2 18 18‚ÄØPM&#34; src=&#34;https://github.com/user-attachments/assets/a400b48f-dc4c-47b1-a42b-26eaf062adb2&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üìö Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#why-potpie&#34;&gt;ü•ß Why Potpie?&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#prebuilt-agents&#34;&gt;ü§ñ Our Prebuilt Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#potpies-tooling-system&#34;&gt;üõ†Ô∏è Tooling&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#getting-started&#34;&gt;üöÄ Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#use-cases&#34;&gt;üí° Use Cases&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#custom-agents-upgrade&#34;&gt;üõ†Ô∏è Custom Agents&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#accessing-agents-via-api-key&#34;&gt;üóùÔ∏è Accessing Agents via API Key&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#make-potpie-your-own&#34;&gt;üé® Make Potpie Your Own&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#contributing&#34;&gt;ü§ù Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#license&#34;&gt;üìú License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/#-thanks-to-all-contributors&#34;&gt;üí™ Contributors&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü•ß Why Potpie?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;Deep Code Understanding&lt;/strong&gt;: Built-in knowledge graph captures relationships between code components&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ &lt;strong&gt;Pre-built &amp;amp; Custom Agents&lt;/strong&gt;: Ready-to-use agents for common tasks + build your own&lt;/li&gt; &#xA; &lt;li&gt;üîÑ &lt;strong&gt;Seamless Integration&lt;/strong&gt;: Works with your existing development workflow&lt;/li&gt; &#xA; &lt;li&gt;üìà &lt;strong&gt;Flexible&lt;/strong&gt;: Handles codebases of any size or language&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ñ Potpie&#39;s Prebuilt Agents&lt;/h2&gt; &#xA;&lt;p&gt;Potpie offers a suite of specialized codebase agents for automating and optimizing key aspects of software development:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Debugging Agent&lt;/strong&gt;: Automatically analyzes stacktraces and provides debugging steps specific to your codebase.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Codebase Q&amp;amp;A Agent&lt;/strong&gt;: Answers questions about your codebase and explains functions, features, and architecture.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code Changes Agent&lt;/strong&gt;: Analyzes code changes, identifies affected APIs, and suggests improvements before merging.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Integration Test Agent&lt;/strong&gt;: Generates integration test plans and code for flows to ensure components work together properly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unit Test Agent&lt;/strong&gt;: Automatically creates unit test plan and code for individual functions to enhance test coverage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLD Agent&lt;/strong&gt;: Creates a low level design for implementing a new feature by providing functional requirements to this agent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Code Generation Agent&lt;/strong&gt;: Generates code for new features, refactors existing code, and suggests optimizations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Potpie&#39;s Tooling System&lt;/h2&gt; &#xA;&lt;p&gt;Potpie provides a set of tools that agents can use to interact with the knowledge graph and the underlying infrastructure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;get_code_from_probable_node_name&lt;/strong&gt;: Retrieves code snippets based on a probable node name.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;get_code_from_node_id&lt;/strong&gt;: Fetches code associated with a specific node ID.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;get_code_from_multiple_node_ids&lt;/strong&gt;: Retrieves code snippets for multiple node IDs simultaneously.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ask_knowledge_graph_queries&lt;/strong&gt;: Executes vector similarity searches to obtain relevant information.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;get_nodes_from_tags&lt;/strong&gt;: Retrieves nodes tagged with specific keywords.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;get_code_graph_from_node_id/name&lt;/strong&gt;: Fetches code graph structures for a specific node.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;change_detection&lt;/strong&gt;: Detects changes in the current branch compared to the default branch.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;get_code_file_structure&lt;/strong&gt;: Retrieves the file structure of the codebase.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docker installed and running&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API key&lt;/li&gt; &#xA; &lt;li&gt;Git installed (for repository access)&lt;/li&gt; &#xA; &lt;li&gt;Python 3.10.x&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Setup Steps&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Install Python 3.10&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download and install Python 3.10 from the official Python website: &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;https://www.python.org/downloads/release/python-3100/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prepare Your Environment&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file based on the &lt;code&gt;.env.template&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Add the following required configurations:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;isDevelopmentMode=enabled&#xA;ENV=development&#xA;OPENAI_API_KEY=&amp;lt;your-openai-key&amp;gt;&#xA;POSTGRES_SERVER=postgresql://postgres:mysecretpassword@localhost:5432/momentum&#xA;NEO4J_URI=bolt://127.0.0.1:7687&#xA;NEO4J_USERNAME=neo4j&#xA;NEO4J_PASSWORD=mysecretpassword&#xA;REDISHOST=127.0.0.1&#xA;REDISPORT=6379&#xA;BROKER_URL=redis://127.0.0.1:6379/0&#xA;CELERY_QUEUE_NAME=dev&#xA;defaultUsername=defaultuser&#xA;PROJECT_PATH=projects #repositories will be downloaded/cloned to this path on your system.&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Create a Virtual Environment using Python 3.10:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3.10 -m venv venv&#xA;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;alternatively, you can also use the &lt;code&gt;virtualenv&lt;/code&gt; library.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Install dependencies in your venv:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Potpie&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;chmod +x start.sh&#xA;./start.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Authentication Setup&lt;/strong&gt; (Skip this step in development mode)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST &#39;http://localhost:8001/api/v1/login&#39; \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{&#xA;    &#34;email&#34;: &#34;your-email&#34;,&#xA;    &#34;password&#34;: &#34;your-password&#34;&#xA;  }&#39;&#xA;# Save the bearer token from the response for subsequent requests&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Initialize Repository Parsing&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For development mode:&#xA;curl -X POST &#39;http://localhost:8001/api/v1/parse&#39; \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{&#xA;    &#34;repo_path&#34;: &#34;path/to/local/repo&#34;,&#xA;    &#34;branch_name&#34;: &#34;main&#34;&#xA;  }&#39;&#xA;&#xA;# For production mode:&#xA;curl -X POST &#39;http://localhost:8001/api/v1/parse&#39; \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{&#xA;    &#34;repo_name&#34;: &#34;owner/repo-name&#34;,&#xA;    &#34;branch_name&#34;: &#34;main&#34;&#xA;  }&#39;&#xA;# Save the project_id from the response&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Monitor Parsing Status&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X GET &#39;http://localhost:8001/api/v1/parsing-status/your-project-id&#39;&#xA;# Wait until parsing is complete&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;View Available Agents&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X GET &#39;http://localhost:8001/api/v1/list-available-agents/?list_system_agents=true&#39;&#xA;# Note down the agent_id you want to use&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create a Conversation&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST &#39;http://localhost:8001/api/v1/conversations/&#39; \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{&#xA;    &#34;user_id&#34;: &#34;your_user_id&#34;,&#xA;    &#34;title&#34;: &#34;My First Conversation&#34;,&#xA;    &#34;status&#34;: &#34;active&#34;,&#xA;    &#34;project_ids&#34;: [&#34;your-project-id&#34;],&#xA;    &#34;agent_ids&#34;: [&#34;chosen-agent-id&#34;]&#xA;  }&#39;&#xA;# Save the conversation_id from the response&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start Interacting with Your Agent&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X POST &#39;http://localhost:8001/api/v1/conversations/your-conversation-id/message/&#39; \&#xA;  -H &#39;Content-Type: application/json&#39; \&#xA;  -d &#39;{&#xA;    &#34;content&#34;: &#34;Your question or request here&#34;&#xA;  }&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;View Conversation History&lt;/strong&gt; (Optional)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -X GET &#39;http://localhost:8001/api/v1/conversations/your-conversation-id/messages/?start=0&amp;amp;limit=10&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üí° Use Cases&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Onboarding&lt;/strong&gt;: For developers new to a codebase, the codebase QnA agent helps them understand the codebase and get up to speed quickly. Ask it how to setup a new project, how to run the tests etc&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We tried to onboard ourselves with Potpie to the &lt;a href=&#34;https://github.com/AgentOps-AI/AgentOps&#34;&gt;&lt;strong&gt;AgentOps&lt;/strong&gt;&lt;/a&gt; codebase and it worked like a charm : Video &lt;a href=&#34;https://youtu.be/_mPixNDn2r8&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Codebase Understanding&lt;/strong&gt;: Answer questions about any library you&#39;re integrating, explain functions, features, and architecture.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We used the Q&amp;amp;A agent to understand the underlying working of a feature of the &lt;a href=&#34;https://github.com/CrewAIInc/CrewAI&#34;&gt;&lt;strong&gt;CrewAI&lt;/strong&gt;&lt;/a&gt; codebase that was not documented in official docs : Video &lt;a href=&#34;https://www.linkedin.com/posts/dhirenmathur_what-do-you-do-when-youre-stuck-and-even-activity-7256704603977613312-8X8G&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Low Level Design&lt;/strong&gt;: Get detailed implementation plans for new features or improvements before writing code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We fed an open issue from the &lt;a href=&#34;https://github.com/Portkey-AI/Gateway&#34;&gt;&lt;strong&gt;Portkey-AI/Gateway&lt;/strong&gt;&lt;/a&gt; project to this agent to generate a low level design for it: Video &lt;a href=&#34;https://www.linkedin.com/posts/dhirenmathur_potpie-ai-agents-vs-llms-i-am-extremely-activity-7255607456448286720-roOC&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reviewing Code Changes&lt;/strong&gt;: Understand the functional impact of changes and compute the blast radius of modifications.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Here we analyse a PR from the &lt;a href=&#34;https://github.com/mem0ai/mem0&#34;&gt;&lt;strong&gt;mem0ai/mem0&lt;/strong&gt;&lt;/a&gt; codebase and understand its blast radius : Video &lt;a href=&#34;https://www.linkedin.com/posts/dhirenmathur_prod-is-down-three-words-every-activity-7257007131613122560-o4A7&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Debugging&lt;/strong&gt;: Get step-by-step debugging guidance based on stacktraces and codebase context.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt;: Generate contextually aware unit and integration test plans and test code that understand your codebase&#39;s structure and purpose.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Custom Agents &lt;a href=&#34;https://potpie.ai/pricing&#34;&gt;Upgrade ‚ú®&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;With Custom Agents, you can design personalized tools that handle repeatable tasks with precision. Key components include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;System Instructions&lt;/strong&gt;: Define the agent&#39;s task, goal, and expected output&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agent Information&lt;/strong&gt;: Metadata about the agent&#39;s role and context&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tasks&lt;/strong&gt;: Individual steps for job completion&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Functions for querying the knowledge graph or retrieving code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üóùÔ∏è Accessing Agents via API Key&lt;/h2&gt; &#xA;&lt;p&gt;You can access Potpie Agents through an API key, enabling integration into CI/CD workflows and other automated processes. For detailed instructions, please refer to the &lt;a href=&#34;https://docs.potpie.ai/agents/api-access&#34;&gt;Potpie API documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generate an API Key&lt;/strong&gt;: Easily create an API key for secure access.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parse Repositories&lt;/strong&gt;: Use the Parse API to analyze code repositories and obtain a project ID.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Monitor Parsing Status&lt;/strong&gt;: Check the status of your parsing requests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create Conversations&lt;/strong&gt;: Initiate conversations with specific agents using project and agent IDs adn get a conversation id.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Send Messages&lt;/strong&gt;: Communicate with agents by sending messages within a conversation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üé® Make Potpie Your Own&lt;/h2&gt; &#xA;&lt;p&gt;Potpie is designed to be flexible and customizable. Here are key areas to personalize your own deployment:&lt;/p&gt; &#xA;&lt;h3&gt;1. System Prompts Configuration&lt;/h3&gt; &#xA;&lt;p&gt;Modify prompts in &lt;code&gt;app/modules/intelligence/prompts/system_prompt_setup.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2. Add New Agents&lt;/h3&gt; &#xA;&lt;p&gt;Create new agents in &lt;code&gt;app/modules/intelligence/agents/chat_agents&lt;/code&gt; and &lt;code&gt;app/modules/intelligence/agents/agentic_tools&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;3. Agent Behavior Customization&lt;/h3&gt; &#xA;&lt;p&gt;Modify guidelines within each agent&#39;s prompt in the &lt;code&gt;app/modules/intelligence/agents&lt;/code&gt; directory&lt;/p&gt; &#xA;&lt;h3&gt;4. Tool Integration&lt;/h3&gt; &#xA;&lt;p&gt;Edit or add tools in the &lt;code&gt;app/modules/intelligence/tools&lt;/code&gt; directory&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions! To contribute:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repository&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch (&lt;code&gt;git checkout -b feature-branch&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Make your changes&lt;/li&gt; &#xA; &lt;li&gt;Commit (&lt;code&gt;git commit -m &#39;Add new feature&#39;&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Push to the branch (&lt;code&gt;git push origin feature-branch&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Open a Pull Request&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/contributing.md&#34;&gt;Contributing Guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;üìú License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache 2.0 License - see the &lt;a href=&#34;https://raw.githubusercontent.com/potpie-ai/potpie/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;üí™ Thanks To All Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Thanks for spending your time helping build Potpie. Keep rocking ü•Ç&lt;/p&gt; &#xA;&lt;img src=&#34;https://contributors-img.web.app/image?repo=potpie-ai/potpie&#34; alt=&#34;Contributors&#34;&gt;</summary>
  </entry>
  <entry>
    <title>browser-use/browser-use</title>
    <updated>2025-02-11T01:28:10Z</updated>
    <id>tag:github.com,2025-02-11:/browser-use/browser-use</id>
    <link href="https://github.com/browser-use/browser-use" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make websites accessible for AI agents&lt;/p&gt;&lt;hr&gt;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;./static/browser-use-dark.png&#34;&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;./static/browser-use.png&#34;&gt; &#xA; &lt;img alt=&#34;Shows a black Browser Use Logo in light color mode and a white one in dark color mode.&#34; src=&#34;https://raw.githubusercontent.com/browser-use/browser-use/main/static/browser-use.png&#34; width=&#34;full&#34;&gt; &#xA;&lt;/picture&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Enable AI to control your browser ü§ñ&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/gregpr07/browser-use/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/gregpr07/browser-use?style=social&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1303749220842340412?color=7289DA&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.browser-use.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-%F0%9F%93%95-blue&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cloud.browser-use.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Cloud-%E2%98%81%EF%B8%8F-blue&#34; alt=&#34;Cloud&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/gregpr07&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Gregor?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/mamagnus00&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Magnus?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://app.workweave.ai/reports/repository/org_T5Pvn3UBswTHIsN1dWS3voPg/881458615&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fapp.workweave.ai%2Fapi%2Frepository%2Fbadge%2Forg_T5Pvn3UBswTHIsN1dWS3voPg%2F881458615&amp;amp;labelColor=#EC6341&#34; alt=&#34;Weave Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;üåê Browser-use is the easiest way to connect your AI agents with the browser.&lt;/p&gt; &#xA;&lt;p&gt;üí° See what others are building and share your projects in our &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;Discord&lt;/a&gt; - we&#39;d love to see what you create!&lt;/p&gt; &#xA;&lt;p&gt;üå©Ô∏è Skip the setup - try our hosted version for instant browser automation! &lt;a href=&#34;https://cloud.browser-use.com&#34;&gt;Try it now&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;p&gt;With pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install browser-use&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;install playwright:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;playwright install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Spin up your agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from langchain_openai import ChatOpenAI&#xA;from browser_use import Agent&#xA;import asyncio&#xA;from dotenv import load_dotenv&#xA;load_dotenv()&#xA;&#xA;async def main():&#xA;    agent = Agent(&#xA;        task=&#34;Go to Reddit, search for &#39;browser-use&#39;, click on the first post and return the first comment.&#34;,&#xA;        llm=ChatOpenAI(model=&#34;gpt-4o&#34;),&#xA;    )&#xA;    result = await agent.run()&#xA;    print(result)&#xA;&#xA;asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add your API keys for the provider you want to use to your &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For other settings, models, and more, check out the &lt;a href=&#34;https://docs.browser-use.com&#34;&gt;documentation üìï&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Test with UI&lt;/h3&gt; &#xA;&lt;p&gt;You can test &lt;a href=&#34;https://github.com/browser-use/web-ui&#34;&gt;browser-use with a UI repository&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Or simply run the gradio example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;uv pip install gradio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python examples/ui/gradio_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Demos&lt;/h1&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/use-cases/shopping.py&#34;&gt;Task&lt;/a&gt;: Add grocery items to cart, and checkout.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=L2Ya9PYNns8&#34;&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/d9359085-bde6-41d4-aa4e-6520d0221872&#34; alt=&#34;AI Did My Groceries&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: Add my latest LinkedIn follower to my leads in Salesforce.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/1440affc-a552-442e-b702-d0d3b277b0ae&#34; alt=&#34;LinkedIn to Salesforce&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/use-cases/find_and_apply_to_jobs.py&#34;&gt;Prompt&lt;/a&gt;: Read my CV &amp;amp; find ML jobs, save them to a file, and then start applying for them in new tabs, if you need help, ask me.&#39;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&#34;&gt;https://github.com/user-attachments/assets/171fb4d6-0355-46f2-863e-edb04a828d04&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/browser/real_browser.py&#34;&gt;Prompt&lt;/a&gt;: Write a letter in Google Docs to my Papa, thanking him for everything, and save the document as a PDF.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/242ade3e-15bc-41c2-988f-cbc5415a66aa&#34; alt=&#34;Letter to Papa&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/browser-use/browser-use/raw/main/examples/custom-functions/save_to_file_hugging_face.py&#34;&gt;Prompt&lt;/a&gt;: Look up models with a license of cc-by-sa-4.0 and sort by most likes on Hugging face, save top 5 to file.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&#34;&gt;https://github.com/user-attachments/assets/de73ee39-432c-4b97-b4e8-939fd7f323b3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;More examples&lt;/h2&gt; &#xA;&lt;p&gt;For more examples see the &lt;a href=&#34;https://raw.githubusercontent.com/browser-use/browser-use/main/examples&#34;&gt;examples&lt;/a&gt; folder or join the &lt;a href=&#34;https://link.browser-use.com/discord&#34;&gt;Discord&lt;/a&gt; and show off your project.&lt;/p&gt; &#xA;&lt;h1&gt;Vision&lt;/h1&gt; &#xA;&lt;p&gt;Tell your computer what to do, and it gets it done.&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve memory management&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enhance planning capabilities&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve self-correction&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fine-tune the model for better performance&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create datasets for complex tasks&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Sandbox browser-use for specific websites&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement deterministic script rerun with LLM fallback&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Cloud-hosted version&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add stop/pause functionality&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve authentication handling&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reduce token consumption&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Implement long-term memory&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Handle repetitive tasks reliably&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Third-party integrations (Slack, etc.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Include more interactive elements&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Human-in-the-loop execution&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Benchmark various models against each other&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Let the user record a workflow and browser-use will execute it&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Improve the generated GIF quality&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create various demos for tutorial execution, job application, QA testing, social media, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We love contributions! Feel free to open issues for bugs or feature requests. To contribute to the docs, check out the &lt;code&gt;/docs&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;Local Setup&lt;/h2&gt; &#xA;&lt;p&gt;To learn more about the library, check out the &lt;a href=&#34;https://docs.browser-use.com/development/local-setup&#34;&gt;local setup üìï&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Cooperations&lt;/h2&gt; &#xA;&lt;p&gt;We are forming a commission to define best practices for UI/UX design for browser agents. Together, we&#39;re exploring how software redesign improves the performance of AI agents and gives these companies a competitive advantage by designing their existing software to be at the forefront of the agent age.&lt;/p&gt; &#xA;&lt;p&gt;Email &lt;a href=&#34;mailto:tbiddle@loop11.com?subject=I%20want%20to%20join%20the%20UI/UX%20commission%20for%20AI%20agents&amp;amp;body=Hi%20Toby%2C%0A%0AI%20found%20you%20in%20the%20browser-use%20GitHub%20README.%0A%0A&#34;&gt;Toby&lt;/a&gt; to apply for a seat on the committee.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Browser Use in your research or project, please cite:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{browser_use2024,&#xA;  author = {M√ºller, Magnus and ≈Ωuniƒç, Gregor},&#xA;  title = {Browser Use: Enable AI to control your browser},&#xA;  year = {2024},&#xA;  publisher = {GitHub},&#xA;  url = {https://github.com/browser-use/browser-use}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/user-attachments/assets/402b2129-b6ac-44d3-a217-01aea3277dce&#34; width=&#34;400&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://x.com/gregpr07&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Gregor?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/mamagnus00&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/Magnus?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Made with ‚ù§Ô∏è in Zurich and San Francisco &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>albertan017/LLM4Decompile</title>
    <updated>2025-02-11T01:28:10Z</updated>
    <id>tag:github.com,2025-02-11:/albertan017/LLM4Decompile</id>
    <link href="https://github.com/albertan017/LLM4Decompile" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Reverse Engineering: Decompiling Binary Code with Large Language Models&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://github.com/albertan017/LLM4Decompile/blob/main/samples/logo-dark.png&#34;&gt; &#xA;  &lt;img alt=&#34;LLM4Decompile&#34; src=&#34;https://github.com/albertan017/LLM4Decompile/raw/main/samples/logo-light.png&#34; width=&#34;55%&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; üìä&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/albertan017/LLM4Decompile/main/#evaluation&#34;&gt;Results&lt;/a&gt; | ü§ó&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/albertan017/LLM4Decompile/main/#models&#34;&gt;Models&lt;/a&gt; | üöÄ&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/albertan017/LLM4Decompile/main/#quick-start&#34;&gt;Quick Start&lt;/a&gt; | üìö&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/albertan017/LLM4Decompile/main/#humaneval-decompile&#34;&gt;HumanEval-Decompile&lt;/a&gt; | üìé&amp;nbsp;&lt;a href=&#34;https://raw.githubusercontent.com/albertan017/LLM4Decompile/main/#citation&#34;&gt;Citation&lt;/a&gt; | üìù&amp;nbsp;&lt;a href=&#34;https://arxiv.org/abs/2403.05286&#34;&gt;Paper&lt;/a&gt; | üñ•Ô∏è&amp;nbsp;&lt;a href=&#34;https://colab.research.google.com/drive/1X5TuUKuNuksGJZz6Cc83KKI0ATBP9q7r?usp=sharing&#34;&gt;Colab&lt;/a&gt; | ‚ñ∂Ô∏è&amp;nbsp;&lt;a href=&#34;https://www.youtube.com/watch?v=x7knF3Z1yLk&#34;&gt;YouTube&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Reverse Engineering: Decompiling Binary Code with Large Language Models&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/8664&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/8664&#34; alt=&#34;GitHub Tread&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024-10-17]: Release &lt;a href=&#34;https://huggingface.co/datasets/LLM4Binary/decompile-ghidra-100k&#34;&gt;decompile-ghidra-100k&lt;/a&gt;, a subset of 100k training samples (25k per optimization level). We provide a &lt;a href=&#34;https://github.com/albertan017/LLM4Decompile/raw/main/train/README.md&#34;&gt;training script&lt;/a&gt; that runs in ~3.5 hours on a single A100 40G GPU. It achieves a 0.26 re-executability rate, with a total cost of under $20 for quick replication of LLM4Decompile.&lt;/li&gt; &#xA; &lt;li&gt;[2024-09-26]: Update a &lt;a href=&#34;https://colab.research.google.com/drive/1X5TuUKuNuksGJZz6Cc83KKI0ATBP9q7r?usp=sharing&#34;&gt;Colab notebook&lt;/a&gt; to demonstrate the usage of the LLM4Decompile model, including examples for the LLM4Decompile-End and LLM4Decompile-Ref models.&lt;/li&gt; &#xA; &lt;li&gt;[2024-09-23]: Release &lt;a href=&#34;https://huggingface.co/LLM4Binary/llm4decompile-9b-v2&#34;&gt;LLM4Decompile-9B-v2&lt;/a&gt;, fine-tuned based on &lt;a href=&#34;https://huggingface.co/01-ai/Yi-Coder-9B&#34;&gt;Yi-Coder-9B&lt;/a&gt;, achieved a re-executability rate of &lt;strong&gt;0.6494&lt;/strong&gt; on the Decompile benchmark.&lt;/li&gt; &#xA; &lt;li&gt;[2024-06-19]: Release &lt;a href=&#34;https://huggingface.co/LLM4Binary/llm4decompile-6.7b-v2&#34;&gt;V2&lt;/a&gt; series (LLM4Decompile-Ref). V2 (1.3B-22B), building upon &lt;strong&gt;Ghidra&lt;/strong&gt;, are trained on 2 billion tokens to &lt;strong&gt;refine&lt;/strong&gt; the decompiled pseudo-code from Ghidra. The 22B-V2 version outperforms the 6.7B-V1.5 by an additional 40.1%. Please check the &lt;a href=&#34;https://github.com/albertan017/LLM4Decompile/tree/main/ghidra&#34;&gt;ghidra folder&lt;/a&gt; for details.&lt;/li&gt; &#xA; &lt;li&gt;[2024-05-13]: Release &lt;a href=&#34;https://huggingface.co/LLM4Binary/llm4decompile-6.7b-v1.5&#34;&gt;V1.5&lt;/a&gt; series (LLM4Decompile-End, directly decompile binary using LLM). V1.5 are trained with a larger dataset (15B tokens) and a maximum token &lt;strong&gt;length of 4,096&lt;/strong&gt;, with remarkable performance (over &lt;strong&gt;100% improvement&lt;/strong&gt;) compared to the previous model.&lt;/li&gt; &#xA; &lt;li&gt;[2024-03-16]: Add &lt;a href=&#34;https://huggingface.co/arise-sustech/llm4decompile-6.7b-uo&#34;&gt;llm4decompile-6.7b-uo&lt;/a&gt; model which is trained without prior knowledge of the optimization levels (O0~O3), the average re-executability is around 0.219, performs the best in our models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM4Decompile&lt;/strong&gt; is the pioneering open-source large language model dedicated to decompilation. Its current version supports decompiling Linux x86_64 binaries, ranging from GCC&#39;s O0 to O3 optimization levels, into human-readable C source code. Our team is committed to expanding this tool&#39;s capabilities, with ongoing efforts to incorporate a broader range of architectures and configurations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM4Decompile-End&lt;/strong&gt; focuses on decompiling the binary directly. &lt;strong&gt;LLM4Decompile-Ref&lt;/strong&gt; refines the pseudo-code decompiled by Ghidra.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;h3&gt;Framework&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/albertan017/LLM4Decompile/raw/main/samples/compile-decompile.png&#34; alt=&#34;image&#34; width=&#34;400&#34; height=&#34;auto&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;During compilation, the Preprocessor processes the source code (SRC) to eliminate comments and expand macros or includes. The cleaned code is then forwarded to the Compiler, which converts it into assembly code (ASM). This ASM is transformed into binary code (0s and 1s) by the Assembler. The Linker finalizes the process by linking function calls to create an executable file. Decompilation, on the other hand, involves converting binary code back into a source file. LLMs, being trained on text, lack the ability to process binary data directly. Therefore, binaries must be disassembled by &lt;code&gt;Objdump&lt;/code&gt; into assembly language (ASM) first. It should be noted that binary and disassembled ASM are equivalent, they can be interconverted, and thus we refer to them interchangeably. Finally, the loss is computed between the decompiled code and source code to guide the training. To assess the quality of the decompiled code (SRC&#39;), it is tested for its functionality through test assertions (re-executability).&lt;/p&gt; &#xA;&lt;h3&gt;Metrics&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Re-executability&lt;/strong&gt; evaluates whether the decompiled code can execute properly and pass all the predefined test cases.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Benchmarks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;HumanEval-Decompile&lt;/strong&gt; A collection of 164 C functions that exclusively rely on &lt;strong&gt;standard&lt;/strong&gt; C libraries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ExeBench&lt;/strong&gt; A collection of 2,621 functions drawn from &lt;strong&gt;real&lt;/strong&gt; projects, each utilizing user-defined functions, structures, and macros.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/albertan017/LLM4Decompile/raw/main/samples/results_end_final.png&#34; alt=&#34;results&#34; width=&#34;800&#34; height=&#34;auto&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/albertan017/LLM4Decompile/raw/main/samples/results_refine_final.png&#34; alt=&#34;image&#34; width=&#34;800&#34; height=&#34;auto&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;Our LLM4Decompile includes models with sizes between 1.3 billion and 33 billion parameters, and we have made these models available on Hugging Face.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoint&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Re-executability&lt;/th&gt; &#xA;   &lt;th&gt;Note&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;llm4decompile-1.3b-v1.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/LLM4Binary/llm4decompile-1.3b-v1.5&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;27.3%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Note 3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;llm4decompile-6.7b-v1.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/LLM4Binary/llm4decompile-6.7b-v1.5&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6.7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;45.4%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Note 3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;llm4decompile-1.3b-v2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/LLM4Binary/llm4decompile-1.3b-v2&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;46.0%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Note 4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;llm4decompile-6.7b-v2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/LLM4Binary/llm4decompile-6.7b-v2&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6.7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;52.7%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Note 4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;llm4decompile-9b-v2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/LLM4Binary/llm4decompile-9b-v2&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;9B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;64.9%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Note 4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;llm4decompile-22b-v2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;ü§ó &lt;a href=&#34;https://huggingface.co/LLM4Binary/llm4decompile-22b-v2&#34;&gt;HF Link&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;22B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;63.6%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Note 4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Note 3: V1.5 series are trained with a larger dataset (15B tokens) and a maximum token size of 4,096, with remarkable performance (over 100% improvement) compared to the previous model.&lt;/p&gt; &#xA;&lt;p&gt;Note 4: V2 series are built upon &lt;strong&gt;Ghidra&lt;/strong&gt; and trained on 2 billion tokens to &lt;strong&gt;refine&lt;/strong&gt; the decompiled pseudo-code from Ghidra. Check &lt;a href=&#34;https://github.com/albertan017/LLM4Decompile/tree/main/ghidra&#34;&gt;ghidra folder&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1X5TuUKuNuksGJZz6Cc83KKI0ATBP9q7r?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt; Please use the script below to install the necessary environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/albertan017/LLM4Decompile.git&#xA;cd LLM4Decompile&#xA;conda create -n &#39;llm4decompile&#39; python=3.9 -y&#xA;conda activate llm4decompile&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here is an example of how to use our model (Revised for V1.5. For previous models, please check the corresponding model page at HF). Note: &lt;strong&gt;Replace the &#34;func0&#34; with the function name you want to decompile&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Preprocessing:&lt;/strong&gt; Compile the C code into binary, and disassemble the binary into assembly instructions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import subprocess&#xA;import os&#xA;func_name = &#39;func0&#39;&#xA;OPT = [&#34;O0&#34;, &#34;O1&#34;, &#34;O2&#34;, &#34;O3&#34;]&#xA;fileName = &#39;samples/sample&#39; #&#39;path/to/file&#39;&#xA;for opt_state in OPT:&#xA;    output_file = fileName +&#39;_&#39; + opt_state&#xA;    input_file = fileName+&#39;.c&#39;&#xA;    compile_command = f&#39;gcc -o {output_file}.o {input_file} -{opt_state} -lm&#39;#compile the code with GCC on Linux&#xA;    subprocess.run(compile_command, shell=True, check=True)&#xA;    compile_command = f&#39;objdump -d {output_file}.o &amp;gt; {output_file}.s&#39;#disassemble the binary file into assembly instructions&#xA;    subprocess.run(compile_command, shell=True, check=True)&#xA;    &#xA;    input_asm = &#39;&#39;&#xA;    with open(output_file+&#39;.s&#39;) as f:#asm file&#xA;        asm= f.read()&#xA;        if &#39;&amp;lt;&#39;+func_name+&#39;&amp;gt;:&#39; not in asm: #IMPORTANT replace func0 with the function name&#xA;            raise ValueError(&#34;compile fails&#34;)&#xA;        asm = &#39;&amp;lt;&#39;+func_name+&#39;&amp;gt;:&#39; + asm.split(&#39;&amp;lt;&#39;+func_name+&#39;&amp;gt;:&#39;)[-1].split(&#39;\n\n&#39;)[0] #IMPORTANT replace func0 with the function name&#xA;        asm_clean = &#34;&#34;&#xA;        asm_sp = asm.split(&#34;\n&#34;)&#xA;        for tmp in asm_sp:&#xA;            if len(tmp.split(&#34;\t&#34;))&amp;lt;3 and &#39;00&#39; in tmp:&#xA;                continue&#xA;            idx = min(&#xA;                len(tmp.split(&#34;\t&#34;)) - 1, 2&#xA;            )&#xA;            tmp_asm = &#34;\t&#34;.join(tmp.split(&#34;\t&#34;)[idx:])  # remove the binary code&#xA;            tmp_asm = tmp_asm.split(&#34;#&#34;)[0].strip()  # remove the comments&#xA;            asm_clean += tmp_asm + &#34;\n&#34;&#xA;    input_asm = asm_clean.strip()&#xA;    before = f&#34;# This is the assembly code:\n&#34;#prompt&#xA;    after = &#34;\n# What is the source code?\n&#34;#prompt&#xA;    input_asm_prompt = before+input_asm.strip()+after&#xA;    with open(fileName +&#39;_&#39; + opt_state +&#39;.asm&#39;,&#39;w&#39;,encoding=&#39;utf-8&#39;) as f:&#xA;        f.write(input_asm_prompt)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Assembly instructions should be in the format:&lt;/p&gt; &#xA;&lt;p&gt;&amp;lt;FUNCTION_NAME&amp;gt;:\nOPERATIONS\nOPERATIONS\n&lt;/p&gt; &#xA;&lt;p&gt;Typical assembly instructions may look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;func0&amp;gt;:&#xA;endbr64&#xA;lea    (%rdi,%rsi,1),%eax&#xA;retq&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Decompilation:&lt;/strong&gt; Use LLM4Decompile to translate the assembly instructions into C:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;import torch&#xA;&#xA;model_path = &#39;LLM4Binary/llm4decompile-6.7b-v1.5&#39; # V1.5 Model&#xA;tokenizer = AutoTokenizer.from_pretrained(model_path)&#xA;model = AutoModelForCausalLM.from_pretrained(model_path,torch_dtype=torch.bfloat16).cuda()&#xA;&#xA;with open(fileName +&#39;_&#39; + OPT[0] +&#39;.asm&#39;,&#39;r&#39;) as f:#optimization level O0&#xA;    asm_func = f.read()&#xA;inputs = tokenizer(asm_func, return_tensors=&#34;pt&#34;).to(model.device)&#xA;with torch.no_grad():&#xA;    outputs = model.generate(**inputs, max_new_tokens=2048)### max length to 4096, max new tokens should be below the range&#xA;c_func_decompile = tokenizer.decode(outputs[0][len(inputs[0]):-1])&#xA;&#xA;with open(fileName +&#39;.c&#39;,&#39;r&#39;) as f:#original file&#xA;    func = f.read()&#xA;&#xA;print(f&#39;original function:\n{func}&#39;)# Note we only decompile one function, where the original file may contain multiple functions&#xA;print(f&#39;decompiled function:\n{c_func_decompile}&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;HumanEval-Decompile&lt;/h2&gt; &#xA;&lt;p&gt;Data are stored in &lt;code&gt;llm4decompile/decompile-eval/decompile-eval-executable-gcc-obj.json&lt;/code&gt;, using JSON list format. There are 164*4 (O0, O1, O2, O3) samples, each with five keys:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;task_id&lt;/code&gt;: indicates the ID of the problem.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;type&lt;/code&gt;: the optimization stage, is one of [O0, O1, O2, O3].&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;c_func&lt;/code&gt;: C solution for HumanEval problem.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;c_test&lt;/code&gt;: C test assertions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;input_asm_prompt&lt;/code&gt;: assembly instructions with prompts, can be derived as in our &lt;a href=&#34;https://github.com/albertan017/LLM4Decompile?tab=readme-ov-file#quick-start&#34;&gt;preprocessing example&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please check the &lt;a href=&#34;https://github.com/albertan017/LLM4Decompile/tree/main/evaluation&#34;&gt;evaluation scripts&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;On Going&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Larger training dataset with the cleaning process. (done:2024.05.13)&lt;/li&gt; &#xA; &lt;li&gt;Support for popular languages/platforms and settings.&lt;/li&gt; &#xA; &lt;li&gt;Support for executable binaries. (done:2024.05.13)&lt;/li&gt; &#xA; &lt;li&gt;Integration with decompilation tools (e.g., Ghidra, Rizin)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This code repository is licensed under the MIT and DeepSeek License.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{tan2024llm4decompile,&#xA;      title={LLM4Decompile: Decompiling Binary Code with Large Language Models}, &#xA;      author={Hanzhuo Tan and Qi Luo and Jing Li and Yuqun Zhang},&#xA;      year={2024},&#xA;      eprint={2403.05286},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.PL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#albertan017/LLM4Decompile&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=albertan017/LLM4Decompile&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>