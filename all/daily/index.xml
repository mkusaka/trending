<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-12T01:24:11Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>miurla/morphic</title>
    <updated>2024-04-12T01:24:11Z</updated>
    <id>tag:github.com,2024-04-12:/miurla/morphic</id>
    <link href="https://github.com/miurla/morphic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An AI-powered answer engine with a generative UI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Morphic&lt;/h1&gt; &#xA;&lt;p&gt;An AI-powered answer engine with a generative UI.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/miurla/morphic/main/public/capture-240404_blk.png&#34; alt=&#34;capture&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üîç Overview&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üß± &lt;a href=&#34;https://raw.githubusercontent.com/miurla/morphic/main/#-stack&#34;&gt;Stack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üöÄ &lt;a href=&#34;https://raw.githubusercontent.com/miurla/morphic/main/#-quickstart&#34;&gt;Quickstart&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üåê &lt;a href=&#34;https://raw.githubusercontent.com/miurla/morphic/main/#-deploy&#34;&gt;Deploy&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üß± Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;App framework: &lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Text streaming / Generative UI: &lt;a href=&#34;https://sdk.vercel.ai/docs&#34;&gt;Vercel AI SDK&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Generative Model: &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Search API: &lt;a href=&#34;https://tavily.com/&#34;&gt;Tavily AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Component library: &lt;a href=&#34;https://ui.shadcn.com/&#34;&gt;shadcn/ui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Headless component primitives: &lt;a href=&#34;https://www.radix-ui.com/&#34;&gt;Radix UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Styling: &lt;a href=&#34;https://tailwindcss.com/&#34;&gt;Tailwind CSS&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;1. Fork and Clone repo&lt;/h3&gt; &#xA;&lt;p&gt;Fork the repo to your Github account, then run the following command to clone the repo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone git@github.com:[YOUR_GITHUB_ACCOUNT]/morphic.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Install dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd morphic&#xA;bun i&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Fill out secrets&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;cp .env.local.example .env.local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Your .env.local file should look like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Used to set the base URL path for OpenAI API requests.&#xA;# If you need to set a BASE URL, uncomment and set the following:&#xA;# OPENAI_API_BASE=&#xA;&#xA;# OpenAI API key retrieved here: https://platform.openai.com/api-keys&#xA;OPENAI_API_KEY=[YOUR_OPENAI_API_KEY]&#xA;&#xA;# Tavily API Key retrieved here: https://app.tavily.com/home&#xA;TAVILY_API_KEY=[YOUR_TAVILY_API_KEY]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Run app locally&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;bun dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now visit &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üåê Deploy&lt;/h2&gt; &#xA;&lt;p&gt;Host your own live version of Morphic with Vercel.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fmiurla%2Fmorphic&amp;amp;env=OPENAI_API_KEY,TAVILY_API_KEY&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nus-apr/auto-code-rover</title>
    <updated>2024-04-12T01:24:11Z</updated>
    <id>tag:github.com,2024-04-12:/nus-apr/auto-code-rover</id>
    <link href="https://github.com/nus-apr/auto-code-rover" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A project structure aware autonomous software engineer aiming for autonomous program improvement&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AutoCodeRover: Autonomous Program Improvement&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/nus-apr/auto-code-rover/assets/48704330/0b8da9ad-588c-4f7d-9c99-53f33d723d35&#34; alt=&#34;overall-workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.05427&#34;&gt;&lt;strong&gt;ArXiv Paper&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üëã Overview&lt;/h2&gt; &#xA;&lt;p&gt;AutoCodeRover is a fully automated approach for resolving GitHub issues (bug fixing and feature addition) where LLMs are combined with analysis and debugging capabilities to prioritize patch locations ultimately leading to a patch.&lt;/p&gt; &#xA;&lt;p&gt;On &lt;a href=&#34;https://www.swebench.com/lite.html&#34;&gt;SWE-bench lite&lt;/a&gt;, which consists of 300 real-world GitHub issues, AutoCodeRover resolves ~&lt;strong&gt;22%&lt;/strong&gt; of issues, improving over the current state-of-the-art efficacy of AI software engineers.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/nus-apr/auto-code-rover/assets/48704330/28e26111-5f15-4ee4-acd1-fa6e2e6e0593&#34; width=&#34;330/&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;AutoCodeRover works in two stages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîé Context retrieval: The LLM is provided with code search APIs to navigate the codebase and collect relevant context.&lt;/li&gt; &#xA; &lt;li&gt;üíä Patch generation: The LLM tries to write a patch, based on retrieved context.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;‚ú® Highlights&lt;/h3&gt; &#xA;&lt;p&gt;AutoCodeRover has two unique features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Code search APIs are &lt;em&gt;Program Structure Aware&lt;/em&gt;. Instead of searching over files by plain string matching, AutoCodeRover searches for relevant code context (methods/classes) in the abstract syntax tree.&lt;/li&gt; &#xA; &lt;li&gt;When a test suite is available, AutoCodeRover can take advantage of test cases to achieve an even higher repair rate, by performing &lt;em&gt;statistical fault localization&lt;/em&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üóé arXiv Paper&lt;/h2&gt; &#xA;&lt;h3&gt;AutoCodeRover: Autonomous Program Improvement &lt;a href=&#34;https://arxiv.org/abs/2404.05427&#34;&gt;[arXiv 2404.05427]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.05427&#34;&gt; &lt;img src=&#34;https://github.com/nus-apr/auto-code-rover/assets/48704330/3d42a873-dd9f-41f3-ae09-eba477db2420&#34; alt=&#34;First page of arXiv paper&#34; width=&#34;570&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;For referring to our work, please cite and mention:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zhang2024autocoderover,&#xA;      title={AutoCodeRover: Autonomous Program Improvement},&#xA;      author={Yuntong Zhang and Haifeng Ruan and Zhiyu Fan and Abhik Roychoudhury},&#xA;      year={2024},&#xA;      eprint={2404.05427},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.SE}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚úîÔ∏è Example: Django Issue #32347&lt;/h2&gt; &#xA;&lt;p&gt;As an example, AutoCodeRover successfully fixed issue &lt;a href=&#34;https://code.djangoproject.com/ticket/32347&#34;&gt;#32347&lt;/a&gt; of Django. See the demo video for the full process:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nus-apr/auto-code-rover/assets/48704330/719c7a56-40b8-4f3d-a90e-0069e37baad3&#34;&gt;https://github.com/nus-apr/auto-code-rover/assets/48704330/719c7a56-40b8-4f3d-a90e-0069e37baad3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Enhancement: leveraging test cases&lt;/h3&gt; &#xA;&lt;p&gt;AutoCodeRover can resolve even more issues, if test cases are available. See an example in the video:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nus-apr/auto-code-rover/assets/48704330/26c9d5d4-04e0-4b98-be55-61c1d10a36e5&#34;&gt;https://github.com/nus-apr/auto-code-rover/assets/48704330/26c9d5d4-04e0-4b98-be55-61c1d10a36e5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Setup &amp;amp; Running&lt;/h2&gt; &#xA;&lt;p&gt;We recommend running AutoCodeRover in a Docker container. First of all, build and start the docker image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -f Dockerfile -t acr .&#xA;docker run -it acr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the docker container, set the &lt;code&gt;OPENAI_KEY&lt;/code&gt; env var to your &lt;a href=&#34;https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key&#34;&gt;OpenAI key&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_KEY=xx-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Set up one or more tasks in SWE-bench&lt;/h3&gt; &#xA;&lt;p&gt;In the docker container, we need to first set up the tasks to run in SWE-bench (e.g., &lt;code&gt;django__django-11133&lt;/code&gt;). The list of all tasks can be found in &lt;a href=&#34;https://raw.githubusercontent.com/nus-apr/auto-code-rover/main/conf/swe_lite_tasks.txt&#34;&gt;&lt;code&gt;conf/swe_lite_tasks.txt&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The tasks need to be put in a file, one per line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd /opt/SWE-bench&#xA;echo django__django-11133 &amp;gt; tasks.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, set up these tasks by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd /opt/SWE-bench&#xA;conda activate swe-bench&#xA;python harness/run_setup.py --log_dir logs --testbed testbed --result_dir setup_result --subset_file tasks.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the setup for this task is completed, the following two lines will be printed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;setup_map is saved to setup_result/setup_map.json&#xA;tasks_map is saved to setup_result/tasks_map.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;testbed&lt;/code&gt; directory will now contain the cloned source code of the target project. A conda environment will also be created for this task instance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;If you want to set up multiple tasks together, put their ids in &lt;code&gt;tasks.txt&lt;/code&gt; and follow the same steps.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Run a single task&lt;/h3&gt; &#xA;&lt;p&gt;Before running the task (&lt;code&gt;django__django-11133&lt;/code&gt; here), make sure it has been set up as mentioned &lt;a href=&#34;https://raw.githubusercontent.com/nus-apr/auto-code-rover/main/#set-up-one-or-more-tasks-in-swe-bench&#34;&gt;above&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd /opt/auto-code-rover&#xA;conda activate auto-code-rover&#xA;PYTHONPATH=. python app/main.py --enable-layered --model gpt-4-0125-preview --setup-map ../SWE-bench/setup_result/setup_map.json --tasks-map ../SWE-bench/setup_result/tasks_map.json --output-dir output --task django__django-11133&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The output of the run can then be found in &lt;code&gt;output/&lt;/code&gt;. For example, the patch generated for &lt;code&gt;django__django-11133&lt;/code&gt; can be found at a location like this: &lt;code&gt;output/applicable_patch/django__django-11133_yyyy-MM-dd_HH-mm-ss/extracted_patch_1.diff&lt;/code&gt; (the date-time field in the directory name will be different depending on when the experiment was run).&lt;/p&gt; &#xA;&lt;h3&gt;Run multiple tasks&lt;/h3&gt; &#xA;&lt;p&gt;First, put the id&#39;s of all tasks to run in a file, one per line. Suppose this file is &lt;code&gt;tasks.txt&lt;/code&gt;, the tasks can be run with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;PYTHONPATH=. python app/main.py --enable-layered --model gpt-4-0125-preview --setup-map ../SWE-bench/setup_result/setup_map.json --tasks-map ../SWE-bench/setup_result/tasks_map.json --output-dir output --task-list-file tasks.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: make sure that the tasks in &lt;code&gt;tasks.txt&lt;/code&gt; have all been set up in SWE-bench. See the steps &lt;a href=&#34;https://raw.githubusercontent.com/nus-apr/auto-code-rover/main/#set-up-one-or-more-tasks-in-swe-bench&#34;&gt;above&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Using a config file&lt;/h4&gt; &#xA;&lt;p&gt;Alternatively, a config file can be used to specify all parameters and tasks to run. See &lt;code&gt;conf/vanilla-lite.conf&lt;/code&gt; for an example. Also see &lt;a href=&#34;https://raw.githubusercontent.com/nus-apr/auto-code-rover/main/EXPERIMENT.md&#34;&gt;EXPERIMENT.md&lt;/a&gt; for the details of the items in a conf file. A config file can be used by:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/run.py conf/vanilla-lite.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Experiment Replication&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://raw.githubusercontent.com/nus-apr/auto-code-rover/main/EXPERIMENT.md&#34;&gt;EXPERIMENT.md&lt;/a&gt; for information on experiment replication.&lt;/p&gt; &#xA;&lt;h2&gt;‚úâÔ∏è Contacts&lt;/h2&gt; &#xA;&lt;p&gt;For any queries, you are welcome to open an issue.&lt;/p&gt; &#xA;&lt;p&gt;Alternatively, contact us at: {yuntong,hruan,zhiyufan}@comp.nus.edu.sg.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This work was partially supported by a Singapore Ministry of Education (MoE) Tier 3 grant &#34;Automated Program Repair&#34;, MOE-MOET32021-0001.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>karpathy/llm.c</title>
    <updated>2024-04-12T01:24:11Z</updated>
    <id>tag:github.com,2024-04-12:/karpathy/llm.c</id>
    <link href="https://github.com/karpathy/llm.c" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM training in simple, raw C/CUDA&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llm.c&lt;/h1&gt; &#xA;&lt;p&gt;LLM training in simple, pure C/CUDA. There is no need for 245MB of PyTorch or 107MB of cPython. For example, training GPT-2 (CPU, fp32) is ~1,000 lines of clean code in a single file. It compiles and runs instantly, and exactly matches the PyTorch reference implementation. I chose GPT-2 as the first working example because it is the grand-daddy of LLMs, the first time the modern stack was put together.&lt;/p&gt; &#xA;&lt;p&gt;Currently, I am working on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;direct CUDA implementation, which will be significantly faster and probably come close to PyTorch.&lt;/li&gt; &#xA; &lt;li&gt;speed up the CPU version with SIMD instructions, AVX2 on x86 / NEON on ARM (e.g. Apple Silicon).&lt;/li&gt; &#xA; &lt;li&gt;more modern architectures, e.g. Llama2, Gemma, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For the repo, I&#39;d like to maintain both clean, simple reference implementations alongside a also lot more optimized versions that can come close to PyTorch, but in a tiny fraction of the code and dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;quick start&lt;/h2&gt; &#xA;&lt;p&gt;Download and tokenize a dataset. The &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#34;&gt;tinyshakespeare&lt;/a&gt; dataset is the fastest to download and tokenize:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python prepro_tinyshakespeare.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This prints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Saved 32768 tokens to data/tiny_shakespeare_val.bin&#xA;Saved 305260 tokens to data/tiny_shakespeare_train.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The .bin files are raw byte streams of int32 numbers indicating the token ids with the GPT-2 tokenizer. Alternatively you could also tokenize the &lt;a href=&#34;https://huggingface.co/datasets/roneneldan/TinyStories&#34;&gt;TinyStories&lt;/a&gt; dataset with &lt;code&gt;prepro_tinystories.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;In principle we&#39;d be ready to train the model right here. However the baseline CPU/fp32 reference code is so inefficient that it&#39;s not practical to train these models from scratch yet. Instead, we initialize with the GPT-2 weights released by OpenAI and just do finetuning. For that, we have to download the GPT-2 weights and save them as a checkpoint we can load in C:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_gpt2.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You&#39;ll recognize this code from nanoGPT as a simple GPT-2 reference implementation in PyTorch. This script will download the GPT-2 (124M) model, overfit a single batch of data for 10 iterations, run a few steps of generation, and most importantly it will save two files: 1) the &lt;code&gt;gpt2_124M.bin&lt;/code&gt; file that contains the raw model weights for loading in C, and &lt;code&gt;gpt2_124M_debug_state.bin&lt;/code&gt;, which also contains more debug state: the inputs, targets, logits and loss. This is very useful for debugging C code, for unit testing, and making sure we&#39;re exactly matching the PyTorch reference implementation. For now all we care about are the model weights in &lt;code&gt;gpt2_124M.bin&lt;/code&gt;. We can now initialize with them and train in raw C. First compile the code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make train_gpt2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can have a look inside the &lt;code&gt;Makefile&lt;/code&gt; and its comments. It will try to autodetect if OpenMP is available on your system, which is very helpful for speeding up the code at very low cost of code complexity. Some people seem to experience problems compiling on Ubuntu, have a look at &lt;a href=&#34;https://github.com/karpathy/llm.c/issues/19&#34;&gt;Issue 19&lt;/a&gt;, TLDR you&#39;d want to modify the &lt;code&gt;CFLAGS&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# try this first&#xA;CFLAGS = -Ofast -fno-fast-math -Wno-unused-result&#xA;# try this second&#xA;CFLAGS = -O3 -Wno-unused-result&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once &lt;code&gt;train_gpt2&lt;/code&gt; is compiled, you can run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OMP_NUM_THREADS=8 ./train_gpt2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should tune the number of threads depending on how many cores your CPU has. The program will load the model weights, the tokens, it will run a finetuning loop for a few iterations with Adam lr 1e-4, and then generate a sample from the model. The file is (I think) very readable and you should have a look. Simply, there are implementations for the forward and backward pass of all the layers, and they get strung together into a large, manual, forward/backward/update loop. The output looks like this on my MacBook Pro (Apple Silicon M3 Max):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[GPT-2]&#xA;max_seq_len: 1024&#xA;vocab_size: 50257&#xA;num_layers: 12&#xA;num_heads: 12&#xA;channels: 768&#xA;num_parameters: 124439808&#xA;train dataset num_batches: 1192&#xA;val dataset num_batches: 128&#xA;num_activations: 73323776&#xA;val loss 5.252026&#xA;step 0: train loss 5.356189 (took 1452.121000 ms)&#xA;step 1: train loss 4.301069 (took 1288.673000 ms)&#xA;step 2: train loss 4.623322 (took 1369.394000 ms)&#xA;step 3: train loss 4.600470 (took 1290.761000 ms)&#xA;... (trunctated) ...&#xA;step 39: train loss 3.970751 (took 1323.779000 ms)&#xA;val loss 4.107781&#xA;generated: 50256 16773 18162 21986 11 198 13681 263 23875 198 3152 262 11773 2910 198 1169 6002 6386 2583 286 262 11858 198 20424 428 3135 7596 995 3675 13 198 40 481 407 736 17903 11 329 703 6029 706 4082 198 42826 1028 1128 633 263 11 198 10594 407 198 2704 454 680 1028 262 1027 28860 286 198 3237 323&#xA;step 40: train loss 4.377757 (took 1366.368000 ms)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The generation just gives you the token ids for now, which we have to decode back to text. We can implement this in C quite easily also, because decoding is very straight-forward, it&#39;s just string chunk lookups and prints. For now we can use tiktoken:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tiktoken&#xA;enc = tiktoken.get_encoding(&#34;gpt2&#34;)&#xA;ptok = lambda x: print(enc.decode(list(map(int, x.strip().split()))))&#xA;ptok(&#34;50256 16773 18162 21986 11 198 13681 263 23875 198 3152 262 11773 2910 198 1169 6002 6386 2583 286 262 11858 198 20424 428 3135 7596 995 3675 13 198 40 481 407 736 17903 11 329 703 6029 706 4082 198 42826 1028 1128 633 263 11 198 10594 407 198 2704 454 680 1028 262 1027 28860 286 198 3237 323&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which prints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;|endoftext|&amp;gt;Come Running Away,&#xA;Greater conquer&#xA;With the Imperial blood&#xA;the heaviest host of the gods&#xA;into this wondrous world beyond.&#xA;I will not back thee, for how sweet after birth&#xA;Netflix against repounder,&#xA;will not&#xA;flourish against the earlocks of&#xA;Allay&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;I like how Netflix comes up, it&#39;s clear that the shadow of the training past is still lurking in the model. I did not attempt to tune the finetuning hyperparameters so it&#39;s quite likely this can be improved quite a bit, most likely especially if one was to train a bit longer.&lt;/p&gt; &#xA;&lt;h2&gt;test&lt;/h2&gt; &#xA;&lt;p&gt;I am also attaching a simple unit test for making sure our C code agrees with the PyTorch code. Compile and run with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make test_gpt2&#xA;./test_gpt2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This now loads the &lt;code&gt;gpt2_124M_debug_state.bin&lt;/code&gt; file, runs a forward pass, compares the logits and loss with the PyTorch reference implementation, then it does 10 iterations of training with Adam and makes sure the losses match PyTorch.&lt;/p&gt; &#xA;&lt;h2&gt;tutorial&lt;/h2&gt; &#xA;&lt;p&gt;I attached a very small tutorial here, in &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/llm.c/master/doc/layernorm/layernorm.md&#34;&gt;doc/layernorm/layernorm.md&lt;/a&gt;. It&#39;s a simple, step-by-step guide to implementing a single layer of the GPT-2 model, the layernorm layer. This is a good starting point to understand how the layers are implemented in C.&lt;/p&gt; &#xA;&lt;h2&gt;cuda&lt;/h2&gt; &#xA;&lt;p&gt;CUDA port is WIP, I&#39;m keeping the growing collection of kernels in the &lt;code&gt;dev&lt;/code&gt; folder, e.g. see &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/llm.c/master/dev/cuda/README.md&#34;&gt;dev/cuda/README.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As of April 10, 2024 the full forward pass is now implemented in pure CUDA in one file. First we can check that all of the logits and the final loss matches the PyTorch reference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make test_gpt2cu&#xA;./test_gpt2cu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This prints &lt;code&gt;overall okay: 1&lt;/code&gt;. Now that we are calculating all the right values, we can time our code. We can&#39;t train yet because the backward pass + update are not implemented yet, but we can run the training loop and see the timings:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make train_gpt2cu&#xA;./train_gpt2cu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run GPT-2 (124M) in one file of pure CUDA (see &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/llm.c/master/train_gpt2.cu&#34;&gt;train_gpt2.cu&lt;/a&gt;), using batch size 4 and sequence length 1024. This will print a bunch of hyperparameters and then the &#34;training&#34;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;val loss 4.517294&#xA;step 0: train loss 4.367857 (took 112.135004 ms)&#xA;step 1: train loss 4.406483 (took 112.555327 ms)&#xA;step 2: train loss 4.484838 (took 111.380248 ms)&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The loss is changing because we are still loading real data batches from our dataset, but there is no training so they won&#39;t go down over time. In any case, on my A100 40GB PCIe GPU we are seeing about 111ms/iteration. We can compare this to PyTorch fp32 training by calling our python script like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_gpt2.py --inference_only 1 --write_tensors 0 --sequence_length 1024 --batch_size 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Which shows time per iteration with the same hyperparameters (batch 4, time 1024) at 180ms/iteration. We can then enable &lt;code&gt;torch.compile&lt;/code&gt; by adding the &lt;code&gt;--compile 1&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_gpt2.py --inference_only 1 --write_tensors 0 --sequence_length 1024 --batch_size 4 --compile 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And see that the first iteration now takes 20 seconds (compilation time), but all following iterations take ~86ms. And if we additionally turn on the use of fp32 tensorcores (only GPUs since Volta) with &lt;code&gt;--tensorcores 1&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_gpt2.py --inference_only 1 --write_tensors 0 --sequence_length 1024 --batch_size 4 --compile 1 --tensorcores 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The time drops down to 26ms/iteration. So we have a gap to close :)! At the current 111ms we are about 4.2X slower.&lt;/p&gt; &#xA;&lt;h2&gt;discussions&lt;/h2&gt; &#xA;&lt;p&gt;Ways of organizing development:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Experiencing a concrete issue with the repo? Use &lt;a href=&#34;https://github.com/karpathy/llm.c/issues&#34;&gt;Issues&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Have some code to contribute? Open a &lt;a href=&#34;https://github.com/karpathy/llm.c/pulls&#34;&gt;PR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat about the repo, ask questions, etc.? Look at &lt;a href=&#34;https://github.com/karpathy/llm.c/discussions&#34;&gt;Discussions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Something faster? I created a new &lt;code&gt;#llmc&lt;/code&gt; channel on my &lt;a href=&#34;https://discord.gg/3zy8kqD9Cp&#34;&gt;Zero to Hero Discord channel&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;license&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
</feed>