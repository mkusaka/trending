<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-28T01:33:35Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xitanggg/open-resume</title>
    <updated>2023-06-28T01:33:35Z</updated>
    <id>tag:github.com,2023-06-28:/xitanggg/open-resume</id>
    <link href="https://github.com/xitanggg/open-resume" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OpenResume is a powerful open-source resume builder and resume parser. https://open-resume.com/&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenResume&lt;/h1&gt; &#xA;&lt;p&gt;OpenResume is a powerful open-source resume builder and resume parser.&lt;/p&gt; &#xA;&lt;p&gt;The goal of OpenResume is to provide everyone with free access to a modern professional resume design and enable anyone to apply for jobs with confidence.&lt;/p&gt; &#xA;&lt;p&gt;Official site: &lt;a href=&#34;https://open-resume.com&#34;&gt;https://open-resume.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚öíÔ∏è Resume Builder&lt;/h2&gt; &#xA;&lt;p&gt;OpenResume&#39;s resume builder allows user to create a modern professional resume easily.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/jzcrrt8/resume-builder-demo-optimize.gif&#34; alt=&#34;Resume Builder Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;It has 5 Core Features:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&#xA;    &lt;div style=&#34;width:285px&#34;&gt;&#xA;     &lt;strong&gt;Feature&lt;/strong&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;1. Real Time UI Update&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The resume PDF is updated in real time as you enter your resume information, so you can easily see the final output.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2. Modern Professional Resume Design&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The resume PDF is a modern professional design that adheres to U.S. best practices and is ATS friendly to top ATS platforms such as Greenhouse and Lever. It automatically formats fonts, sizes, margins, bullet points to ensure consistency and avoid human errors.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3. Privacy Focus&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The app only runs locally on your browser, meaning no sign up is required and no data ever leaves your browser, so it gives you peace of mind on your personal data. (Fun fact: Running only locally means the app still works even if you disconnect the internet.)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;4. Import From Existing Resume PDF&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;If you already have an existing resume PDF, you have the option to import it directly, so you can update your resume design to a modern professional design in literally a few seconds.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5. Successful Track Record&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;OpenResume users have landed interviews and offers from top companies, such as Dropbox, Google, Meta to name a few. It has been proven to work and liken by recruiters and hiring managers.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üîç Resume Parser&lt;/h2&gt; &#xA;&lt;p&gt;OpenResume‚Äôs second component is the resume parser. For those who have an existing resume, the resume parser can help test and confirm its ATS readability.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.ibb.co/JvSVwNk/resume-parser-demo-optimize.gif&#34; alt=&#34;Resume Parser Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can learn more about the resume parser algorithm in the &lt;a href=&#34;https://open-resume.com/resume-parser&#34;&gt;&#34;Resume Parser Algorithm Deep Dive&#34; section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üìö Tech Stack&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&#xA;    &lt;div style=&#34;width:140px&#34;&gt;&#xA;     &lt;strong&gt;Category&lt;/strong&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&#xA;    &lt;div style=&#34;width:100px&#34;&gt;&#xA;     &lt;strong&gt;Choice&lt;/strong&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Descriptions&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Language&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/microsoft/TypeScript&#34;&gt;TypeScript&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;TypeScript is JavaScript with static type checking and helps catch many silly bugs at code time.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;UI Library&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/facebook/react&#34;&gt;React&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;React‚Äôs declarative syntax and component-based architecture make it simple to develop reactive reusable components.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;State Management&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/reduxjs/redux-toolkit&#34;&gt;Redux Toolkit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Redux toolkit reduces the boilerplate to set up and update a central redux store, which is used in managing the complex resume state.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CSS Framework&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/tailwindlabs/tailwindcss&#34;&gt;Tailwind CSS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Tailwind speeds up development by providing helpful css utilities and removing the need to context switch between tsx and css files.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Web Framework&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/vercel/next.js&#34;&gt;NextJS 13&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Next.js supports static site generation and helps build efficient React webpages that support SEO.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;PDF Reader&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mozilla/pdf.js&#34;&gt;PDF.js&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PDF.js reads content from PDF files and is used by the resume parser at its first step to read a resume PDF‚Äôs content.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;PDF Renderer&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/diegomura/react-pdf&#34;&gt;React-pdf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;React-pdf creates PDF files and is used by the resume builder to create a downloadable PDF file.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üìÅ Project Structure&lt;/h2&gt; &#xA;&lt;p&gt;OpenResume is created with the NextJS web framework and follows its project structure. The source code can be found in &lt;code&gt;src/app&lt;/code&gt;. There are a total of 4 page routes as shown in the table below. (Code path is relative to &lt;code&gt;src/app&lt;/code&gt;)&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&#xA;    &lt;div style=&#34;width:115px&#34;&gt;&#xA;     &lt;strong&gt;Page Route&lt;/strong&gt;&#xA;    &lt;/div&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Code Path&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;/&lt;/td&gt; &#xA;   &lt;td&gt;/page.tsx&lt;/td&gt; &#xA;   &lt;td&gt;Home page that contains hero, auto typing resume, steps, testimonials, logo cloud, etc&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;/resume-import&lt;/td&gt; &#xA;   &lt;td&gt;/resume-import/page.tsx&lt;/td&gt; &#xA;   &lt;td&gt;Resume import page, where you can choose to import data from an existing resume PDF. The main component used is &lt;code&gt;ResumeDropzone&lt;/code&gt; (&lt;code&gt;/components/ResumeDropzone.tsx&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;/resume-builder&lt;/td&gt; &#xA;   &lt;td&gt;/resume-builder/page.tsx&lt;/td&gt; &#xA;   &lt;td&gt;Resume builder page to build and download a resume PDF. The main components used are &lt;code&gt;ResumeForm&lt;/code&gt; (&lt;code&gt;/components/ResumeForm&lt;/code&gt;) and &lt;code&gt;Resume&lt;/code&gt; (&lt;code&gt;/components/Resume&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;/resume-parser&lt;/td&gt; &#xA;   &lt;td&gt;/resume-parser/page.tsx&lt;/td&gt; &#xA;   &lt;td&gt;Resume parser page to test a resume‚Äôs AST readability. The main library util used is &lt;code&gt;parseResumeFromPdf&lt;/code&gt; (&lt;code&gt;/lib/parse-resume-from-pdf&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üíª Local Development&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the repo &lt;code&gt;git clone https://github.com/xitanggg/open-resume.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Change the directory &lt;code&gt;cd open-resume&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install the dependency &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start a development server &lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open your browser and visit &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; to see OpenResume live&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>sb-ocr/diy-spacemouse</title>
    <updated>2023-06-28T01:33:35Z</updated>
    <id>tag:github.com,2023-06-28:/sb-ocr/diy-spacemouse</id>
    <link href="https://github.com/sb-ocr/diy-spacemouse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A DIY navigation device for Fusion360&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DIY Spacemouse for Fusion 360&lt;/h1&gt; &#xA;&lt;p&gt;Watch the build video ‚Üì&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/iHBgNGnTiK4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sb-ocr/diy-spacemouse/main/Images/Spacemouse_Thumbnail@2x.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This device is made for Fusion360 but can be adapted to other CAD applications. Current features: Orbit, Pan, Home view and Fit to view.&lt;/p&gt; &#xA;&lt;p&gt;Build instructions (coming soon)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>CASIA-IVA-Lab/FastSAM</title>
    <updated>2023-06-28T01:33:35Z</updated>
    <id>tag:github.com,2023-06-28:/CASIA-IVA-Lab/FastSAM</id>
    <link href="https://github.com/CASIA-IVA-Lab/FastSAM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fast Segment Anything&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/logo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Fast Segment Anything&lt;/h1&gt; &#xA;&lt;p&gt;[&lt;a href=&#34;https://arxiv.org/pdf/2306.12156.pdf&#34;&gt;&lt;code&gt;üìïPaper&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://huggingface.co/spaces/An-619/FastSAM&#34;&gt;&lt;code&gt;ü§óHuggingFace Demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://colab.research.google.com/drive/1oX14f6IneGGw612WgVlAiy91UHwFAvr9?usp=sharing&#34;&gt;&lt;code&gt;Colab demo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://replicate.com/casia-iva-lab/fastsam&#34;&gt;&lt;code&gt;Replicate demo &amp;amp; API&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/#model-checkpoints&#34;&gt;&lt;code&gt;Model Zoo&lt;/code&gt;&lt;/a&gt;] [&lt;a href=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/#citing-fastsam&#34;&gt;&lt;code&gt;BibTeX&lt;/code&gt;&lt;/a&gt;]&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/head_fig.png&#34; alt=&#34;FastSAM Speed&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;Fast Segment Anything Model(FastSAM)&lt;/strong&gt; is a CNN Segment Anything Model trained by only 2% of the SA-1B dataset published by SAM authors. The FastSAM achieve a comparable performance with the SAM method at &lt;strong&gt;50√ó higher run-time speed&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/Overview.png&#34; alt=&#34;FastSAM design&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üçá Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;2023/06/26&lt;/code&gt;&lt;/strong&gt; Release &lt;a href=&#34;https://replicate.com/casia-iva-lab/fastsam&#34;&gt;FastSAM Replicate Online Demo&lt;/a&gt;. Thanks a lot to &lt;a href=&#34;https://chenxwh.github.io/&#34;&gt;Chenxi&lt;/a&gt; for providing this nice demo üåπ.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;2023/06/26&lt;/code&gt;&lt;/strong&gt; Support &lt;a href=&#34;https://huggingface.co/spaces/An-619/FastSAM&#34;&gt;points mode&lt;/a&gt; in HuggingFace Space. Better and faster interaction will come soon!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;2023/06/24&lt;/code&gt;&lt;/strong&gt; Thanks a lot to &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34;&gt;Grounding-SAM&lt;/a&gt; for Combining Grounding-DINO with FastSAM in &lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything/tree/main/FastSAM&#34;&gt;Grounded-FastSAM&lt;/a&gt; üåπ.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Clone the repository locally:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/CASIA-IVA-Lab/FastSAM.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create the conda env. The code requires &lt;code&gt;python&amp;gt;=3.7&lt;/code&gt;, as well as &lt;code&gt;pytorch&amp;gt;=1.7&lt;/code&gt; and &lt;code&gt;torchvision&amp;gt;=0.8&lt;/code&gt;. Please follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; to install both PyTorch and TorchVision dependencies. Installing both PyTorch and TorchVision with CUDA support is strongly recommended.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda create -n FastSAM python=3.9&#xA;conda activate FastSAM&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd FastSAM&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install CLIP:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install git+https://github.com/openai/CLIP.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;GettingStarted&#34;&gt;&lt;/a&gt; Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;First download a &lt;a href=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/#model-checkpoints&#34;&gt;model checkpoint&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then, you can run the scripts to try the everything mode and three prompt modes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Everything mode&#xA;python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Text prompt&#xA;python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg  --text_prompt &#34;the yellow dog&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Box prompt (xywh)&#xA;python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg --box_prompt &#34;[570,200,230,400]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Points prompt&#xA;python Inference.py --model_path ./weights/FastSAM.pt --img_path ./images/dogs.jpg  --point_prompt &#34;[[520,360],[620,300]]&#34; --point_label &#34;[1,0]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You are also welcomed to try our Colab demo: &lt;a href=&#34;https://colab.research.google.com/drive/1oX14f6IneGGw612WgVlAiy91UHwFAvr9?usp=sharing&#34;&gt;FastSAM_example.ipynb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Different Inference Options&lt;/h2&gt; &#xA;&lt;p&gt;We provide various options for different purposes, details are in &lt;a href=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/MORE_USAGES.md&#34;&gt;MORE_USAGES.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Web demo&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We also provide a UI for testing our method that is built with gradio. You can upload a custom image, select the mode and set the parameters, click the segment button, and get a satisfactory segmentation result. Everything mode and points mode are now supported for interaction, other modes will try to support in the future. Running the following command in a terminal will launch the demo:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Download the pre-trained model in &#34;./weights/FastSAM.pt&#34;&#xA;python app_gradio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This demo is also hosted on &lt;a href=&#34;https://huggingface.co/spaces/An-619/FastSAM&#34;&gt;HuggingFace Space&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/hf_everything_mode.png&#34; alt=&#34;HF_Everyhting&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/hf_points_mode.png&#34; alt=&#34;HF_Points&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Replicate demo&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/casia-iva-lab/fastsam&#34;&gt;Replicate demo&lt;/a&gt; has supported all modes, you can experience points/box/text mode.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/replicate-1.png&#34; alt=&#34;Replicate-1&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/replicate-2.png&#34; alt=&#34;Replicate-2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/replicate-3.png&#34; alt=&#34;Replicate-3&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;Models&#34;&gt;&lt;/a&gt;Model Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;Two model versions of the model are available with different sizes. Click the links below to download the checkpoint for the corresponding model type.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;default&lt;/code&gt; or &lt;code&gt;FastSAM&lt;/code&gt;: &lt;a href=&#34;https://drive.google.com/file/d/1m1sjY4ihXBU1fZXdQ-Xdj-mDltW-2Rqv/view?usp=sharing&#34;&gt;YOLOv8x based Segment Anything Model&lt;/a&gt; | &lt;a href=&#34;https://pan.baidu.com/s/18KzBmOTENjByoWWR17zdiQ?pwd=0000&#34;&gt;Baidu Cloud (pwd: 0000).&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;FastSAM-s&lt;/code&gt;: &lt;a href=&#34;https://drive.google.com/file/d/10XmSj6mmpmRb8NhXbtiuO9cTTBwR_9SV/view?usp=sharing&#34;&gt;YOLOv8s based Segment Anything Model.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;All result were tested on a single NVIDIA GeForce RTX 3090.&lt;/p&gt; &#xA;&lt;h3&gt;1. Inference time&lt;/h3&gt; &#xA;&lt;p&gt;Running Speed under Different Point Prompt Numbers(ms).&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;params&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;1&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;10&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;100&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;E(16x16)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;E(32x32*)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;E(64x64)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-H&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.6G&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;446&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;464&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;627&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;852&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2099&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6972&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;136M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;110&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;125&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;230&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;432&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1383&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5417&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FastSAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68M&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;2. Memory usage&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Dataset&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPU Memory (MB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO 2017&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FastSAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2608&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO 2017&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-H&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7060&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;COCO 2017&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4670&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;3. Zero-shot Transfer Experiments&lt;/h3&gt; &#xA;&lt;h4&gt;Edge Detection&lt;/h4&gt; &#xA;&lt;p&gt;Test on the BSDB500 dataset.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;year&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;ODS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;OIS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;R50&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;HED&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2015&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.788&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.808&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.840&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.923&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.768&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.786&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.794&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.928&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FastSAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2023&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.750&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.790&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.793&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.903&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Object Proposals&lt;/h4&gt; &#xA;&lt;h5&gt;COCO&lt;/h5&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AR10&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AR100&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AR1000&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AUC&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-H E64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-H E32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;62.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-B E32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;59.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FastSAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h5&gt;LVIS&lt;/h5&gt; &#xA;&lt;p&gt;bbox AR@1000&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;all&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;small&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;med.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;large&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ViTDet-H&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;65.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;83.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;91.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zero-shot transfer methods&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-H E64&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;75.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;88.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-H E32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;89.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM-B E32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;68.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;80.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FastSAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;57.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;77.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Instance Segmentation On COCO 2017&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;AP&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;APS&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;APM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;APL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ViTDet-H&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.510&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.320&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.543&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.689&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.465&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.308&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.510&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.617&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;FastSAM&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.379&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.239&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.434&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;.500&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;4. Performance Visualization&lt;/h3&gt; &#xA;&lt;p&gt;Several segmentation results:&lt;/p&gt; &#xA;&lt;h4&gt;Natural Images&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/eightpic.png&#34; alt=&#34;Natural Images&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Text to Mask&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/dog_clip.png&#34; alt=&#34;Text to Mask&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5.Downstream tasks&lt;/h3&gt; &#xA;&lt;p&gt;The results of several downstream tasks to show the effectiveness.&lt;/p&gt; &#xA;&lt;h4&gt;Anomaly Detection&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/anomaly.png&#34; alt=&#34;Anomaly Detection&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Salient Object Detection&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/salient.png&#34; alt=&#34;Salient Object Detection&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Building Extracting&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/assets/building.png&#34; alt=&#34;Building Detection&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The model is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/CASIA-IVA-Lab/FastSAM/main/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://segment-anything.com/&#34;&gt;Segment Anything&lt;/a&gt; provides the SA-1B dataset and the base codes.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ultralytics/ultralytics&#34;&gt;YOLOv8&lt;/a&gt; provides codes and pre-trained models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10003&#34;&gt;YOLACT&lt;/a&gt; provides powerful instance segmentation method.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/yizhangliu/Grounded-Segment-Anything&#34;&gt;Grounded-Segment-Anything&lt;/a&gt; provides a useful web demo template.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Our project wouldn&#39;t be possible without the contributions of these amazing people! Thank you all for making this project better.&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/CASIA-IVA-Lab/FastSAM/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=CASIA-IVA-Lab/FastSAM&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Citing FastSAM&lt;/h2&gt; &#xA;&lt;p&gt;If you find this project useful for your research, please consider citing the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{zhao2023fast,&#xA;      title={Fast Segment Anything},&#xA;      author={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},&#xA;      year={2023},&#xA;      eprint={2306.12156},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#CASIA-IVA-Lab/FastSAM&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=CASIA-IVA-Lab/FastSAM&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>