<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-09T01:24:01Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>plandex-ai/plandex</title>
    <updated>2024-04-09T01:24:01Z</updated>
    <id>tag:github.com,2024-04-09:/plandex-ai/plandex</id>
    <link href="https://github.com/plandex-ai/plandex" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An AI coding engine for complex tasks&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a href=&#34;https://plandex.ai&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;images/plandex-logo-dark.png&#34;&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;images/plandex-logo-light.png&#34;&gt; &#xA;   &lt;img width=&#34;370&#34; src=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/images/plandex-logo-dark-bg.png&#34;&gt; &#xA;  &lt;/picture&gt;&lt;/a&gt; &lt;br&gt; &lt;/h1&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;strong&gt;üîÆ An open source, terminal-based AI coding engine for complex tasks.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- Badges --&gt; &lt;a href=&#34;https://github.com/plandex-ai/plandex/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-brightgreen.svg?sanitize=true&#34; alt=&#34;PRs Welcome&#34;&gt;&lt;/a&gt;‚ÄÇ &lt;a href=&#34;https://github.com/plandex-ai/plandex/releases?q=cli&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/plandex-ai/plandex?filter=cli*&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/plandex-ai/plandex/releases?q=server&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/plandex-ai/plandex?filter=server*&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &#xA; &lt;!-- &lt;a href=&#34;https://github.com/your_username/your_project/issues&#34;&gt;&#xA;    &lt;img src=&#34;https://img.shields.io/github/issues-closed/your_username/your_project.svg&#34; alt=&#34;Issues Closed&#34; /&gt;&#xA;  &lt;/a&gt; --&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- Call to Action Links --&gt; &lt;a href=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/#install&#34;&gt; &lt;b&gt;Install&lt;/b&gt; &lt;/a&gt; ¬∑ &#xA; &lt;!-- &lt;a href=&#34;https://plandex.ai&#34;&gt;&#xA;    &lt;b&gt;Website&lt;/b&gt;&#xA;  &lt;/a&gt;&#xA;  ¬∑ --&gt; &lt;a href=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/guides/USAGE.md&#34;&gt; &lt;b&gt;Usage&lt;/b&gt; &lt;/a&gt; ¬∑ &lt;a href=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/guides/HOSTING.md&#34;&gt; &lt;b&gt;Self-Hosting&lt;/b&gt; &lt;/a&gt; ¬∑ &lt;a href=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/guides/DEVELOPMENT.md&#34;&gt; &lt;b&gt;Development&lt;/b&gt; &lt;/a&gt; ¬∑ &lt;a href=&#34;https://discord.gg/plandex-ai&#34;&gt; &lt;b&gt;Discord&lt;/b&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Plandex uses long-running agents to complete tasks that span multiple files and require many steps. It breaks up large tasks into smaller subtasks, then implements each one, continuing until it finishes the job. It helps you churn through your backlog, work with unfamiliar technologies, get unstuck, and spend less time on the boring stuff. &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;!-- Vimeo link is nicer on mobile than embedded video... downside is it navigates to vimeo in same tab (no way to add target=_blank) --&gt; &#xA;&lt;!-- https://github.com/plandex-ai/plandex/assets/545350/c2ee3bcd-1512-493f-bdd5-e3a4ca534a36 --&gt; &#xA;&lt;a href=&#34;https://player.vimeo.com/video/926634577&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/images/plandex-intro-vimeo.png&#34; alt=&#34;Plandex intro video&#34; width=&#34;100%&#34;&gt; &lt;/a&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;üåü&amp;nbsp; Build complex software with LLMs&lt;/h2&gt; &#xA;&lt;p&gt;‚ö°Ô∏è &amp;nbsp;Changes are accumulated in a protected sandbox so that you can review them before automatically applying them to your project files. Built-in version control allows you to easily go backwards and try a different approach. Branches allow you to try multiple approaches and compare the results.&lt;/p&gt; &#xA;&lt;p&gt;üìë &amp;nbsp;Manage context efficiently in the terminal. Easily add files or entire directories to context, and keep them updated automatically as you work so that models always have the latest state of your project.&lt;/p&gt; &#xA;&lt;p&gt;üß† &amp;nbsp;Plandex relies on the OpenAI API and requires an &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable. Support for open source models, Google Gemini, and Anthropic Claude is coming soon.&lt;/p&gt; &#xA;&lt;p&gt;‚úÖ &amp;nbsp;Plandex supports Mac, Linux, FreeBSD, and Windows. It runs from a single binary with no dependencies.&lt;/p&gt; &#xA;&lt;h2&gt;Install&amp;nbsp;&amp;nbsp;üì•&lt;/h2&gt; &#xA;&lt;h3&gt;Quick install&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl -sL https://plandex.ai/install.sh | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Manual install&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt; Grab the appropriate binary for your platform from the latest &lt;a href=&#34;https://github.com/plandex-ai/plandex/releases&#34;&gt;release&lt;/a&gt; and put it somewhere in your &lt;code&gt;PATH&lt;/code&gt;. &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Build from source&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;pre&gt;&lt;code&gt;git clone https://github.com/plandex-ai/plandex.git&#xA;git clone https://github.com/plandex-ai/survey.git&#xA;cd plandex/app/cli&#xA;go build -ldflags &#34;-X plandex/version.Version=$(cat version.txt)&#34;&#xA;mv plandex /usr/local/bin # adapt as needed for your system&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Windows&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt; Windows is supported via &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/wsl/about&#34;&gt;WSL&lt;/a&gt;. &lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Get started&amp;nbsp; üöÄ&lt;/h2&gt; &#xA;&lt;p&gt;If you don&#39;t have an OpenAI account, first &lt;a href=&#34;https://platform.openai.com/signup&#34;&gt;sign up here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;generate an API key here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd your-project&#xA;export OPENAI_API_KEY=...&#xA;plandex new&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After any plandex command is run, commands that could make sense to run next will be suggested. You can learn to use Plandex quickly by jumping in and following these suggestions.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&amp;nbsp; üõ†Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/guides/USAGE.md&#34;&gt;Here&#39;s a quick overview of the commands and functionality.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Help&amp;nbsp; ‚ÑπÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;To see all available commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;plandex help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For help on any command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;plandex [command] --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Why Plandex?&amp;nbsp; ü§î&lt;/h2&gt; &#xA;&lt;p&gt;üèóÔ∏è&amp;nbsp; Go beyond autocomplete to build complex functionality with AI.&lt;br&gt; üö´&amp;nbsp; Stop the mouse-centered, copy-pasting madness of coding with ChatGPT.&lt;br&gt; üìë&amp;nbsp; Manage context efficiently in the terminal.&lt;br&gt; ‚ö°Ô∏è&amp;nbsp; Ensure AI models always have the latest versions of files in context.&lt;br&gt; ü™ô&amp;nbsp; Retain granular control over what&#39;s in context and how many tokens you&#39;re using.&lt;br&gt; üöß&amp;nbsp; Experiment, revise, and review in a protected sandbox before applying changes.&lt;br&gt; ‚è™&amp;nbsp; Rewind and retry as needed.&lt;br&gt; üå±&amp;nbsp; Explore multiple approaches with branches.&lt;br&gt; üîÄ&amp;nbsp; Run tasks in the background or work on multiple tasks in parallel.&lt;br&gt; üéõÔ∏è&amp;nbsp; Try different models and model settings, then compare results.&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Plandex Cloud&amp;nbsp; ‚òÅÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt;Plandex Cloud is the easiest and most reliable way to use Plandex. You&#39;ll be prompted to start an anonymous trial (no email required) when you create your first plan with &lt;code&gt;plandex new&lt;/code&gt;. Trial accounts are limited to 10 plans and 10 AI model replies per plan. You can upgrade to an unlimited account with your name and email.&lt;/p&gt; &#xA;&lt;p&gt;Plandex Cloud accounts are free for now. In the future, they will cost somewhere in the $10-20 per month range.&lt;/p&gt; &#xA;&lt;h2&gt;Self-hosting&amp;nbsp; üè†&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/guides/HOSTING.md&#34;&gt;Read about self-hosting Plandex here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&amp;nbsp;and guidance ‚ö†Ô∏è&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Plandex can provide a significant boost to your productivity, but as with any other AI tool, you shouldn&#39;t expect perfect results. Always review a plan carefully before applying changes, especially if security is involved. Plandex is designed to get you 80-90% of the way there rather than 100%.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Due to the reasoning limitations of LLMs, automatically applied file updates also aren&#39;t perfect. While these can be improved over time with better prompting strategies (contributions welcome) and better models, be prepared for occasional updates that aren&#39;t quite right. Use the &lt;code&gt;plandex changes&lt;/code&gt; command to review pending updates in a TUI. If a file update has mistakes, make those changes yourself with copy-and-paste and reject the file in the changes TUI.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The more direction and detail you provide, the better the results will be. Working with Plandex often involves giving it a prompt, seeing that the results are a bit off, then using &lt;code&gt;plandex rewind&lt;/code&gt; to go back and iterate on the prompt or add context before trying again. Branches are also useful for trying different approaches.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;While it can be tempting to just dump your entire project into context if it fits under the token limit, with current models you will tend to see better results (and pay less) by being more selective about what&#39;s loaded into context.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Security &amp;nbsp;üîê&lt;/h2&gt; &#xA;&lt;p&gt;Plandex Cloud follows best practices for network and data security. And whether cloud or self-hosted, Plandex protects model provider API keys (like your OpenAI API key). &lt;a href=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/guides/SECURITY.md&#34;&gt;Read more here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Privacy and data retention &amp;nbsp;üõ°Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/guides/PRIVACY.md&#34;&gt;Read about Plandex Cloud&#39;s privacy and data retention policies here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap &amp;nbsp;üó∫Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;üß†&amp;nbsp; Support for open source models, Google Gemini, and Anthropic Claude in addition to OpenAI&lt;br&gt; ü§ù&amp;nbsp; Plan sharing and team collaboration&lt;br&gt; üñºÔ∏è&amp;nbsp; Support for GPT4-Vision and other multi-modal models‚Äîadd images and screenshots to context&lt;br&gt; üñ•Ô∏è&amp;nbsp; VSCode and JetBrains extensions&lt;br&gt; üì¶&amp;nbsp; Community plugins and modules&lt;br&gt; üîå&amp;nbsp; Github integration&lt;br&gt; üåê&amp;nbsp; Web dashboard and GUI&lt;br&gt; üîê&amp;nbsp; SOC2 compliance&lt;br&gt; üõ©Ô∏è&amp;nbsp; Fine-tuned models&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;This list will grow and be prioritized based on your feedback.&lt;/p&gt; &#xA;&lt;h2&gt;Discussion and discord &amp;nbsp;üí¨&lt;/h2&gt; &#xA;&lt;p&gt;Speaking of feedback, feel free to give yours, ask questions, report a bug, or just hang out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/plandex-ai&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/plandex-ai/plandex/discussions&#34;&gt;Discussions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/plandex-ai/plandex/issues&#34;&gt;Issues&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors &amp;nbsp;üë•&lt;/h2&gt; &#xA;&lt;p&gt;‚≠êÔ∏è&amp;nbsp;&amp;nbsp;Please star, fork, explore, and contribute to Plandex. There&#39;s a lot of work to do and so much that can be improved.&lt;/p&gt; &#xA;&lt;p&gt;Work on tests, evals, prompts, and bug fixes is especially appreciated.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/plandex-ai/plandex/main/guides/DEVELOPMENT.md&#34;&gt;Here&#39;s an overview on setting up a development environment.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Comparable tools ‚öñÔ∏è&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/paul-gauthier/aider&#34;&gt;Aider&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AbanteAI/mentat&#34;&gt;Mentat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Pythagora-io/gpt-pilot&#34;&gt;Pythagora Gpt-pilot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sourcegraph/cody&#34;&gt;Sourcegraph Cody&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/continuedev/continue&#34;&gt;Continue.dev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sweepai/sweep&#34;&gt;Sweep.dev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/getcursor/cursor&#34;&gt;Cursor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/features/copilot&#34;&gt;Github Copilot&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replit.com/ai&#34;&gt;Replit Ghostwriter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chat.openai.com/g/g-n7Rs0IK86-grimoire&#34;&gt;Grimoire&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About the developer&amp;nbsp; üëã&lt;/h2&gt; &#xA;&lt;p&gt;Hi, I&#39;m Dane. I&#39;ve been building and launching software products for 17 years. I went through YCombinator in winter 2018 with my devops security company, &lt;a href=&#34;https://envkey.com&#34;&gt;EnvKey&lt;/a&gt;, which I continue to run today. I&#39;m fascinated by LLMs and their potential to transform the practice of software development.&lt;/p&gt; &#xA;&lt;p&gt;I live with my wife and 4 year old daughter on the SF peninsula in California. I grew up in the Finger Lakes region of upstate New York. I like reading fiction, listening to podcasts, fitness, and surfing.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>FoundationVision/VAR</title>
    <updated>2024-04-09T01:24:01Z</updated>
    <id>tag:github.com,2024-04-09:/FoundationVision/VAR</id>
    <link href="https://github.com/FoundationVision/VAR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[GPT beats diffusionüî•] [scaling laws in visual generationüìà] Official impl. of &#34;Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VAR: a new visual generation method elevates GPT-style models beyond diffusionüöÄ &amp;amp; Scaling laws observedüìà&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://var.vision/demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Play%20with%20VAR%21-VAR%20demo%20platform-lightblue&#34; alt=&#34;demo platform&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv%20papr-2404.02905-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://huggingface.co/FoundationVision/var&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Weights-FoundationVision/var-yellow&#34; alt=&#34;huggingface weights&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://paperswithcode.com/sota/image-generation-on-imagenet-256x256?tag_filter=485&amp;amp;p=visual-autoregressive-modeling-scalable-image&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/State%20of%20the%20Art-Image%20Generation%20on%20ImageNet%20%28AR%29-32B1B4?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iNjA2IiBoZWlnaHQ9IjYwNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayIgb3ZlcmZsb3c9ImhpZGRlbiI%2BPGRlZnM%2BPGNsaXBQYXRoIGlkPSJjbGlwMCI%2BPHJlY3QgeD0iLTEiIHk9Ii0xIiB3aWR0aD0iNjA2IiBoZWlnaHQ9IjYwNiIvPjwvY2xpcFBhdGg%2BPC9kZWZzPjxnIGNsaXAtcGF0aD0idXJsKCNjbGlwMCkiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDEgMSkiPjxyZWN0IHg9IjUyOSIgeT0iNjYiIHdpZHRoPSI1NiIgaGVpZ2h0PSI0NzMiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIxOSIgeT0iNjYiIHdpZHRoPSI1NyIgaGVpZ2h0PSI0NzMiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIyNzQiIHk9IjE1MSIgd2lkdGg9IjU3IiBoZWlnaHQ9IjMwMiIgZmlsbD0iIzQ0RjJGNiIvPjxyZWN0IHg9IjEwNCIgeT0iMTUxIiB3aWR0aD0iNTciIGhlaWdodD0iMzAyIiBmaWxsPSIjNDRGMkY2Ii8%2BPHJlY3QgeD0iNDQ0IiB5PSIxNTEiIHdpZHRoPSI1NyIgaGVpZ2h0PSIzMDIiIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSIzNTkiIHk9IjE3MCIgd2lkdGg9IjU2IiBoZWlnaHQ9IjI2NCIgZmlsbD0iIzQ0RjJGNiIvPjxyZWN0IHg9IjE4OCIgeT0iMTcwIiB3aWR0aD0iNTciIGhlaWdodD0iMjY0IiBmaWxsPSIjNDRGMkY2Ii8%2BPHJlY3QgeD0iNzYiIHk9IjY2IiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI0ODIiIHk9IjY2IiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI3NiIgeT0iNDgyIiB3aWR0aD0iNDciIGhlaWdodD0iNTciIGZpbGw9IiM0NEYyRjYiLz48cmVjdCB4PSI0ODIiIHk9IjQ4MiIgd2lkdGg9IjQ3IiBoZWlnaHQ9IjU3IiBmaWxsPSIjNDRGMkY2Ii8%2BPC9nPjwvc3ZnPg%3D%3D&#34; alt=&#34;SOTA&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;This is the official PyTorch implementation of &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/9850df90-20b1-4f29-8592-e3526d16d755&#34; width=&#34;95%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h2&gt;üïπÔ∏è Try and Play with VAR!&lt;/h2&gt; &#xA;&lt;p&gt;We provide a &lt;a href=&#34;https://var.vision/demo&#34;&gt;demo website&lt;/a&gt; for you to play with VAR models and generate images interactively. Enjoy the fun of visual autoregressive modeling!&lt;/p&gt; &#xA;&lt;p&gt;We also provide &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/demo_sample.ipynb&#34;&gt;demo_sample.ipynb&lt;/a&gt; for you to see more technical details about VAR.&lt;/p&gt; &#xA;&lt;h2&gt;What&#39;s New?&lt;/h2&gt; &#xA;&lt;h3&gt;üî• Introducing VAR: a new paradigm in autoregressive visual generation‚ú®:&lt;/h3&gt; &#xA;&lt;p&gt;Visual Autoregressive Modeling (VAR) redefines the autoregressive learning on images as coarse-to-fine &#34;next-scale prediction&#34; or &#34;next-resolution prediction&#34;, diverging from the standard raster-scan &#34;next-token prediction&#34;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/3e12655c-37dc-4528-b923-ec6c4cfef178&#34; width=&#34;93%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• For the first time, GPT-style autoregressive models surpass diffusion modelsüöÄ:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/cc30b043-fa4e-4d01-a9b1-e50650d5675d&#34; width=&#34;55%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• Discovering power-law Scaling Laws in VAR transformersüìà:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/c35fb56e-896e-4e4b-9fb9-7a1c38513804&#34; width=&#34;85%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/91d7b92c-8fc3-44d9-8fb4-73d6cdb8ec1e&#34; width=&#34;85%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h3&gt;üî• Zero-shot generalizabilityüõ†Ô∏è:&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/FoundationVision/VAR/assets/39692511/a54a4e52-6793-4130-bae2-9e459a08e96a&#34; width=&#34;70%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h4&gt;For a deep dive into our analyses, discussions, and evaluations, check out our &lt;a href=&#34;https://arxiv.org/abs/2404.02905&#34;&gt;paper&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;h2&gt;VAR zoo&lt;/h2&gt; &#xA;&lt;p&gt;We provide VAR models for you to play with, which are on &lt;a href=&#34;https://huggingface.co/FoundationVision/var&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Huggingface-FoundationVision/var-yellow&#34;&gt;&lt;/a&gt; or can be downloaded from the following links:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;reso.&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;FID&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;rel. cost&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;#params&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;HF weightsü§ó&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3.55&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;310M&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d16.pth&#34;&gt;var_d16.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.95&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;600M&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d20.pth&#34;&gt;var_d20.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.33&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d24.pth&#34;&gt;var_d24.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d30&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.97&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d30.pth&#34;&gt;var_d30.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;VAR-d30-re&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;256&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;1.80&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2.0B&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/var_d30.pth&#34;&gt;var_d30.pth&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;You can load these models to generate images via the codes in &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/demo_sample.ipynb&#34;&gt;demo_sample.ipynb&lt;/a&gt;. Note: you need to download &lt;a href=&#34;https://huggingface.co/FoundationVision/var/resolve/main/vae_ch160v4096z32.pth&#34;&gt;vae_ch160v4096z32.pth&lt;/a&gt; first.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/FoundationVision/VAR/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If our work assists your research, feel free to give us a star ‚≠ê or cite us using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Article{VAR,&#xA;      title={Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction}, &#xA;      author={Keyu Tian and Yi Jiang and Zehuan Yuan and Bingyue Peng and Liwei Wang},&#xA;      year={2024},&#xA;      eprint={2404.02905},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>nashsu/FreeAskInternet</title>
    <updated>2024-04-09T01:24:01Z</updated>
    <id>tag:github.com,2024-04-09:/nashsu/FreeAskInternet</id>
    <link href="https://github.com/nashsu/FreeAskInternet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FreeAskInternet is a completely free, private and locally running search aggregator &amp; answer generate using LLM, without GPU needed. The user can ask a question and the system will make a multi engine search and combine the search result to the ChatGPT3.5 LLM and generate the answer based on search results.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FreeAskInternet&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Running &lt;a href=&#34;http://www.perplexity.ai&#34;&gt;www.perplexity.ai&lt;/a&gt; like app complete FREE, LOCAL, PRIVATE and NO GPU NEED on any computer&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br&gt; &lt;strong&gt;If you are unable to use this project normally, it is most likely due to issues with your internet connection or your IP, you need free internet connection to use this project normally. Â¶ÇÊûúÊÇ®Êó†Ê≥ïÊ≠£Â∏∏‰ΩøÁî®Ê≠§È°πÁõÆÔºåÂæàÂèØËÉΩÊòØÁî±‰∫éÊÇ®ÁöÑ IP Â≠òÂú®ÈóÆÈ¢òÔºåÊàñËÄÖ‰Ω†‰∏çËÉΩËá™Áî±ËÆøÈóÆ‰∫íËÅîÁΩë„ÄÇ&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;What is FreeAskInternet&lt;/h2&gt; &#xA;&lt;p&gt;FreeAskInternet is a completely free, private and locally running search aggregator &amp;amp; answer generate using LLM, Without GPU needed. The user can ask a question and the system will use searxng to make a multi engine search and combine the search result to the ChatGPT3.5 LLM and generate the answer based on search results. All process running locally and No GPU or OpenAI or Google API keys are needed.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üàöÔ∏è Completely FREE (no need for any API keys)&lt;/li&gt; &#xA; &lt;li&gt;üíª Completely LOCAL (no GPU need, any computer can run )&lt;/li&gt; &#xA; &lt;li&gt;üîê Completely PRIVATE (all thing runing locally)&lt;/li&gt; &#xA; &lt;li&gt;üëª Runs WITHOUT LLM Hardware (NO GPU NEED!)&lt;/li&gt; &#xA; &lt;li&gt;ü§© Using Free ChatGPT3.5 API (NO API keys need! Thx OpenAI)&lt;/li&gt; &#xA; &lt;li&gt;üöÄ Fast and easy to deploy with Docker Compose&lt;/li&gt; &#xA; &lt;li&gt;üåê Web and Mobile friendly interface, allowing for easy access from any device ( Thx ChatGPT-Next-Web )&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How It Works?&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;System get user input question in ChatGPT-Next-Web ( running locally), and call searxng (running locally) to make search on multi search engine.&lt;/li&gt; &#xA; &lt;li&gt;crawl search result links content and pass to ChatGPT3.5 (using OpenAI ChatGPT3.5, through FreeGPT35 running locally), ask ChatGPT3.5 to answer user question based on this contents as references.&lt;/li&gt; &#xA; &lt;li&gt;Stream the answer to ChatGPT-Next-Web Chat UI.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;This project is still in its very early days. Expect some bugs.&lt;/p&gt; &#xA;&lt;h3&gt;Run the latest release&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/nashsu/FreeAskInternet.git&#xA;cd ./FreeAskInternet&#xA;docker-compose up -d &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;üéâ You should now be able to open the web interface on &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;. Nothing else is exposed by default.&lt;/p&gt; &#xA;&lt;h3&gt;How to update to latest&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd ./FreeAskInternet&#xA;git pull&#xA;docker compose rm backend&#xA;docker image rm nashsu/free_ask_internet&#xA;docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ChatGPT-Next-Web : &lt;a href=&#34;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web&#34;&gt;https://github.com/ChatGPTNextWeb/ChatGPT-Next-Web&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;FreeGPT35: &lt;a href=&#34;https://github.com/missuo/FreeGPT35&#34;&gt;https://github.com/missuo/FreeGPT35&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;searxng: &lt;a href=&#34;https://github.com/searxng/searxng&#34;&gt;https://github.com/searxng/searxng&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache-2.0 license&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#nashsu/FreeAskInternet&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=nashsu/FreeAskInternet&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>