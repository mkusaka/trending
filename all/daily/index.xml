<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-14T01:22:12Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>modelscope/facechain</title>
    <updated>2023-08-14T01:22:12Z</updated>
    <id>tag:github.com,2023-08-14:/modelscope/facechain</id>
    <link href="https://github.com/modelscope/facechain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;FaceChain is a deep-learning toolchain for generating your Digital-Twin.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif&#34; width=&#34;400&#34;&gt; &lt;br&gt; &lt;/p&gt;&#xA;&lt;h1&gt;FaceChain&lt;/h1&gt; &#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;如果您熟悉中文，可以阅读&lt;a href=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/README_ZH.md&#34;&gt;中文版本的README&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;FaceChain is a deep-learning toolchain for generating your Digital-Twin. With a minimum of 1 portrait-photo, you can create a Digital-Twin of your own and start generating personal photos in different settings (work photos as starter!). You may train your Digital-Twin model and generate photos via FaceChain&#39;s Python scripts, or via the familiar Gradio interface. You can also experience FaceChain directly with our &lt;a href=&#34;https://modelscope.cn/studios/CVstudio/cv_human_portrait/summary&#34;&gt;ModelScope Studio&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;FaceChain is powered by &lt;a href=&#34;https://github.com/modelscope/modelscope&#34;&gt;ModelScope&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/example1.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/example2.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/example3.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Compatibility Verification&lt;/h2&gt; &#xA;&lt;p&gt;The following are the environment dependencies that have been verified:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;python: py3.8, py3.10&lt;/li&gt; &#xA; &lt;li&gt;pytorch: torch2.0.0, torch2.0.1&lt;/li&gt; &#xA; &lt;li&gt;tensorflow: 2.7.0, tensorflow-cpu&lt;/li&gt; &#xA; &lt;li&gt;CUDA: 11.7&lt;/li&gt; &#xA; &lt;li&gt;CUDNN: 8+&lt;/li&gt; &#xA; &lt;li&gt;OS: Ubuntu 20.04, CentOS 7.9&lt;/li&gt; &#xA; &lt;li&gt;GPU: Nvidia-A10 24G&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resource Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;GPU: About 19G&lt;/li&gt; &#xA; &lt;li&gt;Disk: About 50GB&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation Guide&lt;/h2&gt; &#xA;&lt;p&gt;The following installation methods are supported:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;ModelScope notebook【recommended】 The ModelScope notebook has a free tier that allows you to run the FaceChain application, refer to &lt;a href=&#34;https://modelscope.cn/my/mynotebook/preset&#34;&gt;ModelScope Notebook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In addition to ModelScope notebook and ECS, I would suggest that we add that user may also start DSW instance with the option of ModelScope (GPU) image, to create a ready-to-use environment.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Step1&#xA;我的notebook -&amp;gt; PAI-DSW -&amp;gt; GPU环境&#xA;&#xA;# Step2&#xA;Open the Terminal，clone FaceChain from github:&#xA;GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git&#xA;&#xA;# Step3&#xA;Entry the Notebook cell:&#xA;import os&#xA;os.chdir(&#39;/mnt/workspace/facechain&#39;)&#xA;print(os.getcwd())&#xA;&#xA;!pip3 install gradio&#xA;!python3 app.py&#xA;&#xA;&#xA;# Step4&#xA;click &#34;public URL&#34; or &#34;local URL&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Docker&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you are familiar with using docker, we recommend to use this way:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Step1&#xA;Prepare the environment with GPU on local or cloud, we recommend to use Alibaba Cloud ECS, refer to: https://www.aliyun.com/product/ecs&#xA;&#xA;# Step2&#xA;Download the docker image (for installing docker engine, refer to https://docs.docker.com/engine/install/）&#xA;docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.7.1-py38-torch2.0.1-tf1.15.5-1.8.0&#xA;&#xA;# Step3&#xA;docker images&#xA;docker run -it --name facechain -p 7860:7860 --gpus all your_xxx_image_id /bin/bash&#xA;(Note： you may need to install the nvidia-container-runtime, refer to https://github.com/NVIDIA/nvidia-container-runtime)&#xA;&#xA;# Step4&#xA;Install the gradio in the docker container:&#xA;pip3 install gradio&#xA;&#xA;# Step5&#xA;GIT_LFS_SKIP_SMUDGE=1 git clone https://github.com/modelscope/facechain.git&#xA;cd facechain&#xA;python3 app.py&#xA;&#xA;# Step6&#xA;Run the app server: click &#34;public URL&#34; --&amp;gt; in the form of: https://xxx.gradio.live&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Script Execution&lt;/h1&gt; &#xA;&lt;p&gt;FaceChain supports direct training and inference in the python environment. Run the following command in the cloned folder to start training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PYTHONPATH=. sh train_lora.sh &#34;ly261666/cv_portrait_model&#34; &#34;v2.0&#34; &#34;film/film&#34; &#34;./imgs&#34; &#34;./processed&#34; &#34;./output&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Parameter meaning:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;ly261666/cv_portrait_model: The stable diffusion base model of the ModelScope model hub, which will be used for training, no need to be changed.&#xA;v2.0: The version number of this base model, no need to be changed&#xA;film/film: This base model may contains multiple subdirectories of different styles, currently we use film/film, no need to be changed&#xA;./imgs: This parameter needs to be replaced with the actual value. It means a local file directory that contains the original photos used for training and generation&#xA;./processed: The folder of the processed images after preprocessing, this parameter needs to be passed the same value in inference, no need to be changed&#xA;./output: The folder where the model weights stored after training, no need to be changed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait for 5-20 minutes to complete the training. Users can also adjust other training hyperparameters. The hyperparameters supported by training can be viewed in the file of &lt;code&gt;train_lora.sh&lt;/code&gt;, or the complete hyperparameter list in &lt;code&gt;facechain/train_text_to_image_lora.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When inferring, please edit the code in run_inference.py:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Fill in the folder of the images after preprocessing above, it should be the same as during training&#xA;processed_dir = &#39;./processed&#39;&#xA;# The number of images to generate in inference&#xA;num_generate = 5&#xA;# The stable diffusion base model used in training, no need to be changed&#xA;base_model = &#39;ly261666/cv_portrait_model&#39;&#xA;# The version number of this base model, no need to be changed&#xA;revision = &#39;v2.0&#39;&#xA;# This base model may contains multiple subdirectories of different styles, currently we use film/film, no need to be changed&#xA;base_model_sub_dir = &#39;film/film&#39;&#xA;# The folder where the model weights stored after training, it must be the same as during training&#xA;train_output_dir = &#39;./output&#39;&#xA;# Specify a folder to save the generated images, this parameter can be modified as needed&#xA;output_dir = &#39;./generated&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python run_inference.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can find the generated personal digital image photos in the &lt;code&gt;output_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Algorithm Introduction&lt;/h1&gt; &#xA;&lt;h2&gt;Principle&lt;/h2&gt; &#xA;&lt;p&gt;The ability of the personal portrait model comes from the text generation image function of the Stable Diffusion model. It inputs a piece of text or a series of prompt words and outputs corresponding images. We consider the main factors that affect the generation effect of personal portraits: portrait style information and user character information. For this, we use the style LoRA model trained offline and the face LoRA model trained online to learn the above information. LoRA is a fine-tuning model with fewer trainable parameters. In Stable Diffusion, the information of the input image can be injected into the LoRA model by the way of text generation image training with a small amount of input image. Therefore, the ability of the personal portrait model is divided into training and inference stages. The training stage generates image and text label data for fine-tuning the Stable Diffusion model, and obtains the face LoRA model. The inference stage generates personal portrait images based on the face LoRA model and style LoRA model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/modelscope/facechain/main/resources/framework_eng.jpg&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Input: User-uploaded images that contain clear face areas&lt;/p&gt; &#xA;&lt;p&gt;Output: Face LoRA model&lt;/p&gt; &#xA;&lt;p&gt;Description: First, we process the user-uploaded images using an image rotation model based on orientation judgment and a face refinement rotation method based on face detection and keypoint models, and obtain images containing forward faces. Next, we use a human body parsing model and a human portrait beautification model to obtain high-quality face training images. Afterwards, we use a face attribute model and a text annotation model, combined with tag post-processing methods, to generate fine-grained labels for training images. Finally, we use the above images and label data to fine-tune the Stable Diffusion model to obtain the face LoRA model.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Input: User-uploaded images in the training phase, preset input prompt words for generating personal portraits&lt;/p&gt; &#xA;&lt;p&gt;Output: Personal portrait image&lt;/p&gt; &#xA;&lt;p&gt;Description: First, we fuse the weights of the face LoRA model and style LoRA model into the Stable Diffusion model. Next, we use the text generation image function of the Stable Diffusion model to preliminarily generate personal portrait images based on the preset input prompt words. Then we further improve the face details of the above portrait image using the face fusion model. The template face used for fusion is selected from the training images through the face quality evaluation model. Finally, we use the face recognition model to calculate the similarity between the generated portrait image and the template face, and use this to sort the portrait images, and output the personal portrait image that ranks first as the final output result.&lt;/p&gt; &#xA;&lt;h2&gt;Model List&lt;/h2&gt; &#xA;&lt;p&gt;The models used in FaceChain:&lt;/p&gt; &#xA;&lt;p&gt;[1] Face detection model DamoFD：&lt;a href=&#34;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&#34;&gt;https://modelscope.cn/models/damo/cv_ddsar_face-detection_iclr23-damofd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2] Image rotating model, offered in the ModelScope studio&lt;/p&gt; &#xA;&lt;p&gt;[3] Human parsing model M2FP：&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&#34;&gt;https://modelscope.cn/models/damo/cv_resnet101_image-multiple-human-parsing&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[4] Skin retouching model ABPN：&lt;a href=&#34;https://modelscope.cn/models/damo/cv_unet_skin-retouching&#34;&gt;https://modelscope.cn/models/damo/cv_unet_skin-retouching&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[5] Face attribute recognition model FairFace：&lt;a href=&#34;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&#34;&gt;https://modelscope.cn/models/damo/cv_resnet34_face-attribute-recognition_fairface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[6] DeepDanbooru model：&lt;a href=&#34;https://github.com/KichangKim/DeepDanbooru&#34;&gt;https://github.com/KichangKim/DeepDanbooru&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[7] Face quality assessment FQA：&lt;a href=&#34;https://modelscope.cn/models/damo/cv_manual_face-quality-assessment_fqa&#34;&gt;https://modelscope.cn/models/damo/cv_manual_face-quality-assessment_fqa&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[8] Face fusion model：&lt;a href=&#34;https://modelscope.cn/models/damo/cv_unet-image-face-fusion_damo&#34;&gt;https://modelscope.cn/models/damo/cv_unet-image-face-fusion_damo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;[9] Face recognition model RTS：&lt;a href=&#34;https://modelscope.cn/models/damo/cv_ir_face-recognition-ood_rts&#34;&gt;https://modelscope.cn/models/damo/cv_ir_face-recognition-ood_rts&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;More Information&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/modelscope/modelscope/&#34;&gt;ModelScope library&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;​ ModelScope Library provides the foundation for building the model-ecosystem of ModelScope, including the interface and implementation to integrate various models into ModelScope.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88&#34;&gt;Contribute models to ModelScope&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://github.com/modelscope/modelscope/raw/master/LICENSE&#34;&gt;Apache License (Version 2.0)&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>phoboslab/wipeout-rewrite</title>
    <updated>2023-08-14T01:22:12Z</updated>
    <id>tag:github.com,2023-08-14:/phoboslab/wipeout-rewrite</id>
    <link href="https://github.com/phoboslab/wipeout-rewrite" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;wipEout Rewrite&lt;/h1&gt; &#xA;&lt;p&gt;This is a re-implementation of the 1995 PSX game wipEout.&lt;/p&gt; &#xA;&lt;p&gt;Play here: &lt;a href=&#34;https://phoboslab.org/wipegame/&#34;&gt;https://phoboslab.org/wipegame/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;More info in my blog: &lt;a href=&#34;https://phoboslab.org/log/2023/08/rewriting-wipeout&#34;&gt;https://phoboslab.org/log/2023/08/rewriting-wipeout&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;⚠️ Work in progress. Expect bugs.&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;p&gt;The game currently supports two different platform-backends: &lt;a href=&#34;https://github.com/libsdl-org/SDL&#34;&gt;SDL2&lt;/a&gt; and &lt;a href=&#34;https://github.com/floooh/sokol&#34;&gt;Sokol&lt;/a&gt;. The only difference in features is that the SDL2 backend supports game controllers (joysticks, gamepads), while the Sokol backend does not.&lt;/p&gt; &#xA;&lt;h3&gt;Linux&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;# for SDL2 backend&#xA;apt install libsdl2-dev libglew-dev&#xA;make sdl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;# for Sokol backend&#xA;apt install libx11-dev libxcursor-dev libxi-dev libasound2-dev&#xA;make sokol&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;macOS&lt;/h3&gt; &#xA;&lt;p&gt;Currently only the SDL2 backend works. macOS is very picky about the GLSL shader version when compiling with Sokol and OpenGL3.3; it shouldn&#39;t be too difficult to get it working, but will probably require a bunch of &lt;code&gt;#ifdefs&lt;/code&gt; for SDL and WASM. PRs welcome!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;brew install sdl2 glew&#xA;make sdl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Windows&lt;/h3&gt; &#xA;&lt;p&gt;In theory both backends should work on Windows, but the Makefile is missing the proper compiler flags. Please send a PR!&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;todo&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;WASM&lt;/h3&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://emscripten.org/&#34;&gt;emscripten&lt;/a&gt; and activate emsdk, so that &lt;code&gt;emcc&lt;/code&gt; is in your &lt;code&gt;PATH&lt;/code&gt;. The WASM version automatically selects the Sokol backend. I&#39;m not sure what needs to be done to make the SDL2 backend work with WASM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make wasm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This builds the minimal version (no music, no intro) as well as the full version.&lt;/p&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;This repository does not contain the assets (textures, 3d models etc.) required to run the game. This code mostly assumes to have the PSX NTSC data, but some menu models from the PC version are loaded as well. Both of these can be easily found on archive.org and similar sites. The music (optional) needs to be provided in &lt;a href=&#34;https://github.com/phoboslab/qoa&#34;&gt;QOA format&lt;/a&gt;. The intro video as MPEG1.&lt;/p&gt; &#xA;&lt;p&gt;The directory structure is assumed to be as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./wipegame # the executable&#xA;./wipeout/textures/&#xA;./wipeout/music/track01.qoa&#xA;./wipeout/music/track02.qoa&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that the blog post announcing this project may or may not provide a link to a ZIP containing all files needed. Who knows!&lt;/p&gt; &#xA;&lt;h2&gt;Ideas for improvements&lt;/h2&gt; &#xA;&lt;p&gt;PRs Welcome.&lt;/p&gt; &#xA;&lt;h3&gt;Not yet implemented&lt;/h3&gt; &#xA;&lt;p&gt;Some things from the original game are not yet implemented in this rewrite. This includes&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;screen shake effect&lt;/li&gt; &#xA; &lt;li&gt;game-end animations, formerly &lt;code&gt;Spline.cpp&lt;/code&gt; (the end messages are just shown over the attract mode cameras)&lt;/li&gt; &#xA; &lt;li&gt;viewing highscores in options menu&lt;/li&gt; &#xA; &lt;li&gt;controller options menu&lt;/li&gt; &#xA; &lt;li&gt;reverb for sfx and music when there&#39;s more than 4 track faces (tunnels and such)&lt;/li&gt; &#xA; &lt;li&gt;some more? grep the source for &lt;code&gt;TODO&lt;/code&gt; and &lt;code&gt;FIXME&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Gameplay, Visuals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;less punishing physics for ship vs. ship collisions&lt;/li&gt; &#xA; &lt;li&gt;less punishing physics for sideways ship vs. track collisions (i.e. wall grinding like in newer wipEouts)&lt;/li&gt; &#xA; &lt;li&gt;somehow resolve the issue of inevitably running into an enemy that you just shot&lt;/li&gt; &#xA; &lt;li&gt;add option to lessen the roll in the internal view&lt;/li&gt; &#xA; &lt;li&gt;add additional external view that behaves more like in modern racing games&lt;/li&gt; &#xA; &lt;li&gt;dynamic lighting on ships&lt;/li&gt; &#xA; &lt;li&gt;allow lower resolutions and a drawing mode that resembles the PSX original&lt;/li&gt; &#xA; &lt;li&gt;the scene geometry could use some touch-ups to make an infinite draw distance option less awkward&lt;/li&gt; &#xA; &lt;li&gt;increase FOV when going over a boost&lt;/li&gt; &#xA; &lt;li&gt;better menu models for game exit and video options&lt;/li&gt; &#xA; &lt;li&gt;gamepad analog input feels like balancing an egg&lt;/li&gt; &#xA; &lt;li&gt;fix collision issues on junctions (also present in the original)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Technical&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;implement frustum culling for scene geometry, the track and ships. Currently everything within the fadeout radius is drawn.&lt;/li&gt; &#xA; &lt;li&gt;put all static geometry into a GPU-side buffer. Currently all triangles are constructed at draw time. Uploading geometry is complicated a bit by the fact that some scene animations and the ship&#39;s exhaust need to update geometry for each frame.&lt;/li&gt; &#xA; &lt;li&gt;the menu system is... not great. It&#39;s better than the 5000 lines of spaghetti that it was before, but the different layouts need a lot of &lt;code&gt;if&lt;/code&gt;s&lt;/li&gt; &#xA; &lt;li&gt;the save data is just dumping the whole struct on disk. A textual format would be preferable.&lt;/li&gt; &#xA; &lt;li&gt;since this whole thing is relying on some custom assembled assets anyway, maybe all SFX should be in QOA format too (like the music). Or switch everything to Vorbis.&lt;/li&gt; &#xA; &lt;li&gt;a lot of functions assume that there&#39;s just one player. This needs to be fixed for a potential splitscreen mode.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;There is none. This code may or may not be based on the source code of the PC (ATI-Rage) version that was leaked in 2022. If it were, it would probably violate copyright law, but it may also fall under fair use ¯\_(ツ)_/¯&lt;/p&gt; &#xA;&lt;p&gt;Working with this source code is probably fine, considering that this game was originally released 28 years ago (in 1995), that the current copyright holders historically didn&#39;t care about any wipEout related files or code being available on the net and that the game is currently not purchasable in any shape or form.&lt;/p&gt; &#xA;&lt;p&gt;In any case, you may NOT use this source code in a commercial release. A commercial release includes hosting it on a website that shows any forms of advertising.&lt;/p&gt; &#xA;&lt;p&gt;PS.: Hey Sony! If you&#39;re reading this, I would love to work on a proper, officially sanctioned remaster. Please get in touch &amp;lt;3&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Plachtaa/VITS-fast-fine-tuning</title>
    <updated>2023-08-14T01:22:12Z</updated>
    <id>tag:github.com,2023-08-14:/Plachtaa/VITS-fast-fine-tuning</id>
    <link href="https://github.com/Plachtaa/VITS-fast-fine-tuning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repo is a pipeline of VITS finetuning for fast speaker adaptation TTS, and many-to-many voice conversion&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/Plachtaa/VITS-fast-fine-tuning/raw/main/README_ZH.md&#34;&gt;中文文档请点击这里&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;VITS Fast Fine-tuning&lt;/h1&gt; &#xA;&lt;p&gt;This repo will guide you to add your own character voices, or even your own voice, into existing VITS TTS model to make it able to do the following tasks in less than 1 hour:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Many-to-many voice conversion between any characters you added &amp;amp; preset characters in the model.&lt;/li&gt; &#xA; &lt;li&gt;English, Japanese &amp;amp; Chinese Text-to-Speech synthesis with the characters you added &amp;amp; preset characters&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Welcome to play around with the base models!&lt;br&gt; Chinese &amp;amp; English &amp;amp; Japanese：&lt;a href=&#34;https://huggingface.co/spaces/Plachta/VITS-Umamusume-voice-synthesizer&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; Author: Me&lt;/p&gt; &#xA;&lt;p&gt;Chinese &amp;amp; Japanese：&lt;a href=&#34;https://huggingface.co/spaces/sayashi/vits-uma-genshin-honkai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; Author: &lt;a href=&#34;https://github.com/SayaSS&#34;&gt;SayaSS&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chinese only：(No running huggingface spaces) Author: &lt;a href=&#34;https://github.com/Wwwwhy230825&#34;&gt;Wwwwhy230825&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Currently Supported Tasks:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Clone character voice from 10+ short audios&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Clone character voice from long audio(s) &amp;gt;= 3 minutes (one audio should contain single speaker only)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Clone character voice from videos(s) &amp;gt;= 3 minutes (one video should contain single speaker only)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Clone character voice from BILIBILI video links (one video should contain single speaker only)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Currently Supported Characters for TTS &amp;amp; VC:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Any character you wish as long as you have their voices! (Note that voice conversion can only be conducted between any two speakers in the model)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/Plachtaa/VITS-fast-fine-tuning/raw/main/LOCAL.md&#34;&gt;LOCAL.md&lt;/a&gt; for local training guide.&lt;br&gt; Alternatively, you can perform fine-tuning on &lt;a href=&#34;https://colab.research.google.com/drive/1pn1xnFfdLK63gVXDwV4zCXfVeo8c-I-0?usp=sharing&#34;&gt;Google Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;How long does it take?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install dependencies (3 min)&lt;/li&gt; &#xA; &lt;li&gt;Choose pretrained model to start. The detailed differences between them are described in &lt;a href=&#34;https://colab.research.google.com/drive/1pn1xnFfdLK63gVXDwV4zCXfVeo8c-I-0?usp=sharing&#34;&gt;Colab Notebook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Upload the voice samples of the characters you wish to add，see &lt;a href=&#34;https://github.com/Plachtaa/VITS-fast-fine-tuning/raw/main/DATA_EN.MD&#34;&gt;DATA.MD&lt;/a&gt; for detailed uploading options.&lt;/li&gt; &#xA; &lt;li&gt;Start fine-tuning. Time taken varies from 20 minutes ~ 2 hours, depending on the number of voices you uploaded.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Inference or Usage (Currently support Windows only)&lt;/h2&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Remember to download your fine-tuned model!&lt;/li&gt; &#xA; &lt;li&gt;Download the latest release&lt;/li&gt; &#xA; &lt;li&gt;Put your model &amp;amp; config file into the folder &lt;code&gt;inference&lt;/code&gt;, which are named &lt;code&gt;G_latest.pth&lt;/code&gt; and &lt;code&gt;finetune_speaker.json&lt;/code&gt;, respectively.&lt;/li&gt; &#xA; &lt;li&gt;The file structure should be as follows:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;inference&#xA;├───inference.exe&#xA;├───...&#xA;├───finetune_speaker.json&#xA;└───G_latest.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;run &lt;code&gt;inference.exe&lt;/code&gt;, the browser should pop up automatically.&lt;/li&gt; &#xA; &lt;li&gt;Note: you must install &lt;code&gt;ffmpeg&lt;/code&gt; to enable voice conversion feature.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Use in MoeGoe&lt;/h2&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;Prepare downloaded model &amp;amp; config file, which are named &lt;code&gt;G_latest.pth&lt;/code&gt; and &lt;code&gt;moegoe_config.json&lt;/code&gt;, respectively.&lt;/li&gt; &#xA; &lt;li&gt;Follow &lt;a href=&#34;https://github.com/CjangCjengh/MoeGoe&#34;&gt;MoeGoe&lt;/a&gt; page instructions to install, configure path, and use.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Looking for help?&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, please feel free to open an &lt;a href=&#34;https://github.com/Plachtaa/VITS-fast-fine-tuning/issues/new&#34;&gt;issue&lt;/a&gt; or join our &lt;a href=&#34;https://discord.gg/TcrjDFvm5A&#34;&gt;Discord&lt;/a&gt; server.&lt;/p&gt;</summary>
  </entry>
</feed>