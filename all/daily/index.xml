<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-01-25T01:29:34Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>brycedrennan/imaginAIry</title>
    <updated>2023-01-25T01:29:34Z</updated>
    <id>tag:github.com,2023-01-25:/brycedrennan/imaginAIry</id>
    <link href="https://github.com/brycedrennan/imaginAIry" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI imagined images. Pythonic generation of stable diffusion images.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ImaginAIry 🤖🧠&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pepy.tech/project/imaginairy&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/imaginairy&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/imaginairy/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/imaginairy.svg?sanitize=true&#34; alt=&#34;image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/brycedrennan/imaginAIry/raw/master/LICENSE/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-green&#34; alt=&#34;image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ambv/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/brycedrennan/imaginAIry/actions/workflows/ci.yaml&#34;&gt;&lt;img src=&#34;https://github.com/brycedrennan/imaginAIry/actions/workflows/ci.yaml/badge.svg?sanitize=true&#34; alt=&#34;Python Checks&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;AI imagined images. Pythonic generation of stable diffusion images.&lt;/p&gt; &#xA;&lt;p&gt;&#34;just works&#34; on Linux and macOS(M1) (and maybe windows?).&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# on macOS, make sure rust is installed first&#xA;&amp;gt;&amp;gt; pip install imaginairy&#xA;&amp;gt;&amp;gt; imagine &#34;a scenic landscape&#34; &#34;a photo of a dog&#34; &#34;photo of a fruit bowl&#34; &#34;portrait photo of a freckled woman&#34;&#xA;# Stable Diffusion 2.1&#xA;&amp;gt;&amp;gt; imagine --model SD-2.1 &#34;a forest&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details closed&gt; &#xA; &lt;summary&gt;Console Output&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;🤖🧠 received 4 prompt(s) and will repeat them 1 times to create 4 images.&#xA;Loading model onto mps backend...&#xA;Generating 🖼  : &#34;a scenic landscape&#34; 512x512px seed:557988237 prompt-strength:7.5 steps:40 sampler-type:PLMS&#xA;    PLMS Sampler: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:29&amp;lt;00:00,  1.36it/s]&#xA;    🖼  saved to: ./outputs/000001_557988237_PLMS40_PS7.5_a_scenic_landscape.jpg&#xA;Generating 🖼  : &#34;a photo of a dog&#34; 512x512px seed:277230171 prompt-strength:7.5 steps:40 sampler-type:PLMS&#xA;    PLMS Sampler: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:28&amp;lt;00:00,  1.41it/s]&#xA;    🖼  saved to: ./outputs/000002_277230171_PLMS40_PS7.5_a_photo_of_a_dog.jpg&#xA;Generating 🖼  : &#34;photo of a fruit bowl&#34; 512x512px seed:639753980 prompt-strength:7.5 steps:40 sampler-type:PLMS&#xA;    PLMS Sampler: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:28&amp;lt;00:00,  1.40it/s]&#xA;    🖼  saved to: ./outputs/000003_639753980_PLMS40_PS7.5_photo_of_a_fruit_bowl.jpg&#xA;Generating 🖼  : &#34;portrait photo of a freckled woman&#34; 512x512px seed:500686645 prompt-strength:7.5 steps:40 sampler-type:PLMS&#xA;    PLMS Sampler: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40/40 [00:29&amp;lt;00:00,  1.37it/s]&#xA;    🖼  saved to: ./outputs/000004_500686645_PLMS40_PS7.5_portrait_photo_of_a_freckled_woman.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000019_786355545_PLMS50_PS7.5_a_scenic_landscape.jpg&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000032_337692011_PLMS40_PS7.5_a_photo_of_a_dog.jpg&#34; height=&#34;256&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000056_293284644_PLMS40_PS7.5_photo_of_a_bowl_of_fruit.jpg&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000078_260972468_PLMS40_PS7.5_portrait_photo_of_a_freckled_woman.jpg&#34; height=&#34;256&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;🎉 Edit Images with Instructions alone! &lt;a href=&#34;https://github.com/timothybrooks/instruct-pix2pix&#34;&gt;by InstructPix2Pix&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Just tell imaginairy how to edit the image and it will do it for you!&lt;br&gt; Use prompt strength to control how strong the edit is. For extra control you can combine with prompt-based masking.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt; aimg edit scenic_landscape.jpg &#34;make it winter&#34; --prompt-strength 20&#xA;&amp;gt;&amp;gt; aimg edit dog.jpg &#34;make the dog red&#34; --prompt-strength 5&#xA;&amp;gt;&amp;gt; aimg edit bowl_of_fruit.jpg &#34;replace the fruit with strawberries&#34;&#xA;&amp;gt;&amp;gt; aimg edit freckled_woman.jpg &#34;make her a cyborg&#34; --prompt-strength 13&#xA;&amp;gt;&amp;gt; aimg edit pearl_girl.jpg &#34;make her wear clown makup&#34;&#xA;&amp;gt;&amp;gt; aimg edit mona-lisa.jpg &#34;make it a color professional photo headshot&#34; --negative-prompt &#34;old, ugly&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/scenic_landscape_winter.jpg&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/dog_red.jpg&#34; height=&#34;256&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/bowl_of_fruit_strawberries.jpg&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/freckled_woman_cyborg.jpg&#34; height=&#34;256&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/girl_with_a_pearl_earring_clown_makeup.jpg&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mona-lisa-headshot-photo.jpg&#34; height=&#34;256&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;Want just quickly have some fun? Try &lt;code&gt;--surprise-me&lt;/code&gt; to apply some pre-defined edits.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt; aimg edit --gif --surprise-me pearl_girl.jpg&#xA;&amp;gt;&amp;gt; aimg edit --gif --surprise-me mona-lisa.jpg&#xA;&amp;gt;&amp;gt; aimg edit --gif --surprise-me luke.jpg&#xA;&amp;gt;&amp;gt; aimg edit --gif --surprise-me spock.jpg&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/girl_with_a_pearl_earring_suprise.gif&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mona-lisa-suprise.gif&#34; height=&#34;256&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/luke-suprise.gif&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/spock-suprise.gif&#34; height=&#34;256&#34;&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/gg-bridge-suprise.gif&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/shire-suprise.gif&#34; height=&#34;256&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Prompt Based Masking &lt;a href=&#34;https://github.com/timojl/clipseg&#34;&gt;by clipseg&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Specify advanced text based masks using boolean logic and strength modifiers. Mask syntax:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;mask descriptions must be lowercase&lt;/li&gt; &#xA; &lt;li&gt;keywords (&lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;OR&lt;/code&gt;, &lt;code&gt;NOT&lt;/code&gt;) must be uppercase&lt;/li&gt; &#xA; &lt;li&gt;parentheses are supported&lt;/li&gt; &#xA; &lt;li&gt;mask modifiers may be appended to any mask or group of masks. Example: &lt;code&gt;(dog OR cat){+5}&lt;/code&gt; means that we&#39;ll select any dog or cat and then expand the size of the mask area by 5 pixels. Valid mask modifiers: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;{+n}&lt;/code&gt; - expand mask by n pixels&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;{-n}&lt;/code&gt; - shrink mask by n pixels&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;{*n}&lt;/code&gt; - multiply mask strength. will expand mask to areas that weakly matched the mask description&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;{/n}&lt;/code&gt; - divide mask strength. will reduce mask to areas that most strongly matched the mask description. probably not useful&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;When writing strength modifiers keep in mind that pixel values are between 0 and 1.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt; imagine \&#xA;    --init-image pearl_earring.jpg \ &#xA;    --mask-prompt &#34;face AND NOT (bandana OR hair OR blue fabric){*6}&#34; \&#xA;    --mask-mode keep \&#xA;    --init-image-strength .2 \&#xA;    --fix-faces \&#xA;    &#34;a modern female president&#34; &#34;a female robot&#34; &#34;a female doctor&#34; &#34;a female firefighter&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mask_examples/pearl000.jpg&#34; height=&#34;200&#34;&gt;➡️ &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mask_examples/pearl_pres.png&#34; height=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mask_examples/pearl_robot.png&#34; height=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mask_examples/pearl_doctor.png&#34; height=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mask_examples/pearl_firefighter.png&#34; height=&#34;200&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt; imagine \&#xA;    --init-image fruit-bowl.jpg \&#xA;    --mask-prompt &#34;fruit OR fruit stem{*6}&#34; \&#xA;    --mask-mode replace \&#xA;    --mask-modify-original \&#xA;    --init-image-strength .1 \&#xA;    &#34;a bowl of kittens&#34; &#34;a bowl of gold coins&#34; &#34;a bowl of popcorn&#34; &#34;a bowl of spaghetti&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000056_293284644_PLMS40_PS7.5_photo_of_a_bowl_of_fruit.jpg&#34; height=&#34;200&#34;&gt;➡️ &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mask_examples/bowl004.jpg&#34; height=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mask_examples/bowl001.jpg&#34; height=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mask_examples/bowl002.jpg&#34; height=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/mask_examples/bowl003.jpg&#34; height=&#34;200&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Face Enhancement &lt;a href=&#34;https://github.com/sczhou/CodeFormer&#34;&gt;by CodeFormer&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt; imagine &#34;a couple smiling&#34; --steps 40 --seed 1 --fix-faces&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/brycedrennan/imaginAIry/raw/master/assets/000178_1_PLMS40_PS7.5_a_couple_smiling_nofix.png&#34; height=&#34;256&#34;&gt; ➡️ &lt;img src=&#34;https://github.com/brycedrennan/imaginAIry/raw/master/assets/000178_1_PLMS40_PS7.5_a_couple_smiling_fixed.png&#34; height=&#34;256&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Upscaling &lt;a href=&#34;https://github.com/xinntao/Real-ESRGAN&#34;&gt;by RealESRGAN&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt; imagine &#34;colorful smoke&#34; --steps 40 --upscale&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/brycedrennan/imaginAIry/raw/master/assets/000206_856637805_PLMS40_PS7.5_colorful_smoke.jpg&#34; height=&#34;128&#34;&gt; ➡️ &lt;img src=&#34;https://github.com/brycedrennan/imaginAIry/raw/master/assets/000206_856637805_PLMS40_PS7.5_colorful_smoke_upscaled.jpg&#34; height=&#34;256&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Tiled Images&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt; imagine  &#34;gold coins&#34; &#34;a lush forest&#34; &#34;piles of old books&#34; leaves --tile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000066_801493266_PLMS40_PS7.5_gold_coins.jpg&#34; height=&#34;128&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000066_801493266_PLMS40_PS7.5_gold_coins.jpg&#34; height=&#34;128&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000066_801493266_PLMS40_PS7.5_gold_coins.jpg&#34; height=&#34;128&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000118_597948545_PLMS40_PS7.5_a_lush_forest.jpg&#34; height=&#34;128&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000118_597948545_PLMS40_PS7.5_a_lush_forest.jpg&#34; height=&#34;128&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000118_597948545_PLMS40_PS7.5_a_lush_forest.jpg&#34; height=&#34;128&#34;&gt; &lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000075_961095192_PLMS40_PS7.5_piles_of_old_books.jpg&#34; height=&#34;128&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000075_961095192_PLMS40_PS7.5_piles_of_old_books.jpg&#34; height=&#34;128&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000075_961095192_PLMS40_PS7.5_piles_of_old_books.jpg&#34; height=&#34;128&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000040_527733581_PLMS40_PS7.5_leaves.jpg&#34; height=&#34;128&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000040_527733581_PLMS40_PS7.5_leaves.jpg&#34; height=&#34;128&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000040_527733581_PLMS40_PS7.5_leaves.jpg&#34; height=&#34;128&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;360 degree images&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;imagine --tile-x -w 1024 -h 512 &#34;360 degree equirectangular panorama photograph of the desert&#34;  --upscale&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/desert_360.jpg&#34; height=&#34;128&#34;&gt; &#xA;&lt;h3&gt;Image-to-Image&lt;/h3&gt; &#xA;&lt;p&gt;Use depth maps for amazing &#34;translations&#34; of existing images.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt; imagine --model SD-2.0-depth --init-image girl_with_a_pearl_earring_large.jpg --init-image-strength 0.05  &#34;professional headshot photo of a woman with a pearl earring&#34; -r 4 -w 1024 -h 1024 --steps 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/tests/data/girl_with_a_pearl_earring.jpg&#34; height=&#34;256&#34;&gt; ➡️ &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/pearl_depth_1.jpg&#34; height=&#34;512&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/pearl_depth_2.jpg&#34; height=&#34;512&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/pearl_depth_3.jpg&#34; height=&#34;512&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Outpainting&lt;/h3&gt; &#xA;&lt;p&gt;Given a starting image, one can generate it&#39;s &#34;surroundings&#34;.&lt;/p&gt; &#xA;&lt;p&gt;Example: &lt;code&gt;imagine --init-image pearl-earring.jpg --init-image-strength 0 --outpaint all250,up0,down600 &#34;woman standing&#34;&lt;/code&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/tests/data/girl_with_a_pearl_earring.jpg&#34; height=&#34;256&#34;&gt; ➡️ &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/tests/expected_output/test_outpainting_outpaint_.png&#34; height=&#34;256&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Prompt Expansion&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;{}&lt;/code&gt; to randomly pull values from lists. A list of values separated by &lt;code&gt;|&lt;/code&gt; and enclosed in &lt;code&gt;{ }&lt;/code&gt; will be randomly drawn from in a non-repeating fashion. Values that are surrounded by &lt;code&gt;_ _&lt;/code&gt; will pull from a phrase list of the same name. Folders containing .txt phraselist files may be specified via &lt;code&gt;--prompt_library_path&lt;/code&gt;. The option may be specified multiple times. Built-in categories:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  3d-term, adj-architecture, adj-beauty, adj-detailed, adj-emotion, adj-general, adj-horror, animal, art-movement, &#xA;  art-site, artist, artist-botanical, artist-surreal, aspect-ratio, bird, body-of-water, body-pose, camera-brand,&#xA;  camera-model, color, cosmic-galaxy, cosmic-nebula, cosmic-star, cosmic-term, dinosaur, eyecolor, f-stop, &#xA;  fantasy-creature, fantasy-setting, fish, flower, focal-length, food, fruit, games, gen-modifier, hair, hd,&#xA;  iso-stop, landscape-type, national-park, nationality, neg-weight, noun-beauty, noun-fantasy, noun-general, &#xA;  noun-horror, occupation, photo-term, pop-culture, pop-location, punk-style, quantity, rpg-item, scenario-desc, &#xA;  skin-color, spaceship, style, tree-species, trippy, world-heritage-site&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Examples:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;imagine &#34;a {lime|blue|silver|aqua} colored dog&#34; -r 4 --seed 0&lt;/code&gt; (note that it generates a dog of each color without repetition)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000184_0_plms40_PS7.5_a_silver_colored_dog_%5Bgenerated%5D.jpg&#34; height=&#34;200&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000186_0_plms40_PS7.5_a_aqua_colored_dog_%5Bgenerated%5D.jpg&#34; height=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000210_0_plms40_PS7.5_a_lime_colored_dog_%5Bgenerated%5D.jpg&#34; height=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/assets/000211_0_plms40_PS7.5_a_blue_colored_dog_%5Bgenerated%5D.jpg&#34; height=&#34;200&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;imagine &#34;a {_color_} dog&#34; -r 4 --seed 0&lt;/code&gt; will generate four, different colored dogs. The colors will be pulled from an included phraselist of colors.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;imagine &#34;a {_spaceship_|_fruit_|hot air balloon}. low-poly&#34; -r 4 --seed 0&lt;/code&gt; will generate images of spaceships or fruits or a hot air balloon&lt;/p&gt; &#xA;&lt;p&gt;Credit to &lt;a href=&#34;https://github.com/WASasquatch/noodle-soup-prompts/&#34;&gt;noodle-soup-prompts&lt;/a&gt; where most, but not all, of the wordlists originate.&lt;/p&gt; &#xA;&lt;h3&gt;Generate image captions (via &lt;a href=&#34;https://github.com/salesforce/BLIP&#34;&gt;BLIP&lt;/a&gt;)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt;&amp;gt; aimg describe assets/mask_examples/bowl001.jpg&#xA;a bowl full of gold bars sitting on a table&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It makes images from text descriptions! 🎉&lt;/li&gt; &#xA; &lt;li&gt;Generate images either in code or from command line.&lt;/li&gt; &#xA; &lt;li&gt;It just works. Proper requirements are installed. model weights are automatically downloaded. No huggingface account needed. (if you have the right hardware... and aren&#39;t on windows)&lt;/li&gt; &#xA; &lt;li&gt;No more distorted faces!&lt;/li&gt; &#xA; &lt;li&gt;Noisy logs are gone (which was surprisingly hard to accomplish)&lt;/li&gt; &#xA; &lt;li&gt;WeightedPrompts let you smash together separate prompts (cat-dog)&lt;/li&gt; &#xA; &lt;li&gt;Tile Mode creates tileable images&lt;/li&gt; &#xA; &lt;li&gt;Prompt metadata saved into image file metadata&lt;/li&gt; &#xA; &lt;li&gt;Edit images by describing the part you want edited (see example above)&lt;/li&gt; &#xA; &lt;li&gt;Have AI generate captions for images &lt;code&gt;aimg describe &amp;lt;filename-or-url&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Interactive prompt: just run &lt;code&gt;aimg&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;🎉 finetune your own image model. kind of like dreambooth. Read instructions on &lt;a href=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/docs/concept-training.md&#34;&gt;&#34;Concept Training&#34;&lt;/a&gt; page&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How To&lt;/h2&gt; &#xA;&lt;p&gt;For full command line instructions run &lt;code&gt;aimg --help&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from imaginairy import imagine, imagine_image_files, ImaginePrompt, WeightedPrompt, LazyLoadingImage&#xA;&#xA;url = &#34;https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Thomas_Cole_-_Architect%E2%80%99s_Dream_-_Google_Art_Project.jpg/540px-Thomas_Cole_-_Architect%E2%80%99s_Dream_-_Google_Art_Project.jpg&#34;&#xA;prompts = [&#xA;    ImaginePrompt(&#34;a scenic landscape&#34;, seed=1, upscale=True),&#xA;    ImaginePrompt(&#34;a bowl of fruit&#34;),&#xA;    ImaginePrompt([&#xA;        WeightedPrompt(&#34;cat&#34;, weight=1),&#xA;        WeightedPrompt(&#34;dog&#34;, weight=1),&#xA;    ]),&#xA;    ImaginePrompt(&#xA;        &#34;a spacious building&#34;, &#xA;        init_image=LazyLoadingImage(url=url)&#xA;    ),&#xA;    ImaginePrompt(&#xA;        &#34;a bowl of strawberries&#34;, &#xA;        init_image=LazyLoadingImage(filepath=&#34;mypath/to/bowl_of_fruit.jpg&#34;),&#xA;        mask_prompt=&#34;fruit OR stem{*2}&#34;,  # amplify the stem mask x2&#xA;        mask_mode=&#34;replace&#34;,&#xA;        mask_modify_original=True,&#xA;    ),&#xA;    ImaginePrompt(&#34;strawberries&#34;, tile_mode=True),&#xA;]&#xA;for result in imagine(prompts):&#xA;    # do something&#xA;    result.save(&#34;my_image.jpg&#34;)&#xA;&#xA;# or&#xA;&#xA;imagine_image_files(prompts, outdir=&#34;./my-art&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;~10 gb space for models to download&lt;/li&gt; &#xA; &lt;li&gt;A CUDA supported graphics card with &amp;gt;= 11gb VRAM (and CUDA installed) or an M1 processor.&lt;/li&gt; &#xA; &lt;li&gt;Python installed. Preferably Python 3.10. (not conda)&lt;/li&gt; &#xA; &lt;li&gt;For macOS &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;rust&lt;/a&gt; and setuptools-rust must be installed to compile the &lt;code&gt;tokenizer&lt;/code&gt; library. They can be installed via: &lt;code&gt;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh&lt;/code&gt; and &lt;code&gt;pip install setuptools-rust&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running in Docker&lt;/h2&gt; &#xA;&lt;p&gt;See example Dockerfile (works on machine where you can pass the gpu into the container)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build . -t imaginairy&#xA;# you really want to map the cache or you end up wasting a lot of time and space redownloading the model weights&#xA;docker run -it --gpus all -v $HOME/.cache/huggingface:/root/.cache/huggingface -v $HOME/.cache/torch:/root/.cache/torch -v `pwd`/outputs:/outputs imaginairy /bin/bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running on Google Colab&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rOvQNs0Cmn_yU1bKWjCOHzGVDgZkaTtO?usp=sharing&#34;&gt;Example Colab&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ChangeLog&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;8.1.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: image sizes can now be multiples of 8 instead of 64. Inputs will be silently rounded down.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;8.0.5&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: bypass huggingface cache retrieval bug&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;8.0.4&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: limit attention slice size on MacOS machines with 64gb (#175)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;8.0.3&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: use python 3.7 compatible lru_cache&lt;/li&gt; &#xA; &lt;li&gt;fix: use windows compatible filenames&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;8.0.2&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: hf_hub_download() got an unexpected keyword argument &#39;token&#39;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;8.0.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: spelling mistake of &#34;surprise&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;8.0.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: 🎉 edit images with instructions alone!&lt;/li&gt; &#xA; &lt;li&gt;feature: when editing an image add &lt;code&gt;--gif&lt;/code&gt; to create a comparision gif&lt;/li&gt; &#xA; &lt;li&gt;feature: &lt;code&gt;aimg edit --surprise-me --gif my-image.jpg&lt;/code&gt; for some fun pre-programmed edits&lt;/li&gt; &#xA; &lt;li&gt;feature: prune-ckpt command also removes the non-ema weights&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.6.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: default model config was broken&lt;/li&gt; &#xA; &lt;li&gt;feature: print version with &lt;code&gt;--version&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;feature: ability to load safetensors&lt;/li&gt; &#xA; &lt;li&gt;feature: 🎉 outpainting. Examples: &lt;code&gt;--outpaint up10,down300,left50,right50&lt;/code&gt; or &lt;code&gt;--outpaint all100&lt;/code&gt; or &lt;code&gt;--outpaint u100,d200,l300,r400&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.4.3&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: handle old pytorch lightning imports with a graceful failure (fixes #161)&lt;/li&gt; &#xA; &lt;li&gt;fix: handle failed image generations better (fixes #83)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.4.2&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: run face enhancement on GPU for 10x speedup&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.4.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: incorrect config files being used for non-1.0 models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.4.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: 🎉 finetune your own image model. kind of like dreambooth. Read instructions on &lt;a href=&#34;https://raw.githubusercontent.com/brycedrennan/imaginAIry/master/docs/concept-training.md&#34;&gt;&#34;Concept Training&#34;&lt;/a&gt; page&lt;/li&gt; &#xA; &lt;li&gt;feature: image prep command. crops to face or other interesting parts of photo&lt;/li&gt; &#xA; &lt;li&gt;fix: back-compat for hf_hub_download&lt;/li&gt; &#xA; &lt;li&gt;feature: add prune-ckpt command&lt;/li&gt; &#xA; &lt;li&gt;feature: allow specification of model config file&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.3.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: 🎉 depth-based image-to-image generations (and inpainting)&lt;/li&gt; &#xA; &lt;li&gt;fix: k_euler_a produces more consistent images per seed (randomization respects the seed again)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.2.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: 🎉 tile in a single dimension (&#34;x&#34; or &#34;y&#34;). This enables, with a bit of luck, generation of 360 VR images. Try this for example: &lt;code&gt;imagine --tile-x -w 1024 -h 512 &#34;360 degree equirectangular panorama photograph of the mountains&#34; --upscale&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.1.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: memory/speed regression introduced in 6.1.0&lt;/li&gt; &#xA; &lt;li&gt;fix: model switching now clears memory better, thus avoiding out of memory errors&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.1.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: 🎉 Stable Diffusion 2.1. Generated people are no longer (completely) distorted. Use with &lt;code&gt;--model SD-2.1&lt;/code&gt; or &lt;code&gt;--model SD-2.0-v&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;7.0.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: negative prompting. &lt;code&gt;--negative-prompt&lt;/code&gt; or &lt;code&gt;ImaginePrompt(..., negative_prompt=&#34;ugly, deformed, extra arms, etc&#34;)&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;feature: a default negative prompt is added to all generations. Images in SD-2.0 don&#39;t look bad anymore. Images in 1.5 look improved as well.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;6.1.2&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: add back in memory-efficient algorithms&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;6.1.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: xformers will be used if available (for faster generation)&lt;/li&gt; &#xA; &lt;li&gt;fix: version metadata was broken&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;6.1.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: use different default steps and image sizes depending on sampler and model selceted&lt;/li&gt; &#xA; &lt;li&gt;fix: #110 use proper version in image metadata&lt;/li&gt; &#xA; &lt;li&gt;refactor: samplers all have their own class that inherits from ImageSampler&lt;/li&gt; &#xA; &lt;li&gt;feature: 🎉🎉🎉 Stable Diffusion 2.0 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;--model SD-2.0&lt;/code&gt; to use (it makes worse images than 1.5 though...)&lt;/li&gt; &#xA;   &lt;li&gt;Tested on macOS and Linux&lt;/li&gt; &#xA;   &lt;li&gt;All samplers working for new 512x512 model&lt;/li&gt; &#xA;   &lt;li&gt;New inpainting model working&lt;/li&gt; &#xA;   &lt;li&gt;768x768 model working for all samplers except PLMS (&lt;code&gt;--model SD-2.0-v &lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;5.1.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: add progress image callback&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;5.0.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: support larger images on M1. Fixes #8&lt;/li&gt; &#xA; &lt;li&gt;fix: support CPU generation by disabling autocast on CPU. Fixes #81&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;5.0.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: 🎉 inpainting support using new inpainting model from RunwayML. It works really well! By default, the inpainting model will automatically be used for any image-masking task&lt;/li&gt; &#xA; &lt;li&gt;feature: 🎉 new default sampler makes image generation more than twice as fast&lt;/li&gt; &#xA; &lt;li&gt;feature: added &lt;code&gt;DPM++ 2S a&lt;/code&gt; and &lt;code&gt;DPM++ 2M&lt;/code&gt; samplers.&lt;/li&gt; &#xA; &lt;li&gt;feature: improve progress image logging&lt;/li&gt; &#xA; &lt;li&gt;fix: fix bug with &lt;code&gt;--show-work&lt;/code&gt;. fixes #84&lt;/li&gt; &#xA; &lt;li&gt;fix: add workaround for pytorch bug affecting macOS users using the new &lt;code&gt;DPM++ 2S a&lt;/code&gt; and &lt;code&gt;DPM++ 2M&lt;/code&gt; samplers.&lt;/li&gt; &#xA; &lt;li&gt;fix: add workaround for pytorch mps bug affecting &lt;code&gt;k_dpm_fast&lt;/code&gt; sampler. fixes #75&lt;/li&gt; &#xA; &lt;li&gt;fix: larger image sizes now work on macOS. fixes #8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;4.1.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: allow dynamic switching between models/weights &lt;code&gt;--model SD-1.5&lt;/code&gt; or &lt;code&gt;--model SD-1.4&lt;/code&gt; or &lt;code&gt;--model path/my-custom-weights.ckpt&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;feature: log total progress when generating images (image X out of Y)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;4.0.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: stable diffusion 1.5 (slightly improved image quality)&lt;/li&gt; &#xA; &lt;li&gt;feature: dilation and erosion of masks Previously the &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;-&lt;/code&gt; characters in a mask (example: &lt;code&gt;face{+0.1}&lt;/code&gt;) added to the grayscale value of any masked areas. This wasn&#39;t very useful. The new behavior is that the mask will expand or contract by the number of pixel specified. The technical terms for this are dilation and erosion. This allows much greater control over the masked area.&lt;/li&gt; &#xA; &lt;li&gt;feature: update k-diffusion samplers. add k_dpm_adaptive and k_dpm_fast&lt;/li&gt; &#xA; &lt;li&gt;feature: img2img/inpainting supported on all samplers&lt;/li&gt; &#xA; &lt;li&gt;refactor: consolidates img2img/txt2img code. consolidates schedules. consolidates masking&lt;/li&gt; &#xA; &lt;li&gt;ci: minor logging improvements&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;3.0.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: k-samplers were broken&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;3.0.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: improved safety filter&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.4.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🎉 feature: prompt expansion&lt;/li&gt; &#xA; &lt;li&gt;feature: make (blip) photo captions more descriptive&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.3.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: face fidelity default was broken&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.3.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: model weights file can be specified via &lt;code&gt;--model-weights-path&lt;/code&gt; argument at the command line&lt;/li&gt; &#xA; &lt;li&gt;fix: set face fidelity default back to old value&lt;/li&gt; &#xA; &lt;li&gt;fix: handle small images without throwing exception. credit to @NiclasEriksen&lt;/li&gt; &#xA; &lt;li&gt;docs: add setuptools-rust as dependency for macos&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.2.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: init image is fully ignored if init-image-strength = 0&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.2.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;feature: face enhancement fidelity is now configurable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.1.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/timojl/clipseg/issues/8#issuecomment-1259150865&#34;&gt;improved masking accuracy from clipseg&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.0.3&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix memory leak in face enhancer&lt;/li&gt; &#xA; &lt;li&gt;fix blurry inpainting&lt;/li&gt; &#xA; &lt;li&gt;fix for pillow compatibility&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;2.0.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;🎉 fix: inpainted areas correlate with surrounding image, even at 100% generation strength. Previously if the generation strength was high enough the generated image would be uncorrelated to the rest of the surrounding image. It created terrible looking images.&lt;/li&gt; &#xA; &lt;li&gt;🎉 feature: interactive prompt added. access by running &lt;code&gt;aimg&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;🎉 feature: Specify advanced text based masks using boolean logic and strength modifiers. Mask descriptions must be lowercase. Keywords uppercase. Valid symbols: &lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;OR&lt;/code&gt;, &lt;code&gt;NOT&lt;/code&gt;, &lt;code&gt;()&lt;/code&gt;, and mask strength modifier &lt;code&gt;{+0.1}&lt;/code&gt; where &lt;code&gt;+&lt;/code&gt; can be any of &lt;code&gt;+ - * /&lt;/code&gt;. Single character boolean operators also work (&lt;code&gt;|&lt;/code&gt;, &lt;code&gt;&amp;amp;&lt;/code&gt;, &lt;code&gt;!&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;🎉 feature: apply mask edits to original files with &lt;code&gt;mask_modify_original&lt;/code&gt; (on by default)&lt;/li&gt; &#xA; &lt;li&gt;feature: auto-rotate images if exif data specifies to do so&lt;/li&gt; &#xA; &lt;li&gt;fix: mask boundaries are more accurate&lt;/li&gt; &#xA; &lt;li&gt;fix: accept mask images in command line&lt;/li&gt; &#xA; &lt;li&gt;fix: img2img algorithm was wrong and wouldn&#39;t at values close to 0 or 1&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;1.6.2&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: another bfloat16 fix&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;1.6.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: make sure image tensors come to the CPU as float32 so there aren&#39;t compatibility issues with non-bfloat16 cpus&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;1.6.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: &lt;em&gt;maybe&lt;/em&gt; address #13 with &lt;code&gt;expected scalar type BFloat16 but found Float&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;at minimum one can specify &lt;code&gt;--precision full&lt;/code&gt; now and that will probably fix the issue&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;feature: tile mode can now be specified per-prompt&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;1.5.3&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;fix: missing config file for describe feature&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;1.5.1&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;img2img now supported with PLMS (instead of just DDIM)&lt;/li&gt; &#xA; &lt;li&gt;added image captioning feature &lt;code&gt;aimg describe dog.jpg&lt;/code&gt; =&amp;gt; &lt;code&gt;a brown dog sitting on grass&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;added new commandline tool &lt;code&gt;aimg&lt;/code&gt; for additional image manipulation functionality&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;1.4.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;support multiple additive targets for masking with &lt;code&gt;|&lt;/code&gt; symbol. Example: &#34;fruit|stem|fruit stem&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;1.3.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;added prompt based image editing. Example: &#34;fruit =&amp;gt; gold coins&#34;&lt;/li&gt; &#xA; &lt;li&gt;test coverage improved&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;1.2.0&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;allow urls as init-images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;previous&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;img2img actually does # of steps you specify&lt;/li&gt; &#xA; &lt;li&gt;performance optimizations&lt;/li&gt; &#xA; &lt;li&gt;numerous other changes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Not Supported&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;a GUI. this is a python library&lt;/li&gt; &#xA; &lt;li&gt;exploratory features that don&#39;t work well&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Performance Optimizations&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;✅ fp16&lt;/li&gt; &#xA;   &lt;li&gt;✅ &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/compare/main...Doggettx:stable-diffusion:autocast-improvements#&#34;&gt;Doggettx Sliced attention&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;✅ xformers support &lt;a href=&#34;https://www.photoroom.com/tech/stable-diffusion-100-percent-faster-with-memory-efficient-attention/&#34;&gt;https://www.photoroom.com/tech/stable-diffusion-100-percent-faster-with-memory-efficient-attention/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/neonsecret/stable-diffusion&#34;&gt;https://github.com/neonsecret/stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/stable-diffusion/pull/177&#34;&gt;https://github.com/CompVis/stable-diffusion/pull/177&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/diffusers/pull/532/files&#34;&gt;https://github.com/huggingface/diffusers/pull/532/files&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;https://github.com/HazyResearch/flash-attention&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Development Environment&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;✅ add tests&lt;/li&gt; &#xA;   &lt;li&gt;✅ set up ci (test/lint/format)&lt;/li&gt; &#xA;   &lt;li&gt;✅ unified pipeline (txt2img &amp;amp; img2img combined)&lt;/li&gt; &#xA;   &lt;li&gt;✅ setup parallel testing&lt;/li&gt; &#xA;   &lt;li&gt;add docs&lt;/li&gt; &#xA;   &lt;li&gt;remove yaml config&lt;/li&gt; &#xA;   &lt;li&gt;delete more unused code&lt;/li&gt; &#xA;   &lt;li&gt;faster latent logging &lt;a href=&#34;https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204/9&#34;&gt;https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204/9&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Interface improvements&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;✅ init-image at command line&lt;/li&gt; &#xA;   &lt;li&gt;✅ prompt expansion&lt;/li&gt; &#xA;   &lt;li&gt;✅ interactive cli&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Image Generation Features&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;✅ add k-diffusion sampling methods&lt;/li&gt; &#xA;   &lt;li&gt;✅ tiling&lt;/li&gt; &#xA;   &lt;li&gt;generation videos/gifs&lt;/li&gt; &#xA;   &lt;li&gt;Compositional Visual Generation &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch&#34;&gt;https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch/blob/main/notebooks/demo.ipynb#scrollTo=wt_j3uXZGFAS&#34;&gt;https://colab.research.google.com/github/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch/blob/main/notebooks/demo.ipynb#scrollTo=wt_j3uXZGFAS&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;✅ negative prompting &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;some syntax to allow it in a text string&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;🚫 images as actual prompts instead of just init images. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;not directly possible due to model architecture.&lt;/li&gt; &#xA;     &lt;li&gt;requires model fine-tuning since SD1.4 expects 77x768 text encoding input&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://twitter.com/Buntworthy/status/1566744186153484288&#34;&gt;https://twitter.com/Buntworthy/status/1566744186153484288&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/justinpinkney/stable-diffusion&#34;&gt;https://github.com/justinpinkney/stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/LambdaLabsML/lambda-diffusers&#34;&gt;https://github.com/LambdaLabsML/lambda-diffusers&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/MachineLearning/comments/x6k5bm/n_stable_diffusion_image_variations_released/&#34;&gt;https://www.reddit.com/r/MachineLearning/comments/x6k5bm/n_stable_diffusion_image_variations_released/&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Image Editing&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;outpainting &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/parlance-zz/g-diffuser-bot/search?q=noise&amp;amp;type=issues&#34;&gt;https://github.com/parlance-zz/g-diffuser-bot/search?q=noise&amp;amp;type=issues&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;lama cleaner&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;✅ inpainting &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/Jack000/glid-3-xl-stable&#34;&gt;https://github.com/Jack000/glid-3-xl-stable&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/andreas128/RePaint&#34;&gt;https://github.com/andreas128/RePaint&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;✅ img2img but keeps img stable&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/xboy90/a_better_way_of_doing_img2img_by_finding_the/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/xboy90/a_better_way_of_doing_img2img_by_finding_the/&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://gist.github.com/trygvebw/c71334dd127d537a15e9d59790f7f5e1&#34;&gt;https://gist.github.com/trygvebw/c71334dd127d537a15e9d59790f7f5e1&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/pesser/stable-diffusion/commit/bbb52981460707963e2a62160890d7ecbce00e79&#34;&gt;https://github.com/pesser/stable-diffusion/commit/bbb52981460707963e2a62160890d7ecbce00e79&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/SHI-Labs/FcF-Inpainting&#34;&gt;https://github.com/SHI-Labs/FcF-Inpainting&lt;/a&gt; &lt;a href=&#34;https://praeclarumjj3.github.io/fcf-inpainting/&#34;&gt;https://praeclarumjj3.github.io/fcf-inpainting/&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;✅ text based image masking &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;✅ ClipSeg - &lt;a href=&#34;https://github.com/timojl/clipseg&#34;&gt;https://github.com/timojl/clipseg&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/detectron2&#34;&gt;https://github.com/facebookresearch/detectron2&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Attention Control Methods &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/bloc97/CrossAttentionControl&#34;&gt;https://github.com/bloc97/CrossAttentionControl&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/ChenWu98/cycle-diffusion&#34;&gt;https://github.com/ChenWu98/cycle-diffusion&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Image Enhancement&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Photo Restoration - &lt;a href=&#34;https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life&#34;&gt;https://github.com/microsoft/Bringing-Old-Photos-Back-to-Life&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Upscaling &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;✅ realesrgan&lt;/li&gt; &#xA;     &lt;li&gt;ldm&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/lowfuel/progrock-stable&#34;&gt;https://github.com/lowfuel/progrock-stable&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;gobig&lt;/li&gt; &#xA;     &lt;li&gt;stable super-res? &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;todo: try with 1-0-0-0 mask at full image resolution (rencoding entire image+predicted image at every step)&lt;/li&gt; &#xA;       &lt;li&gt;todo: use a gaussian pyramid and only include the &#34;high-detail&#34; level of the pyramid into the next step&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/xkjjf9/upscale_to_huge_sizes_and_add_detail_with_sd/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/xkjjf9/upscale_to_huge_sizes_and_add_detail_with_sd/&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;✅ face enhancers &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;✅ gfpgan - &lt;a href=&#34;https://github.com/TencentARC/GFPGAN&#34;&gt;https://github.com/TencentARC/GFPGAN&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;✅ codeformer - &lt;a href=&#34;https://github.com/sczhou/CodeFormer&#34;&gt;https://github.com/sczhou/CodeFormer&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;✅ image describe feature - &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;✅ &lt;a href=&#34;https://github.com/salesforce/BLIP&#34;&gt;https://github.com/salesforce/BLIP&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;🚫 CLIP brute-force prompt reconstruction &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;The accuracy of this approach is too low for me to include it in imaginAIry&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://github.com/rmokady/CLIP_prefix_caption&#34;&gt;https://github.com/rmokady/CLIP_prefix_caption&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://github.com/pharmapsychotic/clip-interrogator&#34;&gt;https://github.com/pharmapsychotic/clip-interrogator&lt;/a&gt; (blip + clip)&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/KaiyangZhou/CoOp&#34;&gt;https://github.com/KaiyangZhou/CoOp&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;🚫 CPU support. While the code does actually work on some CPUs, the generation takes so long that I don&#39;t think it&#39;s worth the effort to support this feature&lt;/li&gt; &#xA;   &lt;li&gt;✅ img2img for plms&lt;/li&gt; &#xA;   &lt;li&gt;✅ img2img for kdiff functions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Other&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enhancement pipelines&lt;/li&gt; &#xA;   &lt;li&gt;text-to-3d &lt;a href=&#34;https://dreamfusionpaper.github.io/&#34;&gt;https://dreamfusionpaper.github.io/&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://shihmengli.github.io/3D-Photo-Inpainting/&#34;&gt;https://shihmengli.github.io/3D-Photo-Inpainting/&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/thygate/stable-diffusion-webui-depthmap-script/discussions/50&#34;&gt;https://github.com/thygate/stable-diffusion-webui-depthmap-script/discussions/50&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Depth estimation &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;what is SOTA for monocular depth estimation?&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://github.com/compphoto/BoostingMonocularDepth&#34;&gt;https://github.com/compphoto/BoostingMonocularDepth&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;make a video &lt;a href=&#34;https://github.com/lucidrains/make-a-video-pytorch&#34;&gt;https://github.com/lucidrains/make-a-video-pytorch&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;animations &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/francislabountyjr/stable-diffusion/raw/main/inferencing_notebook.ipynb&#34;&gt;https://github.com/francislabountyjr/stable-diffusion/blob/main/inferencing_notebook.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=E7aAFEhdngI&#34;&gt;https://www.youtube.com/watch?v=E7aAFEhdngI&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/pytti-tools/frame-interpolation&#34;&gt;https://github.com/pytti-tools/frame-interpolation&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;guided generation &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1dlgggNa5Mz8sEAGU0wFCHhGLFooW_pf1#scrollTo=UDeXQKbPTdZI&#34;&gt;https://colab.research.google.com/drive/1dlgggNa5Mz8sEAGU0wFCHhGLFooW_pf1#scrollTo=UDeXQKbPTdZI&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/aicrumb/doohickey/blob/main/Doohickey_Diffusion.ipynb#scrollTo=PytCwKXCmPid&#34;&gt;https://colab.research.google.com/github/aicrumb/doohickey/blob/main/Doohickey_Diffusion.ipynb#scrollTo=PytCwKXCmPid&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;https://github.com/mlfoundations/open_clip&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;https://github.com/openai/guided-diffusion&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;image variations &lt;a href=&#34;https://github.com/lstein/stable-diffusion/raw/main/VARIATIONS.md&#34;&gt;https://github.com/lstein/stable-diffusion/blob/main/VARIATIONS.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;textual inversion &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/xbwb5y/how_to_run_textual_inversion_locally_train_your/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/xbwb5y/how_to_run_textual_inversion_locally_train_your/&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb#scrollTo=50JuJUM8EG1h&#34;&gt;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb#scrollTo=50JuJUM8EG1h&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_textual_inversion_library_navigator.ipynb&#34;&gt;https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_textual_inversion_library_navigator.ipynb&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/Jack000/glid-3-xl-stable&#34;&gt;https://github.com/Jack000/glid-3-xl-stable&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;fix saturation at high CFG &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/xalo78/fixing_excessive_contrastsaturation_resulting/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/xalo78/fixing_excessive_contrastsaturation_resulting/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/xbrrgt/a_rundown_of_twenty_new_methodsoptions_added_to/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/xbrrgt/a_rundown_of_twenty_new_methodsoptions_added_to/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;✅ deploy to pypi&lt;/li&gt; &#xA;   &lt;li&gt;find similar images &lt;a href=&#34;https://knn5.laion.ai/?back=https%3A%2F%2Fknn5.laion.ai%2F&amp;amp;index=laion5B&amp;amp;useMclip=false&#34;&gt;https://knn5.laion.ai/?back=https%3A%2F%2Fknn5.laion.ai%2F&amp;amp;index=laion5B&amp;amp;useMclip=false&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/vicgalle/stable-diffusion-aesthetic-gradients&#34;&gt;https://github.com/vicgalle/stable-diffusion-aesthetic-gradients&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Training&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Finetuning &#34;dreambooth&#34; style&lt;/li&gt; &#xA;   &lt;li&gt;Textual Inversion&lt;/li&gt; &#xA;   &lt;li&gt;Performance Improvements&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion&#34;&gt;ColoassalAI&lt;/a&gt; - almost got it working but it&#39;s not easy enough to install to merit inclusion in imaginairy. We should check back in on this.&lt;/li&gt; &#xA;   &lt;li&gt;Xformers&lt;/li&gt; &#xA;   &lt;li&gt;Deepspeed&lt;/li&gt; &#xA;   &lt;li&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Notable Stable Diffusion Implementations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ahrm/UnstableFusion&#34;&gt;https://github.com/ahrm/UnstableFusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/blueturtleai/gimp-stable-diffusion&#34;&gt;https://github.com/blueturtleai/gimp-stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hafriedlander/stable-diffusion-grpcserver&#34;&gt;https://github.com/hafriedlander/stable-diffusion-grpcserver&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion&#34;&gt;https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity&#34;&gt;https://github.com/lkwq007/stablediffusion-infinity&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lstein/stable-diffusion&#34;&gt;https://github.com/lstein/stable-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/parlance-zz/g-diffuser-lib&#34;&gt;https://github.com/parlance-zz/g-diffuser-lib&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hafriedlander/idea2art&#34;&gt;https://github.com/hafriedlander/idea2art&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Online Stable Diffusion Services&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stablecog.com/&#34;&gt;https://stablecog.com/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Further Reading&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openart.ai/promptbook&#34;&gt;Prompt Engineering Handbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Differences between samplers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/xbeyw3/can_anyone_offer_a_little_guidance_on_the/&#34;&gt;https://www.reddit.com/r/StableDiffusion/comments/xbeyw3/can_anyone_offer_a_little_guidance_on_the/&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.reddit.com/r/bigsleep/comments/xb5cat/wiskkeys_lists_of_texttoimage_systems_and_related/&#34;&gt;https://www.reddit.com/r/bigsleep/comments/xb5cat/wiskkeys_lists_of_texttoimage_systems_and_related/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/annotated-diffusion&#34;&gt;https://huggingface.co/blog/annotated-diffusion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jessevig/bertviz&#34;&gt;https://github.com/jessevig/bertviz&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=5pIQFQZsNe8&#34;&gt;https://www.youtube.com/watch?v=5pIQFQZsNe8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/assets/78_annotated-diffusion/unet_architecture.jpg&#34;&gt;https://huggingface.co/blog/assets/78_annotated-diffusion/unet_architecture.jpg&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>cloudflare/wildebeest</title>
    <updated>2023-01-25T01:29:34Z</updated>
    <id>tag:github.com,2023-01-25:/cloudflare/wildebeest</id>
    <link href="https://github.com/cloudflare/wildebeest" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Wildebeest is an ActivityPub and Mastodon-compatible server&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Wildebeest&lt;/h1&gt; &#xA;&lt;p&gt;Wildebeest is an &lt;a href=&#34;https://www.w3.org/TR/activitypub/&#34;&gt;ActivityPub&lt;/a&gt; and &lt;a href=&#34;https://joinmastodon.org/&#34;&gt;Mastodon&lt;/a&gt;-compatible server whose goal is to allow anyone to operate their Fediverse server and identity on their domain without needing to keep infrastructure, with minimal setup and maintenance, and running in minutes.&lt;/p&gt; &#xA;&lt;p&gt;Wildebeest runs on top Cloudflare&#39;s &lt;a href=&#34;https://blog.cloudflare.com/welcome-to-the-supercloud-and-developer-week-2022/&#34;&gt;Supercloud&lt;/a&gt;, uses &lt;a href=&#34;https://workers.cloudflare.com/&#34;&gt;Workers&lt;/a&gt; and &lt;a href=&#34;https://pages.cloudflare.com/&#34;&gt;Pages&lt;/a&gt;, the &lt;a href=&#34;https://developers.cloudflare.com/d1/&#34;&gt;D1 database&lt;/a&gt; to store metadata and configurations, &lt;a href=&#34;https://www.cloudflare.com/en-gb/products/zero-trust/access/&#34;&gt;Zero Trust Access&lt;/a&gt; to handle authentication and &lt;a href=&#34;https://www.cloudflare.com/en-gb/products/cloudflare-images/&#34;&gt;Images&lt;/a&gt; for media handling.&lt;/p&gt; &#xA;&lt;p&gt;Currently, Wildebeest supports the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Authentication and automatic profile creation.&lt;/li&gt; &#xA; &lt;li&gt;Message signing &amp;amp; notification.&lt;/li&gt; &#xA; &lt;li&gt;Inbox and Outbox notes (text, mentions and images), follow, announce (reblog), accept (friend), like.&lt;/li&gt; &#xA; &lt;li&gt;Server to server federation.&lt;/li&gt; &#xA; &lt;li&gt;Web client for content exploration (read-only).&lt;/li&gt; &#xA; &lt;li&gt;Compatibility with &lt;a href=&#34;https://raw.githubusercontent.com/cloudflare/wildebeest/main/#supported-clients&#34;&gt;other Mastodon clients&lt;/a&gt; (Mobile iOS/Android and Web).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Cloudflare will continue to evolve this open-source project with additional features over time and listen to the community feedback to steer our priorities. Pull requests and issues are welcome too.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;Wildebeest is a full-stack app running on top of Cloudflare Pages using &lt;a href=&#34;https://developers.cloudflare.com/pages/platform/functions/&#34;&gt;Pages Functions&lt;/a&gt;. We are of course assuming that you have a Cloudflare account (click &lt;a href=&#34;https://dash.cloudflare.com/sign-up&#34;&gt;here&lt;/a&gt; if you don&#39;t) and have at least one &lt;a href=&#34;https://www.cloudflare.com/en-gb/learning/dns/glossary/dns-zone/&#34;&gt;zone&lt;/a&gt; using Cloudflare. If you don&#39;t have a zone, you can use &lt;a href=&#34;https://www.cloudflare.com/en-gb/products/registrar/&#34;&gt;Cloudflare Registrar&lt;/a&gt; to register a new domain or &lt;a href=&#34;https://developers.cloudflare.com/registrar/get-started/transfer-domain-to-cloudflare/&#34;&gt;transfer&lt;/a&gt; an existing one.&lt;/p&gt; &#xA;&lt;p&gt;Some features like data persistence, access controls, and media storage are handled by other Cloudflare products:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developers.cloudflare.com/d1/&#34;&gt;D1&lt;/a&gt; for the database.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://developers.cloudflare.com/workers/learning/how-kv-works/&#34;&gt;Workers KV&lt;/a&gt; for object caching.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cloudflare.com/en-gb/products/zero-trust/access/&#34;&gt;Zero Trust Access&lt;/a&gt; to handle user authentication and SSO on &lt;a href=&#34;https://developers.cloudflare.com/cloudflare-one/identity/idp-integration/&#34;&gt;any identity provider&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.cloudflare.com/en-gb/products/cloudflare-images/&#34;&gt;Images&lt;/a&gt; for media handling.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Most of our products offer a &lt;a href=&#34;https://www.cloudflare.com/en-gb/plans/&#34;&gt;generous free plan&lt;/a&gt; that allows our users to try them for personal or hobby projects that aren’t business-critical. However the &lt;strong&gt;&lt;em&gt;Images&lt;/em&gt;&lt;/strong&gt; one doesn&#39;t have a free tier, so for setting up your instance you need to activate one of the paid &lt;strong&gt;&lt;em&gt;Images&lt;/em&gt;&lt;/strong&gt; plans.&lt;/p&gt; &#xA;&lt;h3&gt;Images plan&lt;/h3&gt; &#xA;&lt;p&gt;To activate &lt;strong&gt;&lt;em&gt;Images&lt;/em&gt;&lt;/strong&gt;, please login into your account, select &lt;strong&gt;&lt;em&gt;Images&lt;/em&gt;&lt;/strong&gt; on the left menu, and then select the plan that best fits your needs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/fd07dede-a883-4372-b0cf-3afb6b2ab400/public&#34; alt=&#34;images subscription&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Zone and Account IDs&lt;/h3&gt; &#xA;&lt;p&gt;You need to take note of your Zone and Account IDs. To find them, &lt;a href=&#34;https://dash.cloudflare.com/&#34;&gt;login&lt;/a&gt; into your account and select the zone (domain) where you plan to use Wildebeest. Then, on the &lt;strong&gt;&lt;em&gt;Overview&lt;/em&gt;&lt;/strong&gt; page you will enter the following information:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/f595d8b7-6ce9-4ef7-7416-253efd012800/w=306&#34; alt=&#34;zone and account IDs&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re all set now, let&#39;s start the installation process.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;p&gt;Wildebeest uses &lt;a href=&#34;https://deploy.workers.cloudflare.com/&#34;&gt;Deploy to Workers&lt;/a&gt; to automate the installation process.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Click here to start the installation.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://deploy.workers.cloudflare.com/?url=https://github.com/cloudflare/wildebeest&amp;amp;authed=true&amp;amp;fields=%7B%22name%22:%22Zone%20ID%22,%22secret%22:%22CF_ZONE_ID%22,%22descr%22:%22Get%20your%20Zone%20ID%20from%20the%20Cloudflare%20Dashboard%22%7D&amp;amp;fields=%7B%22name%22:%22Domain%22,%22secret%22:%22CF_DEPLOY_DOMAIN%22,%22descr%22:%22Domain%20on%20which%20your%20instance%20will%20be%20running%22%7D&amp;amp;fields=%7B%22name%22:%22Instance%20title%22,%22secret%22:%22INSTANCE_TITLE%22,%22descr%22:%22Title%20of%20your%20instance%22%7D&amp;amp;fields=%7B%22name%22:%22Administrator%20Email%22,%22secret%22:%22ADMIN_EMAIL%22,%22descr%22:%22An%20Email%20address%20that%20can%20be%20messaged%20regarding%20inquiries%20or%20issues%22%7D&amp;amp;fields=%7B%22name%22:%22Instance%20description%22,%22secret%22:%22INSTANCE_DESCR%22,%22descr%22:%22A%20short,%20plain-text%20description%20of%20your%20instance%22%7D&amp;amp;apiTokenTmpl=%5B%7B%22key%22:%22d1%22,%22type%22:%22edit%22%7D,%7B%22key%22:%22page%22,%22type%22:%22edit%22%7D,%7B%22key%22:%22images%22,%22type%22:%22edit%22%7D,%7B%22key%22:%22access%22,%22type%22:%22edit%22%7D,%7B%22key%22:%22workers_kv_storage%22,%22type%22:%22edit%22%7D,%7B%22key%22:%22access_acct%22,%22type%22:%22read%22%7D,%7B%22key%22:%22dns%22,%22type%22:%22edit%22%7D,%7B%22key%22:%22workers_script%22,%22type%22:%22edit%22%7D%5D&amp;amp;apiTokenName=Wildebeest&#34;&gt;&lt;img src=&#34;https://deploy.workers.cloudflare.com/button&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Please pay attention to all the steps involved in the installation process.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Authorize Workers to use your GitHub account.&lt;/li&gt; &#xA; &lt;li&gt;Enter your &lt;strong&gt;Account ID&lt;/strong&gt; (from the previous section) and the &lt;strong&gt;API token&lt;/strong&gt; that you created previously.&lt;/li&gt; &#xA; &lt;li&gt;Configure your instance/project with the &lt;strong&gt;Zone ID&lt;/strong&gt;, &lt;strong&gt;Domain&lt;/strong&gt;, &lt;strong&gt;Title&lt;/strong&gt;, &lt;strong&gt;Admin Email&lt;/strong&gt; and &lt;strong&gt;Description&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Fork the repository into your personal GitHub account.&lt;/li&gt; &#xA; &lt;li&gt;Enable GitHub Actions.&lt;/li&gt; &#xA; &lt;li&gt;Deploy.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Authorizations and API Token&lt;/h3&gt; &#xA;&lt;p&gt;The first two steps are authorizing Workers to use your GitHub account and entering your &lt;strong&gt;Account ID&lt;/strong&gt; and the &lt;strong&gt;API token&lt;/strong&gt; you created in the requirements section.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/00d9a77c-440f-46e5-b2bf-ccd198815800/public&#34; alt=&#34;deploy to workers&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Instance configuration&lt;/h3&gt; &#xA;&lt;p&gt;Configure your instance/project with the &lt;strong&gt;Zone ID&lt;/strong&gt; (see the requirements above), &lt;strong&gt;Domain&lt;/strong&gt; (the full FQDN domain of your zone, where you want to deploy your Wildebeest server), &lt;strong&gt;Title&lt;/strong&gt;, &lt;strong&gt;Admin Email&lt;/strong&gt; and &lt;strong&gt;Description&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/8aa836c5-a8e1-4ea5-d55c-a678aafe0b00/public&#34; alt=&#34;configure instance&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now click &lt;strong&gt;&lt;em&gt;Fork the repository&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Then enable GitHub Actions and confirm by clicking &lt;strong&gt;&lt;em&gt;Workflows enabled&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;And finally click &lt;strong&gt;&lt;em&gt;Deploy&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/be02ef19-b38a-4aef-7591-37dde5161200/public&#34; alt=&#34;deploy&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The installation script will now build and deploy your project to Cloudflare Pages and will run a &lt;a href=&#34;https://github.com/cloudflare/wildebeest/raw/main/tf/main.tf&#34;&gt;Terraform script&lt;/a&gt; to configure the D1, KV, DNS, Images and Access settings automatically for you.&lt;/p&gt; &#xA;&lt;h2&gt;Finish installation&lt;/h2&gt; &#xA;&lt;p&gt;If you followed all the steps, you should see a successful GitHub Actions build.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/2f00e3e4-aace-46f9-f0f4-eaeceb691a00/w=915&#34; alt=&#34;github actions secrets&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also confirm in the Cloudflare &lt;a href=&#34;https://dash.cloudflare.com&#34;&gt;dashboard&lt;/a&gt; that the Pages project, DNS entry, KV namespace, D1 database and Access rule were all created and configured.&lt;/p&gt; &#xA;&lt;p&gt;Almost there, only one last step missing:&lt;/p&gt; &#xA;&lt;h3&gt;Configure the access rule&lt;/h3&gt; &#xA;&lt;p&gt;The installation process automatically created a &lt;a href=&#34;https://developers.cloudflare.com/cloudflare-one/applications/&#34;&gt;Zero Trust Access application&lt;/a&gt; called &lt;code&gt;wildebeest-your-github-user&lt;/code&gt; for you. Now you need to create a &lt;a href=&#34;https://developers.cloudflare.com/cloudflare-one/policies/&#34;&gt;policy&lt;/a&gt; that defines who can have access to your Wildebeest instance.&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;a href=&#34;https://one.dash.cloudflare.com/access&#34;&gt;https://one.dash.cloudflare.com/access&lt;/a&gt; and select your account, then select &lt;strong&gt;&lt;em&gt;Access / Applications&lt;/em&gt;&lt;/strong&gt; and Edit the &lt;code&gt;wildebeest-your-github-user&lt;/code&gt; application.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/c93d68e8-ddfc-457d-bc63-cc50472e9e00/public&#34; alt=&#34;access applications&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Now click &lt;strong&gt;&lt;em&gt;Add a policy&lt;/em&gt;&lt;/strong&gt;. Name the policy &lt;code&gt;wildebeest-policy&lt;/code&gt;, set the action to &lt;strong&gt;&lt;em&gt;Allow&lt;/em&gt;&lt;/strong&gt;, and add an include rule with the list of Emails that you want to allow and then click &lt;strong&gt;&lt;em&gt;Save policy&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/f6b1238f-22c3-4daf-6102-7178fc91ca00/public&#34; alt=&#34;access policy&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;You&#39;re ready&lt;/h3&gt; &#xA;&lt;p&gt;Open your browser and go to your newly deployed Wildebeest domain &lt;code&gt;https://social.example/&lt;/code&gt; (replace social.example with your domain). You should see something like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/8ffd58d6-6b5b-46c0-af21-ec58a57f1600/public&#34; alt=&#34;ready&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Go to &lt;code&gt;https://social.example/api/v1/instance&lt;/code&gt; (replace social.example with your domain) and double-check your configuration. It should show:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;&#x9;&#34;description&#34;: &#34;Private Mastodon Server&#34;,&#xA;&#x9;&#34;email&#34;: &#34;admin@social.example&#34;,&#xA;&#x9;&#34;title&#34;: &#34;My Wildebeest Server&#34;,&#xA;&#x9;&#34;registrations&#34;: false,&#xA;&#x9;&#34;version&#34;: &#34;4.0.2&#34;,&#xA;&#x9;&#34;rules&#34;: [],&#xA;&#x9;&#34;uri&#34;: &#34;social.example&#34;,&#xA;&#x9;&#34;short_description&#34;: &#34;Private Mastodon Server&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;That&#39;s it, you&#39;re ready to start using your Wildebeest Mastodon compatible instance.&lt;/p&gt; &#xA;&lt;h2&gt;Supported clients&lt;/h2&gt; &#xA;&lt;p&gt;Wildebeest is Mastodon API compatible, which means that you should be able to use most of the Web, Desktop, and Mobile clients with it. However, this project is a work in progress, and nuances might affect some of their functionality.&lt;/p&gt; &#xA;&lt;p&gt;This is the list clients that we have been using successfully while developing and testing Wildebeest:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pinafore.social/&#34;&gt;Pinafore&lt;/a&gt; web client (&lt;a href=&#34;https://github.com/nolanlawson/pinafore&#34;&gt;source&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Mastodon &lt;a href=&#34;https://joinmastodon.org/apps&#34;&gt;official&lt;/a&gt; mobile client for &lt;a href=&#34;https://apps.apple.com/us/app/mastodon-for-iphone/id1571998974&#34;&gt;iOS&lt;/a&gt; (&lt;a href=&#34;https://github.com/mastodon/mastodon-ios&#34;&gt;source&lt;/a&gt;) and &lt;a href=&#34;https://play.google.com/store/apps/details?id=org.joinmastodon.android&#34;&gt;Android&lt;/a&gt; (&lt;a href=&#34;https://github.com/mastodon/mastodon-android&#34;&gt;source&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Wildebeest also provides a read-only web client in your instance URL, where you can explore the timelines (local and federated), posts and profiles. Please use the existing Mastodon clients to post and manage your account.&lt;/p&gt; &#xA;&lt;h3&gt;Wildebeest has no user registration&lt;/h3&gt; &#xA;&lt;p&gt;Wildebeest uses &lt;a href=&#34;https://www.cloudflare.com/en-gb/products/zero-trust/access/&#34;&gt;Zero Trust Access&lt;/a&gt; to handle user authentication. It assumes that your users will register with another identity provider (Zero Trust supports &lt;a href=&#34;https://developers.cloudflare.com/cloudflare-one/identity/idp-integration/&#34;&gt;many providers&lt;/a&gt; or your custom one that implements &lt;a href=&#34;https://developers.cloudflare.com/cloudflare-one/identity/idp-integration/generic-saml/&#34;&gt;Generic SAML 2.0&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;When you start using Wildebeest with a client, you don&#39;t need to register. Instead, you go straight to log in, which will redirect you to the Access page and handle the authentication step according to the policy that you defined earlier.&lt;/p&gt; &#xA;&lt;p&gt;When authenticated, Access will redirect you back to Wildebeest. The first time this happens, we will detect that we don&#39;t have information about the user and ask for your &lt;strong&gt;Username&lt;/strong&gt; and &lt;strong&gt;Display Name&lt;/strong&gt;. This will be asked only once and is what will show in your public Mastodon profile.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/4f5d27d0-3d30-49bd-b356-e33c194d7c00/w=450&#34; alt=&#34;first login&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updating Wildebeest&lt;/h2&gt; &#xA;&lt;p&gt;Updating your Wildebeest to the latest version is as easy as going to your forked repo on GitHub and clicking the &lt;strong&gt;&lt;em&gt;Sync fork&lt;/em&gt;&lt;/strong&gt; button:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/92ddc9f2-789b-454d-f6ca-2e9011613900/w=500&#34; alt=&#34;configuration screen&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once your fork is syncronized with the official repo, the GitHub Actions CI is triggered and a new build will be deployed.&lt;/p&gt; &#xA;&lt;h2&gt;Additional Cloudflare services&lt;/h2&gt; &#xA;&lt;p&gt;Since Wildebeest is a Cloudflare app running on Pages, you can seamlessly enable additional Cloudflare services to protect or improve your server.&lt;/p&gt; &#xA;&lt;h3&gt;Email Routing&lt;/h3&gt; &#xA;&lt;p&gt;If you want to receive Email at your @social.example domain, you can enable &lt;a href=&#34;https://developers.cloudflare.com/email-routing/get-started/enable-email-routing/&#34;&gt;Email Routing&lt;/a&gt; for free and take advantage of sophisticated Email forwarding and protection features. Simply log in to your account, select the Wildebeest zone and then click on Email to enable.&lt;/p&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Sometimes things go south. The GitHub Actions deployment can fail for some reason, or some configuration changed or was accidentally removed.&lt;/p&gt; &#xA;&lt;h3&gt;Starting over&lt;/h3&gt; &#xA;&lt;p&gt;If you attempted to deploy Wildebeest in your account and something failed, or you simply want to reinstall everything from scratch again, you need to do manual checkups and cleaning before you start over.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Go to your zone DNS settings and delete the CNAME record that points to &lt;code&gt;wildebeest-username.pages.dev&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go to your account Pages section and delete the &lt;code&gt;wildebeest-username&lt;/code&gt; project (make sure you remove the custom domain first if it&#39;s been configured).&lt;/li&gt; &#xA; &lt;li&gt;Go to your account Workers / KV section and delete the &lt;code&gt;wildebeest-username-cache&lt;/code&gt; and &lt;code&gt;wildebeest-terraform-username-state&lt;/code&gt; namespaces.&lt;/li&gt; &#xA; &lt;li&gt;Go to your account Workers / D1 and delete the &lt;code&gt;wildebeest-username&lt;/code&gt; database.&lt;/li&gt; &#xA; &lt;li&gt;Launch &lt;a href=&#34;https://one.dash.cloudflare.com/&#34;&gt;Zero Trust&lt;/a&gt;, select your account, go to Access / Applications and delete the &lt;code&gt;wildebeest-username&lt;/code&gt; application.&lt;/li&gt; &#xA; &lt;li&gt;Delete your GitHub wildebeest forked repo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can now start a clean install.&lt;/p&gt; &#xA;&lt;h3&gt;Error 1102&lt;/h3&gt; &#xA;&lt;p&gt;Wildebeest runs cryptographical functions and can process lots of data internally, depending on the size of the instance and social graph. It&#39;s possible that, in some cases, a request exceeds the Worker&#39;s resource limits in the free plan.&lt;/p&gt; &#xA;&lt;p&gt;We will keep optimizing our code to run as fast as possible, but if you start seeing 1102 errors when using your Wildebeest pages and APIs, you might need to upgrade to Workers Unbound, which provides much higher limits.&lt;/p&gt; &#xA;&lt;p&gt;To do that, go to your &lt;strong&gt;&lt;em&gt;Account Page&lt;/em&gt;&lt;/strong&gt; / &lt;strong&gt;&lt;em&gt;Pages&lt;/em&gt;&lt;/strong&gt;, select the &lt;code&gt;wildebeest-username&lt;/code&gt; project, go to &lt;strong&gt;&lt;em&gt;Settings&lt;/em&gt;&lt;/strong&gt; / &lt;strong&gt;&lt;em&gt;Functions&lt;/em&gt;&lt;/strong&gt; and change the usage model to &lt;strong&gt;Unbound&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://imagedelivery.net/NkfPDviynOyTAOI79ar_GQ/45de3429-d01a-4cfc-2ffc-819ac4f51900/public&#34; alt=&#34;unbound&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;After you change your Pages project to Unbound, you need to redeploy it. Go to GitHub Actions in your repo, select the latest successful deploy, and press &lt;strong&gt;Re-run all jobs&lt;/strong&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>shadcn/ui</title>
    <updated>2023-01-25T01:29:34Z</updated>
    <id>tag:github.com,2023-01-25:/shadcn/ui</id>
    <link href="https://github.com/shadcn/ui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Beautifully designed components built with Radix UI and Tailwind CSS.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;shadcn/ui&lt;/h1&gt; &#xA;&lt;p&gt;Beautifully designed components built with Radix UI and Tailwind CSS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shadcn/ui/main/apps/www/public/og.jpg&#34; alt=&#34;hero&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; This is work in progress. I&#39;m building this in public. You can follow the progress on Twitter &lt;a href=&#34;https://twitter.com/shadcn&#34;&gt;@shadcn&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Toast&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Toggle&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Toggle Group&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Toolbar&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Navigation Menu&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Figma?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Starting a new project? Check out the Next.js template.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx create-next-app -e https://github.com/shadcn/next-template&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Radix UI Primitives&lt;/li&gt; &#xA; &lt;li&gt;Tailwind CSS&lt;/li&gt; &#xA; &lt;li&gt;Fonts with &lt;code&gt;@next/font&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Icons from &lt;a href=&#34;https://lucide.dev&#34;&gt;Lucide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Dark mode with &lt;code&gt;next-themes&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Automatic import sorting with &lt;code&gt;@ianvs/prettier-plugin-sort-imports&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Tailwind CSS Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Class merging with &lt;code&gt;taiwind-merge&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Animation with &lt;code&gt;tailwindcss-animate&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Conditional classes with &lt;code&gt;clsx&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Variants with &lt;code&gt;class-variance-authority&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Automatic class sorting with &lt;code&gt;eslint-plugin-tailwindcss&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the &lt;a href=&#34;https://github.com/shadcn/ui/raw/main/LICENSE.md&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>