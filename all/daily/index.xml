<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-16T01:27:00Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>HazyResearch/ThunderKittens</title>
    <updated>2024-05-16T01:27:00Z</updated>
    <id>tag:github.com,2024-05-16:/HazyResearch/ThunderKittens</id>
    <link href="https://github.com/HazyResearch/ThunderKittens" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tile primitives for speedy kernels&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ThunderKittens&lt;/h1&gt; &#xA;&lt;h3&gt;Tile primitives for speedy kernels&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/HazyResearch/ThunderKittens/main/thunderkittens.png&#34; height=&#34;350&#34; alt=&#34;ThunderKittens logo&#34; style=&#34;margin-bottom:px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;ThunderKittens is a framework to make it easy to write fast deep learning kernels in CUDA (and, soon, ROCm and others, too!)&lt;/p&gt; &#xA;&lt;p&gt;ThunderKittens is built around three key principles:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Simplicity. ThunderKittens is stupidly simple to write.&lt;/li&gt; &#xA; &lt;li&gt;Extensibility. ThunderKittens embeds itself natively, so that if you need more than ThunderKittens can offer, it won’t get in your way of building it yourself.&lt;/li&gt; &#xA; &lt;li&gt;Speed. Kernels written in ThunderKittens should be at least as fast as those written from scratch -- especially because ThunderKittens can do things the “right” way under the hood. We think our Flash Attention 2 implementation speaks for this point.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/HazyResearch/ThunderKittens/main/attn.png&#34; height=&#34;600&#34; alt=&#34;Flash Attention 2, but with kittens!&#34; style=&#34;margin-bottom:px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;ThunderKittens is built from the hardware up -- we do what the silicon tells us. And modern GPUs tell us that they want to work with fairly small tiles of data. A GPU is not really a 1000x1000 matrix multiply machine (even if it is often used as such); it’s a manycore processor where each core can efficiently run ~16x16 matrix multiplies. Consequently, ThunderKittens is built around manipulating tiles of data no smaller than 16x16 values.&lt;/p&gt; &#xA;&lt;p&gt;ThunderKittens makes a few tricky things easy that enable high utilization on modern hardware.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Tensor cores. ThunderKittens can call fast tensor core functions, including asynchronous WGMMA calls on H100 GPUs.&lt;/li&gt; &#xA; &lt;li&gt;Shared Memory. I got ninety-nine problems but a bank conflict ain’t one.&lt;/li&gt; &#xA; &lt;li&gt;Loads and stores. Hide latencies with asynchronous copies and address generation with TMA.&lt;/li&gt; &#xA; &lt;li&gt;Distributed Shared Memory. L2 is &lt;em&gt;so&lt;/em&gt; last year.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;em&gt;Example: A Simple Atention Kernel&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here’s an example of what a simple FlashAttention-2 kernel for an RTX 4090 looks like written in ThunderKittens.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Cuda&#34;&gt;#define NUM_WORKERS 16 // This kernel uses 16 workers in parallel per block, to help issue instructions more quickly.&#xA;&#xA;using namespace kittens; // this kernel only handles headdim=64 for simplicity. Also n should be a multiple of 256 here.&#xA;__global__ void attend_ker64(int n, const bf16* __restrict__ __q__, const bf16* __restrict__ __k__, const bf16* __restrict__ __v__, bf16* __o__) {&#xA;&#xA;    auto warpid        = kittens::warpid();&#xA;    auto block_start   = blockIdx.x*(n*64);&#xA;    const bf16 *_q = __q__ + block_start, *_k = __k__ + block_start, *_v = __v__ + block_start;&#xA;          bf16 *_o = __o__ + block_start;&#xA;&#xA;    extern __shared__ alignment_dummy __shm[]; // this is the CUDA shared memory&#xA;    shared_allocator al((int*)&amp;amp;__shm[0]);&#xA;    &#xA;    // K and V live in shared memory -- this is about all that will fit.&#xA;    st_bf_1x4&amp;lt;ducks::st_layout::swizzle&amp;gt; (&amp;amp;k_smem)[NUM_WORKERS] = al.allocate&amp;lt;st_bf_1x4&amp;lt;ducks::st_layout::swizzle&amp;gt;, NUM_WORKERS&amp;gt;();&#xA;    st_bf_1x4&amp;lt;ducks::st_layout::swizzle&amp;gt; (&amp;amp;v_smem)[NUM_WORKERS] = al.allocate&amp;lt;st_bf_1x4&amp;lt;ducks::st_layout::swizzle&amp;gt;, NUM_WORKERS&amp;gt;();&#xA;&#xA;    // Initialize all of the register tiles.&#xA;    rt_bf_1x4&amp;lt;&amp;gt; q_reg, k_reg, v_reg; // v_reg need to be swapped into col_l&#xA;    rt_fl_1x1&amp;lt;&amp;gt; att_block;&#xA;    rt_bf_1x1&amp;lt;&amp;gt; att_block_mma;&#xA;    rt_fl_1x4&amp;lt;&amp;gt; o_reg;&#xA;    rt_fl_1x1&amp;lt;&amp;gt;::col_vec max_vec_last, max_vec; // these are column vectors for the attention block&#xA;    rt_fl_1x1&amp;lt;&amp;gt;::col_vec norm_vec_last, norm_vec; // these are column vectors for the attention block&#xA;    &#xA;    int qo_blocks = n / (q_reg.rows*NUM_WORKERS), kv_blocks = n / (q_reg.rows*NUM_WORKERS);&#xA;&#xA;    for(auto q_blk = 0; q_blk &amp;lt; qo_blocks; q_blk++) {&#xA;&#xA;        // each warp loads its own Q tile of 16x64, and then multiplies by 1/sqrt(d)&#xA;        load(q_reg, _q + (q_blk*NUM_WORKERS + warpid)*q_reg.num_elements, q_reg.cols);&#xA;        mul(q_reg, q_reg, __float2bfloat16(0.125f)); // temperature adjustment&#xA;&#xA;        // zero flash attention L, M, and O registers.&#xA;        neg_infty(max_vec); // zero registers for the Q chunk&#xA;        zero(norm_vec);&#xA;        zero(o_reg);&#xA;&#xA;        // iterate over k, v for these q&#39;s that have been loaded&#xA;        for(auto kv_idx = 0; kv_idx &amp;lt; kv_blocks; kv_idx++) {&#xA;&#xA;            // each warp loads its own chunk of k, v into shared memory&#xA;            load(v_smem[warpid], _v + (kv_idx*NUM_WORKERS + warpid)*q_reg.num_elements, q_reg.cols);&#xA;            load(k_smem[warpid], _k + (kv_idx*NUM_WORKERS + warpid)*q_reg.num_elements, q_reg.cols);&#xA;            __syncthreads(); // we need to make sure all memory is loaded before we can begin the compute phase&#xA;&#xA;            // now each warp goes through all of the subtiles, loads them, and then does the flash attention internal alg.&#xA;            for(int subtile = 0; subtile &amp;lt; NUM_WORKERS; subtile++) {&#xA;&#xA;                load(k_reg, k_smem[subtile]); // load k from shared into registers&#xA;&#xA;                zero(att_block); // zero 16x16 attention tile&#xA;                mma_ABt(att_block, q_reg, k_reg, att_block); // Q@K.T&#xA;&#xA;                copy(norm_vec_last, norm_vec);&#xA;                copy(max_vec_last,  max_vec);&#xA;&#xA;                row_max(max_vec, att_block, max_vec); // accumulate onto the max_vec&#xA;                sub_row(att_block, att_block, max_vec); // subtract max from attention -- now all &amp;lt;=0&#xA;                exp(att_block, att_block); // exponentiate the block in-place.&#xA;&#xA;                sub(max_vec_last, max_vec_last, max_vec); // subtract new max from old max to find the new normalization.&#xA;                exp(max_vec_last, max_vec_last); // exponentiate this vector -- this is what we need to normalize by.&#xA;                mul(norm_vec, norm_vec, max_vec_last); // and the norm vec is now normalized.&#xA;&#xA;                row_sum(norm_vec, att_block, norm_vec); // accumulate the new attention block onto the now-rescaled norm_vec&#xA;                div_row(att_block, att_block, norm_vec); // now the attention block is correctly normalized&#xA;&#xA;                mul(norm_vec_last, norm_vec_last, max_vec_last); // normalize the previous norm vec according to the new max&#xA;                div(norm_vec_last, norm_vec_last, norm_vec); // normalize the previous norm vec according to the new norm&#xA;&#xA;                copy(att_block_mma, att_block); // convert to bf16 for mma_AB&#xA;&#xA;                load(v_reg, v_smem[subtile]); // load v from shared into registers.&#xA;                rt_bf_1x4&amp;lt;ducks::rt_layout::col&amp;gt; &amp;amp;v_reg_col = swap_layout_inplace(v_reg); // this is a reference and the call has invalidated v_reg&#xA;&#xA;                mul_row(o_reg, o_reg, norm_vec_last); // normalize o_reg in advance of mma_AB&#39;ing onto it&#xA;                mma_AB(o_reg, att_block_mma, v_reg_col, o_reg); // mfma onto o_reg with the local attention@V matmul.&#xA;            }&#xA;            __syncthreads(); // we need to make sure all warps are done before we can start loading the next kv chunk&#xA;        }&#xA;&#xA;        store(_o + (q_blk*NUM_WORKERS + warpid)*q_reg.num_elements, o_reg, q_reg.cols); // write out o. compiler has an issue with register usage if d is made constexpr q_reg.rows :/&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Altogether, this is 58 lines of code (not counting whitespace), and achieves about 122 TFLOPs on an RTX 4090. (74% of theoretical max.) We’ll go through some of these primitives more carefully in the next section, the ThunderKittens manual.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To use Thunderkittens, there&#39;s not all that much you need to do with TK itself. It&#39;s a header only library, so just clone the repo, and include kittens.cuh. Easy money.&lt;/p&gt; &#xA;&lt;p&gt;But ThunderKittens does use a bunch of modern stuff, so it has fairly aggressive requirements.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA 12.3+. Anything after CUDA 12.1 will &lt;em&gt;probably&lt;/em&gt; work, but you&#39;ll likely end up with serialized wgmma pipelines due to a bug in those earlier versions of CUDA.&lt;/li&gt; &#xA; &lt;li&gt;(Extensive) C++20 use -- TK runs on concepts.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt update&#xA;sudo apt install gcc-10 g++-10&#xA;&#xA;sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 100 --slave /usr/bin/g++ g++ /usr/bin/g++-10&#xA;&#xA;sudo apt update&#xA;sudo apt install clang-10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you can&#39;t find nvcc, or you experience issues where your environment is pointing to the wrong CUDA version:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export CUDA_HOME=/usr/local/cuda-12/&#xA;export PATH=${CUDA_HOME}/bin:${PATH} &#xA;export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, thanks to Jordan Juravskey for putting together a quick doc on setting up a &lt;a href=&#34;https://github.com/HazyResearch/ThunderKittens/raw/main/docs/conda_setup.md&#34;&gt;kittens-compatible conda&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Tests&lt;/h2&gt; &#xA;&lt;p&gt;To validate your install, and run TK&#39;s fairly comprehensive unit testing suite, simply run &lt;code&gt;make -j&lt;/code&gt; in the tests folder. Be warned: this may nuke your computer for a minute or two while it compiles thousands of kernels.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;To compile examples, run &lt;code&gt;source env.src&lt;/code&gt; from the root directory before going into the examples directory. (Many of the examples use the &lt;code&gt;$THUNDERKITTENS_ROOT&lt;/code&gt; environment variable to orient themselves and find the src directory.&lt;/p&gt; &#xA;&lt;h2&gt;ThunderKittens Manual&lt;/h2&gt; &#xA;&lt;p&gt;ThunderKittens is actually a pretty small library, in terms of what it gives you.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Data types: (Register + shared) * (tiles + vectors), all parameterized by layout, type, and size.&lt;/li&gt; &#xA; &lt;li&gt;Operations for manipulating these objects.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Despite its simplicity, there are still a few sharp edges that you might encounter if you don’t know what’s going on under the hood. So, we do recommend giving this manual a good read before sitting down to write a kernel -- it’s not too long, we promise!&lt;/p&gt; &#xA;&lt;h3&gt;NVIDIA’s Programming Model&lt;/h3&gt; &#xA;&lt;p&gt;To understand ThunderKittens, it will help to begin by reviewing a bit of how NVIDIA’s programming model works, as NVIDIA provides a few different “scopes” to think about when writing parallel code.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Thread -- this is the level of doing work on an individual bit of data, like a floating point multiplication. A thread has up to 256 32-bit registers it can access every cycle.&lt;/li&gt; &#xA; &lt;li&gt;Warp -- 32 threads make a warp. This is the level at which instructions are issued by the hardware. It’s also the base (and default) scope from which ThunderKittens operates; most ThunderKittens programming happens here.&lt;/li&gt; &#xA; &lt;li&gt;Warpgroup -- 4 warps make a warpgroup. This is the level from which asynchronous warpgroup matrix multiply-accumulate instructions are issued. (We really wish we could ignore this level, but you unfortunately need it for the H100.) Correspondingly, many matrix multiply and memory operations are supported at the warpgroup level.&lt;/li&gt; &#xA; &lt;li&gt;Block -- N warps make a block, which is the level that shares “shared memory” in the CUDA programming model. In ThunderKittens, N is often 8.&lt;/li&gt; &#xA; &lt;li&gt;Grid -- M blocks make a grid, where M should be equal to (or slightly less) than a multiple of the number of SMs on the GPU to avoid tail effects. ThunderKittens does not touch the grid scope except through helping initialize TMA descriptors.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;“Register” objects exist at the level of warps -- their contents is split amongst the threads of the warp. Register objects include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Register tiles, declared as the &lt;code&gt;kittens::rt&lt;/code&gt; struct in &lt;code&gt;src/register_tile/rt.cuh&lt;/code&gt;. Kittens provides a few useful wrappers -- for example, a 32x16 row-layout bfloat16 register tile can be declared as &lt;code&gt;kittens::rt_bf_2x1;&lt;/code&gt; -- row-layout is implicit by default.&lt;/li&gt; &#xA; &lt;li&gt;Register vectors, which are associated with register tiles. They come in two flavors: column vectors and row vectors. Column vectors are used to reduce or map across tile rows, and row vectors reduce and map across tile columns. For example, to hold the sum of the rows of the tile declared above, we would create a &lt;code&gt;kittens::rt_bf_2x1&amp;lt;&amp;gt;::col_vec;&lt;/code&gt; In contrast, “Shared” objects exist at the level of the block, and sit only in shared memory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All ThunderKittens functions follow a common signature. Much like an assembly language (ThunderKittens is in essence an abstract tile-oriented RISC instruction set), the destination of every function is the first operand, and the source operands are passed sequentially afterwards.&lt;/p&gt; &#xA;&lt;p&gt;For example, if we have three 32x64 floating point register tiles: &lt;code&gt;kittens::rt_fl_2x4 a, b, c;&lt;/code&gt;, we can element-wise multiply &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; and store the result in &lt;code&gt;c&lt;/code&gt; with the following call: &lt;code&gt;kittens::mul(c, a, b);&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Similarly, if we want to then store the result into a shared tile &lt;code&gt;__shared__ kittens:st_bf_2x4 s;&lt;/code&gt;,we write the function analogously: &lt;code&gt;kittens::store(s, c);&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Typing&lt;/h3&gt; &#xA;&lt;p&gt;ThunderKittens tries hard to protect you from yourself. In particular, ThunderKittens wants to know layouts of objects at compile-time and will make sure they’re compatible before letting you do operations. This is important because there are subtleties to the allowable layouts for certain operations, and without static checks it is very easy to get painful silent failures. For example, a normal matrix multiply requires the B operand to be in a column layout, whereas an outer dot product requires the B operand to be in a row layout.&lt;/p&gt; &#xA;&lt;p&gt;If you are being told an operation that you think exists doesn&#39;t exist, double-check your layouts -- this is the most common error. Only then report a bug :)&lt;/p&gt; &#xA;&lt;h3&gt;Scopes&lt;/h3&gt; &#xA;&lt;p&gt;By default, ThunderKittens operations exist at the warp-level. In other words, each function expects to be called by only a single warp, and that single warp will do all of the work of the function. If multiple warps are assigned to the same work, undefined behavior will result. (And if the operation involves memory movement, it is likely to be completely catastrophic.) In general, you should expect your programming pattern to involve instantiating a &lt;code&gt;warpid&lt;/code&gt; at the beginning of the kernel with &lt;code&gt;kittens::warpid()&lt;/code&gt;, and assigning tasks to data based on that id.&lt;/p&gt; &#xA;&lt;p&gt;However, not all ThunderKittens functions operate at the warp level. Many important operations, particularly WGMMA instructions, require collaborative groups of warps. These operations exist in the templated &lt;code&gt;kittens::group&amp;lt;collaborative size&amp;gt;&lt;/code&gt;. For example, wgmma instructions are available through &lt;code&gt;kittens::group&amp;lt;4&amp;gt;::mma_AB&lt;/code&gt; (or &lt;code&gt;kittens::warpgroup::mma_AB&lt;/code&gt;, which is an alias.) Groups of warps can also collaboratively load shared memory or do reductions in shared memory&lt;/p&gt; &#xA;&lt;h3&gt;Other Restrictions&lt;/h3&gt; &#xA;&lt;p&gt;Most operations in ThunderKittens are pure functional. However, some operations &lt;em&gt;do&lt;/em&gt; have special restrictions; ThunderKittens tries to warn you by giving them names that stand out. For example, a register tile transpose needs separable arguments: if it is given the same underlying registers as both source and destination, it will silently fail. Consequently, it is named &lt;code&gt;transpose_sep&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>pipecat-ai/pipecat</title>
    <updated>2024-05-16T01:27:00Z</updated>
    <id>tag:github.com,2024-05-16:/pipecat-ai/pipecat</id>
    <link href="https://github.com/pipecat-ai/pipecat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Source framework for voice and multimodal conversational AI&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt;&#xA;  &amp;nbsp;&#xA; &lt;img alt=&#34;pipecat&#34; width=&#34;300px&#34; height=&#34;auto&#34; src=&#34;https://raw.githubusercontent.com/pipecat-ai/pipecat/main/pipecat.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Pipecat&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/pipecat-ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/pipecat-ai&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/pipecat&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1239284677165056021&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;pipecat&lt;/code&gt; is a framework for building voice (and multimodal) conversational agents. Things like personal coaches, meeting assistants, &lt;a href=&#34;https://storytelling-chatbot.fly.dev/&#34;&gt;story-telling toys for kids&lt;/a&gt;, customer support bots, &lt;a href=&#34;https://www.youtube.com/watch?v=lDevgsp9vn0&#34;&gt;intake flows&lt;/a&gt;, and snarky social companions.&lt;/p&gt; &#xA;&lt;p&gt;Take a look at some example apps:&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;a href=&#34;https://github.com/pipecat-ai/pipecat/tree/main/examples/simple-chatbot&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/simple-chatbot/image.png&#34; width=&#34;280&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://github.com/pipecat-ai/pipecat/tree/main/examples/storytelling-chatbot&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/storytelling-chatbot/image.png&#34; width=&#34;280&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://github.com/pipecat-ai/pipecat/tree/main/examples/translation-chatbot&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/translation-chatbot/image.png&#34; width=&#34;280&#34;&gt;&lt;/a&gt;&amp;nbsp; &lt;a href=&#34;https://github.com/pipecat-ai/pipecat/tree/main/examples/moondream-chatbot&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/pipecat-ai/pipecat/main/examples/moondream-chatbot/image.png&#34; width=&#34;280&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Getting started with voice agents&lt;/h2&gt; &#xA;&lt;p&gt;You can get started with Pipecat running on your local machine, then move your agent processes to the cloud when you’re ready. You can also add a 📞 telephone number, 🖼️ image output, 📺 video input, use different LLMs, and more.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# install the module&#xA;pip install pipecat-ai&#xA;&#xA;# set up an .env file with API keys&#xA;cp dot-env.template .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, in order to minimize dependencies, only the basic framework functionality is available. Some third-party AI services require additional dependencies that you can install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install &#34;pipecat-ai[option,...]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Your project may or may not need these, so they&#39;re made available as optional requirements. Here is a list:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI services&lt;/strong&gt;: &lt;code&gt;anthropic&lt;/code&gt;, &lt;code&gt;azure&lt;/code&gt;, &lt;code&gt;fal&lt;/code&gt;, &lt;code&gt;moondream&lt;/code&gt;, &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;playht&lt;/code&gt;, &lt;code&gt;silero&lt;/code&gt;, &lt;code&gt;whisper&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transports&lt;/strong&gt;: &lt;code&gt;local&lt;/code&gt;, &lt;code&gt;websocket&lt;/code&gt;, &lt;code&gt;daily&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Code examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pipecat-ai/pipecat/tree/main/examples/foundational&#34;&gt;foundational&lt;/a&gt; — small snippets that build on each other, introducing one or two concepts at a time&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pipecat-ai/pipecat/tree/main/examples/&#34;&gt;example apps&lt;/a&gt; — complete applications that you can use as starting points for development&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;A simple voice agent running locally&lt;/h2&gt; &#xA;&lt;p&gt;Here is a very basic Pipecat bot that greets a user when they join a real-time session. We&#39;ll use &lt;a href=&#34;https://daily.co&#34;&gt;Daily&lt;/a&gt; for real-time media transport, and &lt;a href=&#34;https://elevenlabs.io/&#34;&gt;ElevenLabs&lt;/a&gt; for text-to-speech.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#app.py&#xA;&#xA;import asyncio&#xA;import aiohttp&#xA;&#xA;from pipecat.frames.frames import EndFrame, TextFrame&#xA;from pipecat.pipeline.pipeline import Pipeline&#xA;from pipecat.pipeline.task import PipelineTask&#xA;from pipecat.pipeline.runner import PipelineRunner&#xA;from pipecat.services.elevenlabs import ElevenLabsTTSService&#xA;from pipecat.transports.services.daily import DailyParams, DailyTransport&#xA;&#xA;async def main():&#xA;  async with aiohttp.ClientSession() as session:&#xA;    # Use Daily as a real-time media transport (WebRTC)&#xA;    transport = DailyTransport(&#xA;      room_url=...,&#xA;      token=...,&#xA;      &#34;Bot Name&#34;,&#xA;      DailyParams(audio_out_enabled=True))&#xA;&#xA;    # Use Eleven Labs for Text-to-Speech&#xA;    tts = ElevenLabsTTSService(&#xA;      aiohttp_session=session,&#xA;      api_key=...,&#xA;      voice_id=...,&#xA;      )&#xA;&#xA;    # Simple pipeline that will process text to speech and output the result&#xA;    pipeline = Pipeline([tts, transport.output()])&#xA;&#xA;    # Create Pipecat processor that can run one or more pipelines tasks&#xA;    runner = PipelineRunner()&#xA;&#xA;    # Assign the task callable to run the pipeline&#xA;    task = PipelineTask(pipeline)&#xA;&#xA;    # Register an event handler to play audio when a&#xA;    # participant joins the transport WebRTC session&#xA;    @transport.event_handler(&#34;on_participant_joined&#34;)&#xA;    async def on_new_participant_joined(transport, participant):&#xA;      participant_name = participant[&#34;info&#34;][&#34;userName&#34;] or &#39;&#39;&#xA;      # Queue a TextFrame that will get spoken by the TTS service (Eleven Labs)&#xA;      await task.queue_frames([TextFrame(f&#34;Hello there, {participant_name}!&#34;), EndFrame()])&#xA;&#xA;    # Run the pipeline task&#xA;    await runner.run(task)&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;  asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Daily provides a prebuilt WebRTC user interface. Whilst the app is running, you can visit at &lt;code&gt;https://&amp;lt;yourdomain&amp;gt;.daily.co/&amp;lt;room_url&amp;gt;&lt;/code&gt; and listen to the bot say hello!&lt;/p&gt; &#xA;&lt;h2&gt;WebRTC for production use&lt;/h2&gt; &#xA;&lt;p&gt;WebSockets are fine for server-to-server communication or for initial development. But for production use, you’ll need client-server audio to use a protocol designed for real-time media transport. (For an explanation of the difference between WebSockets and WebRTC, see &lt;a href=&#34;https://www.daily.co/blog/how-to-talk-to-an-llm-with-your-voice/#webrtc&#34;&gt;this post.&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;One way to get up and running quickly with WebRTC is to sign up for a Daily developer account. Daily gives you SDKs and global infrastructure for audio (and video) routing. Every account gets 10,000 audio/video/transcription minutes free each month.&lt;/p&gt; &#xA;&lt;p&gt;Sign up &lt;a href=&#34;https://dashboard.daily.co/u/signup&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://docs.daily.co/reference/rest-api/rooms&#34;&gt;create a room&lt;/a&gt; in the developer Dashboard.&lt;/p&gt; &#xA;&lt;h2&gt;What is VAD?&lt;/h2&gt; &#xA;&lt;p&gt;Voice Activity Detection — very important for knowing when a user has finished speaking to your bot. If you are not using press-to-talk, and want Pipecat to detect when the user has finished talking, VAD is an essential component for a natural feeling conversation.&lt;/p&gt; &#xA;&lt;p&gt;Pipecast makes use of WebRTC VAD by default when using a WebRTC transport layer. Optionally, you can use Silero VAD for improved accuracy at the cost of higher CPU usage.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install pipecat-ai[silero]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The first time your run your bot with Silero, startup may take a while whilst it downloads and caches the model in the background. You can check the progress of this in the console.&lt;/p&gt; &#xA;&lt;h2&gt;Hacking on the framework itself&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Note that you may need to set up a virtual environment before following the instructions below. For instance, you might need to run the following from the root of the repo:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 -m venv venv&#xA;source venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From the root of this repo, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r dev-requirements.txt -r {env}-requirements.txt&#xA;python -m build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This builds the package. To use the package locally (eg to run sample files), run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install --editable .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to use this package from another directory, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install path_to_this_repo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Running tests&lt;/h3&gt; &#xA;&lt;p&gt;From the root directory, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pytest --doctest-modules --ignore-glob=&#34;*to_be_updated*&#34; src tests&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setting up your editor&lt;/h2&gt; &#xA;&lt;p&gt;This project uses strict &lt;a href=&#34;https://peps.python.org/pep-0008/&#34;&gt;PEP 8&lt;/a&gt; formatting.&lt;/p&gt; &#xA;&lt;h3&gt;Emacs&lt;/h3&gt; &#xA;&lt;p&gt;You can use &lt;a href=&#34;https://github.com/jwiegley/use-package&#34;&gt;use-package&lt;/a&gt; to install &lt;a href=&#34;https://codeberg.org/ideasman42/emacs-py-autopep8&#34;&gt;py-autopep8&lt;/a&gt; package and configure &lt;code&gt;autopep8&lt;/code&gt; arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-elisp&#34;&gt;(use-package py-autopep8&#xA;  :ensure t&#xA;  :defer t&#xA;  :hook ((python-mode . py-autopep8-mode))&#xA;  :config&#xA;  (setq py-autopep8-options &#39;(&#34;-a&#34; &#34;-a&#34;, &#34;--max-line-length=100&#34;)))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;autopep8&lt;/code&gt; was installed in the &lt;code&gt;venv&lt;/code&gt; environment described before, so you should be able to use &lt;a href=&#34;https://github.com/ryotaro612/pyvenv-auto&#34;&gt;pyvenv-auto&lt;/a&gt; to automatically load that environment inside Emacs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-elisp&#34;&gt;(use-package pyvenv-auto&#xA;  :ensure t&#xA;  :defer t&#xA;  :hook ((python-mode . pyvenv-auto-run)))&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visual Studio Code&lt;/h3&gt; &#xA;&lt;p&gt;Install the &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-python.autopep8&#34;&gt;autopep8&lt;/a&gt; extension. Then edit the user settings (&lt;em&gt;Ctrl-Shift-P&lt;/em&gt; &lt;code&gt;Open User Settings (JSON)&lt;/code&gt;) and set it as the default Python formatter, enable formatting on save and configure &lt;code&gt;autopep8&lt;/code&gt; arguments:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&#34;[python]&#34;: {&#xA;    &#34;editor.defaultFormatter&#34;: &#34;ms-python.autopep8&#34;,&#xA;    &#34;editor.formatOnSave&#34;: true&#xA;},&#xA;&#34;autopep8.args&#34;: [&#xA;    &#34;-a&#34;,&#xA;    &#34;-a&#34;,&#xA;    &#34;--max-line-length=100&#34;&#xA;],&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Getting help&lt;/h2&gt; &#xA;&lt;p&gt;➡️ &lt;a href=&#34;https://discord.gg/pipecat&#34;&gt;Join our Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;➡️ &lt;a href=&#34;https://x.com/pipecat_ai&#34;&gt;Reach us on X&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>taikoxyz/taiko-mono</title>
    <updated>2024-05-16T01:27:00Z</updated>
    <id>tag:github.com,2024-05-16:/taikoxyz/taiko-mono</id>
    <link href="https://github.com/taikoxyz/taiko-mono" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A based rollup. 🥁&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;./packages/branding/RGB SVG (For Digital Use)/Taiko Icon/taiko-icon-blk.svg&#34; width=&#34;80&#34; alt=&#34;Logo for Taiko&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; Taiko &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; A based rollup. &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://twitter.com/taikoxyz&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/taikoxyz?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/taikoxyz&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/984015101017346058?color=%235865F2&amp;amp;label=Discord&amp;amp;logo=discord&amp;amp;logoColor=%23fff&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/@taikoxyz&#34;&gt;&lt;img src=&#34;https://img.shields.io/youtube/channel/subscribers/UCxd_ARE9LtAEdnRQA6g1TaQ&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.gitpoap.io/gh/taikoxyz/taiko-mono&#34;&gt;&lt;img src=&#34;https://public-api.gitpoap.io/v1/repo/taikoxyz/taiko-mono/badge&#34; alt=&#34;GitPOAP Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/taikoxyz/taiko-mono/raw/main/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/taikoxyz/taiko-mono&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;End user documentation can be found at &lt;a href=&#34;https://docs.taiko.xyz&#34;&gt;docs.taiko.xyz&lt;/a&gt;. Protocol specs can be found &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/protocol/docs/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Each package of the monorepo is well documented and includes a README.&lt;/p&gt; &#xA;&lt;h2&gt;Project structure&lt;/h2&gt; &#xA;&lt;pre&gt;&#xA;taiko-mono/&#xA;├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/CHANGELOG.md&#34;&gt;CHANGELOG.md&lt;/a&gt;&#xA;├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;&#xA;├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt;&#xA;├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/README.md&#34;&gt;README.md&lt;/a&gt;&#xA;├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages&#34;&gt;packages&lt;/a&gt;&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/blobstorage&#34;&gt;blob-storage&lt;/a&gt;: Blob storage service.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/branding&#34;&gt;branding&lt;/a&gt;: Taiko branding materials.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/bridge-ui&#34;&gt;bridge-ui&lt;/a&gt;: Bridge UI.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/docs-site&#34;&gt;docs-site&lt;/a&gt;: End user documentation site.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/eventindexer&#34;&gt;eventindexer&lt;/a&gt;: Event indexer.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/fork-diff&#34;&gt;fork-diff&lt;/a&gt;: Fork diff page.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/guardian-prover-health-check&#34;&gt;guardian-prover-health-check&lt;/a&gt;: Guardian prover health check service.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/guardian-prover-health-check-ui&#34;&gt;guardian-prover-health-check-ui&lt;/a&gt;: Guardian prover health check UI.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/protocol&#34;&gt;protocol&lt;/a&gt;: Taiko protocol smart contracts.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/relayer&#34;&gt;relayer&lt;/a&gt;: Bridge backend relayer.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/supplementary-contracts&#34;&gt;supplementary-contracts&lt;/a&gt;: Supplementary smart contracts that are not part of the Taiko rollup protocol.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/taiko-client&#34;&gt;taiko-client&lt;/a&gt;: Taiko client implementation in Go.&#xA;│   ├── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/taikoon&#34;&gt;taikoon&lt;/a&gt;: Taikoon NFT contracts.&#xA;│   └── &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/packages/taikoon-ui&#34;&gt;taikoon-ui&lt;/a&gt;: Taikoon NFT UI.&#xA;...&#xA;&lt;/pre&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;If you find a bug or have a feature request, please &lt;a href=&#34;https://github.com/taikoxyz/taiko-mono/issues/new/choose&#34;&gt;open an issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] Please refrain from submitting typo/comment-only pull requests with the expectation of receiving TKO airdrops.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/taikoxyz/taiko-mono/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for details on how to contribute. You can also check out our grants cycle at &lt;a href=&#34;https://grants.taiko.xyz&#34;&gt;grants.taiko.xyz&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting support&lt;/h2&gt; &#xA;&lt;p&gt;Reach out to the community on &lt;a href=&#34;https://discord.gg/taikoxyz&#34;&gt;Discord&lt;/a&gt; if you need any help!&lt;/p&gt;</summary>
  </entry>
</feed>