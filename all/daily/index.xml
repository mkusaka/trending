<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-05T01:30:07Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>yihong0618/bilingual_book_maker</title>
    <updated>2023-03-05T01:30:07Z</updated>
    <id>tag:github.com,2023-03-05:/yihong0618/bilingual_book_maker</id>
    <link href="https://github.com/yihong0618/bilingual_book_maker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Make bilingual epub books Using AI translate&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/yihong0618/bilingual_book_maker/main/README-CN.md&#34;&gt;中文&lt;/a&gt; | English&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;bilingual_book_maker&lt;/h1&gt; &#xA;&lt;p&gt;Make bilingual epub books Using AI translate&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/15976103/222317531-a05317c5-4eee-49de-95cd-04063d9539d9.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Preparation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ChatGPT or OpenAI token&lt;/li&gt; &#xA; &lt;li&gt;prepared epub books&lt;/li&gt; &#xA; &lt;li&gt;Environment with internet access or proxy&lt;/li&gt; &#xA; &lt;li&gt;python3.8+&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Use&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;pip install -r requirements.txt&lt;/li&gt; &#xA; &lt;li&gt;OpenAI API key. If you have multiple keys, separate them by commas (xxx,xxx,xxx) to reduce errors caused by API call limits.&lt;/li&gt; &#xA; &lt;li&gt;A sample book, test_books/animal_farm.epub, is provided for testing purposes.&lt;/li&gt; &#xA; &lt;li&gt;A sample book, animal_farm.epub, is provided for testing purposes.&lt;/li&gt; &#xA; &lt;li&gt;Use --test command to preview the result if you haven&#39;t paid for the service. Note that there is a limit and it may take some time.&lt;/li&gt; &#xA; &lt;li&gt;Set the target language like &lt;code&gt;--language &#34;Simplified Chinese&#34;&lt;/code&gt;. Support &lt;code&gt; &#34;Japanese&#34; / &#34;Traditional Chinese&#34; / &#34;German&#34; / &#34;French&#34; / &#34;Korean&#34;&lt;/code&gt;. Default target language is &lt;code&gt;&#34;Simplified Chinese&#34;&lt;/code&gt;. Support language list please see the LANGUAGES at &lt;a href=&#34;https://raw.githubusercontent.com/yihong0618/bilingual_book_maker/main/utils.py&#34;&gt;utils.py&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the --proxy parameter to enable users in mainland China to use a proxy when testing locally. Enter a string such as &lt;a href=&#34;http://127.0.0.1:7890&#34;&gt;http://127.0.0.1:7890&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use the --resume command to manually resume the process after an interruption.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;e.g.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Test quickly&#xA;python3 make_book.py --book_name test_books/animal_farm.epub --openai_key ${openai_key} --no_limit --test --language &#34;Simplified Chinese&#34;&#xA;# or do it&#xA;python3 make_book.py --book_name test_books/animal_farm.epub --openai_key ${openai_key} --language &#34;Simplified Chinese&#34;&#xA;# or use the GPT-3 model&#xA;export OPENAI_API_KEY=${your_api_key}&#xA;python3 make_book.py --book_name test_books/animal_farm.epub --model gpt3 --no_limit --language &#34;Simplified Chinese&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;here is a limit. If you want to speed up the process, consider paying for the service or use multiple OpenAI tokens&lt;/li&gt; &#xA; &lt;li&gt;PR welcome&lt;/li&gt; &#xA; &lt;li&gt;The DeepL model will be updated later.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;@&lt;a href=&#34;https://github.com/yetone&#34;&gt;yetone&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Any issues or PRs are welcome.&lt;/li&gt; &#xA; &lt;li&gt;TODOs in the issue can also be selected.&lt;/li&gt; &#xA; &lt;li&gt;Please run black make_book.py before submitting the code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Appreciation&lt;/h2&gt; &#xA;&lt;p&gt;Thank you, that&#39;s enough.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/15976103/222407199-1ed8930c-13a8-402b-9993-aaac8ee84744.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ddiu8081/chatgpt-demo</title>
    <updated>2023-03-05T01:30:07Z</updated>
    <id>tag:github.com,2023-03-05:/ddiu8081/chatgpt-demo</id>
    <link href="https://github.com/ddiu8081/chatgpt-demo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A demo repo based on OpenAI API (gpt-3.5-turbo)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT-API Demo&lt;/h1&gt; &#xA;&lt;p&gt;A demo repo based on &lt;a href=&#34;https://platform.openai.com/docs/guides/chat&#34;&gt;OpenAI GPT-3.5 Turbo API&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Run Locally&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Setup &amp;amp; Install dependencies&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;First, you need &lt;a href=&#34;https://nodejs.org/&#34;&gt;Node.js&lt;/a&gt; installed.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm i&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make a copy of &lt;code&gt;.env.example&lt;/code&gt;, then rename it to &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Add your &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI API key&lt;/a&gt; to &lt;code&gt;.env&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=sk-xxx...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run the app&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Deploy With Vercel&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fddiu8081%2Fchatgpt-demo&amp;amp;env=OPENAI_API_KEY&amp;amp;envDescription=OpenAI%20API%20Key&amp;amp;envLink=https%3A%2F%2Fplatform.openai.com%2Faccount%2Fapi-keys&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>RUB-SysSec/DroneSecurity</title>
    <updated>2023-03-05T01:30:07Z</updated>
    <id>tag:github.com,2023-03-05:/RUB-SysSec/DroneSecurity</id>
    <link href="https://github.com/RUB-SysSec/DroneSecurity" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DroneSecurity (NDSS 2023)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Drone-ID Receiver for DJI OcuSync 2.0&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_f217_paper.pdf&#34;&gt;&lt;img alt=&#34;Paper thumbnail&#34; align=&#34;right&#34; width=&#34;250&#34; src=&#34;https://raw.githubusercontent.com/RUB-SysSec/DroneSecurity/public_squash/img/paper_thumbnail.png&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project is a receiver for DJI&#39;s Drone-ID protocol. The receiver works either live with an SDR, or offline on pre-recorded captures.&lt;/p&gt; &#xA;&lt;p&gt;Our paper from NDSS&#39;23 explains the protocol and receiver design: &lt;a href=&#34;https://www.ndss-symposium.org/wp-content/uploads/2023/02/ndss2023_f217_paper.pdf&#34;&gt;Drone Security and the Mysterious Case of DJI&#39;s DroneID&lt;/a&gt; [pdf]&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you&#39;re looking for the fuzzer, we will upload that shortly :)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The live receiver was tested with:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ettus USRP B205-mini&lt;/li&gt; &#xA; &lt;li&gt;DJI mini 2, DJI Mavic Air 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Our software is a proof-of-concept receiver that we used to reverse-engineer an unknown protocol. Hence, it is not optimized for bad RF conditions, performance, or range.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Decoded Payload&#34; width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/RUB-SysSec/DroneSecurity/public_squash/img/result.png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Sample Files&lt;/h2&gt; &#xA;&lt;p&gt;We provide sample files in the &lt;code&gt;samples/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;The samples were directly dumped from the first stage of the live receiver that &lt;em&gt;detects&lt;/em&gt; candidate frames and performs no other data processing; it usually hands them directly to the rest of the code that you can test offline.&lt;/p&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;inspectrum&lt;/code&gt; to visualize the raw sample file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install inspectrum&#xA;inspectrum -r 50e6 samples/mini2_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Inspectrum screenshot of Drone-ID bursts&#34; width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/RUB-SysSec/DroneSecurity/public_squash/img/inspectrum.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start (Offline)&lt;/h2&gt; &#xA;&lt;p&gt;Create a virtual environment for Python and install the requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m venv .venv&#xA;source .venv/bin/activate&#xA;pip3 install -r requirements.txt &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now run the decoder on the sample file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./src/droneid_receiver_offline.py -i samples/mini2_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Results&lt;/h3&gt; &#xA;&lt;p&gt;The script performs detection and decoding just as the live receiver would. It prints the decoded payload for each Drone-ID frame:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;## Drone-ID Payload ##&#xA;{&#xA;    &#34;pkt_len&#34;: 88,&#xA;    &#34;unk&#34;: 16,&#xA;    &#34;version&#34;: 2,&#xA;    &#34;sequence_number&#34;: 878,&#xA;    &#34;state_info&#34;: 8179,&#xA;    &#34;serial_number&#34;: &#34;SecureStorage?&#34;,&#xA;    &#34;longitude&#34;: 7.267960786785307,&#xA;    &#34;latitude&#34;: 51.446866781640146,&#xA;    &#34;altitude&#34;: 39.32,&#xA;    &#34;height&#34;: 5.49,&#xA;    &#34;v_north&#34;: 0,&#xA;    &#34;v_east&#34;: -7,&#xA;    &#34;v_up&#34;: 0,&#xA;    &#34;d_1_angle&#34;: 16900,&#xA;    &#34;gps_time&#34;: 1650894901980,&#xA;    &#34;app_lat&#34;: 43.26826445428658,&#xA;    &#34;app_lon&#34;: 6.640125363111847,&#xA;    &#34;longitude_home&#34;: 7.26794359805882,&#xA;    &#34;latitude_home&#34;: 51.446883970366635,&#xA;    &#34;device_type&#34;: &#34;Mini 2&#34;,&#xA;    &#34;uuid_len&#34;: 0,&#xA;    &#34;uuid&#34;: &#34;&#34;,&#xA;    &#34;crc-packet&#34;: &#34;c935&#34;,&#xA;    &#34;crc-calculated&#34;: &#34;c935&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The summary contains decoding stats and flight path. In the &lt;code&gt;mini2_sm&lt;/code&gt; sample, the drone did not have GPS coordinates locked yet, and only the smartphone&#39;s location is transmitted:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./src/droneid_receiver_offline.py -i samples/mini2_sm&#xA;… … …&#xA;Frame detection: 10 candidates&#xA;Decoder: 9 total, CRC OK: 7 (2 CRC errors)&#xA;Drone Coordinates:&#xA;App Coordinates:&#xA;(51.447176178716916, 7.266528392911369)&#xA;(51.447176178716916, 7.266528392911369)&#xA;…&#xA;(51.447176178716916, 7.266528392911369)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;code&gt;samples/mavic_air_2&lt;/code&gt; both locations are transmitted:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ./src/droneid_receiver_offline.py -i samples/mavic_air_2&#xA;…&#xA;Decoder: 1 total, CRC OK: 1 (0 CRC errors)&#xA;Drone Coordinates:&#xA;(51.44633393111904, 7.26721594197086, 12.8)&#xA;App Coordinates:&#xA;(51.44620788045814, 7.267101350460944)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Live Receiver&lt;/h1&gt; &#xA;&lt;p&gt;The live receiver additionally requires the UHD driver and &lt;strong&gt;quite powerful machines&lt;/strong&gt; (for captures at 50 MHz bandwidth).&lt;/p&gt; &#xA;&lt;p&gt;Environment:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ettus USRP B205-mini&lt;/li&gt; &#xA; &lt;li&gt;DJI mini 2, DJI Mavic Air 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;First, setup the Python environment. Due to the UHD driver, this does not work with a virtual environment. If you previously activated a virtual environment, exit that environment first. Install Python requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install UHD:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo apt install libuhd-dev uhd-host python3-uhd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the receiver:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./src/droneid_receiver_live.py &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The receiver will hop through a list of frequencies and, if a drone is detected, lock on that band.&lt;/p&gt; &#xA;&lt;h2&gt;Deeper Dive: Script output&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img alt=&#34;Processing Pipeline&#34; align=&#34;right&#34; width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/RUB-SysSec/DroneSecurity/public_squash/img/pipeline.png&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you&#39;re looking for a deeper dive into the processing steps, we suggest calling the offline decoder with &lt;code&gt;--debug&lt;/code&gt;. This will &lt;strong&gt;enable a GUI&lt;/strong&gt; with step-by-step decoding.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code&gt;./src/droneid_receiver_offline.py -i samples/mini2_sm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;First, the &lt;code&gt;SpectrumCapture&lt;/code&gt; class performs &lt;em&gt;packet detection&lt;/em&gt; and splits the capture file into individual frames:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Packet #0, start 0.000076, end 0.000721, length 0.000644, cfo -12207.031250&#xA;Packet #1, start 0.000811, end 0.001456, length 0.000644, cfo 0.000000&#xA;Packet #2, start 0.001546, end 0.002191, length 0.000644, cfo 0.000000&#xA;…&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some of these packets are false-positives and we do not expect successful decoding. Start and end are in seconds, so you can use inspectrum to take a look at individual frames.&lt;/p&gt; &#xA;&lt;p&gt;Next, the &lt;code&gt;Packet&lt;/code&gt; class detects the Zadoff-Chu sequences and performs time and frequency offset corrections. It splits the frames into individual OFDM symbols.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;FFO: -6546.528614&#xA;Found ZC sequences: 600 147&#xA;ZC Offset: -2.867868&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The &lt;code&gt;Decoder&lt;/code&gt; class gets the OFDM symbols and demodulates the subcarriers using QPSK. We do not know the QPSK orientation here, hence, we simply brute-force the orientation. &lt;code&gt;decoder.magic()&lt;/code&gt; performs the descrambling and turbo-decode.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;DroneIDPacket&lt;/code&gt; unpacks the resulting bitstream into the Drone-ID struct. At this point the message could be decoded, but might be corrupted (CRC check needed).&lt;/p&gt; &#xA;&lt;p&gt;CRC check FAIL is easy to spot by looking at the Serial Number (should read &#39;SecureStorage?&#39;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    &#34;serial_number&#34;: &#34;Sa#upeStore&amp;amp;q?\u0010\b&#34;,&#xA;    …&#xA;    &#34;crc-packet&#34;: &#34;d985&#34;,&#xA;    &#34;crc-calculated&#34;: &#34;9b01&#34;&#xA;}&#xA;CRC Check FAILED!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;At the very end, we print some statistics:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Successfully decoded 14 / 34 packets&#xA;4 Packets with CRC error&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;So in total we decoded 18 packets, 14 with correct CRC. Again, this is &lt;em&gt;expected&lt;/em&gt; as the sample file includes Drone-ID Frames with greatly varying quality.&lt;/p&gt; &#xA;&lt;h1&gt;FAQ - Frequently Asked Questions&lt;/h1&gt; &#xA;&lt;p&gt;Is DJI&#39;s Drone-ID the same as the standardized, Bluetooth or WiFi-based &#34;Remote ID&#34;?&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;No. DJI uses a dedicated wireless protocol for its Drone-ID, hence the need to implement an receiver. Check &lt;a href=&#34;https://www.opendroneid.org&#34;&gt;www.opendroneid.org&lt;/a&gt; for an in-depth description of the EU/US-wide standard.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Can I use &lt;em&gt;this software&lt;/em&gt; to locate drones from other manufacturers?&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;No. This software decodes DJI-specific protocols. It does not work with WiFi or Bluetooth-based &#34;Remote ID&#34;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Can I locate drones without this software?&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Maybe. Since mid-2022, the US or EU started requiring drone manufacturers to implement &#34;Drone Remote ID&#34; - an international standard that works on top of WiFi or Bluetooth. You can use a smartphone app to locate drones that support the standard.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;Citing the Paper&lt;/h1&gt; &#xA;&lt;p&gt;If you would like to cite our work, use the following BibTex entry:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{schiller2023drone,&#xA;  title={Drone Security and the Mysterious Case of DJI&#39;s DroneID},&#xA;  author={Schiller, Nico and Chlosta, Merlin and Schloegel, Moritz and Bars, Nils and Eisenhofer, Thorsten and Scharnowski, Tobias and Domke, Felix and Sch{\&#34;o}nherr, Lea and Holz, Thorsten},&#xA;  booktitle={Network and Distributed System Security Symposium (NDSS)},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>