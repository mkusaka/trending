<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-24T01:29:51Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>JiauZhang/DragGAN</title>
    <updated>2023-05-24T01:29:51Z</updated>
    <id>tag:github.com,2023-05-24:/JiauZhang/DragGAN</id>
    <link href="https://github.com/JiauZhang/DragGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of DragGAN: Interactive Point-based Manipulation on the Generative Image Manifold&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/JiauZhang/DragGAN/main/draggan.png&#34; width=&#34;750&#34; alt=&#34;Architecture of DragGAN&#34;&gt; &#xA;&lt;h1&gt;DragGAN&lt;/h1&gt; &#xA;&lt;p&gt;Implementation of &lt;a href=&#34;https://arxiv.org/abs/2305.10973&#34;&gt;DragGAN: Interactive Point-based Manipulation on the Generative Image Manifold&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# gui&#xA;pip install dearpygui&#xA;# run demo&#xA;python gui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/JiauZhang/DragGAN/main/UI.png&#34; width=&#34;600&#34; alt=&#34;Demo UI&#34;&gt; &#xA;&lt;h1&gt;StyleGAN2 Pre-Trained Model&lt;/h1&gt; &#xA;&lt;p&gt;Rosinality&#39;s pre-trained model(256px) on FFHQ 550k iterations [&lt;a href=&#34;https://drive.google.com/open?id=1PQutd-JboOCOZqmd95XWxWrO8gGEvRcO&#34;&gt;Link&lt;/a&gt;].&lt;/p&gt; &#xA;&lt;h1&gt;References&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;https://github.com/rosinality/stylegan2-pytorch&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Zeqiang-Lai/DragGAN</title>
    <updated>2023-05-24T01:29:51Z</updated>
    <id>tag:github.com,2023-05-24:/Zeqiang-Lai/DragGAN</id>
    <link href="https://github.com/Zeqiang-Lai/DragGAN" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unofficial implementation of &#34;Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DragGAN&lt;/h1&gt; &#xA;&lt;p&gt;&lt;span&gt;üí•&lt;/span&gt; &lt;a href=&#34;https://colab.research.google.com/github/Zeqiang-Lai/DragGAN/blob/master/colab.ipynb&#34;&gt;&lt;code&gt;Colab Demo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--  [`Online Demo`](https://6a05f355a8f139550c.gradio.live/)  --&gt; &#xA;&lt;!-- &gt; Note that the link of online demo will be updated regularly. --&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Note for Colab, remember to select a GPU via &lt;code&gt;Runtime/Change runtime type&lt;/code&gt; (&lt;code&gt;‰ª£Á†ÅÊâßË°åÁ®ãÂ∫è/Êõ¥ÊîπËøêË°åÊó∂Á±ªÂûã&lt;/code&gt;).&lt;/p&gt; &#xA; &lt;p&gt;Due to the limitation of GAN inversion, it is possible that your custom images are distorted. Besides, it is also possible the manipulations fail due to the limitation of our implementation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Unofficial implementation of &lt;a href=&#34;https://vcai.mpi-inf.mpg.de/projects/DragGAN/&#34;&gt;Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/paper.png&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;üåü&lt;/span&gt; &lt;strong&gt;Updates&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Tweak performance.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integrate into &lt;a href=&#34;https://github.com/OpenGVLab/InternGPT&#34;&gt;InternGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Automatically determining the number of iterations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Custom Image with GAN inversion.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Download generated image and generation trajectory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Controling generation process with GUI.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Automatically download stylegan2 checkpoint.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support movable region, mutliple handle points.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Gradio and Colab Demo.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;Results of our implementation.&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/mouse.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/nose.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/cat.gif&#34; width=&#34;200&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Zeqiang-Lai/DragGAN/main/assets/horse.gif&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Zeqiang-Lai/DragGAN/assets/26198430/f1516101-5667-4f73-9330-57fc45754283&#34;&gt;https://github.com/Zeqiang-Lai/DragGAN/assets/26198430/f1516101-5667-4f73-9330-57fc45754283&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Ensure you have a GPU and &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;, &lt;a href=&#34;https://gradio.app/quickstart/&#34;&gt;Gradio&lt;/a&gt; installed. You could install all the requirements via,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Lanuch the Gradio demo&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you have any issuse for downloading the checkpoint, you could manually download it from &lt;a href=&#34;https://huggingface.co/aaronb/StyleGAN2/tree/main&#34;&gt;here&lt;/a&gt; and put it into the folder &lt;code&gt;checkpoints&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/XingangPan/DragGAN&#34;&gt;Official DragGAN&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/NVlabs/stylegan2&#34;&gt;StyleGAN2&lt;/a&gt; ‚ÄÇ &lt;a href=&#34;https://github.com/rosinality/stylegan2-pytorch&#34;&gt;StyleGAN2-pytorch&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- https://github.com/omertov/encoder4editing --&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{pan2023draggan,&#xA;    title={Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold}, &#xA;    author={Pan, Xingang and Tewari, Ayush, and Leimk{\&#34;u}hler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},&#xA;    booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>iryna-kondr/scikit-llm</title>
    <updated>2023-05-24T01:29:51Z</updated>
    <id>tag:github.com,2023-05-24:/iryna-kondr/scikit-llm</id>
    <link href="https://github.com/iryna-kondr/scikit-llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Seamlessly integrate powerful language models like ChatGPT into scikit-learn for enhanced text analysis tasks.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/iryna-kondr/scikit-llm/raw/main/logo.png?raw=true&#34; max-height=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Scikit-LLM: Sklearn Meets Large Language Models&lt;/h1&gt; &#xA;&lt;p&gt;Seamlessly integrate powerful language models like ChatGPT into scikit-learn for enhanced text analysis tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Installation üíæ&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install scikit-llm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Support us ü§ù&lt;/h2&gt; &#xA;&lt;p&gt;You can support the project in the following ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚≠ê Star Scikit-LLM on GitHub (click the star button in the top right corner)&lt;/li&gt; &#xA; &lt;li&gt;üê¶ Check out our related project - &lt;a href=&#34;https://github.com/OKUA1/falcon&#34;&gt;Falcon AutoML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;üí° Provide your feedback or propose ideas in the &lt;a href=&#34;https://github.com/iryna-kondr/scikit-llm/issues&#34;&gt;issues&lt;/a&gt; section&lt;/li&gt; &#xA; &lt;li&gt;üîó Post about Scikit-LLM on LinkedIn or other platforms&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation üìö&lt;/h2&gt; &#xA;&lt;h3&gt;Configuring OpenAI API Key&lt;/h3&gt; &#xA;&lt;p&gt;At the moment Scikit-LLM is only compatible with some of the OpenAI models. Hence, a user-provided OpenAI API key is required.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from skllm.config import SKLLMConfig&#xA;SKLLMConfig.set_openai_key(&#34;&amp;lt;YOUR_KEY&amp;gt;&#34;)&#xA;SKLLMConfig.set_openai_org(&#34;&amp;lt;YOUR_ORGANISATION&amp;gt;&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Zero-Shot Text Classification&lt;/h3&gt; &#xA;&lt;p&gt;One of the powerful ChatGPT features is the ability to perform text classification without being re-trained. For that, the only requirement is that the labels must be descriptive.&lt;/p&gt; &#xA;&lt;p&gt;We provide a class &lt;code&gt;ZeroShotGPTClassifier&lt;/code&gt; that allows to create such a model as a regular scikit-learn classifier.&lt;/p&gt; &#xA;&lt;p&gt;Example 1: Training as a regular classifier&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from skllm import ZeroShotGPTClassifier&#xA;from skllm.datasets import get_classification_dataset&#xA;&#xA;# demo sentiment analysis dataset&#xA;# labels: positive, negative, neutral&#xA;X, y = get_classification_dataset() &#xA;&#xA;clf = ZeroShotGPTClassifier(openai_model = &#34;gpt-3.5-turbo&#34;)&#xA;clf.fit(X, y)&#xA;labels = clf.predict(X)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Scikit-LLM will automatically query the OpenAI API and transform the response into a regular list of labels.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, Scikit-LLM will ensure that the obtained response contains a valid label. If this is not the case, a label will be selected randomly (label probabilities are proportional to label occurrences in the training set).&lt;/p&gt; &#xA;&lt;p&gt;Example 2: Training without labeled data&lt;/p&gt; &#xA;&lt;p&gt;Since the training data is not strictly required, it can be fully ommited. The only thing that has to be provided is the list of candidate labels.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from skllm import ZeroShotGPTClassifier&#xA;from skllm.datasets import get_classification_dataset&#xA;&#xA;X, _ = get_classification_dataset()&#xA;&#xA;clf = ZeroShotGPTClassifier()&#xA;clf.fit(None, [&#39;positive&#39;, &#39;negative&#39;, &#39;neutral&#39;])&#xA;labels = clf.predict(X)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; unlike in a typical supervised setting, the performance of a zero-shot classifier greatly depends on how the label itself is structured. It has to be expressed in natural language, be descriptive and self-explanatory. For example, in the previous semantic classification task, it could be beneficial to transform a label from &lt;code&gt;&#34;&amp;lt;semantics&amp;gt;&#34;&lt;/code&gt; to &lt;code&gt;&#34;the semantics of the provided text is &amp;lt;semantics&amp;gt;&#34;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Multi-Label Zero-Shot Text Classification&lt;/h3&gt; &#xA;&lt;p&gt;With a class &lt;code&gt;MultiLabelZeroShotGPTClassifier&lt;/code&gt; it is possible to perform the classification in multi-label setting, which means that each sample might be assigned to one or several distinct classes.&lt;/p&gt; &#xA;&lt;p&gt;Example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from skllm import MultiLabelZeroShotGPTClassifier&#xA;from skllm.datasets import get_multilabel_classification_dataset&#xA;&#xA;X, y = get_multilabel_classification_dataset()&#xA;&#xA;clf = MultiLabelZeroShotGPTClassifier(max_labels=3)&#xA;clf.fit(X, y)&#xA;labels = clf.predict(X)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Similarly to the &lt;code&gt;ZeroShotGPTClassifier&lt;/code&gt; it is sufficient if only candidate labels are provided. However, this time the classifier expects &lt;code&gt;y&lt;/code&gt; of a type &lt;code&gt;List[List[str]]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from skllm import MultiLabelZeroShotGPTClassifier&#xA;from skllm.datasets import get_multilabel_classification_dataset&#xA;&#xA;X, _ = get_multilabel_classification_dataset()&#xA;candidate_labels = [&#xA;    &#34;Quality&#34;, &#xA;    &#34;Price&#34;, &#xA;    &#34;Delivery&#34;, &#xA;    &#34;Service&#34;, &#xA;    &#34;Product Variety&#34;, &#xA;    &#34;Customer Support&#34;, &#xA;    &#34;Packaging&#34;, &#xA;    &#34;User Experience&#34;, &#xA;    &#34;Return Policy&#34;, &#xA;    &#34;Product Information&#34;&#xA;]&#xA;clf = MultiLabelZeroShotGPTClassifier(max_labels=3)&#xA;clf.fit(None, [candidate_labels])&#xA;labels = clf.predict(X)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Text Vectorization&lt;/h3&gt; &#xA;&lt;p&gt;As an alternative to using GPT as a classifier, it can be used solely for data preprocessing. &lt;code&gt;GPTVectorizer&lt;/code&gt; allows to embed a chunk of text of arbitrary length to a fixed-dimensional vector, that can be used with virtually any classification or regression model.&lt;/p&gt; &#xA;&lt;p&gt;Example 1: Embedding the text&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from skllm.preprocessing import GPTVectorizer&#xA;&#xA;model = GPTVectorizer()&#xA;vectors = model.fit_transform(X)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Example 2: Combining the Vectorizer with the XGBoost Classifier in a Sklearn Pipeline&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sklearn.pipeline import Pipeline&#xA;from sklearn.preprocessing import LabelEncoder&#xA;from xgboost import XGBClassifier&#xA;&#xA;le = LabelEncoder()&#xA;y_train_encoded = le.fit_transform(y_train)&#xA;y_test_encoded = le.transform(y_test)&#xA;&#xA;steps = [(&#39;GPT&#39;, GPTVectorizer()), (&#39;Clf&#39;, XGBClassifier())]&#xA;clf = Pipeline(steps)&#xA;clf.fit(X_train, y_train_encoded)&#xA;yh = clf.predict(X_test)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Roadmap üß≠&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Zero-Shot Classification with OpenAI GPT 3/4 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multiclass classification&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Multi-label classification&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; ChatGPT models&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; InstructGPT models&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Few shot classifier&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; GPT Vectorizer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; GPT Fine-tuning (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integration of other LLMs&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>