<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-27T01:30:34Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>sadmann7/skateshop</title>
    <updated>2023-06-27T01:30:34Z</updated>
    <id>tag:github.com,2023-06-27:/sadmann7/skateshop</id>
    <link href="https://github.com/sadmann7/skateshop" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An open source e-commerce skateshop build with everything new in Next.js 13.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://skateshop13.vercel.app/&#34;&gt;Skateshop13&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;This is an open source e-commerce skateshop build with everything new in Next.js 13. It is bootstrapped with &lt;code&gt;create-t3-app&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://skateshop13.vercel.app/&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sadmann7/skateshop/main/public/screenshot.png&#34; alt=&#34;Skateshop13&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; This project is still in development and is not ready for production use.&lt;/p&gt; &#xA; &lt;p&gt;It uses new technologies (server actions, drizzle ORM) which are subject to change and may break your application.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Tech Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nextjs.org&#34;&gt;Next.js&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://clerk.dev&#34;&gt;Clerk Auth&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://orm.drizzle.team&#34;&gt;Drizzle ORM&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tailwindcss.com&#34;&gt;Tailwind CSS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://uploadthing.com&#34;&gt;uploadthing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://stripe.com&#34;&gt;Stripe&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Authentication with Clerk&lt;/li&gt; &#xA; &lt;li&gt;File uploads with uploadthing&lt;/li&gt; &#xA; &lt;li&gt;Subscription, payment, and billing with Stripe&lt;/li&gt; &#xA; &lt;li&gt;Storefront with products and categories&lt;/li&gt; &#xA; &lt;li&gt;Seller and customer workflows&lt;/li&gt; &#xA; &lt;li&gt;Admin dashboard with stores, products, orders, subscriptions, and payments&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;1. Clone the repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/sadmann7/skateshop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. Install dependencies&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Create a &lt;code&gt;.env&lt;/code&gt; file&lt;/h3&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add the environment variables as shown in the &lt;code&gt;.env.example&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;h3&gt;4. Run the application&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;5. Push database&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm run db:push&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;6. Listen for stripe events&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm run stripe:listen&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;How do I deploy this?&lt;/h2&gt; &#xA;&lt;p&gt;Follow the deployment guides for &lt;a href=&#34;https://create.t3.gg/en/deployment/vercel&#34;&gt;Vercel&lt;/a&gt;, &lt;a href=&#34;https://create.t3.gg/en/deployment/netlify&#34;&gt;Netlify&lt;/a&gt; and &lt;a href=&#34;https://create.t3.gg/en/deployment/docker&#34;&gt;Docker&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Stability-AI/stablediffusion</title>
    <updated>2023-06-27T01:30:34Z</updated>
    <id>tag:github.com,2023-06-27:/Stability-AI/stablediffusion</id>
    <link href="https://github.com/Stability-AI/stablediffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion Version 2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/txt2img/768/merged-0006.png&#34; alt=&#34;t2i&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/txt2img/768/merged-0002.png&#34; alt=&#34;t2i&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/txt2img/768/merged-0005.png&#34; alt=&#34;t2i&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; models trained from scratch and will be continuously updated with new checkpoints. The following list provides an overview of all currently available models. More coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;March 24, 2023&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Stable UnCLIP 2.1&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;New stable diffusion finetune (&lt;em&gt;Stable unCLIP 2.1&lt;/em&gt;, &lt;a href=&#34;https://huggingface.co/stabilityai/&#34;&gt;Hugging Face&lt;/a&gt;) at 768x768 resolution, based on SD2.1-768. This model allows for image variations and mixing operations as described in &lt;a href=&#34;https://arxiv.org/abs/2204.06125&#34;&gt;&lt;em&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/em&gt;&lt;/a&gt;, and, thanks to its modularity, can be combined with other models such as &lt;a href=&#34;https://github.com/kakaobrain/karlo&#34;&gt;KARLO&lt;/a&gt;. Comes in two variants: &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip/blob/main/sd21-unclip-l.ckpt&#34;&gt;&lt;em&gt;Stable unCLIP-L&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip/blob/main/sd21-unclip-h.ckpt&#34;&gt;&lt;em&gt;Stable unCLIP-H&lt;/em&gt;&lt;/a&gt;, which are conditioned on CLIP ViT-L and ViT-H image embeddings, respectively. Instructions are available &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/doc/UNCLIP.MD&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A public demo of SD-unCLIP is already available at &lt;a href=&#34;https://clipdrop.co/stable-diffusion-reimagine&#34;&gt;clipdrop.co/stable-diffusion-reimagine&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;December 7, 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Version 2.1&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New stable diffusion model (&lt;em&gt;Stable Diffusion 2.1-v&lt;/em&gt;, &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1&#34;&gt;Hugging Face&lt;/a&gt;) at 768x768 resolution and (&lt;em&gt;Stable Diffusion 2.1-base&lt;/em&gt;, &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1-base&#34;&gt;HuggingFace&lt;/a&gt;) at 512x512 resolution, both based on the same number of parameters and architecture as 2.0 and fine-tuned on 2.0, on a less restrictive NSFW filtering of the &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; dataset. Per default, the attention operation of the model is evaluated at full precision when &lt;code&gt;xformers&lt;/code&gt; is not installed. To enable fp16 (which can cause numerical instabilities with the vanilla attention module on the v2.1 model) , run your script with &lt;code&gt;ATTN_PRECISION=fp16 python &amp;lt;thescript.py&amp;gt;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;November 24, 2022&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Version 2.0&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;New stable diffusion model (&lt;em&gt;Stable Diffusion 2.0-v&lt;/em&gt;) at 768x768 resolution. Same number of parameters in the U-Net as 1.5, but uses &lt;a href=&#34;https://github.com/mlfoundations/open_clip&#34;&gt;OpenCLIP-ViT/H&lt;/a&gt; as the text encoder and is trained from scratch. &lt;em&gt;SD 2.0-v&lt;/em&gt; is a so-called &lt;a href=&#34;https://arxiv.org/abs/2202.00512&#34;&gt;v-prediction&lt;/a&gt; model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The above model is finetuned from &lt;em&gt;SD 2.0-base&lt;/em&gt;, which was trained as a standard noise-prediction model on 512x512 images and is also made available.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Added a &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/#image-upscaling-with-stable-diffusion&#34;&gt;x4 upscaling latent text-guided diffusion model&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;New &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/#depth-conditional-stable-diffusion&#34;&gt;depth-guided stable diffusion model&lt;/a&gt;, finetuned from &lt;em&gt;SD 2.0-base&lt;/em&gt;. The model is conditioned on monocular depth estimates inferred via &lt;a href=&#34;https://github.com/isl-org/MiDaS&#34;&gt;MiDaS&lt;/a&gt; and can be used for structure-preserving img2img and shape-conditional synthesis.&lt;/p&gt; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/depth2img/depth2img01.png&#34; alt=&#34;d2i&#34;&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/#image-inpainting-with-stable-diffusion&#34;&gt;text-guided inpainting model&lt;/a&gt;, finetuned from SD &lt;em&gt;2.0-base&lt;/em&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We follow the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;original repository&lt;/a&gt; and provide basic inference scripts to sample from the models.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;em&gt;The original Stable Diffusion model was created in a collaboration with &lt;a href=&#34;https://arxiv.org/abs/2202.00512&#34;&gt;CompVis&lt;/a&gt; and &lt;a href=&#34;https://runwayml.com/&#34;&gt;RunwayML&lt;/a&gt; and builds upon the work:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;&lt;strong&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/rromb&#34;&gt;Robin Rombach&lt;/a&gt;*, &lt;a href=&#34;https://github.com/ablattmann&#34;&gt;Andreas Blattmann&lt;/a&gt;*, &lt;a href=&#34;https://github.com/qp-qp&#34;&gt;Dominik Lorenz&lt;/a&gt;, &lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick Esser&lt;/a&gt;, &lt;a href=&#34;https://hci.iwr.uni-heidelberg.de/Staff/bommer&#34;&gt;Björn Ommer&lt;/a&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html&#34;&gt;CVPR &#39;22 Oral&lt;/a&gt; | &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;GitHub&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://ommer-lab.com/research/latent-diffusion-models/&#34;&gt;Project page&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;and &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/#shout-outs&#34;&gt;many others&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Stable Diffusion is a latent text-to-image diffusion model.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;You can update an existing &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;latent diffusion&lt;/a&gt; environment by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch==1.12.1 torchvision==0.13.1 -c pytorch&#xA;pip install transformers==4.19.2 diffusers invisible-watermark&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;xformers efficient attention&lt;/h4&gt; &#xA;&lt;p&gt;For more efficiency and speed on GPUs, we highly recommended installing the &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xformers&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;p&gt;Tested on A100 with CUDA 11.4. Installation needs a somewhat recent version of nvcc and gcc/g++, obtain those, e.g., via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;export CUDA_HOME=/usr/local/cuda-11.4&#xA;conda install -c nvidia/label/cuda-11.4.0 cuda-nvcc&#xA;conda install -c conda-forge gcc&#xA;conda install -c conda-forge gxx_linux-64==9.5.0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, run the following (compiling takes up to 30 min).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;cd ..&#xA;git clone https://github.com/facebookresearch/xformers.git&#xA;cd xformers&#xA;git submodule update --init --recursive&#xA;pip install -r requirements.txt&#xA;pip install -e .&#xA;cd ../stablediffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Upon successful installation, the code will automatically default to &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;memory efficient attention&lt;/a&gt; for the self- and cross-attention layers in the U-Net and autoencoder.&lt;/p&gt; &#xA;&lt;h2&gt;General Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion models are general text-to-image diffusion models and therefore mirror biases and (mis-)conceptions that are present in their training data. Although efforts were made to reduce the inclusion of explicit pornographic material, &lt;strong&gt;we do not recommend using the provided weights for services or products without additional safety mechanisms and considerations. The weights are research artifacts and should be treated as such.&lt;/strong&gt; Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2&#34;&gt;model card&lt;/a&gt;. The weights are available via &lt;a href=&#34;https://huggingface.co/StabilityAI&#34;&gt;the StabilityAI organization at Hugging Face&lt;/a&gt; under the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/LICENSE-MODEL&#34;&gt;CreativeML Open RAIL++-M License&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Stable Diffusion v2&lt;/h2&gt; &#xA;&lt;p&gt;Stable Diffusion v2 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 865M UNet and OpenCLIP ViT-H/14 text encoder for the diffusion model. The &lt;em&gt;SD 2-v&lt;/em&gt; model produces 768x768 px outputs.&lt;/p&gt; &#xA;&lt;p&gt;Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 DDIM sampling steps show the relative improvements of the checkpoints:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/model-variants.jpg&#34; alt=&#34;sd evaluation results&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Text-to-Image&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/txt2img/merged-0003.png&#34; alt=&#34;txt2img-stable2&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/txt2img/merged-0001.png&#34; alt=&#34;txt2img-stable2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stable Diffusion 2 is a latent diffusion model conditioned on the penultimate text embeddings of a CLIP ViT-H/14 text encoder. We provide a &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/#reference-sampling-script&#34;&gt;reference script for sampling&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Reference Sampling Script&lt;/h4&gt; &#xA;&lt;p&gt;This script incorporates an &lt;a href=&#34;https://github.com/ShieldMnt/invisible-watermark&#34;&gt;invisible watermarking&lt;/a&gt; of the outputs, to help viewers &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/scripts/tests/test_watermark.py&#34;&gt;identify the images as machine-generated&lt;/a&gt;. We provide the configs for the &lt;em&gt;SD2-v&lt;/em&gt; (768px) and &lt;em&gt;SD2-base&lt;/em&gt; (512px) model.&lt;/p&gt; &#xA;&lt;p&gt;First, download the weights for &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1&#34;&gt;&lt;em&gt;SD2.1-v&lt;/em&gt;&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1-base&#34;&gt;&lt;em&gt;SD2.1-base&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To sample from the &lt;em&gt;SD2.1-v&lt;/em&gt; model, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a professional photograph of an astronaut riding a horse&#34; --ckpt &amp;lt;path/to/768model.ckpt/&amp;gt; --config configs/stable-diffusion/v2-inference-v.yaml --H 768 --W 768  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or try out the Web Demo: &lt;a href=&#34;https://huggingface.co/spaces/stabilityai/stable-diffusion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To sample from the base model, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/txt2img.py --prompt &#34;a professional photograph of an astronaut riding a horse&#34; --ckpt &amp;lt;path/to/model.ckpt/&amp;gt; --config &amp;lt;path/to/config.yaml/&amp;gt;  &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this uses the &lt;a href=&#34;https://arxiv.org/abs/2010.02502&#34;&gt;DDIM sampler&lt;/a&gt;, and renders images of size 768x768 (which it was trained on) in 50 steps. Empirically, the v-models can be sampled with higher guidance scales.&lt;/p&gt; &#xA;&lt;p&gt;Note: The inference config for all model versions is designed to be used with EMA-only checkpoints. For this reason &lt;code&gt;use_ema=False&lt;/code&gt; is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights.&lt;/p&gt; &#xA;&lt;h4&gt;Enable Intel® Extension for PyTorch* optimizations in Text-to-Image script&lt;/h4&gt; &#xA;&lt;p&gt;If you&#39;re planning on running Text-to-Image on Intel® CPU, try to sample an image with TorchScript and Intel® Extension for PyTorch* optimizations. Intel® Extension for PyTorch* extends PyTorch by enabling up-to-date features optimizations for an extra performance boost on Intel® hardware. It can optimize memory layout of the operators to Channel Last memory format, which is generally beneficial for Intel CPUs, take advantage of the most advanced instruction set available on a machine, optimize operators and many more.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Before running the script, make sure you have all needed libraries installed. (the optimization was checked on &lt;code&gt;Ubuntu 20.04&lt;/code&gt;). Install &lt;a href=&#34;https://github.com/jemalloc/jemalloc&#34;&gt;jemalloc&lt;/a&gt;, &lt;a href=&#34;https://linux.die.net/man/8/numactl&#34;&gt;numactl&lt;/a&gt;, Intel® OpenMP and Intel® Extension for PyTorch*.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;apt-get install numactl libjemalloc-dev&#xA;pip install intel-openmp&#xA;pip install intel_extension_for_pytorch -f https://software.intel.com/ipex-whl-stable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To sample from the &lt;em&gt;SD2.1-v&lt;/em&gt; model with TorchScript+IPEX optimizations, run the following. Remember to specify desired number of instances you want to run the program on (&lt;a href=&#34;https://github.com/intel/intel-extension-for-pytorch/raw/master/intel_extension_for_pytorch/cpu/launch.py#L48&#34;&gt;more&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000 python -m intel_extension_for_pytorch.cpu.launch --ninstance &amp;lt;number of an instance&amp;gt; --enable_jemalloc scripts/txt2img.py --prompt \&#34;a corgi is playing guitar, oil on canvas\&#34; --ckpt &amp;lt;path/to/768model.ckpt/&amp;gt; --config configs/stable-diffusion/intel/v2-inference-v-fp32.yaml  --H 768 --W 768 --precision full --device cpu --torchscript --ipex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To sample from the base model with IPEX optimizations, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000 python -m intel_extension_for_pytorch.cpu.launch --ninstance &amp;lt;number of an instance&amp;gt; --enable_jemalloc scripts/txt2img.py --prompt \&#34;a corgi is playing guitar, oil on canvas\&#34; --ckpt &amp;lt;path/to/model.ckpt/&amp;gt; --config configs/stable-diffusion/intel/v2-inference-fp32.yaml  --n_samples 1 --n_iter 4 --precision full --device cpu --torchscript --ipex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you&#39;re using a CPU that supports &lt;code&gt;bfloat16&lt;/code&gt;, consider sample from the model with bfloat16 enabled for a performance boost, like so&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# SD2.1-v&#xA;MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000 python -m intel_extension_for_pytorch.cpu.launch --ninstance &amp;lt;number of an instance&amp;gt; --enable_jemalloc scripts/txt2img.py --prompt \&#34;a corgi is playing guitar, oil on canvas\&#34; --ckpt &amp;lt;path/to/768model.ckpt/&amp;gt; --config configs/stable-diffusion/intel/v2-inference-v-bf16.yaml --H 768 --W 768 --precision full --device cpu --torchscript --ipex --bf16&#xA;# SD2.1-base&#xA;MALLOC_CONF=oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000 python -m intel_extension_for_pytorch.cpu.launch --ninstance &amp;lt;number of an instance&amp;gt; --enable_jemalloc scripts/txt2img.py --prompt \&#34;a corgi is playing guitar, oil on canvas\&#34; --ckpt &amp;lt;path/to/model.ckpt/&amp;gt; --config configs/stable-diffusion/intel/v2-inference-bf16.yaml --precision full --device cpu --torchscript --ipex --bf16&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Image Modification with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/depth2img/merged-0000.png&#34; alt=&#34;depth2img-stable2&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Depth-Conditional Stable Diffusion&lt;/h4&gt; &#xA;&lt;p&gt;To augment the well-established &lt;a href=&#34;https://github.com/CompVis/stable-diffusion#image-modification-with-stable-diffusion&#34;&gt;img2img&lt;/a&gt; functionality of Stable Diffusion, we provide a &lt;em&gt;shape-preserving&lt;/em&gt; stable diffusion model.&lt;/p&gt; &#xA;&lt;p&gt;Note that the original method for image modification introduces significant semantic changes w.r.t. the initial image. If that is not desired, download our &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-depth&#34;&gt;depth-conditional stable diffusion&lt;/a&gt; model and the &lt;code&gt;dpt_hybrid&lt;/code&gt; MiDaS &lt;a href=&#34;https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt&#34;&gt;model weights&lt;/a&gt;, place the latter in a folder &lt;code&gt;midas_models&lt;/code&gt; and sample via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/gradio/depth2img.py configs/stable-diffusion/v2-midas-inference.yaml &amp;lt;path-to-ckpt&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamlit run scripts/streamlit/depth2img.py configs/stable-diffusion/v2-midas-inference.yaml &amp;lt;path-to-ckpt&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This method can be used on the samples of the base model itself. For example, take &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/depth2img/old_man.png&#34;&gt;this sample&lt;/a&gt; generated by an anonymous discord user. Using the &lt;a href=&#34;https://gradio.app&#34;&gt;gradio&lt;/a&gt; or &lt;a href=&#34;https://streamlit.io/&#34;&gt;streamlit&lt;/a&gt; script &lt;code&gt;depth2img.py&lt;/code&gt;, the MiDaS model first infers a monocular depth estimate given this input, and the diffusion model is then conditioned on the (relative) depth output.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt; depth2image &lt;/b&gt;&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/depth2img/d2i.gif&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;This model is particularly useful for a photorealistic style; see the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/depth2img&#34;&gt;examples&lt;/a&gt;. For a maximum strength of 1.0, the model removes all pixel-based information and only relies on the text prompt and the inferred monocular depth estimate.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/depth2img/merged-0005.png&#34; alt=&#34;depth2img-stable3&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Classic Img2Img&lt;/h4&gt; &#xA;&lt;p&gt;For running the &#34;classic&#34; img2img, use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/img2img.py --prompt &#34;A fantasy landscape, trending on artstation&#34; --init-img &amp;lt;path-to-img.jpg&amp;gt; --strength 0.8 --ckpt &amp;lt;path/to/model.ckpt&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and adapt the checkpoint and config paths accordingly.&lt;/p&gt; &#xA;&lt;h3&gt;Image Upscaling with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-samples/upscaling/merged-dog.png&#34; alt=&#34;upscaling-x4&#34;&gt; After &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler&#34;&gt;downloading the weights&lt;/a&gt;, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/gradio/superresolution.py configs/stable-diffusion/x4-upscaling.yaml &amp;lt;path-to-checkpoint&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamlit run scripts/streamlit/superresolution.py -- configs/stable-diffusion/x4-upscaling.yaml &amp;lt;path-to-checkpoint&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a Gradio or Streamlit demo of the text-guided x4 superresolution model.&lt;br&gt; This model can be used both on real inputs and on synthesized examples. For the latter, we recommend setting a higher &lt;code&gt;noise_level&lt;/code&gt;, e.g. &lt;code&gt;noise_level=100&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Image Inpainting with Stable Diffusion&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/assets/stable-inpainting/merged-leopards.png&#34; alt=&#34;inpainting-stable2&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-inpainting&#34;&gt;Download the SD 2.0-inpainting checkpoint&lt;/a&gt; and run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/gradio/inpainting.py configs/stable-diffusion/v2-inpainting-inference.yaml &amp;lt;path-to-checkpoint&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;streamlit run scripts/streamlit/inpainting.py -- configs/stable-diffusion/v2-inpainting-inference.yaml &amp;lt;path-to-checkpoint&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;for a Gradio or Streamlit demo of the inpainting model. This scripts adds invisible watermarking to the demo in the &lt;a href=&#34;https://github.com/runwayml/stable-diffusion/raw/main/scripts/inpaint_st.py&#34;&gt;RunwayML&lt;/a&gt; repository, but both should work interchangeably with the checkpoints/configs.&lt;/p&gt; &#xA;&lt;h2&gt;Shout-Outs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; and in particular &lt;a href=&#34;https://github.com/apolinario&#34;&gt;Apolinário&lt;/a&gt; for support with our model releases!&lt;/li&gt; &#xA; &lt;li&gt;Stable Diffusion would not be possible without &lt;a href=&#34;https://laion.ai/&#34;&gt;LAION&lt;/a&gt; and their efforts to create open, large-scale datasets.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://twitter.com/deepfloydai&#34;&gt;DeepFloyd team&lt;/a&gt; at Stability AI, for creating the subset of &lt;a href=&#34;https://laion.ai/blog/laion-5b/&#34;&gt;LAION-5B&lt;/a&gt; dataset used to train the model.&lt;/li&gt; &#xA; &lt;li&gt;Stable Diffusion 2.0 uses &lt;a href=&#34;https://laion.ai/blog/large-openclip/&#34;&gt;OpenCLIP&lt;/a&gt;, trained by &lt;a href=&#34;https://github.com/rom1504&#34;&gt;Romain Beaumont&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Our codebase for the diffusion models builds heavily on &lt;a href=&#34;https://github.com/openai/guided-diffusion&#34;&gt;OpenAI&#39;s ADM codebase&lt;/a&gt; and &lt;a href=&#34;https://github.com/lucidrains/denoising-diffusion-pytorch&#34;&gt;https://github.com/lucidrains/denoising-diffusion-pytorch&lt;/a&gt;. Thanks for open-sourcing!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;CompVis&lt;/a&gt; initial stable diffusion release&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pesser&#34;&gt;Patrick&lt;/a&gt;&#39;s &lt;a href=&#34;https://github.com/runwayml/stable-diffusion/raw/main/scripts/inpaint_st.py&#34;&gt;implementation&lt;/a&gt; of the streamlit demo for inpainting.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;img2img&lt;/code&gt; is an application of &lt;a href=&#34;https://arxiv.org/abs/2108.01073&#34;&gt;SDEdit&lt;/a&gt; by &lt;a href=&#34;https://cs.stanford.edu/~chenlin/&#34;&gt;Chenlin Meng&lt;/a&gt; from the &lt;a href=&#34;https://cs.stanford.edu/~ermon/website/&#34;&gt;Stanford AI Lab&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/(https://github.com/CompVis/latent-diffusion/pull/51)&#34;&gt;Kat&#39;s implementation&lt;/a&gt; of the &lt;a href=&#34;https://arxiv.org/abs/2202.09778&#34;&gt;PLMS&lt;/a&gt; sampler, and &lt;a href=&#34;https://github.com/crowsonkb/k-diffusion&#34;&gt;more&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.00927&#34;&gt;DPMSolver&lt;/a&gt; &lt;a href=&#34;https://github.com/CompVis/stable-diffusion/pull/440&#34;&gt;integration&lt;/a&gt; by &lt;a href=&#34;https://github.com/LuChengTHU&#34;&gt;Cheng Lu&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Facebook&#39;s &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xformers&lt;/a&gt; for efficient attention computation.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/isl-org/MiDaS&#34;&gt;MiDaS&lt;/a&gt; for monocular depth estimation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code in this repository is released under the MIT License.&lt;/p&gt; &#xA;&lt;p&gt;The weights are available via &lt;a href=&#34;https://huggingface.co/StabilityAI&#34;&gt;the StabilityAI organization at Hugging Face&lt;/a&gt;, and released under the &lt;a href=&#34;https://raw.githubusercontent.com/Stability-AI/stablediffusion/main/LICENSE-MODEL&#34;&gt;CreativeML Open RAIL++-M License&lt;/a&gt; License.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;      title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},&#xA;      year={2021},&#xA;      eprint={2112.10752},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>binpash/try</title>
    <updated>2023-06-27T01:30:34Z</updated>
    <id>tag:github.com,2023-06-27:/binpash/try</id>
    <link href="https://github.com/binpash/try" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&#34;Do, or do not. There is no try.&#34; We&#39;re setting out to change that: `try cmd` and commit---or not.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;try&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/binpash/try/main/docs/try_logo.png&#34; alt=&#34;try logo&#34; width=&#34;100&#34; height=&#34;130&#34;&gt; &#xA;&lt;p&gt;&#34;Do, or do not. There is no try.&#34;&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re setting out to change that.&lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/binpash/try/actions/workflows/test.yaml&#34;&gt;&lt;img src=&#34;https://github.com/binpash/try/actions/workflows/test.yaml/badge.svg?sanitize=true&#34; alt=&#34;LocalTests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/binpash/try/main/#license&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-blue&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binpash/try/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/binpash/try&#34; alt=&#34;issues - try&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;try&lt;/code&gt; lets you run a command and inspect its effects before changing your live system. &lt;code&gt;try&lt;/code&gt; uses Linux&#39;s &lt;a href=&#34;https://docs.kernel.org/userspace-api/unshare.html&#34;&gt;namespaces (via &lt;code&gt;unshare&lt;/code&gt;)&lt;/a&gt; and the &lt;a href=&#34;https://docs.kernel.org/filesystems/overlayfs.html&#34;&gt;overlayfs&lt;/a&gt; union filesystem.&lt;/p&gt; &#xA;&lt;p&gt;Please note that &lt;code&gt;try&lt;/code&gt; is a prototype and not a full sandbox, and should not be used to execute commands that you don&#39;t already trust on your system, (i.e. devices in &lt;code&gt;/dev&lt;/code&gt; are mounted in the sandbox, and network calls are all allowed.) Please do not attempt any commands that will remove everything in /dev or write zeros to your disks.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/binpash/try/main/docs/try_pip_install_example.gif&#34; alt=&#34;try gif&#34;&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Has been tested on the following distributions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;Ubuntu 20.04 LTS&lt;/code&gt; or later&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Debian 12&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Centos 9 Stream 5.14.0-325.el9&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Arch 6.1.33-1-lts&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Alpine 6.1.34-1-lts&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;Rocky 9 5.14.0-284.11.1.el9_2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SteamOS 3.4.8 5.13.0-valve36-1-neptune&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installing&lt;/h3&gt; &#xA;&lt;p&gt;You only need the &lt;a href=&#34;https://raw.githubusercontent.com/binpash/try/main/try&#34;&gt;&lt;code&gt;try&lt;/code&gt; script&lt;/a&gt;, which you can download by cloning this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ShellSession&#34;&gt;$ git clone https://github.com/binpash/try.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Arch Linux&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;Try&lt;/code&gt; is present in &lt;a href=&#34;https://aur.archlinux.org/packages/try&#34;&gt;AUR&lt;/a&gt;, you can install it with your preferred AUR helper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shellsession&#34;&gt;yay -s try&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shellsession&#34;&gt;git clone https://aur.archlinux.org/try.git&#xA;cd try&#xA;makepkg -sic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Example Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;try&lt;/code&gt; is a higher-order command, like &lt;code&gt;xargs&lt;/code&gt;, &lt;code&gt;exec&lt;/code&gt;, &lt;code&gt;nohup&lt;/code&gt;, or &lt;code&gt;find&lt;/code&gt;. For example, to install a package via &lt;code&gt;pip3&lt;/code&gt;, you can invoke &lt;code&gt;try&lt;/code&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ShellSession&#34;&gt;$ try pip3 install libdash&#xA;... # output continued below&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, &lt;code&gt;try&lt;/code&gt; will ask you to commit the changes made at the end of its execution.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ShellSession&#34;&gt;...&#xA;Defaulting to user installation because normal site-packages is not writeable&#xA;Collecting libdash&#xA;  Downloading libdash-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)&#xA;     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 254.6/254.6 KB 2.1 MB/s eta 0:00:00&#xA;Installing collected packages: libdash&#xA;Successfully installed libdash-0.3.1&#xA;WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv&#xA;&#xA;Changes detected in the following files:&#xA;&#xA;/tmp/tmp.zHCkY9jtIT/upperdir/home/gliargovas/.local/lib/python3.10/site-packages/libdash/ast.py (modified/added)&#xA;/tmp/tmp.zHCkY9jtIT/upperdir/home/gliargovas/.local/lib/python3.10/site-packages/libdash/_dash.py (modified/added)&#xA;/tmp/tmp.zHCkY9jtIT/upperdir/home/gliargovas/.local/lib/python3.10/site-packages/libdash/__init__.py (modified/added)&#xA;/tmp/tmp.zHCkY9jtIT/upperdir/home/gliargovas/.local/lib/python3.10/site-packages/libdash/__pycache__/printer.cpython-310.pyc (modified/added)&#xA;/tmp/tmp.zHCkY9jtIT/upperdir/home/gliargovas/.local/lib/python3.10/site-packages/libdash/__pycache__/ast.cpython-310.pyc (modified/added)&#xA;&amp;lt;snip&amp;gt;&#xA;&#xA;Commit these changes? [y/N] y&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Sometimes, you might want to pre-execute a command and commit its result at a later time. Running &lt;code&gt;try -n&lt;/code&gt; will print the overlay directory on STDOUT without committing the result.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ShellSession&#34;&gt;$ try -n &#34;curl https://sh.rustup.rs | sh&#34;&#xA;/tmp/tmp.uCThKq7LBK&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can specify your own existing overlay directory using the &lt;code&gt;-D [dir]&lt;/code&gt; flag:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ShellSession&#34;&gt;$ mkdir rustup-sandbox&#xA;$ try -D rustup-sandbox &#34;curl https://sh.rustup.rs | sh&#34;&#xA;$ ls rustup-sandbox&#xA;temproot  upperdir  workdir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As you can see from the output above, &lt;code&gt;try&lt;/code&gt; has created an overlay environment in the &lt;code&gt;rustup-sandbox&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;Manually inspecting upperdir reveals the changes to the files made inside the overlay during the execution of the previous command with &lt;em&gt;try&lt;/em&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ShellSession&#34;&gt;~/try/rustup-sandbox/upperdir$ du -hs .&#xA;1.2G    .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can inspect the changes made inside a given overlay directory using &lt;code&gt;try&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ShellSession&#34;&gt;$ try summary rustup-sandbox/ | head&#xA;&#xA;Changes detected in the following files:&#xA;&#xA;rustup-sandbox//upperdir/home/ubuntu/.profile (modified/added)&#xA;rustup-sandbox//upperdir/home/ubuntu/.bashrc (modified/added)&#xA;rustup-sandbox//upperdir/home/ubuntu/.rustup/update-hashes/stable-x86_64-unknown-linux-gnu (modified/added)&#xA;rustup-sandbox//upperdir/home/ubuntu/.rustup/settings.toml (modified/added)&#xA;rustup-sandbox//upperdir/home/ubuntu/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/libstd-8389830094602f5a.so (modified/added)&#xA;rustup-sandbox//upperdir/home/ubuntu/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/etc/lldb_commands (modified/added)&#xA;rustup-sandbox//upperdir/home/ubuntu/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/etc/gdb_lookup.py (modified/added)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also choose to commit the overlay directory contents:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ShellSession&#34;&gt;$ try commit rustup-sandbox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;p&gt;Any command that interacts with other users/groups will fail since only the current user&#39;s UID/GID are mapped. However, the &lt;a href=&#34;https://github.com/binpash/try/tree/future&#34;&gt;future branch&lt;/a&gt; has support for uid/mapping; please refer to the that branch&#39;s readme for installation instructions for the uid/gidmapper.&lt;/p&gt; &#xA;&lt;p&gt;Please also report any issue you run into while using the future branch!&lt;/p&gt; &#xA;&lt;h2&gt;Version History&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;0.1.0 - 2023-06-25 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Initial release.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see LICENSE for details.&lt;/p&gt; &#xA;&lt;p&gt;Copyright (c) 2023 The PaSh Authors.&lt;/p&gt;</summary>
  </entry>
</feed>