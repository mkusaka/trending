<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-11T01:29:10Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>longbridge/gpui-component</title>
    <updated>2025-05-11T01:29:10Z</updated>
    <id>tag:github.com,2025-05-11:/longbridge/gpui-component</id>
    <link href="https://github.com/longbridge/gpui-component" rel="alternate"></link>
    <summary type="html">&lt;p&gt;UI components for building fantastic desktop application by using GPUI.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GPUI Component&lt;/h1&gt; &#xA;&lt;p&gt;UI components for building fantastic desktop applications using &lt;a href=&#34;https://gpui.rs&#34;&gt;GPUI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Richness&lt;/strong&gt;: 40+ cross-platform desktop UI components.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Native&lt;/strong&gt;: Inspired by macOS and Windows controls, combined with shadcn/ui design for a modern experience.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ease of Use&lt;/strong&gt;: Stateless &lt;code&gt;RenderOnce&lt;/code&gt; components, simple and user-friendly.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Built-in &lt;code&gt;Theme&lt;/code&gt; and &lt;code&gt;ThemeColor&lt;/code&gt;, supporting multi-theme and variable-based configurations.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Versatile&lt;/strong&gt;: Supports sizes like &lt;code&gt;xs&lt;/code&gt;, &lt;code&gt;sm&lt;/code&gt;, &lt;code&gt;md&lt;/code&gt;, and &lt;code&gt;lg&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible Layout&lt;/strong&gt;: Dock layout for panel arrangements, resizing, and freeform (Tiles) layouts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High Performance&lt;/strong&gt;: Virtualized Table and List components for smooth large-data rendering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Content Rendering&lt;/strong&gt;: Native support for Markdown and simple HTML.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Showcase&lt;/h2&gt; &#xA;&lt;p&gt;Here is the first application: &lt;a href=&#34;https://longbridge.com/desktop&#34;&gt;Longbridge Pro&lt;/a&gt;, built using GPUI Component.&lt;/p&gt; &#xA;&lt;img width=&#34;1763&#34; alt=&#34;Image&#34; src=&#34;https://github.com/user-attachments/assets/3e2f4eb7-fd27-4343-b6dc-184465599e99&#34;&gt; &#xA;&lt;p&gt;We built multi-theme support in the application. This feature is not included in GPUI Component itself, but is based on the &lt;code&gt;Theme&lt;/code&gt; feature, so it&#39;s easy to implement.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;GPUI and GPUI Component are still in development, so you need to add dependencies by git.&lt;/p&gt; &#xA;&lt;p&gt;GPUI Component depends on a specific version of &lt;code&gt;gpui&lt;/code&gt; (kept updated with upstream) to include WebView support.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;gpui = { git = &#34;https://github.com/huacnlee/zed.git&#34;, branch = &#34;webview&#34; }&#xA;gpui-component = { git = &#34;https://github.com/longbridge/gpui-component.git&#34; }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;WebView&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Still early and experimental; there are a lot of limitations.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;GPUI Component has a &lt;code&gt;WebView&lt;/code&gt; element based on &lt;a href=&#34;https://github.com/tauri-apps/wry&#34;&gt;Wry&lt;/a&gt;. This is an optional feature, which you can enable with a feature flag.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-toml&#34;&gt;gpui-component = { git = &#34;https://github.com/longbridge/gpui-component.git&#34;, features = [&#34;webview&#34;] }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More usage examples can be found in the &lt;a href=&#34;https://github.com/longbridge/gpui-component/tree/main/crates/story&#34;&gt;story&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;Icons&lt;/h3&gt; &#xA;&lt;p&gt;GPUI Component has an &lt;code&gt;Icon&lt;/code&gt; element, but it does not include SVG files by default.&lt;/p&gt; &#xA;&lt;p&gt;The example uses &lt;a href=&#34;https://lucide.dev&#34;&gt;Lucide&lt;/a&gt; icons, but you can use any icons you like. Just name the SVG files as defined in &lt;a href=&#34;https://github.com/longbridge/gpui-component/raw/main/crates/ui/src/icon.rs#L86&#34;&gt;IconName&lt;/a&gt;. You can add any icons you need to your project.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;We have a gallery of applications built with GPUI Component.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More examples can be found in the &lt;code&gt;examples&lt;/code&gt; directory. You can run them with &lt;code&gt;cargo run --example &amp;lt;example_name&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/longbridge/gpui-component/main/DEVELOPMENT&#34;&gt;DEVELOPMENT&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Apache-2.0&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;UI design based on &lt;a href=&#34;https://ui.shadcn.com&#34;&gt;shadcn/ui&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Icons from &lt;a href=&#34;https://lucide.dev&#34;&gt;Lucide&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>shane-mason/FieldStation42</title>
    <updated>2025-05-11T01:29:10Z</updated>
    <id>tag:github.com,2025-05-11:/shane-mason/FieldStation42</id>
    <link href="https://github.com/shane-mason/FieldStation42" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Broadcast TV simulator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FieldStation42&lt;/h1&gt; &#xA;&lt;p&gt;Cable and broadcast TV simulator intended to provide an authentic experience of watching OTA television with the following goals:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;When the TV is turned on, a believable show for the time slot and network should be playing&lt;/li&gt; &#xA; &lt;li&gt;When switching between channels, the shows should continue playing serially as though they had been broadcasting the whole time&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shane-mason/FieldStation42/main/docs/retro-tv.png?raw=true&#34; alt=&#34;An older TV with an antenna rotator box in the background&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports multiple simultanous channels&lt;/li&gt; &#xA; &lt;li&gt;Automatically interleaves commercial break and bumps into content&lt;/li&gt; &#xA; &lt;li&gt;Generates weekly schedules based on per-station configurations&lt;/li&gt; &#xA; &lt;li&gt;Feature length content - supports movie length show blocks&lt;/li&gt; &#xA; &lt;li&gt;Randomly selects shows from the programming slot that have not been played recently to keep a fresh lineup&lt;/li&gt; &#xA; &lt;li&gt;Set dates ranges for shows (like seasonal sports or holiday shows)&lt;/li&gt; &#xA; &lt;li&gt;Per-station configuration of station sign-off video and off-air loops&lt;/li&gt; &#xA; &lt;li&gt;UX to manage catalogs and schedules&lt;/li&gt; &#xA; &lt;li&gt;Optional hardware connections to change the channel&lt;/li&gt; &#xA; &lt;li&gt;Loooing channels - useful for community bulliten channels&lt;/li&gt; &#xA; &lt;li&gt;Preview/guide channel with embedded video and configurable messages &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This is a new feature - documentation in progress in the &lt;a href=&#34;https://github.com/shane-mason/FieldStation42/wiki&#34;&gt;FieldStation42 Guide&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Flexible scheduling to support all kinds of channel types &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Traditional networks channels with commercials and bumps&lt;/li&gt; &#xA;   &lt;li&gt;Commercial free channels with optional end bump padding at end (movie channels, public broadcasting networks)&lt;/li&gt; &#xA;   &lt;li&gt;Loop channels, useful for community bulletin style channels or information loops.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shane-mason/FieldStation42/main/docs/cable_cover_3.png?raw=true&#34; alt=&#34;A cable box next to a TV&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Alpha software - installation is not simple&lt;/h2&gt; &#xA;&lt;p&gt;This is a fairly new project and in active development - installation requires some background in the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Basic linux command line usage&lt;/li&gt; &#xA; &lt;li&gt;Reading and editing JSON configuration files&lt;/li&gt; &#xA; &lt;li&gt;Movie file conversion and organizing in folders&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation &amp;amp; Setup&lt;/h2&gt; &#xA;&lt;p&gt;For a complete, step-by-step guide to setting up and administering FieldStation42 software, check out the &lt;a href=&#34;https://github.com/shane-mason/FieldStation42/wiki&#34;&gt;FieldStation42 Guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Quickstart Setup&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure Python 3 and MPV are installed on your system&lt;/li&gt; &#xA; &lt;li&gt;Clone the repository - this will become you main working directory.&lt;/li&gt; &#xA; &lt;li&gt;Run the install script&lt;/li&gt; &#xA; &lt;li&gt;Add your own content (videos)&lt;/li&gt; &#xA; &lt;li&gt;Configure your stations &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Copy an example json file from &lt;code&gt;confs/examples&lt;/code&gt; into &lt;code&gt;confs/&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Generate a weekly schedule &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;python3 station_42.py&lt;/code&gt; on the command line &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Use &lt;code&gt;--rebuild_catalog&lt;/code&gt; option if content has changed&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Watch TV &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;field_player.py&lt;/code&gt; on the command line&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Configure start-on-boot (optional and not recommended unless you are making a dedicated device.) &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;fs42/hot_start.sh&lt;/code&gt; on the command line&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The quickstart above is only designed to provide an overview of the required steps - use the &lt;a href=&#34;https://github.com/shane-mason/FieldStation42/wiki&#34;&gt;FieldStation42 Guide&lt;/a&gt; for more detailed description of the steps.&lt;/p&gt; &#xA;&lt;h1&gt;How It Works&lt;/h1&gt; &#xA;&lt;p&gt;FieldStation42 has multiple components that work together to recreate that old-school TV nostalgia.&lt;/p&gt; &#xA;&lt;h3&gt;station_42.py&lt;/h3&gt; &#xA;&lt;p&gt;Use this to create catalogs and generate schedules. Catalogs are used to store metadata about the stations content, so they need to be rebuilt each time the content changes. Since it is inspecting files on disk, this can take some time depending on the number of videos in your content library. The liquid-scheduler uses the catalogs and the stations configuration to build schedules, so catalogs should be built first. Running &lt;code&gt;station_42.py&lt;/code&gt; with no arguments will start a UI that runs in the terminal. You can use this to manage catalogs and schedules, or you can perform all operations using command line arguments with no UI. To see the list of all options, run &lt;code&gt;station_42.py --help&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;field_player.py&lt;/h3&gt; &#xA;&lt;p&gt;This is the main TV interface. On startup, it will read the schedule and open the correct video file and skip to the correct position based on the current time. It will re-perform this step each time the channel is changed. If you tune back to a previous channel, it will pick up the current time and start playing as though it had been playing the whole time.&lt;/p&gt; &#xA;&lt;p&gt;The player monitors the plain text file &lt;code&gt;runtime/channel.socket&lt;/code&gt; for commands to change the channel and will change to the next station configured in &lt;code&gt;main_config&lt;/code&gt; in &lt;code&gt;confs/fieldStation42_conf.py&lt;/code&gt; if any content is found there - or you can use the following command to cause the player to change to channel 3:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;echo {\&#34;command\&#34;: \&#34;direct\&#34;, \&#34;channel\&#34;: 3} &amp;gt; runtime/channel.socket&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also open &lt;code&gt;runtime/channel.socket&lt;/code&gt; in a text editor and enter the following json snippet (change 3 to whatever number you want to change to)&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;{&#34;command&#34;: &#34;direct&#34;, &#34;channel&#34;: 3}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The following command will cause the player to tune up or down respectively&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;{&#34;command&#34;: &#34;up&#34;, &#34;channel&#34;: -1}&lt;/code&gt; &lt;code&gt;{&#34;command&#34;: &#34;down&#34;, &#34;channel&#34;: -1}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;The player writes its status and current channel to &lt;code&gt;runtime/play_status.socket&lt;/code&gt; - this can be monitored by an external program if needed. See &lt;a href=&#34;https://github.com/shane-mason/FieldStation42/wiki/Changing-Channel-From-Script&#34;&gt;this page&lt;/a&gt; for more information on intgrating with &lt;code&gt;channel.socket&lt;/code&gt; and &lt;code&gt;play_status.socket&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;command_input.py&lt;/h3&gt; &#xA;&lt;p&gt;This is provided as an example component to show how to connect an external device or program to invoke a channel changes and pass status information. This script listens for incoming commands on the pi&#39;s UART connection and then writes channel change commands to &lt;code&gt;runtime/channel.socket&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Using hotstart.sh&lt;/h2&gt; &#xA;&lt;p&gt;This file is for use on a running system that has been configured and testing, because it swallows output so you&#39;ll never know what&#39;s going wrong. This file is intended to be used to start the player running on system boot up.&lt;/p&gt; &#xA;&lt;h2&gt;Connecting to a TV&lt;/h2&gt; &#xA;&lt;p&gt;The Raspberry Pi has an HDMI output, but if you want to connect it to a vintage TV, you will need to convert that to an input signal your TV can understand. If your TV has composite or RF, you can use an HTMI-&amp;gt;Composit or HDMI-&amp;gt;RF adapter. These units are available online or at an electronics retailer.&lt;/p&gt; &#xA;&lt;h2&gt;Connecting a remote control or other device&lt;/h2&gt; &#xA;&lt;p&gt;Since the player can recieve external commands and publishes its status as described above, its easy to connect external devices of all kinds. See &lt;a href=&#34;https://github.com/shane-mason/FieldStation42/wiki/Changing-Channel-From-Script&#34;&gt;this wiki page&lt;/a&gt; for more information on intgrating with &lt;code&gt;channel.socket&lt;/code&gt; and &lt;code&gt;play_status.socket&lt;/code&gt;. For a detailed guide on setting up a bluetooth remote control, &lt;a href=&#34;https://github.com/shane-mason/FieldStation42/discussions/47&#34;&gt;see this page in the discussion boards&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/shane-mason/FieldStation42/main/docs/retro-tv-setup_bb.png?raw=true&#34; alt=&#34;Fritzing diagram for the system&#34; title=&#34;Fritzing Diagram&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Raspberry Pico Setup&lt;/h2&gt; &#xA;&lt;p&gt;This is only required if you are building the channel change detector component (not required).&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install Circuit Python per their instructions and install dependencies for Neopixels.&lt;/li&gt; &#xA; &lt;li&gt;Add the contents of &lt;code&gt;aerial_listener.py&lt;/code&gt; to &lt;code&gt;code.py&lt;/code&gt; on the device so that it starts at boot.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The fritzing diagram shows how to connect the components together to enable channel changes.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>panaversity/learn-agentic-ai</title>
    <updated>2025-05-11T01:29:10Z</updated>
    <id>tag:github.com,2025-05-11:/panaversity/learn-agentic-ai</id>
    <link href="https://github.com/panaversity/learn-agentic-ai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern and Agent-Native Cloud Technologies: OpenAI Agents SDK, Memory, MCP, A2A, Knowledge Graphs, Dapr, Rancher Desktop, and Kubernetes.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn Agentic AI using Dapr Agentic Cloud Ascent (DACA) Design Pattern: From Start to Scale&lt;/h1&gt; &#xA;&lt;p&gt;This repo is part of the &lt;a href=&#34;https://docs.google.com/document/d/15usu1hkrrRLRjcq_3nCTT-0ljEcgiC44iSdvdqrCprk/edit?usp=sharing&#34;&gt;Panaversity Certified Agentic &amp;amp; Robotic AI Engineer&lt;/a&gt; program. It covers AI-201, AI-202 and AI-301 courses.&lt;/p&gt; &#xA;&lt;p&gt;We have Two Hunches, the future of Pakistan depends on it, let&#39;s make sure that we are not wrong:&lt;/p&gt; &#xA;&lt;p&gt;It is very important for Pakistan that we bet on the right horses for the upcoming age of Agentic AI. We will be training millions of Agentic AI Developers all over Pakistan and online around the world and building startups, we cant afford to be wrong.&lt;/p&gt; &#xA;&lt;p&gt;Hunch #1: Dapr We feel Dapr, Dapr Actors, Dapr Workflows, and Dapr Agents will be the core technology in building the next generation multi ai agentic systems, is my hunch correct?&lt;/p&gt; &#xA;&lt;p&gt;Hunch #2: OpenAI Agents SDK We also have a hunch that OpenAI Agents SDK will be the go to framework for beginners to start learning Agentic AI?&lt;/p&gt; &#xA;&lt;p&gt;Let us see what the best AI has to say about our hunches:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chatgpt.com/share/6811b893-82cc-8001-9037-e45bcd91cc64&#34;&gt;https://chatgpt.com/share/6811b893-82cc-8001-9037-e45bcd91cc64&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://g.co/gemini/share/1f31c876520b&#34;&gt;https://g.co/gemini/share/1f31c876520b&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://grok.com/share/bGVnYWN5_4343d342-c7df-4b06-9174-487a64f59d53&#34;&gt;https://grok.com/share/bGVnYWN5_4343d342-c7df-4b06-9174-487a64f59d53&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;This Panaversity Initiative Tackles the Critical Challenge:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;“How do we design AI Agents that can handle 10 million concurrent AI Agents without failing?”&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: The challenge is intensified as we must guide our students to solve this issue with minimal financial resources available during training.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/cover.png&#34; width=&#34;600&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Kubernetes with Dapr can theoretically handle 10 million concurrent agents in an agentic AI system without failing, but achieving this requires extensive optimization, significant infrastructure, and careful engineering. While direct evidence at this scale is limited, logical extrapolation from existing benchmarks, Kubernetes’ scalability, and Dapr’s actor model supports feasibility, especially with rigorous tuning and resource allocation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Condensed Argument with Proof and Logic&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Kubernetes Scalability&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: Kubernetes supports up to 5,000 nodes and 150,000 pods per cluster (Kubernetes docs), with real-world examples like PayPal scaling to 4,000 nodes and 200,000 pods (InfoQ, 2023) and KubeEdge managing 100,000 edge nodes and 1 million pods (KubeEdge case studies). OpenAI’s 2,500-node cluster for AI workloads (OpenAI blog, 2022) shows Kubernetes can handle compute-intensive tasks.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: For 10 million users, a cluster of 5,000–10,000 nodes (e.g., AWS g5 instances with GPUs) can distribute workloads. Each node can run hundreds of pods, and Kubernetes’ horizontal pod autoscaling (HPA) dynamically adjusts to demand. Bottlenecks (e.g., API server, networking) can be mitigated by tuning etcd, using high-performance CNIs like Cilium, and optimizing DNS.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Dapr’s Efficiency for Agentic AI&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: Dapr’s actor model supports thousands of virtual actors per CPU core with double-digit millisecond latency (Dapr docs, 2024). Case studies show Dapr handling millions of events, e.g., Tempestive’s IoT platform processing billions of messages (Dapr blog, 2023) and DeFacto’s system managing 3,700 events/second (320 million daily) on Kubernetes with Kafka (Microsoft case study, 2022).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: Agentic AI relies on stateful, low-latency agents. Dapr Agents, built on the actor model, can represent 10 million users as actors, distributed across a Kubernetes cluster. Dapr’s state management (e.g., Redis) and pub/sub messaging (e.g., Kafka) ensure efficient coordination and resilience, with automatic retries preventing failures. Sharding state stores and message brokers scales to millions of operations/second.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Handling AI Workloads&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: LLM inference frameworks like vLLM and TGI serve thousands of requests/second per GPU (vLLM benchmarks, 2024). Kubernetes orchestrates GPU workloads effectively, as seen Kubernetes manages GPU workloads, as seen in NVIDIA’s AI platform scaling to thousands of GPUs (NVIDIA case study, 2023).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: Assuming each user generates 1 request/second requiring 0.01 GPU, 10 million users need ~100,000 GPUs. Batching, caching, and model parallelism reduce this to a feasible ~10,000–20,000 GPUs, achievable in hyperscale clouds (e.g., AWS). Kubernetes’ resource scheduling ensures optimal GPU utilization.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Networking and Storage&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: EMQX on Kubernetes handled 1 million concurrent connections with tuning (EMQX blog, 2024). C10M benchmarks (2013) achieved 10 million connections using optimized stacks. Dapr’s state stores (e.g., Redis) support millions of operations/second (Redis benchmarks, 2024).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: 10 million connections require ~100–1,000 Gbps bandwidth, supported by modern clouds. High-throughput databases (e.g., CockroachDB) and caching (e.g., Redis Cluster) handle 10 TB of state data for 10 million users (1 KB/user). Kernel bypass (e.g., DPDK) and eBPF-based CNIs (e.g., Cilium) minimize networking latency.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Resilience and Monitoring&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Evidence&lt;/strong&gt;: Dapr’s resiliency policies (retries, circuit breakers) and Kubernetes’ self-healing (pod restarts) ensure reliability (Dapr docs, 2024). Dapr’s OpenTelemetry integration scales monitoring for millions of agents (Prometheus case studies, 2023).&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Logic&lt;/strong&gt;: Real-time metrics (e.g., latency, error rates) and distributed tracing prevent cascading failures. Kubernetes’ liveness probes and Dapr’s workflow engine recover from crashes, ensuring 99.999% uptime.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feasibility with Constraints&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Challenge&lt;/strong&gt;: No direct benchmark exists for 10 million concurrent users with Dapr/Kubernetes in an agentic AI context. Infrastructure costs (e.g., $10M–$100M for 10,000 nodes) are prohibitive for low-budget scenarios.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Solution&lt;/strong&gt;: Use open-source tools (e.g., Minikube, kind) for local testing and cloud credits (e.g., AWS Educate) for students. Simulate 10 million users with tools like Locust on smaller clusters (e.g., 100 nodes), extrapolating results. Optimize Dapr’s actor placement and Kubernetes’ resource quotas to maximize efficiency on limited hardware. Leverage free-tier databases (e.g., MongoDB Atlas) and message brokers (e.g., RabbitMQ).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;: Kubernetes with Dapr can handle 10 million concurrent users in an agentic AI system, supported by their proven scalability, real-world case studies, and logical extrapolation. For students with minimal budgets, small-scale simulations, open-source tools, and cloud credits make the problem tractable, though production-scale deployment requires hyperscale resources and expertise.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Agentic AI Top Trend of 2025&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/toptrend.webp&#34; width=&#34;200&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;The Dapr Agentic Cloud Ascent (DACA) Design Pattern Addresses 10 Million Concurrent Users Challenge&lt;/h2&gt; &#xA;&lt;p&gt;Let&#39;s understand and learn about &#34;Dapr Agentic Cloud Ascent (DACA)&#34;, our winning design pattern for developing and deploying planet scale multi-agent systems.&lt;/p&gt; &#xA;&lt;h3&gt;Executive Summary: Dapr Agentic Cloud Ascent (DACA)&lt;/h3&gt; &#xA;&lt;p&gt;The Dapr Agentic Cloud Ascent (DACA) guide introduces a strategic design pattern for building and deploying sophisticated, scalable, and resilient agentic AI systems. Addressing the complexities of modern AI development, DACA integrates the OpenAI Agents SDK for core agent logic with the Model Context Protocol (MCP) for standardized tool use and the Agent2Agent (A2A) protocol for seamless inter-agent communication, all underpinned by the distributed capabilities of Dapr. &lt;strong&gt;Grounded in AI-first and cloud-first principles&lt;/strong&gt;, DACA promotes the use of stateless, containerized applications deployed on platforms like Azure Container Apps (Serverless Containers) or Kubernetes, enabling efficient scaling from local development to planetary-scale production, potentially leveraging free-tier cloud services and self-hosted LLMs for cost optimization. The pattern emphasizes modularity, context-awareness, and standardized communication, envisioning an &lt;strong&gt;Agentia World&lt;/strong&gt; where diverse AI agents collaborate intelligently. Ultimately, DACA offers a robust, flexible, and cost-effective framework for developers and architects aiming to create complex, cloud-native agentic AI applications that are built for scalability and resilience from the ground up.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/panaversity/learn-agentic-ai/raw/main/comprehensive_guide_daca.md&#34;&gt;Comprehensive Guide to Dapr Agentic Cloud Ascent (DACA) Design Pattern&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/ascent.png&#34; width=&#34;500&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/panaversity/learn-agentic-ai/main/img/architecture1.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Target User&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Agentic AI Developer and AgentOps Professionals&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Why OpenAI Agents SDK should be the main framework for agentic development for most use cases?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Table 1: Comparison of Abstraction Levels in AI Agent Frameworks&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Framework&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Abstraction Level&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Key Characteristics&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Learning Curve&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Control Level&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Simplicity&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OpenAI Agents SDK&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Minimal&lt;/td&gt; &#xA;   &lt;td&gt;Python-first, core primitives (Agents, Handoffs, Guardrails), direct control&lt;/td&gt; &#xA;   &lt;td&gt;Low&lt;/td&gt; &#xA;   &lt;td&gt;High&lt;/td&gt; &#xA;   &lt;td&gt;High&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;CrewAI&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moderate&lt;/td&gt; &#xA;   &lt;td&gt;Role-based agents, crews, tasks, focus on collaboration&lt;/td&gt; &#xA;   &lt;td&gt;Low-Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;AutoGen&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;High&lt;/td&gt; &#xA;   &lt;td&gt;Conversational agents, flexible conversation patterns, human-in-the-loop support&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Google ADK&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moderate&lt;/td&gt; &#xA;   &lt;td&gt;Multi-agent hierarchies, Google Cloud integration (Gemini, Vertex AI), rich tool ecosystem, bidirectional streaming&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium-High&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;LangGraph&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Low-Moderate&lt;/td&gt; &#xA;   &lt;td&gt;Graph-based workflows, nodes, edges, explicit state management&lt;/td&gt; &#xA;   &lt;td&gt;Very High&lt;/td&gt; &#xA;   &lt;td&gt;Very High&lt;/td&gt; &#xA;   &lt;td&gt;Low&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Dapr Agents&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moderate&lt;/td&gt; &#xA;   &lt;td&gt;Stateful virtual actors, event-driven multi-agent workflows, Kubernetes integration, 50+ data connectors, built-in resiliency&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;   &lt;td&gt;Medium-High&lt;/td&gt; &#xA;   &lt;td&gt;Medium&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The table clearly identifies why OpenAI Agents SDK should be the main framework for agentic development for most use cases:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It excels in &lt;strong&gt;simplicity&lt;/strong&gt; and &lt;strong&gt;ease of use&lt;/strong&gt;, making it the best choice for rapid development and broad accessibility.&lt;/li&gt; &#xA; &lt;li&gt;It offers &lt;strong&gt;high control&lt;/strong&gt; with &lt;strong&gt;minimal abstraction&lt;/strong&gt;, providing the flexibility needed for agentic development without the complexity of frameworks like LangGraph.&lt;/li&gt; &#xA; &lt;li&gt;It outperforms most alternatives (CrewAI, AutoGen, Google ADK, Dapr Agents) in balancing usability and power, and while LangGraph offers more control, its complexity makes it less practical for general use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If your priority is ease of use, flexibility, and quick iteration in agentic development, OpenAI Agents SDK is the clear winner based on the table. However, if your project requires enterprise-scale features (e.g., Dapr Agents) or maximum control for complex workflows (e.g., LangGraph), you might consider those alternatives despite their added complexity.&lt;/p&gt; &#xA;&lt;h2&gt;Core DACA Agentic AI Courses:&lt;/h2&gt; &#xA;&lt;h3&gt;AI-201: Fundamentals of Agentic AI and DACA AI-First Development (14 weeks)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;⁠Agentic &amp;amp; DACA Theory - 1 week&lt;/li&gt; &#xA; &lt;li&gt;UV &amp;amp; ⁠OpenAI Agents SDK - 5 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Agentic Design Patterns - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Memory [LangMem &amp;amp; mem0] 1 week&lt;/li&gt; &#xA; &lt;li&gt;Postgres/Redis (Managed Cloud) - 1 week&lt;/li&gt; &#xA; &lt;li&gt;FastAPI (Basic) - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Containerization (Rancher Desktop) - 1 week&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face Docker Spaces - 1 week&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PL0vKVrkG4hWovpr0FX6Gs-06hfsPDEUe6&#34;&gt;AI-201 Video Playlist&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: These videos are for additional learning, and do not cover all the material taught in the onsite classes.&lt;/p&gt; &#xA;&lt;p&gt;Prerequisite: Successful completion of &lt;a href=&#34;https://github.com/panaversity/learn-modern-ai-python&#34;&gt;AI-101: Modern AI Python Programming - Your Launchpad into Intelligent Systems&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;AI-202: DACA Cloud-First Agentic AI Development (14 weeks)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Rancher Desktop with Local Kubernetes - 4 weeks&lt;/li&gt; &#xA; &lt;li&gt;Advanced FastAPI with Kubernetes - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;Dapr [workflows, state, pubsub, secrets] - 3 Week&lt;/li&gt; &#xA; &lt;li&gt;CockRoachdb &amp;amp; RabbitMQ Managed Services - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Model Context Protocol - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Serverless Containers Deployment (ACA) - 2 weeks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Prerequisite: Successful completion of AI-201&lt;/p&gt; &#xA;&lt;h3&gt;AI-301 DACA Planet-Scale Distributed AI Agents (14 Weeks)&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;⁠Certified Kubernetes Application Developer (CKAD) - 4 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠A2A Protocol - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Voice Agents - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Dapr Agents/Google ADK - 2 weeks&lt;/li&gt; &#xA; &lt;li&gt;⁠Self-LLMs Hosting - 1 week&lt;/li&gt; &#xA; &lt;li&gt;Finetuning LLMs - 3 weeks&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Prerequisite: Successful completion of AI-201 &amp;amp; AI-202&lt;/p&gt;</summary>
  </entry>
</feed>