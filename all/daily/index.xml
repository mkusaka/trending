<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-19T01:30:00Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>reactjs/react.dev</title>
    <updated>2023-03-19T01:30:00Z</updated>
    <id>tag:github.com,2023-03-19:/reactjs/react.dev</id>
    <link href="https://github.com/reactjs/react.dev" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The React documentation website&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;react.dev&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains the source code and documentation powering &lt;a href=&#34;https://react.dev/&#34;&gt;react.dev&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Git&lt;/li&gt; &#xA; &lt;li&gt;Node: any 12.x version starting with v12.0.0 or greater&lt;/li&gt; &#xA; &lt;li&gt;Yarn: See &lt;a href=&#34;https://yarnpkg.com/lang/en/docs/install/&#34;&gt;Yarn website for installation instructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A fork of the repo (for any contributions)&lt;/li&gt; &#xA; &lt;li&gt;A clone of the &lt;a href=&#34;https://github.com/reactjs/react.dev&#34;&gt;react.dev repo&lt;/a&gt; on your local machine&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;cd react.dev&lt;/code&gt; to go into the project root&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;yarn&lt;/code&gt; to install the website&#39;s npm dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Running locally&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;yarn dev&lt;/code&gt; to start the development server (powered by &lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;open http://localhost:3000&lt;/code&gt; to open the site in your favorite browser&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;h3&gt;Guidelines&lt;/h3&gt; &#xA;&lt;p&gt;The documentation is divided into several sections with a different tone and purpose. If you plan to write more than a few sentences, you might find it helpful to get familiar with the &lt;a href=&#34;https://github.com/reactjs/react.dev/raw/main/CONTRIBUTING.md#guidelines-for-text&#34;&gt;contributing guidelines&lt;/a&gt; for the appropriate sections.&lt;/p&gt; &#xA;&lt;h3&gt;Create a branch&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git checkout main&lt;/code&gt; from any folder in your local &lt;code&gt;react.dev&lt;/code&gt; repository&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git pull origin main&lt;/code&gt; to ensure you have the latest main code&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git checkout -b the-name-of-my-branch&lt;/code&gt; (replacing &lt;code&gt;the-name-of-my-branch&lt;/code&gt; with a suitable name) to create a branch&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Make the change&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Follow the &lt;a href=&#34;https://raw.githubusercontent.com/reactjs/react.dev/main/#running-locally&#34;&gt;&#34;Running locally&#34;&lt;/a&gt; instructions&lt;/li&gt; &#xA; &lt;li&gt;Save the files and check in the browser&lt;/li&gt; &#xA; &lt;li&gt;Changes to React components in &lt;code&gt;src&lt;/code&gt; will hot-reload&lt;/li&gt; &#xA; &lt;li&gt;Changes to markdown files in &lt;code&gt;content&lt;/code&gt; will hot-reload&lt;/li&gt; &#xA; &lt;li&gt;If working with plugins, you may need to remove the &lt;code&gt;.cache&lt;/code&gt; directory and restart the server&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Test the change&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If possible, test any visual changes in all latest versions of common browsers, on both desktop and mobile.&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;yarn check-all&lt;/code&gt;. (This will run Prettier, ESLint and validate types.)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Push it&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;git add -A &amp;amp;&amp;amp; git commit -m &#34;My message&#34;&lt;/code&gt; (replacing &lt;code&gt;My message&lt;/code&gt; with a commit message, such as &lt;code&gt;Fix header logo on Android&lt;/code&gt;) to stage and commit your changes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;git push my-fork-name the-name-of-my-branch&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Go to the &lt;a href=&#34;https://github.com/reactjs/react.dev&#34;&gt;react.dev repo&lt;/a&gt; and you should see recently pushed branches.&lt;/li&gt; &#xA; &lt;li&gt;Follow GitHub&#39;s instructions.&lt;/li&gt; &#xA; &lt;li&gt;If possible, include screenshots of visual changes. A preview build is triggered after your changes are pushed to GitHub.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Translation&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested in translating &lt;code&gt;react.dev&lt;/code&gt;, please see the current translation efforts &lt;a href=&#34;https://github.com/reactjs/react.dev/issues/4135&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Content submitted to &lt;a href=&#34;https://react.dev/&#34;&gt;react.dev&lt;/a&gt; is CC-BY-4.0 licensed, as found in the &lt;a href=&#34;https://github.com/reactjs/react.dev/raw/master/LICENSE-DOCS.md&#34;&gt;LICENSE-DOCS.md&lt;/a&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>202252197/ChatGPT_JCM</title>
    <updated>2023-03-19T01:30:00Z</updated>
    <id>tag:github.com,2023-03-19:/202252197/ChatGPT_JCM</id>
    <link href="https://github.com/202252197/ChatGPT_JCM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GPT多模型聊天项目，GPT-4已发布，接口开放后本项目将第一时间适配。后期会一点一点的将OpenAI接口进行接入大家支持一下呗，微信群号在下方，右上角点个Star，我会一直更新下去。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Multi Model ChatGPT Web&lt;/h1&gt; &#xA;&lt;p&gt;声明：此项目只发布于 Github，基于 Apache2.0 协议，免费且作为开源学习使用。并且不会有任何形式的卖号、付费服务、卖key等行为。谨防受骗。 项目使用Vue2进行开发，给大家提供一个好看的GPT壳子，有好的建议和bug欢迎大家提出来，星星超过100会分享2个OpenAI的Key哦，先到先用，使用完毕为止。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.328888.xyz/2023/03/18/LaWpw.jpeg&#34; alt=&#34;webui2.0&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;🤭记得点个小星星&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;给粉丝分享的OppenAI key,感谢大家点赞，希望不要恶意使用哦，让每个粉丝都可以使用到，谢谢大家，谢谢大家&lt;/h1&gt; &#xA;&lt;p&gt;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&lt;/p&gt; &#xA;&lt;p&gt;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&lt;/p&gt; &#xA;&lt;h2&gt;下载依赖包&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;运行&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm run serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;编译&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;技术栈&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;名称&lt;/th&gt; &#xA;   &lt;th&gt;版本&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;vue&lt;/td&gt; &#xA;   &lt;td&gt;2.6.14&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;element-ui&lt;/td&gt; &#xA;   &lt;td&gt;2.15.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NodeJS&lt;/td&gt; &#xA;   &lt;td&gt;14.21.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;npm&lt;/td&gt; &#xA;   &lt;td&gt;6.14.18&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;项目进度&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;接口&lt;/th&gt; &#xA;   &lt;th&gt;描述&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;List Models&lt;/td&gt; &#xA;   &lt;td&gt;获取模型列表&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chat Completion&lt;/td&gt; &#xA;   &lt;td&gt;GPT3.5模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Completion&lt;/td&gt; &#xA;   &lt;td&gt;GPT3.5以下模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Create Image&lt;/td&gt; &#xA;   &lt;td&gt;根据描述生成图片&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Create image edit&lt;/td&gt; &#xA;   &lt;td&gt;根据上传的图片结合输入的描述生成图片&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Create Image Variation&lt;/td&gt; &#xA;   &lt;td&gt;根据上传的图片生成变体图片&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Create transcription&lt;/td&gt; &#xA;   &lt;td&gt;音频识别文字&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Create translation&lt;/td&gt; &#xA;   &lt;td&gt;英语音频识别&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;fine-tune&lt;/td&gt; &#xA;   &lt;td&gt;待开发&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Files&lt;/td&gt; &#xA;   &lt;td&gt;待开发&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;多会话储存和上下文逻辑&lt;/td&gt; &#xA;   &lt;td&gt;GPT3.5模型支持上下文逻辑，多会话待开发&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;聊天截图到本地图片&lt;/td&gt; &#xA;   &lt;td&gt;截图功能，有缺陷只能截图当前窗口的图片，建议QQ长截图&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;导出导入数据&lt;/td&gt; &#xA;   &lt;td&gt;待开发&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;更换聊天窗口背景&lt;/td&gt; &#xA;   &lt;td&gt;支持输入背景图片URL&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;更换主题&lt;/td&gt; &#xA;   &lt;td&gt;待开发&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;界面多语言&lt;/td&gt; &#xA;   &lt;td&gt;待开发&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;More&lt;/td&gt; &#xA;   &lt;td&gt;待开发&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Docker部署&lt;/h1&gt; &#xA;&lt;p&gt;待更新&lt;/p&gt; &#xA;&lt;h1&gt;多模型ChatGPT群2&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.328888.xyz/2023/03/16/KsC3b.jpeg&#34; alt=&#34;webui2.0&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;多模型ChatGPT群3&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.328888.xyz/2023/03/18/MOaWC.jpeg&#34; alt=&#34;webui2.0&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;个人说明&lt;/h1&gt; &#xA;&lt;p&gt;14岁进入it行业，如今19了，时间过得太快了，从当初的html，css到java语言，然后到大数据的数据处理框架，学习真的学无止境，非常热爱编程，大家支持一下，你们的支持是我更新最大的动力，点个免费的小星星。 我后期会录制一些大家提出的问题解决方案，热爱分享，热爱技术，更热爱大家。&lt;/p&gt; &#xA;&lt;h1&gt;免责声明&lt;/h1&gt; &#xA;&lt;p&gt;这不是官方的OpenAI产品。这是一个个人项目，与OpenAI没有任何关系。不要起诉我。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mlc-ai/web-stable-diffusion</title>
    <updated>2023-03-19T01:30:00Z</updated>
    <id>tag:github.com,2023-03-19:/mlc-ai/web-stable-diffusion</id>
    <link href="https://github.com/mlc-ai/web-stable-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Bringing stable diffusion models to web browsers. Everything runs inside the browser with no server support.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Web Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;This project brings stable diffusion models onto web browsers. &lt;strong&gt;Everything runs inside the browser with no server support.&lt;/strong&gt; To our knowledge, this is the the world’s first stable diffusion completely running on the browser. Please checkout our &lt;a href=&#34;https://mlc.ai/web-stable-diffusion/#text-to-image-generation-demo&#34;&gt;demo webpage&lt;/a&gt; to try it out.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/mlc-ai/web-stable-diffusion/main/site/img/fig/browser-screenshot.png&#34; alt=&#34;Browser screenshot&#34;&gt; &#xA;&lt;p&gt;We have been seeing amazing progress through AI models recently. Thanks to the open-source effort, developers can now easily compose open-source models together to produce amazing tasks. Stable diffusion enables the automatic creation of photorealistic images as well as images in various styles based on text input. These models are usually big and compute-heavy, which means we have to pipe through all computation requests to (GPU) servers when developing web applications based on these models. Additionally, most of the workloads have to run on a specific type of GPUs where popular deep-learning frameworks are readily available.&lt;/p&gt; &#xA;&lt;p&gt;This project takes a step to change that status quo and bring more diversity to the ecosystem. There are a lot of reasons to get some (or all) of the computation to the client side. There are many possible benefits, such as cost reduction on the service provider side, as well as an enhancement for personalization and privacy protection. The development of personal computers (even mobile devices) is going in the direction that enables such possibilities. The client side is getting pretty powerful. For example, the latest MacBook Pro can have up to 96GB of unified RAM that can be used to store the model weights and a reasonably powerful GPU to run many of the workloads.&lt;/p&gt; &#xA;&lt;p&gt;Building special client apps for those applications is one option (which we also support), but won’t it be even more amazing if we can simply open a browser and directly bring AI natively to your browser tab? There is some level of readiness in the ecosystem. WebAssembly allows us to port more lower-level runtimes onto the web. To solve the compute problem, WebGPU is getting matured lately and enables native GPU executions on the browser.&lt;/p&gt; &#xA;&lt;p&gt;We are just seeing necessary elements coming together on the client side, both in terms of hardware and browser ecosystem. Still, there are big hurdles to cross, to name a few:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We need to bring the models somewhere without the relevant GPU-accelerated Python frameworks.&lt;/li&gt; &#xA; &lt;li&gt;Most of the AI frameworks have a heavy reliance on optimized computed libraries that are maintained by hardware vendors. We need to start from zero. To get the maximum benefit, we might also need to produce variants per client environment.&lt;/li&gt; &#xA; &lt;li&gt;Careful planning of memory usage so we can fit the models into memory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We do not want to only do it for just one model. Instead, we would like to present a repeatable, hackable, composable workflow that enables anyone to easily develop and optimize these models in a &lt;strong&gt;Python-first&lt;/strong&gt; environment and universally &lt;strong&gt;deploy&lt;/strong&gt; them everywhere, including the web.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;We have a &lt;a href=&#34;https://github.com/mlc-ai/web-stable-diffusion/raw/main/walkthrough.ipynb&#34;&gt;Jupyter notebook&lt;/a&gt; that walks you through all the stages, including&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;elaborate the key points of web ML model deployment and how we do to meet these points,&lt;/li&gt; &#xA; &lt;li&gt;import the stable diffusion model,&lt;/li&gt; &#xA; &lt;li&gt;optimize the model,&lt;/li&gt; &#xA; &lt;li&gt;build the model,&lt;/li&gt; &#xA; &lt;li&gt;deploy the model locally with native GPU runtime, and&lt;/li&gt; &#xA; &lt;li&gt;deploy the model on web with WebGPU runtime.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to go through these steps in command line, please follow the commands below:&lt;/p&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;Commands&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;Install TVM Unity. You can either&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;use &lt;code&gt;pip3 install mlc-ai-nightly -f https://mlc.ai/wheels&lt;/code&gt; to install the TVM Unity wheel, or&lt;/li&gt; &#xA;    &lt;li&gt;follow &lt;a href=&#34;https://tvm.apache.org/docs/install/from_source.html&#34;&gt;TVM’s documentation&lt;/a&gt; to build from source. &lt;strong&gt;Please use &lt;code&gt;git checkout origin/unity&lt;/code&gt; to checkout to TVM Unity after git clone.&lt;/strong&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;To import, optimize and build the stable diffusion model:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 build.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By default &lt;code&gt;build.py&lt;/code&gt; takes &lt;code&gt;apple/m2-gpu&lt;/code&gt; as build target. You can also specify CUDA target via&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 build.py --target cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;To deploy the model locally with native GPU runtime:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 deploy.py --prompt &#34;A photo of an astronaut riding a horse on mars.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can substitute the prompt with your own one, and optionally use &lt;code&gt;--negative-prompt &#34;Your negative prompt&#34;&lt;/code&gt; to specify a negative prompt.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;To deploy the model on web with WebGPU runtime, the last section “Deploy on web” of the &lt;a href=&#34;https://github.com/mlc-ai/web-stable-diffusion/raw/main/walkthrough.ipynb&#34;&gt;walkthrough notebook&lt;/a&gt; has listed the full instructions which you can refer to. We also provide the same list of plain instructions here:&lt;/p&gt; &#xA;   &lt;details&gt;&#xA;    &lt;summary&gt;Instructions&lt;/summary&gt; &#xA;    &lt;p&gt;First, let’s install all the prerequisite:&lt;/p&gt; &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://emscripten.org&#34;&gt;emscripten&lt;/a&gt;. It is an LLVM-based compiler which compiles C/C++ source code to WebAssembly. &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Follow the &lt;a href=&#34;https://emscripten.org/docs/getting_started/downloads.html#installation-instructions-using-the-emsdk-recommended&#34;&gt;installation instruction&lt;/a&gt; to install the latest emsdk.&lt;/li&gt; &#xA;       &lt;li&gt;Source &lt;code&gt;emsdk_env.sh&lt;/code&gt; by &lt;code&gt;source path/to/emsdk_env.sh&lt;/code&gt;, so that &lt;code&gt;emcc&lt;/code&gt; is reachable from PATH and the command &lt;code&gt;emcc&lt;/code&gt; works.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;Rust&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://rustwasm.github.io/wasm-pack/installer/&#34;&gt;&lt;code&gt;wasm-pack&lt;/code&gt;&lt;/a&gt;. It helps build Rust-generated WebAssembly, which used for tokenizer in our case here.&lt;/li&gt; &#xA;     &lt;li&gt;Install jekyll by following the &lt;a href=&#34;https://jekyllrb.com/docs/installation/&#34;&gt;official guides&lt;/a&gt;. It is the package we use for website.&lt;/li&gt; &#xA;     &lt;li&gt;Install jekyll-remote-theme by command &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;gem install jekyll-remote-theme&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Install &lt;a href=&#34;https://www.google.com/chrome/canary/&#34;&gt;Chrome Canary&lt;/a&gt;. It is a developer version of Chrome that enables the use of WebGPU.&lt;/li&gt; &#xA;    &lt;/ol&gt; &#xA;    &lt;p&gt;We can verify the success installation by trying out &lt;code&gt;emcc&lt;/code&gt;, &lt;code&gt;jekyll&lt;/code&gt; and &lt;code&gt;wasm-pack&lt;/code&gt; in terminal respectively.&lt;/p&gt; &#xA;    &lt;p&gt;Then, prepare all the necessary dependencies for web build:&lt;/p&gt; &#xA;    &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./scripts/prep_deps.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;We can now build the model to WebGPU backend and export the executable to disk in the WebAssembly file format, by running&lt;/p&gt; &#xA;    &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python3 build.py --target webgpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;The last thing to do is setting up the site with&lt;/p&gt; &#xA;    &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./scripts/local_deploy_site.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;With the site set up, you can go to &lt;code&gt;localhost:8888/web-stable-diffusion/&lt;/code&gt; in Chrome Canary to try out the demo on your local machine. Don’t forget to use&lt;/p&gt; &#xA;    &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/Applications/Google\ Chrome\ Canary.app/Contents/MacOS/Google\ Chrome\ Canary --enable-dawn-features=disable_robustness&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;p&gt;to launch Chrome Canary to turn off the robustness check from Chrome.&lt;/p&gt; &#xA;   &lt;/details&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;How?&lt;/h2&gt; &#xA;&lt;p&gt;The key technology here is machine learning compilation (MLC). Our solution is built on the shoulders of the open-source ecosystem, including PyTorch, Hugging Face diffusers and tokenizers, rust, wasm, and WebGPU. The main flow is built on Apache TVM Unity, an exciting ongoing development in the &lt;a href=&#34;https://github.com/apache/tvm&#34;&gt;Apache TVM&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We take &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main&#34;&gt;Runway’s stable diffusion v1-5&lt;/a&gt; models from the Hugging Face diffuser library.&lt;/li&gt; &#xA; &lt;li&gt;We use &lt;a href=&#34;https://pytorch.org/tutorials/intermediate/dynamo_tutorial.html&#34;&gt;TorchDynamo&lt;/a&gt; and &lt;a href=&#34;https://pytorch.org/docs/stable/fx.html&#34;&gt;Torch FX&lt;/a&gt; to capture key model components into an IRModule in TVM.&lt;/li&gt; &#xA; &lt;li&gt;Each function in TVM’s IRModule can be further transformed and generated with runnable code that can be deployed universally on any environment supported by minimum TVM runtime (javascript being one of them).&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2207.04296&#34;&gt;TensorIR&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2205.13603&#34;&gt;MetaSchedule&lt;/a&gt; are used to build automated solutions to generate optimized programs. These transformations are tuned on a specific device through native GPU runtimes and then used to generate optimized GPU shaders. We provide a database that records these transformations so new builds can be done without tuning.&lt;/li&gt; &#xA; &lt;li&gt;We build static memory planning optimizations to reuse memory across multiple layers.&lt;/li&gt; &#xA; &lt;li&gt;We use &lt;a href=&#34;https://emscripten.org/&#34;&gt;Emscripten&lt;/a&gt; and typescript to build a TVM web runtime that can deploy generated modules.&lt;/li&gt; &#xA; &lt;li&gt;We also leverage the &lt;a href=&#34;https://blog.mithrilsecurity.io/porting-tokenizers-to-wasm/&#34;&gt;wasm port&lt;/a&gt; of the &lt;a href=&#34;https://github.com/huggingface/tokenizers&#34;&gt;rust tokenizers library&lt;/a&gt; from hugging face.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mlc-ai/web-stable-diffusion/main/site/img/fig/workflow.svg?sanitize=true&#34; alt=&#34;workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;All parts of this workflow are done in Python, except, of course, the last part which builds a 400-loc JavaScript app that connects things together. This is also a fun process of interactive development, bringing new models.&lt;/p&gt; &#xA;&lt;p&gt;All these are made possible by the open-source ecosystem that we leverage. Specifically, we make heavy use of &lt;a href=&#34;https://discuss.tvm.apache.org/t/establish-tvm-unity-connection-a-technical-strategy/13344&#34;&gt;TVM Unity&lt;/a&gt;, an exciting latest development in the TVM project that enables such Python-first interactive MLC development experiences which allows us to easily compose new optimizations, all in Python, and incrementally bring our app to the web. TVM Unity also provides an easy way to compose new solutions in the ecosystem. For example, we can bring in other WebGPU shader generators or shader libraries easily to this workflow in the future.&lt;/p&gt; &#xA;&lt;h2&gt;Comparison with Native GPU Runtime, Limitations, and Opportunities&lt;/h2&gt; &#xA;&lt;p&gt;Besides the WebGPU runtime, we also provide options for native deployment with local GPU runtime. These options can be used both as a tool to deploy on a native environment as well as a reference point to compare native GPU driver performance and WebGPU.&lt;/p&gt; &#xA;&lt;p&gt;WebGPU works by translating WGSL (WebGPU Shading Language) shaders to native shaders. So, in theory, we can reach zero gaps between the WebGPU runtime and the native environment. If we directly use Chrome to check the current demo on Apple silicon, however, we can find a performance degradation (about 3x). This is because Chrome’s WebGPU implementation inserts bound clips for all array index access, such that &lt;code&gt;a[i]&lt;/code&gt; becomes &lt;code&gt;a[min(i, a.size)]&lt;/code&gt;. Ideally, downstream shader compilers should be able to optimize the bound clipping out, but here unfortunately, it is not the case. This gap can be fixed once WebGPU implementation becomes more mature, checks the index access range, and drops such clipping.&lt;/p&gt; &#xA;&lt;p&gt;You can get around this by using a special flag to launch Chrome (thanks to Dawn developers for providing the pointers), by exiting Chrome completely, then in the command line, type&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;/path/to/chrome-canary --enable-dawn-features=disable_robustness&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you will find that the execution speed is as fast as the native GPU environment. We anticipate this problem will get resolved as WebGPU matures.&lt;/p&gt; &#xA;&lt;p&gt;We are just seeing the dawn of what we believe to be an eruption. WebGPU is still evolving (though it is getting close to shipping this year), and only available through Chrome Canary, and can be unstable. It also still comes with limitations, such as only support for FP32 (FP16 shader extension is on the spec but not yet implemented). The stable diffusion here would require a GPU with a decent amount of RAM (8GB). We have only tested our solution through Apple silicons so far. There are also opportunities to support advanced optimizations such as &lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34;&gt;FlashAttention&lt;/a&gt; and quantization to further improve the performance of the system.&lt;/p&gt; &#xA;&lt;p&gt;These are opportunities to bring several times of performance improvements to the current solutions. We believe many of them can be tackled in the near future. A single component of this solution can still be useful. For example, one can choose just to deploy the text encoder part of the model. Additionally, the same Python-first development, universal deployment workflow can be used to bring ML models to other environments, such as new hardware or mobile cases. Finally, the same machine learning compilation stack is also shared with server class use cases and can be used to optimize server workloads as well.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;p&gt;This project is made possible thanks to collaboration with&lt;/p&gt; &#xA;&lt;a href=&#34;https://www.scs.cmu.edu&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mlc-ai/web-stable-diffusion/main/site/img/logo/cmuscs.png&#34; alt=&#34;CMU School of Computer Science&#34; height=&#34;50&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://catalyst.cs.cmu.edu&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mlc-ai/web-stable-diffusion/main/site/img/logo/catalyst.svg?sanitize=true&#34; alt=&#34;Catalyst&#34; height=&#34;50&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://mlc.ai&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mlc-ai/web-stable-diffusion/main/site/img/logo/mlc-logo-with-text-landscape.svg?sanitize=true&#34; alt=&#34;MLC&#34; height=&#34;50&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://octoml.ai&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mlc-ai/web-stable-diffusion/main/site/img/logo/octoml.png&#34; alt=&#34;OctoML&#34; height=&#34;50&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;This project is only possible thanks to the shoulders open-source ecosystems that we stand on. We want to thank the Apache TVM community and developers of the TVM Unity effort. We want to thank the open-source ML community members who make these models publicly available, and PyTorch, Hugging Face communities that make these models accessible. We would like to thank the tokenizer wasm port by Mithril Security. We also would like to thank the WebAssembly, Emscripten, Rust, and WebGPU communities. Finally, thanks to Dawn developers, who provide timely answers to questions on Chrome.&lt;/p&gt;</summary>
  </entry>
</feed>