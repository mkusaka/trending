<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-14T01:30:42Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>baichuan-inc/Baichuan-13B</title>
    <updated>2023-07-14T01:30:42Z</updated>
    <id>tag:github.com,2023-07-14:/baichuan-inc/Baichuan-13B</id>
    <link href="https://github.com/baichuan-inc/Baichuan-13B" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A 13B large language model developed by Baichuan Intelligent Technology&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; Baichuan-13B &lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🤗 &lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan-13B-Base&#34; target=&#34;_blank&#34;&gt;Baichuan-13B-Base&lt;/a&gt; • 🤗 &lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan-13B-Chat&#34; target=&#34;_blank&#34;&gt;Baichuan-13B-Chat&lt;/a&gt; • 🤖 &lt;a href=&#34;https://modelscope.cn/organization/baichuan-inc&#34; target=&#34;_blank&#34;&gt;ModelScope&lt;/a&gt; • 💬 &lt;a href=&#34;https://github.com/baichuan-inc/Baichuan-13B/raw/main/media/wechat.jpeg?raw=true&#34; target=&#34;_blank&#34;&gt;WeChat&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Baichuan-inc/baichuan-13B/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/modelscope/modelscope.svg?sanitize=true&#34; alt=&#34;license&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;b&gt;中文&lt;/b&gt; | &lt;a href=&#34;https://github.com/baichuan-inc/Baichuan-13B/raw/main/README_EN.md&#34;&gt;English&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;介绍&lt;/h1&gt; &#xA;&lt;p&gt;Baichuan-13B 是由百川智能继 &lt;a href=&#34;https://github.com/baichuan-inc/baichuan-7B&#34;&gt;Baichuan-7B&lt;/a&gt; 之后开发的包含 130 亿参数的开源可商用的大规模语言模型，在权威的中文和英文 benchmark 上均取得同尺寸最好的效果。本次发布包含有预训练 (&lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan-13B-Base&#34;&gt;Baichuan-13B-Base&lt;/a&gt;) 和对齐 (&lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan-13B-Chat&#34;&gt;Baichuan-13B-Chat&lt;/a&gt;) 两个版本。Baichuan-13B 有如下几个特点：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;更大尺寸、更多数据&lt;/strong&gt;：Baichuan-13B 在 &lt;a href=&#34;https://github.com/baichuan-inc/baichuan-7B&#34;&gt;Baichuan-7B&lt;/a&gt; 的基础上进一步扩大参数量到 130 亿，并且在高质量的语料上训练了 1.4 万亿 tokens，超过 LLaMA-13B 40%，是当前开源 13B 尺寸下训练数据量最多的模型。支持中英双语，使用 ALiBi 位置编码，上下文窗口长度为 4096。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;同时开源预训练和对齐模型&lt;/strong&gt;：预训练模型是适用开发者的『 基座 』，而广大普通用户对有对话功能的对齐模型具有更强的需求。因此本次开源我们同时发布了对齐模型（Baichuan-13B-Chat），具有很强的对话能力，开箱即用，几行代码即可简单的部署。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;更高效的推理&lt;/strong&gt;：为了支持更广大用户的使用，我们本次同时开源了 int8 和 int4 的量化版本，相对非量化版本在几乎没有效果损失的情况下大大降低了部署的机器资源门槛，可以部署在如 Nvidia 3090 这样的消费级显卡上。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;开源免费可商用&lt;/strong&gt;：Baichuan-13B 不仅对学术研究完全开放，开发者也仅需邮件申请并获得官方商用许可后，即可以免费商用。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Benchmark结果&lt;/h1&gt; &#xA;&lt;p&gt;我们在各个权威大语言模型的中英文 benchmark 上进行了&lt;code&gt;5-shot&lt;/code&gt;评测。结果如下：&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://cevalbenchmark.com/index.html#home&#34;&gt;C-Eval&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model 5-shot&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STEM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Social Sciences&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Humanities&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Others&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-Alpaca-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ziya-LLaMA-13B-Pretrain&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon-003-base (16B)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Baichuan-13B-Base&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;45.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;63.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;49.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;52.4&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Baichuan-13B-Chat&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;43.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;64.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;56.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;49.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;51.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://arxiv.org/abs/2009.03300&#34;&gt;MMLU&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model 5-shot&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STEM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Social Sciences&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Humanities&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Others&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;58.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;53.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-Alpaca-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ziya-LLaMA-13B-Pretrain&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;48.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon-003-base (16B)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;22.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Baichuan-13B-Base&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;41.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;47.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;58.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Baichuan-13B-Chat&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;40.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;60.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;48.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;59.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;52.1&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;说明：我们采用了 MMLU 官方的&lt;a href=&#34;https://github.com/hendrycks/test&#34;&gt;评测方案&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/haonan-li/CMMLU&#34;&gt;CMMLU&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model 5-shot&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;STEM&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Humanities&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Social Sciences&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Others&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;China Specific&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vicuna-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;39.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;36.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-Alpaca-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese-LLaMA-Plus-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ziya-LLaMA-13B-Pretrain&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;34.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;33.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;moss-moon-003-base (16B)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;28.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Baichuan-13B-Base&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;41.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;61.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;59.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;59.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;56.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.3&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Baichuan-13B-Chat&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;42.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;62.6&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;59.7&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;59.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;56.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;55.8&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;说明：CMMLU 是一个综合性的中文评估基准，专门用于评估语言模型在中文语境下的知识和推理能力。我们采用了其官方的&lt;a href=&#34;https://github.com/haonan-li/CMMLU&#34;&gt;评测方案&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;模型细节&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;模型名称&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;隐藏层维度&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;层数&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;注意力头数&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;词表大小&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;总参数量&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;训练数据（tokens）&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;位置编码&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;最大长度&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4,096&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64,000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7,000,559,616&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.2 万亿&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.09864&#34;&gt;RoPE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4,096&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5,120&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;64,000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13,264,901,120&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.4 万亿&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.12409&#34;&gt;ALiBi&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4,096&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;推理和部署&lt;/h1&gt; &#xA;&lt;p&gt;推理所需的模型权重、源码、配置已发布在 Hugging Face：&lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan-13B-Base&#34;&gt;Baichuan-13B-Base&lt;/a&gt; 和 &lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan-13B-Chat&#34;&gt;Baichuan-13B-Chat&lt;/a&gt;。下面以 Baichuan-13B-Chat 为例示范多种推理方式。程序会自动从 Hugging Face 下载所需资源。&lt;/p&gt; &#xA;&lt;p&gt;推理前请安装依赖：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Python代码方式&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import torch&#xA;&amp;gt;&amp;gt;&amp;gt; from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&amp;gt;&amp;gt;&amp;gt; from transformers.generation.utils import GenerationConfig&#xA;&amp;gt;&amp;gt;&amp;gt; tokenizer = AutoTokenizer.from_pretrained(&#34;baichuan-inc/Baichuan-13B-Chat&#34;, use_fast=False, trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model = AutoModelForCausalLM.from_pretrained(&#34;baichuan-inc/Baichuan-13B-Chat&#34;, device_map=&#34;auto&#34;, torch_dtype=torch.float16, trust_remote_code=True)&#xA;&amp;gt;&amp;gt;&amp;gt; model.generation_config = GenerationConfig.from_pretrained(&#34;baichuan-inc/Baichuan-13B-Chat&#34;)&#xA;&amp;gt;&amp;gt;&amp;gt; messages = []&#xA;&amp;gt;&amp;gt;&amp;gt; messages.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;世界上第二高的山峰是哪座&#34;})&#xA;&amp;gt;&amp;gt;&amp;gt; response = model.chat(tokenizer, messages)&#xA;&amp;gt;&amp;gt;&amp;gt; print(response)&#xA;乔戈里峰。世界第二高峰———乔戈里峰西方登山者称其为k2峰，海拔高度是8611米，位于喀喇昆仑山脉的中巴边境上&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;在上述代码中，模型加载指定 &lt;code&gt;device_map=&#39;auto&#39;&lt;/code&gt;，会使用所有可用显卡。如需指定使用的设备，可以使用类似 &lt;code&gt;export CUDA_VISIBLE_DEVICES=0,1&lt;/code&gt;（使用了0、1号显卡）的方式控制。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;命令行工具方式&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python cli_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;最后输出示例如下：&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/baichuan-inc/Baichuan-13B/main/media/cn-cli.png&#34; width=&#34;70%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;网页 demo 方式&lt;/h2&gt; &#xA;&lt;p&gt;依靠streamlit运行以下命令，会在本地启动一个 web 服务，把控制台给出的地址放入浏览器即可访问。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;streamlit run web_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;效果如下：&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/baichuan-inc/Baichuan-13B/main/media/cn-web.gif&#34; width=&#34;70%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Baichuan-13B-Chat 示例输出&lt;/h2&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;内容创作&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;用户：&#xA;你是一个内容创作者，编写一段关于对马斯克和比尔盖茨访问中国视角的短视频脚本&#xA;&#xA;Baichun-13B-Chat：&#xA;探索马斯克与比尔·盖茨的中国之旅&#xA;&#xA;镜头1：(背景音乐)画面切入，展示中国的繁华都市景象。&#xA;&#xA;旁白：欢迎来到我们的节目，今天我们将带您一起了解两位科技巨头——埃隆·马斯克和比尔·盖茨在中国的一次特殊访问。&#xA;&#xA;镜头2：(背景音乐)画面切换至马斯克和比尔·盖茨抵达中国的机场。&#xA;&#xA;旁白：这次访问是他们首次共同踏上中国的土地，他们将在这里展开一场关于科技创新、环保和教育的对话。&#xA;&#xA;镜头3：(背景音乐)画面展示马斯克和比尔·盖茨参观一家电动汽车制造厂。&#xA;&#xA;旁白：在这次访问中，他们首先参观了一家电动汽车制造厂，了解了中国在新能源汽车领域的最新发展。&#xA;&#xA;镜头4：(背景音乐)画面展示马斯克和比尔·盖茨参加一场关于人工智能的论坛。&#xA;&#xA;旁白：随后，他们参加了关于人工智能的一场论坛，探讨了这一领域的发展趋势及其对未来的影响。&#xA;&#xA;镜头5：(背景音乐)画面展示马斯克和比尔·盖茨参观一所学校。&#xA;&#xA;旁白：在这次访问中，他们还参观了一所学校，了解了中国在教育领域的创新实践。&#xA;&#xA;镜头6：(背景音乐)画面展示马斯克和比尔·盖茨接受媒体采访。&#xA;&#xA;旁白：最后，他们在媒体采访环节表示，希望通过这次访问，能够为中美两国在科技创新、环保和教育等领域的合作提供一些启示。&#xA;&#xA;镜头7：(背景音乐)画面切回繁华都市景象。&#xA;&#xA;旁白：这就是马斯克和比尔·盖茨的中国之行，他们的访问不仅让我们看到了中国在科技创新方面的巨大潜力，也为我们提供了关于未来发展的更多思考。感谢收看，我们下期再见！&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;广告文案&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;用户：&#xA;请以销售员的角色，将洗衣液的干净、无残留、无异味以及除菌等优点进行推销&#xA;&#xA;Baichun-13B-Chat：&#xA;尊敬的顾客，您好！我是一名洗衣液销售员，非常高兴为您介绍我们这款产品。我们的洗衣液具有以下几个显著优点：&#xA;&#xA;干净：我们的洗衣液采用了先进的洗涤技术，能够深入衣物纤维，有效去除各种污渍，让您的衣物焕然一新。同时，它不会对衣物的颜色和质地造成任何损害，让您的衣物保持原有的品质。&#xA;&#xA;无残留：与传统的洗衣粉相比，我们的洗衣液在清洗过程中不会产生过多的泡沫，因此清洗后的衣物更加柔软，不会有黏腻的感觉。此外，它的去污能力强，能彻底洗净衣物上的污渍，避免了传统洗衣粉容易残留在衣物上的问题。&#xA;&#xA;无异味：我们的洗衣液采用天然植物提取物作为原料，不含有害化学物质，因此在使用过程中不会产生刺鼻的气味。即使是敏感肌肤的人群也可以放心使用，让您的衣物散发出淡雅的清香。&#xA;&#xA;除菌：为了满足现代人对健康生活的需求，我们的洗衣液特别添加了高效除菌成分，能够有效杀灭衣物上的细菌，保护您和家人的身体健康。无论是婴幼儿的衣物还是贴身内衣，都可以放心使用。&#xA;&#xA;总之，我们的洗衣液凭借其干净、无残留、无异味以及除菌等优点，成为了广大消费者的首选。如果您还没有尝试过我们的产品，那么现在就是最好的时机。相信它一定会给您带来全新的洗衣体验！&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code&gt;用户：&#xA;准备一份以宠物食品为主题的报告，包括潜在消费者，市场规模，市场机遇，及盈利模式等内容&#xA;&#xA;&#xA;Baichun-13B-Chat：&#xA;&#xA;一、引言&#xA;&#xA;随着人们生活水平的提高和对宠物的关爱程度不断加深，宠物食品行业得到了快速发展。本报告将对宠物食品行业的潜在消费者、市场规模、市场机遇以及盈利模式等方面进行分析，以期为相关企业提供有益的参考信息。&#xA;&#xA;二、潜在消费者&#xA;&#xA;1. 养宠人群：随着养宠人数的增加，宠物食品的需求也在不断增长。根据统计数据显示，2019年中国城镇养宠(犬猫)人口数量已达7355万人，同比增长4.6%。预计到2023年，这一数字将达到8742万人。&#xA;&#xA;2. 新生代消费者：新生代消费者在消费观念上更加注重健康、环保和个性化，这使得他们更愿意为宠物购买高品质的食品。此外，他们也更倾向于通过网络平台了解和购买宠物食品。&#xA;&#xA;三、市场规模&#xA;&#xA;1. 根据国家统计局数据，2019年我国宠物食品市场规模达到了1,020亿元人民币，同比增长约10%。预计到2023年，市场规模将达到1,  500亿元人民币。&#xA;&#xA;2. 从产品类型来看，宠物干粮市场占比最高，约占总市场的70%;其次是宠物湿粮，占比约为20%;宠物零食市场占比约为10%。&#xA;&#xA;四、市场机遇&#xA;&#xA;1. 功能性宠物食品：随着消费者对健康的关注度不断提高，具有特定功能的宠物食品如处方粮、减肥粮等需求逐渐增大。&#xA;&#xA;2. 天然有机宠物食品：越来越多的消费者倾向于选择天然、无添加剂的宠物食品，这也为市场带来了新的商机。&#xA;&#xA;3. 定制化宠物食品：针对不同品种、年龄、体型的宠物，推出定制化的宠物食品，满足消费者个性化需求。&#xA;&#xA;五、盈利模式&#xA;&#xA;1. 传统销售渠道：通过超市、宠物店、电商平台等传统渠道销售宠物食品。&#xA;&#xA;2. 直销模式：通过自建网站或与第三方平台合作，直接向消费者销售宠物食品。&#xA;&#xA;3. 会员制：通过设立会员制度，为会员提供专属优惠和服务，增加客户粘性。&#xA;&#xA;4. 增值服务：提供宠物美容、寄养、训练等附加服务，实现多元化盈利。&#xA;&#xA;六、结论&#xA;&#xA;宠物食品行业具有巨大的市场潜力和发展空间。企业应抓住市场机遇，关注消费者需求变化，创新产品&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;精准问答&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;用户：&#xA;世界上第二高的山是什么山&#xA;&#xA;Baichun-13B-Chat：&#xA;乔戈里峰(K2) 海拔8611米，位于喀喇昆仑山脉的中巴边境上，是中国以外登山者的最大考验，也是世界上最难征服的山峰之一。它的高度仅次于珠穆朗玛峰，位居世界第二。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code&gt;用户：&#xA;意兴阑珊是什么意思&#xA;&#xA;Baichun-13B-Chat：&#xA;&#34;意兴阑珊&#34;是一个汉语成语，它的意思是形容一个人兴致不高，兴趣减弱。这个词通常用来描述一个人在某个活动中失去了原有的热情和兴趣。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code&gt;用户：&#xA;请问强化学习里面的Reward Shaping是什么意思&#xA;&#xA;Baichun-13B-Chat：&#xA;奖励塑造(Reward Shaping)是强化学习中的一个概念，它是指通过调整环境或设计特定的奖励函数来引导智能体(Agent)学习到更符合人类期望的行为。在传统的强化学习中，智能体通过与环境交互来学习最优策略，但这种方法可能导致智能体学习到一些次优甚至不良行为。&#xA;&#xA;奖励塑造的目的是为了解决这个问题，通过人为地调整奖励函数，使得智能体更容易学到我们希望它学会的行为。这可以通过以下几种方式实现：&#xA;&#xA;1. 增加正向奖励：当智能体表现出期望行为时，给予其更多的奖励，以鼓励这种行为。&#xA;2. 减少负向奖励：当智能体表现出不期望行为时，减少对其进行惩罚，降低其对这种行为的偏好。&#xA;3. 引入新奖励：为智能体提供额外的奖励，以便更好地引导其学习特定行为。&#xA;4. 改变奖励分布：通过调整奖励函数，使其具有不同的奖励分布，从而影响智能体的学习过程。&#xA;&#xA;总之，奖励塑造是一种通过调整环境和奖励函数来引导智能体学习更好行为的方法，有助于提高强化学习的性能。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;&lt;b&gt;语言理解&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;用户：&#xA;小张给领导买了一个礼物说，说“意思一下”，领导说“什么意思”，小张回答说：“没什么意思，就是随便意思意思”。请依次解读一下小张说的这几个“意思”的含义。&#xA;&#xA;Baichun-13B-Chat：&#xA;1. 第一个“意思”：指的是小张送给领导的那个礼物，表示他已经买了这个礼物并准备送给他。&#xA;2. 第二个“意思”：指的是小张对领导的一种礼貌和尊重，表示自己并没有忘记他的身份和地位，在购买礼物时也考虑到了这一点。&#xA;3. 第三个“意思”：指的是小张对于这次赠送礼物的行为本身的态度，即不希望这件事情引起过多的关注或争议，只是想表达一种心意而已。&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;推理性能&lt;/h2&gt; &#xA;&lt;p&gt;Baichuan-13B 使用了 ALiBi 线性偏置技术，相对于 Rotary Embedding 计算量更小，对推理性能有显著提升；与标准的 LLaMA-13B 相比，平均推理速度 (tokens/s) 实测提升 31.6%：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;tokens/s&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;19.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;测试环境和参数：GPU A100-SXM4-80G, PyTorch 2.0.0+cu117, transformers 4.29.1, batch size = 1, 生成长度 = 2048, 精度 fp16, 基于 Baichuan-13B-Base&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;量化部署&lt;/h2&gt; &#xA;&lt;p&gt;Baichuan-13B 支持 int8 和 int4 量化，用户只需在推理代码中简单修改两行即可实现。请注意，如果是为了节省显存而进行量化，应加载原始精度模型到 CPU 后再开始量化；避免在&lt;code&gt;from_pretrained&lt;/code&gt;时添加&lt;code&gt;device_map=&#39;auto&#39;&lt;/code&gt;或者其它会导致把原始精度模型直接加载到 GPU 的行为的参数。&lt;/p&gt; &#xA;&lt;p&gt;如需使用 int8 量化：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#34;baichuan-inc/Baichuan-13B-Chat&#34;, torch_dtype=torch.float16, trust_remote_code=True)&#xA;model = model.quantize(8).cuda() &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;同样的，如需使用 int4 量化：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#34;baichuan-inc/Baichuan-13B-Chat&#34;, torch_dtype=torch.float16, trust_remote_code=True)&#xA;model = model.quantize(4).cuda()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;另外，如果你不想调用 quantize 在线量化，我们有量化好的 int8 Chat 模型可供使用：&lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan-13B-Chat-int8&#34;&gt;Baichuan-13B-Chat-int8&lt;/a&gt;：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#34;baichuan-inc/Baichuan-13B-Chat-int8&#34;, torch_dtype=torch.float16, trust_remote_code=True).cuda()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;量化前后占用显存情况如下：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;GPU Mem (GB)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;bf16 / fp16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;26.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;int8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;15.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;量化后在各个 benchmark 上的结果和原始版本对比如下：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model 5-shot&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;C-Eval&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MMLU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;CMMLU&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan-13B-Base&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan-13B-Base-int8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Baichuan-13B-Base-int4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;47.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;CPU 部署&lt;/h2&gt; &#xA;&lt;p&gt;Baichuan-13B 支持 CPU 推理，但需要强调的是，CPU 的推理速度相对较慢。需按如下方式修改模型加载的方式：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = AutoModelForCausalLM.from_pretrained(&#34;baichuan-inc/Baichuan-13B-Chat&#34;, torch_dtype=torch.float32, trust_remote_code=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;使用CPU进行推理大概需要 60GB 内存。&lt;/p&gt; &#xA;&lt;h1&gt;声明&lt;/h1&gt; &#xA;&lt;p&gt;我们在此声明，我们的开发团队并未基于 Baichuan-13B 模型开发任何应用，无论是在 iOS、Android、网页或任何其他平台。我们强烈呼吁所有使用者，不要利用 Baichuan-13B 模型进行任何危害国家社会安全或违法的活动。另外，我们也要求使用者不要将 Baichuan-13B 模型用于未经适当安全审查和备案的互联网服务。我们希望所有的使用者都能遵守这个原则，确保科技的发展能在规范和合法的环境下进行。&lt;/p&gt; &#xA;&lt;p&gt;我们已经尽我们所能，来确保模型训练过程中使用的数据的合规性。然而，尽管我们已经做出了巨大的努力，但由于模型和数据的复杂性，仍有可能存在一些无法预见的问题。因此，如果由于使用 Baichuan-13B 开源模型而导致的任何问题，包括但不限于数据安全问题、公共舆论风险，或模型被误导、滥用、传播或不当利用所带来的任何风险和问题，我们将不承担任何责任。&lt;/p&gt; &#xA;&lt;h1&gt;协议&lt;/h1&gt; &#xA;&lt;p&gt;对本仓库源码的使用遵循开源许可协议 &lt;a href=&#34;https://github.com/baichuan-inc/Baichuan-13B/raw/main/LICENSE&#34;&gt;Apache 2.0&lt;/a&gt;。对 Baichuan-13B 模型的社区使用见&lt;a href=&#34;https://huggingface.co/baichuan-inc/Baichuan-13B-Chat/resolve/main/Baichuan-13B%20%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf&#34;&gt;《Baichuan-13B 模型社区许可协议》&lt;/a&gt;。Baichuan-13B 支持商用。如果将 Baichuan-13B 模型或其衍生品用作商业用途，请您按照如下方式联系许可方，以进行登记并向许可方申请书面授权：联系邮箱 &lt;a href=&#34;mailto:opensource@baichuan-inc.com&#34;&gt;opensource@baichuan-inc.com&lt;/a&gt;。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OpenLMLab/MOSS-RLHF</title>
    <updated>2023-07-14T01:30:42Z</updated>
    <id>tag:github.com,2023-07-14:/OpenLMLab/MOSS-RLHF</id>
    <link href="https://github.com/OpenLMLab/MOSS-RLHF" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MOSS-RLHF&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;MOSS-RLHF&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;em&gt;MOSS-RLHF &amp;amp; &#34;Secrets of RLHF in Large Language Models Part I: PPO&#34; &lt;br&gt;👉 &lt;a href=&#34;https://arxiv.org/abs/2307.04964&#34; target=&#34;_blank&#34;&gt;[Technical report]&lt;/a&gt; &lt;a href=&#34;https://openlmlab.github.io/MOSS-RLHF/&#34; target=&#34;_blank&#34;&gt;[Home page]&lt;/a&gt;&lt;/em&gt;&lt;/h3&gt;&#xA;&lt;a href=&#34;https://openlmlab.github.io/MOSS-RLHF/&#34; target=&#34;_blank&#34;&gt; &lt;/a&gt;&#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt;&lt;a href=&#34;https://openlmlab.github.io/MOSS-RLHF/&#34; target=&#34;_blank&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.04964&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenLMLab/MOSS-RLHF/main/assets/img/moss.png&#34; alt=&#34;MOSS&#34; style=&#34;width: 50%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenLMLab/MOSS-RLHF/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-brightgreen.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenLMLab/MOSS-RLHF/main/DATA_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Data%20License-CC%20BY--NC%204.0-blue.svg?sanitize=true&#34; alt=&#34;Data License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenLMLab/MOSS-RLHF/main/MODEL_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Model%20License-GNU%20AGPL%203.0-red.svg?sanitize=true&#34; alt=&#34;Model License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🌟 News&lt;/h2&gt; &#xA;&lt;h3&gt;👉 Wed, 12. July 2023. We have released Chinese reward model based OpenChineseLlama-7B!&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Ablustrund/moss-rlhf-reward-model-7B-zh/tree/main&#34;&gt;moss-rlhf-reward-model-7B-zh&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🧾 Open-source List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Open source code for RL training in large language models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; A 7B Chinese reward model based on openChineseLlama.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; A 7B English reward model based on Llama-7B.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; SFT model for English.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Policy model for English after RLHF.&lt;/li&gt; &#xA; &lt;li&gt;...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🌠 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In this technical report, we intend to help researchers to train their models stably with human feedback.&lt;/p&gt; &#xA;&lt;p&gt;Contributions are summarized as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;We release competitive Chinese and English reward models, respectively, which have good cross-model generalization ability, alleviating the cost of relabeling human preference data;&lt;/li&gt; &#xA; &lt;li&gt;We conduct in-depth analysis on the inner workings of PPO algorithm and propose the PPO-max algorithm to ensure stable model training;&lt;/li&gt; &#xA; &lt;li&gt;We release the complete PPO-max codes to ensure that the LLMs in the current SFT stage can be better aligned with humans.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34; width=&#34;100%&#34;&gt; &#xA; &lt;img style=&#34;width: 80%; min-width: 500px; display: block; margin: auto; margin-bottom: 20px&#34; alt=&#34;MOSS-RLHF&#34; src=&#34;https://raw.githubusercontent.com/OpenLMLab/MOSS-RLHF/main/assets/img/img1.jpg&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34; width=&#34;100%&#34;&gt; &#xA; &lt;img style=&#34;width: 80%; min-width: 500px; display: block; margin: auto; margin-bottom: 20px&#34; alt=&#34;MOSS-RLHF&#34; src=&#34;https://raw.githubusercontent.com/OpenLMLab/MOSS-RLHF/main/assets/img/img2.jpg&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;🔩 Requirements &amp;amp; Setup&lt;/h2&gt; &#xA;&lt;p&gt;This repository works on Python 3.8 and PyTorch 1.13.1.&lt;/p&gt; &#xA;&lt;p&gt;We recommend using the &lt;strong&gt;conda&lt;/strong&gt; virtual environment to run the code.&lt;/p&gt; &#xA;&lt;h4&gt;Step 1: Create a new Python virtual environment&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda update conda -n base -c defaults&#xA;conda create -n rlhf python=3.8&#xA;conda activate rlhf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 2: Install PyTorch and TensorBoard&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install pytorch==1.13.1 pytorch-cuda=11.7 tensorboard -c pytorch -c nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Step 3: Install the remaining dependencies&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install datasets accelerate safetensors chardet cchardet -c huggingface -c conda-forge&#xA;pip3 install transformers sentencepiece einops triton==1.0.0 rouge jionlp==1.4.14 nltk sacrebleu cpm_kernels&#xA;&#xA;apt install libaio-dev&#xA;DS_BUILD_OPS=1 pip install deepspeed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;✨ Start training your own model!&lt;/h2&gt; &#xA;&lt;p&gt;Run code in a few steps.&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Recover Reward model weights&lt;/h3&gt; &#xA;&lt;p&gt;We can not directly release the full weight of the reward model because of protocol restrictions. You can merge the diff weight with original Llama-7B to recover the reward model we used.&lt;/p&gt; &#xA;&lt;p&gt;We upload the diff models, thanks to tatsu-lab, you can recover the reward model follow these steps:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;1) Download the weight diff into your local machine. The weight diff is located at:&#xA;# For English:&#xA;TODO&#xA;# For Chinese:&#xA;https://huggingface.co/Ablustrund/moss-rlhf-reward-model-7B-zh/tree/main&#xA;&#xA;2) Merge the weight diff with the original Llama-7B:&#xA;# For English:&#xA;TODO&#xA;# For Chinese:&#xA;python merge_weight_zh.py recover --path_raw decapoda-research/llama-7b-hf --path_diff ./models/moss-rlhf-reward-model-7B-zh/diff --path_tuned ./models/moss-rlhf-reward-model-7B-zh/recover&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Select your own SFT model&lt;/h3&gt; &#xA;&lt;p&gt;Because of some limitations, we can not release the &lt;strong&gt;Chinese&lt;/strong&gt; SFT model (Currently). You can use your own SFT model, or a strong base model instead of our SFT model.&lt;/p&gt; &#xA;&lt;h3&gt;Step 3: Start training&lt;/h3&gt; &#xA;&lt;p&gt;Run the command below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# For Chinese:&#xA;bash run_zh.sh&#xA;&#xA;# For English:&#xA;TODO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zheng2023secrets,&#xA;      title={Secrets of RLHF in Large Language Models Part I: PPO}, &#xA;      author={Rui Zheng and Shihan Dou and Songyang Gao and Wei Shen and Binghai Wang and Yan Liu and Senjie Jin and Qin Liu and Limao Xiong and Lu Chen and Zhiheng Xi and Yuhao Zhou and Nuo Xu and Wenbin Lai and Minghao Zhu and Rongxiang Weng and Wensen Cheng and Cheng Chang and Zhangyue Yin and Yuan Hua and Haoran Huang and Tianxiang Sun and Hang Yan and Tao Gui and Qi Zhang and Xipeng Qiu and Xuanjing Huang},&#xA;      year={2023},&#xA;      eprint={2307.04964},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>assafelovic/gpt-researcher</title>
    <updated>2023-07-14T01:30:42Z</updated>
    <id>tag:github.com,2023-07-14:/assafelovic/gpt-researcher</id>
    <link href="https://github.com/assafelovic/gpt-researcher" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GPT based autonomous agent that does online comprehensive research on any given topic&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;🔎 GPT Researcher&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.producthunt.com/posts/gpt-researcher?utm_source=badge-featured&amp;amp;utm_medium=badge&amp;amp;utm_souce=badge-gpt-researcher&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=404302&amp;amp;theme=light&#34; alt=&#34;GPT Researcher - Autonomous agent designed for comprehensive online research | Product Hunt&#34; style=&#34;width: 250px; height: 54px;&#34; width=&#34;250&#34; height=&#34;54&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://tavily.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Official%20Website-tavily.com-blue?style=flat&amp;amp;logo=world&amp;amp;logoColor=white&#34; alt=&#34;Official Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.com/invite/rqw8dnM8&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/rqw8dnM8?style=flat&#34; alt=&#34;Discord Follow&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/assafelovic/gpt-researcher&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/assafelovic/gpt-researcher?style=social&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/assaf_elovic&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/assaf_elovic?style=social&#34; alt=&#34;Twitter Follow&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GPT Researcher is an autonomous agent designed for comprehensive online research on a variety of tasks.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The agent can produce detailed, factual and unbiased research reports, with customization options for focusing on relevant resources, outlines, and lessons. Inspired by &lt;a href=&#34;https://github.com/Significant-Gravitas/Auto-GPT&#34;&gt;AutoGPT&lt;/a&gt; and the recent &lt;a href=&#34;https://arxiv.org/abs/2305.04091&#34;&gt;Plan-and-Solve&lt;/a&gt; paper, GPT Researcher addresses issues of speed and determinism, offering a more stable performance and increased speed through parallelized agent work, as opposed to synchronous operations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Our mission is to empower individuals and organizations with accurate, unbiased, and factual information by leveraging the power of AI.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Why GPT Researcher?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To form objective conclusions for manual research tasks can take time, sometimes weeks to find the right resources and information.&lt;/li&gt; &#xA; &lt;li&gt;Current LLMs are trained on past and outdated information, with heavy risks of hallucinations, making them almost irrelevant for research tasks.&lt;/li&gt; &#xA; &lt;li&gt;Solutions that enable web search (such as ChatGPT + Web Plugin), only consider limited resources that in some cases result in superficial conclusions or biased answers.&lt;/li&gt; &#xA; &lt;li&gt;Using only a selection of resources can create bias in determining the right conclusions for research questions or tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The main idea is to run &#34;planner&#34; and &#34;execution&#34; agents, whereas the planner generates questions to research, and the execution agents seek the most related information based on each generated research question. Finally, the planner filters and aggregates all related information and creates a research report. The agents leverage both gpt3.5-turbo-16k and gpt-4 to complete a research task.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img align=&#34;center&#34; height=&#34;500&#34; src=&#34;https://cowriter-images.s3.amazonaws.com/arch.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;More specifcally:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generate a set of research questions that together form an objective opinion on any given task.&lt;/li&gt; &#xA; &lt;li&gt;For each research question, trigger a crawler agent that scrapes online resources for information relevant to the given task.&lt;/li&gt; &#xA; &lt;li&gt;For each scraped resources, summarize based on relevant information and keep track of its sources.&lt;/li&gt; &#xA; &lt;li&gt;Finally, filter and aggregate all summarized sources and generate a final research report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/assafelovic/gpt-researcher/assets/13554167/a00c89a6-a295-4dd0-b58d-098a31c40fda&#34;&gt;https://github.com/assafelovic/gpt-researcher/assets/13554167/a00c89a6-a295-4dd0-b58d-098a31c40fda&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📝 Generate research, outlines, resources and lessons reports&lt;/li&gt; &#xA; &lt;li&gt;🌐 Aggregates over 20 web sources per research to form objective and factual conclusions&lt;/li&gt; &#xA; &lt;li&gt;🖥️ Includes an easy-to-use web interface (HTML/CSS/JS)&lt;/li&gt; &#xA; &lt;li&gt;🔍 Scrapes web sources with javascript support&lt;/li&gt; &#xA; &lt;li&gt;📂 Keeps track and context of visited and used web sources&lt;/li&gt; &#xA; &lt;li&gt;📄 Export research reports to PDF and more...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 0&lt;/strong&gt; - Install Python 3.11 or later. &lt;a href=&#34;https://www.tutorialsteacher.com/python/install-python&#34;&gt;See here&lt;/a&gt; for a step-by-step guide.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - Download the project&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/assafelovic/gpt-researcher.git&#xA;$ cd gpt-researcher&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Install dependencies&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Create .env file with your OpenAI Key or simply export it&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export OPENAI_API_KEY={Your API Key here}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - Run the agent with FastAPI&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ uvicorn main:app --reload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt; - Go to &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt; on any browser and enjoy researching!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;update:&lt;/strong&gt; if you are having issues with weasyprint, please visit their website and follow the installation instructions: &lt;a href=&#34;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&#34;&gt;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Try it with Docker&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt; - Install Docker&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Follow instructions at &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;https://docs.docker.com/engine/install/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt; - Create .env file with your OpenAI Key or simply export it&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ export OPENAI_API_KEY={Your API Key here}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt; - Run the application&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt; - Go to &lt;a href=&#34;http://localhost:8000&#34;&gt;http://localhost:8000&lt;/a&gt; on any browser and enjoy researching!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;update:&lt;/strong&gt; if you are having issues with weasyprint, please visit their website and follow the installation instructions: &lt;a href=&#34;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&#34;&gt;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🛡 Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project, GPT Researcher, is an experimental application and is provided &#34;as-is&#34; without any warranty, express or implied. We are sharing codes for academic purposes under the MIT education license. Nothing herein is academic advice, and NOT a recommendation to use in academic or research papers.&lt;/p&gt; &#xA;&lt;p&gt;Our view on unbiased research claims:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The whole point of our scraping system is to reduce incorrect fact. How? The more sites we scrape the less chances of incorrect data. We are scraping 20 per research, the chances that they are all wrong is extremely low.&lt;/li&gt; &#xA; &lt;li&gt;We do not aim to eliminate biases; we aim to reduce it as much as possible. &lt;strong&gt;We are here as a community to figure out the most effective human/llm interactions.&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;In research, people also tend towards biases as most have already opinions on the topics they research about. This tool scrapes many opinions and will evenly explain diverse views that a biased person would never have read.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note that the use of the GPT-4 language model can be expensive due to its token usage.&lt;/strong&gt; By utilizing this project, you acknowledge that you are responsible for monitoring and managing your own token usage and the associated costs. It is highly recommended to check your OpenAI API usage regularly and set up any necessary limits or alerts to prevent unexpected charges.&lt;/p&gt; &#xA;&lt;h2&gt;🔧 Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;re constantly working to provide a more stable version. In the meantime, see here for known issues:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;cannot load library &#39;gobject-2.0-0&#39;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The issue relates to the library WeasyPrint (which is used to generate PDFs from the research report). Please follow this guide to resolve it: &lt;a href=&#34;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&#34;&gt;https://doc.courtbouillon.org/weasyprint/stable/first_steps.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Error processing the url&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;We&#39;re using &lt;a href=&#34;https://www.selenium.dev&#34;&gt;Selenium&lt;/a&gt; for site scraping. Some sites fail to be scraped. In these cases, restart and try running again.&lt;/p&gt;</summary>
  </entry>
</feed>