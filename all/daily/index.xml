<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-11-21T01:32:58Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ShoufaChen/DiffusionDet</title>
    <updated>2022-11-21T01:32:58Z</updated>
    <id>tag:github.com,2022-11-21:/ShoufaChen/DiffusionDet</id>
    <link href="https://github.com/ShoufaChen/DiffusionDet" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch implementation of DiffusionDet (https://arxiv.org/abs/2211.09788)&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;DiffusionDet: Diffusion Model for Object Detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;DiffusionDet is the first work of diffusion model for object detection.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ShoufaChen/DiffusionDet/main/teaser.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.09788&#34;&gt;&lt;strong&gt;DiffusionDet: Diffusion Model for Object Detection&lt;/strong&gt;&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://www.shoufachen.com/&#34;&gt;Shoufa Chen&lt;/a&gt;, &lt;a href=&#34;https://peizesun.github.io/&#34;&gt;Peize Sun&lt;/a&gt;, &lt;a href=&#34;https://ybsong00.github.io/&#34;&gt;Yibing Song&lt;/a&gt;, &lt;a href=&#34;http://luoping.me/&#34;&gt;Ping Luo&lt;/a&gt;&lt;br&gt; &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.09788&#34;&gt;arXiv 2211.09788&lt;/a&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(11/2022) Code is released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Box AP (1 step)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Box AP (4 step)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShoufaChen/DiffusionDet/main/configs/diffdet.coco.res50.yaml&#34;&gt;COCO-Res50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;45.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ShoufaChen/DiffusionDet/releases/download/v0.1/diffdet_coco_res50.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShoufaChen/DiffusionDet/main/configs/diffdet.coco.res101.yaml&#34;&gt;COCO-Res101&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ShoufaChen/DiffusionDet/releases/download/v0.1/diffdet_coco_res101.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShoufaChen/DiffusionDet/main/configs/diffdet.coco.swinbase.yaml&#34;&gt;COCO-SwinBase&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;52.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ShoufaChen/DiffusionDet/releases/download/v0.1/diffdet_coco_swinbase.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShoufaChen/DiffusionDet/main/configs/diffdet.lvis.res50.yaml&#34;&gt;LVIS-Res50&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ShoufaChen/DiffusionDet/releases/download/v0.1/diffdet_lvis_res50.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShoufaChen/DiffusionDet/main/configs/diffdet.lvis.res101.yaml&#34;&gt;LVIS-Res101&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;31.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;32.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ShoufaChen/DiffusionDet/releases/download/v0.1/diffdet_lvis_res101.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ShoufaChen/DiffusionDet/main/configs/diffdet.lvis.swinbase.yaml&#34;&gt;LVIS-SwinBase&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;40.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;41.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/ShoufaChen/DiffusionDet/releases/download/v0.1/diffdet_lvis_swinbase.pth&#34;&gt;model&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;The installation instruction and usage are in &lt;a href=&#34;https://raw.githubusercontent.com/ShoufaChen/DiffusionDet/main/GETTING_STARTED.md&#34;&gt;Getting Started with DiffusionDet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is under the CC-BY-NC 4.0 license. See &lt;a href=&#34;https://raw.githubusercontent.com/ShoufaChen/DiffusionDet/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for details.&lt;/p&gt; &#xA;&lt;h2&gt;Citing DiffusionDet&lt;/h2&gt; &#xA;&lt;p&gt;If you use DiffusionDet in your research or wish to refer to the baseline results published here, please use the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{chen2022diffusiondet,&#xA;      title={DiffusionDet: Diffusion Model for Object Detection},&#xA;      author={Chen, Shoufa and Sun, Peize and Song, Yibing and Luo, Ping},&#xA;      journal={arXiv preprint arXiv:2211.09788},&#xA;      year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>ggerganov/whisper.cpp</title>
    <updated>2022-11-21T01:32:58Z</updated>
    <id>tag:github.com,2022-11-21:/ggerganov/whisper.cpp</id>
    <link href="https://github.com/ggerganov/whisper.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Port of OpenAI&#39;s Whisper model in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;whisper.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/actions&#34;&gt;&lt;img src=&#34;https://github.com/ggerganov/whisper.cpp/workflows/CI/badge.svg?sanitize=true&#34; alt=&#34;Actions Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;High-performance inference of &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;OpenAI&#39;s Whisper&lt;/a&gt; automatic speech recognition (ASR) model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation without dependencies&lt;/li&gt; &#xA; &lt;li&gt;Apple silicon first-class citizen - optimized via Arm Neon and Accelerate framework&lt;/li&gt; &#xA; &lt;li&gt;AVX intrinsics support for x86 architectures&lt;/li&gt; &#xA; &lt;li&gt;Mixed F16 / F32 precision&lt;/li&gt; &#xA; &lt;li&gt;Low memory usage (Flash Attention + Flash Forward)&lt;/li&gt; &#xA; &lt;li&gt;Zero memory allocations at runtime&lt;/li&gt; &#xA; &lt;li&gt;Runs on the CPU&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/raw/master/whisper.h&#34;&gt;C-style API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Supported platforms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mac OS (Intel and Arm)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples/whisper.objc&#34;&gt;iOS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Linux&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples/whisper.wasm&#34;&gt;WebAssembly&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/issues/5&#34;&gt;Windows (MSVC and MinGW)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/issues/7&#34;&gt;Raspberry Pi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/issues/30&#34;&gt;Android&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The entire implementation of the model is contained in 2 source files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tensor operations: &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/ggml.h&#34;&gt;ggml.h&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/ggml.c&#34;&gt;ggml.c&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Transformer inference: &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/whisper.h&#34;&gt;whisper.h&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Having such a lightweight implementation of the model allows to easily integrate it in different platforms and applications. As an example, here is a video of running the model on an iPhone 13 device - fully offline, on-device:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/197385372-962a6dea-bca1-4d50-bf96-1d8c27b98c81.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/197385372-962a6dea-bca1-4d50-bf96-1d8c27b98c81.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Implementation details&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The core tensor operations are implemented in C (&lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/ggml.h&#34;&gt;ggml.h&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/ggml.c&#34;&gt;ggml.c&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;The transformer model and the high-level C-style API are implemented in C++ (&lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/whisper.h&#34;&gt;whisper.h&lt;/a&gt; / &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/whisper.cpp&#34;&gt;whisper.cpp&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Sample usage is demonstrated in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples/main&#34;&gt;main.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Sample real-time audio transcription from the microphone is demonstrated in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples/stream&#34;&gt;stream.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Various other examples are available in the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples&#34;&gt;examples&lt;/a&gt; folder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The tensor operators are optimized heavily for Apple silicon CPUs. Depending on the computation size, Arm Neon SIMD instrisics or CBLAS Accelerate framework routines are used. The latter are especially effective for bigger sizes since the Accelerate framework utilizes the special-purpose AMX coprocessor available in modern Apple products.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Inference only&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;No GPU support&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Very basic greedy sampling scheme - always pick up the token with highest probability. This should be similar to the &lt;a href=&#34;https://github.com/openai/whisper/raw/main/whisper/decoding.py#L249-L274&#34;&gt;GreedyDecoder&lt;/a&gt; from the original python implementation, so in order to make a fair comparison between the 2 implementations, make sure to run the python code with the following parameters:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;whisper --best_of None --beam_size None ...&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the future, &lt;code&gt;whisper.cpp&lt;/code&gt; will support more sampling strategies.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;First, download one of the Whisper models converted in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/models&#34;&gt;ggml format&lt;/a&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash ./models/download-ggml-model.sh base.en&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now build the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples/main&#34;&gt;main&lt;/a&gt; example and transcribe an audio file like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# build the main example&#xA;make&#xA;&#xA;# transcribe an audio file&#xA;./main -f input.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For a quick demo, simply run &lt;code&gt;make base.en&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;$ make base.en&#xA;&#xA;cc  -I.              -O3 -std=c11   -pthread -DGGML_USE_ACCELERATE   -c ggml.c -o ggml.o&#xA;c++ -I. -I./examples -O3 -std=c++11 -pthread -c whisper.cpp -o whisper.o&#xA;c++ -I. -I./examples -O3 -std=c++11 -pthread examples/main/main.cpp whisper.o ggml.o -o main  -framework Accelerate&#xA;./main -h&#xA;&#xA;usage: ./main [options] file0.wav file1.wav ...&#xA;&#xA;options:&#xA;  -h,       --help           show this help message and exit&#xA;  -s SEED,  --seed SEED      RNG seed (default: -1)&#xA;  -t N,     --threads N      number of threads to use during computation (default: 4)&#xA;  -p N,     --processors N   number of processors to use during computation (default: 1)&#xA;  -ot N,    --offset-t N     time offset in milliseconds (default: 0)&#xA;  -on N,    --offset-n N     segment index offset (default: 0)&#xA;  -mc N,    --max-context N  maximum number of text context tokens to store (default: max)&#xA;  -ml N,    --max-len N      maximum segment length in characters (default: 0)&#xA;  -wt N,    --word-thold N   word timestamp probability threshold (default: 0.010000)&#xA;  -v,       --verbose        verbose output&#xA;            --translate      translate from source language to english&#xA;  -otxt,    --output-txt     output result in a text file&#xA;  -ovtt,    --output-vtt     output result in a vtt file&#xA;  -osrt,    --output-srt     output result in a srt file&#xA;  -owts,    --output-words   output script for generating karaoke video&#xA;  -ps,      --print_special  print special tokens&#xA;  -pc,      --print_colors   print colors&#xA;  -nt,      --no_timestamps  do not print timestamps&#xA;  -l LANG,  --language LANG  spoken language (default: en)&#xA;  -m FNAME, --model FNAME    model path (default: models/ggml-base.en.bin)&#xA;  -f FNAME, --file FNAME     input WAV file path&#xA;&#xA;bash ./models/download-ggml-model.sh base.en&#xA;Downloading ggml model base.en ...&#xA;ggml-base.en.bin               100%[========================&amp;gt;] 141.11M  6.34MB/s    in 24s&#xA;Done! Model &#39;base.en&#39; saved in &#39;models/ggml-base.en.bin&#39;&#xA;You can now use it like this:&#xA;&#xA;  $ ./main -m models/ggml-base.en.bin -f samples/jfk.wav&#xA;&#xA;&#xA;===============================================&#xA;Running base.en on all samples in ./samples ...&#xA;===============================================&#xA;&#xA;----------------------------------------------&#xA;[+] Running base.en on samples/jfk.wav ... (run &#39;ffplay samples/jfk.wav&#39; to listen)&#xA;----------------------------------------------&#xA;&#xA;whisper_model_load: loading model from &#39;models/ggml-base.en.bin&#39;&#xA;whisper_model_load: n_vocab       = 51864&#xA;whisper_model_load: n_audio_ctx   = 1500&#xA;whisper_model_load: n_audio_state = 512&#xA;whisper_model_load: n_audio_head  = 8&#xA;whisper_model_load: n_audio_layer = 6&#xA;whisper_model_load: n_text_ctx    = 448&#xA;whisper_model_load: n_text_state  = 512&#xA;whisper_model_load: n_text_head   = 8&#xA;whisper_model_load: n_text_layer  = 6&#xA;whisper_model_load: n_mels        = 80&#xA;whisper_model_load: f16           = 1&#xA;whisper_model_load: type          = 2&#xA;whisper_model_load: mem_required  = 670.00 MB&#xA;whisper_model_load: adding 1607 extra tokens&#xA;whisper_model_load: ggml ctx size = 140.60 MB&#xA;whisper_model_load: memory size =    22.83 MB&#xA;whisper_model_load: model size  =   140.54 MB&#xA;&#xA;system_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |&#xA;&#xA;main: processing &#39;samples/jfk.wav&#39; (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...&#xA;&#xA;&#xA;[00:00:00.000 --&amp;gt; 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.&#xA;&#xA;&#xA;whisper_print_timings:     load time =   105.91 ms&#xA;whisper_print_timings:      mel time =    24.62 ms&#xA;whisper_print_timings:   sample time =     3.63 ms&#xA;whisper_print_timings:   encode time =   324.71 ms / 54.12 ms per layer&#xA;whisper_print_timings:   decode time =    83.58 ms / 13.93 ms per layer&#xA;whisper_print_timings:    total time =   542.81 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The command downloads the &lt;code&gt;base.en&lt;/code&gt; model converted to custom &lt;code&gt;ggml&lt;/code&gt; format and runs the inference on all &lt;code&gt;.wav&lt;/code&gt; samples in the folder &lt;code&gt;samples&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For detailed usage instructions, run: &lt;code&gt;./main -h&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples/main&#34;&gt;main&lt;/a&gt; example currently runs only with 16-bit WAV files, so make sure to convert your input before running the tool. For example, you can use &lt;code&gt;ffmpeg&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;ffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;More audio samples&lt;/h2&gt; &#xA;&lt;p&gt;If you want some extra audio samples to play with, simply run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make samples&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will download a few more audio files from Wikipedia and convert them to 16-bit WAV format via &lt;code&gt;ffmpeg&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can download and run the other models as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make tiny.en&#xA;make tiny&#xA;make base.en&#xA;make base&#xA;make small.en&#xA;make small&#xA;make medium.en&#xA;make medium&#xA;make large&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Memory usage&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Disk&lt;/th&gt; &#xA;   &lt;th&gt;Mem&lt;/th&gt; &#xA;   &lt;th&gt;SHA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tiny&lt;/td&gt; &#xA;   &lt;td&gt;75 MB&lt;/td&gt; &#xA;   &lt;td&gt;~390 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;bd577a113a864445d4c299885e0cb97d4ba92b5f&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;base&lt;/td&gt; &#xA;   &lt;td&gt;142 MB&lt;/td&gt; &#xA;   &lt;td&gt;~500 MB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;465707469ff3a37a2b9b8d8f89f2f99de7299dac&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;small&lt;/td&gt; &#xA;   &lt;td&gt;466 MB&lt;/td&gt; &#xA;   &lt;td&gt;~1.0 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;55356645c2b361a969dfd0ef2c5a50d530afd8d5&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;medium&lt;/td&gt; &#xA;   &lt;td&gt;1.5 GB&lt;/td&gt; &#xA;   &lt;td&gt;~2.6 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;fd9727b6e1217c2f614f9b698455c4ffd82463b4&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;large&lt;/td&gt; &#xA;   &lt;td&gt;2.9 GB&lt;/td&gt; &#xA;   &lt;td&gt;~4.7 GB&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;b1caaf735c4cc1429223d5a74f0f4d0b9b59a299&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Another example&lt;/h2&gt; &#xA;&lt;p&gt;Here is another example of transcribing a &lt;a href=&#34;https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg&#34;&gt;3:24 min speech&lt;/a&gt; in about half a minute on a MacBook M1 Pro, using &lt;code&gt;medium.en&lt;/code&gt; model:&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Expand to see the result&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;$ ./main -m models/ggml-medium.en.bin -f samples/gb1.wav -t 8&#xA;&#xA;whisper_model_load: loading model from &#39;models/ggml-medium.en.bin&#39;&#xA;whisper_model_load: n_vocab       = 51864&#xA;whisper_model_load: n_audio_ctx   = 1500&#xA;whisper_model_load: n_audio_state = 1024&#xA;whisper_model_load: n_audio_head  = 16&#xA;whisper_model_load: n_audio_layer = 24&#xA;whisper_model_load: n_text_ctx    = 448&#xA;whisper_model_load: n_text_state  = 1024&#xA;whisper_model_load: n_text_head   = 16&#xA;whisper_model_load: n_text_layer  = 24&#xA;whisper_model_load: n_mels        = 80&#xA;whisper_model_load: f16           = 1&#xA;whisper_model_load: type          = 4&#xA;whisper_model_load: mem_required  = 2610.00 MB&#xA;whisper_model_load: adding 1607 extra tokens&#xA;whisper_model_load: ggml ctx size = 1644.97 MB&#xA;whisper_model_load: memory size =   182.62 MB&#xA;whisper_model_load: model size  =  1462.12 MB&#xA;&#xA;main: processing &#39;samples/gb1.wav&#39; (3179750 samples, 198.7 sec), 8 threads, lang = en, task = transcribe, timestamps = 1 ...&#xA;&#xA;[00:00.000 --&amp;gt; 00:08.000]   My fellow Americans, this day has brought terrible news and great sadness to our country.&#xA;[00:08.000 --&amp;gt; 00:17.000]   At nine o&#39;clock this morning, Mission Control in Houston lost contact with our Space Shuttle Columbia.&#xA;[00:17.000 --&amp;gt; 00:23.000]   A short time later, debris was seen falling from the skies above Texas.&#xA;[00:23.000 --&amp;gt; 00:29.000]   The Columbia&#39;s lost. There are no survivors.&#xA;[00:29.000 --&amp;gt; 00:32.000]   On board was a crew of seven.&#xA;[00:32.000 --&amp;gt; 00:39.000]   Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark,&#xA;[00:39.000 --&amp;gt; 00:48.000]   Captain David Brown, Commander William McCool, Dr. Kultna Shavla, and Ilan Ramon,&#xA;[00:48.000 --&amp;gt; 00:52.000]   a colonel in the Israeli Air Force.&#xA;[00:52.000 --&amp;gt; 00:58.000]   These men and women assumed great risk in the service to all humanity.&#xA;[00:58.000 --&amp;gt; 01:03.000]   In an age when space flight has come to seem almost routine,&#xA;[01:03.000 --&amp;gt; 01:07.000]   it is easy to overlook the dangers of travel by rocket&#xA;[01:07.000 --&amp;gt; 01:12.000]   and the difficulties of navigating the fierce outer atmosphere of the Earth.&#xA;[01:12.000 --&amp;gt; 01:18.000]   These astronauts knew the dangers, and they faced them willingly,&#xA;[01:18.000 --&amp;gt; 01:23.000]   knowing they had a high and noble purpose in life.&#xA;[01:23.000 --&amp;gt; 01:31.000]   Because of their courage and daring and idealism, we will miss them all the more.&#xA;[01:31.000 --&amp;gt; 01:36.000]   All Americans today are thinking as well of the families of these men and women&#xA;[01:36.000 --&amp;gt; 01:40.000]   who have been given this sudden shock and grief.&#xA;[01:40.000 --&amp;gt; 01:45.000]   You&#39;re not alone. Our entire nation grieves with you,&#xA;[01:45.000 --&amp;gt; 01:52.000]   and those you love will always have the respect and gratitude of this country.&#xA;[01:52.000 --&amp;gt; 01:56.000]   The cause in which they died will continue.&#xA;[01:56.000 --&amp;gt; 02:04.000]   Mankind is led into the darkness beyond our world by the inspiration of discovery&#xA;[02:04.000 --&amp;gt; 02:11.000]   and the longing to understand. Our journey into space will go on.&#xA;[02:11.000 --&amp;gt; 02:16.000]   In the skies today, we saw destruction and tragedy.&#xA;[02:16.000 --&amp;gt; 02:22.000]   Yet farther than we can see, there is comfort and hope.&#xA;[02:22.000 --&amp;gt; 02:29.000]   In the words of the prophet Isaiah, &#34;Lift your eyes and look to the heavens&#xA;[02:29.000 --&amp;gt; 02:35.000]   who created all these. He who brings out the starry hosts one by one&#xA;[02:35.000 --&amp;gt; 02:39.000]   and calls them each by name.&#34;&#xA;[02:39.000 --&amp;gt; 02:46.000]   Because of His great power and mighty strength, not one of them is missing.&#xA;[02:46.000 --&amp;gt; 02:55.000]   The same Creator who names the stars also knows the names of the seven souls we mourn today.&#xA;[02:55.000 --&amp;gt; 03:01.000]   The crew of the shuttle Columbia did not return safely to earth,&#xA;[03:01.000 --&amp;gt; 03:05.000]   yet we can pray that all are safely home.&#xA;[03:05.000 --&amp;gt; 03:13.000]   May God bless the grieving families, and may God continue to bless America.&#xA;[03:13.000 --&amp;gt; 03:41.000]   Audio&#xA;&#xA;&#xA;whisper_print_timings:     load time =   575.92 ms&#xA;whisper_print_timings:      mel time =   230.60 ms&#xA;whisper_print_timings:   sample time =    73.19 ms&#xA;whisper_print_timings:   encode time = 19552.61 ms / 814.69 ms per layer&#xA;whisper_print_timings:   decode time = 13249.96 ms / 552.08 ms per layer&#xA;whisper_print_timings:    total time = 33686.27 ms&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Real-time audio input example&lt;/h2&gt; &#xA;&lt;p&gt;This is a naive example of performing real-time inference on audio from your microphone. The &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples/stream&#34;&gt;stream&lt;/a&gt; tool samples the audio every half a second and runs the transcription continously. More info is available in &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/issues/10&#34;&gt;issue #10&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;./stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/194935793-76afede7-cfa8-48d8-a80f-28ba83be7d09.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Confidence color-coding&lt;/h2&gt; &#xA;&lt;p&gt;Adding the &lt;code&gt;--print-colors&lt;/code&gt; argument will print the transcribed text using an experimental color coding strategy to highlight words with high or low confidence:&lt;/p&gt; &#xA;&lt;img width=&#34;965&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/1991296/197356445-311c8643-9397-4e5e-b46e-0b4b4daa2530.png&#34;&gt; &#xA;&lt;h2&gt;Controlling the length of the generated text segments (experimental)&lt;/h2&gt; &#xA;&lt;p&gt;For example, to limit the line length to a maximum of 16 characters, simply add &lt;code&gt;-ml 16&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 16&#xA;&#xA;whisper_model_load: loading model from &#39;./models/ggml-base.en.bin&#39;&#xA;...&#xA;system_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | &#xA;&#xA;main: processing &#39;./samples/jfk.wav&#39; (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...&#xA;&#xA;[00:00:00.000 --&amp;gt; 00:00:00.850]   And so my&#xA;[00:00:00.850 --&amp;gt; 00:00:01.590]   fellow&#xA;[00:00:01.590 --&amp;gt; 00:00:04.140]   Americans, ask&#xA;[00:00:04.140 --&amp;gt; 00:00:05.660]   not what your&#xA;[00:00:05.660 --&amp;gt; 00:00:06.840]   country can do&#xA;[00:00:06.840 --&amp;gt; 00:00:08.430]   for you, ask&#xA;[00:00:08.430 --&amp;gt; 00:00:09.440]   what you can do&#xA;[00:00:09.440 --&amp;gt; 00:00:10.020]   for your&#xA;[00:00:10.020 --&amp;gt; 00:00:11.000]   country.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Word-level timestamp&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;--max-len&lt;/code&gt; argument can be used to obtain word-level timestamps. Simply use &lt;code&gt;-ml 1&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 1&#xA;&#xA;whisper_model_load: loading model from &#39;./models/ggml-base.en.bin&#39;&#xA;...&#xA;system_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | &#xA;&#xA;main: processing &#39;./samples/jfk.wav&#39; (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...&#xA;&#xA;[00:00:00.000 --&amp;gt; 00:00:00.320]  &#xA;[00:00:00.320 --&amp;gt; 00:00:00.370]   And&#xA;[00:00:00.370 --&amp;gt; 00:00:00.690]   so&#xA;[00:00:00.690 --&amp;gt; 00:00:00.850]   my&#xA;[00:00:00.850 --&amp;gt; 00:00:01.590]   fellow&#xA;[00:00:01.590 --&amp;gt; 00:00:02.850]   Americans&#xA;[00:00:02.850 --&amp;gt; 00:00:03.300]  ,&#xA;[00:00:03.300 --&amp;gt; 00:00:04.140]   ask&#xA;[00:00:04.140 --&amp;gt; 00:00:04.990]   not&#xA;[00:00:04.990 --&amp;gt; 00:00:05.410]   what&#xA;[00:00:05.410 --&amp;gt; 00:00:05.660]   your&#xA;[00:00:05.660 --&amp;gt; 00:00:06.260]   country&#xA;[00:00:06.260 --&amp;gt; 00:00:06.600]   can&#xA;[00:00:06.600 --&amp;gt; 00:00:06.840]   do&#xA;[00:00:06.840 --&amp;gt; 00:00:07.010]   for&#xA;[00:00:07.010 --&amp;gt; 00:00:08.170]   you&#xA;[00:00:08.170 --&amp;gt; 00:00:08.190]  ,&#xA;[00:00:08.190 --&amp;gt; 00:00:08.430]   ask&#xA;[00:00:08.430 --&amp;gt; 00:00:08.910]   what&#xA;[00:00:08.910 --&amp;gt; 00:00:09.040]   you&#xA;[00:00:09.040 --&amp;gt; 00:00:09.320]   can&#xA;[00:00:09.320 --&amp;gt; 00:00:09.440]   do&#xA;[00:00:09.440 --&amp;gt; 00:00:09.760]   for&#xA;[00:00:09.760 --&amp;gt; 00:00:10.020]   your&#xA;[00:00:10.020 --&amp;gt; 00:00:10.510]   country&#xA;[00:00:10.510 --&amp;gt; 00:00:11.000]  .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Karaoke-style movie generation (experimental)&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples/main&#34;&gt;main&lt;/a&gt; example provides support for output of karaoke-style movies, where the currently pronounced word is highlighted. Use the &lt;code&gt;-wts&lt;/code&gt; argument and run the generated bash script. This requires to have &lt;code&gt;ffmpeg&lt;/code&gt; installed.&lt;/p&gt; &#xA;&lt;p&gt;Here are a few &lt;em&gt;&#34;typical&#34;&lt;/em&gt; examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;./main -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -owts&#xA;source ./samples/jfk.wav.wts&#xA;ffplay ./samples/jfk.wav.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/199337465-dbee4b5e-9aeb-48a3-b1c6-323ac4db5b2c.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/199337465-dbee4b5e-9aeb-48a3-b1c6-323ac4db5b2c.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;./main -m ./models/ggml-base.en.bin -f ./samples/mm0.wav -owts&#xA;source ./samples/mm0.wav.wts&#xA;ffplay ./samples/mm0.wav.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/199337504-cc8fd233-0cb7-4920-95f9-4227de3570aa.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/199337504-cc8fd233-0cb7-4920-95f9-4227de3570aa.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;./main -m ./models/ggml-base.en.bin -f ./samples/gb0.wav -owts&#xA;source ./samples/gb0.wav.wts&#xA;ffplay ./samples/gb0.wav.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/1991296/199337538-b7b0c7a3-2753-4a88-a0cd-f28a317987ba.mp4&#34;&gt;https://user-images.githubusercontent.com/1991296/199337538-b7b0c7a3-2753-4a88-a0cd-f28a317987ba.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;In order to have an objective comparison of the performance of the inference across different system configurations, use the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples/bench&#34;&gt;bench&lt;/a&gt; tool. The tool simply runs the Encoder part of the model and prints how much time it took to execute it. The results are summarized in the following Github issue:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/issues/89&#34;&gt;Benchmark results&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ggml format&lt;/h2&gt; &#xA;&lt;p&gt;The original models are converted to a custom binary format. This allows to pack everything needed into a single file:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;model parameters&lt;/li&gt; &#xA; &lt;li&gt;mel filters&lt;/li&gt; &#xA; &lt;li&gt;vocabulary&lt;/li&gt; &#xA; &lt;li&gt;weights&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can download the converted models using the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/models/download-ggml-model.sh&#34;&gt;models/download-ggml-model.sh&lt;/a&gt; script or manually from here:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/datasets/ggerganov/whisper.cpp&#34;&gt;https://huggingface.co/datasets/ggerganov/whisper.cpp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ggml.ggerganov.com&#34;&gt;https://ggml.ggerganov.com&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details, see the conversion script &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/models/convert-pt-to-ggml.py&#34;&gt;models/convert-pt-to-ggml.py&lt;/a&gt; or the README in &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/models&#34;&gt;models&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Bindings&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Rust: &lt;a href=&#34;https://github.com/tazz4843/whisper-rs&#34;&gt;tazz4843/whisper-rs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Objective-C / Swift: &lt;a href=&#34;https://github.com/ggerganov/whisper.spm&#34;&gt;ggerganov/whisper.spm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Python:&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Java:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;There are various examples of using the library for different projects in the &lt;a href=&#34;https://raw.githubusercontent.com/ggerganov/whisper.cpp/master/examples&#34;&gt;examples&lt;/a&gt; folder. Check them out!&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/discussions/126&#34;&gt;Frequently asked questions (#126)&lt;/a&gt;&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>zhongyang219/TrafficMonitor</title>
    <updated>2022-11-21T01:32:58Z</updated>
    <id>tag:github.com,2022-11-21:/zhongyang219/TrafficMonitor</id>
    <link href="https://github.com/zhongyang219/TrafficMonitor" rel="alternate"></link>
    <summary type="html">&lt;p&gt;这是一个用于显示当前网速、CPU及内存利用率的桌面悬浮窗软件，并支持任务栏显示，支持更换皮肤。&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;简体中文 | &lt;a href=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/README_en-us.md&#34;&gt;English&lt;/a&gt;&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://996.icu/#/en_US&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/link-996.icu-%23FF4D5B.svg?style=flat-square&#34; alt=&#34;Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/996icu/996.ICU/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Anti%20996-blue.svg?style=flat-square&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitor/actions?query=workflow:%22Release+CI%22&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/workflow/status/zhongyang219/TrafficMonitor/Release%20CI?label=Release%20CI&amp;amp;logo=github&amp;amp;style=flat-square&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitor/releases/latest&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/zhongyang219/TrafficMonitor.svg?style=flat-square&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;TrafficMonitor 简介&lt;/h1&gt; &#xA;&lt;p&gt;Traffic Monitor是一款用于Windows平台的网速监控悬浮窗软件，可以显示当前网速、CPU及内存利用率，支持嵌入到任务栏显示，支持更换皮肤、历史流量统计等功能。&lt;/p&gt; &#xA;&lt;h1&gt;相关链接：&lt;/h1&gt; &#xA;&lt;p&gt;请&lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitor/releases/latest&#34;&gt;点击此处&lt;/a&gt;下载TrafficMonitor的最新版本。&lt;br&gt; 备用链接：&lt;a href=&#34;https://pan.baidu.com/s/15PMt7s-ASpyDwtS__4cUhg&#34;&gt;百度网盘下载&lt;/a&gt; 提取码：&lt;code&gt;ou0m&lt;/code&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;国内用户如果遇到Github下载缓慢的问题，可以&lt;a href=&#34;https://gitee.com/zhongyang219/TrafficMonitor&#34;&gt;点击此处&lt;/a&gt;转到此项目在Gitee上的页面。&lt;/p&gt; &#xA;&lt;p&gt;如果遇到问题，请&lt;a href=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/Help.md&#34;&gt;点击此处&lt;/a&gt;查看常见问题。&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;你也可以&lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitor/actions?query=workflow:%22Release+CI%22&#34;&gt;点击此处&lt;/a&gt;下载TrafficMonitor的预发行构建版本。&lt;/p&gt; &#xA;&lt;p&gt;从1.80版本开始，TrafficMonitor加入了温度监控功能，如果你不需要温度监控功能，并且在使用1.80以上版本中遇到了问题，建议下载不含温度监控的版本（Lite版本）。（在Release页面找到文件名包含&lt;code&gt;Lite&lt;/code&gt;的版本。）&lt;/p&gt; &#xA;&lt;p&gt;TrafficMonitor依赖于Microsoft Visual C++ 运行环境，如果程序启动时提示“找不到MSVC*.dll”，请点击以下链接下载并安装Microsoft Visual C++ 运行环境。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.microsoft.com/zh-CN/cpp/windows/latest-supported-vc-redist?view=msvc-170&#34;&gt;最新支持的 Visual C++ 可再发行程序包下载 | Microsoft Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;版本说明&lt;/h1&gt; &#xA;&lt;p&gt;TrafficMonitor提供了普通版和Lite版两种版本可用。普通版包含了所有的功能，Lite版本则不包含温度监控、显卡利用率、硬盘利用率等硬件监控功能。普通版运行需要管理员权限，而Lite版本则不需要。&lt;/p&gt; &#xA;&lt;p&gt;如果没有监控温度等硬件信息的需要，建议使用Lite版。&lt;/p&gt; &#xA;&lt;p&gt;以下是两个版本功能对比。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;功能&lt;/th&gt; &#xA;   &lt;th&gt;普通版&lt;/th&gt; &#xA;   &lt;th&gt;Lite版&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;网速监控&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU、内存利用率&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU、显卡、硬盘、主板温度监控&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU频率监控&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;显卡利用率监控&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;硬盘利用率监控&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;❌&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;网络详细信息&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;插件系统&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;主窗口更换皮肤&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;   &lt;td&gt;✔&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;需要管理员权限&lt;/td&gt; &#xA;   &lt;td&gt;是&lt;/td&gt; &#xA;   &lt;td&gt;否&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;主要特性&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;显示当前实现网络传输速率、CPU和内存占用率&lt;/li&gt; &#xA; &lt;li&gt;如果电脑有多个网卡，支持自动和手动选择网络连接&lt;/li&gt; &#xA; &lt;li&gt;查看网络详细信息&lt;/li&gt; &#xA; &lt;li&gt;支持嵌入到任务栏显示&lt;/li&gt; &#xA; &lt;li&gt;支持更换皮肤和自定义皮肤&lt;/li&gt; &#xA; &lt;li&gt;历史流量统计&lt;/li&gt; &#xA; &lt;li&gt;硬件信息监控&lt;/li&gt; &#xA; &lt;li&gt;插件系统&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;使用说明&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitor/wiki&#34;&gt;点击这里&lt;/a&gt;转到Wiki页面查看关于TrafficMonitor的详细说明文档。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;截图&lt;/h1&gt; &#xA;&lt;p&gt;主悬浮窗：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/Screenshots/main1.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; 右键菜单：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/Screenshots/main.png&#34; alt=&#34;&#34;&gt;&lt;br&gt; 任务栏窗口：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/Screenshots/taskbar.PNG&#34; alt=&#34;&#34;&gt;&lt;br&gt; 多彩皮肤：&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/Screenshots/skins.PNG&#34; style=&#34;zoom:80%;&#34;&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;如何使用&lt;/h1&gt; &#xA;&lt;p&gt;程序启动后在会在屏幕中显示一个显示网速的悬浮窗。在悬浮窗上点击鼠标右键可以弹出右键菜单。&lt;/p&gt; &#xA;&lt;p&gt;TrafficMonitor支持将信息显示到任务栏。但是TrafficMonitor默认只显示主窗口（悬浮窗），如果需要让它嵌入到任务栏显示，请在右键菜单中选择“显示任务栏窗口”命令。&lt;/p&gt; &#xA;&lt;p&gt;任务栏窗口支持自定义显示项目，默认情况下只显示网速，如果需要显示CPU和内存利用率，请在任务栏右键菜单中的“显示设置”子菜单下勾选需要显示的项目，如下图所示：&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/Screenshots/taskbar_item_settings.png&#34; style=&#34;zoom:80%;&#34;&gt; &#xA;&lt;h1&gt;自定义皮肤&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/Screenshots/selecte_skin.png&#34; style=&#34;zoom:80%;&#34;&gt;&lt;br&gt; 在主窗口或通知区图标右键菜单上选择“其他功能”——“更换皮肤”可以打开更换皮肤界面。&lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitorSkin/raw/master/%E7%9A%AE%E8%82%A4%E4%B8%8B%E8%BD%BD.md&#34;&gt;点击此处&lt;/a&gt;可以下载更多皮肤。用户还可以根据自己的需要编辑自己的皮肤。&lt;br&gt; 皮肤文件放在程序所在目录的&lt;code&gt;skins&lt;/code&gt;目录下，每个皮肤被放到单独的文件夹下，文件夹的名称就是皮肤的名称。&lt;br&gt; 其中&lt;code&gt;background.bmp&lt;/code&gt;和&lt;code&gt;background_l.bmp&lt;/code&gt;是背景图片，&lt;code&gt;skin.ini&lt;/code&gt;是皮肤的配置文件，可以通过&lt;code&gt;skin.ini&lt;/code&gt;指定文本颜色、字体、皮肤作者、每个项目的大小和位置等信息。&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;从1.80版本开始增加了xml格式的皮肤配置文件&lt;code&gt;skin.xml&lt;/code&gt;，只有xml格式的皮肤配置文件才支持温度和显卡使用率显示。&lt;/p&gt; &#xA;&lt;p&gt;详细的皮肤制作教程请点击以下链接：&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitor/wiki/%E7%9A%AE%E8%82%A4%E5%88%B6%E4%BD%9C%E6%95%99%E7%A8%8B&#34;&gt;皮肤制作教程 · zhongyang219/TrafficMonitor Wiki (github.com)&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;选项设置&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/Screenshots/option.jpg&#34; style=&#34;zoom:80%;&#34;&gt;&lt;br&gt; 在右键菜单选择“选项...”可以进入选项设置。在选项设置对话框中，可以单独设置主窗口和任务栏窗口的文本颜色、字体、背景颜色、网速单位、显示的文本等。&lt;br&gt; 在“常规设置”选项卡中，可以设置是否在程序时自动检查更新，以及是否需要在开机是自动运行。可以设置在什么时候需要发出消息通知。&lt;br&gt; 从1.72版本开始，支持每个项目文本颜色单独设置。勾选“指定每个项目的颜色”后，点击“文本颜色”右边的颜色框，会弹出详细颜色设置的对话框，可以在这里单独指定每个项目的颜色。&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;插件系统&lt;/h1&gt; &#xA;&lt;p&gt;从1.82版本开始增加了插件系统，插件dll必须放在“TrafficMonitor.exe”同级目录的“plugins”目录下。程序启动后，插件会自动加载。你可以在右键菜单“更多功能”——“插件管理”中查看并管理已加载的插件。&lt;/p&gt; &#xA;&lt;p&gt;关于如何开发TrafficMonitor的说明，请参见&lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitor/wiki/%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91%E6%8C%87%E5%8D%97&#34;&gt;插件开发指南 · zhongyang219/TrafficMonitor Wiki (github.com)&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;要下载TrafficMonitor插件，请&lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitorPlugins/raw/main/download/plugin_download.md&#34;&gt;点击这里&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h1&gt;关于硬件监控功能&lt;/h1&gt; &#xA;&lt;p&gt;从1.80版本开始，TrafficMonitor加入了硬件监控功能（包括温度监控和显卡使用率监控、CPU频率监控），它使用了第三方开源库&lt;a href=&#34;https://github.com/LibreHardwareMonitor/LibreHardwareMonitor&#34;&gt;LibreHardwareMonitor&lt;/a&gt;。如果你在使用温度监控功能时遇到了问题，请&lt;a href=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/Help.md#13-%E5%85%B3%E4%BA%8Etrafficmonitor%E6%B8%A9%E5%BA%A6%E7%9B%91%E6%8E%A7%E7%9A%84%E9%97%AE%E9%A2%98&#34;&gt;点击这里&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;需要注意的是，温度监控功能默认是关闭的，如果你要使用TrafficMonitor的温度监控功能，请到&lt;a href=&#34;https://github.com/zhongyang219/TrafficMonitor/wiki/%E9%80%89%E9%A1%B9%E8%AE%BE%E7%BD%AE#%E7%A1%AC%E4%BB%B6%E7%9B%91%E6%8E%A7&#34;&gt;“选项设置”-“常规设置”-“硬件监控”&lt;/a&gt;中开启。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意：硬件监控功能（包括温度监控和显卡使用率监控）可能存在一些问题，它可能会占用更多的CPU和内存。据部分用户反馈，开启温度功能后会导致程序崩溃和系统死机等问题，请在知晓以上风险后再决定开启硬件监控功能。否则，请不要使用硬件监控功能。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;更新日志&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/zhongyang219/TrafficMonitor/master/UpdateLog/update_log.md&#34;&gt;点击此处查看更新日志&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
</feed>