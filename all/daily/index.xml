<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-23T01:29:20Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ZJU-LLMs/Foundations-of-LLMs</title>
    <updated>2025-05-23T01:29:20Z</updated>
    <id>tag:github.com,2025-05-23:/ZJU-LLMs/Foundations-of-LLMs</id>
    <link href="https://github.com/ZJU-LLMs/Foundations-of-LLMs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;大模型基础&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ZJU-LLMs/Foundations-of-LLMs/main/.%5Cfigure%5Ccover.png&#34; style=&#34;width: 50%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/version-1.0.0-blue&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/ZJU-LLMs/Foundations-of-LLMs?style=social&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/ZJU-LLMs/Foundations-of-LLMs?style=social&#34;&gt; &#xA; &lt;!--   &lt;img src=&#34;https://img.shields.io/github/license/ZJU-LLMs/Foundations-of-LLMs&#34;&gt; --&gt; &lt;/p&gt; &#xA;&lt;p&gt;本书旨在为对大语言模型感兴趣的读者系统地讲解相关基础知识、介绍前沿技术。作者团队将认真听取开源社区以及广大专家学者的建议，持续进行&lt;strong&gt;月度更新&lt;/strong&gt;，致力打造&lt;strong&gt;易读、严谨、有深度&lt;/strong&gt;的大模型教材。并且，本书还将针对每章内容配备相关的&lt;strong&gt;Paper List&lt;/strong&gt;，以跟踪相关技术的&lt;strong&gt;最新进展&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;本书第一版包括&lt;strong&gt;传统语言模型&lt;/strong&gt;、&lt;strong&gt;大语言模型架构演化&lt;/strong&gt;、&lt;strong&gt;Prompt工程&lt;/strong&gt;、&lt;strong&gt;参数高效微调&lt;/strong&gt;、&lt;strong&gt;模型编辑&lt;/strong&gt;、&lt;strong&gt;检索增强生成&lt;/strong&gt;等六章内容。为增加本书的易读性，每章分别以&lt;strong&gt;一种动物&lt;/strong&gt;为背景，对具体技术进行举例说明，故此本书以六种动物作为封面。当前版本所含内容均来源于作者团队对相关方向的探索与理解，如有谬误，恳请大家多提issue，多多赐教。后续，作者团队还将继续探索大模型推理加速、大模型智能体等方向。相关内容也将陆续补充到本书的后续版本中，期待封面上的动物越来越多。&lt;/p&gt; &#xA;&lt;p&gt;当前完整的本书PDF版本路径为&lt;a href=&#34;https://github.com/ZJU-LLMs/Foundations-of-LLMs/raw/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%20%E5%AE%8C%E6%95%B4%E7%89%88.pdf&#34;&gt;大模型基础.pdf&lt;/a&gt;。另外，我们还提供了两个文件夹，&lt;a href=&#34;https://github.com/ZJU-LLMs/Foundations-of-LLMs/tree/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9&#34;&gt;大语言模型分章节内容&lt;/a&gt;文件夹中包含了各章节的PDF版本。而&lt;a href=&#34;https://github.com/ZJU-LLMs/Foundations-of-LLMs/tree/main/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E5%88%97%E8%A1%A8&#34;&gt;大语言模型相关论文&lt;/a&gt;文件夹中包含了各章节的相关论文，当前正处于不断更新中。&lt;/p&gt; &#xA;&lt;p&gt;其中每个章节的内容目录如下表所示。&lt;/p&gt; &#xA;&lt;h2&gt;本书目录&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th style=&#34;text-align:center; width: 25%;&#34;&gt;章节&lt;/th&gt; &#xA;   &lt;th style=&#34;text-align:center; width: 75%;&#34; colspan=&#34;3&#34;&gt;所含内容&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://github.com/ZJU-LLMs/Foundations-of-LLMs/raw/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC1%E7%AB%A0%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80.pdf&#34;&gt;第 1 章：语言模型基础&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;width: 25%;&#34;&gt;1.1 基于统计方法的语言模型&lt;/td&gt; &#xA;   &lt;td style=&#34;width: 25%;&#34;&gt;1.2 基于 RNN 的语言模型&lt;/td&gt; &#xA;   &lt;td style=&#34;width: 25%;&#34;&gt;1.3 基于 Transformer 的语言模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;1.4 语言模型的采样方法&lt;/td&gt; &#xA;   &lt;td&gt;1.5 语言模型的评测&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://github.com/ZJU-LLMs/Foundations-of-LLMs/raw/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC2%E7%AB%A0%20%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84.pdf&#34;&gt;第 2 章：大语言模型&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.1 大数据 + 大模型 → 新智能&lt;/td&gt; &#xA;   &lt;td&gt;2.2 大语言模型架构概览&lt;/td&gt; &#xA;   &lt;td&gt;2.3 基于 Encoder-only 架构的大语言模型&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;2.4 基于 Encoder-Decoder 架构的大语言模型&lt;/td&gt; &#xA;   &lt;td&gt;2.5 基于 Decoder-only 架构的大语言模型&lt;/td&gt; &#xA;   &lt;td&gt;2.6 非 Transformer 架构&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://github.com/ZJU-LLMs/Foundations-of-LLMs/raw/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC3%E7%AB%A0%20Prompt%20%E5%B7%A5%E7%A8%8B.pdf&#34;&gt;第 3 章：Prompt 工程&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.1 Prompt 工程简介&lt;/td&gt; &#xA;   &lt;td&gt;3.2 上下文学习&lt;/td&gt; &#xA;   &lt;td&gt;3.3 思维链&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;3.4 Prompt 技巧&lt;/td&gt; &#xA;   &lt;td&gt;3.5 相关应用&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://github.com/ZJU-LLMs/Foundations-of-LLMs/raw/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC4%E7%AB%A0%20%E5%8F%82%E6%95%B0%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83.pdf&#34;&gt;第 4 章：参数高效微调&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4.1 参数高效微调简介&lt;/td&gt; &#xA;   &lt;td&gt;4.2 参数附加方法&lt;/td&gt; &#xA;   &lt;td&gt;4.3 参数选择方法&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4.4 低秩适配方法&lt;/td&gt; &#xA;   &lt;td&gt;4.5 实践与应用&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://github.com/ZJU-LLMs/Foundations-of-LLMs/raw/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC5%E7%AB%A0%20%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%BE%91.pdf&#34;&gt;第 5 章：模型编辑&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;5.1 模型编辑简介&lt;/td&gt; &#xA;   &lt;td&gt;5.2 模型编辑经典方法&lt;/td&gt; &#xA;   &lt;td&gt;5.3 附加参数法：T-Patcher&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5.4 定位编辑法：ROME&lt;/td&gt; &#xA;   &lt;td&gt;5.5 模型编辑应用&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;2&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://github.com/ZJU-LLMs/Foundations-of-LLMs/raw/main/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E6%95%99%E6%9D%90/%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E3%80%8B%E5%88%86%E7%AB%A0%E8%8A%82%E5%86%85%E5%AE%B9/%E7%AC%AC6%E7%AB%A0%20%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90.pdf&#34;&gt;第 6 章：检索增强生成&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6.1 检索增强生成简介&lt;/td&gt; &#xA;   &lt;td&gt;6.2 检索增强生成架构&lt;/td&gt; &#xA;   &lt;td&gt;6.3 知识检索&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;6.4 生成增强&lt;/td&gt; &#xA;   &lt;td&gt;6.5 实践与应用&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;本书的不断优化，将仰仗各位读者的帮助与支持。您的建议将成为我们持续向前的动力！&lt;/p&gt; &#xA;&lt;p&gt;所有提出issue的人，我们都列举在此，以表达我们深深的谢意。&lt;/p&gt; &#xA;&lt;p&gt;如果有此书相关的其他问题，请随时联系我们，可发送邮件至：&lt;a href=&#34;mailto:xuwenyi@zju.edu.cn&#34;&gt;xuwenyi@zju.edu.cn&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/ZJU-LLMs/Foundations-of-LLMs/main/.%5Cfigure%5Cwechat_QR_code.jpg&#34; style=&#34;width: 90%&#34;&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/huggingface.js</title>
    <updated>2025-05-23T01:29:20Z</updated>
    <id>tag:github.com,2025-05-23:/huggingface/huggingface.js</id>
    <link href="https://github.com/huggingface/huggingface.js" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Utilities to use the Hugging Face Hub API&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;br&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingfacejs-dark.svg&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingfacejs-light.svg&#34;&gt; &#xA;  &lt;img alt=&#34;huggingface javascript library logo&#34; src=&#34;https://huggingface.co/datasets/huggingface/documentation-images/raw/main/huggingfacejs-light.svg?sanitize=true&#34; width=&#34;376&#34; height=&#34;59&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA; &lt;/picture&gt; &lt;br&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;// Programmatically interact with the Hub&#xA;&#xA;await createRepo({&#xA;  repo: { type: &#34;model&#34;, name: &#34;my-user/nlp-model&#34; },&#xA;  accessToken: HF_TOKEN&#xA;});&#xA;&#xA;await uploadFile({&#xA;  repo: &#34;my-user/nlp-model&#34;,&#xA;  accessToken: HF_TOKEN,&#xA;  // Can work with native File in browsers&#xA;  file: {&#xA;    path: &#34;pytorch_model.bin&#34;,&#xA;    content: new Blob(...)&#xA;  }&#xA;});&#xA;&#xA;// Use all supported Inference Providers!&#xA;&#xA;await inference.chatCompletion({&#xA;  model: &#34;meta-llama/Llama-3.1-8B-Instruct&#34;,&#xA;  provider: &#34;sambanova&#34;, // or together, fal-ai, replicate, cohere …&#xA;  messages: [&#xA;    {&#xA;      role: &#34;user&#34;,&#xA;      content: &#34;Hello, nice to meet you!&#34;,&#xA;    },&#xA;  ],&#xA;  max_tokens: 512,&#xA;  temperature: 0.5,&#xA;});&#xA;&#xA;await inference.textToImage({&#xA;  model: &#34;black-forest-labs/FLUX.1-dev&#34;,&#xA;  provider: &#34;replicate&#34;,&#xA;  inputs: &#34;a picture of a green bird&#34;,&#xA;});&#xA;&#xA;// and much more…&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Hugging Face JS libraries&lt;/h1&gt; &#xA;&lt;p&gt;This is a collection of JS libraries to interact with the Hugging Face API, with TS types included.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/huggingface.js/main/packages/inference/README.md&#34;&gt;@huggingface/inference&lt;/a&gt;: Use all supported (serverless) Inference Providers or switch to Inference Endpoints (dedicated) to make calls to 100,000+ Machine Learning models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/huggingface.js/main/packages/hub/README.md&#34;&gt;@huggingface/hub&lt;/a&gt;: Interact with huggingface.co to create or delete repos and commit / download files&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/huggingface.js/main/packages/mcp-client/README.md&#34;&gt;@huggingface/mcp-client&lt;/a&gt;: A Model Context Protocol (MCP) client, and a tiny Agent library, built on top of InferenceClient.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/huggingface.js/main/packages/gguf/README.md&#34;&gt;@huggingface/gguf&lt;/a&gt;: A GGUF parser that works on remotely hosted files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/huggingface.js/main/packages/dduf/README.md&#34;&gt;@huggingface/dduf&lt;/a&gt;: Similar package for DDUF (DDUF Diffusers Unified Format)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/huggingface.js/main/packages/tasks/README.md&#34;&gt;@huggingface/tasks&lt;/a&gt;: The definition files and source-of-truth for the Hub&#39;s main primitives like pipeline tasks, model libraries, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/huggingface.js/main/packages/jinja/README.md&#34;&gt;@huggingface/jinja&lt;/a&gt;: A minimalistic JS implementation of the Jinja templating engine, to be used for ML chat templates.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/huggingface.js/main/packages/space-header/README.md&#34;&gt;@huggingface/space-header&lt;/a&gt;: Use the Space &lt;code&gt;mini_header&lt;/code&gt; outside Hugging Face&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/huggingface.js/main/packages/ollama-utils/README.md&#34;&gt;@huggingface/ollama-utils&lt;/a&gt;: Various utilities for maintaining Ollama compatibility with models on the Hugging Face Hub.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We use modern features to avoid polyfills and dependencies, so the libraries will only work on modern browsers / Node.js &amp;gt;= 18 / Bun / Deno.&lt;/p&gt; &#xA;&lt;p&gt;The libraries are still very young, please help us by opening issues!&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;From NPM&lt;/h3&gt; &#xA;&lt;p&gt;To install via NPM, you can download the libraries as needed:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install @huggingface/inference&#xA;npm install @huggingface/hub&#xA;npm install @huggingface/mcp-client&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then import the libraries in your code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { InferenceClient } from &#34;@huggingface/inference&#34;;&#xA;import { createRepo, commit, deleteRepo, listFiles } from &#34;@huggingface/hub&#34;;&#xA;import { McpClient } from &#34;@huggingface/mcp-client&#34;;&#xA;import type { RepoId } from &#34;@huggingface/hub&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;From CDN or Static hosting&lt;/h3&gt; &#xA;&lt;p&gt;You can run our packages with vanilla JS, without any bundler, by using a CDN or static hosting. Using &lt;a href=&#34;https://hacks.mozilla.org/2018/03/es-modules-a-cartoon-deep-dive/&#34;&gt;ES modules&lt;/a&gt;, i.e. &lt;code&gt;&amp;lt;script type=&#34;module&#34;&amp;gt;&lt;/code&gt;, you can import the libraries in your code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;script type=&#34;module&#34;&amp;gt;&#xA;    import { InferenceClient } from &#39;https://cdn.jsdelivr.net/npm/@huggingface/inference@3.13.2/+esm&#39;;&#xA;    import { createRepo, commit, deleteRepo, listFiles } from &#34;https://cdn.jsdelivr.net/npm/@huggingface/hub@2.1.0/+esm&#34;;&#xA;&amp;lt;/script&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Deno&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;// esm.sh&#xA;import { InferenceClient } from &#34;https://esm.sh/@huggingface/inference&#34;&#xA;&#xA;import { createRepo, commit, deleteRepo, listFiles } from &#34;https://esm.sh/@huggingface/hub&#34;&#xA;// or npm:&#xA;import { InferenceClient } from &#34;npm:@huggingface/inference&#34;&#xA;&#xA;import { createRepo, commit, deleteRepo, listFiles } from &#34;npm:@huggingface/hub&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage examples&lt;/h2&gt; &#xA;&lt;p&gt;Get your HF access token in your &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;account settings&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;@huggingface/inference examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { InferenceClient } from &#34;@huggingface/inference&#34;;&#xA;&#xA;const HF_TOKEN = &#34;hf_...&#34;;&#xA;&#xA;const client = new InferenceClient(HF_TOKEN);&#xA;&#xA;// Chat completion API&#xA;const out = await client.chatCompletion({&#xA;  model: &#34;meta-llama/Llama-3.1-8B-Instruct&#34;,&#xA;  messages: [{ role: &#34;user&#34;, content: &#34;Hello, nice to meet you!&#34; }],&#xA;  max_tokens: 512&#xA;});&#xA;console.log(out.choices[0].message);&#xA;&#xA;// Streaming chat completion API&#xA;for await (const chunk of client.chatCompletionStream({&#xA;  model: &#34;meta-llama/Llama-3.1-8B-Instruct&#34;,&#xA;  messages: [{ role: &#34;user&#34;, content: &#34;Hello, nice to meet you!&#34; }],&#xA;  max_tokens: 512&#xA;})) {&#xA;  console.log(chunk.choices[0].delta.content);&#xA;}&#xA;&#xA;/// Using a third-party provider:&#xA;await client.chatCompletion({&#xA;  model: &#34;meta-llama/Llama-3.1-8B-Instruct&#34;,&#xA;  messages: [{ role: &#34;user&#34;, content: &#34;Hello, nice to meet you!&#34; }],&#xA;  max_tokens: 512,&#xA;  provider: &#34;sambanova&#34;, // or together, fal-ai, replicate, cohere …&#xA;})&#xA;&#xA;await client.textToImage({&#xA;  model: &#34;black-forest-labs/FLUX.1-dev&#34;,&#xA;  inputs: &#34;a picture of a green bird&#34;,&#xA;  provider: &#34;fal-ai&#34;,&#xA;})&#xA;&#xA;&#xA;&#xA;// You can also omit &#34;model&#34; to use the recommended model for the task&#xA;await client.translation({&#xA;  inputs: &#34;My name is Wolfgang and I live in Amsterdam&#34;,&#xA;  parameters: {&#xA;    src_lang: &#34;en&#34;,&#xA;    tgt_lang: &#34;fr&#34;,&#xA;  },&#xA;});&#xA;&#xA;// pass multimodal files or URLs as inputs&#xA;await client.imageToText({&#xA;  model: &#39;nlpconnect/vit-gpt2-image-captioning&#39;,&#xA;  data: await (await fetch(&#39;https://picsum.photos/300/300&#39;)).blob(),&#xA;})&#xA;&#xA;// Using your own dedicated inference endpoint: https://hf.co/docs/inference-endpoints/&#xA;const gpt2Client = client.endpoint(&#39;https://xyz.eu-west-1.aws.endpoints.huggingface.cloud/gpt2&#39;);&#xA;const { generated_text } = await gpt2Client.textGeneration({ inputs: &#39;The answer to the universe is&#39; });&#xA;&#xA;// Chat Completion&#xA;const llamaEndpoint = client.endpoint(&#xA;  &#34;https://router.huggingface.co/hf-inference/models/meta-llama/Llama-3.1-8B-Instruct&#34;&#xA;);&#xA;const out = await llamaEndpoint.chatCompletion({&#xA;  model: &#34;meta-llama/Llama-3.1-8B-Instruct&#34;,&#xA;  messages: [{ role: &#34;user&#34;, content: &#34;Hello, nice to meet you!&#34; }],&#xA;  max_tokens: 512,&#xA;});&#xA;console.log(out.choices[0].message);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;@huggingface/hub examples&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { createRepo, uploadFile, deleteFiles } from &#34;@huggingface/hub&#34;;&#xA;&#xA;const HF_TOKEN = &#34;hf_...&#34;;&#xA;&#xA;await createRepo({&#xA;  repo: &#34;my-user/nlp-model&#34;, // or { type: &#34;model&#34;, name: &#34;my-user/nlp-test&#34; },&#xA;  accessToken: HF_TOKEN&#xA;});&#xA;&#xA;await uploadFile({&#xA;  repo: &#34;my-user/nlp-model&#34;,&#xA;  accessToken: HF_TOKEN,&#xA;  // Can work with native File in browsers&#xA;  file: {&#xA;    path: &#34;pytorch_model.bin&#34;,&#xA;    content: new Blob(...)&#xA;  }&#xA;});&#xA;&#xA;await deleteFiles({&#xA;  repo: { type: &#34;space&#34;, name: &#34;my-user/my-space&#34; }, // or &#34;spaces/my-user/my-space&#34;&#xA;  accessToken: HF_TOKEN,&#xA;  paths: [&#34;README.md&#34;, &#34;.gitattributes&#34;]&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;@huggingface/mcp-client example&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { Agent } from &#39;@huggingface/mcp-client&#39;;&#xA;&#xA;const HF_TOKEN = &#34;hf_...&#34;;&#xA;&#xA;const agent = new Agent({&#xA;  provider: &#34;auto&#34;,&#xA;  model: &#34;Qwen/Qwen2.5-72B-Instruct&#34;,&#xA;  apiKey: HF_TOKEN,&#xA;  servers: [&#xA;    {&#xA;      // Playwright MCP&#xA;      command: &#34;npx&#34;,&#xA;      args: [&#34;@playwright/mcp@latest&#34;],&#xA;    },&#xA;  ],&#xA;});&#xA;&#xA;await agent.loadTools();&#xA;&#xA;for await (const chunk of agent.run(&#34;What are the top 5 trending models on Hugging Face?&#34;)) {&#xA;    if (&#34;choices&#34; in chunk) {&#xA;        const delta = chunk.choices[0]?.delta;&#xA;        if (delta.content) {&#xA;            console.log(delta.content);&#xA;        }&#xA;    }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are more features of course, check each library&#39;s README!&lt;/p&gt; &#xA;&lt;h2&gt;Formatting &amp;amp; testing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;sudo corepack enable&#xA;pnpm install&#xA;&#xA;pnpm -r format:check&#xA;pnpm -r lint:check&#xA;pnpm -r test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pnpm -r build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will generate ESM and CJS javascript files in &lt;code&gt;packages/*/dist&lt;/code&gt;, eg &lt;code&gt;packages/inference/dist/index.mjs&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>