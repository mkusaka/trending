<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-15T02:45:34Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>datawhalechina/llm-universe</title>
    <updated>2024-04-15T02:45:34Z</updated>
    <id>tag:github.com,2024-04-15:/datawhalechina/llm-universe</id>
    <link href="https://github.com/datawhalechina/llm-universe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªé¢å‘å°ç™½å¼€å‘è€…çš„å¤§æ¨¡å‹åº”ç”¨å¼€å‘æ•™ç¨‹ï¼Œåœ¨çº¿é˜…è¯»åœ°å€ï¼šhttps://datawhalechina.github.io/llm-universe/&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/figures/C0-0-logo.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;é¡¹ç›®ç®€ä»‹&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªé¢å‘å°ç™½å¼€å‘è€…çš„å¤§æ¨¡å‹åº”ç”¨å¼€å‘æ•™ç¨‹ï¼Œæ—¨åœ¨åŸºäºé˜¿é‡Œäº‘æœåŠ¡å™¨ï¼Œç»“åˆä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹é¡¹ç›®ï¼Œé€šè¿‡ä¸€ä¸ªè¯¾ç¨‹å®Œæˆå¤§æ¨¡å‹å¼€å‘çš„é‡ç‚¹å…¥é—¨ï¼Œä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;å¤§æ¨¡å‹ç®€ä»‹&lt;/strong&gt;ï¼Œä½•ä¸ºå¤§æ¨¡å‹ã€å¤§æ¨¡å‹ç‰¹ç‚¹æ˜¯ä»€ä¹ˆã€LangChain æ˜¯ä»€ä¹ˆï¼Œå¦‚ä½•å¼€å‘ä¸€ä¸ª LLM åº”ç”¨ï¼Œé’ˆå¯¹å°ç™½å¼€å‘è€…çš„ç®€å•ä»‹ç»ï¼›&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;å¦‚ä½•è°ƒç”¨å¤§æ¨¡å‹ API&lt;/strong&gt;ï¼Œæœ¬èŠ‚ä»‹ç»äº†å›½å†…å¤–çŸ¥åå¤§æ¨¡å‹äº§å“ API çš„å¤šç§è°ƒç”¨æ–¹å¼ï¼ŒåŒ…æ‹¬è°ƒç”¨åŸç”Ÿ APIã€å°è£…ä¸º LangChain LLMã€å°è£…ä¸º Fastapi ç­‰è°ƒç”¨æ–¹å¼ï¼ŒåŒæ—¶å°†åŒ…æ‹¬ç™¾åº¦æ–‡å¿ƒã€è®¯é£æ˜Ÿç«ã€æ™ºè°±AIç­‰å¤šç§å¤§æ¨¡å‹ API è¿›è¡Œäº†ç»Ÿä¸€å½¢å¼å°è£…ï¼›&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;çŸ¥è¯†åº“æ­å»º&lt;/strong&gt;ï¼Œä¸åŒç±»å‹çŸ¥è¯†åº“æ–‡æ¡£çš„åŠ è½½ã€å¤„ç†ï¼Œå‘é‡æ•°æ®åº“çš„æ­å»ºï¼›&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æ„å»º RAG åº”ç”¨&lt;/strong&gt;ï¼ŒåŒ…æ‹¬å°† LLM æ¥å…¥åˆ° LangChain æ„å»ºæ£€ç´¢é—®ç­”é“¾ï¼Œä½¿ç”¨ Streamlit è¿›è¡Œåº”ç”¨éƒ¨ç½²&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;éªŒè¯è¿­ä»£&lt;/strong&gt;ï¼Œå¤§æ¨¡å‹å¼€å‘å¦‚ä½•å®ç°éªŒè¯è¿­ä»£ï¼Œä¸€èˆ¬çš„è¯„ä¼°æ–¹æ³•æœ‰ä»€ä¹ˆï¼›&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ä¸»è¦åŒ…æ‹¬ä¸‰éƒ¨åˆ†å†…å®¹ï¼š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM å¼€å‘å…¥é—¨&lt;/strong&gt;ã€‚V1 ç‰ˆæœ¬çš„ç®€åŒ–ç‰ˆï¼Œæ—¨åœ¨å¸®åŠ©åˆå­¦è€…æœ€å¿«ã€æœ€ä¾¿æ·åœ°å…¥é—¨ LLM å¼€å‘ï¼Œç†è§£ LLM å¼€å‘çš„ä¸€èˆ¬æµç¨‹ï¼Œå¯ä»¥æ­å»ºå‡ºä¸€ä¸ªç®€å•çš„ Demoã€‚&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM å¼€å‘æŠ€å·§&lt;/strong&gt;ã€‚LLM å¼€å‘æ›´è¿›é˜¶çš„æŠ€å·§ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼šPrompt Engineeringã€å¤šç±»å‹æºæ•°æ®çš„å¤„ç†ã€ä¼˜åŒ–æ£€ç´¢ã€å¬å›ç²¾æ’ã€Agent æ¡†æ¶ç­‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM åº”ç”¨å®ä¾‹&lt;/strong&gt;ã€‚å¼•å…¥ä¸€äº›æˆåŠŸçš„å¼€æºæ¡ˆä¾‹ï¼Œä»æœ¬è¯¾ç¨‹çš„è§’åº¦å‡ºå‘ï¼Œè§£æè¿™äº›åº”ç”¨èŒƒä¾‹çš„ Ideaã€æ ¸å¿ƒæ€è·¯ã€å®ç°æ¡†æ¶ï¼Œå¸®åŠ©åˆå­¦è€…æ˜ç™½å…¶å¯ä»¥é€šè¿‡ LLM å¼€å‘ä»€ä¹ˆæ ·çš„åº”ç”¨ã€‚&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;ç›®å‰ï¼Œç¬¬ä¸€éƒ¨åˆ†å·²ç»å®Œç¨¿ï¼Œæ¬¢è¿å¤§å®¶é˜…è¯»å­¦ä¹ ï¼›ç¬¬äºŒã€ä¸‰éƒ¨åˆ†æ­£åœ¨åˆ›ä½œä¸­ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;ç›®å½•ç»“æ„è¯´æ˜ï¼š&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  requirements.txtï¼šå®˜æ–¹ç¯å¢ƒä¸‹çš„å®‰è£…ä¾èµ–&#xA;  notebookï¼šNotebook æºä»£ç æ–‡ä»¶&#xA;  docsï¼šMarkdown æ–‡æ¡£æ–‡ä»¶&#xA;  figuresï¼šå›¾ç‰‡&#xA;  data_baseï¼šæ‰€ä½¿ç”¨çš„çŸ¥è¯†åº“æºæ–‡ä»¶&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;é¡¹ç›®æ„ä¹‰&lt;/h2&gt; &#xA;&lt;p&gt;LLM æ­£é€æ­¥æˆä¸ºä¿¡æ¯ä¸–ç•Œçš„æ–°é©å‘½åŠ›é‡ï¼Œå…¶é€šè¿‡å¼ºå¤§çš„è‡ªç„¶è¯­è¨€ç†è§£ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆèƒ½åŠ›ï¼Œä¸ºå¼€å‘è€…æä¾›äº†æ–°çš„ã€æ›´å¼ºå¤§çš„åº”ç”¨å¼€å‘é€‰æ‹©ã€‚éšç€å›½å†…å¤–äº•å–·å¼çš„ LLM API æœåŠ¡å¼€æ”¾ï¼Œå¦‚ä½•åŸºäº LLM API å¿«é€Ÿã€ä¾¿æ·åœ°å¼€å‘å…·å¤‡æ›´å¼ºèƒ½åŠ›ã€é›†æˆ LLM çš„åº”ç”¨ï¼Œå¼€å§‹æˆä¸ºå¼€å‘è€…çš„ä¸€é¡¹é‡è¦æŠ€èƒ½ã€‚&lt;/p&gt; &#xA;&lt;p&gt;ç›®å‰ï¼Œå…³äº LLM çš„ä»‹ç»ä»¥åŠé›¶æ•£çš„ LLM å¼€å‘æŠ€èƒ½è¯¾ç¨‹å·²æœ‰ä¸å°‘ï¼Œä½†è´¨é‡å‚å·®ä¸é½ï¼Œä¸”æ²¡æœ‰å¾ˆå¥½åœ°æ•´åˆï¼Œå¼€å‘è€…éœ€è¦æœç´¢å¤§é‡æ•™ç¨‹å¹¶é˜…è¯»å¤§é‡ç›¸å…³æ€§ä¸å¼ºã€å¿…è¦æ€§è¾ƒä½çš„å†…å®¹ï¼Œæ‰èƒ½åˆæ­¥æŒæ¡å¤§æ¨¡å‹å¼€å‘çš„å¿…å¤‡æŠ€èƒ½ï¼Œå­¦ä¹ æ•ˆç‡ä½ï¼Œå­¦ä¹ é—¨æ§›ä¹Ÿè¾ƒé«˜ã€‚&lt;/p&gt; &#xA;&lt;p&gt;æœ¬é¡¹ç›®ä»å®è·µå‡ºå‘ï¼Œç»“åˆæœ€å¸¸è§ã€é€šç”¨çš„ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹é¡¹ç›®ï¼Œæ·±å…¥æµ…å‡ºé€æ­¥æ‹†è§£ LLM å¼€å‘çš„ä¸€èˆ¬æµç¨‹ã€æ­¥éª¤ï¼Œæ—¨åœ¨å¸®åŠ©æ²¡æœ‰ç®—æ³•åŸºç¡€çš„å°ç™½é€šè¿‡ä¸€ä¸ªè¯¾ç¨‹å®Œæˆå¤§æ¨¡å‹å¼€å‘çš„åŸºç¡€å…¥é—¨ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿä¼šåŠ å…¥ RAG å¼€å‘çš„è¿›é˜¶æŠ€å·§ä»¥åŠä¸€äº›æˆåŠŸçš„ LLM åº”ç”¨æ¡ˆä¾‹çš„è§£è¯»ï¼Œå¸®åŠ©å®Œæˆç¬¬ä¸€éƒ¨åˆ†å­¦ä¹ çš„è¯»è€…è¿›ä¸€æ­¥æŒæ¡æ›´é«˜é˜¶çš„ RAG å¼€å‘æŠ€å·§ï¼Œå¹¶èƒ½å¤Ÿé€šè¿‡å¯¹å·²æœ‰æˆåŠŸé¡¹ç›®çš„å€Ÿé‰´å¼€å‘è‡ªå·±çš„ã€å¥½ç©çš„åº”ç”¨ã€‚&lt;/p&gt; &#xA;&lt;h2&gt;é¡¹ç›®å—ä¼—&lt;/h2&gt; &#xA;&lt;p&gt;æ‰€æœ‰å…·å¤‡åŸºç¡€ Python èƒ½åŠ›ï¼Œæƒ³è¦æŒæ¡ LLM åº”ç”¨å¼€å‘æŠ€èƒ½çš„å¼€å‘è€…ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;æœ¬é¡¹ç›®å¯¹å­¦ä¹ è€…çš„äººå·¥æ™ºèƒ½åŸºç¡€ã€ç®—æ³•åŸºç¡€æ²¡æœ‰ä»»ä½•è¦æ±‚ï¼Œä»…éœ€è¦æŒæ¡åŸºæœ¬ Python è¯­æ³•ã€æŒæ¡åˆçº§ Python å¼€å‘æŠ€èƒ½å³å¯ã€‚&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;è€ƒè™‘åˆ°ç¯å¢ƒæ­å»ºé—®é¢˜ï¼Œæœ¬é¡¹ç›®æä¾›äº†é˜¿é‡Œäº‘æœåŠ¡å™¨å­¦ç”Ÿå…è´¹é¢†å–æ–¹å¼ï¼Œå­¦ç”Ÿè¯»è€…å¯ä»¥å…è´¹é¢†å–é˜¿é‡Œäº‘æœåŠ¡å™¨ï¼Œå¹¶é€šè¿‡é˜¿é‡Œäº‘æœåŠ¡å™¨å®Œæˆæœ¬è¯¾ç¨‹çš„å­¦ä¹ ï¼›æœ¬é¡¹ç›®åŒæ—¶ä¹Ÿæä¾›äº†ä¸ªäººç”µè„‘åŠéé˜¿é‡Œäº‘æœåŠ¡å™¨çš„ç¯å¢ƒæ­å»ºæŒ‡å—ï¼›æœ¬é¡¹ç›®å¯¹æœ¬åœ°ç¡¬ä»¶åŸºæœ¬æ²¡æœ‰è¦æ±‚ï¼Œä¸éœ€è¦ GPU ç¯å¢ƒï¼Œä¸ªäººç”µè„‘åŠæœåŠ¡å™¨å‡å¯ç”¨äºå­¦ä¹ ã€‚&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ³¨ï¼šæœ¬é¡¹ç›®ä¸»è¦ä½¿ç”¨å„å¤§æ¨¡å‹å‚å•†æä¾›çš„ API æ¥è¿›è¡Œåº”ç”¨å¼€å‘ï¼Œå¦‚æœä½ æƒ³è¦å­¦ä¹ éƒ¨ç½²åº”ç”¨æœ¬åœ°å¼€æº LLMï¼Œæ¬¢è¿å­¦ä¹ åŒæ ·ç”± Datawhale å‡ºå“çš„ &lt;a href=&#34;https://github.com/datawhalechina/self-llm&#34;&gt;Self LLM ï½œ å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å—&lt;/a&gt;ï¼Œè¯¥é¡¹ç›®å°†æ‰‹æŠŠæ‰‹æ•™ä½ å¦‚ä½•é€Ÿé€šå¼€æº LLM éƒ¨ç½²å¾®è°ƒå…¨é“¾è·¯ï¼&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ³¨ï¼šè€ƒè™‘åˆ°å­¦ä¹ éš¾åº¦ï¼Œæœ¬é¡¹ç›®ä¸»è¦é¢å‘åˆå­¦è€…ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨ LLM æ¥æ­å»ºåº”ç”¨ã€‚å¦‚æœä½ æƒ³è¦è¿›ä¸€æ­¥æ·±å…¥å­¦ä¹  LLM çš„ç†è®ºåŸºç¡€ï¼Œå¹¶åœ¨ç†è®ºçš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥è®¤è¯†ã€åº”ç”¨ LLMï¼Œæ¬¢è¿å­¦ä¹ åŒæ ·ç”± Datawhale å‡ºå“çš„ &lt;a href=&#34;https://github.com/datawhalechina/so-large-lm&#34;&gt;So Large LM | å¤§æ¨¡å‹åŸºç¡€&lt;/a&gt;ï¼Œè¯¥é¡¹ç›®å°†ä¸ºä½ æä¾›å…¨é¢è€Œæ·±å…¥çš„ LLM ç†è®ºçŸ¥è¯†åŠå®è·µæ–¹æ³•ï¼&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;é¡¹ç›®äº®ç‚¹&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;å……åˆ†é¢å‘å®è·µï¼ŒåŠ¨æ‰‹å­¦ä¹ å¤§æ¨¡å‹å¼€å‘ã€‚ç›¸è¾ƒäºå…¶ä»–ä»ç†è®ºå…¥æ‰‹ã€ä¸å®è·µä»£å·®è¾ƒå¤§çš„ç±»ä¼¼æ•™ç¨‹ï¼Œæœ¬æ•™ç¨‹åŸºäºå…·æœ‰é€šç”¨æ€§çš„ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹é¡¹ç›®æ‰“é€ ï¼Œå°†æ™®é€‚çš„å¤§æ¨¡å‹å¼€å‘ç†å¿µèåˆåœ¨é¡¹ç›®å®è·µä¸­ï¼Œå¸®åŠ©å­¦ä¹ è€…é€šè¿‡åŠ¨æ‰‹æ­å»ºä¸ªäººé¡¹ç›®æ¥æŒæ¡å¤§æ¨¡å‹å¼€å‘æŠ€èƒ½ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;ä»é›¶å¼€å§‹ï¼Œå…¨é¢åˆç®€çŸ­çš„å¤§æ¨¡å‹æ•™ç¨‹ã€‚æœ¬é¡¹ç›®é’ˆå¯¹ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹é¡¹ç›®ï¼Œå¯¹ç›¸å…³å¤§æ¨¡å‹å¼€å‘ç†è®ºã€æ¦‚å¿µå’ŒåŸºæœ¬æŠ€èƒ½è¿›è¡Œäº†é¡¹ç›®ä¸»å¯¼çš„é‡æ„ï¼Œåˆ å»ä¸éœ€è¦ç†è§£çš„åº•å±‚åŸç†å’Œç®—æ³•ç»†èŠ‚ï¼Œæ¶µç›–æ‰€æœ‰å¤§æ¨¡å‹å¼€å‘çš„æ ¸å¿ƒæŠ€èƒ½ã€‚æ•™ç¨‹æ•´ä½“æ—¶é•¿åœ¨æ•°å°æ—¶ä¹‹å†…ï¼Œä½†å­¦ä¹ å®Œæœ¬æ•™ç¨‹ï¼Œå¯ä»¥æŒæ¡åŸºç¡€å¤§æ¨¡å‹å¼€å‘çš„æ‰€æœ‰æ ¸å¿ƒæŠ€èƒ½ã€‚&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;å…¼å…·ç»Ÿä¸€æ€§ä¸æ‹“å±•æ€§ã€‚æœ¬é¡¹ç›®å¯¹ GPTã€ç™¾åº¦æ–‡å¿ƒã€è®¯é£æ˜Ÿç«ã€æ™ºè°±GLM ç­‰å›½å†…å¤–ä¸»è¦ LLM API è¿›è¡Œäº†ç»Ÿä¸€å°è£…ï¼Œæ”¯æŒä¸€é”®è°ƒç”¨ä¸åŒçš„ LLMï¼Œå¸®åŠ©å¼€å‘è€…å°†æ›´å¤šçš„ç²¾åŠ›æ”¾åœ¨å­¦ä¹ åº”ç”¨ä¸æ¨¡å‹æœ¬èº«çš„ä¼˜åŒ–ä¸Šï¼Œè€Œä¸éœ€è¦èŠ±æ—¶é—´åœ¨ç¹ççš„è°ƒç”¨ç»†èŠ‚ä¸Šï¼›åŒæ—¶ï¼Œæœ¬æ•™ç¨‹æ‹Ÿä¸Šçº¿ &lt;a href=&#34;https://1aigc.cn/&#34;&gt;å¥‡æƒ³æ˜Ÿçƒ | AIGCå…±åˆ›ç¤¾åŒºå¹³å°&lt;/a&gt;ï¼Œæ”¯æŒå­¦ä¹ è€…è‡ªå®šä¹‰é¡¹ç›®ä¸ºæœ¬æ•™ç¨‹å¢åŠ æ‹“å±•å†…å®¹ï¼Œå…·å¤‡å……åˆ†çš„æ‹“å±•æ€§ã€‚&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;åœ¨çº¿é˜…è¯»åœ°å€&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://datawhalechina.github.io/llm-universe/&#34;&gt;https://datawhalechina.github.io/llm-universe/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;PDF åœ°å€&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/datawhalechina/llm-universe/releases/tag/v1&#34;&gt;https://github.com/datawhalechina/llm-universe/releases/tag/v1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;å†…å®¹å¤§çº²&lt;/h2&gt; &#xA;&lt;h3&gt;ç¬¬ä¸€éƒ¨åˆ† LLM å¼€å‘å…¥é—¨&lt;/h3&gt; &#xA;&lt;p&gt;è´Ÿè´£äººï¼šé‚¹é›¨è¡¡&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/&#34;&gt;LLM ä»‹ç»&lt;/a&gt; @é«˜ç«‹ä¸š &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/1.%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E7%90%86%E8%AE%BA%E7%AE%80%E4%BB%8B.md&#34;&gt;LLM çš„ç†è®ºä»‹ç»&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/2.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%20RAG%20%E7%AE%80%E4%BB%8B.md&#34;&gt;ä»€ä¹ˆæ˜¯ RAGï¼ŒRAG çš„æ ¸å¿ƒä¼˜åŠ¿&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/3.LangChain%20%E7%AE%80%E4%BB%8B.md&#34;&gt;ä»€ä¹ˆæ˜¯ LangChain&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/4.%E5%BC%80%E5%8F%91%20LLM%20%E5%BA%94%E7%94%A8%E7%9A%84%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B.md&#34;&gt;å¼€å‘ LLM åº”ç”¨çš„æ•´ä½“æµç¨‹&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/5.%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.md&#34;&gt;é˜¿é‡Œäº‘æœåŠ¡å™¨çš„åŸºæœ¬ä½¿ç”¨&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/6.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.md&#34;&gt;ç¯å¢ƒé…ç½®&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/&#34;&gt;ä½¿ç”¨ LLM API å¼€å‘åº”ç”¨&lt;/a&gt; @æ¯›é›¨ &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/1.%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md&#34;&gt;åŸºæœ¬æ¦‚å¿µ&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/2.%20%E4%BD%BF%E7%94%A8%20LLM%20API.ipynb&#34;&gt;ä½¿ç”¨ LLM API&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;ChatGPT&lt;/li&gt; &#xA;     &lt;li&gt;æ–‡å¿ƒä¸€è¨€&lt;/li&gt; &#xA;     &lt;li&gt;è®¯é£æ˜Ÿç«&lt;/li&gt; &#xA;     &lt;li&gt;æ™ºè°± GLM&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/3.%20Prompt%20Engineering.ipynb&#34;&gt;Prompt Engineering&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/&#34;&gt;æ­å»ºçŸ¥è¯†åº“&lt;/a&gt; @å¨„å¤©å¥¥ &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/1.%E8%AF%8D%E5%90%91%E9%87%8F%E5%8F%8A%E5%90%91%E9%87%8F%E7%9F%A5%E8%AF%86%E5%BA%93%E4%BB%8B%E7%BB%8D.md&#34;&gt;è¯å‘é‡åŠå‘é‡çŸ¥è¯†åº“ä»‹ç»&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/2.%E4%BD%BF%E7%94%A8%20Embedding%20API.ipynb&#34;&gt;ä½¿ç”¨ Embedding API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3.%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.ipynb&#34;&gt;æ•°æ®å¤„ç†ï¼šè¯»å–ã€æ¸…æ´—ä¸åˆ‡ç‰‡&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/4.%E6%90%AD%E5%BB%BA%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93.ipynb&#34;&gt;æ­å»ºå¹¶ä½¿ç”¨å‘é‡æ•°æ®åº“&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/&#34;&gt;æ„å»º RAG åº”ç”¨&lt;/a&gt; @å¾è™ &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/1.LLM%20%E6%8E%A5%E5%85%A5%20LangChain.ipynb&#34;&gt;å°† LLM æ¥å…¥ LangChain&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;ChatGPT&lt;/li&gt; &#xA;     &lt;li&gt;æ–‡å¿ƒä¸€è¨€&lt;/li&gt; &#xA;     &lt;li&gt;è®¯é£æ˜Ÿç«&lt;/li&gt; &#xA;     &lt;li&gt;æ™ºè°± GLM&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/2.%E6%9E%84%E5%BB%BA%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE.ipynb&#34;&gt;åŸºäº LangChain æ­å»ºæ£€ç´¢é—®ç­”é“¾&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/3.%E9%83%A8%E7%BD%B2%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.ipynb&#34;&gt;åŸºäº Streamlit éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C5%20%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96/&#34;&gt;ç³»ç»Ÿè¯„ä¼°ä¸ä¼˜åŒ–&lt;/a&gt; @é‚¹é›¨è¡¡ &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C5%20%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96/1.%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0%20LLM%20%E5%BA%94%E7%94%A8.ipynb&#34;&gt;å¦‚ä½•è¯„ä¼° LLM åº”ç”¨&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C5%20%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96/2.%E8%AF%84%E4%BC%B0%E5%B9%B6%E4%BC%98%E5%8C%96%E7%94%9F%E6%88%90%E9%83%A8%E5%88%86.ipynb&#34;&gt;è¯„ä¼°å¹¶ä¼˜åŒ–ç”Ÿæˆéƒ¨åˆ†&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C5%20%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96/3.%E8%AF%84%E4%BC%B0%E5%B9%B6%E4%BC%98%E5%8C%96%E6%A3%80%E7%B4%A2%E9%83%A8%E5%88%86.md&#34;&gt;è¯„ä¼°å¹¶ä¼˜åŒ–æ£€ç´¢éƒ¨åˆ†&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;ç¬¬äºŒéƒ¨åˆ† è¿›é˜¶ RAG æŠ€å·§ï¼ˆæ­£åœ¨åˆ›ä½œï¼‰&lt;/h3&gt; &#xA;&lt;p&gt;è´Ÿè´£äººï¼šé«˜ç«‹ä¸š&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;èƒŒæ™¯ &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;æ¶æ„æ¦‚è§ˆ&lt;/li&gt; &#xA;   &lt;li&gt;å­˜åœ¨çš„é—®é¢˜&lt;/li&gt; &#xA;   &lt;li&gt;è§£å†³æ–¹æ³•&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;æ•°æ®å¤„ç† &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;å¤šç±»å‹æ–‡æ¡£å¤„ç†&lt;/li&gt; &#xA;   &lt;li&gt;åˆ†å—ä¼˜åŒ–&lt;/li&gt; &#xA;   &lt;li&gt;å‘é‡æ¨¡å‹çš„é€‰æ‹©&lt;/li&gt; &#xA;   &lt;li&gt;å¾®è°ƒå‘é‡æ¨¡å‹ï¼ˆè¿›é˜¶ï¼‰&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç´¢å¼•å±‚é¢ &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;ç´¢å¼•ç»“æ„&lt;/li&gt; &#xA;   &lt;li&gt;æ··åˆæ£€ç´¢&lt;/li&gt; &#xA;   &lt;li&gt;å‡è®¾æ€§é—®é¢˜&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;æ£€ç´¢é˜¶æ®µ &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;query è¿‡æ»¤&lt;/li&gt; &#xA;   &lt;li&gt;å¯¹é½ query å’Œ æ–‡æ¡£&lt;/li&gt; &#xA;   &lt;li&gt;å¯¹é½æ£€ç´¢å’Œ LLM&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;ç”Ÿæˆé˜¶æ®µ &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;åå¤„ç†&lt;/li&gt; &#xA;   &lt;li&gt;å¾®è°ƒ LLMï¼ˆè¿›é˜¶ï¼‰&lt;/li&gt; &#xA;   &lt;li&gt;å‚è€ƒå¼•ç”¨&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;å¢å¼ºé˜¶æ®µ &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;ä¸Šä¸‹æ–‡å¢å¼º&lt;/li&gt; &#xA;   &lt;li&gt;å¢å¼ºæµç¨‹&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;RAG å·¥ç¨‹åŒ–è¯„ä¼°&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;ç¬¬ä¸‰éƒ¨åˆ† å¼€æº LLM åº”ç”¨è§£è¯»&lt;/h3&gt; &#xA;&lt;p&gt;è´Ÿè´£äººï¼šå¾è™&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ChatWithDatawhaleâ€”â€”ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹è§£è¯»&lt;/li&gt; &#xA; &lt;li&gt;å¤©æœºâ€”â€”äººæƒ…ä¸–æ•…å¤§æ¨¡å‹è§£è¯»&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;è‡´è°¢&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;æ ¸å¿ƒè´¡çŒ®è€…&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/logan-zou&#34;&gt;é‚¹é›¨è¡¡-é¡¹ç›®è´Ÿè´£äºº&lt;/a&gt;ï¼ˆDatawhaleæˆå‘˜-å¯¹å¤–ç»æµè´¸æ˜“å¤§å­¦ç ”ç©¶ç”Ÿï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/0-yy-0&#34;&gt;é«˜ç«‹ä¸š-ç¬¬äºŒéƒ¨åˆ†è´Ÿè´£äºº&lt;/a&gt;ï¼ˆDataWhaleæˆå‘˜-ç®—æ³•å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xuhu0115&#34;&gt;å¾è™-ç¬¬ä¸‰éƒ¨åˆ†è´Ÿè´£äºº&lt;/a&gt;ï¼ˆDatawhaleæˆå‘˜-ç®—æ³•å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;ä¸»è¦è´¡çŒ®è€…&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Myoungs&#34;&gt;æ¯›é›¨-å†…å®¹åˆ›ä½œè€…&lt;/a&gt;ï¼ˆåç«¯å¼€å‘å·¥ç¨‹å¸ˆï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lta155&#34;&gt;å¨„å¤©å¥¥-å†…å®¹åˆ›ä½œè€…&lt;/a&gt;ï¼ˆDatawhaleé²¸è‹±åŠ©æ•™-ä¸­å›½ç§‘å­¦é™¢å¤§å­¦ç ”ç©¶ç”Ÿï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/2951121599&#34;&gt;å´”è…¾æ¾-é¡¹ç›®æ”¯æŒè€…&lt;/a&gt;ï¼ˆDatawhaleæˆå‘˜-å¥‡æƒ³æ˜Ÿçƒè”åˆå‘èµ·äººï¼‰&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JuneYaooo&#34;&gt;June-é¡¹ç›®æ”¯æŒè€…&lt;/a&gt;ï¼ˆDatawhaleæˆå‘˜-å¥‡æƒ³æ˜Ÿçƒè”åˆå‘èµ·äººï¼‰&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;å…¶ä»–&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ç‰¹åˆ«æ„Ÿè°¢ &lt;a href=&#34;https://github.com/Sm1les&#34;&gt;@Sm1les&lt;/a&gt;ã€&lt;a href=&#34;https://github.com/LSGOMYP&#34;&gt;@LSGOMYP&lt;/a&gt; å¯¹æœ¬é¡¹ç›®çš„å¸®åŠ©ä¸æ”¯æŒï¼›&lt;/li&gt; &#xA; &lt;li&gt;ç‰¹åˆ«æ„Ÿè°¢&lt;a href=&#34;https://1aigc.cn/&#34;&gt;å¥‡æƒ³æ˜Ÿçƒ | AIGCå…±åˆ›ç¤¾åŒºå¹³å°&lt;/a&gt;æä¾›çš„æ”¯æŒï¼Œæ¬¢è¿å¤§å®¶å…³æ³¨ï¼›&lt;/li&gt; &#xA; &lt;li&gt;å¦‚æœæœ‰ä»»ä½•æƒ³æ³•å¯ä»¥è”ç³»æˆ‘ä»¬ DataWhale ä¹Ÿæ¬¢è¿å¤§å®¶å¤šå¤šæå‡º issueï¼›&lt;/li&gt; &#xA; &lt;li&gt;ç‰¹åˆ«æ„Ÿè°¢ä»¥ä¸‹ä¸ºæ•™ç¨‹åšå‡ºè´¡çŒ®çš„åŒå­¦ï¼&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;a href=&#34;https://github.com/datawhalechina/llm-universe/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=datawhalechina/llm-universe&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#datawhalechina/llm-universe&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=datawhalechina/llm-universe&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PKU-YuanGroup/MagicTime</title>
    <updated>2024-04-15T02:45:34Z</updated>
    <id>tag:github.com,2024-04-15:/PKU-YuanGroup/MagicTime</id>
    <link href="https://github.com/PKU-YuanGroup/MagicTime" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/magictime_logo.png&#34; width=&#34;150px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;MagicTime: Time-lapse Video Generation Models &lt;/a&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;as Metamorphic Simulators&lt;/a&gt;&lt;/p&gt;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; If you like our project, please give us a star â­ on GitHub for the latest update. &lt;/h5&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/BestWishYsh/MagicTime?logs=build&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20In%20Spaces-blue.svg?sanitize=true&#34; alt=&#34;hf_space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/camenduru/magictime&#34;&gt;&lt;img src=&#34;https://replicate.com/camenduru/magictime/badge&#34; alt=&#34;Replicate demo and cloud API&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/camenduru/MagicTime-jupyter&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2404.05014&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Paper%20In%20HF-red.svg?sanitize=true&#34; alt=&#34;hf_space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2404.05014-b31b1b.svg?logo=arXiv&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pku-yuangroup.github.io/MagicTime/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-%3CWebsite%3E-blue.svg?sanitize=true&#34; alt=&#34;Home Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/drive/folders/1WsomdkmSp3ql3ImcNsmzFuSQ9Qukuyr8?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Dataset-%3CGoogle%3E-green&#34; alt=&#34;Dataset&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/_akhaliq/status/1777538468043792473&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Twitter@AK%20-black?logo=twitter&amp;amp;logoColor=1D9BF0&#34; alt=&#34;zhihu&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/vhjf36495872/status/1777525817087553827?s=61&amp;amp;t=r2HzCsU2AnJKbR8yKSprKw&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Twitter@Jinfa%20Huang%20-black?logo=twitter&amp;amp;logoColor=1D9BF0&#34; alt=&#34;zhihu&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zenodo.org/doi/10.5281/zenodo.10960665&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/783303222.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-yellow&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/MagicTime&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/p&gt; &lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  This repository is the official implementation of MagicTime, a metamorphic video generation pipeline based on the given prompts. The main idea is to enhance the capacity of video generation models to accurately depict the real world through our proposed methods and dataset. &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary&gt;ğŸ’¡ We also have other video generation project that may interest you âœ¨. &lt;/summary&gt;&#xA; &lt;p&gt; &#xA;  &lt;!--  may --&gt; &lt;/p&gt;&#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;&lt;strong&gt;Open-Sora-Plan&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; PKU-Yuan Lab and Tuzhan AI etc. &lt;br&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Github-black?logo=github&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan.svg?style=social&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;h2&gt;ğŸ“£ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;â³â³â³ Training a stronger model with the support of &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;Open-Sora-Plan&lt;/a&gt; (e.g 257 x 512 Ã— 512).&lt;/li&gt; &#xA; &lt;li&gt;â³â³â³ Release the training code of MagicTime.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.14]&lt;/code&gt; Thanks &lt;a href=&#34;https://twitter.com/camenduru&#34;&gt;@camenduru&lt;/a&gt; and &lt;a href=&#34;https://modelslab.com/&#34;&gt;@ModelsLab&lt;/a&gt; for providing Jupyter Notebook &lt;a href=&#34;https://github.com/camenduru/MagicTime-jupyter&#34;&gt;MagicTime-jupyter&lt;/a&gt; and &lt;a href=&#34;https://replicate.com/camenduru/magictime&#34;&gt;Replicate Demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.13]&lt;/code&gt; ğŸ”¥ We have compressed the size of repo with less than 1.0 MB, so that everyone can clone easier and faster. You can click &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/archive/refs/heads/main.zip&#34;&gt;here&lt;/a&gt; to download, or use &lt;code&gt;git clone --depth=1&lt;/code&gt; command to obtain this repo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.12]&lt;/code&gt; Thanks &lt;a href=&#34;https://github.com/kijai&#34;&gt;@Jukka SeppÃ¤nen&lt;/a&gt; and &lt;a href=&#34;https://www.bilibili.com/video/BV1wx421U7Gn/?spm_id_from=333.1007.top_right_bar_window_history.content.click&#34;&gt;@Baobao Wang&lt;/a&gt; for providing ComfyUI Extension &lt;a href=&#34;https://github.com/kijai/ComfyUI-MagicTimeWrapper&#34;&gt;ComfyUI-MagicTimeWrapper&lt;/a&gt;. If you find related work, please let us know.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.11]&lt;/code&gt; ğŸ”¥ We release the Hugging Face Space of MagicTime, you can &lt;a href=&#34;https://huggingface.co/spaces/BestWishYsh/MagicTime?logs=build&#34;&gt;click&lt;/a&gt; here to have a try.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.10]&lt;/code&gt; ğŸ”¥ We release the inference code and model weight of MagicTime.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.09]&lt;/code&gt; ğŸ”¥ We release the arXiv paper for MagicTime, and you can click &lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;here&lt;/a&gt; to see more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.08]&lt;/code&gt; ğŸ”¥ We released the subset of ChronoMagic dataset used to train MagicTime. The dataset includes 2,265 metamorphic video-text pairs and can be downloaded at &lt;a href=&#34;https://drive.google.com/drive/folders/1WsomdkmSp3ql3ImcNsmzFuSQ9Qukuyr8?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.08]&lt;/code&gt; ğŸ”¥ &lt;strong&gt;All codes &amp;amp; datasets&lt;/strong&gt; are coming soon! Stay tuned ğŸ‘€!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ˜® Highlights&lt;/h2&gt; &#xA;&lt;p&gt;MagicTime shows excellent performance in &lt;strong&gt;metamorphic video generation&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Metamorphic Videos vs. General Videos&lt;/h3&gt; &#xA;&lt;p&gt;Compared to general videos, metamorphic videos contain physical knowledge, long persistence, and strong variation, making them difficult to generate. We show compressed .gif on github, which loses some quality. The general videos are generated by the &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;Animatediff&lt;/a&gt; and &lt;strong&gt;MagicTime&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     Type&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Bean sprouts grow and mature from seeds&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] construction in a Minecraft virtual environment&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Cupcakes baking in an oven [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] transitioning from a tightly closed bud to a fully bloomed state [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;General Videos&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_0_0.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_0_1.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_0_2.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_0_3.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Metamorphic Videos&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_1_0.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_1_1.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_1_2.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_1_3.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Gallery&lt;/h3&gt; &#xA;&lt;p&gt;We showcase some metamorphic videos generated by &lt;strong&gt;MagicTime&lt;/strong&gt;, &lt;a href=&#34;https://github.com/xuduo35/MakeLongVideo&#34;&gt;MakeLongVideo&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope&#34;&gt;ModelScopeT2V&lt;/a&gt;, &lt;a href=&#34;https://github.com/AILab-CVC/VideoCrafter?tab=readme-ov-file&#34;&gt;VideoCrafter&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/cerspense/zeroscope_v2_576w&#34;&gt;ZeroScope&lt;/a&gt;, &lt;a href=&#34;https://github.com/Vchitect/LaVie&#34;&gt;LaVie&lt;/a&gt;, &lt;a href=&#34;https://github.com/Picsart-AI-Research/Text2Video-Zero&#34;&gt;T2V-Zero&lt;/a&gt;, &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt; and &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;Animatediff&lt;/a&gt; below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     Method&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;cherry blossoms transitioning [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;dough balls baking process [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;an ice cube is melting [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;a simple modern house&#39;s construction [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MakeLongVideo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_0_0.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_0_1.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_0_2.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_0_3.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ModelScopeT2V&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_1_0.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_1_1.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_1_2.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_1_3.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VideoCrafter&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_2_0.gif?raw=true&#34; alt=&#34;VideoCrafter&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_2_1.gif?raw=true&#34; alt=&#34;VideoCrafter&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_2_2.gif?raw=true&#34; alt=&#34;VideoCrafter&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_2_3.gif?raw=true&#34; alt=&#34;VideoCrafter&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ZeroScope&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_3_0.gif?raw=true&#34; alt=&#34;ZeroScope&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_3_1.gif?raw=true&#34; alt=&#34;ZeroScope&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_3_2.gif?raw=true&#34; alt=&#34;ZeroScope&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_3_3.gif?raw=true&#34; alt=&#34;ZeroScope&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LaVie&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_4_0.gif?raw=true&#34; alt=&#34;LaVie&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_4_1.gif?raw=true&#34; alt=&#34;LaVie&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_4_2.gif?raw=true&#34; alt=&#34;LaVie&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_4_3.gif?raw=true&#34; alt=&#34;LaVie&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T2V-Zero&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_5_0.gif?raw=true&#34; alt=&#34;T2V-Zero&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_5_1.gif?raw=true&#34; alt=&#34;T2V-Zero&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_5_2.gif?raw=true&#34; alt=&#34;T2V-Zero&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_5_3.gif?raw=true&#34; alt=&#34;T2V-Zero&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Latte&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_6_0.gif?raw=true&#34; alt=&#34;Latte&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_6_1.gif?raw=true&#34; alt=&#34;Latte&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_6_2.gif?raw=true&#34; alt=&#34;Latte&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_6_3.gif?raw=true&#34; alt=&#34;Latte&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Animatediff&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_7_0.gif?raw=true&#34; alt=&#34;Animatediff&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_7_1.gif?raw=true&#34; alt=&#34;Animatediff&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_7_2.gif?raw=true&#34; alt=&#34;Animatediff&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_7_3.gif?raw=true&#34; alt=&#34;Animatediff&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ours&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_8_0.gif?raw=true&#34; alt=&#34;Ours&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_8_1.gif?raw=true&#34; alt=&#34;Ours&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_8_2.gif?raw=true&#34; alt=&#34;Ours&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_8_3.gif?raw=true&#34; alt=&#34;Ours&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;We show more metamorphic videos generated by &lt;strong&gt;MagicTime&lt;/strong&gt; with the help of &lt;a href=&#34;https://civitai.com/models/4201/realistic-vision-v20&#34;&gt;Realistic&lt;/a&gt;, &lt;a href=&#34;https://civitai.com/models/30240/toonyou&#34;&gt;ToonYou&lt;/a&gt; and &lt;a href=&#34;https://civitai.com/models/66347/rcnz-cartoon-3d&#34;&gt;RcnzCartoon&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_0_0.gif?raw=true&#34; alt=&#34;Realistic&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_0_1.gif?raw=true&#34; alt=&#34;Realistic&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_0_2.gif?raw=true&#34; alt=&#34;Realistic&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] bean sprouts grow and mature from seeds&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;dough [...] swells and browns in the oven [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;the construction [...] in Minecraft [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_1_0.gif?raw=true&#34; alt=&#34;RcnzCartoon&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_1_1.gif?raw=true&#34; alt=&#34;RcnzCartoon&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_1_2.gif?raw=true&#34; alt=&#34;RcnzCartoon&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;a bud transforms into a yellow flower&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;time-lapse of a plant germinating [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] a modern house being constructed in Minecraft [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_2_0.gif?raw=true&#34; alt=&#34;ToonYou&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_2_1.gif?raw=true&#34; alt=&#34;ToonYou&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_2_2.gif?raw=true&#34; alt=&#34;ToonYou&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;an ice cube is melting&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;bean plant sprouts grow and mature from the soil&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;time-lapse of delicate pink plum blossoms [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Prompts are trimmed for display, see &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/main/__assets__/promtp_unet.txt&#34;&gt;here&lt;/a&gt; for full prompts.&lt;/p&gt; &#xA;&lt;h3&gt;Integrate into DiT-based Architecture&lt;/h3&gt; &#xA;&lt;p&gt;The mission of this project is to help reproduce Sora and provide high-quality video-text data and data annotation pipelines, to support &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;Open-Sora-Plan&lt;/a&gt; or other DiT-based T2V models. To this end, we take an initial step to integrate our MagicTime scheme into the DiT-based Framework. Specifically, our method supports the Open-Sora-Plan v1.0.0 for fine-tuning. We first scale up with additional metamorphic landscape time-lapse videos in the same annotation framework to get the ChronoMagic-Landscape dataset. Then, we fine-tune the Open-Sora-Plan v1.0.0 with the ChronoMagic-Landscape dataset to get the MagicTime-DiT model. The results are as follows (&lt;strong&gt;257Ã—512Ã—512 (10s)&lt;/strong&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_0.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_1.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_2.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_3.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Time-lapse of a coastal landscape [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Display the serene beauty of twilight [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Sunrise Splendor: Capture the breathtaking moment [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Nightfall Elegance: Embrace the tranquil beauty [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_4.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_5.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_6.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_7.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;The sun descending below the horizon [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] daylight fades into the embrace of the night [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Time-lapse of the dynamic formations of clouds [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Capture the dynamic formations of clouds [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Prompts are trimmed for display, see &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/main/__assets__/promtp_opensora.txt&#34;&gt;here&lt;/a&gt; for full prompts.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ¤— Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;Highly recommend trying out our web demo by the following command, which incorporates all features currently supported by MagicTime. We also provide &lt;a href=&#34;https://huggingface.co/spaces/BestWishYsh/MagicTime?logs=build&#34;&gt;online demo&lt;/a&gt; in Hugging Face Spaces.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Realistic&#xA;python inference_magictime.py --config sample_configs/RealisticVision.yaml --human&#xA;&#xA;# or you can directly run the .sh&#xA;sh inference_cli.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;warning: It is worth noting that even if we use the same seed and prompt but we change a machine, the results will be different.&lt;/p&gt; &#xA;&lt;h2&gt;âš™ï¸ Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;We recommend the requirements as follows.&lt;/p&gt; &#xA;&lt;h3&gt;Environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --depth=1 https://github.com/PKU-YuanGroup/MagicTime.git&#xA;cd MagicTime&#xA;conda create -n magictime python=3.10.13&#xA;conda activate magictime&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Base Model and Dreambooth&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh prepare_weights/down_base_model.sh&#xA;sh prepare_weights/down_dreambooth.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prepare MagicTime Module&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh prepare_weights/down_magictime_module.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ—ï¸ Training &amp;amp; Inference&lt;/h2&gt; &#xA;&lt;p&gt;The training code is coming soon! For inference, some example are shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Realistic&#xA;python inference_magictime.py --config sample_configs/RealisticVision.yaml&#xA;# For ToonYou&#xA;python inference_magictime.py --config sample_configs/ToonYou.yaml&#xA;# For RcnzCartoon&#xA;python inference_magictime.py --config sample_configs/RcnzCartoon.yaml&#xA;# or you can directly run the .sh&#xA;sh inference.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Community Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We found some plugins created by community developers. Thanks for their efforts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ComfyUI Extension &lt;a href=&#34;https://github.com/kijai/ComfyUI-MagicTimeWrapper&#34;&gt;ComfyUI-MagicTimeWrapper&lt;/a&gt; (by &lt;a href=&#34;https://github.com/kijai&#34;&gt;@Jukka SeppÃ¤nen&lt;/a&gt;). And you can click &lt;a href=&#34;https://www.bilibili.com/video/BV1wx421U7Gn/?spm_id_from=333.1007.top_right_bar_window_history.content.click&#34;&gt;here&lt;/a&gt; to view the installation tutorial.&lt;/li&gt; &#xA; &lt;li&gt;Replicate Demo &amp;amp; Cloud API &lt;a href=&#34;https://replicate.com/camenduru/magictime&#34;&gt;Replicate-MagicTime&lt;/a&gt; (by &lt;a href=&#34;https://twitter.com/camenduru&#34;&gt;@camenduru&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Jupyter Notebook &lt;a href=&#34;https://github.com/camenduru/MagicTime-jupyter&#34;&gt;Jupyter-MagicTime&lt;/a&gt; (by &lt;a href=&#34;https://modelslab.com/&#34;&gt;@ModelsLab&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you find related work, please let us know.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ³ ChronoMagic Dataset&lt;/h2&gt; &#xA;&lt;p&gt;ChronoMagic with 2265 metamorphic time-lapse videos, each accompanied by a detailed caption. We released the subset of ChronoMagic used to train MagicTime. The dataset can be downloaded at &lt;a href=&#34;https://drive.google.com/drive/folders/1WsomdkmSp3ql3ImcNsmzFuSQ9Qukuyr8?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;. Some samples can be found on our Project Page.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ‘ Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/guoyww/AnimateDiff/tree/main&#34;&gt;Animatediff&lt;/a&gt; The codebase we built upon and it is a strong U-Net-based text-to-video generation model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;Open-Sora-Plan&lt;/a&gt; The codebase we built upon and it is a simple and scalable DiT-based text-to-video generation repo, to reproduce &lt;a href=&#34;https://openai.com/sora&#34;&gt;Sora&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ğŸ”’ License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The majority of this project is released under the Apache 2.0 license as found in the &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;The service is a research preview. Please contact us if you find any potential violations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;âœï¸ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;â­&lt;/span&gt; and citation &lt;span&gt;ğŸ“&lt;/span&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{yuan2024magictime,&#xA;  title={MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators},&#xA;  author={Yuan, Shenghai and Huang, Jinfa and Shi, Yujun and Xu, Yongqi and Zhu, Ruijie and Lin, Bin and Cheng, Xinhua and Yuan, Li and Luo, Jiebo},&#xA;  journal={arXiv preprint arXiv:2404.05014},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ¤ Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=PKU-YuanGroup/MagicTime&amp;amp;anon=true&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/parler-tts</title>
    <updated>2024-04-15T02:45:34Z</updated>
    <id>tag:github.com,2024-04-15:/huggingface/parler-tts</id>
    <link href="https://github.com/huggingface/parler-tts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference and training library for high-quality TTS models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Parler-TTS&lt;/h1&gt; &#xA;&lt;p&gt;Parler-TTS is a lightweight text-to-speech (TTS) model that can generate high-quality, natural sounding speech in the style of a given speaker (gender, pitch, speaking style, etc). It is a reproduction of work from the paper &lt;a href=&#34;https://www.text-description-to-speech.com&#34;&gt;Natural language guidance of high-fidelity text-to-speech with synthetic annotations&lt;/a&gt; by Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively.&lt;/p&gt; &#xA;&lt;p&gt;Contrarily to other TTS models, Parler-TTS is a &lt;strong&gt;fully open-source&lt;/strong&gt; release. All of the datasets, pre-processing, training code and weights are released publicly under permissive license, enabling the community to build on our work and develop their own powerful TTS models.&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the inference and training code for Parler-TTS. It is designed to accompany the &lt;a href=&#34;https://github.com/huggingface/dataspeech&#34;&gt;Data-Speech&lt;/a&gt; repository for dataset annotation.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] We&#39;re proud to release &lt;a href=&#34;https://huggingface.co/parler-tts/parler_tts_mini_v0.1&#34;&gt;Parler-TTS Mini v0.1&lt;/a&gt;, our first 600M parameter model, trained on 10.5K hours of audio data. In the coming weeks, we&#39;ll be working on scaling up to 50k hours of data, in preparation for the v1 model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;ğŸ“– Quick Index&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/parler-tts/parler_tts_mini&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/parler-tts&#34;&gt;Model weights and datasets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Parler-TTS has light-weight dependencies and can be installed in one line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install git+https://github.com/huggingface/parler-tts.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] You can directly try it out in an interactive demo &lt;a href=&#34;https://huggingface.co/spaces/parler-tts/parler_tts_mini&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Using Parler-TTS is as simple as &#34;bonjour&#34;. Simply use the following inference snippet.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from parler_tts import ParlerTTSForConditionalGeneration&#xA;from transformers import AutoTokenizer&#xA;import soundfile as sf&#xA;import torch&#xA;&#xA;device = &#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;&#xA;model = ParlerTTSForConditionalGeneration.from_pretrained(&#34;parler-tts/parler_tts_mini_v0.1&#34;).to(device)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;parler-tts/parler_tts_mini_v0.1&#34;)&#xA;&#xA;prompt = &#34;Hey, how are you doing today?&#34;&#xA;description = &#34;A female speaker with a slightly low-pitched voice delivers her words quite expressively, in a very confined sounding environment with clear audio quality. She speaks very fast.&#34;&#xA;&#xA;input_ids = tokenizer(description, return_tensors=&#34;pt&#34;).input_ids.to(device)&#xA;prompt_input_ids = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids.to(device)&#xA;&#xA;generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)&#xA;audio_arr = generation.cpu().numpy().squeeze()&#xA;sf.write(&#34;parler_tts_out.wav&#34;, audio_arr, model.config.sampling_rate)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/parler-tts/assets/52246514/251e2488-fe6e-42c1-81cd-814c5b7795b0&#34;&gt;https://github.com/huggingface/parler-tts/assets/52246514/251e2488-fe6e-42c1-81cd-814c5b7795b0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/&#34;&gt;training folder&lt;/a&gt; contains all the information to train or fine-tune your own Parler-TTS model. It consists of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#1-architecture&#34;&gt;1. An introduction to the Parler-TTS architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#2-getting-started&#34;&gt;2. The first steps to get started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#3-training&#34;&gt;3. A training guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;TL;DR:&lt;/strong&gt; After having followed the &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#requirements&#34;&gt;installation steps&lt;/a&gt;, you can reproduce the Parler-TTS Mini v0.1 training recipe with the following command line:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;accelerate launch ./training/run_parler_tts_training.py ./helpers/training_configs/starting_point_0.01.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This library builds on top of a number of open-source giants, to whom we&#39;d like to extend our warmest thanks for providing these tools!&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively, for publishing such a promising and clear research paper: &lt;a href=&#34;https://arxiv.org/abs/2402.01912&#34;&gt;Natural language guidance of high-fidelity text-to-speech with synthetic annotations&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;the many libraries used, namely &lt;a href=&#34;https://huggingface.co/docs/datasets/v2.17.0/en/index&#34;&gt;ğŸ¤— datasets&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/accelerate/en/index&#34;&gt;ğŸ¤— accelerate&lt;/a&gt;, &lt;a href=&#34;https://github.com/jitsi/jiwer&#34;&gt;jiwer&lt;/a&gt;, &lt;a href=&#34;https://wandb.ai/&#34;&gt;wandb&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;ğŸ¤— transformers&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Descript for the &lt;a href=&#34;https://github.com/descriptinc/descript-audio-codec&#34;&gt;DAC codec model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face ğŸ¤— for providing compute resources and time to explore!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this repository useful, please consider citing this work and also the original Stability AI paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lacombe-etal-2024-parler-tts,&#xA;  author = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},&#xA;  title = {Parler-TTS},&#xA;  year = {2024},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/huggingface/parler-tts}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lyth2024natural,&#xA;      title={Natural language guidance of high-fidelity text-to-speech with synthetic annotations},&#xA;      author={Dan Lyth and Simon King},&#xA;      year={2024},&#xA;      eprint={2402.01912},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.SD}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome, as the project offers many possibilities for improvement and exploration.&lt;/p&gt; &#xA;&lt;p&gt;Namely, we&#39;re looking at ways to improve both quality and speed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Datasets: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Train on more data&lt;/li&gt; &#xA;   &lt;li&gt;Add more features such as accents&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Training: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add PEFT compatibility to do Lora fine-tuning.&lt;/li&gt; &#xA;   &lt;li&gt;Add possibility to train without description column.&lt;/li&gt; &#xA;   &lt;li&gt;Add notebook training.&lt;/li&gt; &#xA;   &lt;li&gt;Explore multilingual training.&lt;/li&gt; &#xA;   &lt;li&gt;Explore mono-speaker finetuning.&lt;/li&gt; &#xA;   &lt;li&gt;Explore more architectures.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Optimization: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Compilation and static cache&lt;/li&gt; &#xA;   &lt;li&gt;Support to FA2 and SDPA&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Evaluation: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add more evaluation metrics&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>