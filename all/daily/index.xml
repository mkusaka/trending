<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-10-09T01:31:34Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ashawkey/stable-dreamfusion</title>
    <updated>2022-10-09T01:31:34Z</updated>
    <id>tag:github.com,2022-10-09:/ashawkey/stable-dreamfusion</id>
    <link href="https://github.com/ashawkey/stable-dreamfusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A pytorch implementation of text-to-3D dreamfusion, powered by stable diffusion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable-Dreamfusion&lt;/h1&gt; &#xA;&lt;p&gt;A pytorch implementation of the text-to-3D model &lt;strong&gt;Dreamfusion&lt;/strong&gt;, powered by the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; text-to-2D model.&lt;/p&gt; &#xA;&lt;p&gt;The original paper&#39;s project page: &lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;&lt;em&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Colab notebook for usage: &lt;a href=&#34;https://colab.research.google.com/drive/1MXT3yfOFvO0ooKEfiUUvTKwUkrrlCHpF?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Examples generated from text prompt &lt;code&gt;a high quality photo of a pineapple&lt;/code&gt; viewed with the GUI in real time:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/25863658/194241493-f3e68f78-aefe-479e-a4a8-001424a61b37.mp4&#34;&gt;https://user-images.githubusercontent.com/25863658/194241493-f3e68f78-aefe-479e-a4a8-001424a61b37.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion/issues/1&#34;&gt;Gallery&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ashawkey/stable-dreamfusion/main/assets/update_logs.md&#34;&gt;Update Logs&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h1&gt;Important Notice&lt;/h1&gt; &#xA;&lt;p&gt;This project is a &lt;strong&gt;work-in-progress&lt;/strong&gt;, and contains lots of differences from the paper. Also, many features are still not implemented now. &lt;strong&gt;The current generation quality cannot match the results from the original paper, and many prompts still fail badly!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Notable differences from the paper&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Since the Imagen model is not publicly available, we use &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; to replace it (implementation from &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;). Different from Imagen, Stable-Diffusion is a latent diffusion model, which diffuses in a latent space instead of the original image space. Therefore, we need the loss to propagate back from the VAE&#39;s encoder part too, which introduces extra time cost in training. Currently, 15000 training steps take about 5 hours to train on a V100.&lt;/li&gt; &#xA; &lt;li&gt;We use the &lt;a href=&#34;https://github.com/NVlabs/instant-ngp/&#34;&gt;multi-resolution grid encoder&lt;/a&gt; to implement the NeRF backbone (implementation from &lt;a href=&#34;https://github.com/ashawkey/torch-ngp&#34;&gt;torch-ngp&lt;/a&gt;), which enables much faster rendering (~10FPS at 800x800).&lt;/li&gt; &#xA; &lt;li&gt;We use the Adam optimizer with a larger initial learning rate.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The normal evaluation &amp;amp; shading part.&lt;/li&gt; &#xA; &lt;li&gt;Better mesh (improve the surface quality).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/ashawkey/stable-dreamfusion.git&#xA;cd stable-dreamfusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: To download the Stable Diffusion model checkpoint, you should provide your &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;access token&lt;/a&gt;. You could choose either of the following ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run &lt;code&gt;huggingface-cli login&lt;/code&gt; and enter your token.&lt;/li&gt; &#xA; &lt;li&gt;Create a file called &lt;code&gt;TOKEN&lt;/code&gt; under this directory (i.e., &lt;code&gt;stable-dreamfusion/TOKEN&lt;/code&gt;) and copy your token into it.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Install with pip&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&#xA;# (optional) install nvdiffrast for exporting textured mesh (--save_mesh)&#xA;pip install git+https://github.com/NVlabs/nvdiffrast/&#xA;&#xA;# (optional) install the tcnn backbone if using --tcnn&#xA;pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&#xA;&#xA;# (optional) install CLIP guidance for the dreamfield setting&#xA;pip install git+https://github.com/openai/CLIP.git&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build extension (optional)&lt;/h3&gt; &#xA;&lt;p&gt;By default, we use &lt;a href=&#34;https://pytorch.org/docs/stable/cpp_extension.html#torch.utils.cpp_extension.load&#34;&gt;&lt;code&gt;load&lt;/code&gt;&lt;/a&gt; to build the extension at runtime. We also provide the &lt;code&gt;setup.py&lt;/code&gt; to build each extension:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# install all extension modules&#xA;bash scripts/install_ext.sh&#xA;&#xA;# if you want to install manually, here is an example:&#xA;pip install ./raymarching # install to python path (you still need the raymarching/ folder, since this only installs the built extension.)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Tested environments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ubuntu 22 with torch 1.12 &amp;amp; CUDA 11.6 on a V100.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;First time running will take some time to compile the CUDA extensions.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;### stable-dreamfusion setting&#xA;## train with text prompt&#xA;# `-O` equals `--cuda_ray --fp16 --dir_text`&#xA;python main.py --text &#34;a hamburger&#34; --workspace trial -O&#xA;&#xA;## after the training is finished:&#xA;# test (exporting 360 video, and an obj mesh with png texture)&#xA;python main.py --workspace trial -O --test&#xA;&#xA;# test with a GUI (free view control!)&#xA;python main.py --workspace trial -O --test --gui&#xA;&#xA;### dreamfields (CLIP) setting&#xA;python main.py --text &#34;a hamburger&#34; --workspace trial_clip -O --guidance clip&#xA;python main.py --text &#34;a hamburger&#34; --workspace trial_clip -O --test --gui --guidance clip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Code organization &amp;amp; Advanced tips&lt;/h1&gt; &#xA;&lt;p&gt;This is a simple description of the most important implementation details. If you are interested in improving this repo, this might be a starting point. Any contribution would be greatly appreciated!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The SDS loss is located at &lt;code&gt;./nerf/sd.py &amp;gt; StableDiffusion &amp;gt; train_step&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# 1. we need to interpolate the NeRF rendering to 512x512, to feed it to SD&#39;s VAE.&#xA;pred_rgb_512 = F.interpolate(pred_rgb, (512, 512), mode=&#39;bilinear&#39;, align_corners=False)&#xA;# 2. image (512x512) --- VAE --&amp;gt; latents (64x64), this is SD&#39;s difference from Imagen.&#xA;latents = self.encode_imgs(pred_rgb_512)&#xA;... # timestep sampling, noise adding and UNet noise predicting&#xA;# 3. the SDS loss, since UNet part is ignored and cannot simply audodiff, we manually set the grad for latents.&#xA;w = (1 - self.scheduler.alphas_cumprod[t]).to(self.device)&#xA;grad = w * (noise_pred - noise)&#xA;latents.backward(gradient=grad, retain_graph=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Other regularizations are in &lt;code&gt;./nerf/utils.py &amp;gt; Trainer &amp;gt; train_step&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The generation seems quite sensitive to regularizations on weights_sum (alphas for each ray). The original opacity loss tends to make NeRF disappear (zero density everywhere), so we use an entropy loss to replace it for now (encourages alpha to be either 0 or 1).&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;NeRF Rendering core function: &lt;code&gt;./nerf/renderer.py &amp;gt; NeRFRenderer &amp;gt; run_cuda&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;the occupancy grid based training acceleration (instant-ngp like, enabled by &lt;code&gt;--cuda_ray&lt;/code&gt;) may harm the generation progress, since once a grid cell is marked as empty, rays won&#39;t pass it later...&lt;/li&gt; &#xA;   &lt;li&gt;Not using &lt;code&gt;--cuda_ray&lt;/code&gt; also works now: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# `-O2` equals `--fp16 --dir_text`&#xA;python main.py --text &#34;a hamburger&#34; --workspace trial -O2 # faster training, but slower rendering&#xA;&lt;/code&gt;&lt;/pre&gt; Training is faster if only sample 128 points uniformly per ray (5h --&amp;gt; 2.5h). More testing is needed...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Shading &amp;amp; normal evaluation: &lt;code&gt;./nerf/network*.py &amp;gt; NeRFNetwork &amp;gt; forward&lt;/code&gt;. Current implementation harms training and is disabled. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;use &lt;code&gt;--albedo_iters 1000&lt;/code&gt; to enable random shading mode after 1000 steps from albedo, lambertian, and textureless.&lt;/li&gt; &#xA;   &lt;li&gt;light direction: current implementation use a plane light source, instead of a point light source...&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;View-dependent prompting: &lt;code&gt;./nerf/provider.py &amp;gt; get_view_direction&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;ues &lt;code&gt;--angle_overhead, --angle_front&lt;/code&gt; to set the border. How to better divide front/back/side regions?&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Network backbone (&lt;code&gt;./nerf/network*.py&lt;/code&gt;) can be chosen by the &lt;code&gt;--backbone&lt;/code&gt; option, but &lt;code&gt;tcnn&lt;/code&gt; and &lt;code&gt;vanilla&lt;/code&gt; are not well tested.&lt;/li&gt; &#xA; &lt;li&gt;Spatial density bias (gaussian density blob): &lt;code&gt;./nerf/network*.py &amp;gt; NeRFNetwork &amp;gt; gaussian&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The amazing original work: &lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;&lt;em&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@article{poole2022dreamfusion,&#xA;    author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},&#xA;    title = {DreamFusion: Text-to-3D using 2D Diffusion},&#xA;    journal = {arXiv},&#xA;    year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Huge thanks to the &lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; and the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; library.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;    title={High-Resolution Image Synthesis with Latent Diffusion Models}, &#xA;    author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},&#xA;    year={2021},&#xA;    eprint={2112.10752},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&#xA;@misc{von-platen-etal-2022-diffusers,&#xA;    author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},&#xA;    title = {Diffusers: State-of-the-art diffusion models},&#xA;    year = {2022},&#xA;    publisher = {GitHub},&#xA;    journal = {GitHub repository},&#xA;    howpublished = {\url{https://github.com/huggingface/diffusers}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The GUI is developed with &lt;a href=&#34;https://github.com/hoffstadt/DearPyGui&#34;&gt;DearPyGui&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>mrdbourke/pytorch-deep-learning</title>
    <updated>2022-10-09T01:31:34Z</updated>
    <id>tag:github.com,2022-10-09:/mrdbourke/pytorch-deep-learning</id>
    <link href="https://github.com/mrdbourke/pytorch-deep-learning" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Materials for the Learn PyTorch for Deep Learning: Zero to Mastery course.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Learn PyTorch for Deep Learning&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Update (2 October 2022):&lt;/strong&gt; The &lt;a href=&#34;https://dbourke.link/ZTMPyTorch&#34;&gt;course is out on the Zero to Mastery Academy&lt;/a&gt; with 10 sections and 300+ videos ranging from the fundamentals to state of the art machine learning research paper replicating.&lt;/p&gt; &#xA;&lt;p&gt;Welcome to the &lt;a href=&#34;https://dbourke.link/ZTMPyTorch&#34;&gt;Zero to Mastery Learn PyTorch for Deep Learning course&lt;/a&gt;, the second best place to learn PyTorch on the internet (the first being the &lt;a href=&#34;https://pytorch.org/docs/stable/index.html&#34;&gt;PyTorch documentation&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://learnpytorch.io&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/misc-pytorch-course-launch-cover-white-text-black-background.jpg&#34; width=&#34;750&#34; alt=&#34;pytorch deep learning by zero to mastery cover photo with different sections of the course&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Contents of this page&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#course-materialsoutline&#34;&gt;Course materials/outline&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#about-this-course&#34;&gt;About this course&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#status&#34;&gt;Status&lt;/a&gt; (the progress of the course creation)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#log&#34;&gt;Log&lt;/a&gt; (a log of the course material creation process)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Course materials/outline&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📖 &lt;strong&gt;Online book version:&lt;/strong&gt; All of course materials are available in a readable online book at &lt;a href=&#34;https://learnpytorch.io&#34;&gt;learnpytorch.io&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🎥 &lt;strong&gt;First five sections on YouTube:&lt;/strong&gt; Learn Pytorch in a day by watching the &lt;a href=&#34;https://youtu.be/Z_ikDlimN6A&#34;&gt;first 25-hours of material&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🔬 &lt;strong&gt;Course focus:&lt;/strong&gt; code, code, code, experiment, experiment, experiment.&lt;/li&gt; &#xA; &lt;li&gt;🏃‍♂️ &lt;strong&gt;Teaching style:&lt;/strong&gt; &lt;a href=&#34;https://sive.rs/kimo&#34;&gt;https://sive.rs/kimo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;🤔 &lt;strong&gt;Ask a question:&lt;/strong&gt; See the &lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/discussions&#34;&gt;GitHub Discussions page&lt;/a&gt; for existing questions/ask your own.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Section&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;What does it cover?&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Exercises &amp;amp; Extra-curriculum&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Slides&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/&#34;&gt;00 - PyTorch Fundamentals&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Many fundamental PyTorch operations used for deep learning and neural networks.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/00_pytorch_and_deep_learning_fundamentals.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/01_pytorch_workflow/&#34;&gt;01 - PyTorch Workflow&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Provides an outline for approaching deep learning problems and building neural networks with PyTorch.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/01_pytorch_workflow/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/01_pytorch_workflow.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/02_pytorch_classification/&#34;&gt;02 - PyTorch Neural Network Classification&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Uses the PyTorch workflow from 01 to go through a neural network classification problem.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/02_pytorch_classification/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/02_pytorch_classification.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/03_pytorch_computer_vision/&#34;&gt;03 - PyTorch Computer Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Let&#39;s see how PyTorch can be used for computer vision problems using the same workflow from 01 &amp;amp; 02.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/03_pytorch_computer_vision/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/03_pytorch_computer_vision.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/04_pytorch_custom_datasets/&#34;&gt;04 - PyTorch Custom Datasets&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;How do you load a custom dataset into PyTorch? Also we&#39;ll be laying the foundations in this notebook for our modular code (covered in 05).&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/04_pytorch_custom_datasets/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/04_pytorch_custom_datasets.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/05_pytorch_going_modular/&#34;&gt;05 - PyTorch Going Modular&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch is designed to be modular, let&#39;s turn what we&#39;ve created into a series of Python scripts (this is how you&#39;ll often find PyTorch code in the wild).&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/05_pytorch_going_modular/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/05_pytorch_going_modular.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/06_pytorch_transfer_learning/&#34;&gt;06 - PyTorch Transfer Learning&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Let&#39;s take a well performing pre-trained model and adjust it to one of our own problems.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/06_pytorch_transfer_learning/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/06_pytorch_transfer_learning.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/07_pytorch_experiment_tracking/&#34;&gt;07 - Milestone Project 1: PyTorch Experiment Tracking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;We&#39;ve built a bunch of models... wouldn&#39;t it be good to track how they&#39;re all going?&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/07_pytorch_experiment_tracking/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/07_pytorch_experiment_tracking.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/08_pytorch_paper_replicating/&#34;&gt;08 - Milestone Project 2: PyTorch Paper Replicating&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;PyTorch is the most popular deep learning framework for machine learning research, let&#39;s see why by replicating a machine learning paper.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/08_pytorch_paper_replicating/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/08_pytorch_paper_replicating.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/09_pytorch_model_deployment/&#34;&gt;09 - Milestone Project 3: Model Deployment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;So we&#39;ve built a working PyTorch model... how do we get it in the hands of others? Hint: deploy it to the internet.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/09_pytorch_model_deployment/#exercises&#34;&gt;Go to exercises &amp;amp; extra-curriculum&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/raw/main/slides/09_pytorch_model_deployment.pdf&#34;&gt;Go to slides&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.learnpytorch.io/pytorch_extra_resources/&#34;&gt;PyTorch Extra Resources&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;This course covers a large amount of PyTorch and deep learning but the field of machine learning is vast, inside here you&#39;ll find recommended books and resources for: PyTorch and deep learning, ML engineering, NLP (natural language processing), time series data, where to find datasets and more.&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;p&gt;All materials completed and videos published on Zero to Mastery!&lt;/p&gt; &#xA;&lt;p&gt;See the project page for work-in-progress board - &lt;a href=&#34;https://github.com/users/mrdbourke/projects/1&#34;&gt;https://github.com/users/mrdbourke/projects/1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Total video count:&lt;/strong&gt; 321&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done skeleton code for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done annotations (text) for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done images for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done keynotes for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Done exercises and solutions for:&lt;/strong&gt; 00, 01, 02, 03, 04, 05, 06, 07, 08, 09&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning#log&#34;&gt;log&lt;/a&gt; for almost daily updates.&lt;/p&gt; &#xA;&lt;h2&gt;About this course&lt;/h2&gt; &#xA;&lt;h3&gt;Who is this course for?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;You:&lt;/strong&gt; Are a beginner in the field of machine learning or deep learning and would like to learn PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This course:&lt;/strong&gt; Teaches you PyTorch and many machine learning concepts in a hands-on, code-first way.&lt;/p&gt; &#xA;&lt;p&gt;If you already have 1-year+ experience in machine learning, this course may help but it is specifically designed to be beginner-friendly.&lt;/p&gt; &#xA;&lt;h3&gt;What are the prerequisites?&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;3-6 months coding Python.&lt;/li&gt; &#xA; &lt;li&gt;At least one beginner machine learning course (however this might be able to be skipped, resources are linked for many different topics).&lt;/li&gt; &#xA; &lt;li&gt;Experience using Jupyter Notebooks or Google Colab (though you can pick this up as we go along).&lt;/li&gt; &#xA; &lt;li&gt;A willingness to learn (most important).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;For 1 &amp;amp; 2, I&#39;d recommend the &lt;a href=&#34;https://dbourke.link/ZTMMLcourse&#34;&gt;Zero to Mastery Data Science and Machine Learning Bootcamp&lt;/a&gt;, it&#39;ll teach you the fundamentals of machine learning and Python (I&#39;m biased though, I also teach that course).&lt;/p&gt; &#xA;&lt;h3&gt;How is the course taught?&lt;/h3&gt; &#xA;&lt;p&gt;All of the course materials are available for free in an online book at &lt;a href=&#34;https://learnpytorch.io&#34;&gt;learnpytorch.io&lt;/a&gt;. If you like to read, I&#39;d recommend going through the resources there.&lt;/p&gt; &#xA;&lt;p&gt;If you prefer to learn via video, the course is also taught in apprenticeship-style format, meaning I write PyTorch code, you write PyTorch code.&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s a reason the course motto&#39;s include &lt;em&gt;if in doubt, run the code&lt;/em&gt; and &lt;em&gt;experiment, experiment, experiment!&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;My whole goal is to help you to do one thing: learn machine learning by writing PyTorch code.&lt;/p&gt; &#xA;&lt;p&gt;The code is all written via &lt;a href=&#34;https://colab.research.google.com&#34;&gt;Google Colab Notebooks&lt;/a&gt; (you could also use Jupyter Notebooks), an incredible free resource to experiment with machine learning.&lt;/p&gt; &#xA;&lt;h3&gt;What will I get if I finish the course?&lt;/h3&gt; &#xA;&lt;p&gt;There&#39;s certificates and all that jazz if you go through the videos.&lt;/p&gt; &#xA;&lt;p&gt;But certificates are meh.&lt;/p&gt; &#xA;&lt;p&gt;You can consider this course a machine learning momentum builder.&lt;/p&gt; &#xA;&lt;p&gt;By the end, you&#39;ll have written hundreds of lines of PyTorch code.&lt;/p&gt; &#xA;&lt;p&gt;And will have been exposed to many of the most important concepts in machine learning.&lt;/p&gt; &#xA;&lt;p&gt;So when you go to build your own machine learning projects or inspect a public machine learning project made with PyTorch, it&#39;ll feel familiar and if it doesn&#39;t, at least you&#39;ll know where to look.&lt;/p&gt; &#xA;&lt;h3&gt;What will I build in the course?&lt;/h3&gt; &#xA;&lt;p&gt;We start with the barebone fundamentals of PyTorch and machine learning, so even if you&#39;re new to machine learning you&#39;ll be caught up to speed.&lt;/p&gt; &#xA;&lt;p&gt;Then we’ll explore more advanced areas including PyTorch neural network classification, PyTorch workflows, computer vision, custom datasets, experiment tracking, model deployment, and my personal favourite: transfer learning, a powerful technique for taking what one machine learning model has learned on another problem and applying it to your own!&lt;/p&gt; &#xA;&lt;p&gt;Along the way, you’ll build three milestone projects surrounding an overarching project called FoodVision, a neural network computer vision model to classify images of food.&lt;/p&gt; &#xA;&lt;p&gt;These milestone projects will help you practice using PyTorch to cover important machine learning concepts and create a portfolio you can show employers and say &#34;here&#39;s what I&#39;ve done&#34;.&lt;/p&gt; &#xA;&lt;h3&gt;How do I get started?&lt;/h3&gt; &#xA;&lt;p&gt;You can read the materials on any device but this course is best viewed and coded along within a desktop browser.&lt;/p&gt; &#xA;&lt;p&gt;The course uses a free tool called Google Colab. If you&#39;ve got no experience with it, I&#39;d go through the free &lt;a href=&#34;https://colab.research.google.com/notebooks/basic_features_overview.ipynb&#34;&gt;Introduction to Google Colab tutorial&lt;/a&gt; and then come back here.&lt;/p&gt; &#xA;&lt;p&gt;To start:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Click on one of the notebook or section links above like &#34;&lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/&#34;&gt;00. PyTorch Fundamentals&lt;/a&gt;&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Click the &#34;Open in Colab&#34; button up the top.&lt;/li&gt; &#xA; &lt;li&gt;Press SHIFT+Enter a few times and see what happens.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;My question isn&#39;t answered&lt;/h3&gt; &#xA;&lt;p&gt;Please leave a &lt;a href=&#34;https://github.com/mrdbourke/pytorch-deep-learning/discussions&#34;&gt;discussion&lt;/a&gt; or send me an email directly: daniel (at) mrdbourke (dot) com.&lt;/p&gt; &#xA;&lt;h2&gt;Log&lt;/h2&gt; &#xA;&lt;p&gt;Almost daily updates of what&#39;s happening.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2 Oct 2022 - all videos for section 08 and 09 published (100+ videos for the last two sections)!&lt;/li&gt; &#xA; &lt;li&gt;30 Aug 2022 - recorded 15 videos for 09, total videos: 321, finished section 09 videos!!!! ... even bigger than 08!!&lt;/li&gt; &#xA; &lt;li&gt;29 Aug 2022 - recorded 16 videos for 09, total videos: 306&lt;/li&gt; &#xA; &lt;li&gt;28 Aug 2022 - recorded 11 videos for 09, total videos: 290&lt;/li&gt; &#xA; &lt;li&gt;27 Aug 2022 - recorded 16 videos for 09, total videos: 279&lt;/li&gt; &#xA; &lt;li&gt;26 Aug 2022 - add finishing touchs to notebook 09, add slides for 09, create solutions and exercises for 09&lt;/li&gt; &#xA; &lt;li&gt;25 Aug 2022 - add annotations and cleanup 09, remove TK&#39;s, cleanup images, make slides for 09&lt;/li&gt; &#xA; &lt;li&gt;24 Aug 2022 - add annotations to 09, main takeaways, exercises and extra-curriculum done&lt;/li&gt; &#xA; &lt;li&gt;23 Aug 2022 - add annotations to 09, add plenty of images/slides&lt;/li&gt; &#xA; &lt;li&gt;22 Aug 2022 - add annotations to 09, start working on slides/images&lt;/li&gt; &#xA; &lt;li&gt;20 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;19 Aug 2022 - add annotations to 09, check out the awesome demos!&lt;/li&gt; &#xA; &lt;li&gt;18 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;17 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;16 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;15 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;13 Aug 2022 - add annotations to 09&lt;/li&gt; &#xA; &lt;li&gt;12 Aug 2022 - add demo files for notebook 09 to &lt;code&gt;demos/&lt;/code&gt;, start annotating notebook 09 with explainer text&lt;/li&gt; &#xA; &lt;li&gt;11 Aug 2022 - finish skeleton code for notebook 09, course finishes deploying 2x models, one for FoodVision Mini &amp;amp; one for (secret)&lt;/li&gt; &#xA; &lt;li&gt;10 Aug 2022 - add section for PyTorch Extra Resources (places to learn more about PyTorch/deep learning): &lt;a href=&#34;https://www.learnpytorch.io/pytorch_extra_resources/&#34;&gt;https://www.learnpytorch.io/pytorch_extra_resources/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;09 Aug 2022 - add more skeleton code to notebook 09&lt;/li&gt; &#xA; &lt;li&gt;08 Aug 2022 - create draft notebook for 09, end goal to deploy FoodVision Mini model and make it publically accessible&lt;/li&gt; &#xA; &lt;li&gt;05 Aug 2022 - recorded 11 videos for 08, total videos: 263, section 08 videos finished!... the biggest section so far&lt;/li&gt; &#xA; &lt;li&gt;04 Aug 2022 - recorded 13 videos for 08, total videos: 252&lt;/li&gt; &#xA; &lt;li&gt;03 Aug 2022 - recorded 3 videos for 08, total videos: 239&lt;/li&gt; &#xA; &lt;li&gt;02 Aug 2022 - recorded 12 videos for 08, total videos: 236&lt;/li&gt; &#xA; &lt;li&gt;30 July 2022 - recorded 11 videos for 08, total videos: 224&lt;/li&gt; &#xA; &lt;li&gt;29 July 2022 - add exercises + solutions for 08, see live walkthrough on YouTube: &lt;a href=&#34;https://youtu.be/tjpW_BY8y3g&#34;&gt;https://youtu.be/tjpW_BY8y3g&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;28 July 2022 - add slides for 08&lt;/li&gt; &#xA; &lt;li&gt;27 July 2022 - cleanup much of 08, start on slides for 08, exercises and extra-curriculum next&lt;/li&gt; &#xA; &lt;li&gt;26 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;25 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;24 July 2022 - launched first half of course (notebooks 00-04) in a single video (25+ hours!!!) on YouTube: &lt;a href=&#34;https://youtu.be/Z_ikDlimN6A&#34;&gt;https://youtu.be/Z_ikDlimN6A&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;21 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;20 July 2022 - add annotations and images for 08, getting so close! this is an epic section&lt;/li&gt; &#xA; &lt;li&gt;19 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;15 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;14 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;12 July 2022 - add annotations for 08, woo woo this is bigggg section!&lt;/li&gt; &#xA; &lt;li&gt;11 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;9 July 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;8 July 2022 - add a bunch of annotations to 08&lt;/li&gt; &#xA; &lt;li&gt;6 July 2022 - course launched on ZTM Academy with videos for sections 00-07! 🚀 - &lt;a href=&#34;https://dbourke.link/ZTMPyTorch&#34;&gt;https://dbourke.link/ZTMPyTorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;1 July 2022 - add annotations and images for 08&lt;/li&gt; &#xA; &lt;li&gt;30 June 2022 - add annotations for 08&lt;/li&gt; &#xA; &lt;li&gt;28 June 2022 - recorded 11 videos for section 07, total video count 213, all videos for section 07 complete!&lt;/li&gt; &#xA; &lt;li&gt;27 June 2022 - recorded 11 videos for section 07, total video count 202&lt;/li&gt; &#xA; &lt;li&gt;25 June 2022 - recreated 7 videos for section 06 to include updated APIs, total video count 191&lt;/li&gt; &#xA; &lt;li&gt;24 June 2022 - recreated 12 videos for section 06 to include updated APIs&lt;/li&gt; &#xA; &lt;li&gt;23 June 2022 - finish annotations for 07, add exercise template and solutions for 07 + video walkthrough on YouTube: &lt;a href=&#34;https://youtu.be/cO_r2FYcAjU&#34;&gt;https://youtu.be/cO_r2FYcAjU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;21 June 2022 - make 08 runnable end-to-end, add images and annotations for 07&lt;/li&gt; &#xA; &lt;li&gt;17 June 2022 - fix up 06, 07 v2 for upcoming torchvision version upgrade, add plenty of annotations to 08&lt;/li&gt; &#xA; &lt;li&gt;13 June 2022 - add notebook 08 first version, starting to replicate the Vision Transformer paper&lt;/li&gt; &#xA; &lt;li&gt;10 June 2022 - add annotations for 07 v2&lt;/li&gt; &#xA; &lt;li&gt;09 June 2022 - create 07 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 07 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; &#xA; &lt;li&gt;08 June 2022 - adapt 06 v2 for &lt;code&gt;torchvision&lt;/code&gt; v0.13 (this will replace 06 v1 when &lt;code&gt;torchvision=0.13&lt;/code&gt; is released)&lt;/li&gt; &#xA; &lt;li&gt;07 June 2022 - create notebook 06 v2 for upcoming &lt;code&gt;torchvision&lt;/code&gt; v0.13 update (new transfer learning methods)&lt;/li&gt; &#xA; &lt;li&gt;04 June 2022 - add annotations for 07&lt;/li&gt; &#xA; &lt;li&gt;03 June 2022 - huuuuuuge amount of annotations added to 07&lt;/li&gt; &#xA; &lt;li&gt;31 May 2022 - add a bunch of annotations for 07, make code runnable end-to-end&lt;/li&gt; &#xA; &lt;li&gt;30 May 2022 - record 4 videos for 06, finished section 06, onto section 07, total videos 186&lt;/li&gt; &#xA; &lt;li&gt;28 May 2022 - record 10 videos for 06, total videos 182&lt;/li&gt; &#xA; &lt;li&gt;24 May 2022 - add solutions and exercises for 06&lt;/li&gt; &#xA; &lt;li&gt;23 May 2022 - finished annotations and images for 06, time to do exercises and solutions&lt;/li&gt; &#xA; &lt;li&gt;22 May 2202 - add plenty of images to 06&lt;/li&gt; &#xA; &lt;li&gt;18 May 2022 - add plenty of annotations to 06&lt;/li&gt; &#xA; &lt;li&gt;17 May 2022 - added a bunch of annotations for section 06&lt;/li&gt; &#xA; &lt;li&gt;16 May 2022 - recorded 10 videos for section 05, finish videos for section 05 ✅&lt;/li&gt; &#xA; &lt;li&gt;12 May 2022 - added exercises and solutions for 05&lt;/li&gt; &#xA; &lt;li&gt;11 May 2022 - clean up part 1 and part 2 notebooks for 05, make slides for 05, start on exercises and solutions for 05&lt;/li&gt; &#xA; &lt;li&gt;10 May 2022 - huuuuge updates to the 05 section, see the website, it looks pretty: &lt;a href=&#34;https://www.learnpytorch.io/05_pytorch_going_modular/&#34;&gt;https://www.learnpytorch.io/05_pytorch_going_modular/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;09 May 2022 - add a bunch of materials for 05, cleanup docs&lt;/li&gt; &#xA; &lt;li&gt;08 May 2022 - add a bunch of materials for 05&lt;/li&gt; &#xA; &lt;li&gt;06 May 2022 - continue making materials for 05&lt;/li&gt; &#xA; &lt;li&gt;05 May 2022 - update section 05 with headings/outline&lt;/li&gt; &#xA; &lt;li&gt;28 Apr 2022 - recorded 13 videos for 04, finished videos for 04, now to make materials for 05&lt;/li&gt; &#xA; &lt;li&gt;27 Apr 2022 - recorded 3 videos for 04&lt;/li&gt; &#xA; &lt;li&gt;26 Apr 2022 - recorded 10 videos for 04&lt;/li&gt; &#xA; &lt;li&gt;25 Apr 2022 - recorded 11 videos for 04&lt;/li&gt; &#xA; &lt;li&gt;24 Apr 2022 - prepared slides for 04&lt;/li&gt; &#xA; &lt;li&gt;23 Apr 2022 - recorded 6 videos for 03, finished videos for 03, now to 04&lt;/li&gt; &#xA; &lt;li&gt;22 Apr 2022 - recorded 5 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;21 Apr 2022 - recorded 9 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;20 Apr 2022 - recorded 3 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;19 Apr 2022 - recorded 11 videos for 03&lt;/li&gt; &#xA; &lt;li&gt;18 Apr 2022 - finish exercises/solutions for 04, added live-coding walkthrough of 04 exercises/solutions on YouTube: &lt;a href=&#34;https://youtu.be/vsFMF9wqWx0&#34;&gt;https://youtu.be/vsFMF9wqWx0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;16 Apr 2022 - finish exercises/solutions for 03, added live-coding walkthrough of 03 exercises/solutions on YouTube: &lt;a href=&#34;https://youtu.be/_PibmqpEyhA&#34;&gt;https://youtu.be/_PibmqpEyhA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;14 Apr 2022 - add final images/annotations for 04, begin on exercises/solutions for 03 &amp;amp; 04&lt;/li&gt; &#xA; &lt;li&gt;13 Apr 2022 - add more images/annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;3 Apr 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;2 Apr 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;1 Apr 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;31 Mar 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;29 Mar 2022 - add more annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;27 Mar 2022 - starting to add annotations for 04&lt;/li&gt; &#xA; &lt;li&gt;26 Mar 2022 - making dataset for 04&lt;/li&gt; &#xA; &lt;li&gt;25 Mar 2022 - make slides for 03&lt;/li&gt; &#xA; &lt;li&gt;24 Mar 2022 - fix error for 03 not working in docs (finally)&lt;/li&gt; &#xA; &lt;li&gt;23 Mar 2022 - add more images for 03&lt;/li&gt; &#xA; &lt;li&gt;22 Mar 2022 - add images for 03&lt;/li&gt; &#xA; &lt;li&gt;20 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;18 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;17 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;16 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;15 Mar 2022 - add more annotations for 03&lt;/li&gt; &#xA; &lt;li&gt;14 Mar 2022 - start adding annotations for notebook 03, see the work in progress here: &lt;a href=&#34;https://www.learnpytorch.io/03_pytorch_computer_vision/&#34;&gt;https://www.learnpytorch.io/03_pytorch_computer_vision/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;12 Mar 2022 - recorded 12 videos for 02, finished section 02, now onto making materials for 03, 04, 05&lt;/li&gt; &#xA; &lt;li&gt;11 Mar 2022 - recorded 9 videos for 02&lt;/li&gt; &#xA; &lt;li&gt;10 Mar 2022 - recorded 10 videos for 02&lt;/li&gt; &#xA; &lt;li&gt;9 Mar 2022 - cleaning up slides/code for 02, getting ready for recording&lt;/li&gt; &#xA; &lt;li&gt;8 Mar 2022 - recorded 9 videos for section 01, finished section 01, now onto 02&lt;/li&gt; &#xA; &lt;li&gt;7 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; &#xA; &lt;li&gt;6 Mar 2022 - recorded 4 videos for section 01&lt;/li&gt; &#xA; &lt;li&gt;4 Mar 2022 - recorded 10 videos for section 01&lt;/li&gt; &#xA; &lt;li&gt;20 Feb 2022 - recorded 8 videos for section 00, finished section, now onto 01&lt;/li&gt; &#xA; &lt;li&gt;18 Feb 2022 - recorded 13 videos for section 00&lt;/li&gt; &#xA; &lt;li&gt;17 Feb 2022 - recorded 11 videos for section 00&lt;/li&gt; &#xA; &lt;li&gt;16 Feb 2022 - added setup guide&lt;/li&gt; &#xA; &lt;li&gt;12 Feb 2022 - tidy up README with table of course materials, finish images and slides for 01&lt;/li&gt; &#xA; &lt;li&gt;10 Feb 2022 - finished slides and images for 00, notebook is ready for publishing: &lt;a href=&#34;https://www.learnpytorch.io/00_pytorch_fundamentals/&#34;&gt;https://www.learnpytorch.io/00_pytorch_fundamentals/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;01-07 Feb 2022 - add annotations for 02, finished, still need images, going to work on exercises/solutions today&lt;/li&gt; &#xA; &lt;li&gt;31 Jan 2022 - start adding annotations for 02&lt;/li&gt; &#xA; &lt;li&gt;28 Jan 2022 - add exercies and solutions for 01&lt;/li&gt; &#xA; &lt;li&gt;26 Jan 2022 - lots more annotations to 01, should be finished tomorrow, will do exercises + solutions then too&lt;/li&gt; &#xA; &lt;li&gt;24 Jan 2022 - add a bunch of annotations to 01&lt;/li&gt; &#xA; &lt;li&gt;21 Jan 2022 - start adding annotations for 01&lt;/li&gt; &#xA; &lt;li&gt;20 Jan 2022 - finish annotations for 00 (still need to add images), add exercises and solutions for 00&lt;/li&gt; &#xA; &lt;li&gt;19 Jan 2022 - add more annotations for 00&lt;/li&gt; &#xA; &lt;li&gt;18 Jan 2022 - add more annotations for 00&lt;/li&gt; &#xA; &lt;li&gt;17 Jan 2022 - back from holidays, adding more annotations to 00&lt;/li&gt; &#xA; &lt;li&gt;10 Dec 2021 - start adding annoations for 00&lt;/li&gt; &#xA; &lt;li&gt;9 Dec 2021 - Created a website for the course (&lt;a href=&#34;https://learnpytorch.io&#34;&gt;learnpytorch.io&lt;/a&gt;) you&#39;ll see updates posted there as development continues&lt;/li&gt; &#xA; &lt;li&gt;8 Dec 2021 - Clean up notebook 07, starting to go back through code and add annotations&lt;/li&gt; &#xA; &lt;li&gt;26 Nov 2021 - Finish skeleton code for 07, added four different experiments, need to clean up and make more straightforward&lt;/li&gt; &#xA; &lt;li&gt;25 Nov 2021 - clean code for 06, add skeleton code for 07 (experiment tracking)&lt;/li&gt; &#xA; &lt;li&gt;24 Nov 2021 - Update 04, 05, 06 notebooks for easier digestion and learning, each section should cover a max of 3 big ideas, 05 is now dedicated to turning notebook code into modular code&lt;/li&gt; &#xA; &lt;li&gt;22 Nov 2021 - Update 04 train and test functions to make more straightforward&lt;/li&gt; &#xA; &lt;li&gt;19 Nov 2021 - Added 05 (transfer learning) notebook, update custom data loading code in 04&lt;/li&gt; &#xA; &lt;li&gt;18 Nov 2021 - Updated vision code for 03 and added custom dataset loading code in 04&lt;/li&gt; &#xA; &lt;li&gt;12 Nov 2021 - Added a bunch of skeleton code to notebook 04 for custom dataset loading, next is modelling with custom data&lt;/li&gt; &#xA; &lt;li&gt;10 Nov 2021 - researching best practice for custom datasets for 04&lt;/li&gt; &#xA; &lt;li&gt;9 Nov 2021 - Update 03 skeleton code to finish off building CNN model, onto 04 for loading custom datasets&lt;/li&gt; &#xA; &lt;li&gt;4 Nov 2021 - Add GPU code to 03 + train/test loops + &lt;code&gt;helper_functions.py&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;3 Nov 2021 - Add basic start for 03, going to finish by end of week&lt;/li&gt; &#xA; &lt;li&gt;29 Oct 2021 - Tidied up skeleton code for 02, still a few more things to clean/tidy, created 03&lt;/li&gt; &#xA; &lt;li&gt;28 Oct 2021 - Finished skeleton code for 02, going to clean/tidy tomorrow, 03 next week&lt;/li&gt; &#xA; &lt;li&gt;27 Oct 2021 - add a bunch of code for 02, going to finish tomorrow/by end of week&lt;/li&gt; &#xA; &lt;li&gt;26 Oct 2021 - update 00, 01, 02 with outline/code, skeleton code for 00 &amp;amp; 01 done, 02 next&lt;/li&gt; &#xA; &lt;li&gt;23, 24 Oct 2021 - update 00 and 01 notebooks with more outline/code&lt;/li&gt; &#xA; &lt;li&gt;20 Oct 2021 - add v0 outlines for 01 and 02, add rough outline of course to README, this course will focus on less but better&lt;/li&gt; &#xA; &lt;li&gt;19 Oct 2021 - Start repo 🔥, add fundamentals notebook draft v0&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>streamich/react-use</title>
    <updated>2022-10-09T01:31:34Z</updated>
    <id>tag:github.com,2022-10-09:/streamich/react-use</id>
    <link href="https://github.com/streamich/react-use" rel="alternate"></link>
    <summary type="html">&lt;p&gt;React Hooks — 👍&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt; &lt;br&gt; &lt;br&gt; 👍 &lt;br&gt; react-use &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;br&gt; &lt;/h1&gt; &#xA; &lt;sup&gt; &lt;br&gt; &lt;br&gt; &lt;a href=&#34;https://www.npmjs.com/package/react-use&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/v/react-use.svg?sanitize=true&#34; alt=&#34;npm package&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://circleci.com/gh/streamich/react-use&#34;&gt; &lt;img src=&#34;https://img.shields.io/circleci/project/github/streamich/react-use/master.svg?sanitize=true&#34; alt=&#34;CircleCI master&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/react-use&#34;&gt; &lt;img src=&#34;https://img.shields.io/npm/dm/react-use.svg?sanitize=true&#34; alt=&#34;npm downloads&#34;&gt; &lt;/a&gt; &lt;a href=&#34;http://streamich.github.io/react-use&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/demos-🚀-yellow.svg&#34; alt=&#34;demos&#34;&gt; &lt;/a&gt; &lt;br&gt; Collection of essential &lt;a href=&#34;https://reactjs.org/docs/hooks-intro.html&#34;&gt;React Hooks&lt;/a&gt;. &lt;em&gt;Port of&lt;/em&gt; &lt;a href=&#34;https://github.com/streamich/libreact&#34;&gt;&lt;code&gt;libreact&lt;/code&gt;&lt;/a&gt;. &lt;br&gt; Translations: &lt;a href=&#34;https://github.com/zenghongtu/react-use-chinese/raw/master/README.md&#34;&gt;🇨🇳 汉语&lt;/a&gt; &lt;/sup&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;pre&gt;npm i &lt;a href=&#34;https://www.npmjs.com/package/react-use&#34;&gt;react-use&lt;/a&gt;&lt;/pre&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/Sensors.md&#34;&gt;&lt;strong&gt;Sensors&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useBattery.md&#34;&gt;&lt;code&gt;useBattery&lt;/code&gt;&lt;/a&gt; — tracks device battery state. &lt;a href=&#34;https://codesandbox.io/s/qlvn662zww&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useGeolocation.md&#34;&gt;&lt;code&gt;useGeolocation&lt;/code&gt;&lt;/a&gt; — tracks geo location state of user&#39;s device. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usegeolocation--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useHover.md&#34;&gt;&lt;code&gt;useHover&lt;/code&gt; and &lt;code&gt;useHoverDirty&lt;/code&gt;&lt;/a&gt; — tracks mouse hover state of some element. &lt;a href=&#34;https://codesandbox.io/s/zpn583rvx&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useHash.md&#34;&gt;&lt;code&gt;useHash&lt;/code&gt;&lt;/a&gt; — tracks location hash value. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usehash--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useIdle.md&#34;&gt;&lt;code&gt;useIdle&lt;/code&gt;&lt;/a&gt; — tracks whether user is being inactive.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useIntersection.md&#34;&gt;&lt;code&gt;useIntersection&lt;/code&gt;&lt;/a&gt; — tracks an HTML element&#39;s intersection. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-useintersection--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useKey.md&#34;&gt;&lt;code&gt;useKey&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useKeyPress.md&#34;&gt;&lt;code&gt;useKeyPress&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useKeyboardJs.md&#34;&gt;&lt;code&gt;useKeyboardJs&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useKeyPressEvent.md&#34;&gt;&lt;code&gt;useKeyPressEvent&lt;/code&gt;&lt;/a&gt; — track keys. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usekeypressevent--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useLocation.md&#34;&gt;&lt;code&gt;useLocation&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useSearchParam.md&#34;&gt;&lt;code&gt;useSearchParam&lt;/code&gt;&lt;/a&gt; — tracks page navigation bar location state.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useLongPress.md&#34;&gt;&lt;code&gt;useLongPress&lt;/code&gt;&lt;/a&gt; — tracks long press gesture of some element.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMedia.md&#34;&gt;&lt;code&gt;useMedia&lt;/code&gt;&lt;/a&gt; — tracks state of a CSS media query. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usemedia--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMediaDevices.md&#34;&gt;&lt;code&gt;useMediaDevices&lt;/code&gt;&lt;/a&gt; — tracks state of connected hardware devices.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMotion.md&#34;&gt;&lt;code&gt;useMotion&lt;/code&gt;&lt;/a&gt; — tracks state of device&#39;s motion sensor.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMouse.md&#34;&gt;&lt;code&gt;useMouse&lt;/code&gt; and &lt;code&gt;useMouseHovered&lt;/code&gt;&lt;/a&gt; — tracks state of mouse position. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usemouse--docs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMouseWheel.md&#34;&gt;&lt;code&gt;useMouseWheel&lt;/code&gt;&lt;/a&gt; — tracks deltaY of scrolled mouse wheel. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usemousewheel--docs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useNetworkState.md&#34;&gt;&lt;code&gt;useNetworkState&lt;/code&gt;&lt;/a&gt; — tracks the state of browser&#39;s network connection. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usenetworkstate--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useOrientation.md&#34;&gt;&lt;code&gt;useOrientation&lt;/code&gt;&lt;/a&gt; — tracks state of device&#39;s screen orientation.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/usePageLeave.md&#34;&gt;&lt;code&gt;usePageLeave&lt;/code&gt;&lt;/a&gt; — triggers when mouse leaves page boundaries.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useScratch.md&#34;&gt;&lt;code&gt;useScratch&lt;/code&gt;&lt;/a&gt; — tracks mouse click-and-scrub state.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useScroll.md&#34;&gt;&lt;code&gt;useScroll&lt;/code&gt;&lt;/a&gt; — tracks an HTML element&#39;s scroll position. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usescroll--docs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useScrolling.md&#34;&gt;&lt;code&gt;useScrolling&lt;/code&gt;&lt;/a&gt; — tracks whether HTML element is scrolling.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useStartTyping.md&#34;&gt;&lt;code&gt;useStartTyping&lt;/code&gt;&lt;/a&gt; — detects when user starts typing.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useWindowScroll.md&#34;&gt;&lt;code&gt;useWindowScroll&lt;/code&gt;&lt;/a&gt; — tracks &lt;code&gt;Window&lt;/code&gt; scroll position. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usewindowscroll--docs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useWindowSize.md&#34;&gt;&lt;code&gt;useWindowSize&lt;/code&gt;&lt;/a&gt; — tracks &lt;code&gt;Window&lt;/code&gt; dimensions. &lt;a href=&#34;https://codesandbox.io/s/m7ln22668&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMeasure.md&#34;&gt;&lt;code&gt;useMeasure&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useSize.md&#34;&gt;&lt;code&gt;useSize&lt;/code&gt;&lt;/a&gt; — tracks an HTML element&#39;s dimensions. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usemeasure--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/createBreakpoint.md&#34;&gt;&lt;code&gt;createBreakpoint&lt;/code&gt;&lt;/a&gt; — tracks &lt;code&gt;innerWidth&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useScrollbarWidth.md&#34;&gt;&lt;code&gt;useScrollbarWidth&lt;/code&gt;&lt;/a&gt; — detects browser&#39;s native scrollbars width. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usescrollbarwidth--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/usePinchZoom.md&#34;&gt;&lt;code&gt;usePinchZoom&lt;/code&gt;&lt;/a&gt; — tracks pointer events to detect pinch zoom in and out status. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/sensors-usePinchZoom--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/UI.md&#34;&gt;&lt;strong&gt;UI&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useAudio.md&#34;&gt;&lt;code&gt;useAudio&lt;/code&gt;&lt;/a&gt; — plays audio and exposes its controls. &lt;a href=&#34;https://codesandbox.io/s/2o4lo6rqy&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useClickAway.md&#34;&gt;&lt;code&gt;useClickAway&lt;/code&gt;&lt;/a&gt; — triggers callback when user clicks outside target area.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useCss.md&#34;&gt;&lt;code&gt;useCss&lt;/code&gt;&lt;/a&gt; — dynamically adjusts CSS.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useDrop.md&#34;&gt;&lt;code&gt;useDrop&lt;/code&gt; and &lt;code&gt;useDropArea&lt;/code&gt;&lt;/a&gt; — tracks file, link and copy-paste drops.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useFullscreen.md&#34;&gt;&lt;code&gt;useFullscreen&lt;/code&gt;&lt;/a&gt; — display an element or video full-screen. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/ui-usefullscreen--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useSlider.md&#34;&gt;&lt;code&gt;useSlider&lt;/code&gt;&lt;/a&gt; — provides slide behavior over any HTML element. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/ui-useslider--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useSpeech.md&#34;&gt;&lt;code&gt;useSpeech&lt;/code&gt;&lt;/a&gt; — synthesizes speech from a text string. &lt;a href=&#34;https://codesandbox.io/s/n090mqz69m&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useVibrate.md&#34;&gt;&lt;code&gt;useVibrate&lt;/code&gt;&lt;/a&gt; — provide physical feedback using the &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/API/Vibration_API&#34;&gt;Vibration API&lt;/a&gt;. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/ui-usevibrate--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useVideo.md&#34;&gt;&lt;code&gt;useVideo&lt;/code&gt;&lt;/a&gt; — plays video, tracks its state, and exposes playback controls. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/ui-usevideo--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/Animations.md&#34;&gt;&lt;strong&gt;Animations&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useRaf.md&#34;&gt;&lt;code&gt;useRaf&lt;/code&gt;&lt;/a&gt; — re-renders component on each &lt;code&gt;requestAnimationFrame&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useInterval.md&#34;&gt;&lt;code&gt;useInterval&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useHarmonicIntervalFn.md&#34;&gt;&lt;code&gt;useHarmonicIntervalFn&lt;/code&gt;&lt;/a&gt; — re-renders component on a set interval using &lt;code&gt;setInterval&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useSpring.md&#34;&gt;&lt;code&gt;useSpring&lt;/code&gt;&lt;/a&gt; — interpolates number over time according to spring dynamics.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useTimeout.md&#34;&gt;&lt;code&gt;useTimeout&lt;/code&gt;&lt;/a&gt; — re-renders component after a timeout.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useTimeoutFn.md&#34;&gt;&lt;code&gt;useTimeoutFn&lt;/code&gt;&lt;/a&gt; — calls given function after a timeout. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/animation-usetimeoutfn--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useTween.md&#34;&gt;&lt;code&gt;useTween&lt;/code&gt;&lt;/a&gt; — re-renders component, while tweening a number from 0 to 1. &lt;a href=&#34;https://codesandbox.io/s/52990wwzyl&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useUpdate.md&#34;&gt;&lt;code&gt;useUpdate&lt;/code&gt;&lt;/a&gt; — returns a callback, which re-renders component when called. &lt;br&gt; &lt;br&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/Side-effects.md&#34;&gt;&lt;strong&gt;Side-effects&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useAsync.md&#34;&gt;&lt;code&gt;useAsync&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useAsyncFn.md&#34;&gt;&lt;code&gt;useAsyncFn&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useAsyncRetry.md&#34;&gt;&lt;code&gt;useAsyncRetry&lt;/code&gt;&lt;/a&gt; — resolves an &lt;code&gt;async&lt;/code&gt; function.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useBeforeUnload.md&#34;&gt;&lt;code&gt;useBeforeUnload&lt;/code&gt;&lt;/a&gt; — shows browser alert when user try to reload or close the page.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useCookie.md&#34;&gt;&lt;code&gt;useCookie&lt;/code&gt;&lt;/a&gt; — provides way to read, update and delete a cookie. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/side-effects-usecookie--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useCopyToClipboard.md&#34;&gt;&lt;code&gt;useCopyToClipboard&lt;/code&gt;&lt;/a&gt; — copies text to clipboard.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useDebounce.md&#34;&gt;&lt;code&gt;useDebounce&lt;/code&gt;&lt;/a&gt; — debounces a function. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/side-effects-usedebounce--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useError.md&#34;&gt;&lt;code&gt;useError&lt;/code&gt;&lt;/a&gt; — error dispatcher. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/side-effects-useerror--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useFavicon.md&#34;&gt;&lt;code&gt;useFavicon&lt;/code&gt;&lt;/a&gt; — sets favicon of the page.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useLocalStorage.md&#34;&gt;&lt;code&gt;useLocalStorage&lt;/code&gt;&lt;/a&gt; — manages a value in &lt;code&gt;localStorage&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useLockBodyScroll.md&#34;&gt;&lt;code&gt;useLockBodyScroll&lt;/code&gt;&lt;/a&gt; — lock scrolling of the body element.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useRafLoop.md&#34;&gt;&lt;code&gt;useRafLoop&lt;/code&gt;&lt;/a&gt; — calls given function inside the RAF loop.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useSessionStorage.md&#34;&gt;&lt;code&gt;useSessionStorage&lt;/code&gt;&lt;/a&gt; — manages a value in &lt;code&gt;sessionStorage&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useThrottle.md&#34;&gt;&lt;code&gt;useThrottle&lt;/code&gt; and &lt;code&gt;useThrottleFn&lt;/code&gt;&lt;/a&gt; — throttles a function. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/side-effects-usethrottle--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useTitle.md&#34;&gt;&lt;code&gt;useTitle&lt;/code&gt;&lt;/a&gt; — sets title of the page.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/usePermission.md&#34;&gt;&lt;code&gt;usePermission&lt;/code&gt;&lt;/a&gt; — query permission status for browser APIs. &lt;br&gt; &lt;br&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/Lifecycles.md&#34;&gt;&lt;strong&gt;Lifecycles&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useEffectOnce.md&#34;&gt;&lt;code&gt;useEffectOnce&lt;/code&gt;&lt;/a&gt; — a modified &lt;a href=&#34;https://reactjs.org/docs/hooks-reference.html#useeffect&#34;&gt;&lt;code&gt;useEffect&lt;/code&gt;&lt;/a&gt; hook that only runs once.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useEvent.md&#34;&gt;&lt;code&gt;useEvent&lt;/code&gt;&lt;/a&gt; — subscribe to events.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useLifecycles.md&#34;&gt;&lt;code&gt;useLifecycles&lt;/code&gt;&lt;/a&gt; — calls &lt;code&gt;mount&lt;/code&gt; and &lt;code&gt;unmount&lt;/code&gt; callbacks.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMountedState.md&#34;&gt;&lt;code&gt;useMountedState&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useUnmountPromise.md&#34;&gt;&lt;code&gt;useUnmountPromise&lt;/code&gt;&lt;/a&gt; — track if component is mounted.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/usePromise.md&#34;&gt;&lt;code&gt;usePromise&lt;/code&gt;&lt;/a&gt; — resolves promise only while component is mounted.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useLogger.md&#34;&gt;&lt;code&gt;useLogger&lt;/code&gt;&lt;/a&gt; — logs in console as component goes through life-cycles.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMount.md&#34;&gt;&lt;code&gt;useMount&lt;/code&gt;&lt;/a&gt; — calls &lt;code&gt;mount&lt;/code&gt; callbacks.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useUnmount.md&#34;&gt;&lt;code&gt;useUnmount&lt;/code&gt;&lt;/a&gt; — calls &lt;code&gt;unmount&lt;/code&gt; callbacks.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useUpdateEffect.md&#34;&gt;&lt;code&gt;useUpdateEffect&lt;/code&gt;&lt;/a&gt; — run an &lt;code&gt;effect&lt;/code&gt; only on updates.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useIsomorphicLayoutEffect.md&#34;&gt;&lt;code&gt;useIsomorphicLayoutEffect&lt;/code&gt;&lt;/a&gt; — &lt;code&gt;useLayoutEffect&lt;/code&gt; that that works on server.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useDeepCompareEffect.md&#34;&gt;&lt;code&gt;useDeepCompareEffect&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useShallowCompareEffect.md&#34;&gt;&lt;code&gt;useShallowCompareEffect&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useCustomCompareEffect.md&#34;&gt;&lt;code&gt;useCustomCompareEffect&lt;/code&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/State.md&#34;&gt;&lt;strong&gt;State&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/createMemo.md&#34;&gt;&lt;code&gt;createMemo&lt;/code&gt;&lt;/a&gt; — factory of memoized hooks.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/createReducer.md&#34;&gt;&lt;code&gt;createReducer&lt;/code&gt;&lt;/a&gt; — factory of reducer hooks with custom middleware.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/createReducerContext.md&#34;&gt;&lt;code&gt;createReducerContext&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/createStateContext.md&#34;&gt;&lt;code&gt;createStateContext&lt;/code&gt;&lt;/a&gt; — factory of hooks for a sharing state between components.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useDefault.md&#34;&gt;&lt;code&gt;useDefault&lt;/code&gt;&lt;/a&gt; — returns the default value when state is &lt;code&gt;null&lt;/code&gt; or &lt;code&gt;undefined&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useGetSet.md&#34;&gt;&lt;code&gt;useGetSet&lt;/code&gt;&lt;/a&gt; — returns state getter &lt;code&gt;get()&lt;/code&gt; instead of raw state.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useGetSetState.md&#34;&gt;&lt;code&gt;useGetSetState&lt;/code&gt;&lt;/a&gt; — as if &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useGetSet.md&#34;&gt;&lt;code&gt;useGetSet&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useSetState.md&#34;&gt;&lt;code&gt;useSetState&lt;/code&gt;&lt;/a&gt; had a baby.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useLatest.md&#34;&gt;&lt;code&gt;useLatest&lt;/code&gt;&lt;/a&gt; — returns the latest state or props&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/usePrevious.md&#34;&gt;&lt;code&gt;usePrevious&lt;/code&gt;&lt;/a&gt; — returns the previous state or props. &lt;a href=&#34;https://codesandbox.io/s/fervent-galileo-krgx6&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/usePreviousDistinct.md&#34;&gt;&lt;code&gt;usePreviousDistinct&lt;/code&gt;&lt;/a&gt; — like &lt;code&gt;usePrevious&lt;/code&gt; but with a predicate to determine if &lt;code&gt;previous&lt;/code&gt; should update.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useObservable.md&#34;&gt;&lt;code&gt;useObservable&lt;/code&gt;&lt;/a&gt; — tracks latest value of an &lt;code&gt;Observable&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useRafState.md&#34;&gt;&lt;code&gt;useRafState&lt;/code&gt;&lt;/a&gt; — creates &lt;code&gt;setState&lt;/code&gt; method which only updates after &lt;code&gt;requestAnimationFrame&lt;/code&gt;. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-userafstate--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useSetState.md&#34;&gt;&lt;code&gt;useSetState&lt;/code&gt;&lt;/a&gt; — creates &lt;code&gt;setState&lt;/code&gt; method which works like &lt;code&gt;this.setState&lt;/code&gt;. &lt;a href=&#34;https://codesandbox.io/s/n75zqn1xp0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useStateList.md&#34;&gt;&lt;code&gt;useStateList&lt;/code&gt;&lt;/a&gt; — circularly iterates over an array. &lt;a href=&#34;https://codesandbox.io/s/bold-dewdney-pjzkd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useToggle.md&#34;&gt;&lt;code&gt;useToggle&lt;/code&gt; and &lt;code&gt;useBoolean&lt;/code&gt;&lt;/a&gt; — tracks state of a boolean. &lt;a href=&#34;https://codesandbox.io/s/focused-sammet-brw2d&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useCounter.md&#34;&gt;&lt;code&gt;useCounter&lt;/code&gt; and &lt;code&gt;useNumber&lt;/code&gt;&lt;/a&gt; — tracks state of a number. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-usecounter--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useList.md&#34;&gt;&lt;code&gt;useList&lt;/code&gt;&lt;/a&gt; &lt;del&gt;and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useUpsert.md&#34;&gt;&lt;code&gt;useUpsert&lt;/code&gt;&lt;/a&gt;&lt;/del&gt; — tracks state of an array. &lt;a href=&#34;https://codesandbox.io/s/wonderful-mahavira-1sm0w&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMap.md&#34;&gt;&lt;code&gt;useMap&lt;/code&gt;&lt;/a&gt; — tracks state of an object. &lt;a href=&#34;https://codesandbox.io/s/quirky-dewdney-gi161&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useSet.md&#34;&gt;&lt;code&gt;useSet&lt;/code&gt;&lt;/a&gt; — tracks state of a Set. &lt;a href=&#34;https://codesandbox.io/s/bold-shtern-6jlgw&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useQueue.md&#34;&gt;&lt;code&gt;useQueue&lt;/code&gt;&lt;/a&gt; — implements simple queue.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useStateValidator.md&#34;&gt;&lt;code&gt;useStateValidator&lt;/code&gt;&lt;/a&gt; — tracks state of an object. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-usestatevalidator--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useStateWithHistory.md&#34;&gt;&lt;code&gt;useStateWithHistory&lt;/code&gt;&lt;/a&gt; — stores previous state values and provides handles to travel through them. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-usestatewithhistory--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMultiStateValidator.md&#34;&gt;&lt;code&gt;useMultiStateValidator&lt;/code&gt;&lt;/a&gt; — alike the &lt;code&gt;useStateValidator&lt;/code&gt;, but tracks multiple states at a time. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-usemultistatevalidator--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMediatedState.md&#34;&gt;&lt;code&gt;useMediatedState&lt;/code&gt;&lt;/a&gt; — like the regular &lt;code&gt;useState&lt;/code&gt; but with mediation by custom function. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-usemediatedstate--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useFirstMountState.md&#34;&gt;&lt;code&gt;useFirstMountState&lt;/code&gt;&lt;/a&gt; — check if current render is first. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-usefirstmountstate--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useRendersCount.md&#34;&gt;&lt;code&gt;useRendersCount&lt;/code&gt;&lt;/a&gt; — count component renders. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-userenderscount--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/createGlobalState.md&#34;&gt;&lt;code&gt;createGlobalState&lt;/code&gt;&lt;/a&gt; — cross component shared state.&lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-createglobalstate--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useMethods.md&#34;&gt;&lt;code&gt;useMethods&lt;/code&gt;&lt;/a&gt; — neat alternative to &lt;code&gt;useReducer&lt;/code&gt;. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-usemethods--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;br&gt; &lt;br&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;&#34;&gt;&lt;strong&gt;Miscellaneous&lt;/strong&gt;&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useEnsuredForwardedRef.md&#34;&gt;&lt;code&gt;useEnsuredForwardedRef&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/useEnsuredForwardedRef.md&#34;&gt;&lt;code&gt;ensuredForwardRef&lt;/code&gt;&lt;/a&gt; — use a React.forwardedRef safely. &lt;a href=&#34;https://streamich.github.io/react-use/?path=/story/state-useensuredforwardedref--demo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/demo-%20%20%20%F0%9F%9A%80-green.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/docs/Usage.md&#34;&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/a&gt; — how to import. &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/streamich/react-use/master/LICENSE&#34;&gt;&lt;strong&gt;Unlicense&lt;/strong&gt;&lt;/a&gt; — public domain. &lt;br&gt; &lt;a href=&#34;https://opencollective.com/react-use/contribute&#34;&gt;&lt;strong&gt;Support&lt;/strong&gt;&lt;/a&gt; — add yourself to backer list below. &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;Contributors&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/streamich/react-use/graphs/contributors&#34;&gt;&lt;img src=&#34;https://opencollective.com/react-use/contributors.svg?width=890&amp;amp;button=false&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;br&gt;</summary>
  </entry>
</feed>