<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-08T01:27:29Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>fishaudio/Bert-VITS2</title>
    <updated>2024-01-08T01:27:29Z</updated>
    <id>tag:github.com,2024-01-08:/fishaudio/Bert-VITS2</id>
    <link href="https://github.com/fishaudio/Bert-VITS2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;vits2 backbone with multilingual-bert&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img alt=&#34;LOGO&#34; src=&#34;https://cdn.jsdelivr.net/gh/fishaudio/fish-diffusion@main/images/logo_512x512.png&#34; width=&#34;256&#34; height=&#34;256&#34;&gt; &#xA; &lt;h1&gt;Bert-VITS2&lt;/h1&gt; &#xA; &lt;p&gt;VITS2 Backbone with multilingual bert&lt;/p&gt; &#xA; &lt;p&gt;For quick guide, please refer to &lt;code&gt;webui_preprocess.py&lt;/code&gt;.&lt;/p&gt; &#xA; &lt;p&gt;简易教程请参见 &lt;code&gt;webui_preprocess.py&lt;/code&gt;。&lt;/p&gt; &#xA; &lt;h2&gt;请注意，本项目核心思路来源于&lt;a href=&#34;https://github.com/anyvoiceai/MassTTS&#34;&gt;anyvoiceai/MassTTS&lt;/a&gt; 一个非常好的tts项目&lt;/h2&gt; &#xA; &lt;h2&gt;MassTTS的演示demo为&lt;a href=&#34;https://www.bilibili.com/video/BV1w24y1c7z9&#34;&gt;ai版峰哥锐评峰哥本人,并找回了在金三角失落的腰子&lt;/a&gt;&lt;/h2&gt; &#xA; &lt;h2&gt;成熟的旅行者/开拓者/舰长/博士/sensei/猎魔人/喵喵露/V应当参阅代码自己学习如何训练。&lt;/h2&gt; &#xA; &lt;h3&gt;严禁将此项目用于一切违反《中华人民共和国宪法》，《中华人民共和国刑法》，《中华人民共和国治安管理处罚法》和《中华人民共和国民法典》之用途。&lt;/h3&gt; &#xA; &lt;h3&gt;严禁用于任何政治相关用途。&lt;/h3&gt; &#xA; &lt;h4&gt;Video:&lt;a href=&#34;https://www.bilibili.com/video/BV1hp4y1K78E&#34;&gt;https://www.bilibili.com/video/BV1hp4y1K78E&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;h4&gt;Demo:&lt;a href=&#34;https://www.bilibili.com/video/BV1TF411k78w&#34;&gt;https://www.bilibili.com/video/BV1TF411k78w&lt;/a&gt;&lt;/h4&gt; &#xA; &lt;h4&gt;QQ Group：815818430&lt;/h4&gt; &#xA; &lt;h2&gt;References&lt;/h2&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/anyvoiceai/MassTTS&#34;&gt;anyvoiceai/MassTTS&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;jaywalnut310/vits&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/p0p4k/vits2_pytorch&#34;&gt;p0p4k/vits2_pytorch&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/svc-develop-team/so-vits-svc&#34;&gt;svc-develop-team/so-vits-svc&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleSpeech&#34;&gt;PaddlePaddle/PaddleSpeech&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/innnky/emotional-vits&#34;&gt;emotional-vits&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/fishaudio/fish-speech&#34;&gt;fish-speech&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/jiangyuxiaoxiao/Bert-VITS2-UI&#34;&gt;Bert-VITS2-UI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h2&gt;感谢所有贡献者作出的努力&lt;/h2&gt; &#xA; &lt;a href=&#34;https://github.com/fishaudio/Bert-VITS2/graphs/contributors&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=fishaudio/Bert-VITS2&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>MarkFzp/act-plus-plus</title>
    <updated>2024-01-08T01:27:29Z</updated>
    <id>tag:github.com,2024-01-08:/MarkFzp/act-plus-plus</id>
    <link href="https://github.com/MarkFzp/act-plus-plus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Imitation Learning algorithms with Co-traing for Mobile ALOHA: ACT, Diffusion Policy, VINN&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Imitation Learning algorithms and Co-training for Mobile ALOHA&lt;/h1&gt; &#xA;&lt;h4&gt;Project Website: &lt;a href=&#34;https://mobile-aloha.github.io/&#34;&gt;https://mobile-aloha.github.io/&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;This repo contains the implementation of ACT, Diffusion Policy and VINN, together with 2 simulated environments: Transfer Cube and Bimanual Insertion. You can train and evaluate them in sim or real. For real, you would also need to install &lt;a href=&#34;https://github.com/MarkFzp/mobile-aloha&#34;&gt;Mobile ALOHA&lt;/a&gt;. This repo is forked from the &lt;a href=&#34;https://github.com/tonyzhaozh/act&#34;&gt;ACT repo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Updates:&lt;/h3&gt; &#xA;&lt;p&gt;You can find all scripted/human demo for simulated environments &lt;a href=&#34;https://drive.google.com/drive/folders/1gPR03v05S1xiInoVJn7G7VJ9pDCnxq9O?usp=share_link&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Repo Structure&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;imitate_episodes.py&lt;/code&gt; Train and Evaluate ACT&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;policy.py&lt;/code&gt; An adaptor for ACT policy&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;detr&lt;/code&gt; Model definitions of ACT, modified from DETR&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sim_env.py&lt;/code&gt; Mujoco + DM_Control environments with joint space control&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ee_sim_env.py&lt;/code&gt; Mujoco + DM_Control environments with EE space control&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;scripted_policy.py&lt;/code&gt; Scripted policies for sim environments&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;constants.py&lt;/code&gt; Constants shared across files&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;utils.py&lt;/code&gt; Utils such as data loading and helper functions&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;visualize_episodes.py&lt;/code&gt; Save videos from a .hdf5 dataset&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n aloha python=3.8.10&#xA;conda activate aloha&#xA;pip install torchvision&#xA;pip install torch&#xA;pip install pyquaternion&#xA;pip install pyyaml&#xA;pip install rospkg&#xA;pip install pexpect&#xA;pip install mujoco&#xA;pip install dm_control&#xA;pip install opencv-python&#xA;pip install matplotlib&#xA;pip install einops&#xA;pip install packaging&#xA;pip install h5py&#xA;pip install ipython&#xA;cd act/detr &amp;amp;&amp;amp; pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Example Usages&lt;/h3&gt; &#xA;&lt;p&gt;To set up a new terminal, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate aloha&#xA;cd &amp;lt;path to act repo&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Simulated experiments&lt;/h3&gt; &#xA;&lt;p&gt;We use &lt;code&gt;sim_transfer_cube_scripted&lt;/code&gt; task in the examples below. Another option is &lt;code&gt;sim_insertion_scripted&lt;/code&gt;. To generated 50 episodes of scripted data, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 record_sim_episodes.py --task_name sim_transfer_cube_scripted --dataset_dir &amp;lt;data save dir&amp;gt; --num_episodes 50&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To can add the flag &lt;code&gt;--onscreen_render&lt;/code&gt; to see real-time rendering. To visualize the episode after it is collected, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 visualize_episodes.py --dataset_dir &amp;lt;data save dir&amp;gt; --episode_idx 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To train ACT:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Transfer Cube task&#xA;python3 imitate_episodes.py --task_name sim_transfer_cube_scripted --ckpt_dir &amp;lt;ckpt dir&amp;gt; --policy_class ACT --kl_weight 10 --chunk_size 100 --hidden_dim 512 --batch_size 8 --dim_feedforward 3200 --num_epochs 2000  --lr 1e-5 --seed 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To evaluate the policy, run the same command but add &lt;code&gt;--eval&lt;/code&gt;. This loads the best validation checkpoint. The success rate should be around 90% for transfer cube, and around 50% for insertion. To enable temporal ensembling, add flag &lt;code&gt;--temporal_agg&lt;/code&gt;. Videos will be saved to &lt;code&gt;&amp;lt;ckpt_dir&amp;gt;&lt;/code&gt; for each rollout. You can also add &lt;code&gt;--onscreen_render&lt;/code&gt; to see real-time rendering during evaluation.&lt;/p&gt; &#xA;&lt;p&gt;For real-world data where things can be harder to model, train for at least 5000 epochs or 3-4 times the length after the loss has plateaued. Please refer to &lt;a href=&#34;https://docs.google.com/document/d/1FVIZfoALXg_ZkYKaYVh-qOlaXveq5CtvJHXkY25eYhs/edit?usp=sharing&#34;&gt;tuning tips&lt;/a&gt; for more info.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://docs.google.com/document/d/1FVIZfoALXg_ZkYKaYVh-qOlaXveq5CtvJHXkY25eYhs/edit?usp=sharing&#34;&gt;ACT tuning tips&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;TL;DR: if your ACT policy is jerky or pauses in the middle of an episode, just train for longer! Success rate and smoothness can improve way after loss plateaus.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>menyifang/En3D</title>
    <updated>2024-01-08T01:27:29Z</updated>
    <id>tag:github.com,2024-01-08:/menyifang/En3D</id>
    <link href="https://github.com/menyifang/En3D" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of &#34;En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;En3D - Official PyTorch Implementation&lt;/h1&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://menyifang.github.io/projects/En3D/index.html&#34;&gt;Project page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2401.01173&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=YxMjaKgGdCc&amp;amp;t=5s&#34;&gt;Video&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://menyifang.github.io/&#34;&gt;Yifang Men&lt;/a&gt;, &lt;a href=&#34;mailto:biwen.lbw@alibaba-inc.com&#34;&gt;Biwen Lei&lt;/a&gt;, &lt;a href=&#34;mailto:yaoy92@gmail.com&#34;&gt;Yuan Yao&lt;/a&gt;, &lt;a href=&#34;mailto:miaomiao.cmm@alibaba-inc.com&#34;&gt;Miaomiao Cui&lt;/a&gt;, &lt;a href=&#34;https://www.icst.pku.edu.cn/zlian/&#34;&gt;Zhouhui Lian&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=M0Ei1zkAAAAJ&amp;amp;hl=en&#34;&gt;Xuansong Xie&lt;/a&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;En3D is a large 3D human generative model trained on millions of synthetic 2D data, independently of any pre-existing 3D or 2D assets. This repo contains an implementation of En3D and provides a series of applications built upon it. In addition, this repo aims to be a useful creative tool to produce realistic 3D avatars from seeds, text prompts or images, and support automatic character animation FBX production. All outputs are compatible with the modern graphics workflows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Generative 3D humans&lt;/strong&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/menyifang/En3D/assets/47292223/8b57a74d-6270-4b37-ae1e-ee2c0baad51d&#34;&gt;https://github.com/menyifang/En3D/assets/47292223/8b57a74d-6270-4b37-ae1e-ee2c0baad51d&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text guided synthesis&lt;/strong&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/menyifang/En3D/main/assets/demo_text.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Image guided synthesis&lt;/strong&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/menyifang/En3D/main/assets/demo_img.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;More results can be found in &lt;a href=&#34;https://menyifang.github.io/projects/En3D/index.html&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;p&gt;(2023-01-03) The paper and video are released.&lt;/p&gt; &#xA;&lt;p&gt;(2023-12-20) The project page is available now at &lt;a href=&#34;https://menyifang.github.io/projects/En3D/index.html&#34;&gt;website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code useful for your research, please use the following BibTeX entry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{men2024en3d,&#xA;  title={En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data},&#xA;  author={Men, Yifang and Lei, Biwen and Yao, Yuan and Cui, Miaomiao and Lian, Zhouhui and Xie, Xuansong},&#xA;  journal={arXiv preprint arXiv:2401.01173},&#xA;  website={https://menyifang.github.io/projects/En3D/index.html},&#xA;  year={2024}}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>