<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-03-26T01:28:37Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>alibaba/spring-ai-alibaba</title>
    <updated>2025-03-26T01:28:37Z</updated>
    <id>tag:github.com,2025-03-26:/alibaba/spring-ai-alibaba</id>
    <link href="https://github.com/alibaba/spring-ai-alibaba" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agentic AI Framework for Java Developers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;a href=&#34;https://java2ai.com&#34;&gt;Spring AI Alibaba&lt;/a&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/README-zh.md&#34;&gt;中文版本&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An AI application framework for Java developers built on top of Spring AI that provides seamless integration with Alibaba Cloud QWen LLM services and cloud-native infrastructures.&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://java2ai.com/docs/dev/get-started/&#34;&gt;quick start&lt;/a&gt; for how to quickly add generative AI to your Spring Boot applications.&lt;/p&gt; &#xA;&lt;p&gt;Overall, it takes only two steps to turn your Spring Boot application into an intelligent agent:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Because Spring AI Alibaba is developed based on Spring Boot 3.x, it requires JDK version 17 and above.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Add &lt;code&gt;spring-ai-alibaba-starter&lt;/code&gt; dependency to your project.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;dependency&amp;gt;&#xA; &amp;lt;groupId&amp;gt;com.alibaba.cloud.ai&amp;lt;/groupId&amp;gt;&#xA; &amp;lt;artifactId&amp;gt;spring-ai-alibaba-starter&amp;lt;/artifactId&amp;gt;&#xA; &amp;lt;version&amp;gt;1.0.0-M6.1&amp;lt;/version&amp;gt;&#xA;&amp;lt;/dependency&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;NOTICE: Since spring-ai related packages haven&#39;t been published to the central repo yet, it&#39;s needed to add the following maven repository to your project in order to successfully resolve artifacts like spring-ai-core.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;repositories&amp;gt;&#xA; &amp;lt;repository&amp;gt;&#xA;  &amp;lt;id&amp;gt;spring-milestones&amp;lt;/id&amp;gt;&#xA;  &amp;lt;name&amp;gt;Spring Milestones&amp;lt;/name&amp;gt;&#xA;  &amp;lt;url&amp;gt;https://repo.spring.io/milestone&amp;lt;/url&amp;gt;&#xA;  &amp;lt;snapshots&amp;gt;&#xA;   &amp;lt;enabled&amp;gt;false&amp;lt;/enabled&amp;gt;&#xA;  &amp;lt;/snapshots&amp;gt;&#xA; &amp;lt;/repository&amp;gt;&#xA;&amp;lt;/repositories&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Addendum: If the mirrorOf tag in your local Maven settings. xml is configured with the wildcard *, please modify it according to the following example.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-xml&#34;&gt;&amp;lt;mirror&amp;gt;&#xA;  &amp;lt;id&amp;gt;xxxx&amp;lt;/id&amp;gt;&#xA;  &amp;lt;mirrorOf&amp;gt;*,!spring-milestones&amp;lt;/mirrorOf&amp;gt;&#xA;  &amp;lt;name&amp;gt;xxxx&amp;lt;/name&amp;gt;&#xA;  &amp;lt;url&amp;gt;xxxx&amp;lt;/url&amp;gt;&#xA;&amp;lt;/mirror&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Inject &lt;code&gt;ChatClient&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;@RestController&#xA;public class ChatController {&#xA;&#xA; private final ChatClient chatClient;&#xA;&#xA; public ChatController(ChatClient.Builder builder) {&#xA;  this.chatClient = builder.build();&#xA; }&#xA;&#xA; @GetMapping(&#34;/chat&#34;)&#xA; public String chat(String input) {&#xA;  return this.chatClient.prompt()&#xA;    .user(input)&#xA;    .call()&#xA;    .content();&#xA; }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/springaialibaba/spring-ai-alibaba-examples&#34;&gt;Spring AI Alibaba and Spring AI usage examples&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Core Features&lt;/h2&gt; &#xA;&lt;p&gt;Spring AI Alibaba provides the following features, read the &lt;a href=&#34;https://java2ai.com/&#34;&gt;documentation&lt;/a&gt; on our website for more details of how to use these features.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Support for Alibaba Cloud QWen Model and Dashscope Model service.&lt;/li&gt; &#xA; &lt;li&gt;Support high-level AI agent abstraction -- ChatClient.&lt;/li&gt; &#xA; &lt;li&gt;Support various Model types like Chat, Text to Image, Audio Transcription, Text to Speech.&lt;/li&gt; &#xA; &lt;li&gt;Both synchronous and stream API options are supported.&lt;/li&gt; &#xA; &lt;li&gt;Mapping of AI Model output to POJOs.&lt;/li&gt; &#xA; &lt;li&gt;Portable API across Vector Store providers.&lt;/li&gt; &#xA; &lt;li&gt;Function calling.&lt;/li&gt; &#xA; &lt;li&gt;Spring Boot Auto Configuration and Starters.&lt;/li&gt; &#xA; &lt;li&gt;RAG (Retrieval-Augmented Generation) support: DocumentReader, Splitter, Embedding, VectorStore, and Retriever.&lt;/li&gt; &#xA; &lt;li&gt;Support conversation with ChatMemory&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Spring AI Alibaba aims to reduce the complexity of building ai native java applications, from development, evaluation to deployment and observability. In order to achieve that, we provide both open-source framework and ecosystem integrations around it, below are the features that we plan to support in the near future:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Prompt Template Management&lt;/li&gt; &#xA; &lt;li&gt;Event Driven AI Application&lt;/li&gt; &#xA; &lt;li&gt;Support of more Vector Databases&lt;/li&gt; &#xA; &lt;li&gt;Function Deployment&lt;/li&gt; &#xA; &lt;li&gt;Observability&lt;/li&gt; &#xA; &lt;li&gt;AI proxy support: prompt filtering, rate limit, multiple Model, etc.&lt;/li&gt; &#xA; &lt;li&gt;Development Tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/docs/imgs/spring-ai-alibaba-arch.png&#34; alt=&#34;ai-native-architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribution Guide&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt; to learn how to participate in the development of Spring AI Alibaba.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.spring.io/spring-ai/reference/index.html&#34;&gt;Spring AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://java2ai.com/docs/dev/overview/&#34;&gt;Spring AI Alibaba&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.aliyun.com/zh/model-studio/getting-started/what-is-model-studio/&#34;&gt;Alibaba Cloud Dashscope Model Service Platform (阿里云百炼模型服务及应用开发平台)&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dingtalk Group (钉钉群), search &lt;code&gt;61290041831&lt;/code&gt; and join.&lt;/li&gt; &#xA; &lt;li&gt;Wechat Group (微信公众号), scan the QR code below and follow us.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/alibaba/spring-ai-alibaba/main/docs/imgs/wechat-account.png&#34; style=&#34;width:260px;&#34;&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;p&gt;Some of this project&#39;s ideas and codes are inspired by or rewrote from the following projects. Great thanks to those who have created and open-sourced these projects.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spring-projects/spring-ai&#34;&gt;Spring AI&lt;/a&gt;, a Spring-friendly API and abstractions for developing AI applications licensed under the Apache License 2.0.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/langchain-ai/langgraph&#34;&gt;Langgraph&lt;/a&gt;, a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows licensed under the MIT license.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bsorrentino/langgraph4j&#34;&gt;Langgraph4J&lt;/a&gt;, a porting of original &lt;a href=&#34;https://github.com/langchain-ai/langgraph&#34;&gt;LangGraph&lt;/a&gt; from the &lt;a href=&#34;https://github.com/langchain-ai&#34;&gt;LangChain AI project&lt;/a&gt; in Java fashion.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>agno-agi/agno</title>
    <updated>2025-03-26T01:28:37Z</updated>
    <id>tag:github.com,2025-03-26:/agno-agi/agno</id>
    <link href="https://github.com/agno-agi/agno" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agno is a lightweight library for building Multimodal Agents. It exposes LLMs as a unified API and gives them superpowers like memory, knowledge, tools and reasoning.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; id=&#34;top&#34;&gt; &#xA; &lt;a href=&#34;https://docs.agno.com&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg&#34;&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg&#34;&gt; &#xA;   &lt;img src=&#34;https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg?sanitize=true&#34; alt=&#34;Agno&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://docs.agno.com&#34;&gt;📚 Documentation&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &#xA; &lt;a href=&#34;https://docs.agno.com/examples/introduction&#34;&gt;💡 Examples&lt;/a&gt; &amp;nbsp;|&amp;nbsp; &#xA; &lt;a href=&#34;https://github.com/agno-agi/agno/stargazers&#34;&gt;🌟 Star Us&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.agno.com&#34;&gt;Agno&lt;/a&gt; is a lightweight library for building Multimodal Agents. It exposes LLMs as a unified API and gives them superpowers like memory, knowledge, tools and reasoning.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build lightning-fast Agents that can generate text, image, audio and video.&lt;/li&gt; &#xA; &lt;li&gt;Add memory, knowledge, tools and reasoning as needed.&lt;/li&gt; &#xA; &lt;li&gt;Run anywhere, Agno is open-source.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s an Agent that can search the web:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agno.agent import Agent&#xA;from agno.models.openai import OpenAIChat&#xA;from agno.tools.duckduckgo import DuckDuckGoTools&#xA;&#xA;agent = Agent(&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    tools=[DuckDuckGoTools()],&#xA;    markdown=True&#xA;)&#xA;agent.print_response(&#34;What&#39;s happening in New York?&#34;, stream=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Key features&lt;/h2&gt; &#xA;&lt;p&gt;Agno is simple, fast and model agnostic. Here are some key features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lightning Fast&lt;/strong&gt;: Agent creation is 10,000x faster than LangGraph (see &lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/#performance&#34;&gt;performance&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model Agnostic&lt;/strong&gt;: Use any model, any provider, no lock-in.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi Modal&lt;/strong&gt;: Native support for text, image, audio and video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi Agent&lt;/strong&gt;: Build teams of specialized agents.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory Management&lt;/strong&gt;: Store agent sessions and state in a database.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Knowledge Stores&lt;/strong&gt;: Use vector databases for RAG or dynamic few-shot learning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Structured Outputs&lt;/strong&gt;: Make Agents respond in a structured format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Monitoring&lt;/strong&gt;: Track agent sessions and performance in real-time on &lt;a href=&#34;https://app.agno.com&#34;&gt;agno.com&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start by &lt;a href=&#34;https://docs.agno.com/introduction/agents&#34;&gt;building your first Agent&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Check out the &lt;a href=&#34;https://docs.agno.com/examples/introduction&#34;&gt;examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://docs.agno.com&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -U agno&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What are Agents?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Agents&lt;/strong&gt; are intelligent programs that solve problems autonomously.&lt;/p&gt; &#xA;&lt;p&gt;Agents have memory, domain knowledge and the ability to use tools (like searching the web, querying a database, making API calls). Unlike traditional programs that follow a predefined execution path, Agents dynamically adapt their approach based on the context and tool results.&lt;/p&gt; &#xA;&lt;p&gt;Instead of a rigid binary definition, let&#39;s think of Agents in terms of agency and autonomy.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Level 0&lt;/strong&gt;: Agents with no tools (basic inference tasks).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Level 1&lt;/strong&gt;: Agents with tools for autonomous task execution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Level 2&lt;/strong&gt;: Agents with knowledge, combining memory and reasoning.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Level 3&lt;/strong&gt;: Teams of specialized agents collaborating on complex workflows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Example - Basic Agent&lt;/h2&gt; &#xA;&lt;p&gt;The simplest Agent is just an inference task, no tools, no memory, no knowledge.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agno.agent import Agent&#xA;from agno.models.openai import OpenAIChat&#xA;&#xA;agent = Agent(&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    description=&#34;You are an enthusiastic news reporter with a flair for storytelling!&#34;,&#xA;    markdown=True&#xA;)&#xA;agent.print_response(&#34;Tell me about a breaking news story from New York.&#34;, stream=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run the agent, install dependencies and export your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install agno openai&#xA;&#xA;export OPENAI_API_KEY=sk-xxxx&#xA;&#xA;python basic_agent.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/01_basic_agent.py&#34;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example - Agent with tools&lt;/h2&gt; &#xA;&lt;p&gt;This basic agent will obviously make up a story, lets give it a tool to search the web.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agno.agent import Agent&#xA;from agno.models.openai import OpenAIChat&#xA;from agno.tools.duckduckgo import DuckDuckGoTools&#xA;&#xA;agent = Agent(&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    description=&#34;You are an enthusiastic news reporter with a flair for storytelling!&#34;,&#xA;    tools=[DuckDuckGoTools()],&#xA;    show_tool_calls=True,&#xA;    markdown=True&#xA;)&#xA;agent.print_response(&#34;Tell me about a breaking news story from New York.&#34;, stream=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies and run the Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install duckduckgo-search&#xA;&#xA;python agent_with_tools.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you should see a much more relevant result.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/02_agent_with_tools.py&#34;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example - Agent with knowledge&lt;/h2&gt; &#xA;&lt;p&gt;Agents can store knowledge in a vector database and use it for RAG or dynamic few-shot learning.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Agno agents use Agentic RAG&lt;/strong&gt; by default, which means they will search their knowledge base for the specific information they need to achieve their task.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agno.agent import Agent&#xA;from agno.models.openai import OpenAIChat&#xA;from agno.embedder.openai import OpenAIEmbedder&#xA;from agno.tools.duckduckgo import DuckDuckGoTools&#xA;from agno.knowledge.pdf_url import PDFUrlKnowledgeBase&#xA;from agno.vectordb.lancedb import LanceDb, SearchType&#xA;&#xA;agent = Agent(&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    description=&#34;You are a Thai cuisine expert!&#34;,&#xA;    instructions=[&#xA;        &#34;Search your knowledge base for Thai recipes.&#34;,&#xA;        &#34;If the question is better suited for the web, search the web to fill in gaps.&#34;,&#xA;        &#34;Prefer the information in your knowledge base over the web results.&#34;&#xA;    ],&#xA;    knowledge=PDFUrlKnowledgeBase(&#xA;        urls=[&#34;https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf&#34;],&#xA;        vector_db=LanceDb(&#xA;            uri=&#34;tmp/lancedb&#34;,&#xA;            table_name=&#34;recipes&#34;,&#xA;            search_type=SearchType.hybrid,&#xA;            embedder=OpenAIEmbedder(id=&#34;text-embedding-3-small&#34;),&#xA;        ),&#xA;    ),&#xA;    tools=[DuckDuckGoTools()],&#xA;    show_tool_calls=True,&#xA;    markdown=True&#xA;)&#xA;&#xA;# Comment out after the knowledge base is loaded&#xA;if agent.knowledge is not None:&#xA;    agent.knowledge.load()&#xA;&#xA;agent.print_response(&#34;How do I make chicken and galangal in coconut milk soup&#34;, stream=True)&#xA;agent.print_response(&#34;What is the history of Thai curry?&#34;, stream=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies and run the Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install lancedb tantivy pypdf duckduckgo-search&#xA;&#xA;python agent_with_knowledge.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/03_agent_with_knowledge.py&#34;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Example - Multi Agent Teams&lt;/h2&gt; &#xA;&lt;p&gt;Agents work best when they have a singular purpose, a narrow scope and a small number of tools. When the number of tools grows beyond what the language model can handle or the tools belong to different categories, use a team of agents to spread the load.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from agno.agent import Agent&#xA;from agno.models.openai import OpenAIChat&#xA;from agno.tools.duckduckgo import DuckDuckGoTools&#xA;from agno.tools.yfinance import YFinanceTools&#xA;from agno.team import Team&#xA;&#xA;web_agent = Agent(&#xA;    name=&#34;Web Agent&#34;,&#xA;    role=&#34;Search the web for information&#34;,&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    tools=[DuckDuckGoTools()],&#xA;    instructions=&#34;Always include sources&#34;,&#xA;    show_tool_calls=True,&#xA;    markdown=True,&#xA;)&#xA;&#xA;finance_agent = Agent(&#xA;    name=&#34;Finance Agent&#34;,&#xA;    role=&#34;Get financial data&#34;,&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    tools=[YFinanceTools(stock_price=True, analyst_recommendations=True, company_info=True)],&#xA;    instructions=&#34;Use tables to display data&#34;,&#xA;    show_tool_calls=True,&#xA;    markdown=True,&#xA;)&#xA;&#xA;agent_team = Agent(&#xA;    mode=&#34;coordinate&#34;,&#xA;    members=[web_agent, finance_agent],&#xA;    model=OpenAIChat(id=&#34;gpt-4o&#34;),&#xA;    success_criteria=&#34;A comprehensive financial news report with clear sections and data-driven insights.&#34;,&#xA;    instructions=[&#34;Always include sources&#34;, &#34;Use tables to display data&#34;],&#xA;    show_tool_calls=True,&#xA;    markdown=True,&#xA;)&#xA;&#xA;agent_team.print_response(&#34;What&#39;s the market outlook and financial performance of AI semiconductor companies?&#34;, stream=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install dependencies and run the Agent team:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install duckduckgo-search yfinance&#xA;&#xA;python agent_team.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/cookbook/getting_started/05_agent_team.py&#34;&gt;View this example in the cookbook&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;At Agno, we&#39;re obsessed with performance. Why? because even simple AI workflows can spawn thousands of Agents to achieve their goals. Scale that to a modest number of users and performance becomes a bottleneck. Agno is designed to power high performance agentic systems:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Agent instantiation: ~2μs on average (~10,000x faster than LangGraph).&lt;/li&gt; &#xA; &lt;li&gt;Memory footprint: ~3.75Kib on average (~50x less memory than LangGraph).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Tested on an Apple M4 Mackbook Pro.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;While an Agent&#39;s run-time is bottlenecked by inference, we must do everything possible to minimize execution time, reduce memory usage, and parallelize tool calls. These numbers may seem trivial at first, but our experience shows that they add up even at a reasonably small scale.&lt;/p&gt; &#xA;&lt;h3&gt;Instantiation time&lt;/h3&gt; &#xA;&lt;p&gt;Let&#39;s measure the time it takes for an Agent with 1 tool to start up. We&#39;ll run the evaluation 1000 times to get a baseline measurement.&lt;/p&gt; &#xA;&lt;p&gt;You should run the evaluation yourself on your own machine, please, do not take these results at face value.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Setup virtual environment&#xA;./scripts/perf_setup.sh&#xA;source .venvs/perfenv/bin/activate&#xA;# OR Install dependencies manually&#xA;# pip install openai agno langgraph langchain_openai&#xA;&#xA;# Agno&#xA;python evals/performance/instantiation_with_tool.py&#xA;&#xA;# LangGraph&#xA;python evals/performance/other/langgraph_instantiation.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The following evaluation is run on an Apple M4 Mackbook Pro. It also runs as a Github action on this repo.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;LangGraph is on the right, &lt;strong&gt;let&#39;s start it first and give it a head start&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Agno is on the left, notice how it finishes before LangGraph gets 1/2 way through the runtime measurement, and hasn&#39;t even started the memory measurement. That&#39;s how fast Agno is.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23&#34;&gt;https://github.com/user-attachments/assets/ba466d45-75dd-45ac-917b-0a56c5742e23&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Dividing the average time of a Langgraph Agent by the average time of an Agno Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;0.020526s / 0.000002s ~ 10,263&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In this particular run, &lt;strong&gt;Agno Agents startup is roughly 10,000 times faster than Langgraph Agents&lt;/strong&gt;. The numbers continue to favor Agno as the number of tools grow, and we add memory and knowledge stores.&lt;/p&gt; &#xA;&lt;h3&gt;Memory usage&lt;/h3&gt; &#xA;&lt;p&gt;To measure memory usage, we use the &lt;code&gt;tracemalloc&lt;/code&gt; library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.&lt;/p&gt; &#xA;&lt;p&gt;We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we&#39;ve made a mistake, please let us know.&lt;/p&gt; &#xA;&lt;p&gt;Dividing the average memory usage of a Langgraph Agent by the average memory usage of an Agno Agent:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;0.137273/0.002528 ~ 54.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Langgraph Agents use ~50x more memory than Agno Agents&lt;/strong&gt;. In our opinion, memory usage is a much more important metric than instantiation time. As we start running thousands of Agents in production, these numbers directly start affecting the cost of running the Agents.&lt;/p&gt; &#xA;&lt;h3&gt;Conclusion&lt;/h3&gt; &#xA;&lt;p&gt;Agno agents are designed for performance and while we do share some benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ll be publishing accuracy and reliability benchmarks running on Github actions in the coming weeks. Given that each framework is different and we won&#39;t be able to tune their performance like we do with Agno, for future benchmarks we&#39;ll only be comparing against ourselves.&lt;/p&gt; &#xA;&lt;h2&gt;Cursor Setup&lt;/h2&gt; &#xA;&lt;p&gt;When building Agno agents, using Agno documentation as a source in Cursor is a great way to speed up your development.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In Cursor, go to the settings or preferences section.&lt;/li&gt; &#xA; &lt;li&gt;Find the section to manage documentation sources.&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;code&gt;https://docs.agno.com&lt;/code&gt; to the list of documentation URLs.&lt;/li&gt; &#xA; &lt;li&gt;Save the changes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Now, Cursor will have access to the Agno documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation, Community &amp;amp; More examples&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Docs: &lt;a href=&#34;https://docs.agno.com&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;docs.agno.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Getting Started Examples: &lt;a href=&#34;https://github.com/agno-agi/agno/tree/main/cookbook/getting_started&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Getting Started Cookbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;All Examples: &lt;a href=&#34;https://github.com/agno-agi/agno/tree/main/cookbook&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Cookbook&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Community forum: &lt;a href=&#34;https://community.agno.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;community.agno.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chat: &lt;a href=&#34;https://discord.gg/4MtYHHrgA8&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;discord&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions, read our &lt;a href=&#34;https://github.com/agno-agi/agno/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started.&lt;/p&gt; &#xA;&lt;h2&gt;Telemetry&lt;/h2&gt; &#xA;&lt;p&gt;Agno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting &lt;code&gt;AGNO_TELEMETRY=false&lt;/code&gt; in your environment.&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/agno-agi/agno/main/#top&#34;&gt;⬆️ Back to Top&lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ourongxing/newsnow</title>
    <updated>2025-03-26T01:28:37Z</updated>
    <id>tag:github.com,2025-03-26:/ourongxing/newsnow</id>
    <link href="https://github.com/ourongxing/newsnow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Elegant reading of real-time and hottest news&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;NewsNow&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hellogithub.com/repository/c2978695e74a423189e9ca2543ab3b36&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://api.hellogithub.com/v1/widgets/recommend.svg?rid=c2978695e74a423189e9ca2543ab3b36&amp;amp;claim_uid=SMJiFwlsKCkWf89&amp;amp;theme=small&#34; alt=&#34;Featured｜HelloGitHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ourongxing/newsnow/main/screenshots/preview-1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/ourongxing/newsnow/main/screenshots/preview-2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/ourongxing/newsnow/main/README.zh-CN.md&#34;&gt;简体中文&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Elegant reading of real-time and hottest news&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Elegant design for a pleasant reading experience, keeping you up-to-date with the latest and hottest news.&lt;/li&gt; &#xA; &lt;li&gt;Supports Github login and data synchronization.&lt;/li&gt; &#xA; &lt;li&gt;Default cache duration is 30 minutes. Logged-in users can force fetch the latest data. However, the scraping interval is adjusted based on the update frequency of the content sources (as fast as every two minutes) to save resources and prevent frequent scraping that could lead to IP bans.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;If login and caching are not required, you can directly deploy to platforms like Cloudflare Pages or Vercel. Just fork the repository and import it into the respective platform.&lt;/p&gt; &#xA;&lt;p&gt;For Cloudflare Pages, you need to set the build command to &lt;code&gt;pnpm run build&lt;/code&gt; and the build output directory to &lt;code&gt;dist/output/public&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For login, which involves GitHub OAuth, you only need to &lt;a href=&#34;https://github.com/settings/applications/new&#34;&gt;create a GitHub App&lt;/a&gt;. No special permissions are required. The callback URL should be &lt;code&gt;https://your-domain.com/api/oauth/github&lt;/code&gt; (replace &lt;code&gt;your-domain&lt;/code&gt; with your actual domain).&lt;/p&gt; &#xA;&lt;p&gt;After creating the app, you will get a Client ID and Client Secret. Different platforms have different places to set environment variables; refer to the &lt;code&gt;example.env.server&lt;/code&gt; file. If running locally, rename it to &lt;code&gt;.env.server&lt;/code&gt; and add the necessary values.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;# Github Client ID&#xA;G_CLIENT_ID=&#xA;# Github Client Secret&#xA;G_CLIENT_SECRET=&#xA;# JWT Secret, usually the same as Client Secret&#xA;JWT_SECRET=&#xA;# Initialize database, must be set to true on first run, can be turned off afterward&#xA;INIT_TABLE=true&#xA;# Whether to enable cache&#xA;ENABLE_CACHE=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This project primarily supports deployment on Cloudflare Pages and Docker. For Vercel, you need to set up your own database. Supported databases can be found at &lt;a href=&#34;https://db0.unjs.io/connectors&#34;&gt;https://db0.unjs.io/connectors&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;p&gt;The Cloudflare D1 database can be used for free. To set it up, go to the Cloudflare Worker control panel and manually create a D1 database. Then, add the &lt;code&gt;database_id&lt;/code&gt; and &lt;code&gt;database_name&lt;/code&gt; to the corresponding fields in your &lt;code&gt;wrangler.toml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t have a &lt;code&gt;wrangler.toml&lt;/code&gt; file, you can rename &lt;code&gt;example.wrangler.toml&lt;/code&gt; to &lt;code&gt;wrangler.toml&lt;/code&gt; and modify it with your configuration. The changes will take effect on your next deployment.&lt;/p&gt; &#xA;&lt;p&gt;For Docker deployment. In the project root directory with &lt;code&gt;docker-compose.yml&lt;/code&gt;, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] Node version &amp;gt;= 20&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;corepack enable&#xA;pnpm i&#xA;pnpm dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to add data sources, refer to the &lt;code&gt;shared/sources&lt;/code&gt;, and &lt;code&gt;server/sources&lt;/code&gt; directories. The project has complete types and a simple structure; feel free to explore.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ourongxing/newsnow/main/LICENSE&#34;&gt;MIT&lt;/a&gt; © ourongxing&lt;/p&gt;</summary>
  </entry>
</feed>