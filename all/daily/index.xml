<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-27T01:29:09Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>binary-husky/chatgpt_academic</title>
    <updated>2023-03-27T01:29:09Z</updated>
    <id>tag:github.com,2023-03-27:/binary-husky/chatgpt_academic</id>
    <link href="https://github.com/binary-husky/chatgpt_academic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;中科院科研工作专用ChatGPT，特别优化学术Paper润色体验，支持自定义快捷按钮，支持markdown表格显示，Tex公式双显示，代码显示功能完善，新增本地Python工程剖析功能/自我剖析功能&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGPT 学术优化&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;如果喜欢这个项目，请给它一个Star；如果你发明了更好用的学术快捷键，欢迎发issue或者pull requests&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you like this project, please give it a Star. If you&#39;ve come up with more useful academic shortcuts, feel free to open an issue or pull request.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;功能&lt;/th&gt; &#xA;    &lt;th&gt;描述&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;一键润色&lt;/td&gt; &#xA;    &lt;td&gt;支持一键润色、一键查找论文语法错误&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;一键中英互译&lt;/td&gt; &#xA;    &lt;td&gt;一键中英互译&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;一键代码解释&lt;/td&gt; &#xA;    &lt;td&gt;可以正确显示代码、解释代码&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;自定义快捷键&lt;/td&gt; &#xA;    &lt;td&gt;支持自定义快捷键&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;配置代理服务器&lt;/td&gt; &#xA;    &lt;td&gt;支持配置代理服务器&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;模块化设计&lt;/td&gt; &#xA;    &lt;td&gt;支持自定义高阶的实验性功能&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;自我程序剖析&lt;/td&gt; &#xA;    &lt;td&gt;[实验性功能] 一键读懂本项目的源代码&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;程序剖析&lt;/td&gt; &#xA;    &lt;td&gt;[实验性功能] 一键可以剖析其他Python/C++项目&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;读论文&lt;/td&gt; &#xA;    &lt;td&gt;[实验性功能] 一键解读latex论文全文并生成摘要&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;批量注释生成&lt;/td&gt; &#xA;    &lt;td&gt;[实验性功能] 一键批量生成函数注释&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;chat分析报告生成&lt;/td&gt; &#xA;    &lt;td&gt;[实验性功能] 运行后自动生成总结汇报&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;公式显示&lt;/td&gt; &#xA;    &lt;td&gt;可以同时显示公式的tex形式和渲染形式&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;图片显示&lt;/td&gt; &#xA;    &lt;td&gt;可以在markdown中显示图片&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;支持GPT输出的markdown表格&lt;/td&gt; &#xA;    &lt;td&gt;可以输出支持GPT的markdown表格&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;新界面&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227528413-36ab42da-d589-4ef1-ba75-28aa02442d05.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;所有按钮都通过读取functional.py动态生成，可随意加自定义功能，解放粘贴板&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;img/公式.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;代码的显示自然也不在话下 &lt;a href=&#34;https://www.bilibili.com/video/BV1F24y147PD/&#34;&gt;https://www.bilibili.com/video/BV1F24y147PD/&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;img/润色.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;支持GPT输出的markdown表格&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/binary-husky/chatgpt_academic/master/img/demo2.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;如果输出包含公式，会同时以tex形式和渲染形式显示，方便复制和阅读&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/binary-husky/chatgpt_academic/master/img/demo.jpg&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;懒得看项目代码？整个工程直接给chatgpt炫嘴里&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226935232-6b6a73ce-8900-4aee-93f9-733c7e6fef53.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;直接运行 (Windows or Linux or MacOS)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 下载项目&#xA;git clone https://github.com/binary-husky/chatgpt_academic.git&#xA;cd chatgpt_academic&#xA;# 在config.py中，配置 海外Proxy 和 OpenAI API KEY&#xA;- 1.如果你在国内，需要设置海外代理才能够使用 OpenAI API，你可以通过 config.py 文件来进行设置。&#xA;- 2.配置 OpenAI API KEY。你需要在 OpenAI 官网上注册并获取 API KEY。一旦你拿到了 API KEY，在 config.py 文件里配置好即可。&#xA;# 安装依赖&#xA;python -m pip install -r requirements.txt&#xA;# 运行&#xA;python main.py&#xA;&#xA;# 测试实验性功能&#xA;## 测试C++项目头文件分析&#xA;input区域 输入 ./crazy_functions/test_project/cpp/libJPG ， 然后点击 &#34;[实验] 解析整个C++项目（input输入项目根路径）&#34;&#xA;## 测试给Latex项目写摘要&#xA;input区域 输入 ./crazy_functions/test_project/latex/attention ， 然后点击 &#34;[实验] 读tex论文写摘要（input输入项目根路径）&#34;&#xA;## 测试Python项目分析&#xA;input区域 输入 ./crazy_functions/test_project/python/dqn ， 然后点击 &#34;[实验] 解析整个py项目（input输入项目根路径）&#34;&#xA;## 测试自我代码解读&#xA;点击 &#34;[实验] 请解析并解构此项目本身&#34;&#xA;## 测试实验功能模板函数（要求gpt回答几个数的平方是什么），您可以根据此函数为模板，实现更复杂的功能&#xA;点击 &#34;[实验] 实验功能函数模板&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;使用docker (Linux)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 下载项目&#xA;git clone https://github.com/binary-husky/chatgpt_academic.git&#xA;cd chatgpt_academic&#xA;# 配置 海外Proxy 和 OpenAI API KEY&#xA;config.py&#xA;# 安装&#xA;docker build -t gpt-academic .&#xA;# 运行&#xA;docker run --rm -it --net=host gpt-academic&#xA;&#xA;# 测试实验性功能&#xA;## 测试自我代码解读&#xA;点击 &#34;[实验] 请解析并解构此项目本身&#34;&#xA;## 测试实验功能模板函数（要求gpt回答几个数的平方是什么），您可以根据此函数为模板，实现更复杂的功能&#xA;点击 &#34;[实验] 实验功能函数模板&#34;&#xA;##（请注意在docker中运行时，需要额外注意程序的文件访问权限问题）&#xA;## 测试C++项目头文件分析&#xA;input区域 输入 ./crazy_functions/test_project/cpp/libJPG ， 然后点击 &#34;[实验] 解析整个C++项目（input输入项目根路径）&#34;&#xA;## 测试给Latex项目写摘要&#xA;input区域 输入 ./crazy_functions/test_project/latex/attention ， 然后点击 &#34;[实验] 读tex论文写摘要（input输入项目根路径）&#34;&#xA;## 测试Python项目分析&#xA;input区域 输入 ./crazy_functions/test_project/python/dqn ， 然后点击 &#34;[实验] 解析整个py项目（input输入项目根路径）&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;自定义新的便捷按钮（学术快捷键自定义）&lt;/h2&gt; &#xA;&lt;p&gt;打开functional.py，添加条目如下，然后重启程序即可。（如果按钮已经添加成功并可见，那么前缀、后缀都支持热修改，无需重启程序即可生效。） 例如&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;超级英译中&#34;: {&#xA;&#xA;    # 前缀，会被加在你的输入之前。例如，用来描述你的要求，例如翻译、解释代码、润色等等&#xA;    &#34;Prefix&#34;: &#34;请翻译把下面一段内容成中文，然后用一个markdown表格逐一解释文中出现的专有名词：\n\n&#34;, &#xA;    &#xA;    # 后缀，会被加在你的输入之后。例如，配合前缀可以把你的输入内容用引号圈起来。&#xA;    &#34;Suffix&#34;: &#34;&#34;,&#xA;    &#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226899272-477c2134-ed71-4326-810c-29891fe4a508.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;如果你发明了更好用的学术快捷键，欢迎发issue或者pull requests！&lt;/p&gt; &#xA;&lt;h2&gt;配置代理&lt;/h2&gt; &#xA;&lt;p&gt;在&lt;code&gt;config.py&lt;/code&gt;中修改端口与代理软件对应&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226571294-37a47cd9-4d40-4c16-97a2-d360845406f7.png&#34; width=&#34;500&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226838985-e5c95956-69c2-4c23-a4dd-cd7944eeb451.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;配置完成后，你可以用以下命令测试代理是否工作，如果一切正常，下面的代码将输出你的代理服务器所在地：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python check_proxy.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;兼容性测试&lt;/h2&gt; &#xA;&lt;h3&gt;图片显示：&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226906087-b5f1c127-2060-4db9-af05-487643b21ed9.png&#34; height=&#34;200&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226906703-7226495d-6a1f-4a53-9728-ce6778cbdd19.png&#34; height=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;如果一个程序能够读懂并剖析自己：&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226936850-c77d7183-0749-4c1c-9875-fd4891842d0c.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226936618-9b487e4b-ab5b-4b6e-84c6-16942102e917.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;其他任意Python/Cpp项目剖析：&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226935232-6b6a73ce-8900-4aee-93f9-733c7e6fef53.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226969067-968a27c1-1b9c-486b-8b81-ab2de8d3f88a.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Latex论文一键阅读理解与摘要生成&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227504406-86ab97cd-f208-41c3-8e4a-7000e51cf980.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;自动报告生成&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227503770-fe29ce2c-53fd-47b0-b0ff-93805f0c2ff4.png&#34; height=&#34;300&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227504617-7a497bb3-0a2a-4b50-9a8a-95ae60ea7afd.png&#34; height=&#34;300&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227504005-efeaefe0-b687-49d0-bf95-2d7b7e66c348.png&#34; height=&#34;300&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;模块化功能设计&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227504981-4c6c39c0-ae79-47e6-bffe-0e6442d9da65.png&#34; height=&#34;400&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227504931-19955f78-45cd-4d1c-adac-e71e50957915.png&#34; height=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;参考项目&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/Python-Markdown/markdown&#xA;https://github.com/gradio-app/gradio&#xA;https://github.com/polarwinkel/mdtex2html&#xA;https://github.com/GaiZhenbiao/ChuanhuChatGPT&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>sahil280114/codealpaca</title>
    <updated>2023-03-27T01:29:09Z</updated>
    <id>tag:github.com,2023-03-27:/sahil280114/codealpaca</id>
    <link href="https://github.com/sahil280114/codealpaca" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Code Alpaca: An Instruction-following LLaMA Model trained on code generation instructions&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the repo for the Code Alpaca project, which aims to build and share an instruction-following LLaMA model for code generation. This repo is fully based on &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca&lt;/a&gt; ,and only changes the data used for training. Training approach is the same.&lt;/p&gt; &#xA;&lt;p&gt;The repo contains:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/#data-release&#34;&gt;20K data&lt;/a&gt; used for fine-tuning the model&lt;/li&gt; &#xA; &lt;li&gt;The code for &lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/#data-generation-process&#34;&gt;generating the data&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The code for &lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/#fine-tuning&#34;&gt;fine-tuning the model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Demo for the model can be found &lt;a href=&#34;https://code-alpaca-demo.vercel.app/&#34;&gt;https://code-alpaca-demo.vercel.app/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The Code Alpaca models are fine-tuned from a 7B and 13B LLaMA model on 20K instruction-following data generated by the techniques in the Self-Instruct [1] paper, with some modifications that we discuss in the next section. Evals are still a todo.&lt;/p&gt; &#xA;&lt;p&gt;The model is not finetuned to be safe and harmless, so be cautious.&lt;/p&gt; &#xA;&lt;p&gt;Current release contains the data generation procedure, dataset, and training code. Model weights aren&#39;t part of the release for now, to respect OpenAI TOS and LLaMA license.&lt;/p&gt; &#xA;&lt;p&gt;[1]: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. &lt;a href=&#34;https://arxiv.org/abs/2212.10560&#34;&gt;https://arxiv.org/abs/2212.10560&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Data Release&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/data/code_alpaca_20k.json&#34;&gt;&lt;code&gt;data/code_alpaca_20k.json&lt;/code&gt;&lt;/a&gt; contains 20K instruction-following data used for fine-tuning the Code Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;instruction&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, describes the task the model should perform. Each of the 20K instructions is unique.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;input&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, optional context or input for the task. For example, when the instruction is &#34;Amend the following SQL query to select distinct elements&#34;, the input is the SQL query. Around 40% of the examples have an input.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;output&lt;/code&gt;: &lt;code&gt;str&lt;/code&gt;, the answer to the instruction as generated by &lt;code&gt;text-davinci-003&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We used the following prompts for fine-tuning the model:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with a non-empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Input:&#xA;{input}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;for examples with an empty input field:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Below is an instruction that describes a task. Write a response that appropriately completes the request.&#xA;&#xA;### Instruction:&#xA;{instruction}&#xA;&#xA;### Response:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During inference (eg for the web demo), we use the user instruction with an empty input field (second option).&lt;/p&gt; &#xA;&lt;h2&gt;Data Generation Process&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;strong&gt; Running the code &lt;/strong&gt; &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Set environment variables &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; to your OpenAI API key.&lt;/li&gt; &#xA;  &lt;li&gt;Install the dependencies with &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;li&gt;Run &lt;code&gt;python -m generate_instruction generate_instruction_following_data&lt;/code&gt; to generate the data.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; Data generation pipeline had minor changes from [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) - Modified prompt to focus on code generation/editing/optimization tasks instead of general tasks. - Modified seed tasks to only be related to code generation. &#xA;&lt;p&gt;This produced an instruction-following dataset with 20K examples obtained at a much lower cost (less than $200). Also including a smaller 2k samples dataset which was used to derisk the approach and quality of the model.&lt;/p&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;Finetuned the models using standard Hugging Face training code and deepspeed with the following hyperparameters:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hyperparameter&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Learning rate&lt;/td&gt; &#xA;   &lt;td&gt;2e-5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Epochs&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Max length&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Weight decay&lt;/td&gt; &#xA;   &lt;td&gt;0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Given Hugging Face hasn&#39;t officially supported the LLaMA models, we fine-tuned LLaMA with Hugging Face&#39;s transformers library by installing it from a particular fork (i.e. this &lt;a href=&#34;https://github.com/huggingface/transformers/pull/21955&#34;&gt;PR&lt;/a&gt; to be merged). The hash of the specific commit we installed was &lt;code&gt;68d640f7c368bcaaaecfc678f11908ebbd3d6176&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The code runs on a 8xA100 80GB, but can also run on 8xA10040GB or 4xA100 with lower batch size and gradient accumulation steps. To get the GPUs, I suggest using &lt;a href=&#34;https://cloud.lambdalabs.com/login?redirect_to=/instances?&#34;&gt;Lambda Labs&lt;/a&gt;, best pricing for the best hardware.&lt;/p&gt; &#xA;&lt;p&gt;To reproduce the fine-tuning runs for LLaMA, first install the requirements&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install the particular fork of Hugging Face&#39;s transformers library.&lt;/p&gt; &#xA;&lt;p&gt;Below is a command that fine-tunes LLaMA-7B with our dataset on a machine with 4 A100 80G GPUs in FSDP &lt;code&gt;full_shard&lt;/code&gt; mode. We were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using &lt;strong&gt;Python 3.10&lt;/strong&gt;. Replace &lt;code&gt;&amp;lt;your_random_port&amp;gt;&lt;/code&gt; with a port of your own, &lt;code&gt;&amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt;&lt;/code&gt; with the path to your converted checkpoint and tokenizer (following instructions in the PR), and &lt;code&gt;&amp;lt;your_output_dir&amp;gt;&lt;/code&gt; with where you want to store your outputs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=8 --master_port=&amp;lt;your_random_port&amp;gt; train.py \&#xA;    --model_name_or_path &amp;lt;your_path_to_hf_converted_llama_ckpt_and_tokenizer&amp;gt;&#xA;    --data_path ./data/code_alpaca_20k.json \&#xA;    --fp16 True \&#xA;    --output_dir &amp;lt;your_output_dir&amp;gt; \&#xA;    --num_train_epochs 3 \&#xA;    --per_device_train_batch_size 8 \&#xA;    --per_device_eval_batch_size 8 \&#xA;    --gradient_accumulation_steps 4 \&#xA;    --evaluation_strategy &#34;no&#34; \&#xA;    --save_strategy &#34;steps&#34; \&#xA;    --save_steps 500 \&#xA;    --save_total_limit 1 \&#xA;    --learning_rate 2e-5 \&#xA;    --weight_decay 0. \&#xA;    --warmup_ratio 0.03 \&#xA;    --lr_scheduler_type &#34;cosine&#34; \&#xA;    --logging_steps 1 \&#xA;    --deepspeed ds_config.json&#xA;    --tf32 False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note the given training script is meant to be simple and easy to use, and is not particularly optimized.&lt;/p&gt; &#xA;&lt;p&gt;For convenience I have included the &lt;a href=&#34;https://raw.githubusercontent.com/sahil280114/codealpaca/master/convert_to_hf.py&#34;&gt;&lt;code&gt;convert_to_hf.py&lt;/code&gt;&lt;/a&gt; to covnert llama checkpoints to huggingface compatible checkpoints. (This file is taken from the hugginface transformers repo)&lt;/p&gt; &#xA;&lt;h3&gt;Citation&lt;/h3&gt; &#xA;&lt;p&gt;Cite this repo if you want to, or don&#39;t, both are fine.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{codealpaca,&#xA;  author = {Sahil Chaudhary},&#xA;  title = {Code Alpaca: An Instruction-following LLaMA model for code generation},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/sahil280114/codealpaca}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Naturally, you should also cite the original LLaMA paper [1] and the Self-Instruct paper [2] and the &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;Stanford Alpaca repo&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>showlab/Tune-A-Video</title>
    <updated>2023-03-27T01:29:09Z</updated>
    <id>tag:github.com,2023-03-27:/showlab/Tune-A-Video</id>
    <link href="https://github.com/showlab/Tune-A-Video" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Tune-A-Video&lt;/h1&gt; &#xA;&lt;p&gt;This repository is the official implementation of &lt;a href=&#34;https://arxiv.org/abs/2212.11565&#34;&gt;Tune-A-Video&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.11565&#34;&gt;Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation&lt;/a&gt;&lt;/strong&gt; &lt;br&gt; &lt;a href=&#34;https://zhangjiewu.github.io/&#34;&gt;Jay Zhangjie Wu&lt;/a&gt;, &lt;a href=&#34;https://geyixiao.com/&#34;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Stan Weixian Lei&lt;/a&gt;, &lt;a href=&#34;https://ycgu.site/&#34;&gt;Yuchao Gu&lt;/a&gt;, &lt;a href=&#34;&#34;&gt;Yufei Shi&lt;/a&gt;, &lt;a href=&#34;https://www.comp.nus.edu.sg/~whsu/&#34;&gt;Wynne Hsu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=4oXBp9UAAAAJ&amp;amp;hl=en&#34;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?user=mk-F69UAAAAJ&amp;amp;hl=en&#34;&gt;Xiaohu Qie&lt;/a&gt;, &lt;a href=&#34;https://sites.google.com/view/showlab&#34;&gt;Mike Zheng Shou&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tuneavideo.github.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Website-orange&#34; alt=&#34;Project Website&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2212.11565&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2212.11565-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Tune-A-Video-library/Tune-A-Video-Training-UI&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Hugging Face Spaces&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/showlab/Tune-A-Video/blob/main/notebooks/Tune-A-Video.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://tuneavideo.github.io/assets/overview.png&#34; width=&#34;800px&#34;&gt; &lt;br&gt; &lt;em&gt;Given a video-text pair as input, our method, Tune-A-Video, fine-tunes a pre-trained text-to-image diffusion model for text-to-video generation.&lt;/em&gt; &lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[02/22/2023] Improved consistency using DDIM inversion.&lt;/li&gt; &#xA; &lt;li&gt;[02/08/2023] &lt;a href=&#34;https://colab.research.google.com/github/showlab/Tune-A-Video/blob/main/notebooks/Tune-A-Video.ipynb&#34;&gt;Colab demo&lt;/a&gt; released!&lt;/li&gt; &#xA; &lt;li&gt;[02/03/2023] Pre-trained Tune-A-Video models are available on &lt;a href=&#34;https://huggingface.co/Tune-A-Video-library&#34;&gt;Hugging Face Library&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;[01/28/2023] New Feature: tune a video on personalized &lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;DreamBooth&lt;/a&gt; models.&lt;/li&gt; &#xA; &lt;li&gt;[01/28/2023] Code released!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Installing &lt;a href=&#34;https://github.com/facebookresearch/xformers&#34;&gt;xformers&lt;/a&gt; is highly recommended for more efficiency and speed on GPUs. To enable xformers, set &lt;code&gt;enable_xformers_memory_efficient_attention=True&lt;/code&gt; (default).&lt;/p&gt; &#xA;&lt;h3&gt;Weights&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;[Stable Diffusion]&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34;&gt;Stable Diffusion&lt;/a&gt; is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The pre-trained Stable Diffusion models can be downloaded from Hugging Face (e.g., &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;Stable Diffusion v1-4&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-1&#34;&gt;v2-1&lt;/a&gt;). You can also use fine-tuned Stable Diffusion models trained on different styles (e.g, &lt;a href=&#34;https://huggingface.co/nitrosocke/mo-di-diffusion&#34;&gt;Modern Disney&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/nitrosocke/redshift-diffusion&#34;&gt;Redshift&lt;/a&gt;, etc.).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[DreamBooth]&lt;/strong&gt; &lt;a href=&#34;https://dreambooth.github.io/&#34;&gt;DreamBooth&lt;/a&gt; is a method to personalize text-to-image models like Stable Diffusion given just a few images (3~5 images) of a subject. Tuning a video on DreamBooth models allows personalized text-to-video generation of a specific subject. There are some public DreamBooth models available on &lt;a href=&#34;https://huggingface.co/sd-dreambooth-library&#34;&gt;Hugging Face&lt;/a&gt; (e.g., &lt;a href=&#34;https://huggingface.co/sd-dreambooth-library/mr-potato-head&#34;&gt;mr-potato-head&lt;/a&gt;). You can also train your own DreamBooth model following &lt;a href=&#34;https://github.com/huggingface/diffusers/tree/main/examples/dreambooth&#34;&gt;this training example&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;To fine-tune the text-to-image diffusion models for text-to-video generation, run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch train_tuneavideo.py --config=&#34;configs/man-skiing.yaml&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Tuning a 24-frame video usually takes &lt;code&gt;300~500&lt;/code&gt; steps, about &lt;code&gt;10~15&lt;/code&gt; minutes using one A100 GPU. Reduce &lt;code&gt;n_sample_frames&lt;/code&gt; if your GPU memory is limited.&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Once the training is done, run inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tuneavideo.pipelines.pipeline_tuneavideo import TuneAVideoPipeline&#xA;from tuneavideo.models.unet import UNet3DConditionModel&#xA;from tuneavideo.util import save_videos_grid&#xA;import torch&#xA;&#xA;pretrained_model_path = &#34;./checkpoints/stable-diffusion-v1-4&#34;&#xA;my_model_path = &#34;./outputs/man-skiing&#34;&#xA;unet = UNet3DConditionModel.from_pretrained(my_model_path, subfolder=&#39;unet&#39;, torch_dtype=torch.float16).to(&#39;cuda&#39;)&#xA;pipe = TuneAVideoPipeline.from_pretrained(pretrained_model_path, unet=unet, torch_dtype=torch.float16).to(&#34;cuda&#34;)&#xA;pipe.enable_xformers_memory_efficient_attention()&#xA;pipe.enable_vae_slicing()&#xA;&#xA;prompt = &#34;spider man is skiing&#34;&#xA;ddim_inv_latent = torch.load(f&#34;{my_model_path}/inv_latents/ddim_latent-500.pt&#34;).to(torch.float16)&#xA;video = pipe(prompt, latents=ddim_inv_latent, video_length=24, height=512, width=512, num_inference_steps=50, guidance_scale=12.5).videos&#xA;&#xA;save_videos_grid(video, f&#34;./{prompt}.gif&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;h3&gt;Pretrained T2I (Stable Diffusion)&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34;&gt;&lt;b&gt;Input Video&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34; colspan=&#34;3&#34;&gt;&lt;b&gt;Output Video&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/data/man-skiing.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-skiing/spiderman-beach.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-skiing/wonder-woman.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-skiing/pink-sunset.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;A man is skiing&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Spider Man is skiing on the beach, cartoon style”&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Wonder Woman, wearing a cowboy hat, is skiing&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A man, wearing pink clothes, is skiing at sunset&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/data/rabbit-watermelon.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/rabbit-watermelon/rabbit.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/rabbit-watermelon/cat.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/rabbit-watermelon/puppy.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;A rabbit is eating a watermelon&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A rabbit is &lt;del&gt;eating a watermelon&lt;/del&gt; on the table&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A cat with sunglasses is eating a watermelon on the beach&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A puppy is eating a cheeseburger on the table, comic style&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/data/car-turn.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/car-turn/porsche-beach.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/car-turn/car-cartoon.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/car-turn/car-snow.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;A jeep car is moving on the road&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A Porsche car is moving on the beach&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A car is moving on the road, cartoon style&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A car is moving on the snow&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/data/man-basketball.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-basketball/trump.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-basketball/astronaut.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/man-basketball/lego.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;A man is dribbling a basketball&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Trump is dribbling a basketball&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;An astronaut is dribbling a basketball, cartoon style&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A lego man in a black suit is dribbling a basketball&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;!-- &lt;tr&gt;&#xA;  &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/data/lion-roaring.gif&#34;&gt;&lt;/td&gt;&#xA;  &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/lion-roaring/tiger-roar.gif&#34;&gt;&lt;/td&gt;&#xA;  &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/lion-roaring/lion-vangogh.gif&#34;&gt;&lt;/td&gt;              &#xA;  &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/lion-roaring/wolf-nyc.gif&#34;&gt;&lt;/td&gt;&#xA;&lt;/tr&gt;&#xA;&lt;tr&gt;&#xA;  &lt;td width=25% style=&#34;text-align:center;color:gray;&#34;&gt;&#34;A lion is roaring&#34;&lt;/td&gt;&#xA;  &lt;td width=25% style=&#34;text-align:center;&#34;&gt;&#34;A tiger is roaring&#34;&lt;/td&gt;&#xA;  &lt;td width=25% style=&#34;text-align:center;&#34;&gt;&#34;A lion is roaring, Van Gogh style&#34;&lt;/td&gt;&#xA;  &lt;td width=25% style=&#34;text-align:center;&#34;&gt;&#34;A wolf is roaring in New York City&#34;&lt;/td&gt;&#xA;&lt;/tr&gt; --&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Pretrained T2I (personalized DreamBooth)&lt;/h3&gt; &#xA;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/modern-disney/modern-disney.png&#34; width=&#34;240px&#34;&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34;&gt;&lt;b&gt;Input Video&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34; colspan=&#34;3&#34;&gt;&lt;b&gt;Output Video&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/data/bear-guitar.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/modern-disney/bear-guitar/rabbit.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/modern-disney/bear-guitar/prince.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/modern-disney/bear-guitar/princess.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;A bear is playing guitar&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A rabbit is playing guitar, modern disney style&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A handsome prince is playing guitar, modern disney style&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;A magic princess with sunglasses is playing guitar on the stage, modern disney style&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/mr-potato-head/mr-potato-head.png&#34; width=&#34;240px&#34;&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34;&gt;&lt;b&gt;Input Video&lt;/b&gt;&lt;/td&gt; &#xA;   &lt;td style=&#34;text-align:center;&#34; colspan=&#34;3&#34;&gt;&lt;b&gt;Output Video&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/data/bear-guitar.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/mr-potato-head/bear-guitar/lego-snow.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/mr-potato-head/bear-guitar/sunglasses-beach.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://tuneavideo.github.io/assets/results/tuneavideo/mr-potato-head/bear-guitar/van-gogh.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;color:gray;&#34;&gt;&#34;A bear is playing guitar&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Mr Potato Head, made of lego, is playing guitar on the snow&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Mr Potato Head, wearing sunglasses, is playing guitar on the beach&#34;&lt;/td&gt; &#xA;   &lt;td width=&#34;25%&#34; style=&#34;text-align:center;&#34;&gt;&#34;Mr Potato Head is playing guitar in the starry night, Van Gogh style&#34;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you make use of our work, please cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{wu2022tuneavideo,&#xA;    title={Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation},&#xA;    author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},&#xA;    journal={arXiv preprint arXiv:2212.11565},&#xA;    year={2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Shoutouts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This code builds on &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;. Thanks for open-sourcing!&lt;/li&gt; &#xA; &lt;li&gt;Thanks &lt;a href=&#34;https://github.com/hysts&#34;&gt;hysts&lt;/a&gt; for the awesome &lt;a href=&#34;https://huggingface.co/spaces/Tune-A-Video-library/Tune-A-Video-Training-UI&#34;&gt;gradio demo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>