<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-18T01:28:11Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>OpenBMB/MiniCPM-o</title>
    <updated>2025-01-18T01:28:11Z</updated>
    <id>tag:github.com,2025-01-18:/OpenBMB/MiniCPM-o</id>
    <link href="https://github.com/OpenBMB/MiniCPM-o" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech and Multimodal Live Streaming on Your Phone&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/MiniCPM-o.png&#34; width=&#34;300em&#34;&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;A GPT-4o Level MLLM for Vision, Speech and Multimodal Live Streaming on Your Phone&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/README_zh.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; | English&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;span style=&#34;display: inline-flex; align-items: center; margin-right: 2px;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/wechat.png&#34; alt=&#34;WeChat&#34; style=&#34;margin-right: 4px;&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/docs/wechat.md&#34; target=&#34;_blank&#34;&gt; WeChat&lt;/a&gt; &amp;nbsp;| &lt;/span&gt; &amp;nbsp; &#xA; &lt;span style=&#34;display: inline-flex; align-items: center; margin-left: -8px;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/discord.png&#34; alt=&#34;Discord&#34; style=&#34;margin-right: 4px;&#34;&gt; &lt;a href=&#34;https://discord.gg/uQpn8kKx&#34; target=&#34;_blank&#34;&gt; Discord&lt;/a&gt; &lt;/span&gt; &#xA; &lt;p align=&#34;center&#34;&gt; MiniCPM-o 2.6 &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6&#34;&gt;ü§ó&lt;/a&gt; &lt;a href=&#34;https://minicpm-omni-webdemo-us.modelbest.cn/&#34;&gt; ü§ñ&lt;/a&gt; | MiniCPM-V 2.6 &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2_6&#34;&gt;ü§ó&lt;/a&gt; &lt;a href=&#34;http://120.92.209.146:8887/&#34;&gt;ü§ñ&lt;/a&gt; | Technical Blog Coming Soon &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-o&lt;/strong&gt; is the latest series of end-side multimodal LLMs (MLLMs) ungraded from MiniCPM-V. The models can now take image, video, text, and audio as inputs and provide high-quality text and speech outputs in an end-to-end fashion. Since February 2024, we have released 6 versions of the model, aiming to achieve &lt;strong&gt;strong performance and efficient deployment&lt;/strong&gt;. The most notable models in the series currently include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt;: üî•üî•üî• The latest and most capable model in the MiniCPM-o series. With a total of 8B parameters, this end-to-end model &lt;strong&gt;achieves comparable performance to GPT-4o-202405 in vision, speech, and multimodal live streaming&lt;/strong&gt;, making it one of the most versatile and performant models in the open-source community. For the new voice mode, MiniCPM-o 2.6 &lt;strong&gt;supports bilingual real-time speech conversation with configurable voices&lt;/strong&gt;, and also allows for fun capabilities such as emotion/speed/style control, end-to-end voice cloning, role play, etc. It also advances MiniCPM-V 2.6&#39;s visual capabilities such &lt;strong&gt;strong OCR capability, trustworthy behavior, multilingual support, and video understanding&lt;/strong&gt;. Due to its superior token density, MiniCPM-o 2.6 can for the first time &lt;strong&gt;support multimodal live streaming on end-side devices&lt;/strong&gt; such as iPad.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;MiniCPM-V 2.6&lt;/strong&gt;: The most capable model in the MiniCPM-V series. With a total of 8B parameters, the model &lt;strong&gt;surpasses GPT-4V in single image, multi-image and video understanding&lt;/strong&gt;. It outperforms &lt;strong&gt;GPT-4o mini, Gemini 1.5 Pro and Claude 3.5 Sonnet&lt;/strong&gt; in single image understanding, and can for the first time support real-time video understanding on iPad.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;üìå Pinned&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[2025.01.17] We have updated the usage of MiniCPM-o 2.6 int4 quantization version, and resolved the model initialization error. Click &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6-int4&#34;&gt;here&lt;/a&gt; and try it now!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2025.01.16] ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è MiniCPM-o tops GitHub Trending and reaches top-3 on Hugging Face Trending!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2025.01.13] üî•üî•üî• We open-source MiniCPM-o 2.6, which matches GPT-4o-202405 on vision, speech and multimodal live streaming. It advances popular capabitlies of MiniCPM-V 2.6, and supports various new fun features. Try it now!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024.08.17] üöÄüöÄüöÄ MiniCPM-V 2.6 is now fully supported by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024.08.06] üî•üî•üî• We open-source MiniCPM-V 2.6, which outperforms GPT-4V on single image, multi-image and video understanding. It advances popular features of MiniCPM-Llama3-V 2.5, and can support real-time video understanding on iPad. Try it now!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024.08.03] MiniCPM-Llama3-V 2.5 technical report is released! See &lt;a href=&#34;https://arxiv.org/abs/2408.01800&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[2024.05.23] üî•üî•üî• MiniCPM-V tops GitHub Trending and Hugging Face Trending! Our demo, recommended by Hugging Face Gradio‚Äôs official account, is available &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5&#34;&gt;here&lt;/a&gt;. Come and try it out!&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view more news.&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.08.15] We now also support multi-image SFT. For more details, please refer to the &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune&#34;&gt;document&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.08.14] MiniCPM-V 2.6 now also supports &lt;a href=&#34;https://github.com/modelscope/ms-swift/issues/1613&#34;&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.08.10] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 is now fully supported by &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;official&lt;/a&gt; llama.cpp! GGUF models of various sizes are available &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.07.19] MiniCPM-Llama3-V 2.5 supports vLLM now! See &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#inference-with-vllm&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.06.03] Now, you can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs(12 GB or 16 GB) by distributing the model&#39;s layers across multiple GPUs. For more details, Check this &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md&#34;&gt;link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.05.28] üöÄüöÄüöÄ MiniCPM-Llama3-V 2.5 now fully supports its feature in llama.cpp and ollama! Please pull the latest code &lt;strong&gt;of our provided forks&lt;/strong&gt; (&lt;a href=&#34;https://github.com/OpenBMB/llama.cpp/raw/minicpm-v2.5/examples/minicpmv/README.md&#34;&gt;llama.cpp&lt;/a&gt;, &lt;a href=&#34;https://github.com/OpenBMB/ollama/tree/minicpm-v2.5/examples/minicpm-v2.5&#34;&gt;ollama&lt;/a&gt;). GGUF models in various sizes are available &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf/tree/main&#34;&gt;here&lt;/a&gt;. MiniCPM-Llama3-V 2.5 series is &lt;strong&gt;not supported by the official repositories yet&lt;/strong&gt;, and we are working hard to merge PRs. Please stay tuned!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.05.28] üí´ We now support LoRA fine-tuning for MiniCPM-Llama3-V 2.5, using only 2 V100 GPUs! See more statistics &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V/tree/main/finetune#model-fine-tuning-memory-usage-statistics&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.05.25] MiniCPM-Llama3-V 2.5 now supports streaming outputs and customized system prompts. Try it &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5#usage&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.05.24] We release the MiniCPM-Llama3-V 2.5 &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-Llama3-V-2_5-gguf&#34;&gt;gguf&lt;/a&gt;, which supports &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#inference-with-llamacpp&#34;&gt;llama.cpp&lt;/a&gt; inference and provides a 6~8 token/s smooth decoding on mobile phones. Try it now!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.05.23] üîç We&#39;ve released a comprehensive comparison between Phi-3-vision-128k-instruct and MiniCPM-Llama3-V 2.5, including benchmarks evaluations, multilingual capabilities, and inference efficiency üåüüìäüåçüöÄ. Click &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/docs/compare_with_phi-3_vision.md&#34;&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.05.20] We open-soure MiniCPM-Llama3-V 2.5, it has improved OCR capability and supports 30+ languages, representing the first end-side MLLM achieving GPT-4V level performance! We provide &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#deployment-on-mobile-phone&#34;&gt;efficient inference&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/finetune/readme.md&#34;&gt;simple fine-tuning&lt;/a&gt;. Try it now!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.04.23] MiniCPM-V-2.0 supports vLLM now! Click &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#inference-with-vllm&#34;&gt;here&lt;/a&gt; to view more details.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.04.18] We create a HuggingFace Space to host the demo of MiniCPM-V 2.0 at &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-V-2&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.04.17] MiniCPM-V-2.0 supports deploying &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#webui-demo&#34;&gt;WebUI Demo&lt;/a&gt; now!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.04.15] MiniCPM-V-2.0 now also supports &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;fine-tuning&lt;/a&gt; with the SWIFT framework!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.04.12] We open-source MiniCPM-V 2.0, which achieves comparable performance with Gemini Pro in understanding scene text and outperforms strong Qwen-VL-Chat 9.6B and Yi-VL 34B on &lt;a href=&#34;https://rank.opencompass.org.cn/leaderboard-multimodal&#34;&gt;OpenCompass&lt;/a&gt;, a comprehensive evaluation over 11 popular benchmarks. Click &lt;a href=&#34;https://openbmb.vercel.app/minicpm-v-2&#34;&gt;here&lt;/a&gt; to view the MiniCPM-V 2.0 technical blog.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.03.14] MiniCPM-V now supports &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;fine-tuning&lt;/a&gt; with the SWIFT framework. Thanks to &lt;a href=&#34;https://github.com/Jintao-Huang&#34;&gt;Jintao&lt;/a&gt; for the contributionÔºÅ&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.03.01] MiniCPM-V now can be deployed on Mac!&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;[2024.02.01] We open-source MiniCPM-V and OmniLMM-12B, which support efficient end-side deployment and powerful multimodal capabilities correspondingly.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contents &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#minicpm-o-26&#34;&gt;MiniCPM-o 2.6&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#minicpm-v-26&#34;&gt;MiniCPM-V 2.6&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#chat-with-our-demo-on-gradio-&#34;&gt;Chat with Our Demo on Gradio ü§ó&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#inference&#34;&gt;Inference&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#model-zoo&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#multi-turn-conversation&#34;&gt;Multi-turn Conversation&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#chat-with-multiple-images&#34;&gt;Chat with Multiple Images&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#in-context-few-shot-learning&#34;&gt;In-context Few-shot Learning&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#chat-with-video&#34;&gt;Chat with Video&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#speech-conversation&#34;&gt;Speech Conversation&lt;/a&gt; &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#mimick&#34;&gt;Mimick&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#general-speech-conversation-with-configurable-voices&#34;&gt;General Speech Conversation with Configurable Voices&lt;/a&gt;&lt;/li&gt; &#xA;       &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#addressing-various-audio-tasks&#34;&gt;Addressing Various Audio Tasks&lt;/a&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#multimodal-live-streaming&#34;&gt;Multimodal Live Streaming&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#inference-on-multiple-gpus&#34;&gt;Inference on Multiple GPUs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#inference-on-mac&#34;&gt;Inference on Mac&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#deployment-on-mobile-phone&#34;&gt;Deployment on Mobile Phone&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#efficient-inference-with-llamacpp-ollama-vllm&#34;&gt;Efficient Inference with llama.cpp, ollama, vLLM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#faqs&#34;&gt;FAQs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#limitations&#34;&gt;Limitations&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;MiniCPM-o 2.6&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;MiniCPM-o 2.6&lt;/strong&gt; is the latest and most capable model in the MiniCPM-o series. The model is built in an end-to-end fashion based on SigLip-400M, Whisper-medium-300M, ChatTTS-200M, and Qwen2.5-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-V 2.6, and introduces new features for real-time speech conversation and multimodal live streaming. Notable features of MiniCPM-o 2.6 include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üî• &lt;strong&gt;Leading Visual Capability.&lt;/strong&gt; MiniCPM-o 2.6 achieves an average score of 70.2 on OpenCompass, a comprehensive evaluation over 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-202405, Gemini 1.5 Pro, and Claude 3.5 Sonnet&lt;/strong&gt; for single image understanding. It also &lt;strong&gt;outperforms GPT-4V and Claude 3.5 Sonnet&lt;/strong&gt; in mutli-image and video understanding, and shows promising in-context learning capability.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üéô &lt;strong&gt;State-of-the-art Speech Capability.&lt;/strong&gt; MiniCPM-o 2.6 supports &lt;strong&gt;bilingual real-time speech conversation with configurable voices&lt;/strong&gt; in English and Chinese. It &lt;strong&gt;outperforms GPT-4o-realtime on audio understanding tasks&lt;/strong&gt; such as ASR and STT translation, and shows &lt;strong&gt;state-of-the-art performance on speech conversation in both semantic and acoustic evaluations in the open-source community&lt;/strong&gt;. It also allows for fun features such as emotion/speed/style control, end-to-end voice cloning, role play, etc.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üé¨ &lt;strong&gt;Strong Multimodal Live Streaming Capability.&lt;/strong&gt; As a new feature, MiniCPM-o 2.6 can &lt;strong&gt;accept continous video and audio streams independent of user queries, and support real-time speech interaction&lt;/strong&gt;. It &lt;strong&gt;outperforms GPT-4o-202408 and Claude 3.5 Sonnet and shows state-of-art performance in open-source community on StreamingBench&lt;/strong&gt;, a comprehensive benchmark for real-time video understanding, omni-source (video &amp;amp; audio) understanding, and multimodal contextual understanding.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üí™ &lt;strong&gt;Strong OCR Capability and Others.&lt;/strong&gt; Advancing popular visual capabilites from MiniCPM-V series, MiniCPM-o 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves &lt;strong&gt;state-of-the-art performance on OCRBench for models under 25B, surpassing proprietary models such as GPT-4o-202405&lt;/strong&gt;. Based on the the latest &lt;a href=&#34;https://github.com/RLHF-V/RLAIF-V/&#34;&gt;RLAIF-V&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenBMB/VisCPM&#34;&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, outperforming GPT-4o and Claude 3.5 Sonnet on MMHal-Bench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; on more than 30 languages.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Superior Efficiency.&lt;/strong&gt; In addition to its friendly size, MiniCPM-o 2.6 also shows &lt;strong&gt;state-of-the-art token density&lt;/strong&gt; (i.e., number of pixels encoded into each visual token). &lt;strong&gt;It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models&lt;/strong&gt;. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-o 2.6 can efficiently support &lt;strong&gt;multimodal live streaming&lt;/strong&gt; on end-side devices such as iPad.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üí´ &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-o 2.6 can be easily used in various ways: (1) &lt;a href=&#34;https://github.com/OpenBMB/llama.cpp/raw/minicpm-omni/examples/llava/README-minicpmo2.6.md&#34;&gt;llama.cpp&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6-int4&#34;&gt;int4&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf&#34;&gt;GGUF&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#efficient-inference-with-llamacpp-ollama-vllm&#34;&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/docs/llamafactory_train_and_infer.md&#34;&gt;LLaMA-Factory&lt;/a&gt;, (5) quick &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#chat-with-our-demo-on-gradio&#34;&gt;local WebUI demo&lt;/a&gt;, and (6) online web demo on &lt;a href=&#34;https://minicpm-omni-webdemo-us.modelbest.cn/&#34;&gt;server&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Model Architecture.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;End-to-end Omni-modal Architecture.&lt;/strong&gt; Different modality encoder/decoders are connected and trained in an &lt;strong&gt;end-to-end&lt;/strong&gt; fashion to fully exploit rich multimodal knowledge. The model is trained in a fully end-to-end manner with only CE loss.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Omni-modal Live Streaming Mechanism.&lt;/strong&gt; (1) We change the offline modality encoder/decoders into online ones for &lt;strong&gt;streaming inputs/outputs.&lt;/strong&gt; (2) We devise a &lt;strong&gt;time-division multiplexing (TDM) mechanism&lt;/strong&gt; for omni-modality streaming processing in the LLM backbone. It divides parallel omni-modality streams into sequential info within small periodic time slices.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configurable Speech Modeling Design.&lt;/strong&gt; We devise a multimodal system prompt, including traditional text system prompt, and &lt;strong&gt;a new audio system prompt to determine the assistant voice&lt;/strong&gt;. This enables flexible voice configurations in inference time, and also facilitates end-to-end voice cloning and description-based voice creation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpm-o-26-framework-v2.png&#34; , width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Evaluation &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/radar.jpg&#34; , width=&#34;80%&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view visual understanding results.&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Image Understanding&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;Size&lt;/th&gt; &#xA;     &lt;th&gt;Token Density&lt;sup&gt;+&lt;/sup&gt;&lt;/th&gt; &#xA;     &lt;th&gt;OpenCompass&lt;/th&gt; &#xA;     &lt;th&gt;OCRBench&lt;/th&gt; &#xA;     &lt;th&gt;MathVista mini&lt;/th&gt; &#xA;     &lt;th&gt;ChartQA&lt;/th&gt; &#xA;     &lt;th&gt;MMVet&lt;/th&gt; &#xA;     &lt;th&gt;MMStar&lt;/th&gt; &#xA;     &lt;th&gt;MME&lt;/th&gt; &#xA;     &lt;th&gt;MMB1.1 test&lt;/th&gt; &#xA;     &lt;th&gt;AI2D&lt;/th&gt; &#xA;     &lt;th&gt;MMMU val&lt;/th&gt; &#xA;     &lt;th&gt;HallusionBench&lt;/th&gt; &#xA;     &lt;th&gt;TextVQA val&lt;/th&gt; &#xA;     &lt;th&gt;DocVQA test&lt;/th&gt; &#xA;     &lt;th&gt;MathVerse mini&lt;/th&gt; &#xA;     &lt;th&gt;MathVision&lt;/th&gt; &#xA;     &lt;th&gt;MMHal Score&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody align=&#34;center&#34;&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;19&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT-4o-20240513&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1088&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;69.9&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;736&lt;/td&gt; &#xA;     &lt;td&gt;61.3&lt;/td&gt; &#xA;     &lt;td&gt;85.7&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;69.1&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;63.9&lt;/td&gt; &#xA;     &lt;td&gt;2328.7&lt;/td&gt; &#xA;     &lt;td&gt;82.2&lt;/td&gt; &#xA;     &lt;td&gt;84.6&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;69.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;55.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;92.8&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;50.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;30.4&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;3.6&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Claude3.5-Sonnet&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;750&lt;/td&gt; &#xA;     &lt;td&gt;67.9&lt;/td&gt; &#xA;     &lt;td&gt;788&lt;/td&gt; &#xA;     &lt;td&gt;61.6&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;90.8&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;66.0&lt;/td&gt; &#xA;     &lt;td&gt;62.2&lt;/td&gt; &#xA;     &lt;td&gt;1920.0&lt;/td&gt; &#xA;     &lt;td&gt;78.5&lt;/td&gt; &#xA;     &lt;td&gt;80.2&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;65.9&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;49.9&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;95.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;3.4&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Gemini 1.5 Pro&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;64.4&lt;/td&gt; &#xA;     &lt;td&gt;754&lt;/td&gt; &#xA;     &lt;td&gt;57.7&lt;/td&gt; &#xA;     &lt;td&gt;81.3&lt;/td&gt; &#xA;     &lt;td&gt;64.0&lt;/td&gt; &#xA;     &lt;td&gt;59.1&lt;/td&gt; &#xA;     &lt;td&gt;2110.6&lt;/td&gt; &#xA;     &lt;td&gt;73.9&lt;/td&gt; &#xA;     &lt;td&gt;79.1&lt;/td&gt; &#xA;     &lt;td&gt;60.6&lt;/td&gt; &#xA;     &lt;td&gt;45.6&lt;/td&gt; &#xA;     &lt;td&gt;73.5&lt;/td&gt; &#xA;     &lt;td&gt;86.5&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;19.2&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT-4o-mini-20240718&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;1088&lt;/td&gt; &#xA;     &lt;td&gt;64.1&lt;/td&gt; &#xA;     &lt;td&gt;785&lt;/td&gt; &#xA;     &lt;td&gt;52.4&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;66.9&lt;/td&gt; &#xA;     &lt;td&gt;54.8&lt;/td&gt; &#xA;     &lt;td&gt;2003.4&lt;/td&gt; &#xA;     &lt;td&gt;76.0&lt;/td&gt; &#xA;     &lt;td&gt;77.8&lt;/td&gt; &#xA;     &lt;td&gt;60.0&lt;/td&gt; &#xA;     &lt;td&gt;46.1&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;3.3&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;19&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open Source&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Cambrian-34B&lt;/td&gt; &#xA;     &lt;td&gt;34B&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;1820&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;58.3&lt;/td&gt; &#xA;     &lt;td&gt;591&lt;/td&gt; &#xA;     &lt;td&gt;50.3&lt;/td&gt; &#xA;     &lt;td&gt;75.6&lt;/td&gt; &#xA;     &lt;td&gt;53.2&lt;/td&gt; &#xA;     &lt;td&gt;54.2&lt;/td&gt; &#xA;     &lt;td&gt;2049.9&lt;/td&gt; &#xA;     &lt;td&gt;77.8&lt;/td&gt; &#xA;     &lt;td&gt;79.5&lt;/td&gt; &#xA;     &lt;td&gt;50.4&lt;/td&gt; &#xA;     &lt;td&gt;41.6&lt;/td&gt; &#xA;     &lt;td&gt;76.7&lt;/td&gt; &#xA;     &lt;td&gt;75.5&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GLM-4V-9B&lt;/td&gt; &#xA;     &lt;td&gt;13B&lt;/td&gt; &#xA;     &lt;td&gt;784&lt;/td&gt; &#xA;     &lt;td&gt;59.1&lt;/td&gt; &#xA;     &lt;td&gt;776&lt;/td&gt; &#xA;     &lt;td&gt;51.1&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;58.0&lt;/td&gt; &#xA;     &lt;td&gt;54.8&lt;/td&gt; &#xA;     &lt;td&gt;2018.8&lt;/td&gt; &#xA;     &lt;td&gt;67.9&lt;/td&gt; &#xA;     &lt;td&gt;71.2&lt;/td&gt; &#xA;     &lt;td&gt;46.9&lt;/td&gt; &#xA;     &lt;td&gt;45.0&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Pixtral-12B&lt;/td&gt; &#xA;     &lt;td&gt;12B&lt;/td&gt; &#xA;     &lt;td&gt;256&lt;/td&gt; &#xA;     &lt;td&gt;61.0&lt;/td&gt; &#xA;     &lt;td&gt;685&lt;/td&gt; &#xA;     &lt;td&gt;56.9&lt;/td&gt; &#xA;     &lt;td&gt;81.8&lt;/td&gt; &#xA;     &lt;td&gt;58.5&lt;/td&gt; &#xA;     &lt;td&gt;54.5&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;72.7&lt;/td&gt; &#xA;     &lt;td&gt;79.0&lt;/td&gt; &#xA;     &lt;td&gt;51.1&lt;/td&gt; &#xA;     &lt;td&gt;47.0&lt;/td&gt; &#xA;     &lt;td&gt;75.7&lt;/td&gt; &#xA;     &lt;td&gt;90.7&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;DeepSeek-VL2-27B (4B)&lt;/td&gt; &#xA;     &lt;td&gt;27B&lt;/td&gt; &#xA;     &lt;td&gt;672&lt;/td&gt; &#xA;     &lt;td&gt;66.4&lt;/td&gt; &#xA;     &lt;td&gt;809&lt;/td&gt; &#xA;     &lt;td&gt;63.9&lt;/td&gt; &#xA;     &lt;td&gt;86.0&lt;/td&gt; &#xA;     &lt;td&gt;60.0&lt;/td&gt; &#xA;     &lt;td&gt;61.9&lt;/td&gt; &#xA;     &lt;td&gt;2253.0&lt;/td&gt; &#xA;     &lt;td&gt;81.2&lt;/td&gt; &#xA;     &lt;td&gt;83.8&lt;/td&gt; &#xA;     &lt;td&gt;54.0&lt;/td&gt; &#xA;     &lt;td&gt;45.3&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;84.2&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;93.3&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;3.0&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Qwen2-VL-7B&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;784&lt;/td&gt; &#xA;     &lt;td&gt;67.1&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;866&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;58.2&lt;/td&gt; &#xA;     &lt;td&gt;83.0&lt;/td&gt; &#xA;     &lt;td&gt;62.0&lt;/td&gt; &#xA;     &lt;td&gt;60.7&lt;/td&gt; &#xA;     &lt;td&gt;2326.0&lt;/td&gt; &#xA;     &lt;td&gt;81.8&lt;/td&gt; &#xA;     &lt;td&gt;83.0&lt;/td&gt; &#xA;     &lt;td&gt;54.1&lt;/td&gt; &#xA;     &lt;td&gt;50.6&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;84.3&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;94.5&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;31.9&lt;/td&gt; &#xA;     &lt;td&gt;16.3&lt;/td&gt; &#xA;     &lt;td&gt;3.2&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-OneVision-72B&lt;/td&gt; &#xA;     &lt;td&gt;72B&lt;/td&gt; &#xA;     &lt;td&gt;182&lt;/td&gt; &#xA;     &lt;td&gt;68.1&lt;/td&gt; &#xA;     &lt;td&gt;741&lt;/td&gt; &#xA;     &lt;td&gt;67.5&lt;/td&gt; &#xA;     &lt;td&gt;83.7&lt;/td&gt; &#xA;     &lt;td&gt;60.6&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;65.8&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;2261.0&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;85.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;85.6&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;56.8&lt;/td&gt; &#xA;     &lt;td&gt;49.0&lt;/td&gt; &#xA;     &lt;td&gt;80.5&lt;/td&gt; &#xA;     &lt;td&gt;91.3&lt;/td&gt; &#xA;     &lt;td&gt;39.1&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;3.5&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;InternVL2.5-8B&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;706&lt;/td&gt; &#xA;     &lt;td&gt;68.3&lt;/td&gt; &#xA;     &lt;td&gt;822&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;64.4&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;84.8&lt;/td&gt; &#xA;     &lt;td&gt;62.8&lt;/td&gt; &#xA;     &lt;td&gt;62.8&lt;/td&gt; &#xA;     &lt;td&gt;2344.0&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;83.6&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;84.5&lt;/td&gt; &#xA;     &lt;td&gt;56.0&lt;/td&gt; &#xA;     &lt;td&gt;50.1&lt;/td&gt; &#xA;     &lt;td&gt;79.1&lt;/td&gt; &#xA;     &lt;td&gt;93.0&lt;/td&gt; &#xA;     &lt;td&gt;39.5&lt;/td&gt; &#xA;     &lt;td&gt;19.7&lt;/td&gt; &#xA;     &lt;td&gt;3.4&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V 2.6&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;65.2&lt;/td&gt; &#xA;     &lt;td&gt;852*&lt;/td&gt; &#xA;     &lt;td&gt;60.6&lt;/td&gt; &#xA;     &lt;td&gt;79.4&lt;/td&gt; &#xA;     &lt;td&gt;60.0&lt;/td&gt; &#xA;     &lt;td&gt;57.5&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;2348.4*&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;78.0&lt;/td&gt; &#xA;     &lt;td&gt;82.1&lt;/td&gt; &#xA;     &lt;td&gt;49.8*&lt;/td&gt; &#xA;     &lt;td&gt;48.1*&lt;/td&gt; &#xA;     &lt;td&gt;80.1&lt;/td&gt; &#xA;     &lt;td&gt;90.8&lt;/td&gt; &#xA;     &lt;td&gt;25.7&lt;/td&gt; &#xA;     &lt;td&gt;18.3&lt;/td&gt; &#xA;     &lt;td&gt;3.6&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-o 2.6&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;70.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;897*&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;71.9*&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;86.9*&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;67.5&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;64.0&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;2372.0*&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;80.5&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;85.8&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;50.4*&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;51.9&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;82.0&lt;/td&gt; &#xA;     &lt;td&gt;93.5&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;41.4*&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;23.1*&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;3.8&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt; * We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set. &#xA; &lt;p&gt;&lt;sup&gt;+&lt;/sup&gt; Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.&lt;/p&gt; &#xA; &lt;p&gt;Note: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Multi-image and Video Understanding&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;     &lt;th&gt;Size&lt;/th&gt; &#xA;     &lt;th&gt;BLINK val&lt;/th&gt; &#xA;     &lt;th&gt;Mantis Eval&lt;/th&gt; &#xA;     &lt;th&gt;MIRB&lt;/th&gt; &#xA;     &lt;th&gt;Video-MME (wo / w subs)&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody align=&#34;center&#34;&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;6&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT-4o-20240513&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;68.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;71.9/77.2&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT4V&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;54.6&lt;/td&gt; &#xA;     &lt;td&gt;62.7&lt;/td&gt; &#xA;     &lt;td&gt;53.1&lt;/td&gt; &#xA;     &lt;td&gt;59.9/63.3&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;6&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-NeXT-Interleave 14B&lt;/td&gt; &#xA;     &lt;td&gt;14B&lt;/td&gt; &#xA;     &lt;td&gt;52.6&lt;/td&gt; &#xA;     &lt;td&gt;66.4&lt;/td&gt; &#xA;     &lt;td&gt;30.2&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-OneVision-72B&lt;/td&gt; &#xA;     &lt;td&gt;72B&lt;/td&gt; &#xA;     &lt;td&gt;55.4&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;77.6&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;66.2/69.5&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MANTIS 8B&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;49.1&lt;/td&gt; &#xA;     &lt;td&gt;59.5&lt;/td&gt; &#xA;     &lt;td&gt;34.8&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Qwen2-VL-7B&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;53.2&lt;/td&gt; &#xA;     &lt;td&gt;69.6*&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;67.6*&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;63.3/69.0&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;InternVL2.5-8B&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;54.8&lt;/td&gt; &#xA;     &lt;td&gt;67.7&lt;/td&gt; &#xA;     &lt;td&gt;52.5&lt;/td&gt; &#xA;     &lt;td&gt;64.2/66.9&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V 2.6&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;53.0&lt;/td&gt; &#xA;     &lt;td&gt;69.1&lt;/td&gt; &#xA;     &lt;td&gt;53.8&lt;/td&gt; &#xA;     &lt;td&gt;60.9/63.6&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-o 2.6&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;56.7&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;71.9&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;58.6&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;63.9/67.9&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt; * We evaluate officially released checkpoints by ourselves. &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view audio understanding and speech conversation results.&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Audio Understanding&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Task&lt;/th&gt; &#xA;     &lt;th&gt;Size&lt;/th&gt; &#xA;     &lt;th colspan=&#34;3&#34;&gt;ASR (zh)&lt;/th&gt; &#xA;     &lt;th colspan=&#34;3&#34;&gt;ASR (en)&lt;/th&gt; &#xA;     &lt;th colspan=&#34;2&#34;&gt;AST&lt;/th&gt; &#xA;     &lt;th&gt;Emotion&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Metric&lt;/th&gt; &#xA;     &lt;td&gt;&lt;/td&gt; &#xA;     &lt;th colspan=&#34;3&#34;&gt;CER‚Üì&lt;/th&gt; &#xA;     &lt;th colspan=&#34;3&#34;&gt;WER‚Üì&lt;/th&gt; &#xA;     &lt;th colspan=&#34;2&#34;&gt;BLEU‚Üë&lt;/th&gt; &#xA;     &lt;th&gt;ACC‚Üë&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Dataset&lt;/th&gt; &#xA;     &lt;td&gt;&lt;/td&gt; &#xA;     &lt;th&gt;AISHELL-1&lt;/th&gt; &#xA;     &lt;th&gt;Fleurs zh&lt;/th&gt; &#xA;     &lt;th&gt;WenetSpeech test-net&lt;/th&gt; &#xA;     &lt;th&gt;LibriSpeech test-clean&lt;/th&gt; &#xA;     &lt;th&gt;GigaSpeech&lt;/th&gt; &#xA;     &lt;th&gt;TED-LIUM&lt;/th&gt; &#xA;     &lt;th&gt;CoVoST en2zh&lt;/th&gt; &#xA;     &lt;th&gt;CoVoST zh2en&lt;/th&gt; &#xA;     &lt;th&gt;MELD emotion&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody align=&#34;center&#34;&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;11&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT-4o-Realtime&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;7.3*&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;5.4*&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;28.9*&lt;/td&gt; &#xA;     &lt;td&gt;2.6*&lt;/td&gt; &#xA;     &lt;td&gt;12.9*&lt;/td&gt; &#xA;     &lt;td&gt;4.8*&lt;/td&gt; &#xA;     &lt;td&gt;37.1*&lt;/td&gt; &#xA;     &lt;td&gt;15.7*&lt;/td&gt; &#xA;     &lt;td&gt;33.2*&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Gemini 1.5 Pro&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;4.5*&lt;/td&gt; &#xA;     &lt;td&gt;5.9*&lt;/td&gt; &#xA;     &lt;td&gt;14.3*&lt;/td&gt; &#xA;     &lt;td&gt;2.9*&lt;/td&gt; &#xA;     &lt;td&gt;10.6*&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;3.0*&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;47.3*&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;22.6*&lt;/td&gt; &#xA;     &lt;td&gt;48.4*&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;11&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Qwen2-Audio-7B&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;7.5&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;45.2&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;24.4&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;55.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Qwen2-Audio-7B-Instruct&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;2.6*&lt;/td&gt; &#xA;     &lt;td&gt;6.9*&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;10.3*&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;3.1*&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;9.7&lt;/u&gt;*&lt;/td&gt; &#xA;     &lt;td&gt;5.9*&lt;/td&gt; &#xA;     &lt;td&gt;39.5*&lt;/td&gt; &#xA;     &lt;td&gt;22.9*&lt;/td&gt; &#xA;     &lt;td&gt;17.4*&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GLM-4-Voice-Base&lt;/td&gt; &#xA;     &lt;td&gt;9B&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;2.5&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;2.8&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-o 2.6&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;1.6&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;4.4&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;6.9&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;1.7&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;8.7&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;3.0&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;48.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;27.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;52.4&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt; * We evaluate officially released checkpoints by ourselves.&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;strong&gt;Speech Generation&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Task&lt;/th&gt; &#xA;     &lt;th&gt;Size&lt;/th&gt; &#xA;     &lt;th colspan=&#34;9&#34;&gt;SpeechQA&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Metric&lt;/th&gt; &#xA;     &lt;th&gt;&lt;/th&gt; &#xA;     &lt;th colspan=&#34;3&#34;&gt;ACC‚Üë&lt;/th&gt; &#xA;     &lt;th&gt;G-Eval (10 point)‚Üë&lt;/th&gt; &#xA;     &lt;th&gt;Semantic ELO score‚Üë&lt;/th&gt; &#xA;     &lt;th&gt;Acoustic ELO score‚Üë&lt;/th&gt; &#xA;     &lt;th&gt;Overall ELO score‚Üë&lt;/th&gt; &#xA;     &lt;th&gt;UTMOS‚Üë&lt;/th&gt; &#xA;     &lt;th&gt;ASR-WER‚Üì&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Dataset&lt;/th&gt; &#xA;     &lt;th&gt;&lt;/th&gt; &#xA;     &lt;th&gt;Speech Llama Q.&lt;/th&gt; &#xA;     &lt;th&gt;Speech Web Q.&lt;/th&gt; &#xA;     &lt;th&gt;Speech Trivia QA&lt;/th&gt; &#xA;     &lt;th&gt;Speech AlpacaEval&lt;/th&gt; &#xA;     &lt;th colspan=&#34;5&#34;&gt;AudioArena&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody align=&#34;center&#34;&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;11&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GPT-4o-Realtime&lt;/td&gt; &#xA;     &lt;td&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;71.7&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;51.6&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;69.7&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;7.4&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;1157&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;1203&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;1200&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;2.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td colspan=&#34;11&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-Source&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;GLM-4-Voice&lt;/td&gt; &#xA;     &lt;td&gt;9B&lt;/td&gt; &#xA;     &lt;td&gt;50.0&lt;/td&gt; &#xA;     &lt;td&gt;32.0&lt;/td&gt; &#xA;     &lt;td&gt;36.4&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;999&lt;/td&gt; &#xA;     &lt;td&gt;1147&lt;/td&gt; &#xA;     &lt;td&gt;1035&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;4.1&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;11.7&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Llama-Omni&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;45.3&lt;/td&gt; &#xA;     &lt;td&gt;22.9&lt;/td&gt; &#xA;     &lt;td&gt;10.7&lt;/td&gt; &#xA;     &lt;td&gt;3.9&lt;/td&gt; &#xA;     &lt;td&gt;960&lt;/td&gt; &#xA;     &lt;td&gt;878&lt;/td&gt; &#xA;     &lt;td&gt;897&lt;/td&gt; &#xA;     &lt;td&gt;3.2&lt;/td&gt; &#xA;     &lt;td&gt;24.3&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Moshi&lt;/td&gt; &#xA;     &lt;td&gt;7B&lt;/td&gt; &#xA;     &lt;td&gt;43.7&lt;/td&gt; &#xA;     &lt;td&gt;23.8&lt;/td&gt; &#xA;     &lt;td&gt;16.7&lt;/td&gt; &#xA;     &lt;td&gt;2.4&lt;/td&gt; &#xA;     &lt;td&gt;871&lt;/td&gt; &#xA;     &lt;td&gt;808&lt;/td&gt; &#xA;     &lt;td&gt;875&lt;/td&gt; &#xA;     &lt;td&gt;2.8&lt;/td&gt; &#xA;     &lt;td&gt;8.2&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;Mini-Omni&lt;/td&gt; &#xA;     &lt;td&gt;1B&lt;/td&gt; &#xA;     &lt;td&gt;22.0&lt;/td&gt; &#xA;     &lt;td&gt;12.8&lt;/td&gt; &#xA;     &lt;td&gt;6.9&lt;/td&gt; &#xA;     &lt;td&gt;2.5&lt;/td&gt; &#xA;     &lt;td&gt;926&lt;/td&gt; &#xA;     &lt;td&gt;803&lt;/td&gt; &#xA;     &lt;td&gt;865&lt;/td&gt; &#xA;     &lt;td&gt;3.4&lt;/td&gt; &#xA;     &lt;td&gt;10.0&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-o 2.6&lt;/td&gt; &#xA;     &lt;td&gt;8B&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;61.0&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;40.0&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;40.2&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;5.1&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;1088&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;1163&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;1131&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;4.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;9.8&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt; All results are from AudioEvals, and the evaluation methods along with further details can be found in &#xA; &lt;a href=&#34;https://github.com/OpenBMB/UltraEval-Audio&#34; target=&#34;_blank&#34;&gt;AudioEvals&lt;/a&gt;.&#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;strong&gt;End-to-end Voice Cloning&lt;/strong&gt;&lt;/p&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;   &lt;thead&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Task&lt;/th&gt; &#xA;     &lt;th colspan=&#34;2&#34;&gt;Voice cloning&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Metric&lt;/th&gt; &#xA;     &lt;th&gt;SIMO‚Üë&lt;/th&gt; &#xA;     &lt;th&gt;SIMO‚Üë&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;th align=&#34;left&#34;&gt;Dataset&lt;/th&gt; &#xA;     &lt;th&gt;Seed-TTS test-zh&lt;/th&gt; &#xA;     &lt;th&gt;Seed-TTS test-en&lt;/th&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/thead&gt; &#xA;   &lt;tbody align=&#34;center&#34;&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;F5-TTS&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;76&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;strong&gt;67&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;CosyVoice&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;75&lt;/u&gt;&lt;/td&gt; &#xA;     &lt;td&gt;&lt;u&gt;64&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;FireRedTTS&lt;/td&gt; &#xA;     &lt;td&gt;63&lt;/td&gt; &#xA;     &lt;td&gt;46&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;    &lt;tr&gt; &#xA;     &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-o 2.6&lt;/td&gt; &#xA;     &lt;td&gt;57&lt;/td&gt; &#xA;     &lt;td&gt;47&lt;/td&gt; &#xA;    &lt;/tr&gt; &#xA;   &lt;/tbody&gt; &#xA;  &lt;/table&gt; &#xA; &lt;/div&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view multimodal live streaming results.&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Multimodal Live Streaming&lt;/strong&gt;: results on StreamingBench&lt;/p&gt; &#xA; &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;Real-Time Video Understanding&lt;/th&gt; &#xA;    &lt;th&gt;Omni-Source Understanding&lt;/th&gt; &#xA;    &lt;th&gt;Contextual Understanding&lt;/th&gt; &#xA;    &lt;th&gt;Overall&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody align=&#34;center&#34;&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;7&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;Gemini 1.5 Pro&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;77.4&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;67.8&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;51.1&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;70.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;GPT-4o-202408&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;74.5&lt;/td&gt; &#xA;    &lt;td&gt;51.0&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;48.0&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;64.1&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;Claude-3.5-Sonnet&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;74.0&lt;/td&gt; &#xA;    &lt;td&gt;41.4&lt;/td&gt; &#xA;    &lt;td&gt;37.8&lt;/td&gt; &#xA;    &lt;td&gt;59.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td colspan=&#34;9&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;VILA-1.5&lt;/td&gt; &#xA;    &lt;td&gt;8B&lt;/td&gt; &#xA;    &lt;td&gt;61.5&lt;/td&gt; &#xA;    &lt;td&gt;37.5&lt;/td&gt; &#xA;    &lt;td&gt;26.7&lt;/td&gt; &#xA;    &lt;td&gt;49.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;LongVA&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;63.1&lt;/td&gt; &#xA;    &lt;td&gt;35.9&lt;/td&gt; &#xA;    &lt;td&gt;30.2&lt;/td&gt; &#xA;    &lt;td&gt;50.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-Next-Video-34B&lt;/td&gt; &#xA;    &lt;td&gt;34B&lt;/td&gt; &#xA;    &lt;td&gt;69.8&lt;/td&gt; &#xA;    &lt;td&gt;41.7&lt;/td&gt; &#xA;    &lt;td&gt;34.3&lt;/td&gt; &#xA;    &lt;td&gt;56.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;Qwen2-VL-7B&lt;/td&gt; &#xA;    &lt;td&gt;8B&lt;/td&gt; &#xA;    &lt;td&gt;71.2&lt;/td&gt; &#xA;    &lt;td&gt;40.7&lt;/td&gt; &#xA;    &lt;td&gt;33.1&lt;/td&gt; &#xA;    &lt;td&gt;57.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;InternVL2-8B&lt;/td&gt; &#xA;    &lt;td&gt;8B&lt;/td&gt; &#xA;    &lt;td&gt;70.1&lt;/td&gt; &#xA;    &lt;td&gt;42.7&lt;/td&gt; &#xA;    &lt;td&gt;34.1&lt;/td&gt; &#xA;    &lt;td&gt;57.0&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;VITA-1.5&lt;/td&gt; &#xA;    &lt;td&gt;8B&lt;/td&gt; &#xA;    &lt;td&gt;70.9&lt;/td&gt; &#xA;    &lt;td&gt;40.8&lt;/td&gt; &#xA;    &lt;td&gt;35.8&lt;/td&gt; &#xA;    &lt;td&gt;57.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-OneVision-7B&lt;/td&gt; &#xA;    &lt;td&gt;8B&lt;/td&gt; &#xA;    &lt;td&gt;74.3&lt;/td&gt; &#xA;    &lt;td&gt;40.8&lt;/td&gt; &#xA;    &lt;td&gt;31.0&lt;/td&gt; &#xA;    &lt;td&gt;58.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;InternLM-XC2.5-OL-7B&lt;/td&gt; &#xA;    &lt;td&gt;8B&lt;/td&gt; &#xA;    &lt;td&gt;75.4&lt;/td&gt; &#xA;    &lt;td&gt;46.2&lt;/td&gt; &#xA;    &lt;td&gt;33.6&lt;/td&gt; &#xA;    &lt;td&gt;60.8&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V 2.6&lt;/td&gt; &#xA;    &lt;td&gt;8B&lt;/td&gt; &#xA;    &lt;td&gt;72.4&lt;/td&gt; &#xA;    &lt;td&gt;40.2&lt;/td&gt; &#xA;    &lt;td&gt;33.4&lt;/td&gt; &#xA;    &lt;td&gt;57.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-o 2.6&lt;/td&gt; &#xA;    &lt;td&gt;8B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;79.9&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;53.4&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;38.5&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;66.0&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Examples &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We deploy MiniCPM-o 2.6 on end devices. The demo video is the raw-speed recording on an iPad Pro and a Web demo.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://youtu.be/JFJg9KZ_iZk&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/o-2dot6-demo-video-preview.png&#34; , width=&#34;70%&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div style=&#34;display: flex; flex-direction: column; align-items: center;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmo2_6/minicpmo2_6_math_intersect.png&#34; alt=&#34;math&#34; style=&#34;margin-bottom: 5px;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmo2_6/minicpmo2_6_diagram_train_NN.png&#34; alt=&#34;diagram&#34; style=&#34;margin-bottom: 5px;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmo2_6/minicpmo2_6_multi-image_bike.png&#34; alt=&#34;bike&#34; style=&#34;margin-bottom: 5px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;MiniCPM-V 2.6&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view more details of MiniCPM-V 2.6&lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;MiniCPM-V 2.6&lt;/strong&gt; is the latest and most capable model in the MiniCPM-V series. The model is built on SigLip-400M and Qwen2-7B with a total of 8B parameters. It exhibits a significant performance improvement over MiniCPM-Llama3-V 2.5, and introduces new features for multi-image and video understanding. Notable features of MiniCPM-V 2.6 include:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;üî• &lt;strong&gt;Leading Performance.&lt;/strong&gt; MiniCPM-V 2.6 achieves an average score of 65.2 on the latest version of OpenCompass, a comprehensive evaluation over 8 popular benchmarks. &lt;strong&gt;With only 8B parameters, it surpasses widely used proprietary models like GPT-4o mini, GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet&lt;/strong&gt; for single image understanding.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;üñºÔ∏è &lt;strong&gt;Multi Image Understanding and In-context Learning.&lt;/strong&gt; MiniCPM-V 2.6 can also perform &lt;strong&gt;conversation and reasoning over multiple images&lt;/strong&gt;. It achieves &lt;strong&gt;state-of-the-art performance&lt;/strong&gt; on popular multi-image benchmarks such as Mantis-Eval, BLINK, Mathverse mv and Sciverse mv, and also shows promising in-context learning capability.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;üé¨ &lt;strong&gt;Video Understanding.&lt;/strong&gt; MiniCPM-V 2.6 can also &lt;strong&gt;accept video inputs&lt;/strong&gt;, performing conversation and providing dense captions for spatial-temporal information. It outperforms &lt;strong&gt;GPT-4V, Claude 3.5 Sonnet and LLaVA-NeXT-Video-34B&lt;/strong&gt; on Video-MME with/without subtitles.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;üí™ &lt;strong&gt;Strong OCR Capability and Others.&lt;/strong&gt; MiniCPM-V 2.6 can process images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344). It achieves &lt;strong&gt;state-of-the-art performance on OCRBench, surpassing proprietary models such as GPT-4o, GPT-4V, and Gemini 1.5 Pro&lt;/strong&gt;. Based on the the latest &lt;a href=&#34;https://github.com/RLHF-V/RLAIF-V/&#34;&gt;RLAIF-V&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenBMB/VisCPM&#34;&gt;VisCPM&lt;/a&gt; techniques, it features &lt;strong&gt;trustworthy behaviors&lt;/strong&gt;, with significantly lower hallucination rates than GPT-4o and GPT-4V on Object HalBench, and supports &lt;strong&gt;multilingual capabilities&lt;/strong&gt; on English, Chinese, German, French, Italian, Korean, etc.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;üöÄ &lt;strong&gt;Superior Efficiency.&lt;/strong&gt; In addition to its friendly size, MiniCPM-V 2.6 also shows &lt;strong&gt;state-of-the-art token density&lt;/strong&gt; (i.e., number of pixels encoded into each visual token). &lt;strong&gt;It produces only 640 tokens when processing a 1.8M pixel image, which is 75% fewer than most models&lt;/strong&gt;. This directly improves the inference speed, first-token latency, memory usage, and power consumption. As a result, MiniCPM-V 2.6 can efficiently support &lt;strong&gt;real-time video understanding&lt;/strong&gt; on end-side devices such as iPad.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;üí´ &lt;strong&gt;Easy Usage.&lt;/strong&gt; MiniCPM-V 2.6 can be easily used in various ways: (1) &lt;a href=&#34;https://github.com/OpenBMB/llama.cpp/raw/minicpmv-main/examples/llava/README-minicpmv2.6.md&#34;&gt;llama.cpp&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md&#34;&gt;ollama&lt;/a&gt; support for efficient CPU inference on local devices, (2) &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2_6-int4&#34;&gt;int4&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf&#34;&gt;GGUF&lt;/a&gt; format quantized models in 16 sizes, (3) &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#inference-with-vllm&#34;&gt;vLLM&lt;/a&gt; support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks, (5) quick local WebUI demo setup with &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/#chat-with-our-demo-on-gradio&#34;&gt;Gradio&lt;/a&gt;, and (6) online web &lt;a href=&#34;http://120.92.209.146:8887/&#34;&gt;demo&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Evaluation &#xA;  &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/radar_final.png&#34; width=&#34;80%&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Click to view single image results on OpenCompass, MME, MMVet, OCRBench, MMMU, MathVista, MMB, AI2D, TextVQA, DocVQA, HallusionBench, Object HalBench. &lt;/summary&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;      &lt;th&gt;Size&lt;/th&gt; &#xA;      &lt;th&gt;Token Density&lt;sup&gt;+&lt;/sup&gt;&lt;/th&gt; &#xA;      &lt;th&gt;OpenCompass&lt;/th&gt; &#xA;      &lt;th&gt;MME&lt;/th&gt; &#xA;      &lt;th&gt;MMVet&lt;/th&gt; &#xA;      &lt;th&gt;OCRBench&lt;/th&gt; &#xA;      &lt;th&gt;MMMU val&lt;/th&gt; &#xA;      &lt;th&gt;MathVista mini&lt;/th&gt; &#xA;      &lt;th&gt;MMB1.1 test&lt;/th&gt; &#xA;      &lt;th&gt;AI2D&lt;/th&gt; &#xA;      &lt;th&gt;TextVQA val&lt;/th&gt; &#xA;      &lt;th&gt;DocVQA test&lt;/th&gt; &#xA;      &lt;th&gt;HallusionBench&lt;/th&gt; &#xA;      &lt;th&gt;Object HalBench&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody align=&#34;center&#34;&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td colspan=&#34;15&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;GPT-4o&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;1088&lt;/td&gt; &#xA;      &lt;td&gt;69.9&lt;/td&gt; &#xA;      &lt;td&gt;2328.7&lt;/td&gt; &#xA;      &lt;td&gt;69.1&lt;/td&gt; &#xA;      &lt;td&gt;736&lt;/td&gt; &#xA;      &lt;td&gt;69.2&lt;/td&gt; &#xA;      &lt;td&gt;61.3&lt;/td&gt; &#xA;      &lt;td&gt;82.2&lt;/td&gt; &#xA;      &lt;td&gt;84.6&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;92.8&lt;/td&gt; &#xA;      &lt;td&gt;55.0&lt;/td&gt; &#xA;      &lt;td&gt;17.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;Claude 3.5 Sonnet&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;750&lt;/td&gt; &#xA;      &lt;td&gt;67.9&lt;/td&gt; &#xA;      &lt;td&gt;1920.0&lt;/td&gt; &#xA;      &lt;td&gt;66.0&lt;/td&gt; &#xA;      &lt;td&gt;788&lt;/td&gt; &#xA;      &lt;td&gt;65.9&lt;/td&gt; &#xA;      &lt;td&gt;61.6&lt;/td&gt; &#xA;      &lt;td&gt;78.5&lt;/td&gt; &#xA;      &lt;td&gt;80.2&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;95.2&lt;/td&gt; &#xA;      &lt;td&gt;49.9&lt;/td&gt; &#xA;      &lt;td&gt;13.8&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;Gemini 1.5 Pro&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;64.4&lt;/td&gt; &#xA;      &lt;td&gt;2110.6&lt;/td&gt; &#xA;      &lt;td&gt;64.0&lt;/td&gt; &#xA;      &lt;td&gt;754&lt;/td&gt; &#xA;      &lt;td&gt;60.6&lt;/td&gt; &#xA;      &lt;td&gt;57.7&lt;/td&gt; &#xA;      &lt;td&gt;73.9&lt;/td&gt; &#xA;      &lt;td&gt;79.1&lt;/td&gt; &#xA;      &lt;td&gt;73.5&lt;/td&gt; &#xA;      &lt;td&gt;86.5&lt;/td&gt; &#xA;      &lt;td&gt;45.6&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;GPT-4o mini&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;1088&lt;/td&gt; &#xA;      &lt;td&gt;64.1&lt;/td&gt; &#xA;      &lt;td&gt;2003.4&lt;/td&gt; &#xA;      &lt;td&gt;66.9&lt;/td&gt; &#xA;      &lt;td&gt;785&lt;/td&gt; &#xA;      &lt;td&gt;60.0&lt;/td&gt; &#xA;      &lt;td&gt;52.4&lt;/td&gt; &#xA;      &lt;td&gt;76.0&lt;/td&gt; &#xA;      &lt;td&gt;77.8&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;46.1&lt;/td&gt; &#xA;      &lt;td&gt;12.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;GPT-4V&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;1088&lt;/td&gt; &#xA;      &lt;td&gt;63.5&lt;/td&gt; &#xA;      &lt;td&gt;2070.2&lt;/td&gt; &#xA;      &lt;td&gt;67.5&lt;/td&gt; &#xA;      &lt;td&gt;656&lt;/td&gt; &#xA;      &lt;td&gt;61.7&lt;/td&gt; &#xA;      &lt;td&gt;54.7&lt;/td&gt; &#xA;      &lt;td&gt;79.8&lt;/td&gt; &#xA;      &lt;td&gt;78.6&lt;/td&gt; &#xA;      &lt;td&gt;78.0&lt;/td&gt; &#xA;      &lt;td&gt;87.2&lt;/td&gt; &#xA;      &lt;td&gt;43.9&lt;/td&gt; &#xA;      &lt;td&gt;14.2&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;Step-1V&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;59.5&lt;/td&gt; &#xA;      &lt;td&gt;2206.4&lt;/td&gt; &#xA;      &lt;td&gt;63.3&lt;/td&gt; &#xA;      &lt;td&gt;625&lt;/td&gt; &#xA;      &lt;td&gt;49.9&lt;/td&gt; &#xA;      &lt;td&gt;44.8&lt;/td&gt; &#xA;      &lt;td&gt;78.0&lt;/td&gt; &#xA;      &lt;td&gt;79.2&lt;/td&gt; &#xA;      &lt;td&gt;71.6&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;48.4&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;Qwen-VL-Max&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;784&lt;/td&gt; &#xA;      &lt;td&gt;58.3&lt;/td&gt; &#xA;      &lt;td&gt;2281.7&lt;/td&gt; &#xA;      &lt;td&gt;61.8&lt;/td&gt; &#xA;      &lt;td&gt;684&lt;/td&gt; &#xA;      &lt;td&gt;52.0&lt;/td&gt; &#xA;      &lt;td&gt;43.4&lt;/td&gt; &#xA;      &lt;td&gt;74.6&lt;/td&gt; &#xA;      &lt;td&gt;75.7&lt;/td&gt; &#xA;      &lt;td&gt;79.5&lt;/td&gt; &#xA;      &lt;td&gt;93.1&lt;/td&gt; &#xA;      &lt;td&gt;41.2&lt;/td&gt; &#xA;      &lt;td&gt;13.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td colspan=&#34;15&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-NeXT-Yi-34B&lt;/td&gt; &#xA;      &lt;td&gt;34B&lt;/td&gt; &#xA;      &lt;td&gt;157&lt;/td&gt; &#xA;      &lt;td&gt;55.0&lt;/td&gt; &#xA;      &lt;td&gt;2006.5&lt;/td&gt; &#xA;      &lt;td&gt;50.7&lt;/td&gt; &#xA;      &lt;td&gt;574&lt;/td&gt; &#xA;      &lt;td&gt;48.8&lt;/td&gt; &#xA;      &lt;td&gt;40.4&lt;/td&gt; &#xA;      &lt;td&gt;77.8&lt;/td&gt; &#xA;      &lt;td&gt;78.9&lt;/td&gt; &#xA;      &lt;td&gt;69.3&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;34.8&lt;/td&gt; &#xA;      &lt;td&gt;12.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;Mini-Gemini-HD-34B&lt;/td&gt; &#xA;      &lt;td&gt;34B&lt;/td&gt; &#xA;      &lt;td&gt;157&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;2141.0&lt;/td&gt; &#xA;      &lt;td&gt;59.3&lt;/td&gt; &#xA;      &lt;td&gt;518&lt;/td&gt; &#xA;      &lt;td&gt;48.0&lt;/td&gt; &#xA;      &lt;td&gt;43.3&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;80.5&lt;/td&gt; &#xA;      &lt;td&gt;74.1&lt;/td&gt; &#xA;      &lt;td&gt;78.9&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;Cambrian-34B&lt;/td&gt; &#xA;      &lt;td&gt;34B&lt;/td&gt; &#xA;      &lt;td&gt;1820&lt;/td&gt; &#xA;      &lt;td&gt;58.3&lt;/td&gt; &#xA;      &lt;td&gt;2049.9&lt;/td&gt; &#xA;      &lt;td&gt;53.2&lt;/td&gt; &#xA;      &lt;td&gt;591&lt;/td&gt; &#xA;      &lt;td&gt;50.4&lt;/td&gt; &#xA;      &lt;td&gt;50.3&lt;/td&gt; &#xA;      &lt;td&gt;77.8&lt;/td&gt; &#xA;      &lt;td&gt;79.5&lt;/td&gt; &#xA;      &lt;td&gt;76.7&lt;/td&gt; &#xA;      &lt;td&gt;75.5&lt;/td&gt; &#xA;      &lt;td&gt;41.6&lt;/td&gt; &#xA;      &lt;td&gt;14.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;GLM-4V-9B&lt;/td&gt; &#xA;      &lt;td&gt;13B&lt;/td&gt; &#xA;      &lt;td&gt;784&lt;/td&gt; &#xA;      &lt;td&gt;59.1&lt;/td&gt; &#xA;      &lt;td&gt;2018.8&lt;/td&gt; &#xA;      &lt;td&gt;58.0&lt;/td&gt; &#xA;      &lt;td&gt;776&lt;/td&gt; &#xA;      &lt;td&gt;46.9&lt;/td&gt; &#xA;      &lt;td&gt;51.1&lt;/td&gt; &#xA;      &lt;td&gt;67.9&lt;/td&gt; &#xA;      &lt;td&gt;71.2&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;45.0&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;InternVL2-8B&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;706&lt;/td&gt; &#xA;      &lt;td&gt;64.1&lt;/td&gt; &#xA;      &lt;td&gt;2215.1&lt;/td&gt; &#xA;      &lt;td&gt;54.3&lt;/td&gt; &#xA;      &lt;td&gt;794&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;51.2&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;58.3&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;79.4&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;83.6&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;77.4&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;91.6&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;45.0&lt;/td&gt; &#xA;      &lt;td&gt;21.3&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-Llama-V 2.5&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;1882&lt;/td&gt; &#xA;      &lt;td&gt;58.8&lt;/td&gt; &#xA;      &lt;td&gt;2024.6&lt;/td&gt; &#xA;      &lt;td&gt;52.8&lt;/td&gt; &#xA;      &lt;td&gt;725&lt;/td&gt; &#xA;      &lt;td&gt;45.8&lt;/td&gt; &#xA;      &lt;td&gt;54.3&lt;/td&gt; &#xA;      &lt;td&gt;72.0&lt;/td&gt; &#xA;      &lt;td&gt;78.4&lt;/td&gt; &#xA;      &lt;td&gt;76.6&lt;/td&gt; &#xA;      &lt;td&gt;84.8&lt;/td&gt; &#xA;      &lt;td&gt;42.4&lt;/td&gt; &#xA;      &lt;td&gt;10.3&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V 2.6&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;2822&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;65.2&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;2348.4&lt;/strong&gt;*&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;60.0&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;852&lt;/strong&gt;*&lt;/td&gt; &#xA;      &lt;td&gt;49.8*&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;60.6&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;78.0&lt;/td&gt; &#xA;      &lt;td&gt;82.1&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;80.1&lt;strong&gt;&lt;/strong&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;90.8&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;48.1&lt;/strong&gt;*&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;8.2&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;  &lt;/div&gt; * We evaluate this benchmark using chain-of-thought prompting. Specifically, for MME, we used this technique only for the Cognition set. &#xA;  &lt;p&gt;&lt;sup&gt;+&lt;/sup&gt; Token Density: number of pixels encoded into each visual token at maximum resolution, i.e., # pixels at maximum resolution / # visual tokens.&lt;/p&gt; &#xA;  &lt;p&gt;Note: For proprietary models, we calculate token density based on the image encoding charging strategy defined in the official API documentation, which provides an upper-bound estimation.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Click to view multi-image results on Mantis Eval, BLINK, Mathverse mv, Sciverse mv, MIRB.&lt;/summary&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;      &lt;th&gt;Size&lt;/th&gt; &#xA;      &lt;th&gt;Mantis Eval&lt;/th&gt; &#xA;      &lt;th&gt;BLINK val&lt;/th&gt; &#xA;      &lt;th&gt;Mathverse mv&lt;/th&gt; &#xA;      &lt;th&gt;Sciverse mv&lt;/th&gt; &#xA;      &lt;th&gt;MIRB&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody align=&#34;center&#34;&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td colspan=&#34;7&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;GPT-4V&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;62.7&lt;/td&gt; &#xA;      &lt;td&gt;54.6&lt;/td&gt; &#xA;      &lt;td&gt;60.3&lt;/td&gt; &#xA;      &lt;td&gt;66.9&lt;/td&gt; &#xA;      &lt;td&gt;53.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-NeXT-Interleave-14B&lt;/td&gt; &#xA;      &lt;td&gt;14B&lt;/td&gt; &#xA;      &lt;td&gt;66.4&lt;/td&gt; &#xA;      &lt;td&gt;52.6&lt;/td&gt; &#xA;      &lt;td&gt;32.7&lt;/td&gt; &#xA;      &lt;td&gt;30.2&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td colspan=&#34;7&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;Emu2-Chat&lt;/td&gt; &#xA;      &lt;td&gt;37B&lt;/td&gt; &#xA;      &lt;td&gt;37.8&lt;/td&gt; &#xA;      &lt;td&gt;36.2&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;27.2&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;CogVLM&lt;/td&gt; &#xA;      &lt;td&gt;17B&lt;/td&gt; &#xA;      &lt;td&gt;45.2&lt;/td&gt; &#xA;      &lt;td&gt;41.1&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;VPG-C&lt;/td&gt; &#xA;      &lt;td&gt;7B&lt;/td&gt; &#xA;      &lt;td&gt;52.4&lt;/td&gt; &#xA;      &lt;td&gt;43.1&lt;/td&gt; &#xA;      &lt;td&gt;24.3&lt;/td&gt; &#xA;      &lt;td&gt;23.1&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;VILA 8B&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;51.2&lt;/td&gt; &#xA;      &lt;td&gt;39.3&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;36.5&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;InternLM-XComposer-2.5&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;53.1*&lt;/td&gt; &#xA;      &lt;td&gt;48.9&lt;/td&gt; &#xA;      &lt;td&gt;32.1*&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;42.5&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;InternVL2-8B&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;59.0*&lt;/td&gt; &#xA;      &lt;td&gt;50.9&lt;/td&gt; &#xA;      &lt;td&gt;30.5*&lt;/td&gt; &#xA;      &lt;td&gt;34.4*&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;56.9*&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V 2.6&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;69.1&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;53.0&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;84.9&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;74.9&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;53.8&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;  &lt;/div&gt; * We evaluate the officially released checkpoint by ourselves. &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Click to view video results on Video-MME and Video-ChatGPT.&lt;/summary&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;      &lt;th&gt;Size&lt;/th&gt; &#xA;      &lt;th colspan=&#34;2&#34;&gt;Video-MME&lt;/th&gt; &#xA;      &lt;th colspan=&#34;5&#34;&gt;Video-ChatGPT&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;      &lt;th&gt;&lt;/th&gt; &#xA;      &lt;th&gt;w/o subs&lt;/th&gt; &#xA;      &lt;th&gt;w subs&lt;/th&gt; &#xA;      &lt;th&gt;Correctness&lt;/th&gt; &#xA;      &lt;th&gt;Detail&lt;/th&gt; &#xA;      &lt;th&gt;Context&lt;/th&gt; &#xA;      &lt;th&gt;Temporal&lt;/th&gt; &#xA;      &lt;th&gt;Consistency&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody align=&#34;center&#34;&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td colspan=&#34;9&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Proprietary&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;Claude 3.5 Sonnet&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;60.0&lt;/td&gt; &#xA;      &lt;td&gt;62.9&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;GPT-4V&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;59.9&lt;/td&gt; &#xA;      &lt;td&gt;63.3&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td colspan=&#34;9&#34; align=&#34;left&#34;&gt;&lt;strong&gt;Open-source&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-NeXT-7B&lt;/td&gt; &#xA;      &lt;td&gt;7B&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;3.39&lt;/td&gt; &#xA;      &lt;td&gt;3.29&lt;/td&gt; &#xA;      &lt;td&gt;3.92&lt;/td&gt; &#xA;      &lt;td&gt;2.60&lt;/td&gt; &#xA;      &lt;td&gt;3.12&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-NeXT-34B&lt;/td&gt; &#xA;      &lt;td&gt;34B&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;3.29&lt;/td&gt; &#xA;      &lt;td&gt;3.23&lt;/td&gt; &#xA;      &lt;td&gt;3.83&lt;/td&gt; &#xA;      &lt;td&gt;2.51&lt;/td&gt; &#xA;      &lt;td&gt;3.47&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;CogVLM2-Video&lt;/td&gt; &#xA;      &lt;td&gt;12B&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;3.49&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;3.46&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;3.23&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;2.98&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;3.64&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;LongVA&lt;/td&gt; &#xA;      &lt;td&gt;7B&lt;/td&gt; &#xA;      &lt;td&gt;52.4&lt;/td&gt; &#xA;      &lt;td&gt;54.3&lt;/td&gt; &#xA;      &lt;td&gt;3.05&lt;/td&gt; &#xA;      &lt;td&gt;3.09&lt;/td&gt; &#xA;      &lt;td&gt;3.77&lt;/td&gt; &#xA;      &lt;td&gt;2.44&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;3.64&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;InternVL2-8B&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;54.0&lt;/td&gt; &#xA;      &lt;td&gt;56.9&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;InternLM-XComposer-2.5&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;55.8&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;      &lt;td&gt;-&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;LLaVA-NeXT-Video&lt;/td&gt; &#xA;      &lt;td&gt;32B&lt;/td&gt; &#xA;      &lt;td&gt;60.2&lt;/td&gt; &#xA;      &lt;td&gt;63.0&lt;/td&gt; &#xA;      &lt;td&gt;3.48&lt;/td&gt; &#xA;      &lt;td&gt;3.37&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;3.95&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;2.64&lt;/td&gt; &#xA;      &lt;td&gt;3.28&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td nowrap align=&#34;left&#34;&gt;MiniCPM-V 2.6&lt;/td&gt; &#xA;      &lt;td&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;60.9&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;63.6&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;3.59&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;3.28&lt;/td&gt; &#xA;      &lt;td&gt;3.93&lt;/td&gt; &#xA;      &lt;td&gt;2.73&lt;/td&gt; &#xA;      &lt;td&gt;3.62&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;  &lt;/div&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Click to view few-shot results on TextVQA, VizWiz, VQAv2, OK-VQA.&lt;/summary&gt; &#xA;  &lt;div align=&#34;center&#34;&gt; &#xA;   &lt;table style=&#34;margin: 0px auto;&#34;&gt; &#xA;    &lt;thead&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;      &lt;th&gt;Size&lt;/th&gt; &#xA;      &lt;th&gt;Shot&lt;/th&gt; &#xA;      &lt;th&gt;TextVQA val&lt;/th&gt; &#xA;      &lt;th&gt;VizWiz test-dev&lt;/th&gt; &#xA;      &lt;th&gt;VQAv2 test-dev&lt;/th&gt; &#xA;      &lt;th&gt;OK-VQA val&lt;/th&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/thead&gt; &#xA;    &lt;tbody align=&#34;center&#34;&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;left&#34; nowrap rowspan=&#34;3&#34;&gt;Flamingo&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;3&#34;&gt;80B&lt;/td&gt; &#xA;      &lt;td&gt;0*&lt;/td&gt; &#xA;      &lt;td&gt;35.0&lt;/td&gt; &#xA;      &lt;td&gt;31.6&lt;/td&gt; &#xA;      &lt;td&gt;56.3&lt;/td&gt; &#xA;      &lt;td&gt;40.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;4&lt;/td&gt; &#xA;      &lt;td&gt;36.5&lt;/td&gt; &#xA;      &lt;td&gt;39.6&lt;/td&gt; &#xA;      &lt;td&gt;63.1&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;57.4&lt;/strong&gt;&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;8&lt;/td&gt; &#xA;      &lt;td&gt;37.3&lt;/td&gt; &#xA;      &lt;td&gt;44.8&lt;/td&gt; &#xA;      &lt;td&gt;65.6&lt;/td&gt; &#xA;      &lt;td&gt;57.5&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;left&#34; nowrap rowspan=&#34;3&#34;&gt;IDEFICS&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;3&#34;&gt;80B&lt;/td&gt; &#xA;      &lt;td&gt;0*&lt;/td&gt; &#xA;      &lt;td&gt;30.9&lt;/td&gt; &#xA;      &lt;td&gt;36.0&lt;/td&gt; &#xA;      &lt;td&gt;60.0&lt;/td&gt; &#xA;      &lt;td&gt;45.2&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;4&lt;/td&gt; &#xA;      &lt;td&gt;34.3&lt;/td&gt; &#xA;      &lt;td&gt;40.4&lt;/td&gt; &#xA;      &lt;td&gt;63.6&lt;/td&gt; &#xA;      &lt;td&gt;52.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;8&lt;/td&gt; &#xA;      &lt;td&gt;35.7&lt;/td&gt; &#xA;      &lt;td&gt;46.1&lt;/td&gt; &#xA;      &lt;td&gt;64.8&lt;/td&gt; &#xA;      &lt;td&gt;55.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;left&#34; nowrap rowspan=&#34;3&#34;&gt;OmniCorpus&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;3&#34;&gt;7B&lt;/td&gt; &#xA;      &lt;td&gt;0*&lt;/td&gt; &#xA;      &lt;td&gt;43.0&lt;/td&gt; &#xA;      &lt;td&gt;49.8&lt;/td&gt; &#xA;      &lt;td&gt;63.2&lt;/td&gt; &#xA;      &lt;td&gt;45.5&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;4&lt;/td&gt; &#xA;      &lt;td&gt;45.4&lt;/td&gt; &#xA;      &lt;td&gt;51.3&lt;/td&gt; &#xA;      &lt;td&gt;64.5&lt;/td&gt; &#xA;      &lt;td&gt;46.5&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;8&lt;/td&gt; &#xA;      &lt;td&gt;45.6&lt;/td&gt; &#xA;      &lt;td&gt;52.2&lt;/td&gt; &#xA;      &lt;td&gt;64.7&lt;/td&gt; &#xA;      &lt;td&gt;46.6&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;left&#34; nowrap rowspan=&#34;3&#34;&gt;Emu2&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;3&#34;&gt;37B&lt;/td&gt; &#xA;      &lt;td&gt;0&lt;/td&gt; &#xA;      &lt;td&gt;26.4&lt;/td&gt; &#xA;      &lt;td&gt;40.4&lt;/td&gt; &#xA;      &lt;td&gt;33.5&lt;/td&gt; &#xA;      &lt;td&gt;26.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;4&lt;/td&gt; &#xA;      &lt;td&gt;48.2&lt;/td&gt; &#xA;      &lt;td&gt;54.6&lt;/td&gt; &#xA;      &lt;td&gt;67.0&lt;/td&gt; &#xA;      &lt;td&gt;53.2&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;8&lt;/td&gt; &#xA;      &lt;td&gt;49.3&lt;/td&gt; &#xA;      &lt;td&gt;54.7&lt;/td&gt; &#xA;      &lt;td&gt;67.8&lt;/td&gt; &#xA;      &lt;td&gt;54.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;left&#34; nowrap rowspan=&#34;2&#34;&gt;MM1&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;2&#34;&gt;30B&lt;/td&gt; &#xA;      &lt;td&gt;0&lt;/td&gt; &#xA;      &lt;td&gt;26.2&lt;/td&gt; &#xA;      &lt;td&gt;40.4&lt;/td&gt; &#xA;      &lt;td&gt;48.9&lt;/td&gt; &#xA;      &lt;td&gt;26.7&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;8&lt;/td&gt; &#xA;      &lt;td&gt;49.3&lt;/td&gt; &#xA;      &lt;td&gt;54.7&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;70.9&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;54.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td align=&#34;left&#34; nowrap rowspan=&#34;3&#34;&gt;MiniCPM-V 2.6&lt;sup&gt;+&lt;/sup&gt;&lt;/td&gt; &#xA;      &lt;td rowspan=&#34;3&#34;&gt;8B&lt;/td&gt; &#xA;      &lt;td&gt;0&lt;/td&gt; &#xA;      &lt;td&gt;43.9&lt;/td&gt; &#xA;      &lt;td&gt;33.8&lt;/td&gt; &#xA;      &lt;td&gt;45.4&lt;/td&gt; &#xA;      &lt;td&gt;23.9&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;4&lt;/td&gt; &#xA;      &lt;td&gt;63.6&lt;/td&gt; &#xA;      &lt;td&gt;60.5&lt;/td&gt; &#xA;      &lt;td&gt;65.5&lt;/td&gt; &#xA;      &lt;td&gt;50.1&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;     &lt;tr&gt; &#xA;      &lt;td&gt;8&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;64.6&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;&lt;strong&gt;63.4&lt;/strong&gt;&lt;/td&gt; &#xA;      &lt;td&gt;68.2&lt;/td&gt; &#xA;      &lt;td&gt;51.4&lt;/td&gt; &#xA;     &lt;/tr&gt; &#xA;    &lt;/tbody&gt; &#xA;   &lt;/table&gt; &#xA;  &lt;/div&gt; * denotes zero image shot and two additional text shots following Flamingo. &#xA;  &lt;p&gt;&lt;sup&gt;+&lt;/sup&gt; We evaluate the pretraining ckpt without SFT.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;h3&gt;Examples &#xA;  &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA; &lt;div style=&#34;display: flex; flex-direction: column; align-items: center;&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmv2_6/multi_img-bike.png&#34; alt=&#34;Bike&#34; style=&#34;margin-bottom: 5px;&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmv2_6/multi_img-menu.png&#34; alt=&#34;Menu&#34; style=&#34;margin-bottom: 5px;&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmv2_6/multi_img-code.png&#34; alt=&#34;Code&#34; style=&#34;margin-bottom: 5px;&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmv2_6/ICL-Mem.png&#34; alt=&#34;Mem&#34; style=&#34;margin-bottom: 5px;&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmv2_6/multiling-medal.png&#34; alt=&#34;medal&#34; style=&#34;margin-bottom: 10px;&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt;Click to view more cases.&lt;/summary&gt; &#xA;  &lt;div style=&#34;display: flex; flex-direction: column; align-items: center;&#34;&gt; &#xA;   &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmv2_6/ICL-elec.png&#34; alt=&#34;elec&#34; style=&#34;margin-bottom: 5px;&#34;&gt; &#xA;   &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmv2_6/multiling-olympic.png&#34; alt=&#34;Menu&#34; style=&#34;margin-bottom: 10px;&#34;&gt; &#xA;  &lt;/div&gt; &#xA; &lt;/details&gt; &#xA; &lt;p&gt;We deploy MiniCPM-V 2.6 on end devices. The demo video is the raw screen recording on a iPad Pro without edition.&lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/gif_cases/ai.gif&#34; width=&#34;32%/&#34;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/gif_cases/beer.gif&#34; width=&#34;32%/&#34;&gt; &lt;/p&gt;&#xA; &lt;table align=&#34;center&#34;&gt;  &#xA; &lt;/table&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/gif_cases/ticket.gif&#34; width=&#34;32%/&#34;&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/gif_cases/wfh.gif&#34; width=&#34;32%/&#34;&gt; &lt;/p&gt;&#xA; &lt;table align=&#34;center&#34;&gt;  &#xA; &lt;/table&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &#xA;  &lt;video src=&#34;https://github.com/user-attachments/assets/21f4b818-ede1-4822-920e-91281725c830&#34; width=&#34;360&#34;&gt;&lt;/video&gt;  &#xA;  &lt;!-- &lt;video src=&#34;https://github.com/user-attachments/assets/c835f757-206b-4d9c-8e36-70d67b453628&#34; width=&#34;360&#34; /&gt; &lt;/video&gt; --&gt; &lt;/p&gt;&#xA; &lt;table align=&#34;center&#34;&gt;  &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Legacy Models &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Introduction and Guidance&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-Llama3-V 2.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/docs/minicpm_llama3_v2dot5.md&#34;&gt;Document&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 2.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/docs/minicpm_v2.md&#34;&gt;Document&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 1.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/docs/minicpm_v1.md&#34;&gt;Document&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;OmniLMM-12B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/docs/omnilmm_en.md&#34;&gt;Document&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Chat with Our Demo on Gradio ü§ó&lt;/h2&gt; &#xA;&lt;p&gt;We provide online and local demos powered by Hugging Face Gradio &lt;a href=&#34;https://github.com/gradio-app/gradio&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/gradio-app/gradio&#34;&gt;&lt;/a&gt;, the most popular model deployment framework nowadays. It supports streaming outputs, progress bars, queuing, alerts, and other useful features.&lt;/p&gt; &#xA;&lt;h3&gt;Online Demo &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Click here to try out the online demo of &lt;a href=&#34;https://minicpm-omni-webdemo-us.modelbest.cn/&#34;&gt;MiniCPM-o 2.6&lt;/a&gt; | &lt;a href=&#34;http://120.92.209.146:8887/&#34;&gt;MiniCPM-V 2.6&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-Llama3-V-2_5&#34;&gt;MiniCPM-Llama3-V 2.5&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/openbmb/MiniCPM-V-2&#34;&gt;MiniCPM-V 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Local WebUI Demo &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;You can easily build your own local WebUI demo using the following commands, experience real-time streaming voice/video call.&lt;/p&gt; &#xA;&lt;p&gt;Please ensure that &lt;code&gt;transformers==4.44.2&lt;/code&gt; is installed, as other versions may have compatibility issues. We are investigating this issue.&lt;/p&gt; &#xA;&lt;p&gt;If you are using an older version of PyTorch, you might encounter this issue &lt;code&gt;&#34;weight_norm_fwd_first_dim_kernel&#34; not implemented for &#39;BFloat16&#39;&lt;/code&gt;, Please add &lt;code&gt;self.minicpmo_model.tts.float()&lt;/code&gt; during the model initialization.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;launch model server:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements_o2.6.txt&#xA;&#xA;python web_demos/minicpm-o_2.6/model_server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;launch web server:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Make sure Node and PNPM is installed.&#xA;sudo apt-get update&#xA;sudo apt-get install nodejs npm&#xA;npm install -g pnpm&#xA;&#xA;&#xA;cd web_demos/minicpm-o_2.6/web_server&#xA;# create ssl cert for https, https is required to request camera and microphone permissions.&#xA;bash ./make_ssl_cert.sh  # output key.pem and cert.pem&#xA;&#xA;pnpm install  # install requirements&#xA;pnpm run dev  # start server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Model Zoo&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Device&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Memory&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ Description&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-o 2.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The latest version, achieving GPT-4o level performance for vision, speech and multimodal live streaming on end-side devices.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6&#34;&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-o 2.6 gguf&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6-gguf&#34;&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-gguf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-o 2.6 int4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-o-2_6-int4&#34;&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-o-2_6-int4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 2.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Strong end-side multimodal performance for single image, multi-image and video understanding.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2_6&#34;&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 2.6 gguf&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The gguf version, lower memory usage and faster inference.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf&#34;&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6-gguf&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;MiniCPM-V 2.6 int4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPU&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;The int4 quantized version, lower GPU memory usage.&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-V-2_6-int4&#34;&gt;ü§ó&lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://modelscope.cn/models/OpenBMB/MiniCPM-V-2_6-int4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/modelscope_logo.png&#34; width=&#34;20px&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Multi-turn Conversation&lt;/h3&gt; &#xA;&lt;p&gt;Please ensure that &lt;code&gt;transformers==4.44.2&lt;/code&gt; is installed, as other versions may have compatibility issues.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements_o2.6.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please refer to the following codes to run.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/minicpmo2_6/show_demo.jpg&#34; width=&#34;500px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;torch.manual_seed(100)&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True,&#xA;    attn_implementation=&#39;sdpa&#39;, torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager&#xA;model = model.eval().cuda()&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True)&#xA;&#xA;image = Image.open(&#39;./assets/minicpmo2_6/show_demo.jpg&#39;).convert(&#39;RGB&#39;)&#xA;&#xA;# First round chat &#xA;question = &#34;What is the landform in the picture?&#34;&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [image, question]}]&#xA;&#xA;answer = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer&#xA;)&#xA;print(answer)&#xA;&#xA;# Second round chat, pass history context of multi-turn conversation&#xA;msgs.append({&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: [answer]})&#xA;msgs.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: [&#34;What should I pay attention to when traveling here?&#34;]})&#xA;&#xA;answer = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will get the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;The landform in the picture is a mountain range. The mountains appear to be karst formations, characterized by their steep, rugged peaks and smooth, rounded shapes. These types of mountains are often found in regions with limestone bedrock and are shaped by processes such as erosion and weathering. The reflection of the mountains in the water adds to the scenic beauty of the landscape.&#34;&#xA;&#xA;&#34;When traveling to this scenic location, it&#39;s important to pay attention to the weather conditions, as the area appears to be prone to fog and mist, especially during sunrise or sunset. Additionally, ensure you have proper footwear for navigating the potentially slippery terrain around the water. Lastly, respect the natural environment by not disturbing the local flora and fauna.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Chat with Multiple Images&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with multiple images input. &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True,&#xA;    attn_implementation=&#39;sdpa&#39;, torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager&#xA;model = model.eval().cuda()&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True)&#xA;&#xA;image1 = Image.open(&#39;image1.jpg&#39;).convert(&#39;RGB&#39;)&#xA;image2 = Image.open(&#39;image2.jpg&#39;).convert(&#39;RGB&#39;)&#xA;question = &#39;Compare image 1 and image 2, tell me about the differences between image 1 and image 2.&#39;&#xA;&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [image1, image2, question]}]&#xA;&#xA;answer = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;In-context Few-shot Learning&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with few-shot input. &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True,&#xA;    attn_implementation=&#39;sdpa&#39;, torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager&#xA;model = model.eval().cuda()&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True)&#xA;&#xA;question = &#34;production date&#34; &#xA;image1 = Image.open(&#39;example1.jpg&#39;).convert(&#39;RGB&#39;)&#xA;answer1 = &#34;2023.08.04&#34;&#xA;image2 = Image.open(&#39;example2.jpg&#39;).convert(&#39;RGB&#39;)&#xA;answer2 = &#34;2007.04.24&#34;&#xA;image_test = Image.open(&#39;test.jpg&#39;).convert(&#39;RGB&#39;)&#xA;&#xA;msgs = [&#xA;    {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [image1, question]}, {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: [answer1]},&#xA;    {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [image2, question]}, {&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: [answer2]},&#xA;    {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [image_test, question]}&#xA;]&#xA;&#xA;answer = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Chat with Video&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with video input. &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, AutoTokenizer&#xA;from decord import VideoReader, cpu    # pip install decord&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True,&#xA;    attn_implementation=&#39;sdpa&#39;, torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager&#xA;model = model.eval().cuda()&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True)&#xA;&#xA;MAX_NUM_FRAMES=64 # if cuda OOM set a smaller number&#xA;&#xA;def encode_video(video_path):&#xA;    def uniform_sample(l, n):&#xA;        gap = len(l) / n&#xA;        idxs = [int(i * gap + gap / 2) for i in range(n)]&#xA;        return [l[i] for i in idxs]&#xA;&#xA;    vr = VideoReader(video_path, ctx=cpu(0))&#xA;    sample_fps = round(vr.get_avg_fps() / 1)  # FPS&#xA;    frame_idx = [i for i in range(0, len(vr), sample_fps)]&#xA;    if len(frame_idx) &amp;gt; MAX_NUM_FRAMES:&#xA;        frame_idx = uniform_sample(frame_idx, MAX_NUM_FRAMES)&#xA;    frames = vr.get_batch(frame_idx).asnumpy()&#xA;    frames = [Image.fromarray(v.astype(&#39;uint8&#39;)) for v in frames]&#xA;    print(&#39;num frames:&#39;, len(frames))&#xA;    return frames&#xA;&#xA;video_path=&#34;video_test.mp4&#34;&#xA;frames = encode_video(video_path)&#xA;question = &#34;Describe the video&#34;&#xA;msgs = [&#xA;    {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: frames + [question]}, &#xA;]&#xA;&#xA;# Set decode params for video&#xA;params = {}&#xA;params[&#34;use_image_id&#34;] = False&#xA;params[&#34;max_slice_nums&#34;] = 2 # use 1 if cuda OOM and video resolution &amp;gt; 448*448&#xA;&#xA;answer = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer,&#xA;    **params&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Speech Conversation&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Model initialization &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import librosa&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True,&#xA;    attn_implementation=&#39;sdpa&#39;, torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager&#xA;model = model.eval().cuda()&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True)&#xA;&#xA;model.init_tts()&#xA;model.tts.float()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h5&gt;Mimick&lt;/h5&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click here to experience the capability of end-to-end audio understanding and generation. &lt;/summary&gt; &#xA; &lt;p&gt;&lt;code&gt;Mimick&lt;/code&gt; task reflects a model&#39;s end-to-end speech modeling capability. The model takes audio input, and outputs an ASR transcription and subsequently reconstructs the original audio with high similarity. The higher the similarity between the reconstructed audio and the original audio, the stronger the model&#39;s foundational capability in end-to-end speech modeling.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mimick_prompt = &#34;Please repeat each user&#39;s speech, including voice style and speech content.&#34;&#xA;audio_input, _ = librosa.load(&#39;xxx.wav&#39;, sr=16000, mono=True)&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [mimick_prompt,audio_input]}]&#xA;res = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True,&#xA;    max_new_tokens=128,&#xA;    use_tts_template=True,&#xA;    temperature=0.3,&#xA;    generate_audio=True,&#xA;    output_audio_path=&#39;output.wav&#39;, # save the tts result to output_audio_path&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h5&gt;General Speech Conversation with Configurable Voices&lt;/h5&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view the Python code for enabling MiniCPM-o 2.6 to interact with you in a specified voice.&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ref_audio, _ = librosa.load(&#39;./assets/voice_01.wav&#39;, sr=16000, mono=True) # load the reference audio&#xA;&#xA;# Audio RolePlay:  # With this mode, model will role-play the character based on the audio prompt.&#xA;sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode=&#39;audio_roleplay&#39;, language=&#39;en&#39;)&#xA;user_question = {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [librosa.load(&#39;xxx.wav&#39;, sr=16000, mono=True)[0]]}&#xA;&#xA;# Audio Assistant: # With this mode, model will speak with the voice in ref_audio as a AI assistant.&#xA;# sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode=&#39;audio_assistant&#39;, language=&#39;en&#39;) &#xA;# user_question = {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [librosa.load(&#39;xxx.wav&#39;, sr=16000, mono=True)[0]]} # Try to ask something!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;msgs = [sys_prompt, user_question]&#xA;res = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True,&#xA;    max_new_tokens=128,&#xA;    use_tts_template=True,&#xA;    generate_audio=True,&#xA;    temperature=0.3,&#xA;    output_audio_path=&#39;result.wav&#39;,&#xA;)&#xA;&#xA;# round two&#xA;history = msgs.append({&#39;role&#39;: &#39;assistant&#39;, &#39;content&#39;: res})&#xA;user_question = {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [librosa.load(&#39;xxx.wav&#39;, sr=16000, mono=True)[0]]}&#xA;msgs = history.append(user_question)&#xA;res = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True,&#xA;    max_new_tokens=128,&#xA;    use_tts_template=True,&#xA;    generate_audio=True,&#xA;    temperature=0.3,&#xA;    output_audio_path=&#39;result_round_2.wav&#39;,&#xA;)&#xA;print(res)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h5&gt;Addressing Various Audio Tasks&lt;/h5&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to show Python code running MiniCPM-o 2.6 with specific audioQA task. &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#39;&#39;&#39;&#xA;Audio Understanding Task Prompt:&#xA;Speech:&#xA;    ASR with ZH(same as AST en2zh): ËØ∑‰ªîÁªÜÂê¨ËøôÊÆµÈü≥È¢ëÁâáÊÆµÔºåÂπ∂Â∞ÜÂÖ∂ÂÜÖÂÆπÈÄêÂ≠óËÆ∞ÂΩï„ÄÇ&#xA;    ASR with EN(same as AST zh2en): Please listen to the audio snippet carefully and transcribe the content.&#xA;    Speaker Analysis: Based on the speaker&#39;s content, speculate on their gender, condition, age range, and health status.&#xA;General Audio:&#xA;    Audio Caption: Summarize the main content of the audio.&#xA;    Sound Scene Tagging: Utilize one keyword to convey the audio&#39;s content or the associated scene.&#xA;&#39;&#39;&#39;&#xA;task_prompt = &#34;\n&#34;&#xA;audio_input, _ = librosa.load(&#39;xxx.wav&#39;, sr=16000, mono=True)&#xA;&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [task_prompt,audio_input]}]&#xA;&#xA;res = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True,&#xA;    max_new_tokens=128,&#xA;    use_tts_template=True,&#xA;    generate_audio=True,&#xA;    temperature=0.3,&#xA;    output_audio_path=&#39;result.wav&#39;,&#xA;)&#xA;print(res)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#39;&#39;&#39;&#xA;Speech Generation Task Prompt:&#xA;    Human Instruction-to-Speech: see https://voxinstruct.github.io/VoxInstruct/&#xA;    Example:&#xA;        # Âú®Êñ∞Èóª‰∏≠Ôºå‰∏Ä‰∏™Âπ¥ËΩªÁî∑ÊÄßÂÖ¥Ëá¥ÂãÉÂãÉÂú∞ËØ¥Ôºö‚ÄúÁ•ùÁ¶è‰∫≤Áà±ÁöÑÁ•ñÂõΩÊØç‰∫≤Áæé‰∏ΩÂØåÂº∫ÔºÅ‚Äù‰ªñÁî®‰ΩéÈü≥Ë∞ÉÂíå‰ΩéÈü≥ÈáèÔºåÊÖ¢ÊÖ¢Âú∞ËØ¥Âá∫‰∫ÜËøôÂè•ËØù„ÄÇ&#xA;        # Delighting in a surprised tone, an adult male with low pitch and low volume comments:&#34;One even gave my little dog a biscuit&#34; This dialogue takes place at a leisurely pace, delivering a sense of excitement and surprise in the context. &#xA;&#xA;    Voice Cloning or Voice Creation: With this mode, model will act like a TTS model. &#xA;&#39;&#39;&#39;&#xA;# Human Instruction-to-Speech:&#xA;task_prompt = &#39;&#39; #Try to make some Human Instruction-to-Speech prompt&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [task_prompt]}] # you can try to use the same audio question. (Voice Creation)&#xA;&#xA;# Voice Cloning mode: With this mode, model will act like a TTS model. &#xA;# sys_prompt = model.get_sys_prompt(ref_audio=ref_audio, mode=&#39;voice_cloning&#39;, language=&#39;en&#39;)&#xA;# text_prompt = f&#34;Please read the text below.&#34;&#xA;# user_question = {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [text_prompt, &#34;content that you want to read&#34;]} # using same voice in sys_prompt to read the text. (Voice Cloning)&#xA;# user_question = {&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: [text_prompt, librosa.load(&#39;xxx.wav&#39;, sr=16000, mono=True)[0]]} # using same voice in sys_prompt to read &#39;xxx.wav&#39;. (Voice Conversion)&#xA;&#xA;msgs = [sys_prompt, user_question]&#xA;res = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True,&#xA;    max_new_tokens=128,&#xA;    use_tts_template=True,&#xA;    generate_audio=True,&#xA;    temperature=0.3,&#xA;    output_audio_path=&#39;result.wav&#39;,&#xA;)&#xA;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Multimodal Live Streaming&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with chat inference. &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import math&#xA;import numpy as np&#xA;from PIL import Image&#xA;from moviepy.editor import VideoFileClip&#xA;import tempfile&#xA;import librosa&#xA;import soundfile as sf&#xA;import torch&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;def get_video_chunk_content(video_path, flatten=True):&#xA;    video = VideoFileClip(video_path)&#xA;    print(&#39;video_duration:&#39;, video.duration)&#xA;    &#xA;    with tempfile.NamedTemporaryFile(suffix=&#34;.wav&#34;, delete=True) as temp_audio_file:&#xA;        temp_audio_file_path = temp_audio_file.name&#xA;        video.audio.write_audiofile(temp_audio_file_path, codec=&#34;pcm_s16le&#34;, fps=16000)&#xA;        audio_np, sr = librosa.load(temp_audio_file_path, sr=16000, mono=True)&#xA;    num_units = math.ceil(video.duration)&#xA;    &#xA;    # 1 frame + 1s audio chunk&#xA;    contents= []&#xA;    for i in range(num_units):&#xA;        frame = video.get_frame(i+1)&#xA;        image = Image.fromarray((frame).astype(np.uint8))&#xA;        audio = audio_np[sr*i:sr*(i+1)]&#xA;        if flatten:&#xA;            contents.extend([&#34;&amp;lt;unit&amp;gt;&#34;, image, audio])&#xA;        else:&#xA;            contents.append([&#34;&amp;lt;unit&amp;gt;&#34;, image, audio])&#xA;    &#xA;    return contents&#xA;&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True,&#xA;    attn_implementation=&#39;sdpa&#39;, torch_dtype=torch.bfloat16)&#xA;model = model.eval().cuda()&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-o-2_6&#39;, trust_remote_code=True)&#xA;&#xA;model.init_tts()&#xA;&#xA;# If you are using an older version of PyTorch, you might encounter this issue &#34;weight_norm_fwd_first_dim_kernel&#34; not implemented for &#39;BFloat16&#39;, Please convert the TTS to float32 type.&#xA;# model.tts.float()&#xA;&#xA;# https://huggingface.co/openbmb/MiniCPM-o-2_6/blob/main/assets/Skiing.mp4&#xA;video_path=&#34;assets/Skiing.mp4&#34;&#xA;sys_msg = model.get_sys_prompt(mode=&#39;omni&#39;, language=&#39;en&#39;)&#xA;# if use voice clone prompt, please set ref_audio&#xA;# ref_audio_path = &#39;/path/to/ref_audio&#39;&#xA;# ref_audio, _ = librosa.load(ref_audio_path, sr=16000, mono=True)&#xA;# sys_msg = model.get_sys_prompt(ref_audio=ref_audio, mode=&#39;omni&#39;, language=&#39;en&#39;)&#xA;&#xA;contents = get_video_chunk_content(video_path)&#xA;msg = {&#34;role&#34;:&#34;user&#34;, &#34;content&#34;: contents}&#xA;msgs = [sys_msg, msg]&#xA;&#xA;# please set generate_audio=True and output_audio_path to save the tts result&#xA;generate_audio = True&#xA;output_audio_path = &#39;output.wav&#39;&#xA;&#xA;res = model.chat(&#xA;    msgs=msgs,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True,&#xA;    temperature=0.5,&#xA;    max_new_tokens=4096,&#xA;    omni_input=True, # please set omni_input=True when omni inference&#xA;    use_tts_template=True,&#xA;    generate_audio=generate_audio,&#xA;    output_audio_path=output_audio_path,&#xA;    max_slice_nums=1,&#xA;    use_image_id=False,&#xA;    return_dict=True&#xA;)&#xA;print(res)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to view Python code running MiniCPM-o 2.6 with streaming inference. &lt;/summary&gt; &#xA; &lt;p&gt;Note: The streaming inference has a slight performance degradation because the audio encoding is not global.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# a new conversation need reset session first, it will reset the kv-cache&#xA;model.reset_session()&#xA;&#xA;contents = get_video_chunk_content(video_path, flatten=False)&#xA;session_id = &#39;123&#39;&#xA;generate_audio = True&#xA;&#xA;# 1. prefill system prompt&#xA;res = model.streaming_prefill(&#xA;    session_id=session_id,&#xA;    msgs=[sys_msg], &#xA;    tokenizer=tokenizer&#xA;)&#xA;&#xA;# 2. prefill video/audio chunks&#xA;for content in contents:&#xA;    msgs = [{&#34;role&#34;:&#34;user&#34;, &#34;content&#34;: content}]&#xA;    res = model.streaming_prefill(&#xA;        session_id=session_id,&#xA;        msgs=msgs, &#xA;        tokenizer=tokenizer&#xA;    )&#xA;&#xA;# 3. generate&#xA;res = model.streaming_generate(&#xA;    session_id=session_id,&#xA;    tokenizer=tokenizer,&#xA;    temperature=0.5,&#xA;    generate_audio=generate_audio&#xA;)&#xA;&#xA;audios = []&#xA;text = &#34;&#34;&#xA;&#xA;if generate_audio:&#xA;    for r in res:&#xA;        audio_wav = r.audio_wav&#xA;        sampling_rate = r.sampling_rate&#xA;        txt = r.text&#xA;&#xA;        audios.append(audio_wav)&#xA;        text += txt&#xA;        &#xA;    res = np.concatenate(audios)&#xA;    sf.write(&#34;output.wav&#34;, res, samplerate=sampling_rate)&#xA;    print(&#34;text:&#34;, text)&#xA;    print(&#34;audio saved to output.wav&#34;)&#xA;else:&#xA;    for r in res:&#xA;        text += r[&#39;text&#39;]&#xA;    print(&#34;text:&#34;, text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Inference on Multiple GPUs&lt;/h3&gt; &#xA;&lt;p&gt;You can run MiniCPM-Llama3-V 2.5 on multiple low VRAM GPUs (12 GB or 16 GB) by distributing the model&#39;s layers across multiple GPUs. Please refer to this &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM-V/raw/main/docs/inference_on_multiple_gpus.md&#34;&gt;tutorial&lt;/a&gt; for detailed instructions on how to load the model and inference using multiple low VRAM GPUs.&lt;/p&gt; &#xA;&lt;h3&gt;Inference on Mac&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Click to view an example, to run MiniCPM-Llama3-V 2.5 on üíª Mac with MPS (Apple silicon or AMD GPUs). &lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# test.py  Need more than 16GB memory.&#xA;import torch&#xA;from PIL import Image&#xA;from transformers import AutoModel, AutoTokenizer&#xA;&#xA;model = AutoModel.from_pretrained(&#39;openbmb/MiniCPM-Llama3-V-2_5&#39;, trust_remote_code=True, low_cpu_mem_usage=True)&#xA;model = model.to(device=&#39;mps&#39;)&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;openbmb/MiniCPM-Llama3-V-2_5&#39;, trust_remote_code=True)&#xA;model.eval()&#xA;&#xA;image = Image.open(&#39;./assets/hk_OCR.jpg&#39;).convert(&#39;RGB&#39;)&#xA;question = &#39;Where is this photo taken?&#39;&#xA;msgs = [{&#39;role&#39;: &#39;user&#39;, &#39;content&#39;: question}]&#xA;&#xA;answer, context, _ = model.chat(&#xA;    image=image,&#xA;    msgs=msgs,&#xA;    context=None,&#xA;    tokenizer=tokenizer,&#xA;    sampling=True&#xA;)&#xA;print(answer)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Run with command:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;PYTORCH_ENABLE_MPS_FALLBACK=1 python test.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Deployment on Mobile Phone&lt;/h3&gt; &#xA;&lt;p&gt;MiniCPM-V 2.0 can be deployed on mobile phones with Android operating systems. üöÄ Click &lt;a href=&#34;https://github.com/OpenBMB/mlc-MiniCPM&#34;&gt;MiniCPM-V 2.0&lt;/a&gt; to install apk.&lt;/p&gt; &#xA;&lt;h3&gt;Efficient Inference with llama.cpp, ollama, vLLM&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/OpenBMB/llama.cpp/tree/minicpmv-main/examples/llava/README-minicpmv2.6.md&#34;&gt;our fork of llama.cpp&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/OpenBMB/ollama/raw/minicpm-v2.6/examples/minicpm-v2.6/README.md&#34;&gt;our fork of ollama&lt;/a&gt; for more detail. This implementation supports smooth inference of 16~18 token/s on iPad (test environmentÔºöiPad Pro + M4).&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; vLLM now officially supports MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0. And you can use our fork to run MiniCPM-o 2.6 for now. Click to see. &lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;For MiniCPM-o 2.6&lt;/p&gt; &#xA;   &lt;ol&gt; &#xA;    &lt;li&gt;Clone our fork of vLLM:&lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/OpenBMB/vllm.git&#xA;cd vllm&#xA;git checkout minicpmo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ol start=&#34;2&#34;&gt; &#xA;    &lt;li&gt;Install vLLM from source:&lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;VLLM_USE_PRECOMPILED=1 pip install --editable . &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ol start=&#34;3&#34;&gt; &#xA;    &lt;li&gt;Run MiniCPM-o 2.6 in the same way as the previous models (shown in the following example).&lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;For previous MiniCPM-V models&lt;/p&gt; &#xA;   &lt;ol&gt; &#xA;    &lt;li&gt;Install vLLM(&amp;gt;=0.5.4):&lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install vllm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ol start=&#34;2&#34;&gt; &#xA;    &lt;li&gt;Install timm: (optional, MiniCPM-V 2.0 need timm)&lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install timm==0.9.10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ol start=&#34;3&#34;&gt; &#xA;    &lt;li&gt;Run the example(for image):&lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer&#xA;from PIL import Image&#xA;from vllm import LLM, SamplingParams&#xA;&#xA;MODEL_NAME = &#34;openbmb/MiniCPM-V-2_6&#34;&#xA;# MODEL_NAME = &#34;openbmb/MiniCPM-O-2_6&#34;&#xA;# Also available for previous models&#xA;# MODEL_NAME = &#34;openbmb/MiniCPM-Llama3-V-2_5&#34;&#xA;# MODEL_NAME = &#34;HwwwH/MiniCPM-V-2&#34;&#xA;&#xA;image = Image.open(&#34;xxx.png&#34;).convert(&#34;RGB&#34;)&#xA;tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)&#xA;llm = LLM(&#xA;    model=MODEL_NAME,&#xA;    trust_remote_code=True,&#xA;    gpu_memory_utilization=1,&#xA;    max_model_len=2048&#xA;)&#xA;&#xA;messages = [{&#xA;    &#34;role&#34;:&#xA;    &#34;user&#34;,&#xA;    &#34;content&#34;:&#xA;    # Number of images&#xA;    &#34;(&amp;lt;image&amp;gt;./&amp;lt;/image&amp;gt;)&#34; + \&#xA;    &#34;\nWhat is the content of this image?&#34; &#xA;}]&#xA;prompt = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True&#xA;)&#xA;&#xA;# Single Inference&#xA;inputs = {&#xA;    &#34;prompt&#34;: prompt,&#xA;    &#34;multi_modal_data&#34;: {&#xA;        &#34;image&#34;: image&#xA;        # Multi images, the number of images should be equal to that of `(&amp;lt;image&amp;gt;./&amp;lt;/image&amp;gt;)`&#xA;        # &#34;image&#34;: [image, image] &#xA;    },&#xA;}&#xA;# Batch Inference&#xA;# inputs = [{&#xA;#     &#34;prompt&#34;: prompt,&#xA;#     &#34;multi_modal_data&#34;: {&#xA;#         &#34;image&#34;: image&#xA;#     },&#xA;# } for _ in 2]&#xA;&#xA;&#xA;# 2.6&#xA;stop_tokens = [&#39;&amp;lt;|im_end|&amp;gt;&#39;, &#39;&amp;lt;|endoftext|&amp;gt;&#39;]&#xA;stop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]&#xA;# 2.0&#xA;# stop_token_ids = [tokenizer.eos_id]&#xA;# 2.5&#xA;# stop_token_ids = [tokenizer.eos_id, tokenizer.eot_id]&#xA;&#xA;sampling_params = SamplingParams(&#xA;    stop_token_ids=stop_token_ids, &#xA;    use_beam_search=True,&#xA;    temperature=0, &#xA;    best_of=3,&#xA;    max_tokens=1024&#xA;)&#xA;&#xA;outputs = llm.generate(inputs, sampling_params=sampling_params)&#xA;&#xA;print(outputs[0].outputs[0].text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;ol start=&#34;4&#34;&gt; &#xA;    &lt;li&gt;click &lt;a href=&#34;https://modelbest.feishu.cn/wiki/C2BWw4ZP0iCDy7kkCPCcX2BHnOf?from=from_copylink&#34;&gt;here&lt;/a&gt; if you want to use it with &lt;em&gt;video&lt;/em&gt;, or get more details about &lt;code&gt;vLLM&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;/ol&gt; &lt;/li&gt;&#xA; &lt;/ol&gt;&#xA;&lt;/details&gt;   &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;h3&gt;Simple Fine-tuning &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We support simple fine-tuning with Hugging Face for MiniCPM-o 2.6, MiniCPM-V 2.6, MiniCPM-Llama3-V 2.5 and MiniCPM-V 2.0.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/finetune/readme.md&#34;&gt;Reference Document&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;With LLaMA-Factory &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We support fine-tuning MiniCPM-o-2.6 and MiniCPM-V 2.6 with the LLaMA-Factory framework. LLaMA-Factory provides a solution for flexibly customizing the fine-tuning (Lora/Full/Qlora) of 200+ LLMs without the need for coding through the built-in web UI LLaMABoard. It supports various training methods like sft/ppo/dpo/kto and advanced algorithms like Galore/BAdam/LLaMA-Pro/Pissa/LongLoRA.&lt;/p&gt; &#xA;&lt;p&gt;Best Practices: &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/docs/llamafactory_train_and_infer.md&#34;&gt;MiniCPM-o-2.6 | MiniCPM-V-2.6&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;With the SWIFT Framework &#xA; &lt;!-- omit in toc --&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We now support MiniCPM-V series fine-tuning with the SWIFT framework. SWIFT supports training, inference, evaluation and deployment of nearly 200 LLMs and MLLMs . It supports the lightweight training solutions provided by PEFT and a complete Adapters Library including techniques such as NEFTune, LoRA+ and LLaMA-PRO.&lt;/p&gt; &#xA;&lt;p&gt;Best PracticesÔºö&lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;MiniCPM-V 1.0&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift/raw/main/docs/source/Multi-Modal/minicpm-v-2%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.md&#34;&gt;MiniCPM-V 2.0&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/ms-swift/issues/1613&#34;&gt;MiniCPM-V 2.6&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;FAQs&lt;/h2&gt; &#xA;&lt;p&gt;Click here to view the &lt;a href=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/docs/faqs.md&#34;&gt;FAQs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;p&gt;As an experimental trial, we find MiniCPM-o 2.6 has notable limitations worth further investigation and improvement.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Unstable speech output.&lt;/strong&gt; The speech generation can be flawed with noisy background and unmeaningful sound.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repeated response.&lt;/strong&gt; The model tends to repeat its response when encounting similar consecutive user queries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-latency on Web Demo.&lt;/strong&gt; Users may experience unusual high-latency when using web demo hosted on overseas servers. We recommend deploying the demo locally or with good network connections.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Model License &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;This repository is released under the &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/raw/main/LICENSE&#34;&gt;Apache-2.0&lt;/a&gt; License.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The usage of MiniCPM-o/V model weights must strictly follow &lt;a href=&#34;https://github.com/OpenBMB/MiniCPM/raw/main/MiniCPM%20Model%20License.md&#34;&gt;MiniCPM Model License.md&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The models and weights of MiniCPM are completely free for academic research. after filling out a &lt;a href=&#34;https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g&#34;&gt;&#34;questionnaire&#34;&lt;/a&gt; for registration, are also available for free commercial use.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Statement &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;As MLLMs, MiniCPM-o/V models generate contents by learning a large amount of multimodal corpora, but they cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-o/V models does not represent the views and positions of the model developers&lt;/p&gt; &#xA;&lt;p&gt;We will not be liable for any problems arising from the use of MiniCPM-o/V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.&lt;/p&gt; &#xA;&lt;h2&gt;Institutions &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;This project is developed by the following institutions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/thunlp.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://nlp.csai.tsinghua.edu.cn/&#34;&gt;THUNLP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/modelbest.png&#34; width=&#34;28px&#34;&gt; &lt;a href=&#34;https://modelbest.cn/&#34;&gt;ModelBest&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üåü Star History &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OpenBMB/MiniCPM-o/main/assets/star_history.svg?sanitize=true&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt;  &#xA;&lt;/table&gt; &#xA;&lt;!-- &lt;picture&gt;&#xA;  &lt;source&#xA;    media=&#34;(prefers-color-scheme: dark)&#34;&#xA;    srcset=&#34;&#xA;      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;type=Date&amp;theme=dark&#xA;    &#34;&#xA;  /&gt;&#xA;  &lt;source&#xA;    media=&#34;(prefers-color-scheme: light)&#34;&#xA;    srcset=&#34;&#xA;      https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;type=Date&#xA;    &#34;&#xA;  /&gt;&#xA;  &lt;img&#xA;    alt=&#34;Star History Chart&#34;&#xA;    src=&#34;https://api.star-history.com/svg?repos=OpenBMB/MiniCPM-o&amp;type=Date&#34;&#xA;  /&gt;&#xA;&lt;/picture&gt; --&gt; &#xA;&lt;h2&gt;Key Techniques and Other Multimodal Projects &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;üëè Welcome to explore key techniques of MiniCPM-o/V and other multimodal projects of our team:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OpenBMB/VisCPM/tree/main&#34;&gt;VisCPM&lt;/a&gt; | &lt;a href=&#34;https://github.com/RLHF-V/RLHF-V&#34;&gt;RLHF-V&lt;/a&gt; | &lt;a href=&#34;https://github.com/thunlp/LLaVA-UHD&#34;&gt;LLaVA-UHD&lt;/a&gt; | &lt;a href=&#34;https://github.com/RLHF-V/RLAIF-V&#34;&gt;RLAIF-V&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation &#xA; &lt;!-- omit in toc --&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If you find our model/code/paper helpful, please consider citing our papers üìù and staring us ‚≠êÔ∏èÔºÅ&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bib&#34;&gt;@article{yao2024minicpm,&#xA;  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},&#xA;  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},&#xA;  journal={arXiv preprint arXiv:2408.01800},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>