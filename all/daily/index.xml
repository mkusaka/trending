<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-16T01:28:32Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>NVlabs/Sana</title>
    <updated>2025-01-16T01:28:32Z</updated>
    <id>tag:github.com,2025-01-16:/NVlabs/Sana</id>
    <link href="https://github.com/NVlabs/Sana" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; style=&#34;border-radius: 10px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/logo.png&#34; width=&#34;35%&#34; alt=&#34;logo&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;‚ö°Ô∏èSana: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://nvlabs.github.io/Sana/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Project&amp;amp;message=Github&amp;amp;color=blue&amp;amp;logo=github-pages&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://hanlab.mit.edu/projects/sana/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Page&amp;amp;message=MIT&amp;amp;color=darkred&amp;amp;logo=github-pages&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://arxiv.org/abs/2410.10629&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Arxiv&amp;amp;message=Sana&amp;amp;color=red&amp;amp;logo=arxiv&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://nv-sana.mit.edu/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Demo:8x3090&amp;amp;message=MIT&amp;amp;color=yellow&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://replicate.com/chenxwh/sana&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=API:H100&amp;amp;message=Replicate&amp;amp;color=pink&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA; &lt;a href=&#34;https://discord.gg/rde6eaE5Ta&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Discuss&amp;amp;message=Discord&amp;amp;color=purple&amp;amp;logo=discord&#34;&gt;&lt;/a&gt; ‚ÄÇ &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34; border-raduis=&#34;10px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/Sana.jpg&#34; width=&#34;90%&#34; alt=&#34;teaser_page1&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üí° Introduction&lt;/h2&gt; &#xA;&lt;p&gt;We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096 √ó 4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include:&lt;/p&gt; &#xA;&lt;p&gt;(1) &lt;a href=&#34;https://hanlab.mit.edu/projects/dc-ae&#34;&gt;&lt;strong&gt;DC-AE&lt;/strong&gt;&lt;/a&gt;: unlike traditional AEs, which compress images only 8√ó, we trained an AE that can compress images 32√ó, effectively reducing the number of latent tokens. &lt;br&gt; (2) &lt;strong&gt;Linear DiT&lt;/strong&gt;: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. &lt;br&gt; (3) &lt;strong&gt;Decoder-only text encoder&lt;/strong&gt;: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. &lt;br&gt; (4) &lt;strong&gt;Efficient training and sampling&lt;/strong&gt;: we propose &lt;strong&gt;Flow-DPM-Solver&lt;/strong&gt; to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence.&lt;/p&gt; &#xA;&lt;p&gt;As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024 √ó 1024 resolution image. Sana enables content creation at low cost.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; border-raduis=&#34;10px&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/model-incremental.jpg&#34; width=&#34;90%&#34; alt=&#34;teaser_page2&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üî•üî• News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/12] DC-AE tiling makes Sana-4K inferences 4096x4096px images within 22GB GPU memory.&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md#-3-4k-models&#34;&gt;[Guidance]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/11] Sana code-base license changed to Apache 2.0.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/10] Inference Sana with 8bit quantization.&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/8bit_sana.md#quantization&#34;&gt;[Guidance]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/8] 4K resolution &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Sana models&lt;/a&gt; is supported in &lt;a href=&#34;https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels&#34;&gt;Sana-ComfyUI&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/ComfyUI/Sana_FlowEuler_4K.json&#34;&gt;work flow&lt;/a&gt; is also prepared. &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/ComfyUI/comfyui.md&#34;&gt;[4K guidance]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/8] 1.6B 4K resolution &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Sana models&lt;/a&gt; are released: &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_4Kpx_BF16&#34;&gt;[BF16 pth]&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_4Kpx_BF16_diffusers&#34;&gt;[BF16 diffusers]&lt;/a&gt;. üöÄ Get your 4096x4096 resolution images within 20 seconds! Find more samples in &lt;a href=&#34;https://nvlabs.github.io/Sana/&#34;&gt;Sana page&lt;/a&gt;. Thanks &lt;a href=&#34;https://github.com/Fanghua-Yu/SUPIR&#34;&gt;SUPIR&lt;/a&gt; for their wonderful work and support.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/2] Bug in the &lt;code&gt;diffusers&lt;/code&gt; pipeline is solved. &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/10431&#34;&gt;Solved PR&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2025/1/2] 2K resolution &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Sana models&lt;/a&gt; is supported in &lt;a href=&#34;https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels&#34;&gt;Sana-ComfyUI&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/ComfyUI/Sana_FlowEuler_2K.json&#34;&gt;work flow&lt;/a&gt; is also prepared.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/20] 1.6B 2K resolution &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Sana models&lt;/a&gt; are released: &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_2Kpx_BF16&#34;&gt;[BF16 pth]&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_2Kpx_BF16_diffusers&#34;&gt;[BF16 diffusers]&lt;/a&gt;. üöÄ Get your 2K resolution images within 4 seconds! Find more samples in &lt;a href=&#34;https://nvlabs.github.io/Sana/&#34;&gt;Sana page&lt;/a&gt;. Thanks &lt;a href=&#34;https://github.com/Fanghua-Yu/SUPIR&#34;&gt;SUPIR&lt;/a&gt; for their wonderful work and support.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/18] &lt;code&gt;diffusers&lt;/code&gt; supports Sana-LoRA fine-tuning! Sana-LoRA&#39;s training and convergence speed is supper fast. &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/sana_lora_dreambooth.md&#34;&gt;[Guidance]&lt;/a&gt; or &lt;a href=&#34;https://github.com/huggingface/diffusers/raw/main/examples/dreambooth/README_sana.md&#34;&gt;[diffusers docs]&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/13] &lt;code&gt;diffusers&lt;/code&gt; has Sana! &lt;a href=&#34;https://huggingface.co/collections/Efficient-Large-Model/sana-673efba2a57ed99843f11f9e&#34;&gt;All Sana models in diffusers safetensors&lt;/a&gt; are released and diffusers pipeline &lt;code&gt;SanaPipeline&lt;/code&gt;, &lt;code&gt;SanaPAGPipeline&lt;/code&gt;, &lt;code&gt;DPMSolverMultistepScheduler(with FlowMatching)&lt;/code&gt; are all supported now. We prepare a &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Model Card&lt;/a&gt; for you to choose.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/10] 1.6B BF16 &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_BF16&#34;&gt;Sana model&lt;/a&gt; is released for stable fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;(üî• New) [2024/12/9] We release the &lt;a href=&#34;https://github.com/Efficient-Large-Model/ComfyUI_ExtraModels&#34;&gt;ComfyUI node&lt;/a&gt; for Sana. &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/ComfyUI/comfyui.md&#34;&gt;[Guidance]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] All multi-linguistic (Emoji &amp;amp; Chinese &amp;amp; English) SFT models are released: &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_512px_MultiLing&#34;&gt;1.6B-512px&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_MultiLing&#34;&gt;1.6B-1024px&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_600M_512px&#34;&gt;600M-512px&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px&#34;&gt;600M-1024px&lt;/a&gt;. The metric performance is shown &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#performance&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] Sana Replicate API is launching at &lt;a href=&#34;https://replicate.com/chenxwh/sana&#34;&gt;Sana-API&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] 1.6B &lt;a href=&#34;https://huggingface.co/collections/Efficient-Large-Model/sana-673efba2a57ed99843f11f9e&#34;&gt;Sana models&lt;/a&gt; are released.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] Training &amp;amp; Inference &amp;amp; Metrics code are released.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ [2024/11] Working on &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/9982&#34;&gt;&lt;code&gt;diffusers&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] &lt;a href=&#34;https://nv-sana.mit.edu/&#34;&gt;Demo&lt;/a&gt; is released.&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] &lt;a href=&#34;https://github.com/mit-han-lab/efficientvit/raw/master/applications/dc_ae/README.md&#34;&gt;DC-AE Code&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/collections/mit-han-lab/dc-ae-670085b9400ad7197bb1009b&#34;&gt;weights&lt;/a&gt; are released!&lt;/li&gt; &#xA; &lt;li&gt;[2024/10] &lt;a href=&#34;https://arxiv.org/abs/2410.10629&#34;&gt;Paper&lt;/a&gt; is on Arxiv!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Methods (1024x1024)&lt;/th&gt; &#xA;   &lt;th&gt;Throughput (samples/s)&lt;/th&gt; &#xA;   &lt;th&gt;Latency (s)&lt;/th&gt; &#xA;   &lt;th&gt;Params (B)&lt;/th&gt; &#xA;   &lt;th&gt;Speedup&lt;/th&gt; &#xA;   &lt;th&gt;FID üëá&lt;/th&gt; &#xA;   &lt;th&gt;CLIP üëÜ&lt;/th&gt; &#xA;   &lt;th&gt;GenEval üëÜ&lt;/th&gt; &#xA;   &lt;th&gt;DPG üëÜ&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FLUX-dev&lt;/td&gt; &#xA;   &lt;td&gt;0.04&lt;/td&gt; &#xA;   &lt;td&gt;23.0&lt;/td&gt; &#xA;   &lt;td&gt;12.0&lt;/td&gt; &#xA;   &lt;td&gt;1.0√ó&lt;/td&gt; &#xA;   &lt;td&gt;10.15&lt;/td&gt; &#xA;   &lt;td&gt;27.47&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;0.67&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;84.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Sana-0.6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.7&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;   &lt;td&gt;0.6&lt;/td&gt; &#xA;   &lt;td&gt;39.5√ó&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;5.81&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;28.36&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;83.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_600M_1024px&#34;&gt;Sana-0.6B-MultiLing&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.7&lt;/td&gt; &#xA;   &lt;td&gt;0.9&lt;/td&gt; &#xA;   &lt;td&gt;0.6&lt;/td&gt; &#xA;   &lt;td&gt;39.5√ó&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;5.61&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;u&gt;28.80&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;u&gt;0.68&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;84.2&lt;/em&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Sana-1.6B&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;23.3√ó&lt;/td&gt; &#xA;   &lt;td&gt;&lt;u&gt;5.76&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;28.67&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;84.8&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/Efficient-Large-Model/Sana_1600M_1024px_MultiLing&#34;&gt;Sana-1.6B-MultiLing&lt;/a&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.0&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;23.3√ó&lt;/td&gt; &#xA;   &lt;td&gt;5.92&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;28.94&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;0.69&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;u&gt;84.5&lt;/u&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Click to show all&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Methods&lt;/th&gt; &#xA;    &lt;th&gt;Throughput (samples/s)&lt;/th&gt; &#xA;    &lt;th&gt;Latency (s)&lt;/th&gt; &#xA;    &lt;th&gt;Params (B)&lt;/th&gt; &#xA;    &lt;th&gt;Speedup&lt;/th&gt; &#xA;    &lt;th&gt;FID üëÜ&lt;/th&gt; &#xA;    &lt;th&gt;CLIP üëÜ&lt;/th&gt; &#xA;    &lt;th&gt;GenEval üëÜ&lt;/th&gt; &#xA;    &lt;th&gt;DPG üëÜ&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;em&gt;&lt;strong&gt;512 √ó 512 resolution&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PixArt-Œ±&lt;/td&gt; &#xA;    &lt;td&gt;1.5&lt;/td&gt; &#xA;    &lt;td&gt;1.2&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;1.0√ó&lt;/td&gt; &#xA;    &lt;td&gt;6.14&lt;/td&gt; &#xA;    &lt;td&gt;27.55&lt;/td&gt; &#xA;    &lt;td&gt;0.48&lt;/td&gt; &#xA;    &lt;td&gt;71.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PixArt-Œ£&lt;/td&gt; &#xA;    &lt;td&gt;1.5&lt;/td&gt; &#xA;    &lt;td&gt;1.2&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;1.0√ó&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;6.34&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;27.62&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;0.52&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;79.5&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Sana-0.6B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;6.7&lt;/td&gt; &#xA;    &lt;td&gt;0.8&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;5.0√ó&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;5.67&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;27.92&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;0.64&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;84.3&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Sana-1.6B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;3.8&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;1.6&lt;/td&gt; &#xA;    &lt;td&gt;2.5√ó&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;5.16&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;28.19&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;0.66&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;85.5&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;em&gt;&lt;strong&gt;1024 √ó 1024 resolution&lt;/strong&gt;&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;LUMINA-Next&lt;/td&gt; &#xA;    &lt;td&gt;0.12&lt;/td&gt; &#xA;    &lt;td&gt;9.1&lt;/td&gt; &#xA;    &lt;td&gt;2.0&lt;/td&gt; &#xA;    &lt;td&gt;2.8√ó&lt;/td&gt; &#xA;    &lt;td&gt;7.58&lt;/td&gt; &#xA;    &lt;td&gt;26.84&lt;/td&gt; &#xA;    &lt;td&gt;0.46&lt;/td&gt; &#xA;    &lt;td&gt;74.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;SDXL&lt;/td&gt; &#xA;    &lt;td&gt;0.15&lt;/td&gt; &#xA;    &lt;td&gt;6.5&lt;/td&gt; &#xA;    &lt;td&gt;2.6&lt;/td&gt; &#xA;    &lt;td&gt;3.5√ó&lt;/td&gt; &#xA;    &lt;td&gt;6.63&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;29.03&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;0.55&lt;/td&gt; &#xA;    &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PlayGroundv2.5&lt;/td&gt; &#xA;    &lt;td&gt;0.21&lt;/td&gt; &#xA;    &lt;td&gt;5.3&lt;/td&gt; &#xA;    &lt;td&gt;2.6&lt;/td&gt; &#xA;    &lt;td&gt;4.9√ó&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;6.09&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;29.13&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;0.56&lt;/td&gt; &#xA;    &lt;td&gt;75.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Hunyuan-DiT&lt;/td&gt; &#xA;    &lt;td&gt;0.05&lt;/td&gt; &#xA;    &lt;td&gt;18.2&lt;/td&gt; &#xA;    &lt;td&gt;1.5&lt;/td&gt; &#xA;    &lt;td&gt;1.2√ó&lt;/td&gt; &#xA;    &lt;td&gt;6.54&lt;/td&gt; &#xA;    &lt;td&gt;28.19&lt;/td&gt; &#xA;    &lt;td&gt;0.63&lt;/td&gt; &#xA;    &lt;td&gt;78.9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;PixArt-Œ£&lt;/td&gt; &#xA;    &lt;td&gt;0.4&lt;/td&gt; &#xA;    &lt;td&gt;2.7&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;9.3√ó&lt;/td&gt; &#xA;    &lt;td&gt;6.15&lt;/td&gt; &#xA;    &lt;td&gt;28.26&lt;/td&gt; &#xA;    &lt;td&gt;0.54&lt;/td&gt; &#xA;    &lt;td&gt;80.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;DALLE3&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;0.67&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;83.5&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;SD3-medium&lt;/td&gt; &#xA;    &lt;td&gt;0.28&lt;/td&gt; &#xA;    &lt;td&gt;4.4&lt;/td&gt; &#xA;    &lt;td&gt;2.0&lt;/td&gt; &#xA;    &lt;td&gt;6.5√ó&lt;/td&gt; &#xA;    &lt;td&gt;11.92&lt;/td&gt; &#xA;    &lt;td&gt;27.83&lt;/td&gt; &#xA;    &lt;td&gt;0.62&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;84.1&lt;/u&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;FLUX-dev&lt;/td&gt; &#xA;    &lt;td&gt;0.04&lt;/td&gt; &#xA;    &lt;td&gt;23.0&lt;/td&gt; &#xA;    &lt;td&gt;12.0&lt;/td&gt; &#xA;    &lt;td&gt;1.0√ó&lt;/td&gt; &#xA;    &lt;td&gt;10.15&lt;/td&gt; &#xA;    &lt;td&gt;27.47&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;0.67&lt;/em&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;em&gt;84.0&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;FLUX-schnell&lt;/td&gt; &#xA;    &lt;td&gt;0.5&lt;/td&gt; &#xA;    &lt;td&gt;2.1&lt;/td&gt; &#xA;    &lt;td&gt;12.0&lt;/td&gt; &#xA;    &lt;td&gt;11.6√ó&lt;/td&gt; &#xA;    &lt;td&gt;7.94&lt;/td&gt; &#xA;    &lt;td&gt;28.14&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;0.71&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;84.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Sana-0.6B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;1.7&lt;/td&gt; &#xA;    &lt;td&gt;0.9&lt;/td&gt; &#xA;    &lt;td&gt;0.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;39.5√ó&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;5.81&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;28.36&lt;/td&gt; &#xA;    &lt;td&gt;0.64&lt;/td&gt; &#xA;    &lt;td&gt;83.6&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;strong&gt;Sana-1.6B&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;1.0&lt;/td&gt; &#xA;    &lt;td&gt;1.2&lt;/td&gt; &#xA;    &lt;td&gt;1.6&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;23.3√ó&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;5.76&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;28.67&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;u&gt;0.66&lt;/u&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;84.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#-1-dependencies-and-installation&#34;&gt;Env&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#-2-how-to-play-with-sana-inference&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Model Zoo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#-3-how-to-train-sana&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#-4-metric-toolkit&#34;&gt;Testing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#to-do-list&#34;&gt;TODO&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/#bibtex&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üîß 1. Dependencies and Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.10.0 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 2.0.1+cu12.1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/NVlabs/Sana.git&#xA;cd Sana&#xA;&#xA;./environment_setup.sh sana&#xA;# or you can install each components step by step following environment_setup.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üíª 2. How to Play with Sana (Inference)&lt;/h1&gt; &#xA;&lt;h2&gt;üí∞Hardware requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;9GB VRAM is required for 0.6B model and 12GB VRAM for 1.6B model. Our later quantization version will require less than 8GB for inference.&lt;/li&gt; &#xA; &lt;li&gt;All the tests are done on A100 GPUs. Different GPU version may be different.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîõ Choose your model: &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Model card&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;üîõ Quick start with &lt;a href=&#34;https://www.gradio.app/guides/quickstart&#34;&gt;Gradio&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# official online demo&#xA;DEMO_PORT=15432 \&#xA;python app/app_sana.py \&#xA;    --share \&#xA;    --config=configs/sana_config/1024ms/Sana_1600M_img1024.yaml \&#xA;    --model_path=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \&#xA;    --image_size=1024&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;1. How to use &lt;code&gt;SanaPipeline&lt;/code&gt; with &lt;code&gt;üß®diffusers&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Upgrade your &lt;code&gt;diffusers&amp;gt;=0.32.0.dev&lt;/code&gt; to make the &lt;code&gt;SanaPipeline&lt;/code&gt; and &lt;code&gt;SanaPAGPipeline&lt;/code&gt; available!&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/diffusers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Make sure to specify &lt;code&gt;pipe.transformer&lt;/code&gt; to default &lt;code&gt;torch_dtype&lt;/code&gt; and &lt;code&gt;variant&lt;/code&gt; according to &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/model_zoo.md&#34;&gt;Model Card&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;p&gt;Set &lt;code&gt;pipe.text_encoder&lt;/code&gt; to BF16 and &lt;code&gt;pipe.vae&lt;/code&gt; to FP32 or BF16. For more info, &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/sana#sanapipeline&#34;&gt;docs&lt;/a&gt; are here.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# run `pip install git+https://github.com/huggingface/diffusers` before use Sana in diffusers&#xA;import torch&#xA;from diffusers import SanaPipeline&#xA;&#xA;pipe = SanaPipeline.from_pretrained(&#xA;    &#34;Efficient-Large-Model/Sana_1600M_1024px_BF16_diffusers&#34;,&#xA;    variant=&#34;bf16&#34;,&#xA;    torch_dtype=torch.bfloat16,&#xA;)&#xA;pipe.to(&#34;cuda&#34;)&#xA;&#xA;pipe.vae.to(torch.bfloat16)&#xA;pipe.text_encoder.to(torch.bfloat16)&#xA;&#xA;prompt = &#39;a cyberpunk cat with a neon sign that says &#34;Sana&#34;&#39;&#xA;image = pipe(&#xA;    prompt=prompt,&#xA;    height=1024,&#xA;    width=1024,&#xA;    guidance_scale=4.5,&#xA;    num_inference_steps=20,&#xA;    generator=torch.Generator(device=&#34;cuda&#34;).manual_seed(42),&#xA;)[0]&#xA;&#xA;image[0].save(&#34;sana.png&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. How to use &lt;code&gt;SanaPAGPipeline&lt;/code&gt; with &lt;code&gt;üß®diffusers&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# run `pip install git+https://github.com/huggingface/diffusers` before use Sana in diffusers&#xA;import torch&#xA;from diffusers import SanaPAGPipeline&#xA;&#xA;pipe = SanaPAGPipeline.from_pretrained(&#xA;  &#34;Efficient-Large-Model/Sana_1600M_1024px_diffusers&#34;,&#xA;  variant=&#34;fp16&#34;,&#xA;  torch_dtype=torch.float16,&#xA;  pag_applied_layers=&#34;transformer_blocks.8&#34;,&#xA;)&#xA;pipe.to(&#34;cuda&#34;)&#xA;&#xA;pipe.text_encoder.to(torch.bfloat16)&#xA;pipe.vae.to(torch.bfloat16)&#xA;&#xA;prompt = &#39;a cyberpunk cat with a neon sign that says &#34;Sana&#34;&#39;&#xA;image = pipe(&#xA;    prompt=prompt,&#xA;    guidance_scale=5.0,&#xA;    pag_scale=2.0,&#xA;    num_inference_steps=20,&#xA;    generator=torch.Generator(device=&#34;cuda&#34;).manual_seed(42),&#xA;)[0]&#xA;image[0].save(&#39;sana.png&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;3. How to use Sana in this repo&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from app.sana_pipeline import SanaPipeline&#xA;from torchvision.utils import save_image&#xA;&#xA;device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)&#xA;generator = torch.Generator(device=device).manual_seed(42)&#xA;&#xA;sana = SanaPipeline(&#34;configs/sana_config/1024ms/Sana_1600M_img1024.yaml&#34;)&#xA;sana.from_pretrained(&#34;hf://Efficient-Large-Model/Sana_1600M_1024px_BF16/checkpoints/Sana_1600M_1024px_BF16.pth&#34;)&#xA;prompt = &#39;a cyberpunk cat with a neon sign that says &#34;Sana&#34;&#39;&#xA;&#xA;image = sana(&#xA;    prompt=prompt,&#xA;    height=1024,&#xA;    width=1024,&#xA;    guidance_scale=5.0,&#xA;    pag_guidance_scale=2.0,&#xA;    num_inference_steps=18,&#xA;    generator=generator,&#xA;)&#xA;save_image(image, &#39;output/sana.png&#39;, nrow=1, normalize=True, value_range=(-1, 1))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt;4. Run Sana (Inference) with Docker&lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code&gt;# Pull related models&#xA;huggingface-cli download google/gemma-2b-it&#xA;huggingface-cli download google/shieldgemma-2b&#xA;huggingface-cli download mit-han-lab/dc-ae-f32c32-sana-1.0&#xA;huggingface-cli download Efficient-Large-Model/Sana_1600M_1024px&#xA;&#xA;# Run with docker&#xA;docker build . -t sana&#xA;docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \&#xA;    -v ~/.cache:/root/.cache \&#xA;    sana&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üîõ Run inference with TXT or JSON files&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run samples in a txt file&#xA;python scripts/inference.py \&#xA;      --config=configs/sana_config/1024ms/Sana_1600M_img1024.yaml \&#xA;      --model_path=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \&#xA;      --txt_file=asset/samples_mini.txt&#xA;&#xA;# Run samples in a json file&#xA;python scripts/inference.py \&#xA;      --config=configs/sana_config/1024ms/Sana_1600M_img1024.yaml \&#xA;      --model_path=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \&#xA;      --json_file=asset/samples_mini.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where each line of &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/samples_mini.txt&#34;&gt;&lt;code&gt;asset/samples_mini.txt&lt;/code&gt;&lt;/a&gt; contains a prompt to generate&lt;/p&gt; &#xA;&lt;h1&gt;üî• 3. How to Train Sana&lt;/h1&gt; &#xA;&lt;h2&gt;üí∞Hardware requirement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;32GB VRAM is required for both 0.6B and 1.6B model&#39;s training&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;1). Train with image-text pairs in directory&lt;/h3&gt; &#xA;&lt;p&gt;We provide a training example here and you can also select your desired config file from &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/configs/sana_config&#34;&gt;config files dir&lt;/a&gt; based on your data structure.&lt;/p&gt; &#xA;&lt;p&gt;To launch Sana training, you will first need to prepare data in the following formats. &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/example_data&#34;&gt;Here&lt;/a&gt; is an example for the data structure for reference.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;asset/example_data&#xA;‚îú‚îÄ‚îÄ AAA.txt&#xA;‚îú‚îÄ‚îÄ AAA.png&#xA;‚îú‚îÄ‚îÄ BCC.txt&#xA;‚îú‚îÄ‚îÄ BCC.png&#xA;‚îú‚îÄ‚îÄ ......&#xA;‚îú‚îÄ‚îÄ CCC.txt&#xA;‚îî‚îÄ‚îÄ CCC.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then Sana&#39;s training can be launched via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example of training Sana 0.6B with 512x512 resolution from scratch&#xA;bash train_scripts/train.sh \&#xA;  configs/sana_config/512ms/Sana_600M_img512.yaml \&#xA;  --data.data_dir=&#34;[asset/example_data]&#34; \&#xA;  --data.type=SanaImgDataset \&#xA;  --model.multi_scale=false \&#xA;  --train.train_batch_size=32&#xA;&#xA;# Example of fine-tuning Sana 1.6B with 1024x1024 resolution&#xA;bash train_scripts/train.sh \&#xA;  configs/sana_config/1024ms/Sana_1600M_img1024.yaml \&#xA;  --data.data_dir=&#34;[asset/example_data]&#34; \&#xA;  --data.type=SanaImgDataset \&#xA;  --model.load_from=hf://Efficient-Large-Model/Sana_1600M_1024px/checkpoints/Sana_1600M_1024px.pth \&#xA;  --model.multi_scale=false \&#xA;  --train.train_batch_size=8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2). Train with image-text pairs in directory&lt;/h3&gt; &#xA;&lt;p&gt;We also provide conversion scripts to convert your data to the required format. You can refer to the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/data_conversion_scripts&#34;&gt;data conversion scripts&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python tools/convert_ImgDataset_to_WebDatasetMS_format.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then Sana&#39;s training can be launched via&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Example of training Sana 0.6B with 512x512 resolution from scratch&#xA;bash train_scripts/train.sh \&#xA;  configs/sana_config/512ms/Sana_600M_img512.yaml \&#xA;  --data.data_dir=&#34;[asset/example_data_tar]&#34; \&#xA;  --data.type=SanaWebDatasetMS \&#xA;  --model.multi_scale=true \&#xA;  --train.train_batch_size=32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üíª 4. Metric toolkit&lt;/h1&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/Sana/main/asset/docs/metrics_toolkit.md&#34;&gt;Toolkit Manual&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;üí™To-Do List&lt;/h1&gt; &#xA;&lt;p&gt;We will try our best to release&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[‚úÖ] Training code&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] Inference code&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] Model zoo&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] ComfyUI&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] DC-AE Diffusers&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] Sana merged in Diffusers(&lt;a href=&#34;https://github.com/huggingface/diffusers/pull/9982&#34;&gt;https://github.com/huggingface/diffusers/pull/9982&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] LoRA training by &lt;a href=&#34;https://github.com/sayakpaul&#34;&gt;@paul&lt;/a&gt;(&lt;code&gt;diffusers&lt;/code&gt;: &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/10234&#34;&gt;https://github.com/huggingface/diffusers/pull/10234&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;[‚úÖ] 2K/4K resolution models.(Thanks &lt;a href=&#34;https://github.com/Fanghua-Yu/SUPIR&#34;&gt;@SUPIR&lt;/a&gt; to provide a 4K super-resolution model)&lt;/li&gt; &#xA; &lt;li&gt;[üíª] ControlNet (train &amp;amp; inference &amp;amp; models)&lt;/li&gt; &#xA; &lt;li&gt;[üíª] 8bit / 4bit Laptop development&lt;/li&gt; &#xA; &lt;li&gt;[üíª] Larger model size&lt;/li&gt; &#xA; &lt;li&gt;[üíª] Better re-construction F32/F64 VAEs.&lt;/li&gt; &#xA; &lt;li&gt;[üíª] &lt;strong&gt;Sana1.5 (Focus on: Human body / Human face / Text rendering / Realism / Efficiency)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;ü§óAcknowledgements&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-Œ±&lt;/a&gt;, &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-sigma&#34;&gt;PixArt-Œ£&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/efficientvit&#34;&gt;Efficient-ViT&lt;/a&gt;, &lt;a href=&#34;https://github.com/city96/ComfyUI_ExtraModels&#34;&gt;ComfyUI_ExtraModels&lt;/a&gt; and &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; for their wonderful work and codebase!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üìñBibTeX&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{xie2024sana,&#xA;      title={Sana: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer},&#xA;      author={Enze Xie and Junsong Chen and Junyu Chen and Han Cai and Haotian Tang and Yujun Lin and Zhekai Zhang and Muyang Li and Ligeng Zhu and Yao Lu and Song Han},&#xA;      year={2024},&#xA;      eprint={2410.10629},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2410.10629},&#xA;    }&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>KoljaB/RealtimeSTT</title>
    <updated>2025-01-16T01:28:32Z</updated>
    <id>tag:github.com,2025-01-16:/KoljaB/RealtimeSTT</id>
    <link href="https://github.com/KoljaB/RealtimeSTT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A robust, efficient, low-latency speech-to-text library with advanced voice activity detection, wake word activation and instant transcription.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RealtimeSTT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/RealtimeSTT/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/RealtimeSTT&#34; alt=&#34;PyPI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/KoljaB/RealtimeSTT&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/badge/RealtimeSTT&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/KoljaB/RealtimeSTT/releases/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/KoljaB/RealtimeSTT.svg?sanitize=true&#34; alt=&#34;GitHub release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/Naereen/KoljaB/RealtimeSTT/commit/&#34;&gt;&lt;img src=&#34;https://badgen.net/github/commits/KoljaB/RealtimeSTT&#34; alt=&#34;GitHub commits&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/KoljaB/RealtimeSTT/network/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/KoljaB/RealtimeSTT.svg?style=social&amp;amp;label=Fork&amp;amp;maxAge=2592000&#34; alt=&#34;GitHub forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://GitHub.com/KoljaB/RealtimeSTT/stargazers/&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/KoljaB/RealtimeSTT.svg?style=social&amp;amp;label=Star&amp;amp;maxAge=2592000&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Easy-to-use, low-latency speech-to-text library for realtime applications&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;New&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AudioToTextRecorderClient class, which automatically starts a server if none is running and connects to it. The class shares the same interface as AudioToTextRecorder, making it easy to upgrade or switch between the two. (Work in progress, most parameters and callbacks of AudioToTextRecorder are already implemented into AudioToTextRecorderClient, but not all. Also the server can not handle concurrent (parallel) requests yet.)&lt;/li&gt; &#xA; &lt;li&gt;reworked CLI interface (&#34;stt-server&#34; to start the server, &#34;stt&#34; to start the client, look at &#34;server&#34; folder for more info)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;About the Project&lt;/h2&gt; &#xA;&lt;p&gt;RealtimeSTT listens to the microphone and transcribes voice into text.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Hint:&lt;/strong&gt; &lt;em&gt;&lt;strong&gt;Check out &lt;a href=&#34;https://github.com/KoljaB/Linguflex&#34;&gt;Linguflex&lt;/a&gt;&lt;/strong&gt;, the original project from which RealtimeSTT is spun off. It lets you control your environment by speaking and is one of the most capable and sophisticated open-source assistants currently available.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;It&#39;s ideal for:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Voice Assistants&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Applications requiring &lt;strong&gt;fast and precise&lt;/strong&gt; speech-to-text conversion&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/797e6552-27cd-41b1-a7f3-e5cbc72094f5&#34;&gt;https://github.com/user-attachments/assets/797e6552-27cd-41b1-a7f3-e5cbc72094f5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Updates&lt;/h3&gt; &#xA;&lt;p&gt;Latest Version: v0.3.92&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/KoljaB/RealtimeSTT/releases&#34;&gt;release history&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Hint:&lt;/strong&gt; &lt;em&gt;Since we use the &lt;code&gt;multiprocessing&lt;/code&gt; module now, ensure to include the &lt;code&gt;if __name__ == &#39;__main__&#39;:&lt;/code&gt; protection in your code to prevent unexpected behavior, especially on platforms like Windows. For a detailed explanation on why this is important, visit the &lt;a href=&#34;https://docs.python.org/3/library/multiprocessing.html#multiprocessing-programming&#34;&gt;official Python documentation on &lt;code&gt;multiprocessing&lt;/code&gt;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Quick Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Print everything being said:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;def process_text(text):&#xA;    print(text)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    print(&#34;Wait until it says &#39;speak now&#39;&#34;)&#xA;    recorder = AudioToTextRecorder()&#xA;&#xA;    while True:&#xA;        recorder.text(process_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Type everything being said:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;import pyautogui&#xA;&#xA;def process_text(text):&#xA;    pyautogui.typewrite(text + &#34; &#34;)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    print(&#34;Wait until it says &#39;speak now&#39;&#34;)&#xA;    recorder = AudioToTextRecorder()&#xA;&#xA;    while True:&#xA;        recorder.text(process_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Will type everything being said into your selected text box&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Voice Activity Detection&lt;/strong&gt;: Automatically detects when you start and stop speaking.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Realtime Transcription&lt;/strong&gt;: Transforms speech to text in real-time.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wake Word Activation&lt;/strong&gt;: Can activate upon detecting a designated wake word.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Hint&lt;/strong&gt;: &lt;em&gt;Check out &lt;a href=&#34;https://github.com/KoljaB/RealtimeTTS&#34;&gt;RealtimeTTS&lt;/a&gt;, the output counterpart of this library, for text-to-voice capabilities. Together, they form a powerful realtime audio wrapper around large language models.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Tech Stack&lt;/h2&gt; &#xA;&lt;p&gt;This library uses:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Voice Activity Detection&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/wiseman/py-webrtcvad&#34;&gt;WebRTCVAD&lt;/a&gt; for initial voice activity detection.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;&gt;SileroVAD&lt;/a&gt; for more accurate verification.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speech-To-Text&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/guillaumekln/faster-whisper&#34;&gt;Faster_Whisper&lt;/a&gt; for instant (GPU-accelerated) transcription.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wake Word Detection&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Picovoice/porcupine&#34;&gt;Porcupine&lt;/a&gt; or &lt;a href=&#34;https://github.com/dscripka/openWakeWord&#34;&gt;OpenWakeWord&lt;/a&gt; for wake word detection.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;These components represent the &#34;industry standard&#34; for cutting-edge applications, providing the most modern and effective foundation for building high-end solutions.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install RealtimeSTT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will install all the necessary dependencies, including a &lt;strong&gt;CPU support only&lt;/strong&gt; version of PyTorch.&lt;/p&gt; &#xA;&lt;p&gt;Although it is possible to run RealtimeSTT with a CPU installation only (use a small model like &#34;tiny&#34; or &#34;base&#34; in this case) you will get way better experience using CUDA (please scroll down).&lt;/p&gt; &#xA;&lt;h3&gt;Linux Installation&lt;/h3&gt; &#xA;&lt;p&gt;Before installing RealtimeSTT please execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get update&#xA;sudo apt-get install python3-dev&#xA;sudo apt-get install portaudio19-dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;MacOS Installation&lt;/h3&gt; &#xA;&lt;p&gt;Before installing RealtimeSTT please execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install portaudio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;GPU Support with CUDA (recommended)&lt;/h3&gt; &#xA;&lt;h3&gt;Updating PyTorch for CUDA Support&lt;/h3&gt; &#xA;&lt;p&gt;To upgrade your PyTorch installation to enable GPU support with CUDA, follow these instructions based on your specific CUDA version. This is useful if you wish to enhance the performance of RealtimeSTT with CUDA capabilities.&lt;/p&gt; &#xA;&lt;h4&gt;For CUDA 11.8:&lt;/h4&gt; &#xA;&lt;p&gt;To update PyTorch and Torchaudio to support CUDA 11.8, use the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch==2.5.1+cu118 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For CUDA 12.X:&lt;/h4&gt; &#xA;&lt;p&gt;To update PyTorch and Torchaudio to support CUDA 12.X, execute the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch==2.5.1+cu121 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;2.5.1&lt;/code&gt; with the version of PyTorch that matches your system and requirements.&lt;/p&gt; &#xA;&lt;h3&gt;Steps That Might Be Necessary Before&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;em&gt;To check if your NVIDIA GPU supports CUDA, visit the &lt;a href=&#34;https://developer.nvidia.com/cuda-gpus&#34;&gt;official CUDA GPUs list&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you didn&#39;t use CUDA models before, some additional steps might be needed one time before installation. These steps prepare the system for CUDA support and installation of the &lt;strong&gt;GPU-optimized&lt;/strong&gt; installation. This is recommended for those who require &lt;strong&gt;better performance&lt;/strong&gt; and have a compatible NVIDIA GPU. To use RealtimeSTT with GPU support via CUDA please also follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA CUDA Toolkit&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;select between CUDA 11.8 or CUDA 12.X Toolkit &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;for 12.X visit &lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit-archive&#34;&gt;NVIDIA CUDA Toolkit Archive&lt;/a&gt; and select latest version.&lt;/li&gt; &#xA;     &lt;li&gt;for 11.8 visit &lt;a href=&#34;https://developer.nvidia.com/cuda-11-8-0-download-archive&#34;&gt;NVIDIA CUDA Toolkit 11.8&lt;/a&gt;.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Select operating system and version.&lt;/li&gt; &#xA;   &lt;li&gt;Download and install the software.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install NVIDIA cuDNN&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;select between CUDA 11.8 or CUDA 12.X Toolkit &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;for 12.X visit &lt;a href=&#34;https://developer.nvidia.com/cudnn-downloads&#34;&gt;cuDNN Downloads&lt;/a&gt;. &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Select operating system and version.&lt;/li&gt; &#xA;       &lt;li&gt;Download and install the software.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;for 11.8 visit &lt;a href=&#34;https://developer.nvidia.com/rdp/cudnn-archive&#34;&gt;NVIDIA cuDNN Archive&lt;/a&gt;. &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Click on &#34;Download cuDNN v8.7.0 (November 28th, 2022), for CUDA 11.x&#34;.&lt;/li&gt; &#xA;       &lt;li&gt;Download and install the software.&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install ffmpeg&lt;/strong&gt;:&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;em&gt;Installation of ffmpeg might not actually be needed to operate RealtimeSTT&lt;/em&gt; &lt;sup&gt; *thanks to jgilbert2017 for pointing this out&lt;/sup&gt;&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;You can download an installer for your OS from the &lt;a href=&#34;https://ffmpeg.org/download.html&#34;&gt;ffmpeg Website&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Or use a package manager:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Ubuntu or Debian&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt update &amp;amp;&amp;amp; sudo apt install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Arch Linux&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo pacman -S ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On MacOS using Homebrew&lt;/strong&gt; (&lt;a href=&#34;https://brew.sh/&#34;&gt;https://brew.sh/&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;brew install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Windows using Winget&lt;/strong&gt; &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/package-manager/winget/&#34;&gt;official documentation&lt;/a&gt; :&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;winget install Gyan.FFmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Windows using Chocolatey&lt;/strong&gt; (&lt;a href=&#34;https://chocolatey.org/&#34;&gt;https://chocolatey.org/&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;choco install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;On Windows using Scoop&lt;/strong&gt; (&lt;a href=&#34;https://scoop.sh/&#34;&gt;https://scoop.sh/&lt;/a&gt;):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;scoop install ffmpeg&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Basic usage:&lt;/p&gt; &#xA;&lt;h3&gt;Manual Recording&lt;/h3&gt; &#xA;&lt;p&gt;Start and stop of recording are manually triggered.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recorder.start()&#xA;recorder.stop()&#xA;print(recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder()&#xA;    recorder.start()&#xA;    input(&#34;Press Enter to stop recording...&#34;)&#xA;    recorder.stop()&#xA;    print(&#34;Transcription: &#34;, recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Automatic Recording&lt;/h3&gt; &#xA;&lt;p&gt;Recording based on voice activity detection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with AudioToTextRecorder() as recorder:&#xA;    print(recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    with AudioToTextRecorder() as recorder:&#xA;        print(&#34;Transcription: &#34;, recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When running recorder.text in a loop it is recommended to use a callback, allowing the transcription to be run asynchronously:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def process_text(text):&#xA;    print (text)&#xA;    &#xA;while True:&#xA;    recorder.text(process_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;def process_text(text):&#xA;    print(text)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder()&#xA;&#xA;    while True:&#xA;        recorder.text(process_text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Wakewords&lt;/h3&gt; &#xA;&lt;p&gt;Keyword activation before detecting voice. Write the comma-separated list of your desired activation keywords into the wake_words parameter. You can choose wake words from these list: alexa, americano, blueberry, bumblebee, computer, grapefruits, grasshopper, hey google, hey siri, jarvis, ok google, picovoice, porcupine, terminator.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recorder = AudioToTextRecorder(wake_words=&#34;jarvis&#34;)&#xA;&#xA;print(&#39;Say &#34;Jarvis&#34; then speak.&#39;)&#xA;print(recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder(wake_words=&#34;jarvis&#34;)&#xA;&#xA;    print(&#39;Say &#34;Jarvis&#34; to start recording.&#39;)&#xA;    print(recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Callbacks&lt;/h3&gt; &#xA;&lt;p&gt;You can set callback functions to be executed on different events (see &lt;a href=&#34;https://raw.githubusercontent.com/KoljaB/RealtimeSTT/master/#configuration&#34;&gt;Configuration&lt;/a&gt;) :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def my_start_callback():&#xA;    print(&#34;Recording started!&#34;)&#xA;&#xA;def my_stop_callback():&#xA;    print(&#34;Recording stopped!&#34;)&#xA;&#xA;recorder = AudioToTextRecorder(on_recording_start=my_start_callback,&#xA;                               on_recording_stop=my_stop_callback)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;def start_callback():&#xA;    print(&#34;Recording started!&#34;)&#xA;&#xA;def stop_callback():&#xA;    print(&#34;Recording stopped!&#34;)&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder(on_recording_start=start_callback,&#xA;                                   on_recording_stop=stop_callback)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Feed chunks&lt;/h3&gt; &#xA;&lt;p&gt;If you don&#39;t want to use the local microphone set use_microphone parameter to false and provide raw PCM audiochunks in 16-bit mono (samplerate 16000) with this method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recorder.feed_audio(audio_chunk)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    recorder = AudioToTextRecorder(use_microphone=False)&#xA;    with open(&#34;audio_chunk.pcm&#34;, &#34;rb&#34;) as f:&#xA;        audio_chunk = f.read()&#xA;&#xA;    recorder.feed_audio(audio_chunk)&#xA;    print(&#34;Transcription: &#34;, recorder.text())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Shutdown&lt;/h3&gt; &#xA;&lt;p&gt;You can shutdown the recorder safely by using the context manager protocol:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with AudioToTextRecorder() as recorder:&#xA;    [...]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can call the shutdown method manually (if using &#34;with&#34; is not feasible):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;recorder.shutdown()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Standalone Example:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from RealtimeSTT import AudioToTextRecorder&#xA;&#xA;if __name__ == &#39;__main__&#39;:&#xA;    with AudioToTextRecorder() as recorder:&#xA;        [...]&#xA;    # or manually shutdown if &#34;with&#34; is not used&#xA;    recorder.shutdown()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Testing the Library&lt;/h2&gt; &#xA;&lt;p&gt;The test subdirectory contains a set of scripts to help you evaluate and understand the capabilities of the RealtimeTTS library.&lt;/p&gt; &#xA;&lt;p&gt;Test scripts depending on RealtimeTTS library may require you to enter your azure service region within the script. When using OpenAI-, Azure- or Elevenlabs-related demo scripts the API Keys should be provided in the environment variables OPENAI_API_KEY, AZURE_SPEECH_KEY and ELEVENLABS_API_KEY (see &lt;a href=&#34;https://github.com/KoljaB/RealtimeTTS&#34;&gt;RealtimeTTS&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;simple_test.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: A &#34;hello world&#34; styled demonstration of the library&#39;s simplest usage.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;realtimestt_test.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Showcasing live-transcription.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wakeword_test.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: A demonstration of the wakeword activation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;translator.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Run &lt;code&gt;pip install openai realtimetts&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Real-time translations into six different languages.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;openai_voice_interface.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Run &lt;code&gt;pip install openai realtimetts&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Wake word activated and voice based user interface to the OpenAI API.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;advanced_talk.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Run &lt;code&gt;pip install openai keyboard realtimetts&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: Choose TTS engine and voice before starting AI conversation.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;minimalistic_talkbot.py&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;: Run &lt;code&gt;pip install openai realtimetts&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Description&lt;/strong&gt;: A basic talkbot in 20 lines of code.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The example_app subdirectory contains a polished user interface application for the OpenAI API based on PyQt5.&lt;/p&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;h3&gt;Initialization Parameters for &lt;code&gt;AudioToTextRecorder&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;When you initialize the &lt;code&gt;AudioToTextRecorder&lt;/code&gt; class, you have various options to customize its behavior.&lt;/p&gt; &#xA;&lt;h4&gt;General Parameters&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;model&lt;/strong&gt; (str, default=&#34;tiny&#34;): Model size or path for transcription.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Options: &#39;tiny&#39;, &#39;tiny.en&#39;, &#39;base&#39;, &#39;base.en&#39;, &#39;small&#39;, &#39;small.en&#39;, &#39;medium&#39;, &#39;medium.en&#39;, &#39;large-v1&#39;, &#39;large-v2&#39;.&lt;/li&gt; &#xA;   &lt;li&gt;Note: If a size is provided, the model will be downloaded from the Hugging Face Hub.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;language&lt;/strong&gt; (str, default=&#34;&#34;): Language code for transcription. If left empty, the model will try to auto-detect the language. Supported language codes are listed in &lt;a href=&#34;https://github.com/openai/whisper/raw/main/whisper/tokenizer.py&#34;&gt;Whisper Tokenizer library&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;compute_type&lt;/strong&gt; (str, default=&#34;default&#34;): Specifies the type of computation to be used for transcription. See &lt;a href=&#34;https://opennmt.net/CTranslate2/quantization.html&#34;&gt;Whisper Quantization&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;input_device_index&lt;/strong&gt; (int, default=0): Audio Input Device Index to use.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;gpu_device_index&lt;/strong&gt; (int, default=0): GPU Device Index to use. The model can also be loaded on multiple GPUs by passing a list of IDs (e.g. [0, 1, 2, 3]).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;device&lt;/strong&gt; (str, default=&#34;cuda&#34;): Device for model to use. Can either be &#34;cuda&#34; or &#34;cpu&#34;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_recording_start&lt;/strong&gt;: A callable function triggered when recording starts.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_recording_stop&lt;/strong&gt;: A callable function triggered when recording ends.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_transcription_start&lt;/strong&gt;: A callable function triggered when transcription starts.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ensure_sentence_starting_uppercase&lt;/strong&gt; (bool, default=True): Ensures that every sentence detected by the algorithm starts with an uppercase letter.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;ensure_sentence_ends_with_period&lt;/strong&gt; (bool, default=True): Ensures that every sentence that doesn&#39;t end with punctuation such as &#34;?&#34;, &#34;!&#34; ends with a period&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;use_microphone&lt;/strong&gt; (bool, default=True): Usage of local microphone for transcription. Set to False if you want to provide chunks with feed_audio method.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;spinner&lt;/strong&gt; (bool, default=True): Provides a spinner animation text with information about the current recorder state.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;level&lt;/strong&gt; (int, default=logging.WARNING): Logging level.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;batch_size&lt;/strong&gt; (int, default=16): Batch size for the main transcription. Set to 0 to deactivate.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;init_logging&lt;/strong&gt; (bool, default=True): Whether to initialize the logging framework. Set to False to manage this yourself.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;handle_buffer_overflow&lt;/strong&gt; (bool, default=True): If set, the system will log a warning when an input overflow occurs during recording and remove the data from the buffer.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;beam_size&lt;/strong&gt; (int, default=5): The beam size to use for beam search decoding.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;initial_prompt&lt;/strong&gt; (str or iterable of int, default=None): Initial prompt to be fed to the transcription models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;suppress_tokens&lt;/strong&gt; (list of int, default=[-1]): Tokens to be suppressed from the transcription output.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_recorded_chunk&lt;/strong&gt;: A callback function that is triggered when a chunk of audio is recorded. Submits the chunk data as parameter.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;debug_mode&lt;/strong&gt; (bool, default=False): If set, the system prints additional debug information to the console.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;print_transcription_time&lt;/strong&gt; (bool, default=False): Logs the processing time of the main model transcription. This can be useful for performance monitoring and debugging.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;early_transcription_on_silence&lt;/strong&gt; (int, default=0): If set, the system will transcribe audio faster when silence is detected. Transcription will start after the specified milliseconds. Keep this value lower than &lt;code&gt;post_speech_silence_duration&lt;/code&gt;, ideally around &lt;code&gt;post_speech_silence_duration&lt;/code&gt; minus the estimated transcription time with the main model. If silence lasts longer than &lt;code&gt;post_speech_silence_duration&lt;/code&gt;, the recording is stopped, and the transcription is submitted. If voice activity resumes within this period, the transcription is discarded. This results in faster final transcriptions at the cost of additional GPU load due to some unnecessary final transcriptions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;allowed_latency_limit&lt;/strong&gt; (int, default=100): Specifies the maximum number of unprocessed chunks in the queue before discarding chunks. This helps prevent the system from being overwhelmed and losing responsiveness in real-time applications.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;no_log_file&lt;/strong&gt; (bool, default=False): If set, the system will skip writing the debug log file, reducing disk I/O. Useful if logging to a file is not needed and performance is a priority.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Real-time Transcription Parameters&lt;/h4&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;em&gt;When enabling realtime description a GPU installation is strongly advised. Using realtime transcription may create high GPU loads.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;enable_realtime_transcription&lt;/strong&gt; (bool, default=False): Enables or disables real-time transcription of audio. When set to True, the audio will be transcribed continuously as it is being recorded.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;use_main_model_for_realtime&lt;/strong&gt; (bool, default=False): If set to True, the main transcription model will be used for both regular and real-time transcription. If False, a separate model specified by &lt;code&gt;realtime_model_type&lt;/code&gt; will be used for real-time transcription. Using a single model can save memory and potentially improve performance, but may not be optimized for real-time processing. Using separate models allows for a smaller, faster model for real-time transcription while keeping a more accurate model for final transcription.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;realtime_model_type&lt;/strong&gt; (str, default=&#34;tiny&#34;): Specifies the size or path of the machine learning model to be used for real-time transcription.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Valid options: &#39;tiny&#39;, &#39;tiny.en&#39;, &#39;base&#39;, &#39;base.en&#39;, &#39;small&#39;, &#39;small.en&#39;, &#39;medium&#39;, &#39;medium.en&#39;, &#39;large-v1&#39;, &#39;large-v2&#39;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;realtime_processing_pause&lt;/strong&gt; (float, default=0.2): Specifies the time interval in seconds after a chunk of audio gets transcribed. Lower values will result in more &#34;real-time&#34; (frequent) transcription updates but may increase computational load.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_realtime_transcription_update&lt;/strong&gt;: A callback function that is triggered whenever there&#39;s an update in the real-time transcription. The function is called with the newly transcribed text as its argument.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_realtime_transcription_stabilized&lt;/strong&gt;: A callback function that is triggered whenever there&#39;s an update in the real-time transcription and returns a higher quality, stabilized text as its argument.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;realtime_batch_size&lt;/strong&gt;: (int, default=16): Batch size for the real-time transcription model. Set to 0 to deactivate.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;beam_size_realtime&lt;/strong&gt; (int, default=3): The beam size to use for real-time transcription beam search decoding.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Voice Activation Parameters&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;silero_sensitivity&lt;/strong&gt; (float, default=0.6): Sensitivity for Silero&#39;s voice activity detection ranging from 0 (least sensitive) to 1 (most sensitive). Default is 0.6.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;silero_use_onnx&lt;/strong&gt; (bool, default=False): Enables usage of the pre-trained model from Silero in the ONNX (Open Neural Network Exchange) format instead of the PyTorch format. Default is False. Recommended for faster performance.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;silero_deactivity_detection&lt;/strong&gt; (bool, default=False): Enables the Silero model for end-of-speech detection. More robust against background noise. Utilizes additional GPU resources but improves accuracy in noisy environments. When False, uses the default WebRTC VAD, which is more sensitive but may continue recording longer due to background sounds.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;webrtc_sensitivity&lt;/strong&gt; (int, default=3): Sensitivity for the WebRTC Voice Activity Detection engine ranging from 0 (least aggressive / most sensitive) to 3 (most aggressive, least sensitive). Default is 3.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;post_speech_silence_duration&lt;/strong&gt; (float, default=0.2): Duration in seconds of silence that must follow speech before the recording is considered to be completed. This ensures that any brief pauses during speech don&#39;t prematurely end the recording.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;min_gap_between_recordings&lt;/strong&gt; (float, default=1.0): Specifies the minimum time interval in seconds that should exist between the end of one recording session and the beginning of another to prevent rapid consecutive recordings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;min_length_of_recording&lt;/strong&gt; (float, default=1.0): Specifies the minimum duration in seconds that a recording session should last to ensure meaningful audio capture, preventing excessively short or fragmented recordings.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;pre_recording_buffer_duration&lt;/strong&gt; (float, default=0.2): The time span, in seconds, during which audio is buffered prior to formal recording. This helps counterbalancing the latency inherent in speech activity detection, ensuring no initial audio is missed.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_vad_detect_start&lt;/strong&gt;: A callable function triggered when the system starts to listen for voice activity.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_vad_detect_stop&lt;/strong&gt;: A callable function triggered when the system stops to listen for voice activity.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Wake Word Parameters&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wakeword_backend&lt;/strong&gt; (str, default=&#34;pvporcupine&#34;): Specifies the backend library to use for wake word detection. Supported options include &#39;pvporcupine&#39; for using the Porcupine wake word engine or &#39;oww&#39; for using the OpenWakeWord engine.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;openwakeword_model_paths&lt;/strong&gt; (str, default=None): Comma-separated paths to model files for the openwakeword library. These paths point to custom models that can be used for wake word detection when the openwakeword library is selected as the wakeword_backend.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;openwakeword_inference_framework&lt;/strong&gt; (str, default=&#34;onnx&#34;): Specifies the inference framework to use with the openwakeword library. Can be either &#39;onnx&#39; for Open Neural Network Exchange format or &#39;tflite&#39; for TensorFlow Lite.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_words&lt;/strong&gt; (str, default=&#34;&#34;): Initiate recording when using the &#39;pvporcupine&#39; wakeword backend. Multiple wake words can be provided as a comma-separated string. Supported wake words are: alexa, americano, blueberry, bumblebee, computer, grapefruits, grasshopper, hey google, hey siri, jarvis, ok google, picovoice, porcupine, terminator. For the &#39;openwakeword&#39; backend, wake words are automatically extracted from the provided model files, so specifying them here is not necessary.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_words_sensitivity&lt;/strong&gt; (float, default=0.6): Sensitivity level for wake word detection (0 for least sensitive, 1 for most sensitive).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_word_activation_delay&lt;/strong&gt; (float, default=0): Duration in seconds after the start of monitoring before the system switches to wake word activation if no voice is initially detected. If set to zero, the system uses wake word activation immediately.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_word_timeout&lt;/strong&gt; (float, default=5): Duration in seconds after a wake word is recognized. If no subsequent voice activity is detected within this window, the system transitions back to an inactive state, awaiting the next wake word or voice activation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;wake_word_buffer_duration&lt;/strong&gt; (float, default=0.1): Duration in seconds to buffer audio data during wake word detection. This helps in cutting out the wake word from the recording buffer so it does not falsely get detected along with the following spoken text, ensuring cleaner and more accurate transcription start triggers. Increase this if parts of the wake word get detected as text.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_wakeword_detected&lt;/strong&gt;: A callable function triggered when a wake word is detected.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_wakeword_timeout&lt;/strong&gt;: A callable function triggered when the system goes back to an inactive state after when no speech was detected after wake word activation.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_wakeword_detection_start&lt;/strong&gt;: A callable function triggered when the system starts to listen for wake words&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;on_wakeword_detection_end&lt;/strong&gt;: A callable function triggered when stopping to listen for wake words (e.g. because of timeout or wake word detected)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;OpenWakeWord&lt;/h2&gt; &#xA;&lt;h3&gt;Training models&lt;/h3&gt; &#xA;&lt;p&gt;Look &lt;a href=&#34;https://github.com/dscripka/openWakeWord?tab=readme-ov-file#training-new-models&#34;&gt;here&lt;/a&gt; for information about how to train your own OpenWakeWord models. You can use a &lt;a href=&#34;https://colab.research.google.com/drive/1q1oe2zOyZp7UsB3jJiQ1IFn8z5YfjwEb?usp=sharing&#34;&gt;simple Google Colab notebook&lt;/a&gt; for a start or use a &lt;a href=&#34;https://github.com/dscripka/openWakeWord/raw/main/notebooks/automatic_model_training.ipynb&#34;&gt;more detailed notebook&lt;/a&gt; that enables more customization (can produce high quality models, but requires more development experience).&lt;/p&gt; &#xA;&lt;h3&gt;Convert model to ONNX format&lt;/h3&gt; &#xA;&lt;p&gt;You might need to use tf2onnx to convert tensorflow tflite models to onnx format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -U tf2onnx&#xA;python -m tf2onnx.convert --tflite my_model_filename.tflite --output my_model_filename.onnx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configure RealtimeSTT&lt;/h3&gt; &#xA;&lt;p&gt;Suggested starting parameters for OpenWakeWord usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    with AudioToTextRecorder(&#xA;        wakeword_backend=&#34;oww&#34;,&#xA;        wake_words_sensitivity=0.35,&#xA;        openwakeword_model_paths=&#34;word1.onnx,word2.onnx&#34;,&#xA;        wake_word_buffer_duration=1,&#xA;        ) as recorder:&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Q: I encountered the following error: &#34;Unable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so} Invalid handle. Cannot load symbol cudnnCreateTensorDescriptor.&#34; How do I fix this?&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; This issue arises from a mismatch between the version of &lt;code&gt;ctranslate2&lt;/code&gt; and cuDNN. The &lt;code&gt;ctranslate2&lt;/code&gt; library was updated to version 4.5.0, which uses cuDNN 9.2. There are two ways to resolve this issue:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Downgrade &lt;code&gt;ctranslate2&lt;/code&gt; to version 4.4.0&lt;/strong&gt;: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ctranslate2==4.4.0&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Upgrade cuDNN&lt;/strong&gt; on your system to version 9.2 or above.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are always welcome!&lt;/p&gt; &#xA;&lt;p&gt;Shoutout to &lt;a href=&#34;https://github.com/stevenlafl&#34;&gt;Steven Linn&lt;/a&gt; for providing docker support.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KoljaB/RealtimeSTT?tab=MIT-1-ov-file&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;p&gt;Kolja Beigel&lt;br&gt; Email: &lt;a href=&#34;mailto:kolja.beigel@web.de&#34;&gt;kolja.beigel@web.de&lt;/a&gt;&lt;br&gt; &lt;a href=&#34;https://github.com/KoljaB/RealtimeSTT&#34;&gt;GitHub&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nautechsystems/nautilus_trader</title>
    <updated>2025-01-16T01:28:32Z</updated>
    <id>tag:github.com,2025-01-16:/nautechsystems/nautilus_trader</id>
    <link href="https://github.com/nautechsystems/nautilus_trader" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A high-performance algorithmic trading platform and event-driven backtester&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;&lt;img src=&#34;https://github.com/nautechsystems/nautilus_trader/raw/develop/docs/_images/nautilus-trader-logo.png&#34; width=&#34;500&#34;&gt;&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://codecov.io/gh/nautechsystems/nautilus_trader&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/nautechsystems/nautilus_trader/branch/master/graph/badge.svg?token=DXO9QQI40H&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codspeed.io/nautechsystems/nautilus_trader&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://codspeed.io/badge.json&#34; alt=&#34;codspeed&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/pyversions/nautilus_trader&#34; alt=&#34;pythons&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/nautilus_trader&#34; alt=&#34;pypi-version&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/format/nautilus_trader?color=blue&#34; alt=&#34;pypi-format&#34;&gt; &lt;a href=&#34;https://pepy.tech/project/nautilus-trader&#34;&gt;&lt;img src=&#34;https://pepy.tech/badge/nautilus-trader&#34; alt=&#34;Downloads&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Branch&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Version&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;master&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fmaster%2Fversion.json&#34; alt=&#34;version&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=master&#34; alt=&#34;build&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;nightly&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fnightly%2Fversion.json&#34; alt=&#34;version&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=nightly&#34; alt=&#34;build&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;develop&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fraw.githubusercontent.com%2Fnautechsystems%2Fnautilus_trader%2Fdevelop%2Fversion.json&#34; alt=&#34;version&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml&#34;&gt;&lt;img src=&#34;https://github.com/nautechsystems/nautilus_trader/actions/workflows/build.yml/badge.svg?branch=develop&#34; alt=&#34;build&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Platform&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Rust&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Python&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Linux (x86_64)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.84.0+&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.11, 3.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;macOS (arm64)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.84.0+&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.11, 3.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;Windows (x86_64)&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;1.84.0+&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;3.11, 3.12&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/AUWVs3XaCS&#34;&gt;&lt;img src=&#34;https://dcbadge.limes.pink/api/server/AUWVs3XaCS&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Docs&lt;/strong&gt;: &lt;a href=&#34;https://nautilustrader.io/docs/&#34;&gt;https://nautilustrader.io/docs/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: &lt;a href=&#34;https://nautilustrader.io&#34;&gt;https://nautilustrader.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support&lt;/strong&gt;: &lt;a href=&#34;mailto:support@nautilustrader.io&#34;&gt;support@nautilustrader.io&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;NautilusTrader is an open-source, high-performance, production-grade algorithmic trading platform, providing quantitative traders with the ability to backtest portfolios of automated trading strategies on historical data with an event-driven engine, and also deploy those same strategies live, with no code changes.&lt;/p&gt; &#xA;&lt;p&gt;The platform is &lt;em&gt;AI-first&lt;/em&gt;, designed to develop and deploy algorithmic trading strategies within a highly performant and robust Python-native environment. This helps to address the parity challenge of keeping the Python research/backtest environment consistent with the production live trading environment.&lt;/p&gt; &#xA;&lt;p&gt;NautilusTrader&#39;s design, architecture, and implementation philosophy holds software correctness and safety at the highest level, with the aim of supporting Python-native, mission-critical, trading system backtesting and live deployment workloads.&lt;/p&gt; &#xA;&lt;p&gt;The platform is also universal and asset-class-agnostic ‚Äî with any REST, WebSocket or FIX API able to be integrated via modular adapters. It supports high-frequency trading across a wide range of asset classes and instrument types including FX, Equities, Futures, Options, Crypto and Betting, enabling seamless operations across multiple venues simultaneously.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Fast&lt;/strong&gt;: Core is written in Rust with asynchronous networking using &lt;a href=&#34;https://crates.io/crates/tokio&#34;&gt;tokio&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reliable&lt;/strong&gt;: Type safety and thread safety through Rust. Redis-backed performant state persistence (optional).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Portable&lt;/strong&gt;: OS independent, runs on Linux, macOS, and Windows. Deploy using Docker.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: Modular adapters mean any REST, WebSocket, or FIX API can be integrated.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced&lt;/strong&gt;: Time in force &lt;code&gt;IOC&lt;/code&gt;, &lt;code&gt;FOK&lt;/code&gt;, &lt;code&gt;GTD&lt;/code&gt;, &lt;code&gt;AT_THE_OPEN&lt;/code&gt;, &lt;code&gt;AT_THE_CLOSE&lt;/code&gt;, advanced order types and conditional triggers. Execution instructions &lt;code&gt;post-only&lt;/code&gt;, &lt;code&gt;reduce-only&lt;/code&gt;, and icebergs. Contingency order lists including &lt;code&gt;OCO&lt;/code&gt;, &lt;code&gt;OTO&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Customizable&lt;/strong&gt;: Add user-defined custom components, or assemble entire systems from scratch leveraging the cache and message bus.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backtesting&lt;/strong&gt;: Run with multiple venues, instruments and strategies simultaneously using historical quote tick, trade tick, bar, order book and custom data with nanosecond resolution.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Live&lt;/strong&gt;: Use identical strategy implementations between backtesting and live deployments.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-venue&lt;/strong&gt;: Multiple venue capabilities facilitate market-making and statistical arbitrage strategies.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;AI Training&lt;/strong&gt;: Backtest engine fast enough to be used to train AI trading agents (RL/ES).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/nautechsystems/nautilus_trader/raw/develop/docs/_images/nautilus-art.png?raw=true&#34; alt=&#34;Alt text&#34; title=&#34;nautilus&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;nautilus - from ancient Greek &#39;sailor&#39; and naus &#39;ship&#39;.&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;The nautilus shell consists of modular chambers with a growth factor which approximates a logarithmic spiral. The idea is that this can be translated to the aesthetics of design and architecture.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Why NautilusTrader?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Highly performant event-driven Python&lt;/strong&gt;: Native binary core components.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parity between backtesting and live trading&lt;/strong&gt;: Identical strategy code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Reduced operational risk&lt;/strong&gt;: Enhanced risk management functionality, logical accuracy, and type safety.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Highly extendable&lt;/strong&gt;: Message bus, custom components and actors, custom data, custom adapters.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Traditionally, trading strategy research and backtesting might be conducted in Python using vectorized methods, with the strategy then needing to be reimplemented in a more event-driven way using C++, C#, Java or other statically typed language(s). The reasoning here is that vectorized backtesting code cannot express the granular time and event dependent complexity of real-time trading, where compiled languages have proven to be more suitable due to their inherently higher performance, and type safety.&lt;/p&gt; &#xA;&lt;p&gt;One of the key advantages of NautilusTrader here, is that this reimplementation step is now circumvented - as the critical core components of the platform have all been written entirely in &lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt; or &lt;a href=&#34;https://cython.org/&#34;&gt;Cython&lt;/a&gt;. This means we&#39;re using the right tools for the job, where systems programming languages compile performant binaries, with CPython C extension modules then able to offer a Python-native environment, suitable for professional quantitative traders and trading firms.&lt;/p&gt; &#xA;&lt;h2&gt;Why Python?&lt;/h2&gt; &#xA;&lt;p&gt;Python was originally created decades ago as a simple scripting language with a clean straight forward syntax. It has since evolved into a fully fledged general purpose object-oriented programming language. Based on the TIOBE index, Python is currently the most popular programming language in the world. Not only that, Python has become the &lt;em&gt;de facto lingua franca&lt;/em&gt; of data science, machine learning, and artificial intelligence.&lt;/p&gt; &#xA;&lt;p&gt;The language out of the box is not without its drawbacks however, especially in the context of implementing large performance-critical systems. Cython has addressed a lot of these issues, offering all the advantages of a statically typed language, embedded into Pythons rich ecosystem of software libraries and developer/user communities.&lt;/p&gt; &#xA;&lt;h2&gt;What is Rust?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt; is a multi-paradigm programming language designed for performance and safety, especially safe concurrency. Rust is blazingly fast and memory-efficient (comparable to C and C++) with no garbage collector. It can power mission-critical systems, run on embedded devices, and easily integrates with other languages.&lt;/p&gt; &#xA;&lt;p&gt;Rust‚Äôs rich type system and ownership model guarantees memory-safety and thread-safety deterministically ‚Äî eliminating many classes of bugs at compile-time.&lt;/p&gt; &#xA;&lt;p&gt;The project increasingly utilizes Rust for core performance-critical components. Python language binding is handled through Cython and &lt;a href=&#34;https://pyo3.rs/latest&#34;&gt;PyO3&lt;/a&gt;, with static libraries linked at compile-time before the wheel binaries are packaged, so a user does not need to have Rust installed to run NautilusTrader.&lt;/p&gt; &#xA;&lt;p&gt;This project makes the &lt;a href=&#34;https://raphlinus.github.io/rust/2020/01/18/soundness-pledge.html&#34;&gt;Soundness Pledge&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ÄúThe intent of this project is to be free of soundness bugs. The developers will do their best to avoid them, and welcome help in analyzing and fixing them.‚Äù&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;MSRV:&lt;/strong&gt; NautilusTrader relies heavily on improvements in the Rust language and compiler. As a result, the Minimum Supported Rust Version (MSRV) is generally equal to the latest stable release of Rust.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Architecture (data flow)&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/nautechsystems/nautilus_trader/raw/develop/docs/_images/architecture-overview.png?raw=true&#34; alt=&#34;Architecture&#34; title=&#34;architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;p&gt;NautilusTrader is modularly designed to work with &lt;em&gt;adapters&lt;/em&gt;, enabling connectivity to trading venues and data providers by converting their raw APIs into a unified interface.&lt;/p&gt; &#xA;&lt;p&gt;The following integrations are currently supported:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Name&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;ID&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Type&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Status&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Docs&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://betfair.com&#34;&gt;Betfair&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BETFAIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Sports Betting Exchange&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/betfair.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://binance.com&#34;&gt;Binance&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Crypto Exchange (CEX)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/binance.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://binance.us&#34;&gt;Binance US&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Crypto Exchange (CEX)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/binance.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.binance.com/en/futures&#34;&gt;Binance Futures&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BINANCE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Crypto Exchange (CEX)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/binance.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.bybit.com&#34;&gt;Bybit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;BYBIT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Crypto Exchange (CEX)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/bybit.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://databento.com&#34;&gt;Databento&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DATABENTO&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Data Provider&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/databento.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://dydx.exchange/&#34;&gt;dYdX&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;DYDX&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Crypto Exchange (DEX)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/dydx.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://www.interactivebrokers.com&#34;&gt;Interactive Brokers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;INTERACTIVE_BROKERS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Brokerage (multi-venue)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/ib.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://okx.com&#34;&gt;OKX&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;OKX&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Crypto Exchange (CEX)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/building-orange&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/okx.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://polymarket.com&#34;&gt;Polymarket&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;POLYMARKET&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Prediction Market (DEX)&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/polymarket.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://tardis.dev&#34;&gt;Tardis&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;code&gt;TARDIS&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Data Provider&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stable-green&#34; alt=&#34;status&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/tardis.html&#34;&gt;Guide&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;ID&lt;/strong&gt;: The default client ID for the integrations adapter clients.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: The type of integration (often the venue type).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Status&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;building&lt;/code&gt;: Under construction and likely not in a usable state.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;beta&lt;/code&gt;: Completed to a minimally working state and in a beta testing phase.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;stable&lt;/code&gt;: Stabilized feature set and API, the integration has been tested by both developers and users to a reasonable level (some bugs may still remain).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://nautilustrader.io/docs/latest/integrations/index.html&#34;&gt;Integrations&lt;/a&gt; documentation for further details.&lt;/p&gt; &#xA;&lt;h3&gt;Branches&lt;/h3&gt; &#xA;&lt;p&gt;We aim to maintain a stable, passing build across all branches.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;master&lt;/code&gt;: Reflects the source code for the latest released version.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt;: Includes experimental and in-progress features, merged from the &lt;code&gt;develop&lt;/code&gt; branch daily at &lt;strong&gt;14:00 UTC&lt;/strong&gt; and also when required.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;develop&lt;/code&gt;: The most active branch, frequently updated with new commits, including experimental and in-progress features.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;Our &lt;a href=&#34;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/ROADMAP.md&#34;&gt;roadmap&lt;/a&gt; aims to achieve a &lt;strong&gt;stable API for version 2.x&lt;/strong&gt; (likely after the Rust port). Once this milestone is reached, we plan to implement a formal deprecation process for any API changes. This approach allows us to maintain a rapid development pace for now.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Versioning and releases&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;NautilusTrader is still under active development&lt;/strong&gt;. Some features may be incomplete, and while the API is becoming more stable, breaking changes can occur between releases. We strive to document these changes in the release notes on a &lt;strong&gt;best-effort basis&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We aim to follow a &lt;strong&gt;weekly release schedule&lt;/strong&gt;, though experimental or larger features may cause delays.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;From PyPI&lt;/h3&gt; &#xA;&lt;p&gt;We recommend using the latest supported version of Python and setting up &lt;a href=&#34;https://pypi.org/project/nautilus_trader/&#34;&gt;nautilus_trader&lt;/a&gt; in a virtual environment to isolate dependencies&lt;/p&gt; &#xA;&lt;p&gt;To install the latest binary wheel (or sdist package) from PyPI using Pythons pip package manager:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U nautilus_trader&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;From the Nautech Systems package index&lt;/h3&gt; &#xA;&lt;p&gt;The Nautech Systems package index (&lt;code&gt;packages.nautechsystems.io&lt;/code&gt;) is &lt;a href=&#34;https://peps.python.org/pep-0503/&#34;&gt;PEP-503&lt;/a&gt; compliant and hosts both stable and development binary wheels for &lt;code&gt;nautilus_trader&lt;/code&gt;. This enables users to install either the latest stable release or pre-release versions for testing.&lt;/p&gt; &#xA;&lt;h4&gt;Stable wheels&lt;/h4&gt; &#xA;&lt;p&gt;Stable wheels correspond to official releases of &lt;code&gt;nautilus_trader&lt;/code&gt; on PyPI, and use standard versioning.&lt;/p&gt; &#xA;&lt;p&gt;To install the latest stable release:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U nautilus_trader --index-url=https://packages.nautechsystems.io/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Development wheels&lt;/h4&gt; &#xA;&lt;p&gt;Development wheels are published from both the &lt;code&gt;develop&lt;/code&gt; and &lt;code&gt;nightly&lt;/code&gt; branches for Linux and macOS, allowing users to test features and fixes ahead of stable releases.&lt;/p&gt; &#xA;&lt;p&gt;This process also helps preserve compute resources and ensures easy access to the exact binaries tested in CI pipelines, while adhering to &lt;a href=&#34;https://peps.python.org/pep-0440/&#34;&gt;PEP-440&lt;/a&gt; versioning standards:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; wheels use the version format &lt;code&gt;dev{date}+{build_number}&lt;/code&gt; (e.g., &lt;code&gt;1.208.0.dev20241212+7001&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; wheels use the version format &lt;code&gt;a{date}&lt;/code&gt; (alpha) (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING]&lt;/p&gt; &#xA; &lt;p&gt;We don&#39;t recommend using development wheels in production environments, such as live trading controlling real capital.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Installation commands&lt;/h4&gt; &#xA;&lt;p&gt;By default, pip installs the latest stable release. Adding the &lt;code&gt;--pre&lt;/code&gt; flag ensures that pre-release versions, including development wheels, are considered.&lt;/p&gt; &#xA;&lt;p&gt;To install the latest available pre-release (including development wheels):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U nautilus_trader --pre --index-url=https://packages.nautechsystems.io/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To install a specific development wheel (e.g., &lt;code&gt;1.208.0a20241212&lt;/code&gt; for December 12, 2024):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install nautilus_trader==1.208.0a20241212 --index-url=https://packages.nautechsystems.io/simple&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Available versions&lt;/h4&gt; &#xA;&lt;p&gt;You can view all available versions of &lt;code&gt;nautilus_trader&lt;/code&gt; on the &lt;a href=&#34;https://packages.nautechsystems.io/simple/nautilus-trader/index.html&#34;&gt;package index&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To programmatically fetch and list available versions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -s https://packages.nautechsystems.io/simple/nautilus-trader/index.html | grep -oP &#39;(?&amp;lt;=&amp;lt;a href=&#34;)[^&#34;]+(?=&#34;)&#39; | awk -F&#39;#&#39; &#39;{print $1}&#39; | sort&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Branch updates&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): Are built and published continuously with every merged commit.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): Are built and published daily when &lt;code&gt;develop&lt;/code&gt; branch is automatically merged at &lt;strong&gt;14:00 UTC&lt;/strong&gt; (if there are changes).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Retention policies&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;develop&lt;/code&gt; branch wheels (&lt;code&gt;.dev&lt;/code&gt;): Only the most recent wheel build is retained.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nightly&lt;/code&gt; branch wheels (&lt;code&gt;a&lt;/code&gt;): Only the 3 most recent wheel builds are retained.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;From Source&lt;/h3&gt; &#xA;&lt;p&gt;Installation from source requires the &lt;code&gt;Python.h&lt;/code&gt; header file, which is included in development releases such as &lt;code&gt;python-dev&lt;/code&gt;. You&#39;ll also need the latest stable &lt;code&gt;rustc&lt;/code&gt; and &lt;code&gt;cargo&lt;/code&gt; to compile the Rust libraries.&lt;/p&gt; &#xA;&lt;p&gt;For MacBook Pro M1/M2, make sure your Python installed using pyenv is configured with &lt;code&gt;--enable-shared&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;PYTHON_CONFIGURE_OPTS=&#34;--enable-shared&#34; pyenv install &amp;lt;python_version&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://pyo3.rs/latest/getting-started#virtualenvs&#34;&gt;PyO3 user guide&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s possible to install from source using pip if you first install the build dependencies as specified in the &lt;code&gt;pyproject.toml&lt;/code&gt;. We highly recommend installing using &lt;a href=&#34;https://python-poetry.org/&#34;&gt;poetry&lt;/a&gt; as below.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://rustup.rs/&#34;&gt;rustup&lt;/a&gt; (the Rust toolchain installer):&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux and macOS: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl https://sh.rustup.rs -sSf | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Windows: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Download and install &lt;a href=&#34;https://win.rustup.rs/x86_64&#34;&gt;&lt;code&gt;rustup-init.exe&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Install &#34;Desktop development with C++&#34; with &lt;a href=&#34;https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16&#34;&gt;Build Tools for Visual Studio 2019&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Verify (any system): from a terminal session run: &lt;code&gt;rustc --version&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enable &lt;code&gt;cargo&lt;/code&gt; in the current shell:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux and macOS: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source $HOME/.cargo/env&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Windows: &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Start a new PowerShell&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;a href=&#34;https://clang.llvm.org/&#34;&gt;clang&lt;/a&gt; (a C language frontend for LLVM):&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Linux: &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get install clang&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Windows: &#xA;    &lt;ol&gt; &#xA;     &lt;li&gt;Add Clang to your &lt;a href=&#34;https://visualstudio.microsoft.com/thank-you-downloading-visual-studio/?sku=BuildTools&amp;amp;rel=16&#34;&gt;Build Tools for Visual Studio 2019&lt;/a&gt;: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;Start | Visual Studio Installer | Modify | C++ Clang tools for Windows (12.0.0 - x64‚Ä¶) = checked | Modify&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Enable &lt;code&gt;clang&lt;/code&gt; in the current shell: &lt;pre&gt;&lt;code class=&#34;language-powershell&#34;&gt;[System.Environment]::SetEnvironmentVariable(&#39;path&#39;, &#34;C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools\VC\Tools\Llvm\x64\bin\;&#34; + $env:Path,&#34;User&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;    &lt;/ol&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Verify (any system): from a terminal session run: &lt;code&gt;clang --version&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install poetry (or follow the installation guide on their site):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -sSL https://install.python-poetry.org | python3 -&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the source with &lt;code&gt;git&lt;/code&gt;, and install from the projects root directory:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone --branch develop --depth 1 https://github.com/nautechsystems/nautilus_trader&#xA;cd nautilus_trader&#xA;poetry install --only main --all-extras&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;The &lt;code&gt;--depth 1&lt;/code&gt; flag fetches just the latest commit for a faster, lightweight clone.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://nautilustrader.io/docs/latest/getting_started/installation&#34;&gt;Installation Guide&lt;/a&gt; for other options and further details.&lt;/p&gt; &#xA;&lt;h2&gt;Redis&lt;/h2&gt; &#xA;&lt;p&gt;Using Redis with NautilusTrader is &lt;strong&gt;optional&lt;/strong&gt; and only required if configured as the backend for a cache database or &lt;a href=&#34;https://nautilustrader.io/docs/latest/concepts/message_bus&#34;&gt;message bus&lt;/a&gt;. See the Redis section of the &lt;a href=&#34;https://nautilustrader.io/docs/latest/getting_started/installation#redis&#34;&gt;Installation Guide&lt;/a&gt; for further details.&lt;/p&gt; &#xA;&lt;h2&gt;Makefile&lt;/h2&gt; &#xA;&lt;p&gt;A &lt;code&gt;Makefile&lt;/code&gt; is provided to automate most installation and build tasks for development. It provides the following targets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;make install&lt;/code&gt;: Installs in &lt;code&gt;release&lt;/code&gt; build mode with &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; dependencies then installs the package using poetry (default).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make install-debug&lt;/code&gt;: Same as &lt;code&gt;make install&lt;/code&gt; but with &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make install-just-deps&lt;/code&gt;: Installs just the &lt;code&gt;main&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; dependencies (does not install package).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make build&lt;/code&gt;: Runs the build script in &lt;code&gt;release&lt;/code&gt; build mode (default).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make build-debug&lt;/code&gt;: Runs the build script in &lt;code&gt;debug&lt;/code&gt; build mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make build-wheel&lt;/code&gt;: Runs the Poetry build with a wheel format in &lt;code&gt;release&lt;/code&gt; mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make build-wheel-debug&lt;/code&gt;: Runs the Poetry build with a wheel format in &lt;code&gt;debug&lt;/code&gt; mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make clean&lt;/code&gt;: &lt;strong&gt;CAUTION&lt;/strong&gt; removes all non-source artifacts from the repository.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make docs&lt;/code&gt;: Builds the documentation HTML using Sphinx.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make pre-commit&lt;/code&gt;: Runs the pre-commit checks over all files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make ruff&lt;/code&gt;: Runs ruff over all files using the &lt;code&gt;pyproject.toml&lt;/code&gt; config (with autofix).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make pytest&lt;/code&gt;: Runs all tests with &lt;code&gt;pytest&lt;/code&gt; (except performance tests).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;make pytest-coverage&lt;/code&gt;: Same as &lt;code&gt;make pytest&lt;/code&gt; and additionally runs with test coverage and produces a report.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;p&gt;Run &lt;code&gt;make build-debug&lt;/code&gt; to compile after changes to Rust or Cython code for the most efficient development workflow.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Indicators and strategies can be developed in both Python and Cython. For performance and latency-sensitive applications, we recommend using Cython. Below are some examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/indicators/ema_python.py&#34;&gt;indicator&lt;/a&gt; example written in Python.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/indicators/&#34;&gt;indicator&lt;/a&gt; examples written in Cython.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/nautilus_trader/examples/strategies/&#34;&gt;strategy&lt;/a&gt; examples written in both Python and Cython.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nautechsystems/nautilus_trader/develop/examples/backtest/&#34;&gt;backtest&lt;/a&gt; examples using a &lt;code&gt;BacktestEngine&lt;/code&gt; directly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;Docker containers are built using the base image &lt;code&gt;python:3.12-slim&lt;/code&gt; with the following variant tags:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;nautilus_trader:latest&lt;/code&gt; has the latest release version installed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;nautilus_trader:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;jupyterlab:latest&lt;/code&gt; has the latest release version installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;jupyterlab:nightly&lt;/code&gt; has the head of the &lt;code&gt;nightly&lt;/code&gt; branch installed along with &lt;code&gt;jupyterlab&lt;/code&gt; and an example backtest notebook with accompanying data.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The container images can be pulled as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker pull ghcr.io/nautechsystems/&amp;lt;image_variant_tag&amp;gt; --platform linux/amd64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can launch the backtest example container by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker pull ghcr.io/nautechsystems/jupyterlab:nightly --platform linux/amd64&#xA;docker run -p 8888:8888 ghcr.io/nautechsystems/jupyterlab:nightly&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then open your browser at the following address:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://127.0.0.1:8888/lab&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING]&lt;/p&gt; &#xA; &lt;p&gt;NautilusTrader currently exceeds the rate limit for Jupyter notebook logging (stdout output). As a result, the &lt;code&gt;log_level&lt;/code&gt; in the examples is set to &lt;code&gt;ERROR&lt;/code&gt;. Lowering this level to see more logging will cause the notebook to hang during cell execution. We are investigating a fix, which may involve either raising the configured rate limits for Jupyter or throttling the log flushing from Nautilus.&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/jupyterlab/jupyterlab/issues/12845&#34;&gt;https://github.com/jupyterlab/jupyterlab/issues/12845&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/deshaw/jupyterlab-limit-output&#34;&gt;https://github.com/deshaw/jupyterlab-limit-output&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;We aim to provide the most pleasant developer experience possible for this hybrid codebase of Python, Cython and Rust. See the &lt;a href=&#34;https://nautilustrader.io/docs/latest/developer_guide/index.html&#34;&gt;Developer Guide&lt;/a&gt; for helpful information.&lt;/p&gt; &#xA;&lt;h3&gt;Testing with Rust&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nexte.st&#34;&gt;cargo-nextest&lt;/a&gt; is the standard Rust test runner for NautilusTrader. You can install it by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo install cargo-nextest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;/p&gt; &#xA; &lt;p&gt;Run Rust tests with &lt;code&gt;make cargo-test&lt;/code&gt;, as they only pass via &lt;strong&gt;cargo-nextest&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for considering contributing to NautilusTrader! We welcome any and all help to improve the project. If you have an idea for an enhancement or a bug fix, the first step is to open an &lt;a href=&#34;https://github.com/nautechsystems/nautilus_trader/issues&#34;&gt;issue&lt;/a&gt; on GitHub to discuss it with the team. This helps to ensure that your contribution will be well-aligned with the goals of the project and avoids duplication of effort.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;re ready to start working on your contribution, make sure to follow the guidelines outlined in the &lt;a href=&#34;https://github.com/nautechsystems/nautilus_trader/raw/develop/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file. This includes signing a Contributor License Agreement (CLA) to ensure that your contributions can be included in the project.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE]&lt;/p&gt; &#xA; &lt;p&gt;Pull requests should target the &lt;code&gt;develop&lt;/code&gt; branch (the default branch). This is where new features and improvements are integrated before release.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Thank you again for your interest in NautilusTrader! We look forward to reviewing your contributions and working with you to improve the project.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Join our community of users and contributors on &lt;a href=&#34;https://discord.gg/AUWVs3XaCS&#34;&gt;Discord&lt;/a&gt; to chat and stay up-to-date with the latest announcements and features of NautilusTrader. Whether you&#39;re a developer looking to contribute or just want to learn more about the platform, all are welcome on our server.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The source code for NautilusTrader is available on GitHub under the &lt;a href=&#34;https://www.gnu.org/licenses/lgpl-3.0.en.html&#34;&gt;GNU Lesser General Public License v3.0&lt;/a&gt;. Contributions to the project are welcome and require the completion of a standard Contributor License Agreement (CLA).&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;NautilusTrader is developed and maintained by Nautech Systems, a technology company specializing in the development of high-performance trading systems. Although the project utilizes the Rust programming language and benefits from its ecosystem, Nautech Systems is not affiliated with the Rust Foundation, and this project is not an official work of the Rust Foundation. For more information, visit &lt;a href=&#34;https://nautilustrader.io&#34;&gt;https://nautilustrader.io&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Copyright (C) 2015-2025 Nautech Systems Pty Ltd. All rights reserved.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/nautechsystems/nautilus_trader/raw/develop/docs/_images/ns-logo.png?raw=true&#34; alt=&#34;nautechsystems&#34; title=&#34;nautechsystems&#34;&gt; &lt;img src=&#34;https://github.com/nautechsystems/nautilus_trader/raw/develop/docs/_images/ferris.png&#34; width=&#34;128&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>