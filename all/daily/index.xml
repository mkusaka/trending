<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-03T01:28:35Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>novitalabs/AnimateAnyone</title>
    <updated>2024-06-03T01:28:35Z</updated>
    <id>tag:github.com,2024-06-03:/novitalabs/AnimateAnyone</id>
    <link href="https://github.com/novitalabs/AnimateAnyone" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Unofficial Implementation of Animate Anyone by Novita AI&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Animate Anyone&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://novita.ai&#34;&gt;&lt;img src=&#34;https://github.com/novitalabs/AnimateAnyone/assets/4327933/2a6ef880-e5c3-437e-adc5-2ae8601ac4f8&#34; alt=&#34;Novita AI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;This repository currently provides the unofficial pre-trained weights and inference code of &lt;a href=&#34;https://humanaigc.github.io/animate-anyone&#34;&gt;Animate Anyone&lt;/a&gt;. It is inspired by the implementation of the &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;MooreThreads/Moore-AnimateAnyone&lt;/a&gt; repository and we made some adjustments to the training process and datasets.&lt;/p&gt; &#xA;&lt;h2&gt;Samples&lt;/h2&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/novitalabs/AnimateAnyone/assets/4327933/49d9c98c-a3bb-4cfc-b1ce-c0e85731e7f8&#34;&gt;&#xA;     Demo 1&#xA;    &lt;/video&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/novitalabs/AnimateAnyone/assets/4327933/cd58d1e8-95d8-46e2-8b34-ba004067c6c9&#34;&gt;&#xA;     Demo 2&#xA;    &lt;/video&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/novitalabs/AnimateAnyone/assets/4327933/1f07f5e7-073e-4d02-872b-da63e2a97c1b&#34;&gt;&#xA;     Demo 3&#xA;    &lt;/video&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/novitalabs/AnimateAnyone/assets/4327933/3e492adf-9d07-493d-b3c9-65db47713bf3&#34;&gt;&#xA;     Demo 4&#xA;    &lt;/video&gt;&lt;/td&gt;&#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;Build Environtment&lt;/h3&gt; &#xA;&lt;p&gt;We Recommend a python version &lt;code&gt;&amp;gt;=3.10&lt;/code&gt; and cuda version &lt;code&gt;=11.7&lt;/code&gt;. Then build environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# [Optional] Create a virtual env&#xA;python -m venv .venv&#xA;source .venv/bin/activate&#xA;# Install with pip:&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download weights&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatically downloading&lt;/strong&gt;: You can run the following command to download weights automatically:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/download_weights.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Weights will be placed under the &lt;code&gt;./pretrained_weights&lt;/code&gt; direcotry. The whole downloading process may take a long time.&lt;/p&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;Here is the cli command for running inference scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.pose2vid --config ./configs/prompts/animation.yaml -W 512 -H 784 -L 64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can refer the format of &lt;code&gt;animation.yaml&lt;/code&gt; to add your own reference images or pose videos. To convert the raw video into a pose video (keypoint sequence), you can run with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/vid2pose.py --video_path /path/to/your/video.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Or try it on Novita AI&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ve deployed this model on Novita AI, and you can try it out with Playground ➡️ &lt;a href=&#34;https://novita.ai/playground#animate-anyone&#34;&gt;https://novita.ai/playground#animate-anyone&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This project is based on &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;MooreThreads/Moore-AnimateAnyone&lt;/a&gt; which is licensed under the Apache License 2.0. We thank to the authors of &lt;a href=&#34;https://humanaigc.github.io/animate-anyone&#34;&gt;Animate Anyone&lt;/a&gt; and &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;MooreThreads/Moore-AnimateAnyone&lt;/a&gt;, for their open research and exploration.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>lllyasviel/Omost</title>
    <updated>2024-06-03T01:28:35Z</updated>
    <id>tag:github.com,2024-06-03:/lllyasviel/Omost</id>
    <link href="https://github.com/lllyasviel/Omost" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Your image is almost there!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Omost&lt;/h1&gt; &#xA;&lt;p&gt;Omost is a project to convert LLM&#39;s coding capability to image generation (or more accurately, image composing) capability.&lt;/p&gt; &#xA;&lt;p&gt;The name &lt;code&gt;Omost&lt;/code&gt; (pronunciation: almost) has two meanings: 1) everytime after you use Omost, your image is almost there; 2) the &lt;code&gt;O&lt;/code&gt; mean &#34;omni&#34; (multi-modal) and &lt;code&gt;most&lt;/code&gt; means we want to get the most out of it.&lt;/p&gt; &#xA;&lt;p&gt;Omost provides LLMs models that will write codes to compose image visual contents with Omost&#39;s virtual &lt;code&gt;Canvas&lt;/code&gt; agent. This &lt;code&gt;Canvas&lt;/code&gt; can be rendered by specific implementations of image generators to actually generate images.&lt;/p&gt; &#xA;&lt;p&gt;Currently, we provide 3 pretrained LLM models based on variations of Llama3 and Phi3 (see also the model notes at the end of this page).&lt;/p&gt; &#xA;&lt;p&gt;All models are trained with mixed data of (1) ground-truth annotations of several datasets including Open-Images, (2) extracted data by automatically annotating images, (3) reinforcement from DPO (Direct Preference Optimization, &#34;whether the codes can be compiled by python 3.10 or not&#34; as a direct preference), and (4) a small amount of tuning data from OpenAI GPT4o&#39;s multi-modal capability.&lt;/p&gt; &#xA;&lt;h1&gt;Get Started&lt;/h1&gt; &#xA;&lt;p&gt;You can just use the &lt;a href=&#34;https://huggingface.co/spaces/lllyasviel/Omost&#34;&gt;official HuggingFace space&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Or, you can use the below deployment (requires 8GB Nvidia VRAM):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/lllyasviel/Omost.git&#xA;cd Omost&#xA;conda create -n omost python=3.10&#xA;conda activate omost&#xA;pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121&#xA;pip install -r requirements.txt&#xA;python gradio_app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(Note that quant LLM requires &lt;code&gt;bitsandbytes&lt;/code&gt; - some 9XX or 10XX or 20XX GPUs may have trouble in running it. If that happens, just use our official huggingface space.)&lt;/p&gt; &#xA;&lt;h1&gt;Screenshots&lt;/h1&gt; &#xA;&lt;p&gt;(All with random seed 12345)&lt;/p&gt; &#xA;&lt;p&gt;a ragged man wearing a tattered jacket in the nineteenth century：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/6acc5eac-87e4-428b-a209-1d4b947c590c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/c60dadfc-6e82-4582-9561-8389260714c0&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;the greatest escape after the apocalypse, the hope of all humanity：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/997d41ab-ea52-4036-898b-47c8b863acf4&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/c6653558-bfd8-40d9-95fb-c5ec1b68c289&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;jurassic dinosaur battle：&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/7efe0b56-9da2-4967-bbb0-ed3cf4eddd23&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/f4b39798-488d-429c-bf96-27a7da6f91f7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;the supreme ruler with endless magic on a gothic throne, with undead armies kneeling:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/336b7f94-0f8e-425a-b541-6e8cd5642a2e&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/9de68d20-0c01-4cee-b656-04d59cbdc695&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;a funny cartoon batman fights joker:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/dc751ded-f0f7-4312-8cd2-2a3aced15a60&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/f3e6e8e8-2e08-4e5e-a109-a8054461e71f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Example Transcripts (with Conversational Editing)&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;(Below is a raw transcript from default model, with random seed 123456)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;(If the GitHub&#39;s code block rendering is causing troubles to reading long lines, click &lt;a href=&#34;https://raw.githubusercontent.com/lllyasviel/Omost/main/README.md&#34;&gt;here&lt;/a&gt; to read raw texts.)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;User&lt;/h3&gt; &#xA;&lt;p&gt;generate an image of the fierce battle of warriors and the dragon&lt;/p&gt; &#xA;&lt;h3&gt;Assistant&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize the canvas&#xA;canvas = Canvas()&#xA;&#xA;# Set a global description for the canvas&#xA;canvas.set_global_description(&#xA;    description=&#39;A fierce battle between warriors and a dragon.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;In this intense scene, a group of fierce warriors is engaged in an epic battle with a mighty dragon.&#39;,&#xA;        &#39;The warriors, clad in armor and wielding swords and shields, are positioned on the left side of the image.&#39;,&#xA;        &#39;Their expressions are determined and focused, reflecting their resolve to defeat the dragon.&#39;,&#xA;        &#39;The dragon, with its massive wings spread wide and its fiery breath illuminating the scene, dominates the center of the image.&#39;,&#xA;        &#39;Its scales glisten with a metallic sheen, and its eyes burn with a fierce intensity.&#39;,&#xA;        &#39;The background is a dark, stormy sky with flashes of lightning, adding to the drama and tension of the battle.&#39;,&#xA;        &#39;The ground is littered with debris and the remnants of previous battles, enhancing the sense of a long and brutal conflict.&#39;,&#xA;        &#39;The overall atmosphere is one of chaos and intensity, with the warriors and the dragon locked in a fierce struggle for survival.&#39;,&#xA;    ],&#xA;    tags=&#39;battle, warriors, dragon, fierce, armor, swords, shields, determined, focused, epic, intense, metallic, glistening, fiery breath, stormy sky, lightning, debris, conflict, struggle, survival, chaos, tension, drama, wings, scales, eyes, burning, illuminated&#39;,&#xA;    HTML_web_color_name=&#39;darkslategray&#39;,&#xA;)&#xA;&#xA;# Add a group of fierce warriors.&#xA;canvas.add_local_description(&#xA;    location=&#39;on the left&#39;,&#xA;    offset=&#39;no offset&#39;,&#xA;    area=&#39;a large horizontal area&#39;,&#xA;    distance_to_viewer=5.0,&#xA;    description=&#39;A group of fierce warriors.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;The warriors, clad in gleaming armor, are positioned on the left side of the image.&#39;,&#xA;        &#39;They are armed with swords, shields, and spears, ready for battle.&#39;,&#xA;        &#39;Their faces are set with determination and focus, reflecting their resolve to defeat the dragon.&#39;,&#xA;        &#39;Some warriors are in mid-action, swinging their swords or shields, while others stand ready to strike.&#39;,&#xA;        &#39;The armor they wear is intricately designed, with symbols and patterns that signify their rank and bravery.&#39;,&#xA;        &#39;The ground beneath them is uneven and littered with debris, adding to the sense of a chaotic and intense battle.&#39;,&#xA;    ],&#xA;    tags=&#39;warriors, armor, swords, shields, spears, determined, focused, mid-action, intricate design, symbols, patterns, rank, bravery, uneven ground, debris, chaotic, intense, battle&#39;,&#xA;    atmosphere=&#39;Determined and focused, ready for the fierce battle.&#39;,&#xA;    style=&#39;Highly detailed and dynamic, capturing the intensity of the warriors.&#39;,&#xA;    quality_meta=&#39;High resolution with intricate details and dynamic poses.&#39;,&#xA;    HTML_web_color_name=&#39;darkgoldenrod&#39;,&#xA;)&#xA;&#xA;# Add a mighty dragon.&#xA;canvas.add_local_description(&#xA;    location=&#39;in the center&#39;,&#xA;    offset=&#39;no offset&#39;,&#xA;    area=&#39;a large square area&#39;,&#xA;    distance_to_viewer=7.0,&#xA;    description=&#39;A mighty dragon.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;The dragon is a massive creature, dominating the center of the image with its wide-spread wings and fiery breath.&#39;,&#xA;        &#39;Its scales glisten with a metallic sheen, reflecting the light from its fiery breath.&#39;,&#xA;        &#34;The dragon&#39;s eyes burn with a fierce intensity, and its teeth are sharp and menacing.&#34;,&#xA;        &#39;The wings of the dragon are powerful and spread wide, casting shadows over the battlefield.&#39;,&#xA;        &#39;The dragon’s body is muscular and covered in protective scales, with a long, sinewy tail that adds to its formidable appearance.&#39;,&#xA;        &#39;The fiery breath of the dragon illuminates the scene, casting a reddish glow over the warriors and the battlefield.&#39;,&#xA;    ],&#xA;    tags=&#39;dragon, massive, wings, fiery breath, glistening scales, metallic sheen, fierce eyes, sharp teeth, powerful wings, shadows, battlefield, muscular body, protective scales, sinewy tail, formidable, reddish glow, illumination&#39;,&#xA;    atmosphere=&#39;Intense and menacing, with a powerful presence.&#39;,&#xA;    style=&#39;Epic and dramatic, emphasizing the grandeur and danger of the dragon.&#39;,&#xA;    quality_meta=&#39;High resolution with dramatic lighting and detailed textures.&#39;,&#xA;    HTML_web_color_name=&#39;firebrick&#39;,&#xA;)&#xA;&#xA;# Add a stormy sky with flashes of lightning.&#xA;canvas.add_local_description(&#xA;    location=&#39;on the top&#39;,&#xA;    offset=&#39;no offset&#39;,&#xA;    area=&#39;a large horizontal area&#39;,&#xA;    distance_to_viewer=10.0,&#xA;    description=&#39;A stormy sky with flashes of lightning.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;The background of the image is a dark, stormy sky filled with swirling clouds and flashes of lightning.&#39;,&#xA;        &#39;The sky is turbulent, with clouds dark and foreboding, adding to the dramatic tension of the battle.&#39;,&#xA;        &#39;The lightning flashes illuminate the scene, casting sharp, brief lights over the warriors and the dragon.&#39;,&#xA;        &#39;The stormy sky creates a sense of chaos and unpredictability, heightening the intensity of the battle below.&#39;,&#xA;        &#39;The overall atmosphere is one of impending doom and relentless conflict, with the storm mirroring the fierce struggle between the warriors and the dragon.&#39;,&#xA;    ],&#xA;    tags=&#39;stormy sky, dark clouds, lightning, turbulent, foreboding, dramatic tension, illumination, chaos, unpredictability, intensity, impending doom, relentless conflict, battle, warriors, dragon, swirling clouds, sharp lights, brief lights&#39;,&#xA;    atmosphere=&#39;Chaotic and intense, mirroring the fierce battle below.&#39;,&#xA;    style=&#39;Dramatic and turbulent, emphasizing the conflict and tension.&#39;,&#xA;    quality_meta=&#39;High resolution with dynamic lighting and detailed cloud textures.&#39;,&#xA;    HTML_web_color_name=&#39;midnightblue&#39;,&#xA;)&#xA;&#xA;# Add a debris-covered battlefield.&#xA;canvas.add_local_description(&#xA;    location=&#39;on the bottom&#39;,&#xA;    offset=&#39;no offset&#39;,&#xA;    area=&#39;a large horizontal area&#39;,&#xA;    distance_to_viewer=5.0,&#xA;    description=&#39;A debris-covered battlefield.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;The ground of the battlefield is littered with debris, remnants of previous battles.&#39;,&#xA;        &#39;Broken weapons, shattered shields, and scattered armor pieces are strewn across the battlefield.&#39;,&#xA;        &#39;The terrain is uneven, with patches of mud and dirt, adding to the sense of a long and brutal conflict.&#39;,&#xA;        &#39;The debris-covered battlefield enhances the chaotic and intense atmosphere of the scene, reflecting the ferocity and duration of the battle.&#39;,&#xA;        &#39;The overall appearance is one of destruction and turmoil, with the remnants of previous battles serving as a grim reminder of the ongoing struggle.&#39;,&#xA;    ],&#xA;    tags=&#39;battlefield, debris, broken weapons, shattered shields, scattered armor, uneven terrain, mud, dirt, brutal conflict, chaos, intensity, destruction, turmoil, previous battles, ongoing struggle, remnants, ferocity, duration&#39;,&#xA;    atmosphere=&#39;Chaotic and intense, reflecting the ferocity of the battle.&#39;,&#xA;    style=&#39;Detailed and realistic, emphasizing the destruction and chaos.&#39;,&#xA;    quality_meta=&#39;High resolution with detailed textures and realistic debris.&#39;,&#xA;    HTML_web_color_name=&#39;darkolivegreen&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;User&lt;/h3&gt; &#xA;&lt;p&gt;change the dragon to a dinosaur&lt;/p&gt; &#xA;&lt;h3&gt;Assistant&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Initialize the canvas&#xA;canvas = Canvas()&#xA;&#xA;# Set a global description for the canvas&#xA;canvas.set_global_description(&#xA;    description=&#39;A fierce battle between warriors and a dinosaur.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;In this intense scene, a group of fierce warriors is engaged in an epic battle with a mighty dinosaur.&#39;,&#xA;        &#39;The warriors, clad in armor and wielding swords and shields, are positioned on the left side of the image.&#39;,&#xA;        &#39;Their expressions are determined and focused, reflecting their resolve to defeat the dinosaur.&#39;,&#xA;        &#39;The dinosaur, with its massive body and powerful legs, dominates the center of the image.&#39;,&#xA;        &#39;Its scales glisten with a metallic sheen, and its eyes burn with a fierce intensity.&#39;,&#xA;        &#39;The background is a dark, stormy sky with flashes of lightning, adding to the drama and tension of the battle.&#39;,&#xA;        &#39;The ground is littered with debris and the remnants of previous battles, enhancing the sense of a long and brutal conflict.&#39;,&#xA;        &#39;The overall atmosphere is one of chaos and intensity, with the warriors and the dinosaur locked in a fierce struggle for survival.&#39;,&#xA;    ],&#xA;    tags=&#39;battle, warriors, dinosaur, fierce, armor, swords, shields, determined, focused, epic, intense, metallic, glistening, stormy sky, lightning, debris, conflict, struggle, survival, chaos, tension, drama, powerful legs, scales, eyes, burning, illuminated&#39;,&#xA;    HTML_web_color_name=&#39;darkslategray&#39;,&#xA;)&#xA;&#xA;# Add a group of fierce warriors.&#xA;canvas.add_local_description(&#xA;    location=&#39;on the left&#39;,&#xA;    offset=&#39;no offset&#39;,&#xA;    area=&#39;a large horizontal area&#39;,&#xA;    distance_to_viewer=5.0,&#xA;    description=&#39;A group of fierce warriors.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;The warriors, clad in gleaming armor, are positioned on the left side of the image.&#39;,&#xA;        &#39;They are armed with swords, shields, and spears, ready for battle.&#39;,&#xA;        &#39;Their faces are set with determination and focus, reflecting their resolve to defeat the dinosaur.&#39;,&#xA;        &#39;Some warriors are in mid-action, swinging their swords or shields, while others stand ready to strike.&#39;,&#xA;        &#39;The armor they wear is intricately designed, with symbols and patterns that signify their rank and bravery.&#39;,&#xA;        &#39;The ground beneath them is uneven and littered with debris, adding to the sense of a chaotic and intense battle.&#39;,&#xA;    ],&#xA;    tags=&#39;warriors, armor, swords, shields, spears, determined, focused, mid-action, intricate design, symbols, patterns, rank, bravery, uneven ground, debris, chaotic, intense, battle&#39;,&#xA;    atmosphere=&#39;Determined and focused, ready for the fierce battle.&#39;,&#xA;    style=&#39;Highly detailed and dynamic, capturing the intensity of the warriors.&#39;,&#xA;    quality_meta=&#39;High resolution with intricate details and dynamic poses.&#39;,&#xA;    HTML_web_color_name=&#39;darkgoldenrod&#39;,&#xA;)&#xA;&#xA;# Add a mighty dinosaur.&#xA;canvas.add_local_description(&#xA;    location=&#39;in the center&#39;,&#xA;    offset=&#39;no offset&#39;,&#xA;    area=&#39;a large square area&#39;,&#xA;    distance_to_viewer=7.0,&#xA;    description=&#39;A mighty dinosaur.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;The dinosaur is a massive creature, dominating the center of the image with its powerful legs and fierce roar.&#39;,&#xA;        &#39;Its scales glisten with a metallic sheen, reflecting the light from the stormy sky.&#39;,&#xA;        &#34;The dinosaur&#39;s eyes burn with a fierce intensity, and its teeth are sharp and menacing.&#34;,&#xA;        &#39;The powerful legs of the dinosaur are muscular and add to its formidable appearance.&#39;,&#xA;        &#39;The ground shakes beneath its feet, enhancing the sense of a long and brutal conflict.&#39;,&#xA;        &#39;The overall appearance is one of destruction and turmoil, with the dinosaur adding to the chaotic atmosphere.&#39;,&#xA;    ],&#xA;    tags=&#39;dinosaur, massive, powerful legs, fierce roar, glistening scales, metallic sheen, fierce eyes, sharp teeth, muscular legs, formidable, ground shaking, destruction, turmoil, chaos, battle&#39;,&#xA;    atmosphere=&#39;Intense and menacing, with a powerful presence.&#39;,&#xA;    style=&#39;Epic and dramatic, emphasizing the grandeur and danger of the dinosaur.&#39;,&#xA;    quality_meta=&#39;High resolution with dramatic lighting and detailed textures.&#39;,&#xA;    HTML_web_color_name=&#39;firebrick&#39;,&#xA;)&#xA;&#xA;# Add a stormy sky with flashes of lightning.&#xA;canvas.add_local_description(&#xA;    location=&#39;on the top&#39;,&#xA;    offset=&#39;no offset&#39;,&#xA;    area=&#39;a large horizontal area&#39;,&#xA;    distance_to_viewer=10.0,&#xA;    description=&#39;A stormy sky with flashes of lightning.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;The background of the image is a dark, stormy sky filled with swirling clouds and flashes of lightning.&#39;,&#xA;        &#39;The sky is turbulent, with clouds dark and foreboding, adding to the dramatic tension of the battle.&#39;,&#xA;        &#39;The lightning flashes illuminate the scene, casting sharp, brief lights over the warriors and the dinosaur.&#39;,&#xA;        &#39;The stormy sky creates a sense of chaos and unpredictability, heightening the intensity of the battle below.&#39;,&#xA;        &#39;The overall atmosphere is one of impending doom and relentless conflict, with the storm mirroring the fierce struggle between the warriors and the dinosaur.&#39;,&#xA;    ],&#xA;    tags=&#39;stormy sky, dark clouds, lightning, turbulent, foreboding, dramatic tension, illumination, chaos, unpredictability, intensity, impending doom, relentless conflict, battle, warriors, dinosaur, swirling clouds, sharp lights, brief lights&#39;,&#xA;    atmosphere=&#39;Chaotic and intense, mirroring the fierce battle below.&#39;,&#xA;    style=&#39;Dramatic and turbulent, emphasizing the conflict and tension.&#39;,&#xA;    quality_meta=&#39;High resolution with dynamic lighting and detailed cloud textures.&#39;,&#xA;    HTML_web_color_name=&#39;midnightblue&#39;,&#xA;)&#xA;&#xA;# Add a debris-covered battlefield.&#xA;canvas.add_local_description(&#xA;    location=&#39;on the bottom&#39;,&#xA;    offset=&#39;no offset&#39;,&#xA;    area=&#39;a large horizontal area&#39;,&#xA;    distance_to_viewer=5.0,&#xA;    description=&#39;A debris-covered battlefield.&#39;,&#xA;    detailed_descriptions=[&#xA;        &#39;The ground of the battlefield is littered with debris, remnants of previous battles.&#39;,&#xA;        &#39;Broken weapons, shattered shields, and scattered armor pieces are strewn across the battlefield.&#39;,&#xA;        &#39;The terrain is uneven, with patches of mud and dirt, adding to the sense of a long and brutal conflict.&#39;,&#xA;        &#39;The debris-covered battlefield enhances the chaotic and intense atmosphere of the scene, reflecting the ferocity and duration of the battle.&#39;,&#xA;        &#39;The overall appearance is one of destruction and turmoil, with the remnants of previous battles serving as a grim reminder of the ongoing struggle.&#39;,&#xA;    ],&#xA;    tags=&#39;battlefield, debris, broken weapons, shattered shields, scattered armor, uneven terrain, mud, dirt, brutal conflict, chaos, intensity, destruction, turmoil, previous battles, ongoing struggle, remnants, ferocity, duration&#39;,&#xA;    atmosphere=&#39;Chaotic and intense, reflecting the ferocity of the battle.&#39;,&#xA;    style=&#39;Detailed and realistic, emphasizing the destruction and chaos.&#39;,&#xA;    quality_meta=&#39;High resolution with detailed textures and realistic debris.&#39;,&#xA;    HTML_web_color_name=&#39;darkolivegreen&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(User can click the render button to actually render the image from any of those &lt;code&gt;Canvas&lt;/code&gt; instances.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/c8d56f4c-f3e3-4338-8ffc-d9b14445fb54&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/23d8b0fb-b504-442f-a449-48a7bf613bb4&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/39434e27-aaf9-441f-a76a-3850ff480c5f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Symbols&lt;/h1&gt; &#xA;&lt;p&gt;All Omost LLMs are trained to obey the following symbols&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class Canvas:&#xA;    def set_global_description(&#xA;            self, &#xA;            description: str, &#xA;            detailed_descriptions: list[str], &#xA;            tags: str, &#xA;            HTML_web_color_name: str&#xA;    ):&#xA;        pass&#xA;&#xA;    def add_local_description(&#xA;            self, &#xA;            location: str, &#xA;            offset: str, &#xA;            area: str, &#xA;            distance_to_viewer: float, &#xA;            description: str, &#xA;            detailed_descriptions: list[str], &#xA;            tags: str, &#xA;            atmosphere: str, &#xA;            style: str, &#xA;            quality_meta: str, &#xA;            HTML_web_color_name: str&#xA;    ):&#xA;        assert location in [&#xA;            &#34;in the center&#34;, &#xA;            &#34;on the left&#34;, &#xA;            &#34;on the right&#34;, &#xA;            &#34;on the top&#34;, &#xA;            &#34;on the bottom&#34;, &#xA;            &#34;on the top-left&#34;, &#xA;            &#34;on the top-right&#34;, &#xA;            &#34;on the bottom-left&#34;, &#xA;            &#34;on the bottom-right&#34;&#xA;        ]&#xA;        assert offset in [&#xA;            &#34;no offset&#34;, &#xA;            &#34;slightly to the left&#34;, &#xA;            &#34;slightly to the right&#34;, &#xA;            &#34;slightly to the upper&#34;, &#xA;            &#34;slightly to the lower&#34;, &#xA;            &#34;slightly to the upper-left&#34;, &#xA;            &#34;slightly to the upper-right&#34;, &#xA;            &#34;slightly to the lower-left&#34;, &#xA;            &#34;slightly to the lower-right&#34;&#xA;        ]&#xA;        assert area in [&#xA;            &#34;a small square area&#34;, &#xA;            &#34;a small vertical area&#34;, &#xA;            &#34;a small horizontal area&#34;, &#xA;            &#34;a medium-sized square area&#34;, &#xA;            &#34;a medium-sized vertical area&#34;, &#xA;            &#34;a medium-sized horizontal area&#34;, &#xA;            &#34;a large square area&#34;, &#xA;            &#34;a large vertical area&#34;, &#xA;            &#34;a large horizontal area&#34;&#xA;        ]&#xA;        assert distance_to_viewer &amp;gt; 0&#xA;        pass&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During training, the above symbols are associated with specific concepts and use cases related to image generation.&lt;/p&gt; &#xA;&lt;p&gt;The design is to make those codes easy to learn for LLMs, but also easy to handle for diffusion models.&lt;/p&gt; &#xA;&lt;p&gt;Lets breakdown each part:&lt;/p&gt; &#xA;&lt;h2&gt;Function: Canvas.set_global_description and Canvas.add_local_description&lt;/h2&gt; &#xA;&lt;p&gt;They set descriptions to images. The meanings of the parameters are same for them, with &lt;code&gt;add_local_description&lt;/code&gt; having more fields than &lt;code&gt;set_global_description&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;set_global_description&lt;/code&gt; annotate entire image, while &lt;code&gt;add_local_description&lt;/code&gt; annotates a part of image.&lt;/p&gt; &#xA;&lt;h2&gt;Parameter: description and detailed_descriptions&lt;/h2&gt; &#xA;&lt;p&gt;Let us introduce a concept called &#34;sub-prompt&#34;. If a prompt is less than 75 tokens, and is self-supported to describe a thing without relying on other prompts, we call it a &#34;sub-prompt&#34;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;description&lt;/code&gt; is a sub-prompt, and the &lt;code&gt;detailed_descriptions&lt;/code&gt; is a list of sub-prompts.&lt;/p&gt; &#xA;&lt;p&gt;Note that each sub-prompt is strictly less than 75 tokens (and typically less than 40 tokens), you can safely encode them with any clip without worrying the truncation position affecting the semantics.&lt;/p&gt; &#xA;&lt;p&gt;The design of sub-prompt also allows more satisfying text encoding based on greedy merge. For example, if you have&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sub-prompt A: 25 tokens&#xA;sub-prompt B: 35 tokens&#xA;sub-prompt C: 5 tokens&#xA;sub-prompt D: 60 tokens&#xA;sub-prompt E: 15 tokens&#xA;sub-prompt F: 25 tokens&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and since every sub-prompt is promised to be self-supported to describe a thing independently, we can use greedy method to merge them to bags like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bag 1 {A, B, C} : 65 tokens&#xA;bag 2 {D} : 60 tokens&#xA;bag 1 {E, F} : 40 tokens&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;where each bag is less than 75 tokens and can be encoded by any clip in one pass (and then concat them).&lt;/p&gt; &#xA;&lt;p&gt;Encoding texts in this way will make sure that text-encoder will never make semantic truncation mistakes.&lt;/p&gt; &#xA;&lt;p&gt;One may ask - if all sub-prompts are less than 75 tokens with independent semantics, why not just encode them without merge and then concat? This is mainly because we want the text embedding to be more coherent. For example, lets say sub-prompt A is &#34;a man&#34; while sub-prompt B is &#34;handsome, professional&#34;, then merging them before encoding will give you a more mixed text embedding concept with coherent features of a handsome professional man.&lt;/p&gt; &#xA;&lt;p&gt;All Omost LLMs are trained to give strictly well-defined sub-prompts. You can make use of these definitions to design lossless text encoding methods.&lt;/p&gt; &#xA;&lt;h3&gt;Parameter: location, offset, area&lt;/h3&gt; &#xA;&lt;p&gt;The three parameters defines a bounding box. Note that they must obey&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;assert location in [&#xA;    &#34;in the center&#34;, &#xA;    &#34;on the left&#34;, &#xA;    &#34;on the right&#34;, &#xA;    &#34;on the top&#34;, &#xA;    &#34;on the bottom&#34;, &#xA;    &#34;on the top-left&#34;, &#xA;    &#34;on the top-right&#34;, &#xA;    &#34;on the bottom-left&#34;, &#xA;    &#34;on the bottom-right&#34;&#xA;]&#xA;assert offset in [&#xA;    &#34;no offset&#34;, &#xA;    &#34;slightly to the left&#34;, &#xA;    &#34;slightly to the right&#34;, &#xA;    &#34;slightly to the upper&#34;, &#xA;    &#34;slightly to the lower&#34;, &#xA;    &#34;slightly to the upper-left&#34;, &#xA;    &#34;slightly to the upper-right&#34;, &#xA;    &#34;slightly to the lower-left&#34;, &#xA;    &#34;slightly to the lower-right&#34;&#xA;]&#xA;assert area in [&#xA;    &#34;a small square area&#34;, &#xA;    &#34;a small vertical area&#34;, &#xA;    &#34;a small horizontal area&#34;, &#xA;    &#34;a medium-sized square area&#34;, &#xA;    &#34;a medium-sized vertical area&#34;, &#xA;    &#34;a medium-sized horizontal area&#34;, &#xA;    &#34;a large square area&#34;, &#xA;    &#34;a large vertical area&#34;, &#xA;    &#34;a large horizontal area&#34;&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;First we divide a canvas into 3*3=9 locations:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/5d39cf93-c229-4c83-ae82-3eeeae2fabea&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then we further divide each location to 3*3 offsets, resulting in 9*9=81 positions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/b744d787-11f3-4aeb-9d3a-aeba7a41b433&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Using these positions as centers, we further define 9 types of bounding boxes:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/0e484b73-680f-486b-8b61-4373c9eec9a0&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We can see that this method allows 9*9*9=729 different bounding boxes, covering almost all common possible locations of an object in the image.&lt;/p&gt; &#xA;&lt;p&gt;One may argue that why this is necessary - why not just let the LLMs to learn pixel index or x, y coordinates - and should that be much more accurate? Below is several of my notes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;I have tried several representations, including pixel index like {x=32, y=16, w=58, h=99}, or margin pixels like {left=32, right=15, top=27, bottom=33}, or percentage pixel index like {x=0.124, y=0.65, w=0.335, h=0.251}, or percentage margin like {left=0.251, right=0.154, top=0.254, bottom=0.441}. The result is that opensource LLMs are really not very good at learning these representations even for Llama3 (perhaps GPT4o can learn it). Sometimes it works sometimes it gives completely random numbers. Note that our problem is very different from MLLM. The vision-LLM usually have image embedding as inputs and in that case estimating numeric position is like a look-up table problem and can somewhat be learned, but our case is where the LLM need to generate every composition from scratch without help of any image embedding to look-up.&lt;/li&gt; &#xA; &lt;li&gt;But the natural language like &#34;on the right&#34;, &#34;slightly to the top-right&#34;, &#34;a small vertical area&#34; etc, works very well. The model converges very fast and the learning is stable. It aligns to the pretrained knowledge of LLMs very well.&lt;/li&gt; &#xA; &lt;li&gt;I have also tried adding some special tokens to represent spatial locations and also train the embedding layers. But that model is very difficult to train and debug. Also, the token-embedding-based method needs many hyperparameter tuning everytime we change the LLM - for example when changing from Llama3 to Phi, if we use the token-embedding method, we need to design training parameters again.&lt;/li&gt; &#xA; &lt;li&gt;The number 9*9*9=729 is not really a small number from the perspective of bounding box proposals. This can also be called ROI (region of interest) and some old semantic segmentation tech uses (RPN) Region Proposal Network to produce a similar number (&amp;lt;1000) of regions.&lt;/li&gt; &#xA; &lt;li&gt;Most region-guided diffusion methods are coarse-level methods (like multi-diffusion and attention couple and gligen), and they do not need pixel-perfect regions.&lt;/li&gt; &#xA; &lt;li&gt;These are very personal results from me - if you are working on some similar multi-modal LLM research, using pixel indices is completely okay, worth trying, and probably other training methods can also achieve a robust system.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Parameter: distance_to_viewer and HTML_web_color_name&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;code&gt;distance_to_viewer&lt;/code&gt; can be viewed as relative depth. Note that this value&#39;s absolute number is not reliable at all (because opensource LLMs are not very good at producing image-space numbers) and it should only be used in sorting elements into background-to-foreground layers.&lt;/p&gt; &#xA;&lt;p&gt;You can always use &lt;code&gt;distance_to_viewer&lt;/code&gt; to sort all local elements before rendering them using a diffusion model. The global description can be always viewed as the most far away background layer.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;HTML_web_color_name&lt;/code&gt; is one of these:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;possible_HTML_web_color_names = {  # r, g, b&#xA;    &#39;aliceblue&#39;: (240, 248, 255), &#39;antiquewhite&#39;: (250, 235, 215), &#39;aqua&#39;: (0, 255, 255),&#xA;    &#39;aquamarine&#39;: (127, 255, 212), &#39;azure&#39;: (240, 255, 255), &#39;beige&#39;: (245, 245, 220),&#xA;    &#39;bisque&#39;: (255, 228, 196), &#39;black&#39;: (0, 0, 0), &#39;blanchedalmond&#39;: (255, 235, 205), &#39;blue&#39;: (0, 0, 255),&#xA;    &#39;blueviolet&#39;: (138, 43, 226), &#39;brown&#39;: (165, 42, 42), &#39;burlywood&#39;: (222, 184, 135),&#xA;    &#39;cadetblue&#39;: (95, 158, 160), &#39;chartreuse&#39;: (127, 255, 0), &#39;chocolate&#39;: (210, 105, 30),&#xA;    &#39;coral&#39;: (255, 127, 80), &#39;cornflowerblue&#39;: (100, 149, 237), &#39;cornsilk&#39;: (255, 248, 220),&#xA;    &#39;crimson&#39;: (220, 20, 60), &#39;cyan&#39;: (0, 255, 255), &#39;darkblue&#39;: (0, 0, 139), &#39;darkcyan&#39;: (0, 139, 139),&#xA;    &#39;darkgoldenrod&#39;: (184, 134, 11), &#39;darkgray&#39;: (169, 169, 169), &#39;darkgrey&#39;: (169, 169, 169),&#xA;    &#39;darkgreen&#39;: (0, 100, 0), &#39;darkkhaki&#39;: (189, 183, 107), &#39;darkmagenta&#39;: (139, 0, 139),&#xA;    &#39;darkolivegreen&#39;: (85, 107, 47), &#39;darkorange&#39;: (255, 140, 0), &#39;darkorchid&#39;: (153, 50, 204),&#xA;    &#39;darkred&#39;: (139, 0, 0), &#39;darksalmon&#39;: (233, 150, 122), &#39;darkseagreen&#39;: (143, 188, 143),&#xA;    &#39;darkslateblue&#39;: (72, 61, 139), &#39;darkslategray&#39;: (47, 79, 79), &#39;darkslategrey&#39;: (47, 79, 79),&#xA;    &#39;darkturquoise&#39;: (0, 206, 209), &#39;darkviolet&#39;: (148, 0, 211), &#39;deeppink&#39;: (255, 20, 147),&#xA;    &#39;deepskyblue&#39;: (0, 191, 255), &#39;dimgray&#39;: (105, 105, 105), &#39;dimgrey&#39;: (105, 105, 105),&#xA;    &#39;dodgerblue&#39;: (30, 144, 255), &#39;firebrick&#39;: (178, 34, 34), &#39;floralwhite&#39;: (255, 250, 240),&#xA;    &#39;forestgreen&#39;: (34, 139, 34), &#39;fuchsia&#39;: (255, 0, 255), &#39;gainsboro&#39;: (220, 220, 220),&#xA;    &#39;ghostwhite&#39;: (248, 248, 255), &#39;gold&#39;: (255, 215, 0), &#39;goldenrod&#39;: (218, 165, 32),&#xA;    &#39;gray&#39;: (128, 128, 128), &#39;grey&#39;: (128, 128, 128), &#39;green&#39;: (0, 128, 0), &#39;greenyellow&#39;: (173, 255, 47),&#xA;    &#39;honeydew&#39;: (240, 255, 240), &#39;hotpink&#39;: (255, 105, 180), &#39;indianred&#39;: (205, 92, 92),&#xA;    &#39;indigo&#39;: (75, 0, 130), &#39;ivory&#39;: (255, 255, 240), &#39;khaki&#39;: (240, 230, 140), &#39;lavender&#39;: (230, 230, 250),&#xA;    &#39;lavenderblush&#39;: (255, 240, 245), &#39;lawngreen&#39;: (124, 252, 0), &#39;lemonchiffon&#39;: (255, 250, 205),&#xA;    &#39;lightblue&#39;: (173, 216, 230), &#39;lightcoral&#39;: (240, 128, 128), &#39;lightcyan&#39;: (224, 255, 255),&#xA;    &#39;lightgoldenrodyellow&#39;: (250, 250, 210), &#39;lightgray&#39;: (211, 211, 211), &#39;lightgrey&#39;: (211, 211, 211),&#xA;    &#39;lightgreen&#39;: (144, 238, 144), &#39;lightpink&#39;: (255, 182, 193), &#39;lightsalmon&#39;: (255, 160, 122),&#xA;    &#39;lightseagreen&#39;: (32, 178, 170), &#39;lightskyblue&#39;: (135, 206, 250), &#39;lightslategray&#39;: (119, 136, 153),&#xA;    &#39;lightslategrey&#39;: (119, 136, 153), &#39;lightsteelblue&#39;: (176, 196, 222), &#39;lightyellow&#39;: (255, 255, 224),&#xA;    &#39;lime&#39;: (0, 255, 0), &#39;limegreen&#39;: (50, 205, 50), &#39;linen&#39;: (250, 240, 230), &#39;magenta&#39;: (255, 0, 255),&#xA;    &#39;maroon&#39;: (128, 0, 0), &#39;mediumaquamarine&#39;: (102, 205, 170), &#39;mediumblue&#39;: (0, 0, 205),&#xA;    &#39;mediumorchid&#39;: (186, 85, 211), &#39;mediumpurple&#39;: (147, 112, 219), &#39;mediumseagreen&#39;: (60, 179, 113),&#xA;    &#39;mediumslateblue&#39;: (123, 104, 238), &#39;mediumspringgreen&#39;: (0, 250, 154),&#xA;    &#39;mediumturquoise&#39;: (72, 209, 204), &#39;mediumvioletred&#39;: (199, 21, 133), &#39;midnightblue&#39;: (25, 25, 112),&#xA;    &#39;mintcream&#39;: (245, 255, 250), &#39;mistyrose&#39;: (255, 228, 225), &#39;moccasin&#39;: (255, 228, 181),&#xA;    &#39;navajowhite&#39;: (255, 222, 173), &#39;navy&#39;: (0, 0, 128), &#39;navyblue&#39;: (0, 0, 128),&#xA;    &#39;oldlace&#39;: (253, 245, 230), &#39;olive&#39;: (128, 128, 0), &#39;olivedrab&#39;: (107, 142, 35),&#xA;    &#39;orange&#39;: (255, 165, 0), &#39;orangered&#39;: (255, 69, 0), &#39;orchid&#39;: (218, 112, 214),&#xA;    &#39;palegoldenrod&#39;: (238, 232, 170), &#39;palegreen&#39;: (152, 251, 152), &#39;paleturquoise&#39;: (175, 238, 238),&#xA;    &#39;palevioletred&#39;: (219, 112, 147), &#39;papayawhip&#39;: (255, 239, 213), &#39;peachpuff&#39;: (255, 218, 185),&#xA;    &#39;peru&#39;: (205, 133, 63), &#39;pink&#39;: (255, 192, 203), &#39;plum&#39;: (221, 160, 221), &#39;powderblue&#39;: (176, 224, 230),&#xA;    &#39;purple&#39;: (128, 0, 128), &#39;rebeccapurple&#39;: (102, 51, 153), &#39;red&#39;: (255, 0, 0),&#xA;    &#39;rosybrown&#39;: (188, 143, 143), &#39;royalblue&#39;: (65, 105, 225), &#39;saddlebrown&#39;: (139, 69, 19),&#xA;    &#39;salmon&#39;: (250, 128, 114), &#39;sandybrown&#39;: (244, 164, 96), &#39;seagreen&#39;: (46, 139, 87),&#xA;    &#39;seashell&#39;: (255, 245, 238), &#39;sienna&#39;: (160, 82, 45), &#39;silver&#39;: (192, 192, 192),&#xA;    &#39;skyblue&#39;: (135, 206, 235), &#39;slateblue&#39;: (106, 90, 205), &#39;slategray&#39;: (112, 128, 144),&#xA;    &#39;slategrey&#39;: (112, 128, 144), &#39;snow&#39;: (255, 250, 250), &#39;springgreen&#39;: (0, 255, 127),&#xA;    &#39;steelblue&#39;: (70, 130, 180), &#39;tan&#39;: (210, 180, 140), &#39;teal&#39;: (0, 128, 128), &#39;thistle&#39;: (216, 191, 216),&#xA;    &#39;tomato&#39;: (255, 99, 71), &#39;turquoise&#39;: (64, 224, 208), &#39;violet&#39;: (238, 130, 238),&#xA;    &#39;wheat&#39;: (245, 222, 179), &#39;white&#39;: (255, 255, 255), &#39;whitesmoke&#39;: (245, 245, 245),&#xA;    &#39;yellow&#39;: (255, 255, 0), &#39;yellowgreen&#39;: (154, 205, 50)&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By combining &lt;code&gt;distance_to_viewer&lt;/code&gt; and &lt;code&gt;HTML_web_color_name&lt;/code&gt;, you can draw a very coarse image of the composition. For example, if the LLM works well, &#34;a green bottle in front of a red bottle on a wood table in a dark room&#34; should make it possible for you to compute an image like:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/ab501872-bbcc-4fd4-8ab4-6fecd1a44d4d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use this image as an initial latent and use denoise strength like 0.95 to 0.99 to generate the image.&lt;/p&gt; &#xA;&lt;p&gt;Or if you do not like this and still prefer to let diffusion models to generate from zero-mean (even when you know that most diffusion models have tsnr problems), you can ignore this image and or just use this image as a debugger.&lt;/p&gt; &#xA;&lt;p&gt;Besides, the layer sorting can also be useful in some very special attention formulation - we will discuss this later.&lt;/p&gt; &#xA;&lt;h1&gt;Parameter: tags and atmosphere and style and quality_meta&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;code&gt;tags&lt;/code&gt; is designed as a possible replacement for the &lt;code&gt;description&lt;/code&gt; since many diffusion models prefer tags. If used with anime models, one may hard code some logics to replace all &#34;girl&#34; to &#34;1girl&#34;. If used with Pony then probably always hard code adding &#34;score_9, score_8 ...&#34; to this.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;atmosphere&lt;/code&gt; and &lt;code&gt;style&lt;/code&gt; and &lt;code&gt;quality_meta&lt;/code&gt; are some experimental parameters without very specific use cases. Current we can just treat them as sub-prompts and involve them in the greedy merge of sub-prompt bags. This in my experiments will improve the atmosphere and quality a bit.&lt;/p&gt; &#xA;&lt;h1&gt;A Baseline Renderer&lt;/h1&gt; &#xA;&lt;p&gt;In this repo, we provide a baseline render for Omost LLMs based on attention manipulation.&lt;/p&gt; &#xA;&lt;h3&gt;Regional Prompter&lt;/h3&gt; &#xA;&lt;p&gt;As of 2024, if we want to achieve a region guided diffusion system, some possible options are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;multi-diffusion / mixture-of-diffusers: these method run UNet on different locations, and then merge the estimated epsilon or x0 using weights or masks for different regions.&lt;/li&gt; &#xA; &lt;li&gt;attention decomposition: lets say attention is like &lt;code&gt;y=softmax(q@k)@v&lt;/code&gt;, then one can achieve attention decomposition like &lt;code&gt;y=mask_A * softmax(q@k_A)@v_A + mask_B * softmax(q@k_B)@v_B&lt;/code&gt; where mask_A, k_A, v_A are masks, k, v for region A; mask_B, k_B, v_B are masks, k, v for region B. This method usually yields image quality a bit better than (1) and some people call it Attention Couple or Region Prompter Attention Mode. But this method has a consideration: the mask only makes regional attention numerically possible, but it does not force the UNet to really attend its activations in those regions. That is to say, the attention is indeed masked, but there is no promise that the attention softmax will really be activated in the masked area, and there is also no promise that the attention softmax will never be activated outside the masked area.&lt;/li&gt; &#xA; &lt;li&gt;attention score manipulation: this is a more advanced method compared to (2). It directly manipulates the attention scores to make sure that the activations in mask each area are encouraged and those outside the masks are discouraged. The formulation is like &lt;code&gt;y=softmax(modify(q@k))@v&lt;/code&gt; where &lt;code&gt;modify()&lt;/code&gt; is a complicated non-linear function with many normalizations and tricks to change the score&#39;s distributions. This method goes beyond a simple masked attention to really make sure that those layers get wanted activations. A typical example is &lt;a href=&#34;https://github.com/naver-ai/DenseDiffusion&#34;&gt;Dense Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;gradient optimization: since the attention can tell us where each part is corresponding to what prompts, we can split prompts into segments and then get attention activations to each prompt segment. Then we compare those activations with external masks to compute a loss function, and back propagate the gradients. Those methods are usually very high quality but VRAM hungry and very slow. Typical methods are &lt;a href=&#34;https://github.com/showlab/BoxDiff&#34;&gt;BoxDiff&lt;/a&gt; and &lt;a href=&#34;https://github.com/yuval-alaluf/Attend-and-Excite&#34;&gt;Attend-and-Excite&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use external control models like gligen and &lt;a href=&#34;https://github.com/frank-xwang/InstanceDiffusion&#34;&gt;InstanceDiffusion&lt;/a&gt;. Those methods give the highest benchmark performance on region following but will also introduce some style offset to the base model since they are trained parameters. Also, those methods need to convert prompts to vectors and usually do not support prompts of arbitary length (but one can use them together with other attention methods to achieve arbitrary length).&lt;/li&gt; &#xA; &lt;li&gt;Some more possible layer options like layerdiffuse and &lt;a href=&#34;https://mulan-dataset.github.io/&#34;&gt;mulan&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;In this repo I wrote a baseline formulation based on (3). I consider this parameter-free formulation as a very standard baseline implementation that will almost introduce zero style offsets or quality degradation. In the future we may consider training some parametrized methods for Omost.&lt;/p&gt; &#xA;&lt;p&gt;Lets consider an extremely simplified image with only 2*2=4 pixels:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/00f97ad6-202b-4a39-9091-da6d76b0aacb&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then we have three prompts &#34;two cats&#34;, &#34;a black cat&#34;, &#34;a white cat&#34;, and we have their masks:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/f9f5e87c-5f82-41fe-8a49-580d3eb6f2be&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then we can draw this attention score table:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/a77936b3-050e-4894-9252-476713144f6c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;where the upper arrow mean that we want to encourage the activation, while the lower arrow means we want to get rid of those activation.&lt;/p&gt; &#xA;&lt;p&gt;This manipulation directly modify attention scores and compute all prompts conditions in one single SDP attention pass. (See also the codes for more details.)&lt;/p&gt; &#xA;&lt;h3&gt;Prompt Prefix Tree&lt;/h3&gt; &#xA;&lt;p&gt;In this repo, I also included another trick that I find out to improve prompt understanding a lot. Lets call it a Prompt Prefix Tree. The motivation is that, since now that all our prompts are sub-prompts that can be merged arbitrarily (recall that all sub-prompts are strictly less than 75 tokens and typically less than 40 tokens, describe independent concepts, and can be arbitrarily merged as common prompts for clip to encode), finding a better method to merge those sub-prompts may improve the results and prompt interpretation.&lt;/p&gt; &#xA;&lt;p&gt;For example below is a tree structure of global/local overall/detailed descriptions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/2b3a895f-9bb5-4da8-9d5d-989bac9c1a7e&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The idea is that, since all sub-prompts can be merged arbitrarily, we can use the paths in this tree graph as prompts.&lt;/p&gt; &#xA;&lt;p&gt;For example the below path will give a prompt &#34;A cat and a dog. The cat on sofa.&#34;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/Omost/assets/19834515/902ec939-65be-4232-912b-d1bc6f5da44f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that we can use this together with greedy subprompt bag merging when a path exceed 75 tokens. And, if a path has remaining place to contain more subprompts, the greedy subprompt bag merging will also take care of it. And again, since all sub prompts describe independent concepts, the greedy subprompt bag merging never makes semantic truncation mistakes. So satisfying!&lt;/p&gt; &#xA;&lt;h1&gt;Model Notes&lt;/h1&gt; &#xA;&lt;p&gt;Currently, we provide 3 models (you can get them by adding the prefix &lt;code&gt;https://huggingface.co/lllyasviel/&lt;/code&gt; to the below names):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;omost-llama-3-8b&#xA;omost-dolphin-2.9-llama3-8b&#xA;omost-phi-3-mini-128k&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And their quant versions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;omost-llama-3-8b-4bits&#xA;omost-dolphin-2.9-llama3-8b-4bits&#xA;omost-phi-3-mini-128k-8bits&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some notes:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The recommended quant for &lt;code&gt;omost-llama-3-8b&lt;/code&gt; is 4bits, and for &lt;code&gt;omost-phi-3-mini-128k&lt;/code&gt; (3.8B) is 8 bits. They all fit in 8GB VRAM without offloads. The performance degradation caused by quant is very minimal and I personally never observed any evidences of degradation. However, quant &lt;code&gt;omost-phi-3-mini-128k&lt;/code&gt; into 4 bits is not recommended since I noticed some obvious performance degradation. The 4bit inference of &lt;code&gt;omost-phi-3-mini-128k&lt;/code&gt; should be viewed as a last method in extreme cases when you really do not have more capable GPUs.&lt;/li&gt; &#xA; &lt;li&gt;My user study shows that &lt;code&gt;omost-llama-3-8b-4bits&lt;/code&gt; &amp;gt; &lt;code&gt;omost-dolphin-2.9-llama3-8b-4bits&lt;/code&gt; &amp;gt; &lt;code&gt;omost-phi-3-mini-128k-8bits&lt;/code&gt;. So in most cases one should just use &lt;code&gt;omost-llama-3-8b-4bits&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;omost-llama-3-8b&lt;/code&gt; and &lt;code&gt;omost-phi-3-mini-128k&lt;/code&gt; are trained with filtered safe data without NSFW or inappropriate contents. See (4) if you need a different option.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;omost-dolphin-2.9-llama3-8b&lt;/code&gt; is trained with all data WITHOUT any filtering. You must apply your own safety alignment methods if you expose any service of &lt;code&gt;omost-dolphin-2.9-llama3-8b&lt;/code&gt; to public.&lt;/li&gt; &#xA; &lt;li&gt;Note that the filtering in (3) is not because of any policy - the reason is that I noticed slight instability in training gradients in those models since they are pretrained with instruct following regulated by safety alignment, causing the performance to degrade a bit. But the instruct following of &lt;code&gt;omost-dolphin-2.9-llama3-8b&lt;/code&gt; is pretrained with community efforts and do not have this problem.&lt;/li&gt; &#xA; &lt;li&gt;The 128k context length of &lt;code&gt;omost-phi-3-mini-128k&lt;/code&gt; cannot be trusted. The performance of it will degrade a lot after the tokens reach about 8k. One should just view it as a model with about 8k content length.&lt;/li&gt; &#xA; &lt;li&gt;A model of 8k context length can do about 5 to 6 rounds of conversational editing. If you are about to run out of token lengths, use the UI to modify your message and respond again (this can be done with infinite times).&lt;/li&gt; &#xA; &lt;li&gt;All models are fully trained with our H100 clusters at precision fp16 without any tricks like quant or Q-LoRA etc. The optimizer is Adam without any tricks.&lt;/li&gt; &#xA; &lt;li&gt;You must also follow the licenses of Llama-3 and Phi-3.&lt;/li&gt; &#xA; &lt;li&gt;You can request us to train on other LLMs if reasonable and necessary.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Cite&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;@Misc{omost,&#xA;  author = {Omost Team},&#xA;  title  = {Omost GitHub Page},&#xA;  year   = {2024},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Related Work&lt;/h1&gt; &#xA;&lt;p&gt;Also read ...&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://google.github.io/docci/&#34;&gt;DOCCI: Descriptions of Connected and Contrasting Images&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/YangLing0818/RPG-DiffusionMaster&#34;&gt;(RPG-DiffusionMaster) Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.13655&#34;&gt;LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2311.16090&#34;&gt;Self-correcting LLM-controlled Diffusion Models&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://multidiffusion.github.io/&#34;&gt;MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/hako-mikan/sd-webui-regional-prompter&#34;&gt;sd-webui-regional-prompter&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;(please open issue or email me if you want to add more links here)&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>VikParuchuri/marker</title>
    <updated>2024-06-03T01:28:35Z</updated>
    <id>tag:github.com,2024-06-03:/VikParuchuri/marker</id>
    <link href="https://github.com/VikParuchuri/marker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Convert PDF to markdown quickly with high accuracy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Marker&lt;/h1&gt; &#xA;&lt;p&gt;Marker converts PDF to markdown quickly and accurately.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports a wide range of documents (optimized for books and scientific papers)&lt;/li&gt; &#xA; &lt;li&gt;Supports all languages&lt;/li&gt; &#xA; &lt;li&gt;Removes headers/footers/other artifacts&lt;/li&gt; &#xA; &lt;li&gt;Formats tables and code blocks&lt;/li&gt; &#xA; &lt;li&gt;Extracts and saves images along with the markdown&lt;/li&gt; &#xA; &lt;li&gt;Converts most equations to latex&lt;/li&gt; &#xA; &lt;li&gt;Works on GPU, CPU, or MPS&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How it works&lt;/h2&gt; &#xA;&lt;p&gt;Marker is a pipeline of deep learning models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extract text, OCR if necessary (heuristics, &lt;a href=&#34;https://github.com/VikParuchuri/surya&#34;&gt;surya&lt;/a&gt;, tesseract)&lt;/li&gt; &#xA; &lt;li&gt;Detect page layout and find reading order (&lt;a href=&#34;https://github.com/VikParuchuri/surya&#34;&gt;surya&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Clean and format each block (heuristics, &lt;a href=&#34;https://github.com/VikParuchuri/texify&#34;&gt;texify&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Combine blocks and postprocess complete text (heuristics, &lt;a href=&#34;https://huggingface.co/vikp/pdf_postprocessor_t5&#34;&gt;pdf_postprocessor&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It only uses models where necessary, which improves speed and accuracy.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;PDF&lt;/th&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Marker&lt;/th&gt; &#xA;   &lt;th&gt;Nougat&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://greenteapress.com/thinkpython/thinkpython.pdf&#34;&gt;Think Python&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Textbook&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/marker/thinkpython.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/nougat/thinkpython.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://greenteapress.com/thinkos/thinkos.pdf&#34;&gt;Think OS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Textbook&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/marker/thinkos.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/nougat/thinkos.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2101.03961.pdf&#34;&gt;Switch Transformers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;arXiv paper&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/marker/switch_transformers.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/nougat/switch_transformers.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/1804.07821.pdf&#34;&gt;Multi-column CNN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;arXiv paper&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/marker/multicolcnn.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/VikParuchuri/marker/raw/master/data/examples/nougat/multicolcnn.md&#34;&gt;View&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/marker/master/data/images/overall.png&#34; alt=&#34;Benchmark overall&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The above results are with marker and nougat setup so they each take ~4GB of VRAM on an A6000.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/marker/master/#benchmarks&#34;&gt;below&lt;/a&gt; for detailed speed and accuracy benchmarks, and instructions on how to run your own benchmarks.&lt;/p&gt; &#xA;&lt;h1&gt;Commercial usage&lt;/h1&gt; &#xA;&lt;p&gt;I want marker to be as widely accessible as possible, while still funding my development/training costs. Research and personal usage is always okay, but there are some restrictions on commercial usage.&lt;/p&gt; &#xA;&lt;p&gt;The weights for the models are licensed &lt;code&gt;cc-by-nc-sa-4.0&lt;/code&gt;, but I will waive that for any organization under $5M USD in gross revenue in the most recent 12-month period AND under $5M in lifetime VC/angel funding raised. If you want to remove the GPL license requirements (dual-license) and/or use the weights commercially over the revenue limit, check out the options &lt;a href=&#34;https://www.datalab.to&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Hosted API&lt;/h1&gt; &#xA;&lt;p&gt;There is a hosted API for marker available &lt;a href=&#34;https://www.datalab.to/&#34;&gt;here&lt;/a&gt;. It&#39;s currently in beta, and I&#39;m working on optimizing speed.&lt;/p&gt; &#xA;&lt;h1&gt;Community&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg//KuZwXNGnfH&#34;&gt;Discord&lt;/a&gt; is where we discuss future development.&lt;/p&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;p&gt;PDF is a tricky format, so marker will not always work perfectly. Here are some known limitations that are on the roadmap to address:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Marker will not convert 100% of equations to LaTeX. This is because it has to detect then convert.&lt;/li&gt; &#xA; &lt;li&gt;Tables are not always formatted 100% correctly - text can be in the wrong column.&lt;/li&gt; &#xA; &lt;li&gt;Whitespace and indentations are not always respected.&lt;/li&gt; &#xA; &lt;li&gt;Not all lines/spans will be joined properly.&lt;/li&gt; &#xA; &lt;li&gt;This works best on digital PDFs that won&#39;t require a lot of OCR. It&#39;s optimized for speed, and limited OCR is used to fix errors.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;You&#39;ll need python 3.9+ and PyTorch. You may need to install the CPU version of torch first if you&#39;re not using a Mac or a GPU machine. See &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install marker-pdf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Optional: OCRMyPDF&lt;/h2&gt; &#xA;&lt;p&gt;Only needed if you want to use the optional &lt;code&gt;ocrmypdf&lt;/code&gt; as the ocr backend. Note that &lt;code&gt;ocrmypdf&lt;/code&gt; includes Ghostscript, an AGPL dependency, but calls it via CLI, so it does not trigger the license provisions.&lt;/p&gt; &#xA;&lt;p&gt;See the instructions &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/marker/master/docs/install_ocrmypdf.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;p&gt;First, some configuration:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspect the settings in &lt;code&gt;marker/settings.py&lt;/code&gt;. You can override any settings with environment variables.&lt;/li&gt; &#xA; &lt;li&gt;Your torch device will be automatically detected, but you can override this. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt;. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;If using GPU, set &lt;code&gt;INFERENCE_RAM&lt;/code&gt; to your GPU VRAM (per GPU). For example, if you have 16 GB of VRAM, set &lt;code&gt;INFERENCE_RAM=16&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Depending on your document types, marker&#39;s average memory usage per task can vary slightly. You can configure &lt;code&gt;VRAM_PER_TASK&lt;/code&gt; to adjust this if you notice tasks failing with GPU out of memory errors.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;By default, marker will use &lt;code&gt;surya&lt;/code&gt; for OCR. Surya is slower on CPU, but more accurate than tesseract. If you want faster OCR, set &lt;code&gt;OCR_ENGINE&lt;/code&gt; to &lt;code&gt;ocrmypdf&lt;/code&gt;. This also requires external dependencies (see above). If you don&#39;t want OCR at all, set &lt;code&gt;OCR_ENGINE&lt;/code&gt; to &lt;code&gt;None&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Convert a single file&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;marker_single /path/to/file.pdf /path/to/output/folder --batch_multiplier 2 --max_pages 10 --langs English&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch_multiplier&lt;/code&gt; is how much to multiply default batch sizes by if you have extra VRAM. Higher numbers will take more VRAM, but process faster. Set to 2 by default. The default batch sizes will take ~3GB of VRAM.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max_pages&lt;/code&gt; is the maximum number of pages to process. Omit this to convert the entire document.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--langs&lt;/code&gt; is a comma separated list of the languages in the document, for OCR&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Make sure the &lt;code&gt;DEFAULT_LANG&lt;/code&gt; setting is set appropriately for your document. The list of supported languages for OCR is &lt;a href=&#34;https://github.com/VikParuchuri/surya/raw/master/surya/languages.py&#34;&gt;here&lt;/a&gt;. If you need more languages, you can use any language supported by &lt;a href=&#34;https://tesseract-ocr.github.io/tessdoc/Data-Files#data-files-for-version-400-november-29-2016&#34;&gt;Tesseract&lt;/a&gt; if you set &lt;code&gt;OCR_ENGINE&lt;/code&gt; to &lt;code&gt;ocrmypdf&lt;/code&gt;. If you don&#39;t need OCR, marker can work with any language.&lt;/p&gt; &#xA;&lt;h2&gt;Convert multiple files&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;marker /path/to/input/folder /path/to/output/folder --workers 10 --max 10 --metadata_file /path/to/metadata.json --min_length 10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--workers&lt;/code&gt; is the number of pdfs to convert at once. This is set to 1 by default, but you can increase it to increase throughput, at the cost of more CPU/GPU usage. Parallelism will not increase beyond &lt;code&gt;INFERENCE_RAM / VRAM_PER_TASK&lt;/code&gt; if you&#39;re using GPU.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; is the maximum number of pdfs to convert. Omit this to convert all pdfs in the folder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--min_length&lt;/code&gt; is the minimum number of characters that need to be extracted from a pdf before it will be considered for processing. If you&#39;re processing a lot of pdfs, I recommend setting this to avoid OCRing pdfs that are mostly images. (slows everything down)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--metadata_file&lt;/code&gt; is an optional path to a json file with metadata about the pdfs. If you provide it, it will be used to set the language for each pdf. If not, &lt;code&gt;DEFAULT_LANG&lt;/code&gt; will be used. The format is:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#xA;  &#34;pdf1.pdf&#34;: {&#34;languages&#34;: [&#34;English&#34;]},&#xA;  &#34;pdf2.pdf&#34;: {&#34;languages&#34;: [&#34;Spanish&#34;, &#34;Russian&#34;]},&#xA;  ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use language names or codes. The exact codes depend on the OCR engine. See &lt;a href=&#34;https://github.com/VikParuchuri/surya/raw/master/surya/languages.py&#34;&gt;here&lt;/a&gt; for a full list for surya codes, and &lt;a href=&#34;https://tesseract-ocr.github.io/tessdoc/Data-Files#data-files-for-version-400-november-29-2016&#34;&gt;here&lt;/a&gt; for tesseract.&lt;/p&gt; &#xA;&lt;h2&gt;Convert multiple files on multiple GPUs&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;MIN_LENGTH=10000 METADATA_FILE=../pdf_meta.json NUM_DEVICES=4 NUM_WORKERS=15 marker_chunk_convert ../pdf_in ../md_out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;METADATA_FILE&lt;/code&gt; is an optional path to a json file with metadata about the pdfs. See above for the format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;NUM_DEVICES&lt;/code&gt; is the number of GPUs to use. Should be &lt;code&gt;2&lt;/code&gt; or greater.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;NUM_WORKERS&lt;/code&gt; is the number of parallel processes to run on each GPU. Per-GPU parallelism will not increase beyond &lt;code&gt;INFERENCE_RAM / VRAM_PER_TASK&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;MIN_LENGTH&lt;/code&gt; is the minimum number of characters that need to be extracted from a pdf before it will be considered for processing. If you&#39;re processing a lot of pdfs, I recommend setting this to avoid OCRing pdfs that are mostly images. (slows everything down)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Note that the env variables above are specific to this script, and cannot be set in &lt;code&gt;local.env&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Troubleshooting&lt;/h1&gt; &#xA;&lt;p&gt;There are some settings that you may find useful if things aren&#39;t working the way you expect:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OCR_ALL_PAGES&lt;/code&gt; - set this to true to force OCR all pages. This can be very useful if the table layouts aren&#39;t recognized properly by default, or if there is garbled text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;TORCH_DEVICE&lt;/code&gt; - set this to force marker to use a given torch device for inference.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;OCR_ENGINE&lt;/code&gt; - can set this to &lt;code&gt;surya&lt;/code&gt; or &lt;code&gt;ocrmypdf&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DEBUG&lt;/code&gt; - setting this to &lt;code&gt;True&lt;/code&gt; shows ray logs when converting multiple pdfs&lt;/li&gt; &#xA; &lt;li&gt;Verify that you set the languages correctly, or passed in a metadata file.&lt;/li&gt; &#xA; &lt;li&gt;If you&#39;re getting out of memory errors, decrease worker count (increased the &lt;code&gt;VRAM_PER_TASK&lt;/code&gt; setting). You can also try splitting up long PDFs into multiple files.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In general, if output is not what you expect, trying to OCR the PDF is a good first step. Not all PDFs have good text/bboxes embedded in them.&lt;/p&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;p&gt;Benchmarking PDF extraction quality is hard. I&#39;ve created a test set by finding books and scientific papers that have a pdf version and a latex source. I convert the latex to text, and compare the reference to the output of text extraction methods. It&#39;s noisy, but at least directionally correct.&lt;/p&gt; &#xA;&lt;p&gt;Benchmarks show that marker is 4x faster than nougat, and more accurate outside arXiv (nougat was trained on arXiv data). We show naive text extraction (pulling text out of the pdf with no processing) for comparison.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Speed&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Average Score&lt;/th&gt; &#xA;   &lt;th&gt;Time per page&lt;/th&gt; &#xA;   &lt;th&gt;Time per document&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;marker&lt;/td&gt; &#xA;   &lt;td&gt;0.613721&lt;/td&gt; &#xA;   &lt;td&gt;0.631991&lt;/td&gt; &#xA;   &lt;td&gt;58.1432&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nougat&lt;/td&gt; &#xA;   &lt;td&gt;0.406603&lt;/td&gt; &#xA;   &lt;td&gt;2.59702&lt;/td&gt; &#xA;   &lt;td&gt;238.926&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;First 3 are non-arXiv books, last 3 are arXiv papers.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;multicolcnn.pdf&lt;/th&gt; &#xA;   &lt;th&gt;switch_trans.pdf&lt;/th&gt; &#xA;   &lt;th&gt;thinkpython.pdf&lt;/th&gt; &#xA;   &lt;th&gt;thinkos.pdf&lt;/th&gt; &#xA;   &lt;th&gt;thinkdsp.pdf&lt;/th&gt; &#xA;   &lt;th&gt;crowd.pdf&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;marker&lt;/td&gt; &#xA;   &lt;td&gt;0.536176&lt;/td&gt; &#xA;   &lt;td&gt;0.516833&lt;/td&gt; &#xA;   &lt;td&gt;0.70515&lt;/td&gt; &#xA;   &lt;td&gt;0.710657&lt;/td&gt; &#xA;   &lt;td&gt;0.690042&lt;/td&gt; &#xA;   &lt;td&gt;0.523467&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;nougat&lt;/td&gt; &#xA;   &lt;td&gt;0.44009&lt;/td&gt; &#xA;   &lt;td&gt;0.588973&lt;/td&gt; &#xA;   &lt;td&gt;0.322706&lt;/td&gt; &#xA;   &lt;td&gt;0.401342&lt;/td&gt; &#xA;   &lt;td&gt;0.160842&lt;/td&gt; &#xA;   &lt;td&gt;0.525663&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Peak GPU memory usage during the benchmark is &lt;code&gt;4.2GB&lt;/code&gt; for nougat, and &lt;code&gt;4.1GB&lt;/code&gt; for marker. Benchmarks were run on an A6000 Ada.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Marker takes about 4GB of VRAM on average per task, so you can convert 12 documents in parallel on an A6000.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/marker/master/data/images/per_doc.png&#34; alt=&#34;Benchmark results&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;You can benchmark the performance of marker on your machine. Install marker manually with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone https://github.com/VikParuchuri/marker.git&#xA;poetry install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Download the benchmark data &lt;a href=&#34;https://drive.google.com/file/d/1ZSeWDo2g1y0BRLT7KnbmytV2bjWARWba/view?usp=sharing&#34;&gt;here&lt;/a&gt; and unzip. Then run &lt;code&gt;benchmark.py&lt;/code&gt; like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python benchmark.py data/pdfs data/references report.json --nougat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will benchmark marker against other text extraction methods. It sets up batch sizes for nougat and marker to use a similar amount of GPU RAM for each.&lt;/p&gt; &#xA;&lt;p&gt;Omit &lt;code&gt;--nougat&lt;/code&gt; to exclude nougat from the benchmark. I don&#39;t recommend running nougat on CPU, since it is very slow.&lt;/p&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;p&gt;This work would not have been possible without amazing open source models and datasets, including (but not limited to):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Surya&lt;/li&gt; &#xA; &lt;li&gt;Texify&lt;/li&gt; &#xA; &lt;li&gt;Pypdfium2/pdfium&lt;/li&gt; &#xA; &lt;li&gt;DocLayNet from IBM&lt;/li&gt; &#xA; &lt;li&gt;ByT5 from Google&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thank you to the authors of these models and datasets for making them available to the community!&lt;/p&gt;</summary>
  </entry>
</feed>