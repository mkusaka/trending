<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-12-10T01:28:45Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>nexus-xyz/nexus-zkvm</title>
    <updated>2024-12-10T01:28:45Z</updated>
    <id>tag:github.com,2024-12-10:/nexus-xyz/nexus-zkvm</id>
    <link href="https://github.com/nexus-xyz/nexus-zkvm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Nexus zkVM: The zero-knowledge virtual machine&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;The Nexus zkVM&lt;/h1&gt; &#xA;&lt;div align=&#34;left&#34;&gt; &#xA; &lt;a href=&#34;https://t.me/nexus_zkvm&#34;&gt; &lt;img src=&#34;https://img.shields.io/endpoint?color=neon&amp;amp;logo=telegram&amp;amp;label=chat&amp;amp;url=https%3A%2F%2Fmogyo.ro%2Fquart-apis%2Ftgmembercount%3Fchat_id%3Dnexus_zkvm&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/nexus-xyz/nexus-zkvm/graphs/contributors&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/contributors/nexus-xyz/nexus-zkvm.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/NexusLabsHQ&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Twitter-black?logo=x&amp;amp;logoColor=white&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://nexus.xyz&#34;&gt; &lt;img src=&#34;https://img.shields.io/static/v1?label=Stage&amp;amp;message=Alpha&amp;amp;color=2BB4AB&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/nexus-xyz/nexus-zkvm/raw/main/LICENSE-MIT&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://github.com/nexus-xyz/nexus-zkvm/raw/main/LICENSE-APACHE&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/license-APACHE-blue&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://raw.githubusercontent.com/nexus-xyz/nexus-zkvm/main/assets/nexus_docs-header.png&#34; alt=&#34;Logo&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;The Nexus zkVM is a modular, extensible, open-source, and highly-parallelized zkVM, designed to run at &lt;em&gt;a trillion CPU cycles proved per second&lt;/em&gt; given enough machine power.&lt;/p&gt; &#xA;&lt;h2&gt;Folding schemes&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re interested in our implementation of folding schemes, check the &lt;a href=&#34;https://raw.githubusercontent.com/nexus-xyz/nexus-zkvm/main/nova/&#34;&gt;&lt;code&gt;nexus-nova&lt;/code&gt;&lt;/a&gt; crate.&lt;/p&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;1. Install the Nexus zkVM&lt;/h3&gt; &#xA;&lt;p&gt;First, install Rust: &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;https://www.rust-lang.org/tools/install&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Also, make sure you have a working version of &lt;a href=&#34;https://cmake.org/&#34;&gt;cmake&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Next, install the RISC-V target:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;rustup target add riscv32i-unknown-none-elf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install the Nexus zkVM:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo install --git https://github.com/nexus-xyz/nexus-zkvm cargo-nexus --tag &#39;v0.2.4&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Verify the installation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo nexus --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This should print the available CLI commands.&lt;/p&gt; &#xA;&lt;h3&gt;2. Create a new Nexus project&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo nexus new nexus-project&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will create a new Rust project directory with the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./nexus-project&#xA;‚îú‚îÄ‚îÄ Cargo.lock&#xA;‚îú‚îÄ‚îÄ Cargo.toml&#xA;‚îî‚îÄ‚îÄ src&#xA;    ‚îî‚îÄ‚îÄ main.rs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As an example, you can change the content of &lt;code&gt;./src/main.rs&lt;/code&gt; to:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-rust&#34;&gt;#![cfg_attr(target_arch = &#34;riscv32&#34;, no_std, no_main)]&#xA;&#xA;fn fib(n: u32) -&amp;gt; u32 {&#xA;    match n {&#xA;        0 =&amp;gt; 0,&#xA;        1 =&amp;gt; 1,&#xA;        _ =&amp;gt; fib(n - 1) + fib(n - 2),&#xA;    }&#xA;}&#xA;&#xA;#[nexus_rt::main]&#xA;fn main() {&#xA;    let n = 7;&#xA;    let result = fib(n);&#xA;    assert_eq!(result, 13);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3. Run your program&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo nexus run&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command should run successfully. To print the full step-by-step execution trace on the NVM, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo nexus run -v&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;4. Prove your program&lt;/h3&gt; &#xA;&lt;p&gt;Generate a proof for your Rust program using the Nexus zkVM.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo nexus prove&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will save the proof to &lt;code&gt;./nexus-proof&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;5. Verify your proof&lt;/h3&gt; &#xA;&lt;p&gt;Finally, load and verify the proof:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cargo nexus verify&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Learn More&lt;/h2&gt; &#xA;&lt;p&gt;Run &lt;code&gt;cargo nexus --help&lt;/code&gt; to see all the available commands.&lt;/p&gt; &#xA;&lt;p&gt;Also check out the documentation at &lt;a href=&#34;https://docs.nexus.xyz&#34;&gt;docs.nexus.xyz&lt;/a&gt;, or join our &lt;a href=&#34;https://t.me/nexus_zkvm&#34;&gt;Telegram&lt;/a&gt; chat to discuss!&lt;/p&gt; &#xA;&lt;p&gt;Nexus is committed to open-source. All of our code is dual licensed under MIT and Apache licenses. We encourage and appreciate contributions.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>meta-llama/llama-models</title>
    <updated>2024-12-10T01:28:45Z</updated>
    <id>tag:github.com,2024-12-10:/meta-llama/llama-models</id>
    <link href="https://github.com/meta-llama/llama-models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Utilities intended for use with Llama models.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/Llama_Repo.jpeg&#34; width=&#34;400&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; ü§ó &lt;a href=&#34;https://huggingface.co/meta-Llama&#34;&gt; Models on Hugging Face&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://ai.meta.com/blog/&#34;&gt; Blog&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://llama.meta.com/&#34;&gt;Website&lt;/a&gt;&amp;nbsp; | &lt;a href=&#34;https://llama.meta.com/get-started/&#34;&gt;Get Started&lt;/a&gt;&amp;nbsp; &lt;br&gt; &lt;/p&gt;&#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Llama Models&lt;/h1&gt; &#xA;&lt;p&gt;Llama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Open access&lt;/strong&gt;: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Broad ecosystem&lt;/strong&gt;: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Trust &amp;amp; safety&lt;/strong&gt;: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Our mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.&lt;/p&gt; &#xA;&lt;h2&gt;Llama Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pypi.org/project/llama-models/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/llama-models&#34; alt=&#34;PyPI - Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/TZAAYNVtrU&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1257833999603335178&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Launch date&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model sizes&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Context Length&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Tokenizer&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Acceptable use policy&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;Model Card&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7/18/2023&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7B, 13B, 70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Sentencepiece&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama2/USE_POLICY.md&#34;&gt;Use Policy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama2/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama2/MODEL_CARD.md&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4/18/2024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B, 70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TikToken-based&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3/USE_POLICY.md&#34;&gt;Use Policy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3/MODEL_CARD.md&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7/23/2024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8B, 70B, 405B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;128K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TikToken-based&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/USE_POLICY.md&#34;&gt;Use Policy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_1/MODEL_CARD.md&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9/25/2024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1B, 3B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;128K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TikToken-based&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_2/USE_POLICY.md&#34;&gt;Use Policy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_2/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_2/MODEL_CARD.md&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.2-Vision&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9/25/2024&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;11B, 90B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;128K&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TikToken-based&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_2/USE_POLICY.md&#34;&gt;Use Policy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_2/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/meta-llama/llama-models/main/models/llama3_2/MODEL_CARD_VISION.md&#34;&gt;Model Card&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Download&lt;/h2&gt; &#xA;&lt;p&gt;To download the model weights and tokenizer:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Visit the &lt;a href=&#34;https://llama.meta.com/llama-downloads/&#34;&gt;Meta Llama website&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Read and accept the license.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once your request is approved you will receive a signed URL via email.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the &lt;a href=&#34;https://github.com/meta-llama/llama-stack&#34;&gt;Llama CLI&lt;/a&gt;: &lt;code&gt;pip install llama-stack&lt;/code&gt;. (&lt;strong&gt;&amp;lt;-- Start Here if you have received an email already.&lt;/strong&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run &lt;code&gt;llama model list&lt;/code&gt; to show the latest available models and determine the model ID you wish to download. &lt;strong&gt;NOTE&lt;/strong&gt;: If you want older versions of models, run &lt;code&gt;llama model list --show-all&lt;/code&gt; to show all the available Llama models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run: &lt;code&gt;llama download --source meta --model-id CHOSEN_MODEL_ID&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Pass the URL provided when prompted to start the download.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Remember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as &lt;code&gt;403: Forbidden&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running the models&lt;/h2&gt; &#xA;&lt;p&gt;You need to install the following dependencies (in addition to the &lt;code&gt;requirements.txt&lt;/code&gt; in the root directory of this repository) to run the models:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install torch fairscale fire blobfile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installing the dependencies, you can run the example scripts (within &lt;code&gt;llama_models/scripts/&lt;/code&gt; sub-directory) as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash&#xA;&#xA;CHECKPOINT_DIR=~/.llama/checkpoints/Meta-Llama3.1-8B-Instruct&#xA;PYTHONPATH=$(git rev-parse --show-toplevel) torchrun llama_models/scripts/example_chat_completion.py $CHECKPOINT_DIR&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above script should be used with an Instruct (Chat) model. For a Base model, use the script &lt;code&gt;llama_models/scripts/example_text_completion.py&lt;/code&gt;. Note that you can use these scripts with both Llama3 and Llama3.1 series of models.&lt;/p&gt; &#xA;&lt;p&gt;For running larger models with tensor parallelism, you should modify as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash&#xA;&#xA;NGPUS=8&#xA;PYTHONPATH=$(git rev-parse --show-toplevel) torchrun \&#xA;  --nproc_per_node=$NGPUS \&#xA;  llama_models/scripts/example_chat_completion.py $CHECKPOINT_DIR \&#xA;  --model_parallel_size $NGPUS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more flexibility in running inference (including running FP8 inference), please see the &lt;a href=&#34;https://github.com/meta-llama/llama-stack&#34;&gt;&lt;code&gt;Llama Stack&lt;/code&gt;&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h2&gt;Access to Hugging Face&lt;/h2&gt; &#xA;&lt;p&gt;We also provide downloads on &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;Hugging Face&lt;/a&gt;, in both transformers and native &lt;code&gt;llama3&lt;/code&gt; formats. To download the weights from Hugging Face, please follow these steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visit one of the repos, for example &lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct&#34;&gt;meta-llama/Meta-Llama-3.1-8B-Instruct&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Read and accept the license. Once your request is approved, you&#39;ll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.&lt;/li&gt; &#xA; &lt;li&gt;To download the original native weights to use with this repo, click on the &#34;Files and versions&#34; tab and download the contents of the &lt;code&gt;original&lt;/code&gt; folder. You can also download them from the command line if you &lt;code&gt;pip install huggingface-hub&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli download meta-llama/Meta-Llama-3.1-8B-Instruct --include &#34;original/*&#34; --local-dir meta-llama/Meta-Llama-3.1-8B-Instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; The original native weights of meta-llama/Meta-Llama-3.1-405B would not be available through this HugginFace repo.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;To use with transformers, the following &lt;a href=&#34;https://huggingface.co/docs/transformers/en/main_classes/pipelines&#34;&gt;pipeline&lt;/a&gt; snippet will download and cache the weights:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import transformers&#xA;import torch&#xA;&#xA;model_id = &#34;meta-llama/Meta-Llama-3.1-8B-Instruct&#34;&#xA;&#xA;pipeline = transformers.pipeline(&#xA;  &#34;text-generation&#34;,&#xA;  model=&#34;meta-llama/Meta-Llama-3.1-8B-Instruct&#34;,&#xA;  model_kwargs={&#34;torch_dtype&#34;: torch.bfloat16},&#xA;  device=&#34;cuda&#34;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installations&lt;/h2&gt; &#xA;&lt;p&gt;You can install this repository as a &lt;a href=&#34;https://pypi.org/project/llama-models/&#34;&gt;package&lt;/a&gt; by just doing &lt;code&gt;pip install llama-models&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Responsible Use&lt;/h2&gt; &#xA;&lt;p&gt;Llama models are a new technology that carries potential risks with use. Testing conducted to date has not ‚Äî and could not ‚Äî cover all scenarios. To help developers address these risks, we have created the &lt;a href=&#34;https://ai.meta.com/static-resource/responsible-use-guide/&#34;&gt;Responsible Use Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Issues&lt;/h2&gt; &#xA;&lt;p&gt;Please report any software ‚Äúbug‚Äù or other problems with the models through one of the following means:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reporting issues with the model: &lt;a href=&#34;https://github.com/meta-llama/llama-models/issues&#34;&gt;https://github.com/meta-llama/llama-models/issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting risky content generated by the model: &lt;a href=&#34;http://developers.facebook.com/llama_output_feedback&#34;&gt;developers.facebook.com/llama_output_feedback&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reporting bugs and security concerns: &lt;a href=&#34;http://facebook.com/whitehat/info&#34;&gt;facebook.com/whitehat/info&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Questions&lt;/h2&gt; &#xA;&lt;p&gt;For common questions, the FAQ can be found &lt;a href=&#34;https://llama.meta.com/faq&#34;&gt;here&lt;/a&gt;, which will be updated over time as new questions arise.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/torchgeo</title>
    <updated>2024-12-10T01:28:45Z</updated>
    <id>tag:github.com,2024-12-10:/microsoft/torchgeo</id>
    <link href="https://github.com/microsoft/torchgeo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;TorchGeo: datasets, samplers, transforms, and pre-trained models for geospatial data&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/torchgeo/main/logo/logo-color.svg?sanitize=true&#34; width=&#34;400&#34; alt=&#34;TorchGeo logo&#34;&gt; &#xA;&lt;p&gt;TorchGeo is a &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt; domain library, similar to &lt;a href=&#34;https://pytorch.org/vision&#34;&gt;torchvision&lt;/a&gt;, providing datasets, samplers, transforms, and pre-trained models specific to geospatial data.&lt;/p&gt; &#xA;&lt;p&gt;The goal of this library is to make it simple:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;for machine learning experts to work with geospatial data, and&lt;/li&gt; &#xA; &lt;li&gt;for remote sensing experts to explore machine learning solutions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Community: &lt;a href=&#34;https://join.slack.com/t/torchgeo/shared_invite/zt-22rse667m-eqtCeNW0yI000Tl4B~2PIw&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join-4A154B?logo=slack&#34; alt=&#34;slack&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.osgeo.org/community/getting-started-osgeo/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OSGeo-join-4CB05B?logo=osgeo&#34; alt=&#34;osgeo&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/torchgeo&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hugging_Face-join-FFD21E?logo=huggingface&#34; alt=&#34;huggingface&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pytorch.org/ecosystem/join&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PyTorch-join-DE3412?logo=pytorch&#34; alt=&#34;pytorch&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Packaging: &lt;a href=&#34;https://pypi.org/project/torchgeo/&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/torchgeo.svg?sanitize=true&#34; alt=&#34;pypi&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://anaconda.org/conda-forge/torchgeo&#34;&gt;&lt;img src=&#34;https://anaconda.org/conda-forge/torchgeo/badges/version.svg?sanitize=true&#34; alt=&#34;conda&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://packages.spack.io/package.html?name=py-torchgeo&#34;&gt;&lt;img src=&#34;https://img.shields.io/spack/v/py-torchgeo&#34; alt=&#34;spack&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Testing: &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/&#34;&gt;&lt;img src=&#34;https://readthedocs.org/projects/torchgeo/badge/?version=latest&#34; alt=&#34;docs&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/torchgeo/actions/workflows/style.yaml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/torchgeo/actions/workflows/style.yaml/badge.svg?sanitize=true&#34; alt=&#34;style&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/microsoft/torchgeo/actions/workflows/tests.yaml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/torchgeo/actions/workflows/tests.yaml/badge.svg?sanitize=true&#34; alt=&#34;tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/microsoft/torchgeo&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/microsoft/torchgeo/branch/main/graph/badge.svg?token=oa3Z3PMVOg&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;The recommended way to install TorchGeo is with &lt;a href=&#34;https://pip.pypa.io/&#34;&gt;pip&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ pip install torchgeo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For &lt;a href=&#34;https://docs.conda.io/&#34;&gt;conda&lt;/a&gt; and &lt;a href=&#34;https://spack.io/&#34;&gt;spack&lt;/a&gt; installation instructions, see the &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/user/installation.html&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;You can find the documentation for TorchGeo on &lt;a href=&#34;https://torchgeo.readthedocs.io&#34;&gt;ReadTheDocs&lt;/a&gt;. This includes API documentation, contributing instructions, and several &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/tutorials/getting_started.html&#34;&gt;tutorials&lt;/a&gt;. For more details, check out our &lt;a href=&#34;https://arxiv.org/abs/2111.08872&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=ET8Hb_HqNJQ&#34;&gt;podcast episode&lt;/a&gt;, &lt;a href=&#34;https://www.youtube.com/watch?v=R_FhY8aq708&#34;&gt;tutorial&lt;/a&gt;, and &lt;a href=&#34;https://pytorch.org/blog/geospatial-deep-learning-with-torchgeo/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=ET8Hb_HqNJQ&#34;&gt; &lt;img src=&#34;https://img.youtube.com/vi/ET8Hb_HqNJQ/0.jpg&#34; style=&#34;width:49%;&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=R_FhY8aq708&#34;&gt; &lt;img src=&#34;https://img.youtube.com/vi/R_FhY8aq708/0.jpg&#34; style=&#34;width:49%;&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Example Usage&lt;/h2&gt; &#xA;&lt;p&gt;The following sections give basic examples of what you can do with TorchGeo.&lt;/p&gt; &#xA;&lt;p&gt;First we&#39;ll import various classes and functions used in the following sections:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from lightning.pytorch import Trainer&#xA;from torch.utils.data import DataLoader&#xA;&#xA;from torchgeo.datamodules import InriaAerialImageLabelingDataModule&#xA;from torchgeo.datasets import CDL, Landsat7, Landsat8, VHR10, stack_samples&#xA;from torchgeo.samplers import RandomGeoSampler&#xA;from torchgeo.trainers import SemanticSegmentationTask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Geospatial datasets and samplers&lt;/h3&gt; &#xA;&lt;p&gt;Many remote sensing applications involve working with &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/api/datasets.html#geospatial-datasets&#34;&gt;&lt;em&gt;geospatial datasets&lt;/em&gt;&lt;/a&gt;‚Äîdatasets with geographic metadata. These datasets can be challenging to work with due to the sheer variety of data. Geospatial imagery is often multispectral with a different number of spectral bands and spatial resolution for every satellite. In addition, each file may be in a different coordinate reference system (CRS), requiring the data to be reprojected into a matching CRS.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/torchgeo/main/images/geodataset.png&#34; alt=&#34;Example application in which we combine Landsat and CDL and sample from both&#34;&gt; &#xA;&lt;p&gt;In this example, we show how easy it is to work with geospatial data and to sample small image patches from a combination of &lt;a href=&#34;https://www.usgs.gov/landsat-missions&#34;&gt;Landsat&lt;/a&gt; and &lt;a href=&#34;https://data.nal.usda.gov/dataset/cropscape-cropland-data-layer&#34;&gt;Cropland Data Layer (CDL)&lt;/a&gt; data using TorchGeo. First, we assume that the user has Landsat 7 and 8 imagery downloaded. Since Landsat 8 has more spectral bands than Landsat 7, we&#39;ll only use the bands that both satellites have in common. We&#39;ll create a single dataset including all images from both Landsat 7 and 8 data by taking the union between these two datasets.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;landsat7 = Landsat7(paths=&#34;...&#34;, bands=[&#34;B1&#34;, ..., &#34;B7&#34;])&#xA;landsat8 = Landsat8(paths=&#34;...&#34;, bands=[&#34;B2&#34;, ..., &#34;B8&#34;])&#xA;landsat = landsat7 | landsat8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, we take the intersection between this dataset and the CDL dataset. We want to take the intersection instead of the union to ensure that we only sample from regions that have both Landsat and CDL data. Note that we can automatically download and checksum CDL data. Also note that each of these datasets may contain files in different coordinate reference systems (CRS) or resolutions, but TorchGeo automatically ensures that a matching CRS and resolution is used.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cdl = CDL(paths=&#34;...&#34;, download=True, checksum=True)&#xA;dataset = landsat &amp;amp; cdl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This dataset can now be used with a PyTorch data loader. Unlike benchmark datasets, geospatial datasets often include very large images. For example, the CDL dataset consists of a single image covering the entire continental United States. In order to sample from these datasets using geospatial coordinates, TorchGeo defines a number of &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/api/samplers.html&#34;&gt;&lt;em&gt;samplers&lt;/em&gt;&lt;/a&gt;. In this example, we&#39;ll use a random sampler that returns 256 x 256 pixel images and 10,000 samples per epoch. We also use a custom collation function to combine each sample dictionary into a mini-batch of samples.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sampler = RandomGeoSampler(dataset, size=256, length=10000)&#xA;dataloader = DataLoader(dataset, batch_size=128, sampler=sampler, collate_fn=stack_samples)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This data loader can now be used in your normal training/evaluation pipeline.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;for batch in dataloader:&#xA;    image = batch[&#34;image&#34;]&#xA;    mask = batch[&#34;mask&#34;]&#xA;&#xA;    # train a model, or make predictions using a pre-trained model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Many applications involve intelligently composing datasets based on geospatial metadata like this. For example, users may want to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Combine datasets for multiple image sources and treat them as equivalent (e.g., Landsat 7 and 8)&lt;/li&gt; &#xA; &lt;li&gt;Combine datasets for disparate geospatial locations (e.g., Chesapeake NY and PA)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These combinations require that all queries are present in at least one dataset, and can be created using a &lt;code&gt;UnionDataset&lt;/code&gt;. Similarly, users may want to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Combine image and target labels and sample from both simultaneously (e.g., Landsat and CDL)&lt;/li&gt; &#xA; &lt;li&gt;Combine datasets for multiple image sources for multimodal learning or data fusion (e.g., Landsat and Sentinel)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These combinations require that all queries are present in both datasets, and can be created using an &lt;code&gt;IntersectionDataset&lt;/code&gt;. TorchGeo automatically composes these datasets for you when you use the intersection (&lt;code&gt;&amp;amp;&lt;/code&gt;) and union (&lt;code&gt;|&lt;/code&gt;) operators.&lt;/p&gt; &#xA;&lt;h3&gt;Benchmark datasets&lt;/h3&gt; &#xA;&lt;p&gt;TorchGeo includes a number of &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/api/datasets.html#non-geospatial-datasets&#34;&gt;&lt;em&gt;benchmark datasets&lt;/em&gt;&lt;/a&gt;‚Äîdatasets that include both input images and target labels. This includes datasets for tasks like image classification, regression, semantic segmentation, object detection, instance segmentation, change detection, and more.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;ve used &lt;a href=&#34;https://pytorch.org/vision&#34;&gt;torchvision&lt;/a&gt; before, these datasets should seem very familiar. In this example, we&#39;ll create a dataset for the Northwestern Polytechnical University (NWPU) very-high-resolution ten-class (&lt;a href=&#34;https://github.com/chaozhong2010/VHR-10_dataset_coco&#34;&gt;VHR-10&lt;/a&gt;) geospatial object detection dataset. This dataset can be automatically downloaded, checksummed, and extracted, just like with torchvision.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.utils.data import DataLoader&#xA;&#xA;from torchgeo.datamodules.utils import collate_fn_detection&#xA;from torchgeo.datasets import VHR10&#xA;&#xA;# Initialize the dataset&#xA;dataset = VHR10(root=&#34;...&#34;, download=True, checksum=True)&#xA;&#xA;# Initialize the dataloader with the custom collate function&#xA;dataloader = DataLoader(&#xA;    dataset,&#xA;    batch_size=128,&#xA;    shuffle=True,&#xA;    num_workers=4,&#xA;    collate_fn=collate_fn_detection,&#xA;)&#xA;&#xA;# Training loop&#xA;for batch in dataloader:&#xA;    images = batch[&#34;image&#34;]  # list of images&#xA;    boxes = batch[&#34;boxes&#34;]  # list of boxes&#xA;    labels = batch[&#34;labels&#34;]  # list of labels&#xA;    masks = batch[&#34;masks&#34;]  # list of masks&#xA;&#xA;    # train a model, or make predictions using a pre-trained model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/torchgeo/main/images/vhr10.png&#34; alt=&#34;Example predictions from a Mask R-CNN model trained on the VHR-10 dataset&#34;&gt; &#xA;&lt;p&gt;All TorchGeo datasets are compatible with PyTorch data loaders, making them easy to integrate into existing training workflows. The only difference between a benchmark dataset in TorchGeo and a similar dataset in torchvision is that each dataset returns a dictionary with keys for each PyTorch &lt;code&gt;Tensor&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Pre-trained Weights&lt;/h3&gt; &#xA;&lt;p&gt;Pre-trained weights have proven to be tremendously beneficial for transfer learning tasks in computer vision. Practitioners usually utilize models pre-trained on the ImageNet dataset, containing RGB images. However, remote sensing data often goes beyond RGB with additional multispectral channels that can vary across sensors. TorchGeo is the first library to support models pre-trained on different multispectral sensors, and adopts torchvision&#39;s &lt;a href=&#34;https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/&#34;&gt;multi-weight API&lt;/a&gt;. A summary of currently available weights can be seen in the &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/api/models.html#pretrained-weights&#34;&gt;docs&lt;/a&gt;. To create a &lt;a href=&#34;https://github.com/huggingface/pytorch-image-models&#34;&gt;timm&lt;/a&gt; Resnet-18 model with weights that have been pretrained on Sentinel-2 imagery, you can do the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import timm&#xA;from torchgeo.models import ResNet18_Weights&#xA;&#xA;weights = ResNet18_Weights.SENTINEL2_ALL_MOCO&#xA;model = timm.create_model(&#34;resnet18&#34;, in_chans=weights.meta[&#34;in_chans&#34;], num_classes=10)&#xA;model.load_state_dict(weights.get_state_dict(progress=True), strict=False)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These weights can also directly be used in TorchGeo Lightning modules that are shown in the following section via the &lt;code&gt;weights&lt;/code&gt; argument. For a notebook example, see this &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/tutorials/pretrained_weights.html&#34;&gt;tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Reproducibility with Lightning&lt;/h3&gt; &#xA;&lt;p&gt;In order to facilitate direct comparisons between results published in the literature and further reduce the boilerplate code needed to run experiments with datasets in TorchGeo, we have created Lightning &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/api/datamodules.html&#34;&gt;&lt;em&gt;datamodules&lt;/em&gt;&lt;/a&gt; with well-defined train-val-test splits and &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/api/trainers.html&#34;&gt;&lt;em&gt;trainers&lt;/em&gt;&lt;/a&gt; for various tasks like classification, regression, and semantic segmentation. These datamodules show how to incorporate augmentations from the kornia library, include preprocessing transforms (with pre-calculated channel statistics), and let users easily experiment with hyperparameters related to the data itself (as opposed to the modeling process). Training a semantic segmentation model on the &lt;a href=&#34;https://project.inria.fr/aerialimagelabeling/&#34;&gt;Inria Aerial Image Labeling&lt;/a&gt; dataset is as easy as a few imports and four lines of code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;datamodule = InriaAerialImageLabelingDataModule(root=&#34;...&#34;, batch_size=64, num_workers=6)&#xA;task = SemanticSegmentationTask(&#xA;    model=&#34;unet&#34;,&#xA;    backbone=&#34;resnet50&#34;,&#xA;    weights=True,&#xA;    in_channels=3,&#xA;    num_classes=2,&#xA;    loss=&#34;ce&#34;,&#xA;    ignore_index=None,&#xA;    lr=0.1,&#xA;    patience=6,&#xA;)&#xA;trainer = Trainer(default_root_dir=&#34;...&#34;)&#xA;&#xA;trainer.fit(model=task, datamodule=datamodule)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/torchgeo/main/images/inria.png&#34; alt=&#34;Building segmentations produced by a U-Net model trained on the Inria Aerial Image Labeling dataset&#34;&gt; &#xA;&lt;p&gt;TorchGeo also supports command-line interface training using &lt;a href=&#34;https://lightning.ai/docs/pytorch/stable/cli/lightning_cli.html&#34;&gt;LightningCLI&lt;/a&gt;. It can be invoked in two ways:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# If torchgeo has been installed&#xA;torchgeo&#xA;# If torchgeo has been installed, or if it has been cloned to the current directory&#xA;python3 -m torchgeo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It supports command-line configuration or YAML/JSON config files. Valid options can be found from the help messages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# See valid stages&#xA;torchgeo --help&#xA;# See valid trainer options&#xA;torchgeo fit --help&#xA;# See valid model options&#xA;torchgeo fit --model.help ClassificationTask&#xA;# See valid data options&#xA;torchgeo fit --data.help EuroSAT100DataModule&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using the following config file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;trainer:&#xA;  max_epochs: 20&#xA;model:&#xA;  class_path: ClassificationTask&#xA;  init_args:&#xA;    model: &#39;resnet18&#39;&#xA;    in_channels: 13&#xA;    num_classes: 10&#xA;data:&#xA;  class_path: EuroSAT100DataModule&#xA;  init_args:&#xA;    batch_size: 8&#xA;  dict_kwargs:&#xA;    download: true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;we can see the script in action:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;# Train and validate a model&#xA;torchgeo fit --config config.yaml&#xA;# Validate-only&#xA;torchgeo validate --config config.yaml&#xA;# Calculate and report test accuracy&#xA;torchgeo test --config config.yaml --ckpt_path=...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It can also be imported and used in a Python script if you need to extend it to add new features:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torchgeo.main import main&#xA;&#xA;main([&#34;fit&#34;, &#34;--config&#34;, &#34;config.yaml&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://lightning.ai/docs/pytorch/stable/cli/lightning_cli.html&#34;&gt;Lightning documentation&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use this software in your work, please cite our &lt;a href=&#34;https://dl.acm.org/doi/10.1145/3557915.3560953&#34;&gt;paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{Stewart_TorchGeo_Deep_Learning_2022,&#xA;    address = {Seattle, Washington},&#xA;    author = {Stewart, Adam J. and Robinson, Caleb and Corley, Isaac A. and Ortiz, Anthony and Lavista Ferres, Juan M. and Banerjee, Arindam},&#xA;    booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},&#xA;    doi = {10.1145/3557915.3560953},&#xA;    month = nov,&#xA;    pages = {1--12},&#xA;    publisher = {Association for Computing Machinery},&#xA;    series = {SIGSPATIAL &#39;22},&#xA;    title = {{TorchGeo}: Deep Learning With Geospatial Data},&#xA;    url = {https://dl.acm.org/doi/10.1145/3557915.3560953},&#xA;    year = {2022}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. If you would like to submit a pull request, see our &lt;a href=&#34;https://torchgeo.readthedocs.io/en/stable/user/contributing.html&#34;&gt;Contribution Guide&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</summary>
  </entry>
</feed>