<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-03T01:21:55Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>danielmiessler/fabric</title>
    <updated>2024-02-03T01:21:55Z</updated>
    <id>tag:github.com,2024-02-03:/danielmiessler/fabric</id>
    <link href="https://github.com/danielmiessler/fabric" rel="alternate"></link>
    <summary type="html">&lt;p&gt;fabric is an open-source framework for augmenting humans using AI.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/images/fabric-logo-gif.gif&#34; alt=&#34;fabriclogo&#34; width=&#34;400&#34; height=&#34;400&#34;&gt; &#xA; &lt;h1&gt;&lt;code&gt;fabric&lt;/code&gt;&lt;/h1&gt; &#xA; &lt;h4&gt;&lt;code&gt;fabric&lt;/code&gt; is an open-source framework for augmenting humans using AI.&lt;/h4&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#whatandwhy&#34;&gt;What and Why&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#usage&#34;&gt;Usage&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#examples&#34;&gt;Examples&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#structure&#34;&gt;Structure&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#naming&#34;&gt;Naming&lt;/a&gt; • &lt;a href=&#34;https://raw.githubusercontent.com/danielmiessler/fabric/main/#meta&#34;&gt;Meta&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;What and Why&lt;/h2&gt; &#xA;&lt;p&gt;Since the start of 2023 and GenAI we&#39;ve seen a massive number of AI applications for accomplishing tasks. It&#39;s powerful, but &lt;strong&gt;it&#39;s not easy to integrate this functionality into our lives.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;In other words, AI doesn&#39;t have a capabilities problem—it has an &lt;strong&gt;integration&lt;/strong&gt; problem.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Fabric was created to address that problem by allowing everyone to leverage AI throughout our life and work.&lt;/p&gt; &#xA;&lt;h3&gt;Too many prompts&lt;/h3&gt; &#xA;&lt;p&gt;The biggest challenge I faced in 2023——and still have today—is &lt;strong&gt;the sheer number of AI prompts out there&lt;/strong&gt;. We all have prompts that are useful, but it&#39;s hard to manage them, discover new ones, &lt;em&gt;and manage the different versions of the ones we like&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;One of &lt;code&gt;fabric&lt;/code&gt;&#39;s main features is helping people collect and integrate modular AI functionality (in this case: prompts), which we call &lt;em&gt;Patterns&lt;/em&gt;, into various parts of their lives.&lt;/p&gt; &#xA;&lt;p&gt;Fabric has patterns (prompts) for all sorts of life and work activities, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extracting the most interesting parts of YouTube videos and podcasts&lt;/li&gt; &#xA; &lt;li&gt;Writing an essay in your own voice with just an idea as an input&lt;/li&gt; &#xA; &lt;li&gt;Summarizing opaque academic papers&lt;/li&gt; &#xA; &lt;li&gt;Creating perfectly matched AI art prompts for a piece of writing&lt;/li&gt; &#xA; &lt;li&gt;Rating the quality of content to see if you want to read/watch the whole thing&lt;/li&gt; &#xA; &lt;li&gt;Getting summaries of long, boring content&lt;/li&gt; &#xA; &lt;li&gt;Explaining code to you&lt;/li&gt; &#xA; &lt;li&gt;Turning bad documentation into usable documentation&lt;/li&gt; &#xA; &lt;li&gt;Create social media posts from any content input&lt;/li&gt; &#xA; &lt;li&gt;And a million more…&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;There are three main ways to get started with Fabric.&lt;/p&gt; &#xA;&lt;img width=&#34;1173&#34; alt=&#34;fabric-patterns-screenshot&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8&#34;&gt; &#xA;&lt;h3&gt;1. Just use the patterns (prompts)&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main/patterns&#34;&gt;&lt;code&gt;/patterns&lt;/code&gt;&lt;/a&gt; directory and start exploring!&lt;/p&gt; &#xA;&lt;p&gt;You can use any of those in any AI application that you have!&lt;/p&gt; &#xA;&lt;h3&gt;2. Create your own Fabric Mill (server)&lt;/h3&gt; &#xA;&lt;p&gt;If you want your very own Fabric server, head over to the &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main/server&#34;&gt;&lt;code&gt;/server/&lt;/code&gt;&lt;/a&gt; directory and set up your own Fabric Mill with your own patterns running! You can then use the &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main/client/standalone_client_examples&#34;&gt;&lt;code&gt;/client/standalone_client_examples&lt;/code&gt;&lt;/a&gt; to connect to it.&lt;/p&gt; &#xA;&lt;h3&gt;3. The standalone client&lt;/h3&gt; &#xA;&lt;p&gt;We&#39;re almost done with a universal client that will let you do all sorts of cool stuff, including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Calling patterns without connecting to a Fabric server (direct to OpenAI).&lt;/li&gt; &#xA; &lt;li&gt;Streaming mode to get instant and dynamic results.&lt;/li&gt; &#xA; &lt;li&gt;Other cool stuff…&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We expect this client to be ready very within a day or two, and we&#39;ll update the Quickstart as soon as it is.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;fabric&lt;/code&gt;&#39;s main function is to make &lt;strong&gt;Patterns&lt;/strong&gt; available to everyone in an open ecosystem, i.e., to allow people to share and fork prompts in a transparent, scalable, and dependable way.&lt;/p&gt; &#xA;&lt;p&gt;But it also includes two other components that make it possible for AI enthusiasts and developers to &lt;em&gt;build your own Personal AI Ecosystem&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;In other words you can have your own server, with your own copy of &lt;code&gt;fabric&lt;/code&gt;, running your own specific combination of &lt;strong&gt;Patterns&lt;/strong&gt; for your specific use cases.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Components&lt;/h3&gt; &#xA;&lt;p&gt;Here are the three &lt;code&gt;fabric&lt;/code&gt; ecosystem pieces, and how they work together.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;strong&gt;Mill&lt;/strong&gt; is the (optional) server that makes &lt;strong&gt;Patterns&lt;/strong&gt; available.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Patterns&lt;/strong&gt; are the actual AI use cases.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Looms&lt;/strong&gt; are the modular, client-side apps that call a specific &lt;strong&gt;Pattern&lt;/strong&gt; hosted by a &lt;strong&gt;Mill&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;One key feature of &lt;code&gt;fabric&lt;/code&gt; and its Markdown-based format is the ability to ** directly reference** (and edit) individual &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main#naming&#34;&gt;patterns&lt;/a&gt; directly—on their own—without surrounding code.&lt;/p&gt; &#xA;&lt;p&gt;As an example, here&#39;s how to call &lt;em&gt;the direct location&lt;/em&gt; of the &lt;strong&gt;system&lt;/strong&gt; prompt for the &lt;code&gt;extract_wisdom&lt;/code&gt; pattern.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;https://github.com/danielmiessler/fabric/blob/main/patterns/extract_wisdom/system.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This means you can cleanly, and directly reference any pattern for use in a web-based AI app, your own code, or wherever!&lt;/p&gt; &#xA;&lt;p&gt;Even better, you can also have your &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main#naming&#34;&gt;Mill&lt;/a&gt; functionality directly call &lt;strong&gt;system&lt;/strong&gt; and &lt;strong&gt;user&lt;/strong&gt; prompts from &lt;code&gt;fabric&lt;/code&gt;, meaning you can have your personal AI ecosystem automatically kept up to date with the latest version of your favorite &lt;a href=&#34;https://github.com/danielmiessler/fabric/tree/main#naming&#34;&gt;Patterns&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s an abridged output example from the &lt;a href=&#34;https://github.com/danielmiessler/fabric/raw/main/patterns/extract_wisdom/system.md&#34;&gt;&lt;code&gt;extract_wisdom&lt;/code&gt;&lt;/a&gt; pattern (limited to only 10 items per section).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Paste in the transcript of a YouTube video of Riva Tez on David Perrel&#39;s podcast&#xA;pbpaste | extract_wisdom&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;## SUMMARY:&#xA;&#xA;The content features a conversation between two individuals discussing various topics, including the decline of Western culture, the importance of beauty and subtlety in life, the impact of technology and AI, the resonance of Rilke&#39;s poetry, the value of deep reading and revisiting texts, the captivating nature of Ayn Rand&#39;s writing, the role of philosophy in understanding the world, and the influence of drugs on society. They also touch upon creativity, attention spans, and the importance of introspection.&#xA;&#xA;## IDEAS:&#xA;&#xA;1. Western culture is perceived to be declining due to a loss of values and an embrace of mediocrity.&#xA;2. Mass media and technology have contributed to shorter attention spans and a need for constant stimulation.&#xA;3. Rilke&#39;s poetry resonates due to its focus on beauty and ecstasy in everyday objects.&#xA;4. Subtlety is often overlooked in modern society due to sensory overload.&#xA;5. The role of technology in shaping music and performance art is significant.&#xA;6. Reading habits have shifted from deep, repetitive reading to consuming large quantities of new material.&#xA;7. Revisiting influential books as one ages can lead to new insights based on accumulated wisdom and experiences.&#xA;8. Fiction can vividly illustrate philosophical concepts through characters and narratives.&#xA;9. Many influential thinkers have backgrounds in philosophy, highlighting its importance in shaping reasoning skills.&#xA;10. Philosophy is seen as a bridge between theology and science, asking questions that both fields seek to answer.&#xA;&#xA;## QUOTES:&#xA;&#xA;1. &#34;You can&#39;t necessarily think yourself into the answers. You have to create space for the answers to come to you.&#34;&#xA;2. &#34;The West is dying and we are killing her.&#34;&#xA;3. &#34;The American Dream has been replaced by mass packaged mediocrity porn, encouraging us to revel like happy pigs in our own meekness.&#34;&#xA;4. &#34;There&#39;s just not that many people who have the courage to reach beyond consensus and go explore new ideas.&#34;&#xA;5. &#34;I&#39;ll start watching Netflix when I&#39;ve read the whole of human history.&#34;&#xA;6. &#34;Rilke saw beauty in everything... He sees it&#39;s in one little thing, a representation of all things that are beautiful.&#34;&#xA;7. &#34;Vanilla is a very subtle flavor... it speaks to sort of the sensory overload of the modern age.&#34;&#xA;8. &#34;When you memorize chapters [of the Bible], it takes a few months, but you really understand how things are structured.&#34;&#xA;9. &#34;As you get older, if there&#39;s books that moved you when you were younger, it&#39;s worth going back and rereading them.&#34;&#xA;10. &#34;She [Ayn Rand] took complicated philosophy and embodied it in a way that anybody could resonate with.&#34;&#xA;&#xA;## HABITS:&#xA;&#xA;1. Avoiding mainstream media consumption for deeper engagement with historical texts and personal research.&#xA;2. Regularly revisiting influential books from youth to gain new insights with age.&#xA;3. Engaging in deep reading practices rather than skimming or speed-reading material.&#xA;4. Memorizing entire chapters or passages from significant texts for better understanding.&#xA;5. Disengaging from social media and fast-paced news cycles for more focused thought processes.&#xA;6. Walking long distances as a form of meditation and reflection.&#xA;7. Creating space for thoughts to solidify through introspection and stillness.&#xA;8. Embracing emotions such as grief or anger fully rather than suppressing them.&#xA;9. Seeking out varied experiences across different careers and lifestyles.&#xA;10. Prioritizing curiosity-driven research without specific goals or constraints.&#xA;&#xA;## FACTS:&#xA;&#xA;1. The West is perceived as declining due to cultural shifts away from traditional values.&#xA;2. Attention spans have shortened due to technological advancements and media consumption habits.&#xA;3. Rilke&#39;s poetry emphasizes finding beauty in everyday objects through detailed observation.&#xA;4. Modern society often overlooks subtlety due to sensory overload from various stimuli.&#xA;5. Reading habits have evolved from deep engagement with texts to consuming large quantities quickly.&#xA;6. Revisiting influential books can lead to new insights based on accumulated life experiences.&#xA;7. Fiction can effectively illustrate philosophical concepts through character development and narrative arcs.&#xA;8. Philosophy plays a significant role in shaping reasoning skills and understanding complex ideas.&#xA;9. Creativity may be stifled by cultural nihilism and protectionist attitudes within society.&#xA;10. Short-term thinking undermines efforts to create lasting works of beauty or significance.&#xA;&#xA;## REFERENCES:&#xA;&#xA;1. Rainer Maria Rilke&#39;s poetry&#xA;2. Netflix&#xA;3. Underworld concert&#xA;4. Katy Perry&#39;s theatrical performances&#xA;5. Taylor Swift&#39;s performances&#xA;6. Bible study&#xA;7. Atlas Shrugged by Ayn Rand&#xA;8. Robert Pirsig&#39;s writings&#xA;9. Bertrand Russell&#39;s definition of philosophy&#xA;10. Nietzsche&#39;s walks&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Structure&lt;/h2&gt; &#xA;&lt;img width=&#34;2070&#34; alt=&#34;fabric_mill_architecture&#34; src=&#34;https://github.com/danielmiessler/fabric/assets/50654/ec3bd9b5-d285-483d-9003-7a8e6d842584&#34;&gt; &#xA;&lt;p&gt;There are multiple ways to use &lt;code&gt;fabric&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;You can just use the &lt;code&gt;/patterns&lt;/code&gt; in other AI applications/websites.&lt;/li&gt; &#xA; &lt;li&gt;You can build your own server and host these patterns there (plus your own) using the Mill code in &lt;code&gt;/infrastructure/server&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;You can use the &lt;code&gt;fabric&lt;/code&gt; client to switch between these (coming soon)!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you are self-hosting your own Mill, the image above shows you what&#39;s going on. Basically, you are sending your input to your Fabric Mill, and your Fabric Mill then sends the input and the pattern on to OpenAI. Local model options also being added soon.&lt;/p&gt; &#xA;&lt;h2&gt;CLI-native&lt;/h2&gt; &#xA;&lt;p&gt;One of the coolest parts of the project is that it&#39;s &lt;strong&gt;command-line native&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;p&gt;Each pattern (prompt) you see in the &lt;code&gt;/patterns&lt;/code&gt; directory can be used in any AI application you use, but you can also set up your own server using the &lt;code&gt;/server&lt;/code&gt; code and then call APIs directly!&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;re set up, you can do things like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Take any idea from `stdin` and send it to the `/write_essay` API!&#xA;cat &#34;An idea that coding is like speaking with rules.&#34; | write_essay&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Naming&lt;/h2&gt; &#xA;&lt;p&gt;Fabric is themed off of, well… &lt;em&gt;fabric&lt;/em&gt;—as in…woven materials. So, think blankets, quilts, patterns, etc. Here&#39;s the concept and structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The project itself is called &lt;strong&gt;Fabric&lt;/strong&gt;, and it&#39;s the parent concept.&lt;/li&gt; &#xA; &lt;li&gt;Individual AI modules (think prompts) are called &lt;strong&gt;Patterns&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Chaining together &lt;em&gt;Patterns&lt;/em&gt; to create advanced functionality is called a &lt;strong&gt;Stitch&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The optional server-side functionality of &lt;code&gt;fabric&lt;/code&gt; is called the &lt;strong&gt;Mill&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The optional client-side scripts within &lt;code&gt;fabric&lt;/code&gt; are called &lt;strong&gt;Looms&lt;/strong&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;More Documentation&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br&gt; We are pushing hard to add lots more functionality and documentation. Please be patient and let us know what you&#39;d like to see in Issues. Thank you!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Meta&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;fabric&lt;/code&gt; was created by &lt;a href=&#34;https://danielmiessler.com/&#34; target=&#34;_blank&#34;&gt;Daniel Miessler&lt;/a&gt; in January of 2024.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to the following people for inspiration and contributions.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Caleb Sima&lt;/strong&gt; for pushing me over the edge of whether to make this a public project or not.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Joel Parish&lt;/strong&gt; for super useful input on the project&#39;s Github directory structure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Jonathan Dunn&lt;/strong&gt; for spectacular work on the soon-to-be-released universal client.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>code100x/daily-code</title>
    <updated>2024-02-03T01:21:55Z</updated>
    <id>tag:github.com,2024-02-03:/code100x/daily-code</id>
    <link href="https://github.com/code100x/daily-code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Turborepo starter&lt;/h1&gt; &#xA;&lt;p&gt;This is an official starter Turborepo.&lt;/p&gt; &#xA;&lt;h2&gt;Using this example&lt;/h2&gt; &#xA;&lt;p&gt;Run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npx create-turbo@latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What&#39;s inside?&lt;/h2&gt; &#xA;&lt;p&gt;This Turborepo includes the following packages/apps:&lt;/p&gt; &#xA;&lt;h3&gt;Apps and Packages&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;docs&lt;/code&gt;: a &lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt; app&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;web&lt;/code&gt;: another &lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt; app&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;@repo/ui&lt;/code&gt;: a stub React component library shared by both &lt;code&gt;web&lt;/code&gt; and &lt;code&gt;docs&lt;/code&gt; applications&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;@repo/eslint-config&lt;/code&gt;: &lt;code&gt;eslint&lt;/code&gt; configurations (includes &lt;code&gt;eslint-config-next&lt;/code&gt; and &lt;code&gt;eslint-config-prettier&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;@repo/typescript-config&lt;/code&gt;: &lt;code&gt;tsconfig.json&lt;/code&gt;s used throughout the monorepo&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each package/app is 100% &lt;a href=&#34;https://www.typescriptlang.org/&#34;&gt;TypeScript&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Utilities&lt;/h3&gt; &#xA;&lt;p&gt;This Turborepo has some additional tools already setup for you:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.typescriptlang.org/&#34;&gt;TypeScript&lt;/a&gt; for static type checking&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://eslint.org/&#34;&gt;ESLint&lt;/a&gt; for code linting&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://prettier.io&#34;&gt;Prettier&lt;/a&gt; for code formatting&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;To build all apps and packages, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd my-turborepo&#xA;pnpm build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Develop&lt;/h3&gt; &#xA;&lt;p&gt;To develop all apps and packages, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd my-turborepo&#xA;pnpm dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Remote Caching&lt;/h3&gt; &#xA;&lt;p&gt;Turborepo can use a technique known as &lt;a href=&#34;https://turbo.build/repo/docs/core-concepts/remote-caching&#34;&gt;Remote Caching&lt;/a&gt; to share cache artifacts across machines, enabling you to share build caches with your team and CI/CD pipelines.&lt;/p&gt; &#xA;&lt;p&gt;By default, Turborepo will cache locally. To enable Remote Caching you will need an account with Vercel. If you don&#39;t have an account you can &lt;a href=&#34;https://vercel.com/signup&#34;&gt;create one&lt;/a&gt;, then enter the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd my-turborepo&#xA;npx turbo login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will authenticate the Turborepo CLI with your &lt;a href=&#34;https://vercel.com/docs/concepts/personal-accounts/overview&#34;&gt;Vercel account&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Next, you can link your Turborepo to your Remote Cache by running the following command from the root of your Turborepo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npx turbo link&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Useful Links&lt;/h2&gt; &#xA;&lt;p&gt;Learn more about the power of Turborepo:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://turbo.build/repo/docs/core-concepts/monorepos/running-tasks&#34;&gt;Tasks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://turbo.build/repo/docs/core-concepts/caching&#34;&gt;Caching&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://turbo.build/repo/docs/core-concepts/remote-caching&#34;&gt;Remote Caching&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://turbo.build/repo/docs/core-concepts/monorepos/filtering&#34;&gt;Filtering&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://turbo.build/repo/docs/reference/configuration&#34;&gt;Configuration Options&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://turbo.build/repo/docs/reference/command-line-reference&#34;&gt;CLI Usage&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Fanghua-Yu/SUPIR</title>
    <updated>2024-02-03T01:21:55Z</updated>
    <id>tag:github.com,2024-02-03:/Fanghua-Yu/SUPIR</id>
    <link href="https://github.com/Fanghua-Yu/SUPIR" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SUPIR aims at developing Practical Algorithms for Photo-Realistic Image Restoration In the Wild&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[&lt;a href=&#34;https://arxiv.org/abs/2401.13627&#34;&gt;Paper&lt;/a&gt;]   [&lt;a href=&#34;http://supir.xpixel.group/&#34;&gt;Project Page&lt;/a&gt;]   [Online Demo (Coming soon)] &lt;br&gt; Fanghua, Yu, &lt;a href=&#34;https://www.jasongt.com/&#34;&gt;Jinjin Gu&lt;/a&gt;, Zheyuan Li, Jinfan Hu, Xiangtao Kong, &lt;a href=&#34;https://xinntao.github.io/&#34;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=GUxrycUAAAAJ&#34;&gt;Jingwen He&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=gFtI-8QAAAAJ&#34;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&#34;&gt;Chao Dong&lt;/a&gt; &lt;br&gt; Shenzhen Institute of Advanced Technology; Shanghai AI Laboratory; University of Sydney; The Hong Kong Polytechnic University; ARC Lab, Tencent PCG; The Chinese University of Hong Kong &lt;br&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Fanghua-Yu/SUPIR/master/assets/teaser.png&#34;&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;🔧 Dependencies and Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone repo&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Fanghua-Yu/SUPIR.git&#xA;cd SUPIR&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependent packages&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n SUPIR python=3.8 -y&#xA;conda activate SUPIR&#xA;pip install --upgrade pip&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download Checkpoints&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Dependent Models&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/openai/clip-vit-large-patch14&#34;&gt;SDXL CLIP Encoder-1&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k&#34;&gt;SDXL CLIP Encoder-2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/blob/main/sd_xl_base_1.0_0.9vae.safetensors&#34;&gt;SDXL base 1.0_0.9vae&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/openai/clip-vit-large-patch14-336&#34;&gt;LLaVA CLIP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.5-13b&#34;&gt;LLaVA v1.5 13B&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Models we provided:&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;SUPIR-v0Q&lt;/code&gt;: (Coming Soon) Google Drive, Baidu Netdisk&lt;/p&gt; &lt;p&gt;Default training settings with paper. High generalization and high image quality in most cases.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;SUPIR-v0F&lt;/code&gt;: (Coming Soon) Google Drive, Baidu Netdisk&lt;/p&gt; &lt;p&gt;Training with light degradation settings. Stage1 encoder of &lt;code&gt;SUPIR-v0F&lt;/code&gt; remains more details when facing light degradations.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Edit Custom Path for Checkpoints &lt;pre&gt;&lt;code&gt;* [CKPT_PTH.py] --&amp;gt; LLAVA_CLIP_PATH, LLAVA_MODEL_PATH, SDXL_CLIP1_PATH, SDXL_CLIP2_CACHE_DIR &#xA;* [options/SUPIR_v0.yaml] --&amp;gt; SDXL_CKPT, SUPIR_CKPT_Q, SUPIR_CKPT_F&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;⚡ Quick Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Usage of SUPIR&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;Usage: &#xA;-- python test.py [options] &#xA;-- python gradio_demo.py [interactive options]&#xA;&#xA;--img_dir                Input folder.&#xA;--save_dir               Output folder.&#xA;--upscale                Upsampling ratio of given inputs. Default: 1&#xA;--SUPIR_sign             Model selection. Default: &#39;Q&#39;; Options: [&#39;F&#39;, &#39;Q&#39;]&#xA;--seed                   Random seed. Default: 1234&#xA;--min_size               Minimum resolution of output images. Default: 1024&#xA;--edm_steps              Numb of steps for EDM Sampling Scheduler. Default: 50&#xA;--s_stage1               Control Strength of Stage1. Default: -1 (negative means invalid)&#xA;--s_churn                Original hy-param of EDM. Default: 5&#xA;--s_noise                Original hy-param of EDM. Default: 1.003&#xA;--s_cfg                  Classifier-free guidance scale for prompts. Default: 7.5&#xA;--s_stage2               Control Strength of Stage2. Default: 1.0&#xA;--num_samples            Number of samples for each input. Default: 1&#xA;--a_prompt               Additive positive prompt for all inputs. &#xA;    Default: &#39;Cinematic, High Contrast, highly detailed, taken using a Canon EOS R camera, &#xA;    hyper detailed photo - realistic maximum detail, 32k, Color Grading, ultra HD, extreme&#xA;     meticulous detailing, skin pore detailing, hyper sharpness, perfect without deformations.&#39;&#xA;--n_prompt               Fixed negative prompt for all inputs. &#xA;    Default: &#39;painting, oil painting, illustration, drawing, art, sketch, oil painting, &#xA;    cartoon, CG Style, 3D render, unreal engine, blurring, dirty, messy, worst quality, &#xA;    low quality, frames, watermark, signature, jpeg artifacts, deformed, lowres, over-smooth&#39;&#xA;--color_fix_type         Color Fixing Type. Default: &#39;Wavelet&#39;; Options: [&#39;None&#39;, &#39;AdaIn&#39;, &#39;Wavelet&#39;]&#xA;--linear_CFG             Linearly (with sigma) increase CFG from &#39;spt_linear_CFG&#39; to s_cfg. Default: False&#xA;--linear_s_stage2        Linearly (with sigma) increase s_stage2 from &#39;spt_linear_s_stage2&#39; to s_stage2. Default: False&#xA;--spt_linear_CFG         Start point of linearly increasing CFG. Default: 1.0&#xA;--spt_linear_s_stage2    Start point of linearly increasing s_stage2. Default: 0.0&#xA;--ae_dtype               Inference data type of AutoEncoder. Default: &#39;bf16&#39;; Options: [&#39;fp32&#39;, &#39;bf16&#39;]&#xA;--diff_dtype             Inference data type of Diffusion. Default: &#39;fp16&#39;; Options: [&#39;fp32&#39;, &#39;fp16&#39;, &#39;bf16&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Python Script&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;# Seek for best quality for most cases&#xA;CUDA_VISIBLE_DEVICES=0,1 python test.py --img_dir &#39;/opt/data/private/LV_Dataset/DiffGLV-Test-All/RealPhoto60/LQ&#39; --save_dir ./results-Q --SUPIR_sign Q --upscale 2&#xA;# for light degradation and high fidelity&#xA;CUDA_VISIBLE_DEVICES=0,1 python test.py --img_dir &#39;/opt/data/private/LV_Dataset/DiffGLV-Test-All/RealPhoto60/LQ&#39; --save_dir ./results-F --SUPIR_sign F --upscale 2 --s_cfg 4.0 --linear_CFG&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Gradio Demo&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Shell&#34;&gt;CUDA_VISIBLE_DEVICES=0,1 python gradio_demo.py --ip 0.0.0.0 --port 6688&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Online Demo (Coming Soon)&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{yu2024scaling,&#xA;  title={Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild}, &#xA;  author={Fanghua Yu and Jinjin Gu and Zheyuan Li and Jinfan Hu and Xiangtao Kong and Xintao Wang and Jingwen He and Yu Qiao and Chao Dong},&#xA;  year={2024},&#xA;  eprint={2401.13627},&#xA;  archivePrefix={arXiv},&#xA;  primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📧 Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any question, please email &lt;code&gt;fanghuayu96@gmail.com&lt;/code&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>