<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-31T01:26:35Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>agiresearch/AIOS</title>
    <updated>2024-03-31T01:26:35Z</updated>
    <id>tag:github.com,2024-03-31:/agiresearch/AIOS</id>
    <link href="https://github.com/agiresearch/AIOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AIOS: LLM Agent Operating System&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AIOS: LLM Agent Operating System&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.16971&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2312.03815&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-PDF-blue&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/agiresearch/AIOS/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-MIT-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;AIOS, a Large Language Model (LLM) Agent operating system, embeds large language model into Operating Systems (OS) as the brain of the OS, enabling an operating system &#34;with soul&#34; -- an important step towards AGI. AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents, maintain access control for agents, and provide a rich set of toolkits for LLM Agent developers.&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ  Architecture of AIOS&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/agiresearch/AIOS/main/images/AIOS-Architecture.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ğŸ“° News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024-03-25]&lt;/strong&gt; âœˆï¸ Our paper &lt;a href=&#34;https://arxiv.org/abs/2403.16971&#34;&gt;AIOS: LLM Agent Operating System&lt;/a&gt; is released and AIOS repository is officially launched!&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2023-12-06]&lt;/strong&gt; ğŸ“‹ After several months of working, our perspective paper &lt;a href=&#34;https://arxiv.org/abs/2312.03815&#34;&gt;LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem&lt;/a&gt; is officially released.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;âœˆï¸ Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Make sure you have Python &amp;gt;= 3.9&lt;/strong&gt;&lt;br&gt; Install the required packages using pip&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Usage&lt;/h3&gt; &#xA;&lt;p&gt;Set up Hugging Face token and cache directory&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export HUGGING_FACE_HUB_TOKEN=&amp;lt;YOUR READ TOKEN&amp;gt;&#xA;export HF_HOME=&amp;lt;YOUR CACHE DIRECTORY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the main.py to start (replace the max_gpu_memory and eval_device with your own)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use Gemma-2b-it&#xA;python main.py --llm_name gemma-2b-it --max_gpu_memory &#39;{&#34;0&#34;: &#34;24GB&#34;}&#39; --eval_device &#34;cuda:0&#34; --max_new_tokens 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use Mixtral-8x7b-it&#xA;python main.py --llm_name mixtral-8x7b-it --max_gpu_memory &#39;{&#34;0&#34;: &#34;48GB&#34;, &#34;1&#34;: &#34;48GB&#34;, &#34;2&#34;: &#34;48GB&#34;}&#39; --eval_device &#34;cuda:0&#34; --max_new_tokens 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸŒŸ Join Us!&lt;/h2&gt; &#xA;&lt;p&gt;AIOS is dedicated to facilitating LLM agents&#39; development and deployment in a systematic way, we are always looking for passionate collaborators to join us to foster a more cohesive, effective and efficient AIOS-Agent ecosystem. Suggestions and pull requests are always welcome!&lt;/p&gt; &#xA;&lt;h2&gt;ğŸ–‹ï¸ Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{mei2024aios,&#xA;  title={AIOS: LLM Agent Operating System},&#xA;  author={Mei, Kai and Li, Zelong and Xu, Shuyuan and Ye, Ruosong and Ge, Yingqiang and Zhang, Yongfeng}&#xA;  journal={arXiv:2403.16971},&#xA;  year={2024}&#xA;}&#xA;@article{ge2023llm,&#xA;  title={LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem},&#xA;  author={Ge, Yingqiang and Ren, Yujie and Hua, Wenyue and Xu, Shuyuan and Tan, Juntao and Zhang, Yongfeng},&#xA;  journal={arXiv:2312.03815},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ğŸ“ª Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any suggestions, or wish to contact us for any reason, feel free to email us at &lt;a href=&#34;mailto:marknju2018@gmail.com&#34;&gt;marknju2018@gmail.com&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Zejun-Yang/AniPortrait</title>
    <updated>2024-03-31T01:26:35Z</updated>
    <id>tag:github.com,2024-03-31:/Zejun-Yang/AniPortrait</id>
    <link href="https://github.com/Zejun-Yang/AniPortrait" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AniPortrait&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animations&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Author: Huawei Wei, Zejun Yang, Zhisheng Wang&lt;/p&gt; &#xA;&lt;p&gt;Organization: Tencent Games Zhiji, Tencent&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zejun-Yang/AniPortrait/main/asset/zhiji_logo.png&#34; alt=&#34;zhiji_logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Here we propose AniPortrait, a novel framework for generating high-quality animation driven by audio and a reference portrait image. You can also provide a video to achieve face reenacment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.17694&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Pipeline&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Zejun-Yang/AniPortrait/main/asset/pipeline.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Now our paper is available on arXiv.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;p&gt;Update the code to generate pose_temp.npy for head pose control.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;p&gt;We will release audio2pose pre-trained weight for audio2video after futher optimization. You can choose head pose template in &lt;code&gt;./configs/inference/head_pose_temp&lt;/code&gt; as substitution.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Various Generated Videos&lt;/h2&gt; &#xA;&lt;h3&gt;Self driven&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/82c0f0b0-9c7c-4aad-bf0e-27e6098ffbe1&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/51a502d9-1ce2-48d2-afbe-767a0b9b9166&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Face reenacment&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/d4e0add6-20a2-4f4b-808c-530a6f4d3331&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/849fce22-0db1-4257-a75f-a5dc655e6b9e&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Video Source: &lt;a href=&#34;https://www.bilibili.com/video/BV1H4421F7dE/?spm_id_from=333.337.search-card.all.click&#34;&gt;é¹¿ç«CAVY from bilibili&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Audio driven&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/63171e5a-e4c1-4383-8f20-9764524928d0&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/6fd74024-ba19-4f6b-b37a-10df5cf2c934&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/9e516cc5-bf09-4d45-b5e3-820030764982&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/Zejun-Yang/AniPortrait/assets/21038147/7c68148b-8022-453f-be9a-c69590038197&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Build environment&lt;/h3&gt; &#xA;&lt;p&gt;We recommend a python version &amp;gt;=3.10 and cuda version =11.7. Then build environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download weights&lt;/h3&gt; &#xA;&lt;p&gt;All the weights should be placed under the &lt;code&gt;./pretrained_weights&lt;/code&gt; direcotry. You can download weights manually as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our trained &lt;a href=&#34;https://huggingface.co/ZJYang/AniPortrait/tree/main&#34;&gt;weights&lt;/a&gt;, which include four parts: &lt;code&gt;denoising_unet.pth&lt;/code&gt;, &lt;code&gt;reference_unet.pth&lt;/code&gt;, &lt;code&gt;pose_guider.pth&lt;/code&gt;, &lt;code&gt;motion_module.pth&lt;/code&gt; and &lt;code&gt;audio2mesh.pt&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pretrained weight of based models and other components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;StableDiffusion V1.5&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/image_encoder&#34;&gt;image_encoder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/facebook/wav2vec2-base-960h&#34;&gt;wav2vec2-base-960h&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, these weights should be orgnized as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;./pretrained_weights/&#xA;|-- image_encoder&#xA;|   |-- config.json&#xA;|   `-- pytorch_model.bin&#xA;|-- sd-vae-ft-mse&#xA;|   |-- config.json&#xA;|   |-- diffusion_pytorch_model.bin&#xA;|   `-- diffusion_pytorch_model.safetensors&#xA;|-- stable-diffusion-v1-5&#xA;|   |-- feature_extractor&#xA;|   |   `-- preprocessor_config.json&#xA;|   |-- model_index.json&#xA;|   |-- unet&#xA;|   |   |-- config.json&#xA;|   |   `-- diffusion_pytorch_model.bin&#xA;|   `-- v1-inference.yaml&#xA;|-- wav2vec2-base-960h&#xA;|   |-- config.json&#xA;|   |-- feature_extractor_config.json&#xA;|   |-- preprocessor_config.json&#xA;|   |-- pytorch_model.bin&#xA;|   |-- README.md&#xA;|   |-- special_tokens_map.json&#xA;|   |-- tokenizer_config.json&#xA;|   `-- vocab.json&#xA;|-- audio2mesh.pt&#xA;|-- denoising_unet.pth&#xA;|-- motion_module.pth&#xA;|-- pose_guider.pth&#xA;`-- reference_unet.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: If you have installed some of the pretrained models, such as &lt;code&gt;StableDiffusion V1.5&lt;/code&gt;, you can specify their paths in the config file (e.g. &lt;code&gt;./config/prompts/animation.yaml&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Here are the cli commands for running inference scripts:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Kindly note that you can set -L to the desired number of generating frames in the command, for example, -L 300.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Self driven&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.pose2vid --config ./configs/prompts/animation.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can refer the format of animation.yaml to add your own reference images or pose videos. To convert the raw video into a pose video (keypoint sequence), you can run with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.vid2pose --video_path pose_video_path.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Face reenacment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.vid2vid --config ./configs/prompts/animation_facereenac.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add source face videos and reference images in the animation_facereenac.yaml.&lt;/p&gt; &#xA;&lt;h3&gt;Audio driven&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.audio2vid --config ./configs/prompts/animation_audio.yaml -W 512 -H 512&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Add audios and reference images in the animation_audio.yaml.&lt;/p&gt; &#xA;&lt;p&gt;You can use this command to generate a pose_temp.npy for head pose control:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.generate_ref_pose --ref_video ./configs/inference/head_pose_temp/pose_ref_video.mp4 --save_path ./configs/inference/head_pose_temp/pose.npy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Data preparation&lt;/h3&gt; &#xA;&lt;p&gt;Download &lt;a href=&#34;https://liangbinxie.github.io/projects/vfhq/&#34;&gt;VFHQ&lt;/a&gt; and &lt;a href=&#34;https://github.com/CelebV-HQ/CelebV-HQ&#34;&gt;CelebV-HQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Extract keypoints from raw videos and write training json file (here is an example of processing VFHQ):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.preprocess_dataset --input_dir VFHQ_PATH --output_dir SAVE_PATH --training_json JSON_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Update lines in the training config file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;data:&#xA;  json_path: JSON_PATH&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stage1&lt;/h3&gt; &#xA;&lt;p&gt;Run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch train_stage_1.py --config ./configs/train/stage1.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stage2&lt;/h3&gt; &#xA;&lt;p&gt;Put the pretrained motion module weights &lt;code&gt;mm_sd_v15_v2.ckpt&lt;/code&gt; (&lt;a href=&#34;https://huggingface.co/guoyww/animatediff/blob/main/mm_sd_v15_v2.ckpt&#34;&gt;download link&lt;/a&gt;) under &lt;code&gt;./pretrained_weights&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Specify the stage1 training weights in the config file &lt;code&gt;stage2.yaml&lt;/code&gt;, for example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;stage1_ckpt_dir: &#39;./exp_output/stage1&#39;&#xA;stage1_ckpt_step: 30000 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;accelerate launch train_stage_2.py --config ./configs/train/stage2.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We first thank the authors of &lt;a href=&#34;https://github.com/HumanAIGC/EMO&#34;&gt;EMO&lt;/a&gt;, and part of the images and audios in our demos are from EMO. Additionally, we would like to thank the contributors to the &lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone&#34;&gt;Moore-AnimateAnyone&lt;/a&gt;, &lt;a href=&#34;https://github.com/magic-research/magic-animate&#34;&gt;majic-animate&lt;/a&gt;, &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;animatediff&lt;/a&gt; and &lt;a href=&#34;https://github.com/guoqincode/Open-AnimateAnyone&#34;&gt;Open-AnimateAnyone&lt;/a&gt; repositories, for their open research and exploration.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{wei2024aniportrait,&#xA;      title={AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animations}, &#xA;      author={Huawei Wei and Zejun Yang and Zhisheng Wang},&#xA;      year={2024},&#xA;      eprint={2403.17694},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>easychen/one-person-businesses-methodology-v2.0</title>
    <updated>2024-03-31T01:26:35Z</updated>
    <id>tag:github.com,2024-03-31:/easychen/one-person-businesses-methodology-v2.0</id>
    <link href="https://github.com/easychen/one-person-businesses-methodology-v2.0" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ã€Šä¸€äººä¼ä¸šæ–¹æ³•è®ºã€‹ç¬¬äºŒç‰ˆï¼Œä¹Ÿé€‚åˆåšå…¶ä»–å‰¯ä¸šï¼ˆæ¯”å¦‚è‡ªåª’ä½“ã€ç”µå•†ã€æ•°å­—å•†å“ï¼‰çš„éæŠ€æœ¯äººç¾¤ã€‚&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ã€Šä¸€äººä¼ä¸šæ–¹æ³•è®ºã€‹ç¬¬äºŒç‰ˆ&lt;/h1&gt; &#xA;&lt;h2&gt;å¯¹ç¬¬ä¸€ç‰ˆçš„æ”¹è¿›&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ä»é•¿æ–‡åˆ°ä¸€æœ¬è¿‘6ä¸‡å­—çš„å°ä¹¦ï¼Œä»æœ‰æ„Ÿè€Œå‘çš„åˆ†äº«åˆ°ä¸¤å¹´è¿­ä»£è€Œå¾—çš„&lt;strong&gt;å®Œæ•´æ–¹æ³•è®º&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;ä¸å†å±€é™åœ¨ç‹¬ç«‹å¼€å‘ï¼Œå‘å±•ä¸º&lt;strong&gt;æ›´ä¸ºé€šç”¨çš„æ–¹æ³•è®º&lt;/strong&gt;ï¼ŒéæŠ€æœ¯è¯»è€…ä¹Ÿå¯åˆ›ä½œæ•°å­—å•†å“æˆ–åŸºäºNoCode/å¼€æºé¡¹ç›®+AIè¾…åŠ©æ„å»ºåœ¨çº¿æœåŠ¡&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/easychen/one-person-businesses-methodology-v2.0/master/src/images/opb-book-cover.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;ä½œè€…ä¿¡æ¯&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ä½œè€… &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ftqq.com&#34;&gt;Easy&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Email: &lt;a href=&#34;mailto:easychen@gmail.com&#34;&gt;easychen@gmail.com&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;å¾®åšï¼š&lt;a href=&#34;https://weibo.com/easy&#34;&gt;https://weibo.com/easy&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Xï¼š&lt;a href=&#34;https://twitter.com/easychen&#34;&gt;https://twitter.com/easychen&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;æˆæƒè¯´æ˜&lt;/h2&gt; &#xA;&lt;p&gt;æœ¬ä¹¦é‡‡ç”¨&lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh-hans&#34;&gt;CC-BY-NC-SAåè®®&lt;/a&gt;å‘å¸ƒã€‚&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;æ‚¨å¯ä»¥å¤åˆ¶ã€å‘è¡Œã€å±•è§ˆã€è¡¨æ¼”ã€æ”¾æ˜ ã€å¹¿æ’­æˆ–é€šè¿‡ä¿¡æ¯ç½‘ç»œä¼ æ’­æœ¬ä½œå“ï¼Œä½†å¿…é¡»ç½²åä½œè€…å¹¶æ·»åŠ é“¾æ¥åˆ°&lt;a href=&#34;https://github.com/easychen/one-person-businesses-methodology-v2.0&#34;&gt;æœ¬ä¹¦GitHubä»“åº“&lt;/a&gt;ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ä¸å¾—ä¸ºå•†ä¸šç›®çš„è€Œä½¿ç”¨æœ¬ä½œå“ã€‚&lt;/li&gt; &#xA; &lt;li&gt;ä»…åœ¨éµå®ˆä¸æœ¬ä½œå“ç›¸åŒçš„è®¸å¯æ¡æ¬¾ä¸‹ï¼Œæ‚¨æ‰èƒ½æ•£å¸ƒç”±æœ¬ä½œå“äº§ç”Ÿçš„æ´¾ç”Ÿä½œå“ã€‚&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ç”µå­ä¹¦&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;å¯ä½¿ç”¨ mdbook-epub å·¥å…·è‡ªè¡Œç¼–è¯‘&lt;/li&gt; &#xA; &lt;li&gt;æ‰«ç è®¢é˜…ã€Šæ–¹æ³•è®ºã€‹æ›´æ–°é¢‘é“åä¸‹è½½: &lt;a href=&#34;https://subdeer.cn/channel/landing/11&#34;&gt;è¿›å…¥&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;åœ¨çº¿é˜…è¯»&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;å®šä¹‰ä¸€äººä¼ä¸š&lt;/strong&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/opb-methodology-new-version-and-author?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;æ–°ç‰ˆæ–¹æ³•è®ºæ¦‚è¿°&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/define-opb?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;ä¸€äººä¼ä¸šçš„å®šä¹‰&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;è§„åˆ’ä¸€äººä¼ä¸š&lt;/strong&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/why-thinking-big-is-possible?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;åº•å±‚é€»è¾‘:ä¸ºä»€ä¹ˆä»¥å°åšå¤§æ˜¯å¯èƒ½çš„&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/why-scalability-is-possible?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;åº•å±‚é€»è¾‘:ä¸ºä»€ä¹ˆè§„æ¨¡åŒ–æ˜¯å¯èƒ½çš„&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/assets-and-passive-income?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;åº•å±‚é€»è¾‘:èµ„äº§å’Œè¢«åŠ¨æ”¶å…¥&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/snowballing-and-chain-propagation?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;åº•å±‚é€»è¾‘:æ»šé›ªçƒå’Œé“¾å¼ä¼ æ’­&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/race-track-selection-for-opb?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;èµ›é“é€‰æ‹©:ä¸€äººä¼ä¸šå¦‚ä½•é€‰æ‹©èµ›é“&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/non-competition-strategy?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;ç«äº‰ç­–ç•¥:ä¸ç«äº‰ç­–ç•¥&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/structured-advantage?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;ç«äº‰ç­–ç•¥:ç»“æ„åŒ–ä¼˜åŠ¿&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/opb-canvas-and-opb-report?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;æ€è€ƒå·¥å…·:ã€Šä¸€äººä¼ä¸šç”»å¸ƒã€‹å’Œã€Šä¸€äººä¼ä¸šæœˆæŠ¥ã€‹&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;æ„å»ºä¸€äººä¸šåŠ¡&lt;/strong&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/one-person-enterprise-does-not-equal-one-person-business?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;ä¸€äººä¼ä¸šâ‰ ä¸€äººä¸šåŠ¡&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/discovery-of-by-product-advantages?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;ä¼˜åŠ¿å‘ç°:å‰¯äº§å“ä¼˜åŠ¿&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/start-from-side-project?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;é£é™©è¯„æ§:ä»å‰¯ä¸šå¼€å§‹&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ft07.com/managing-and-utilizing-uncertainty?mtm_campaign=github&amp;amp;mtm_kwd=opbmv2&#34;&gt;é£é™©è¯„æ§:ç®¡ç†å’Œåˆ©ç”¨ä¸ç¡®å®šæ€§&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
</feed>