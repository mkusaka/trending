<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-10T01:26:56Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Blealtan/efficient-kan</title>
    <updated>2024-05-10T01:26:56Z</updated>
    <id>tag:github.com,2024-05-10:/Blealtan/efficient-kan</id>
    <link href="https://github.com/Blealtan/efficient-kan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN).&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;An Efficient Implementation of Kolmogorov-Arnold Network&lt;/h1&gt; &#xA;&lt;p&gt;This repository contains an efficient implementation of Kolmogorov-Arnold Network (KAN). The original implementation of KAN is available &lt;a href=&#34;https://github.com/KindXiaoming/pykan&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The performance issue of the original implementation is mostly because it needs to expand all intermediate variables to perform the different activation functions. For a layer with &lt;code&gt;in_features&lt;/code&gt; input and &lt;code&gt;out_features&lt;/code&gt; output, the original implementation needs to expand the input to a tensor with shape &lt;code&gt;(batch_size, out_features, in_features)&lt;/code&gt; to perform the activation functions. However, all activation functions are linear combination of a fixed set of basis functions which are B-splines; given that, we can reformulate the computation as activate the input with different basis functions and then combine them linearly. This reformulation can significantly reduce the memory cost and make the computation a straightforward matrix multiplication, and works with both forward and backward pass naturally.&lt;/p&gt; &#xA;&lt;p&gt;The problem is in the &lt;strong&gt;sparsification&lt;/strong&gt; which is claimed to be critical to KAN&#39;s interpretability. The authors proposed a L1 regularization defined on the input samples, which requires non-linear operations on the &lt;code&gt;(batch_size, out_features, in_features)&lt;/code&gt; tensor, and is thus not compatible with the reformulation. I instead replace the L1 regularization with a L1 regularization on the weights, which is more common in neural networks and is compatible with the reformulation. The author&#39;s implementation indeed include this kind of regularization alongside the one described in the paper as well, so I think it might help. More experiments are needed to verify this; but at least the original approach is infeasible if efficiency is wanted.&lt;/p&gt; &#xA;&lt;p&gt;Another difference is that, beside the learnable activation functions (B-splines), the original implementation also includes a learnable scale on each activation function. I provided an option &lt;code&gt;enable_standalone_scale_spline&lt;/code&gt; that defaults to &lt;code&gt;True&lt;/code&gt; to include this feature; disable it will make the model more efficient, but potentially hurts results. It needs more experiments.&lt;/p&gt; &#xA;&lt;p&gt;2024-05-04 Update: @xiaol hinted that the constant initialization of &lt;code&gt;base_weight&lt;/code&gt; parameters can be a problem on MNIST. For now I&#39;ve changed both the &lt;code&gt;base_weight&lt;/code&gt; and &lt;code&gt;spline_scaler&lt;/code&gt; matrices to be initialized with &lt;code&gt;kaiming_uniform_&lt;/code&gt;, following &lt;code&gt;nn.Linear&lt;/code&gt;&#39;s initialization. It seems to work much much better on MNIST (~20% to ~97%), but I&#39;m not sure if it&#39;s a good idea in general.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>AdityaNG/kan-gpt</title>
    <updated>2024-05-10T01:26:56Z</updated>
    <id>tag:github.com,2024-05-10:/AdityaNG/kan-gpt</id>
    <link href="https://github.com/AdityaNG/kan-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The PyTorch implementation of Generative Pre-trained Transformers (GPTs) using Kolmogorov-Arnold Networks (KANs) for language modeling&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KAN-GPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/pypi/dd/kan-gpt&#34; alt=&#34;PyPI - Downloads&#34;&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/kan-gpt&#34; alt=&#34;PyPI - Version&#34;&gt; &lt;a href=&#34;https://codecov.io/gh/AdityaNG/kan-gpt&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/AdityaNG/kan-gpt/branch/main/graph/badge.svg?token=kan-gpt_token_here&#34; alt=&#34;codecov&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/AdityaNG/kan-gpt/actions/workflows/main.yml&#34;&gt;&lt;img src=&#34;https://github.com/AdityaNG/kan-gpt/actions/workflows/main.yml/badge.svg?sanitize=true&#34; alt=&#34;CI&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/license/AdityaNG/kan-gpt&#34; alt=&#34;GitHub License&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The PyTorch implementation of Generative Pre-trained Transformers (GPTs) using Kolmogorov-Arnold Networks (KANs) for language modeling&lt;/p&gt; &#xA;&lt;h2&gt;Install it from PyPI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install kan_gpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Refer to the &lt;a href=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/KAN_GPT.ipynb&#34;&gt;KAN_GPT.ipynb&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/kan_gpt/prompt.py&#34;&gt;kan_gpt/prompt.py&lt;/a&gt; for usage examples. The following is an outine of how to use the model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from kan_gpt.model import GPT&#xA;from transformers import GPT2Tokenizer&#xA;&#xA;model_config = GPT.get_default_config()&#xA;model_config.model_type = &#34;gpt2&#34;&#xA;model_config.vocab_size = 50257&#xA;model_config.block_size = 1024&#xA;model = GPT(model_config)&#xA;&#xA;tokenizer = GPT2Tokenizer.from_pretrained(&#39;gpt2&#39;)&#xA;&#xA;prompt = &#34;Bangalore is often described as the &#34;&#xA;&#xA;prompt_encoded = tokenizer.encode(&#xA;  text=prompt, add_special_tokens=False&#xA;)&#xA;&#xA;x = torch.tensor(prompt_encoded).unsqueeze(0)&#xA;&#xA;model.eval()&#xA;y = model.generate(x, 50)  # sample 50 tokens&#xA;&#xA;result = tokenizer.decode(y)&#xA;&#xA;print(result)&#xA;&#xA;# Bangalore is often described as the Silicon Valley of India.&#xA;# The city has witnessed rapid growth in the past two decades.....&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setup for Development&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Download Repo&#xA;git clone https://github.com/AdityaNG/kan-gpt&#xA;cd kan-gpt&#xA;git pull&#xA;&#xA;# Download Dataset&#xA;./scripts/download_webtext.sh&#xA;./scripts/download_tinyshakespeare.sh&#xA;&#xA;# Install dependencies for development&#xA;pip install -r requirements.txt&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Train&lt;/h2&gt; &#xA;&lt;p&gt;Use the following dummy script to make sure everything is working as expected&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;WANDB_MODE=offline CUDA_VISIBLE_DEVICE=&#34;&#34; python3 -m kan_gpt.train --architecture MLP --batch_size 1 --dummy_dataset --device cpu --max_iters 200&#xA;WANDB_MODE=offline CUDA_VISIBLE_DEVICE=&#34;&#34; python3 -m kan_gpt.train --architecture KAN --batch_size 1 --dummy_dataset --device cpu --max_iters 200&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then make use of the training script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m kan_gpt.train&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Prompt&lt;/h2&gt; &#xA;&lt;p&gt;You can prompt the model to produce text as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m kan_gpt.prompt --prompt &#34;Bangalore is often described as the &#34; --model_path (checkpoint)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results&lt;/h2&gt; &#xA;&lt;p&gt;We train and compare KAN-GPT with an equivalent MLP-GPT model on the Tiny Shakespeare dataset. We observe that the KAN-GPT performs slightly better than the MLP-GPT. We are looking into further experiments to dive deeper. The results are shown below:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Metrics&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/media/results_loss.png&#34; alt=&#34;results_loss&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/media/results_cross_entropy.png&#34; alt=&#34;results_cross_entropy&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/media/results_perplexity.png&#34; alt=&#34;results_perplexity&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate &lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;minGPT&lt;/a&gt; and &lt;a href=&#34;https://github.com/KindXiaoming/pykan&#34;&gt;pykan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Dataset downloading script for &lt;a href=&#34;https://github.com/openai/gpt-2-output-dataset&#34;&gt;WebText&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; PyTorch Dataset parser for &lt;a href=&#34;https://github.com/openai/gpt-2-output-dataset&#34;&gt;WebText&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; PyTorch Dataset parser for &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#34;&gt;tinyshakespeare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mini training POC for KAN-GPT &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate KAN training logic from &lt;code&gt;KAN.train_kan&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train a dummy batch w/o any memory issues&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Mini training POC for MLP-GPT&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train MLP-GPT on the webtext dataset as a baseline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train KAN-GPT on the webtext dataset as a baseline&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Metrics comparing KAN-GPT and MLP-GPT&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Auto Save checkpoints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Auto Save checkpoints to W&amp;amp;B&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Auto Download model weights from git / huggingface&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; W&amp;amp;B hyperparam sweep script&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Script to load checkpoint in interactive mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Reduce requrements.txt constraints&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Define pydantic model for training and sweep args&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Pruning the package, get rid of unused code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training script to PyTorch Lighting&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Documentation: &lt;code&gt;mkdocs gh-deploy&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate with &lt;a href=&#34;https://github.com/Blealtan/efficient-kan/raw/master/src/efficient_kan/kan.py&#34;&gt;efficient-kan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Test Cases &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; KAN: Forward-Backward test&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; GPT: Forward-Backward test&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; KAN_GPT: Forward-Backward test&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; EFFICIENT_KAN: Forward-Backward test&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/AdityaNG/kan-gpt/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/karpathy/minGPT&#34;&gt;minGPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/KindXiaoming/pykan&#34;&gt;pykan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/gpt-2-output-dataset&#34;&gt;webtext&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&#34;&gt;tinyshakespeare&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>infiniflow/ragflow</title>
    <updated>2024-05-10T01:26:56Z</updated>
    <id>tag:github.com,2024-05-10:/infiniflow/ragflow</id>
    <link href="https://github.com/infiniflow/ragflow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://demo.ragflow.io/&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/web/src/assets/logo-with-text.png&#34; width=&#34;520&#34; alt=&#34;ragflow logo&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README_zh.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/README_ja.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/infiniflow/ragflow/releases/latest&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&amp;amp;label=Latest%20Release&#34; alt=&#34;Latest Release&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://demo.ragflow.io&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Static Badge&#34; src=&#34;https://img.shields.io/badge/Online-Demo-4e6b99&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/infiniflow/ragflow&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/docker_pull-ragflow:v0.5.0-brightgreen&#34; alt=&#34;docker pull infiniflow/ragflow:v0.5.0&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/infiniflow/ragflow/raw/main/LICENSE&#34;&gt; &lt;img height=&#34;21&#34; src=&#34;https://img.shields.io/badge/License-Apache--2.0-ffffff?style=flat-square&amp;amp;labelColor=d4eaf7&amp;amp;color=1570EF&#34; alt=&#34;license&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üí° What is RAGFlow?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://demo.ragflow.io&#34;&gt;RAGFlow&lt;/a&gt; is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding. It offers a streamlined RAG workflow for businesses of any scale, combining LLM (Large Language Models) to provide truthful question-answering capabilities, backed by well-founded citations from various complex formatted data.&lt;/p&gt; &#xA;&lt;h2&gt;üåü Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;üç≠ &lt;strong&gt;&#34;Quality in, quality out&#34;&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/deepdoc/README.md&#34;&gt;Deep document understanding&lt;/a&gt;-based knowledge extraction from unstructured data with complicated formats.&lt;/li&gt; &#xA; &lt;li&gt;Finds &#34;needle in a data haystack&#34; of literally unlimited tokens.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üç± &lt;strong&gt;Template-based chunking&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Intelligent and explainable.&lt;/li&gt; &#xA; &lt;li&gt;Plenty of template options to choose from.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üå± &lt;strong&gt;Grounded citations with reduced hallucinations&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Visualization of text chunking to allow human intervention.&lt;/li&gt; &#xA; &lt;li&gt;Quick view of the key references and traceable citations to support grounded answers.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üçî &lt;strong&gt;Compatibility with heterogeneous data sources&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üõÄ &lt;strong&gt;Automated and effortless RAG workflow&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streamlined RAG orchestration catered to both personal and large businesses.&lt;/li&gt; &#xA; &lt;li&gt;Configurable LLMs as well as embedding models.&lt;/li&gt; &#xA; &lt;li&gt;Multiple recall paired with fused re-ranking.&lt;/li&gt; &#xA; &lt;li&gt;Intuitive APIs for seamless integration with business.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìå Latest Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024-05-08 Integrates LLM DeepSeek-V2.&lt;/li&gt; &#xA; &lt;li&gt;2024-04-26 Adds file management.&lt;/li&gt; &#xA; &lt;li&gt;2024-04-19 Supports conversation API (&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docs/conversation_api.md&#34;&gt;detail&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;2024-04-16 Integrates an embedding model &#39;bce-embedding-base_v1&#39; from &lt;a href=&#34;https://github.com/netease-youdao/BCEmbedding&#34;&gt;BCEmbedding&lt;/a&gt;, and &lt;a href=&#34;https://github.com/qdrant/fastembed&#34;&gt;FastEmbed&lt;/a&gt;, which is designed specifically for light and speedy embedding.&lt;/li&gt; &#xA; &lt;li&gt;2024-04-11 Supports &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docs/xinference.md&#34;&gt;Xinference&lt;/a&gt; for local LLM deployment.&lt;/li&gt; &#xA; &lt;li&gt;2024-04-10 Adds a new layout recognition model for analyzing legal documents.&lt;/li&gt; &#xA; &lt;li&gt;2024-04-08 Supports &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docs/ollama.md&#34;&gt;Ollama&lt;/a&gt; for local LLM deployment.&lt;/li&gt; &#xA; &lt;li&gt;2024-04-07 Supports Chinese UI.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üîé System Architecture&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;margin-top:20px;margin-bottom:20px;&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/infiniflow/ragflow/assets/12318111/d6ac5664-c237-4200-a7c2-a4a00691b485&#34; width=&#34;1000&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üé¨ Get Started&lt;/h2&gt; &#xA;&lt;h3&gt;üìù Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CPU &amp;gt;= 4 cores&lt;/li&gt; &#xA; &lt;li&gt;RAM &amp;gt;= 16 GB&lt;/li&gt; &#xA; &lt;li&gt;Disk &amp;gt;= 50 GB&lt;/li&gt; &#xA; &lt;li&gt;Docker &amp;gt;= 24.0.0 &amp;amp; Docker Compose &amp;gt;= v2.26.1 &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;If you have not installed Docker on your local machine (Windows, Mac, or Linux), see &lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Install Docker Engine&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;üöÄ Start up the server&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure &lt;code&gt;vm.max_map_count&lt;/code&gt; &amp;gt;= 262144 (&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docs/max_map_count.md&#34;&gt;more&lt;/a&gt;):&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;To check the value of &lt;code&gt;vm.max_map_count&lt;/code&gt;:&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ sysctl vm.max_map_count&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;Reset &lt;code&gt;vm.max_map_count&lt;/code&gt; to a value at least 262144 if it is not.&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# In this case, we set it to 262144:&#xA;$ sudo sysctl -w vm.max_map_count=262144&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;This change will be reset after a system reboot. To ensure your change remains permanent, add or update the &lt;code&gt;vm.max_map_count&lt;/code&gt; value in &lt;strong&gt;/etc/sysctl.conf&lt;/strong&gt; accordingly:&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vm.max_map_count=262144&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repo:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/infiniflow/ragflow.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the pre-built Docker images and start up the server:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd ragflow/docker&#xA;$ chmod +x ./entrypoint.sh&#xA;$ docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Please note that running the above commands will automatically download the development version docker image of RAGFlow. If you want to download and run a specific version of docker image, please find the RAGFLOW_VERSION variable in the docker/.env file, change it to the corresponding version, for example, RAGFLOW_VERSION=v0.5.0, and run the above commands.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;The core image is about 9 GB in size and may take a while to load.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Check the server status after having the server up and running:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker logs -f ragflow-server&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;The following output confirms a successful launch of the system:&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    ____                 ______ __&#xA;   / __ \ ____ _ ____ _ / ____// /____  _      __&#xA;  / /_/ // __ `// __ `// /_   / // __ \| | /| / /&#xA; / _, _// /_/ // /_/ // __/  / // /_/ /| |/ |/ /&#xA;/_/ |_| \__,_/ \__, //_/    /_/ \____/ |__/|__/&#xA;              /____/&#xA;&#xA; * Running on all addresses (0.0.0.0)&#xA; * Running on http://127.0.0.1:9380&#xA; * Running on http://x.x.x.x:9380&#xA; INFO:werkzeug:Press CTRL+C to quit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a &lt;code&gt;network anomaly&lt;/code&gt; error because, at that moment, your RAGFlow may not be fully initialized.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In your web browser, enter the IP address of your server and log in to RAGFlow.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;With default settings, you only need to enter &lt;code&gt;http://IP_OF_YOUR_MACHINE&lt;/code&gt; (&lt;strong&gt;sans&lt;/strong&gt; port number) as the default HTTP serving port &lt;code&gt;80&lt;/code&gt; can be omitted when using the default configurations.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt;, select the desired LLM factory in &lt;code&gt;user_default_llm&lt;/code&gt; and update the &lt;code&gt;API_KEY&lt;/code&gt; field with the corresponding API key.&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docs/llm_api_key_setup.md&#34;&gt;./docs/llm_api_key_setup.md&lt;/a&gt; for more information.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;p&gt;&lt;em&gt;The show is now on!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üîß Configurations&lt;/h2&gt; &#xA;&lt;p&gt;When it comes to system configurations, you will need to manage the following files:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env&#34;&gt;.env&lt;/a&gt;: Keeps the fundamental setups for the system, such as &lt;code&gt;SVR_HTTP_PORT&lt;/code&gt;, &lt;code&gt;MYSQL_PASSWORD&lt;/code&gt;, and &lt;code&gt;MINIO_PASSWORD&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt;: Configures the back-end services.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt;: The system relies on &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt; to start up.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You must ensure that changes to the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/.env&#34;&gt;.env&lt;/a&gt; file are in line with what are in the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md&#34;&gt;./docker/README&lt;/a&gt; file provides a detailed description of the environment settings and service configurations, and you are REQUIRED to ensure that all environment settings listed in the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/README.md&#34;&gt;./docker/README&lt;/a&gt; file are aligned with the corresponding configurations in the &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/service_conf.yaml&#34;&gt;service_conf.yaml&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;To update the default HTTP serving port (80), go to &lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docker/docker-compose.yml&#34;&gt;docker-compose.yml&lt;/a&gt; and change &lt;code&gt;80:80&lt;/code&gt; to &lt;code&gt;&amp;lt;YOUR_SERVING_PORT&amp;gt;:80&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Updates to all system configurations require a system reboot to take effect:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ docker-compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Build from source&lt;/h2&gt; &#xA;&lt;p&gt;To build the Docker images from source:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/infiniflow/ragflow.git&#xA;$ cd ragflow/&#xA;$ docker build -t infiniflow/ragflow:dev .&#xA;$ cd ragflow/docker&#xA;$ chmod +x ./entrypoint.sh&#xA;$ docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Launch Service from Source&lt;/h2&gt; &#xA;&lt;p&gt;To launch the service from source, please follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ git clone https://github.com/infiniflow/ragflow.git&#xA;$ cd ragflow/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create a virtual environment (ensure Anaconda or Miniconda is installed)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ conda create -n ragflow python=3.11.0&#xA;$ conda activate ragflow&#xA;$ pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If CUDA version is greater than 12.0, execute the following additional commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip uninstall -y onnxruntime-gpu&#xA;$ pip install onnxruntime-gpu --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Copy the entry script and configure environment variables&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cp docker/entrypoint.sh .&#xA;$ vi entrypoint.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Use the following commands to obtain the Python path and the ragflow project path:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ which python&#xA;$ pwd&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set the output of &lt;code&gt;which python&lt;/code&gt; as the value for &lt;code&gt;PY&lt;/code&gt; and the output of &lt;code&gt;pwd&lt;/code&gt; as the value for &lt;code&gt;PYTHONPATH&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt; is already configured, it can be commented out.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Adjust configurations according to your actual situation; the two export commands are newly added.&#xA;PY=${PY}&#xA;export PYTHONPATH=${PYTHONPATH}&#xA;# Optional: Add Hugging Face mirror&#xA;export HF_ENDPOINT=https://hf-mirror.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Start the base services&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ cd docker&#xA;$ docker compose -f docker-compose-base.yml up -d &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;Check the configuration files Ensure that the settings in &lt;strong&gt;docker/.env&lt;/strong&gt; match those in &lt;strong&gt;conf/service_conf.yaml&lt;/strong&gt;. The IP addresses and ports for related services in &lt;strong&gt;service_conf.yaml&lt;/strong&gt; should be changed to the local machine IP and ports exposed by the container.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Launch the service&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ chmod +x ./entrypoint.sh&#xA;$ bash ./entrypoint.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìö Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/infiniflow/ragflow/main/docs/faq.md&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìú Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://github.com/infiniflow/ragflow/issues/162&#34;&gt;RAGFlow Roadmap 2024&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üèÑ Community&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/4XxujFgUN7&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/infiniflowai&#34;&gt;Twitter&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üôå Contributing&lt;/h2&gt; &#xA;&lt;p&gt;RAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community. If you would like to be a part, review our &lt;a href=&#34;https://github.com/infiniflow/ragflow/raw/main/docs/CONTRIBUTING.md&#34;&gt;Contribution Guidelines&lt;/a&gt; first.&lt;/p&gt;</summary>
  </entry>
</feed>