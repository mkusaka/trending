<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-25T01:25:33Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>heyman/heynote</title>
    <updated>2023-12-25T01:25:33Z</updated>
    <id>tag:github.com,2023-12-25:/heyman/heynote</id>
    <link href="https://github.com/heyman/heynote" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A dedicated scratchpad for developers&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Heynote&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/heyman/heynote/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/heyman/heynote&#34; alt=&#34;GitHub release (latest SemVer)&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Heynote is a dedicated scratchpad for developers. It functions as a large persistent text buffer where you can write down anything you like. Works great for that Slack message you don&#39;t want to accidentally send, a JSON response from an API you&#39;re working with, notes from a meeting, your daily to-do list, etc.&lt;/p&gt; &#xA;&lt;p&gt;The Heynote buffer is divided into blocks, and each block can have its own Language set (e.g. JavaScript, JSON, Markdown, etc.). This gives you syntax highlighting and lets you auto-format that JSON response.&lt;/p&gt; &#xA;&lt;p&gt;Available for Mac, Windows, and Linux.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Persistent text buffer&lt;/li&gt; &#xA; &lt;li&gt;Block-based&lt;/li&gt; &#xA; &lt;li&gt;Syntax highlighting &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;C++&lt;/li&gt; &#xA;   &lt;li&gt;C#&lt;/li&gt; &#xA;   &lt;li&gt;CSS&lt;/li&gt; &#xA;   &lt;li&gt;HTML&lt;/li&gt; &#xA;   &lt;li&gt;Java&lt;/li&gt; &#xA;   &lt;li&gt;JavaScript&lt;/li&gt; &#xA;   &lt;li&gt;JSON&lt;/li&gt; &#xA;   &lt;li&gt;Markdown&lt;/li&gt; &#xA;   &lt;li&gt;PHP&lt;/li&gt; &#xA;   &lt;li&gt;Python&lt;/li&gt; &#xA;   &lt;li&gt;Rust&lt;/li&gt; &#xA;   &lt;li&gt;SQL&lt;/li&gt; &#xA;   &lt;li&gt;XML&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Language auto-detection&lt;/li&gt; &#xA; &lt;li&gt;Auto-formatting&lt;/li&gt; &#xA; &lt;li&gt;Math/Calculator mode&lt;/li&gt; &#xA; &lt;li&gt;Currency conversion&lt;/li&gt; &#xA; &lt;li&gt;Multi-cursor editing&lt;/li&gt; &#xA; &lt;li&gt;Dark &amp;amp; Light themes&lt;/li&gt; &#xA; &lt;li&gt;Option to set a global hotkey to show/hide the app&lt;/li&gt; &#xA; &lt;li&gt;Default or Emacs-like key bindings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download the appropriate (Mac, Windows or Linux) version from the latest Github release (or from &lt;a href=&#34;https://heynote.com&#34;&gt;heynote.com&lt;/a&gt;). The Windows build is not signed, so you might see some scary warning (I can not justify paying a yearly fee for a certificate just to get rid of that).&lt;/p&gt; &#xA;&lt;h3&gt;Notes on Linux installation&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s been reported &lt;a href=&#34;https://github.com/heyman/heynote/issues/48&#34;&gt;(#48)&lt;/a&gt; that ChromeOS&#39;s Debian VM need the following packages installed to run the Heynote AppImage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;libfuse2&#xA;libnss3&#xA;libnspr4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;To develop Heynote you need Node.js and you should (hopefully) just need to check out the code and then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;gt; npm install&#xA;&amp;gt; npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Contributions&lt;/h3&gt; &#xA;&lt;p&gt;I&#39;m happy to merge contributions that fit my vision for the app. Bug fixes are always welcome.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;Where is the buffer data stored?&lt;/h3&gt; &#xA;&lt;p&gt;The default paths for the buffer data for the respective OS are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mac: &lt;code&gt;~/Library/Application Support/Heynote/buffer.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Windows: &lt;code&gt;%APPDATA%\Heynote\buffer.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Linux: &lt;code&gt;~/.config/Heynote/buffer.txt&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;From version &amp;gt;=1.5.0, symlinks will be supported and you&#39;ll be able to configure the path where &lt;code&gt;buffer.txt&lt;/code&gt; is stored.&lt;/p&gt; &#xA;&lt;h3&gt;Can you make a mobile app?&lt;/h3&gt; &#xA;&lt;p&gt;No, at the moment this is out of scope, sorry.&lt;/p&gt; &#xA;&lt;h3&gt;Can you add a feature for naming blocks and/or adding tags? &lt;a href=&#34;https://github.com/heyman/heynote/issues/44&#34;&gt;(#44)&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Currently, I&#39;m not planning on adding this. The main reason is that it goes against the scratchpadness of the program.&lt;/p&gt; &#xA;&lt;p&gt;I can totally see the usefulness of such a feature, and it&#39;s definitely something that I would expect from a more traditional Notes app. However a large part of Heynote&#39;s appeal is it&#39;s simplicity, and if that is to remain so, I&#39;m going to have to say no to a lot of actually useful features.&lt;/p&gt; &#xA;&lt;h2&gt;Thanks!&lt;/h2&gt; &#xA;&lt;p&gt;Heynote is built upon &lt;a href=&#34;https://codemirror.net/&#34;&gt;CodeMirror&lt;/a&gt;, &lt;a href=&#34;https://vuejs.org/&#34;&gt;Vue&lt;/a&gt;, &lt;a href=&#34;https://www.electronjs.org/&#34;&gt;Electron&lt;/a&gt;, &lt;a href=&#34;https://mathjs.org/&#34;&gt;Math.js&lt;/a&gt;, &lt;a href=&#34;https://prettier.io/&#34;&gt;Prettier&lt;/a&gt; and other great open-source projects.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cumulo-autumn/StreamDiffusion</title>
    <updated>2023-12-25T01:25:33Z</updated>
    <id>tag:github.com,2023-12-25:/cumulo-autumn/StreamDiffusion</id>
    <link href="https://github.com/cumulo-autumn/StreamDiffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StreamDiffusion&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/README.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/README-ja.md&#34;&gt;日本語&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_07.gif&#34; width=&#34;90%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_09.gif&#34; width=&#34;90%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;StreamDiffusion: A Pipeline-Level Solution for Real-Time Interactive Generation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt; &lt;a href=&#34;https://www.linkedin.com/in/akio-kodaira-1a7b98252/&#34;&gt;Akio Kodaira&lt;/a&gt;, &lt;a href=&#34;https://www.chenfengx.com/&#34;&gt;Chenfeng Xu&lt;/a&gt;, Toshiki Hazama, &lt;a href=&#34;https://twitter.com/__ramu0e__&#34;&gt;Takanori Yoshimoto&lt;/a&gt;, &lt;a href=&#34;https://www.linkedin.com/in/kohei--ohno/&#34;&gt;Kohei Ohno&lt;/a&gt;, &lt;a href=&#34;https://me.ddpn.world/&#34;&gt;Shogo Mitsuhori&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/toni_nimono&#34;&gt;Soichi Sugano&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/hanyingcl&#34;&gt;Hanying Cho&lt;/a&gt;, &lt;a href=&#34;https://zhijianliu.com/&#34;&gt;Zhijian Liu&lt;/a&gt;, &lt;a href=&#34;https://scholar.google.com/citations?hl=en&amp;amp;user=ID9QePIAAAAJ&#34;&gt;Kurt Keutzer&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;StreamDiffusion is an innovative diffusion pipeline designed for real-time interactive generation. It introduces significant performance enhancements to current diffusion-based image generation techniques.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.12491&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2307.04725-b31b1b.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2312.12491&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-papers-yellow&#34; alt=&#34;Hugging Face Papers&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We sincerely thank &lt;a href=&#34;https://twitter.com/AttaQjp&#34;&gt;Taku Fujimoto&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/radamar&#34;&gt;Radamés Ajna&lt;/a&gt; and Hugging Face team for their invaluable feedback, courteous support, and insightful discussions.&lt;/p&gt; &#xA;&lt;h2&gt;Key Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stream Batch&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Streamlined data processing through efficient batch operations.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Residual Classifier-Free Guidance&lt;/strong&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/#residual-cfg-rcfg&#34;&gt;Learn More&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Improved guidance mechanism that minimizes computational redundancy.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Stochastic Similarity Filter&lt;/strong&gt; - &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/#stochastic-similarity-filter&#34;&gt;Learn More&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Improves GPU utilization efficiency through advanced filtering techniques.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;IO Queues&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Efficiently manages input and output operations for smoother execution.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Pre-Computation for KV-Caches&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Optimizes caching strategies for accelerated processing.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Model Acceleration Tools&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Utilizes various tools for model optimization and performance boost.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;When images are produced using our proposed StreamDiffusion pipeline in an environment with &lt;strong&gt;GPU: RTX 4090&lt;/strong&gt;, &lt;strong&gt;CPU: Core i9-13900K&lt;/strong&gt;, and &lt;strong&gt;OS: Ubuntu 22.04.3 LTS&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Denoising Step&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;fps on Txt2Img&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;fps on Img2Img&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;SD-turbo&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;106.16&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;93.897&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LCM-LoRA &lt;br&gt;+&lt;br&gt; KohakuV2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38.023&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.133&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Feel free to explore each feature by following the provided links to learn more about StreamDiffusion&#39;s capabilities. If you find it helpful, please consider citing our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;@article{kodaira2023streamdiffusion,&#xA;      title={StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation},&#xA;      author={Akio Kodaira and Chenfeng Xu and Toshiki Hazama and Takanori Yoshimoto and Kohei Ohno and Shogo Mitsuhori and Soichi Sugano and Hanying Cho and Zhijian Liu and Kurt Keutzer},&#xA;      year={2023},&#xA;      eprint={2312.12491},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Step0: clone this repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/cumulo-autumn/StreamDiffusion.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step1: Make Environment&lt;/h3&gt; &#xA;&lt;p&gt;You can install StreamDiffusion via pip, conda, or Docker(explanation below).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n streamdiffusion python=3.10&#xA;conda activate streamdiffusion&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;OR&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-cmd&#34;&gt;python -m venv .venv&#xA;# Windows&#xA;.\.venv\Scripts\activate&#xA;# Linux&#xA;source .venv/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step2: Install PyTorch&lt;/h3&gt; &#xA;&lt;p&gt;Select the appropriate version for your system.&lt;/p&gt; &#xA;&lt;p&gt;CUDA 11.8&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;CUDA 12.1&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install torch==2.1.0 torchvision==0.16.0 xformers --index-url https://download.pytorch.org/whl/cu121&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;details: &lt;a href=&#34;https://pytorch.org/&#34;&gt;https://pytorch.org/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Step3: Install StreamDiffusion&lt;/h3&gt; &#xA;&lt;h4&gt;For User&lt;/h4&gt; &#xA;&lt;p&gt;Install StreamDiffusion&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#for Latest Version (recommended)&#xA;pip install git+https://github.com/cumulo-autumn/StreamDiffusion.git@main#egg=streamdiffusion[tensorrt]&#xA;&#xA;&#xA;#or&#xA;&#xA;&#xA;#for Stable Version&#xA;pip install streamdiffusion[tensorrt]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install TensorRT extension and pywin32 (※※pywin32 is required only for Windows.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m streamdiffusion.tools.install-tensorrt&#xA;# If you use Windows, you need to install pywin32 &#xA;pip install pywin32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For Developer&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python setup.py develop easy_install streamdiffusion[tensorrt]&#xA;python -m streamdiffusion.tools.install-tensorrt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker Installation (TensorRT Ready)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/cumulo-autumn/StreamDiffusion.git&#xA;cd StreamDiffusion&#xA;docker build -t stream-diffusion:latest -f Dockerfile .&#xA;docker run --gpus all -it -v $(pwd):/home/ubuntu/streamdiffusion stream-diffusion:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;You can try StreamDiffusion in &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_02.gif&#34; alt=&#34;画像3&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_03.gif&#34; alt=&#34;画像4&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_04.gif&#34; alt=&#34;画像5&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_05.gif&#34; alt=&#34;画像6&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Real-Time Txt2Img Demo&lt;/h2&gt; &#xA;&lt;p&gt;There is an interactive txt2img demo in &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/demo/realtime-txt2img&#34;&gt;&lt;code&gt;demo/realtime-txt2img&lt;/code&gt;&lt;/a&gt; directory!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_01.gif&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Usage Example&lt;/h2&gt; &#xA;&lt;p&gt;We provide a simple example of how to use StreamDiffusion. For more detailed examples, please refer to &lt;a href=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/examples&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;h3&gt;Image-to-Image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import AutoencoderTiny, StableDiffusionPipeline&#xA;from diffusers.utils import load_image&#xA;&#xA;from streamdiffusion import StreamDiffusion&#xA;from streamdiffusion.image_utils import postprocess_image&#xA;&#xA;# You can load any models using diffuser&#39;s StableDiffusionPipeline&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#34;KBlueLeaf/kohaku-v2.1&#34;).to(&#xA;    device=torch.device(&#34;cuda&#34;),&#xA;    dtype=torch.float16,&#xA;)&#xA;&#xA;# Wrap the pipeline in StreamDiffusion&#xA;stream = StreamDiffusion(&#xA;    pipe,&#xA;    t_index_list=[32, 45],&#xA;    torch_dtype=torch.float16,&#xA;)&#xA;&#xA;# If the loaded model is not LCM, merge LCM&#xA;stream.load_lcm_lora()&#xA;stream.fuse_lora()&#xA;# Use Tiny VAE for further acceleration&#xA;stream.vae = AutoencoderTiny.from_pretrained(&#34;madebyollin/taesd&#34;).to(device=pipe.device, dtype=pipe.dtype)&#xA;# Enable acceleration&#xA;pipe.enable_xformers_memory_efficient_attention()&#xA;&#xA;&#xA;prompt = &#34;1girl with dog hair, thick frame glasses&#34;&#xA;# Prepare the stream&#xA;stream.prepare(prompt)&#xA;&#xA;# Prepare image&#xA;init_image = load_image(&#34;assets/img2img_example.png&#34;).resize((512, 512))&#xA;&#xA;# Warmup &amp;gt;= len(t_index_list) x frame_buffer_size&#xA;for _ in range(2):&#xA;    stream(init_image)&#xA;&#xA;# Run the stream infinitely&#xA;while True:&#xA;    x_output = stream(init_image)&#xA;    postprocess_image(x_output, output_type=&#34;pil&#34;)[0].show()&#xA;    input_response = input(&#34;Press Enter to continue or type &#39;stop&#39; to exit: &#34;)&#xA;    if input_response == &#34;stop&#34;:&#xA;        break&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Text-to-Image&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from diffusers import AutoencoderTiny, StableDiffusionPipeline&#xA;&#xA;from streamdiffusion import StreamDiffusion&#xA;from streamdiffusion.image_utils import postprocess_image&#xA;&#xA;# You can load any models using diffuser&#39;s StableDiffusionPipeline&#xA;pipe = StableDiffusionPipeline.from_pretrained(&#34;KBlueLeaf/kohaku-v2.1&#34;).to(&#xA;    device=torch.device(&#34;cuda&#34;),&#xA;    dtype=torch.float16,&#xA;)&#xA;&#xA;# Wrap the pipeline in StreamDiffusion&#xA;# Requires more long steps (len(t_index_list)) in text2image&#xA;# You recommend to use cfg_type=&#34;none&#34; when text2image&#xA;stream = StreamDiffusion(&#xA;    pipe,&#xA;    t_index_list=[0, 16, 32, 45],&#xA;    torch_dtype=torch.float16,&#xA;    cfg_type=&#34;none&#34;,&#xA;)&#xA;&#xA;# If the loaded model is not LCM, merge LCM&#xA;stream.load_lcm_lora()&#xA;stream.fuse_lora()&#xA;# Use Tiny VAE for further acceleration&#xA;stream.vae = AutoencoderTiny.from_pretrained(&#34;madebyollin/taesd&#34;).to(device=pipe.device, dtype=pipe.dtype)&#xA;# Enable acceleration&#xA;pipe.enable_xformers_memory_efficient_attention()&#xA;&#xA;&#xA;prompt = &#34;1girl with dog hair, thick frame glasses&#34;&#xA;# Prepare the stream&#xA;stream.prepare(prompt)&#xA;&#xA;# Warmup &amp;gt;= len(t_index_list) x frame_buffer_size&#xA;for _ in range(4):&#xA;    stream()&#xA;&#xA;# Run the stream infinitely&#xA;while True:&#xA;    x_output = stream.txt2img()&#xA;    postprocess_image(x_output, output_type=&#34;pil&#34;)[0].show()&#xA;    input_response = input(&#34;Press Enter to continue or type &#39;stop&#39; to exit: &#34;)&#xA;    if input_response == &#34;stop&#34;:&#xA;        break&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can make it faster by using SD-Turbo.&lt;/p&gt; &#xA;&lt;h3&gt;Faster generation&lt;/h3&gt; &#xA;&lt;p&gt;Replace the following code in the above example.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pipe.enable_xformers_memory_efficient_attention()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from streamdiffusion.acceleration.tensorrt import accelerate_with_tensorrt&#xA;&#xA;stream = accelerate_with_tensorrt(&#xA;    stream, &#34;engines&#34;, max_batch_size=2,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It requires TensorRT extension and time to build the engine, but it will be faster than the above example.&lt;/p&gt; &#xA;&lt;h2&gt;Optionals&lt;/h2&gt; &#xA;&lt;h3&gt;Stochastic Similarity Filter&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/demo_06.gif&#34; alt=&#34;demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Stochastic Similarity Filter reduces processing during video input by minimizing conversion operations when there is little change from the previous frame, thereby alleviating GPU processing load, as shown by the red frame in the above GIF. The usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stream = StreamDiffusion(&#xA;    pipe,&#xA;    [32, 45],&#xA;    torch_dtype=torch.float16,&#xA;)&#xA;stream.enable_similar_image_filter(&#xA;    similar_image_filter_threshold,&#xA;    similar_image_filter_max_skip_frame,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are the following parameters that can be set as arguments in the function:&lt;/p&gt; &#xA;&lt;h4&gt;&lt;code&gt;similar_image_filter_threshold&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The threshold for similarity between the previous frame and the current frame before the processing is paused.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;&lt;code&gt;similar_image_filter_max_skip_frame&lt;/code&gt;&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The maximum interval during the pause before resuming the conversion.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Residual CFG (RCFG)&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cumulo-autumn/StreamDiffusion/main/assets/cfg_conparision.png&#34; alt=&#34;rcfg&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;RCFG is a method for approximately realizing CFG with competitive computational complexity compared to cases where CFG is not used. It can be specified through the cfg_type argument in the StreamDiffusion. There are two types of RCFG: one with no specified items for negative prompts RCFG Self-Negative and one where negative prompts can be specified RCFG Onetime-Negative. In terms of computational complexity, denoting the complexity without CFG as N and the complexity with a regular CFG as 2N, RCFG Self-Negative can be computed in N steps, while RCFG Onetime-Negative can be computed in N+1 steps.&lt;/p&gt; &#xA;&lt;p&gt;The usage is as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# w/0 CFG&#xA;cfg_type = &#34;none&#34;&#xA;# CFG&#xA;cfg_type = &#34;full&#34;&#xA;# RCFG Self-Negative&#xA;cfg_type = &#34;self&#34;&#xA;# RCFG Onetime-Negative&#xA;cfg_type = &#34;initialize&#34;&#xA;stream = StreamDiffusion(&#xA;    pipe,&#xA;    [32, 45],&#xA;    torch_dtype=torch.float16,&#xA;    cfg_type=cfg_type,&#xA;)&#xA;stream.prepare(&#xA;    prompt=&#34;1girl, purple hair&#34;,&#xA;    guidance_scale=guidance_scale,&#xA;    delta=delta,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The delta has a moderating effect on the effectiveness of RCFG.&lt;/p&gt; &#xA;&lt;h2&gt;Development Team&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/cumulo_autumn&#34;&gt;Aki&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/AttaQjp&#34;&gt;Ararat&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/Chenfeng_X&#34;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/ddPn08&#34;&gt;ddPn08&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/ArtengMimi&#34;&gt;kizamimi&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/__ramu0e__&#34;&gt;ramune&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/hanyingcl&#34;&gt;teftef&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/toni_nimono&#34;&gt;Tonimono&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/IMG_5955&#34;&gt;Verb&lt;/a&gt;,&lt;/p&gt; &#xA;&lt;p&gt;(*alphabetical order) &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The video and image demos in this GitHub repository were generated using &lt;a href=&#34;https://huggingface.co/latent-consistency/lcm-lora-sdv1-5&#34;&gt;LCM-LoRA&lt;/a&gt; + &lt;a href=&#34;https://civitai.com/models/136268/kohaku-v2&#34;&gt;KohakuV2&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2311.17042&#34;&gt;SD-Turbo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to &lt;a href=&#34;https://latent-consistency-models.github.io/&#34;&gt;LCM-LoRA authors&lt;/a&gt; for providing the LCM-LoRA and Kohaku BlueLeaf (&lt;a href=&#34;https://twitter.com/KBlueleaf&#34;&gt;@KBlueleaf&lt;/a&gt;) for providing the KohakuV2 model and ,to &lt;a href=&#34;https://ja.stability.ai/&#34;&gt;Stability AI&lt;/a&gt; for &lt;a href=&#34;https://arxiv.org/abs/2311.17042&#34;&gt;SD-Turbo&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;KohakuV2 Models can be downloaded from &lt;a href=&#34;https://civitai.com/models/136268/kohaku-v2&#34;&gt;Civitai&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/KBlueLeaf/kohaku-v2.1&#34;&gt;Hugging Face&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;SD-Turbo is also available on &lt;a href=&#34;https://huggingface.co/stabilityai/sd-turbo&#34;&gt;Hugging Face Space&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/cumulo-autumn/StreamDiffusion/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=cumulo-autumn/StreamDiffusion&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>SoftFever/OrcaSlicer</title>
    <updated>2023-12-25T01:25:33Z</updated>
    <id>tag:github.com,2023-12-25:/SoftFever/OrcaSlicer</id>
    <link href="https://github.com/SoftFever/OrcaSlicer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;G-code generator for 3D printers (Bambu, Prusa, Voron, VzBot, RatRig, Creality, etc.)&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://github.com/SoftFever/OrcaSlicer/actions/workflows/build_all.yml&#34;&gt;&lt;img src=&#34;https://github.com/SoftFever/OrcaSlicer/actions/workflows/build_all.yml/badge.svg?branch=main&#34; alt=&#34;Build all&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Orca Slicer&lt;/h1&gt; &#xA;&lt;p&gt;Orca Slicer is an open source slicer for FDM printers.&lt;br&gt; You can download Orca Slicer here: &lt;a href=&#34;https://github.com/SoftFever/OrcaSlicer/releases/&#34;&gt;github releases page&lt;/a&gt;.&lt;br&gt; &lt;img src=&#34;https://github.com/SoftFever/OrcaSlicer/assets/103989404/b97d5ffc-072d-4d0a-bbda-e67ef373876f&#34; alt=&#34;discord-mark-blue&#34;&gt; Join community: &lt;a href=&#34;https://discord.gg/P4VE9UY9gJ&#34;&gt;OrcaSlicer Official Discord Server&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Main features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Auto calibrations for all printers&lt;/li&gt; &#xA; &lt;li&gt;Sandwich(inner-outer-inner) mode - an improved version of the &lt;code&gt;External perimeters first&lt;/code&gt; mode&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/SoftFever/OrcaSlicer/wiki/Precise-wall&#34;&gt;Precise wall&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Polyholes conversion support &lt;a href=&#34;https://github.com/supermerill/SuperSlicer/wiki/Polyholes&#34;&gt;SuperSlicer Wiki: Polyholes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Klipper support&lt;/li&gt; &#xA; &lt;li&gt;More granular controls&lt;/li&gt; &#xA; &lt;li&gt;More features can be found in &lt;a href=&#34;https://github.com/SoftFever/OrcaSlicer/releases/&#34;&gt;change notes&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Some background&lt;/h3&gt; &#xA;&lt;p&gt;OrcaSlicer is fork of Bambu Studio&lt;br&gt; It was previously known as BambuStudio-SoftFever&lt;br&gt; Bambu Studio is forked from &lt;a href=&#34;https://github.com/prusa3d/PrusaSlicer&#34;&gt;PrusaSlicer&lt;/a&gt; by Prusa Research, which is from &lt;a href=&#34;https://github.com/Slic3r/Slic3r&#34;&gt;Slic3r&lt;/a&gt; by Alessandro Ranellucci and the RepRap community. Orca Slicer incorporates a lot of features from SuperSlicer by @supermerill Orca Slicer&#39;s logo is designed by community member Justin Levine(@freejstnalxndr)&lt;/p&gt; &#xA;&lt;h1&gt;How to install&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Windows&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install and run &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;em&gt;If you have troubles to run the build, you might need to install following runtimes:&lt;/em&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/SoftFever/BambuStudio-SoftFever/releases/download/v1.0.10-sf2/MicrosoftEdgeWebView2RuntimeInstallerX64.exe&#34;&gt;MicrosoftEdgeWebView2RuntimeInstallerX64&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://github.com/SoftFever/BambuStudio-SoftFever/releases/download/v1.0.10-sf2/vcredist2019_x64.exe&#34;&gt;vcredist2019_x64&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Mac&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the DMG for your computer: &lt;code&gt;arm64&lt;/code&gt; version for Apple Silicon and &lt;code&gt;x86_64&lt;/code&gt; for Intel CPU.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Drag OrcaSlicer.app to Application folder.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;em&gt;If you want to run a build from a PR, you also need following instructions bellow&lt;/em&gt;&lt;/p&gt; &#xA;  &lt;details quarantine&gt;&#xA;    - Option 1 (You only need to do this once. After that the app can be opened normally.): - Step 1: Hold _cmd_ and right click the app, from the context menu choose **Open**. - Step 2: A warning window will pop up, click _Open_ &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Option 2:&lt;br&gt; Execute this command in terminal: &lt;code&gt;xattr -dr com.apple.quarantine /Applications/OrcaSlicer.app&lt;/code&gt; &lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;    softfever@mac:~$ xattr -dr com.apple.quarantine /Applications/OrcaSlicer.app&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Option 3: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Step 1: open the app, a warning window will pop up&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SoftFever/OrcaSlicer/main/SoftFever_doc/mac_cant_open.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA;      &lt;li&gt;Step 2: in &lt;code&gt;System Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Privacy &amp;amp; Security&lt;/code&gt;, click &lt;code&gt;Open Anyway&lt;/code&gt;:&lt;br&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SoftFever/OrcaSlicer/main/SoftFever_doc/mac_security_setting.png&#34; alt=&#34;image&#34;&gt;&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Linux(Ubuntu)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;If you run into trouble to execute it, try this command in terminal:&lt;br&gt; &lt;code&gt;chmod +x /path_to_appimage/OrcaSlicer_ubu64.AppImage&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;How to compile&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Windows 64-bit&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Tools needed: Visual Studio 2019, Cmake, git, Strawberry Perl.&lt;/li&gt; &#xA;   &lt;li&gt;Run &lt;code&gt;build_release.bat&lt;/code&gt; in &lt;code&gt;x64 Native Tools Command Prompt for VS 2019&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Mac 64-bit&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Tools needed: Xcode, Cmake, git, gettext, libtool, automake, autoconf&lt;/li&gt; &#xA;   &lt;li&gt;run &lt;code&gt;build_release_macos.sh&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ubuntu&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;run &#39;sudo ./BuildLinux.sh -u&#39;&lt;/li&gt; &#xA;   &lt;li&gt;run &#39;./BuildLinux.sh -dsir&#39;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Note:&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;re running Klipper, it&#39;s recommended to add the following configuration to your &lt;code&gt;printer.cfg&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Enable object exclusion&#xA;[exclude_object]&#xA;&#xA;# Enable arcs support&#xA;[gcode_arcs]&#xA;resolution: 0.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Supports&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Orca Slicer&lt;/strong&gt; is an open-source project, and I&#39;m deeply grateful to all my sponsors and backers.&lt;br&gt; Their generous support enables me to purchase filaments and other essential 3D printing materials for the project.&lt;br&gt; Thank you! :)&lt;/p&gt; &#xA;&lt;h3&gt;Sponsors:&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://peopoly.net/&#34;&gt; &lt;img src=&#34;SoftFever_doc\sponsor_logos\peopoly-standard-logo.png&#34; alt=&#34;Peopoly&#34; width=&#34;64&#34; height=&#34;&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt; &lt;a href=&#34;https://qidi3d.com/&#34;&gt; &lt;img src=&#34;SoftFever_doc\sponsor_logos\QIDI.png&#34; alt=&#34;QIDI&#34; width=&#34;64&#34; height=&#34;&#34;&gt; &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Backers:&lt;/h3&gt; &#xA;&lt;p&gt;Ko-fi supporters: &lt;a href=&#34;https://github.com/SoftFever/OrcaSlicer/wiki/OrcaSlicer-backers-%E2%80%90-28-Oct-2023&#34;&gt;Backers list&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Support me&lt;br&gt; &lt;a href=&#34;https://ko-fi.com/G2G5IP3CP&#34;&gt;&lt;img src=&#34;https://ko-fi.com/img/githubbutton_sm.svg?sanitize=true&#34; alt=&#34;ko-fi&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Orca Slicer is licensed under the GNU Affero General Public License, version 3. Orca Slicer is based on Bambu Studio by BambuLab.&lt;/p&gt; &#xA;&lt;p&gt;Bambu Studio is licensed under the GNU Affero General Public License, version 3. Bambu Studio is based on PrusaSlicer by PrusaResearch.&lt;/p&gt; &#xA;&lt;p&gt;PrusaSlicer is licensed under the GNU Affero General Public License, version 3. PrusaSlicer is owned by Prusa Research. PrusaSlicer is originally based on Slic3r by Alessandro Ranellucci.&lt;/p&gt; &#xA;&lt;p&gt;Slic3r is licensed under the GNU Affero General Public License, version 3. Slic3r was created by Alessandro Ranellucci with the help of many other contributors.&lt;/p&gt; &#xA;&lt;p&gt;The GNU Affero General Public License, version 3 ensures that if you use any part of this software in any way (even behind a web server), your software must be released under the same license.&lt;/p&gt; &#xA;&lt;p&gt;Orca Slicer includes a pressure advance calibration pattern test adapted from Andrew Ellis&#39; generator, which is licensed under GNU General Public License, version 3. Ellis&#39; generator is itself adapted from a generator developed by Sineos for Marlin, which is licensed under GNU General Public License, version 3.&lt;/p&gt; &#xA;&lt;p&gt;The bambu networking plugin is based on non-free libraries from Bambulab. It is optional to the Orca Slicer and provides extended functionalities for Bambulab printer users.&lt;/p&gt;</summary>
  </entry>
</feed>