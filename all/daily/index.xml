<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-11T01:30:55Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>umami-software/umami</title>
    <updated>2025-08-11T01:30:55Z</updated>
    <id>tag:github.com,2025-08-11:/umami-software/umami</id>
    <link href="https://github.com/umami-software/umami" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Umami is a modern, privacy-focused alternative to Google Analytics.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://content.umami.is/website/images/umami-logo.png&#34; alt=&#34;Umami Logo&#34; width=&#34;100&#34; /&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Umami&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;i&gt;Umami is a simple, fast, privacy-focused alternative to Google Analytics.&lt;/i&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/umami-software/umami/releases&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/release/umami-software/umami.svg?sanitize=true&#34; alt=&#34;GitHub Release&#34; /&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/umami-software/umami/raw/master/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/umami-software/umami.svg?sanitize=true&#34; alt=&#34;MIT License&#34; /&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/umami-software/umami/actions&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/umami-software/umami/ci.yml&#34; alt=&#34;Build Status&#34; /&gt; &lt;/a&gt; &lt;a href=&#34;https://analytics.umami.is/share/LGazGOecbDtaIwDr/umami.is&#34; style=&#34;text-decoration: none;&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Try%20Demo%20Now-Click%20Here-brightgreen&#34; alt=&#34;Umami Demo&#34; /&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;A detailed getting started guide can be found at &lt;a href=&#34;https://umami.is/docs/&#34;&gt;umami.is/docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;üõ† Installing from Source&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A server with Node.js version 18.18 or newer&lt;/li&gt; &#xA; &lt;li&gt;A database. Umami supports &lt;a href=&#34;https://www.mariadb.org/&#34;&gt;MariaDB&lt;/a&gt; (minimum v10.5), &lt;a href=&#34;https://www.mysql.com/&#34;&gt;MySQL&lt;/a&gt; (minimum v8.0) and &lt;a href=&#34;https://www.postgresql.org/&#34;&gt;PostgreSQL&lt;/a&gt; (minimum v12.14) databases.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Get the Source Code and Install Packages&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/umami-software/umami.git&#xA;cd umami&#xA;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Configure Umami&lt;/h3&gt; &#xA;&lt;p&gt;Create an &lt;code&gt;.env&lt;/code&gt; file with the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;DATABASE_URL=connection-url&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The connection URL format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;postgresql://username:mypassword@localhost:5432/mydb&#xA;mysql://username:mypassword@localhost:3306/mydb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build the Application&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;The build step will create tables in your database if you are installing for the first time. It will also create a login user with username &lt;strong&gt;admin&lt;/strong&gt; and password &lt;strong&gt;umami&lt;/strong&gt;.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Start the Application&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;By default, this will launch the application on &lt;code&gt;http://localhost:3000&lt;/code&gt;. You will need to either &lt;a href=&#34;https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/&#34;&gt;proxy&lt;/a&gt; requests from your web server or change the &lt;a href=&#34;https://nextjs.org/docs/api-reference/cli#production&#34;&gt;port&lt;/a&gt; to serve the application directly.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;üê≥ Installing with Docker&lt;/h2&gt; &#xA;&lt;p&gt;To build the Umami container and start up a Postgres database, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, to pull just the Umami Docker image with PostgreSQL support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull docker.umami.is/umami-software/umami:postgresql-latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or with MySQL support:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker pull docker.umami.is/umami-software/umami:mysql-latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;üîÑ Getting Updates&lt;/h2&gt; &#xA;&lt;p&gt;To get the latest features, simply do a pull, install any new dependencies, and rebuild:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git pull&#xA;npm install&#xA;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To update the Docker image, simply pull the new images and rebuild:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose pull&#xA;docker compose up --force-recreate -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr /&gt; &#xA;&lt;h2&gt;üõü Support&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/umami-software/umami&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/GitHub--blue?style=social&amp;amp;logo=github&#34; alt=&#34;GitHub&#34; /&gt; &lt;/a&gt; &lt;a href=&#34;https://twitter.com/umami_software&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Twitter--blue?style=social&amp;amp;logo=twitter&#34; alt=&#34;Twitter&#34; /&gt; &lt;/a&gt; &lt;a href=&#34;https://linkedin.com/company/umami-software&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/LinkedIn--blue?style=social&amp;amp;logo=linkedin&#34; alt=&#34;LinkedIn&#34; /&gt; &lt;/a&gt; &lt;a href=&#34;https://umami.is/discord&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Discord--blue?style=social&amp;amp;logo=discord&#34; alt=&#34;Discord&#34; /&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>idosal/git-mcp</title>
    <updated>2025-08-11T01:30:55Z</updated>
    <id>tag:github.com,2025-08-11:/idosal/git-mcp</id>
    <link href="https://github.com/idosal/git-mcp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Put an end to code hallucinations! GitMCP is a free, open-source, remote MCP server for any GitHub project&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GitMCP&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img width=&#34;884&#34; alt=&#34;image&#34; src=&#34;https://github.com/user-attachments/assets/2bf3e3df-556c-49c6-ab7b-36c279d53bba&#34; /&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-what-is-gitmcp&#34;&gt;What is GitMCP&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-features&#34;&gt;Features&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-getting-started&#34;&gt;Getting Started&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-how-it-works&#34;&gt;How It Works&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-badge&#34;&gt;Badge&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-examples&#34;&gt;Examples&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-faq&#34;&gt;FAQ&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-privacy&#34;&gt;Privacy&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-contributing&#34;&gt;Contributing&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/#-license&#34;&gt;License&lt;/a&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://gitmcp.io/idosal/git-mcp&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://gitmcp.io/badge/idosal/git-mcp&#34; alt=&#34;GitMCP&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/idosal1&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/idosal1?style=social&#34; alt=&#34;Twitter Follow&#34; /&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/liadyosef&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/liadyosef?style=social&#34; alt=&#34;Twitter Follow&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://www.pulsemcp.com/servers/idosal-git-mcp&#34;&gt;&lt;img src=&#34;https://www.pulsemcp.com/badge/top-pick/idosal-git-mcp&#34; width=&#34;400&#34; alt=&#34;Pulse MCP Badge&#34; /&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ü§î What is GitMCP?&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Stop vibe-hallucinating and start vibe-coding!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://gitmcp.io&#34;&gt;GitMCP&lt;/a&gt; is a free, open-source, remote &lt;a href=&#34;https://docs.anthropic.com/en/docs/agents-and-tools/mcp&#34;&gt;Model Context Protocol (MCP)&lt;/a&gt; server that transforms &lt;strong&gt;any&lt;/strong&gt; GitHub project (repositories or GitHub pages) into a documentation hub. It enables AI tools like Cursor to access up-to-date documentation and code, even if the LLM has never encountered them, thereby eliminating code hallucinations seamlessly.&lt;/p&gt; &#xA;&lt;p&gt;GitMCP supports &lt;strong&gt;two flavors&lt;/strong&gt; -&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Specific Repository (&lt;code&gt;gitmcp.io/{owner}/{repo}&lt;/code&gt; or &lt;code&gt;{owner}.gitmcp.io/{repo}&lt;/code&gt;):&lt;/strong&gt; Use these when you primarily work with a select number of libraries. This ensures your AI assistant always targets the correct project, enhancing security and relevance by preventing access to unintended repositories.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Generic Server (&lt;code&gt;gitmcp.io/docs&lt;/code&gt;):&lt;/strong&gt; Use this for maximum flexibility when you need to switch between different repositories frequently. The AI assistant will prompt you (or decide based on context) which repository to access for each request. Be mindful that this relies on correctly identifying the target repository each time.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;With GitMCP:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;AI assistants access the &lt;em&gt;latest&lt;/em&gt; documentation and code directly from the source.&lt;/li&gt; &#xA; &lt;li&gt;Get accurate API usage and reliable code examples.&lt;/li&gt; &#xA; &lt;li&gt;Work effectively even with niche, new, or rapidly changing libraries.&lt;/li&gt; &#xA; &lt;li&gt;Significantly reduced hallucinations and improved code correctness.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For example, this side-by-side comparison shows the result for the same one-shot prompt in Cursor when creating a &lt;a href=&#34;https://github.com/mrdoob/three.js&#34;&gt;three.js&lt;/a&gt; scene -&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/fbf1b4a7-f9f0-4c0e-831c-4d64faae2c45&#34;&gt;https://github.com/user-attachments/assets/fbf1b4a7-f9f0-4c0e-831c-4d64faae2c45&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;‚ú® Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üòé &lt;strong&gt;Latest Documentation on ANY GitHub Project&lt;/strong&gt;: Grant your AI assistant seamless access to the GitHub project&#39;s documentation and code. The built-in smart search capabilities help find exactly what the AI needs without using too many tokens!&lt;/li&gt; &#xA; &lt;li&gt;üß† &lt;strong&gt;No More Hallucinations&lt;/strong&gt;: With GitMCP, your AI assistant can provide accurate and relevant answers to your questions.&lt;/li&gt; &#xA; &lt;li&gt;‚òÅÔ∏è &lt;strong&gt;Zero Setup&lt;/strong&gt;: GitMCP runs in the cloud. Simply add the chosen GitMCP URL as an MCP server in your IDE ‚Äî no downloads, installations, signups, or changes are required.&lt;/li&gt; &#xA; &lt;li&gt;üí¨ &lt;strong&gt;Embedded Chat&lt;/strong&gt;: Start quickly by chatting directly with the repository&#39;s documentation through our in-browser chat!&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;Open, Free, and Private&lt;/strong&gt;: GitMCP is open-source and completely free to use. It doesn&#39;t collect personal information or store queries. You can even self-host it!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&#xA; &lt;video src=&#34;https://github.com/user-attachments/assets/2c3afaf9-6c08-436e-9efd-db8710554430&#34;&gt;&lt;/video&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;Using GitMCP is easy! Simply follow these steps:&lt;/p&gt; &#xA;&lt;h3&gt;Step 1: Choose the type of server you want&lt;/h3&gt; &#xA;&lt;p&gt;Choose one of these URL formats depending on what you want to connect to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For GitHub repositories: &lt;code&gt;gitmcp.io/{owner}/{repo}&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;For GitHub Pages sites: &lt;code&gt;{owner}.gitmcp.io/{repo}&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;For a generic tool that supports any repository (dynamic): &lt;code&gt;gitmcp.io/docs&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;{owner}&lt;/code&gt; with the GitHub username or organization name, and &lt;code&gt;{repo}&lt;/code&gt; with the repository name.&lt;/p&gt; &#xA;&lt;p&gt;For your convenience, you can also use the conversion tool on the landing page to format the GitHub URL into an MCP URL!&lt;/p&gt; &#xA;&lt;h3&gt;Step 2: Connect your AI assistant&lt;/h3&gt; &#xA;&lt;p&gt;Select your AI assistant from the options below and follow the configuration instructions:&lt;/p&gt; &#xA;&lt;h4&gt;Connecting Cursor&lt;/h4&gt; &#xA;&lt;p&gt;Update your Cursor configuration file at &lt;code&gt;~/.cursor/mcp.json&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;mcpServers&#34;: {&#xA;    &#34;gitmcp&#34;: {&#xA;      &#34;url&#34;: &#34;https://gitmcp.io/{owner}/{repo}&#34;&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Connecting Claude Desktop&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;In Claude Desktop, go to Settings &amp;gt; Developer &amp;gt; Edit Config&lt;/li&gt; &#xA; &lt;li&gt;Replace the configuration with: &lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;mcpServers&#34;: {&#xA;    &#34;gitmcp&#34;: {&#xA;      &#34;command&#34;: &#34;npx&#34;,&#xA;      &#34;args&#34;: [&#xA;        &#34;mcp-remote&#34;,&#xA;        &#34;https://gitmcp.io/{owner}/{repo}&#34;&#xA;      ]&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Connecting Windsurf&lt;/h4&gt; &#xA;&lt;p&gt;Update your Windsurf configuration file at &lt;code&gt;~/.codeium/windsurf/mcp_config.json&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;mcpServers&#34;: {&#xA;    &#34;gitmcp&#34;: {&#xA;      &#34;serverUrl&#34;: &#34;https://gitmcp.io/{owner}/{repo}&#34;&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Connecting VSCode&lt;/h4&gt; &#xA;&lt;p&gt;Update your VSCode configuration file at &lt;code&gt;.vscode/mcp.json&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;servers&#34;: {&#xA;    &#34;gitmcp&#34;: {&#xA;      &#34;type&#34;: &#34;sse&#34;,&#xA;      &#34;url&#34;: &#34;https://gitmcp.io/{owner}/{repo}&#34;&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Connecting Cline&lt;/h4&gt; &#xA;&lt;p&gt;Update your Cline configuration file at &lt;code&gt;~/Library/Application Support/Code/User/globalStorage/saoudrizwan.claude-dev/settings/cline_mcp_settings.json&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;mcpServers&#34;: {&#xA;    &#34;gitmcp&#34;: {&#xA;      &#34;url&#34;: &#34;https://gitmcp.io/{owner}/{repo}&#34;,&#xA;      &#34;disabled&#34;: false,&#xA;      &#34;autoApprove&#34;: []&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Connecting Highlight AI&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open Highlight AI and click the plugins icon (@ symbol) in the sidebar&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;Installed Plugins&lt;/strong&gt; at the top of the sidebar&lt;/li&gt; &#xA; &lt;li&gt;Select &lt;strong&gt;Custom Plugin&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;Click &lt;strong&gt;Add a plugin using a custom SSE URL&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Plugin name: &lt;code&gt;gitmcp&lt;/code&gt; SSE URL: &lt;code&gt;https://gitmcp.io/{owner}/{repo}&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more details on adding custom MCP servers to HighlightAI, refer to &lt;a href=&#34;https://docs.highlightai.com/learn/developers/plugins/custom-plugins-setup&#34;&gt;the documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Connecting Augment Code&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open Augment Code settings&lt;/li&gt; &#xA; &lt;li&gt;Navigate to the MCP section&lt;/li&gt; &#xA; &lt;li&gt;Add a new MCP server with the following details:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Name the MCP server: &lt;code&gt;git-mcp Docs&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Use this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx mcp-remote https://gitmcp.io/{owner}/{repo}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use the following configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;mcpServers&#34;: {&#xA;    &#34;git-mcp Docs&#34;: {&#xA;      &#34;command&#34;: &#34;npx&#34;,&#xA;      &#34;args&#34;: [&#xA;        &#34;mcp-remote&#34;,&#xA;        &#34;https://gitmcp.io/{owner}/{repo}&#34;&#xA;      ]&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Connecting Msty AI&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open Msty Studio&lt;/li&gt; &#xA; &lt;li&gt;Go to Tools &amp;gt; Import Tools from JSON Clipboard&lt;/li&gt; &#xA; &lt;li&gt;Paste the following configuration:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;mcpServers&#34;: {&#xA;    &#34;git-mcp Docs&#34;: {&#xA;      &#34;command&#34;: &#34;npx&#34;,&#xA;      &#34;args&#34;: [&#xA;        &#34;mcp-remote&#34;,&#xA;        &#34;https://gitmcp.io/{owner}/{repo}&#34;&#xA;      ]&#xA;    }&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details on configuring MCP servers in Augment Code, visit &lt;a href=&#34;https://docs.augmentcode.com/setup-augment/mcp&#34;&gt;the Augment Code documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Remember to replace &lt;code&gt;{owner}&lt;/code&gt; and &lt;code&gt;{repo}&lt;/code&gt; with the actual GitHub username/organization and repository name. You can also use the dynamic endpoint &lt;code&gt;https://gitmcp.io/docs&lt;/code&gt; to allow your AI to access any repository on demand.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;‚öô How It Works&lt;/h2&gt; &#xA;&lt;p&gt;GitMCP connects your AI assistant to GitHub repositories using the Model Context Protocol (MCP), a standard that lets AI tools request additional information from external sources.&lt;/p&gt; &#xA;&lt;p&gt;What happens when you use GitMCP:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;You provide the GitMCP URL&lt;/strong&gt; to your AI assistant (e.g., &lt;code&gt;gitmcp.io/microsoft/typescript&lt;/code&gt;). GitMCP exposes tools like documentation fetching, smart search, code search, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt the AI assistant&lt;/strong&gt; on documentation/code-related questions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Your AI sends requests&lt;/strong&gt; to GitMCP to use its tools (with your approval).&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GitMCP executes the AI&#39;s request&lt;/strong&gt; and returns the requested data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Your AI receives the information&lt;/strong&gt; and generates a more accurate, grounded response without hallucinations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Supported Documentation&lt;/h3&gt; &#xA;&lt;p&gt;GitMCP currently supports the following documents (in order of priority):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://llmstxt.org&#34;&gt;llms.txt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AI-optimized version of the project&#39;s documentation&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;README.md&lt;/code&gt;/root&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üí° Examples&lt;/h2&gt; &#xA;&lt;p&gt;Here are some examples of how to use GitMCP with different AI assistants and repositories:&lt;/p&gt; &#xA;&lt;h3&gt;Example 1: Using Windsurf with a specific repository&lt;/h3&gt; &#xA;&lt;p&gt;For the GitHub repository &lt;code&gt;https://github.com/microsoft/playwright-mcp&lt;/code&gt;, add &lt;code&gt;https://gitmcp.io/microsoft/playwright-mcp&lt;/code&gt; as an MCP server to Windsurf.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt to Claude:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;How do I use the Playwright MCP&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Windsurf will pull the relevant documentation from GitMCP to implement the memory feature correctly.&lt;/p&gt; &#xA;&lt;h3&gt;Example 2: Using Cursor with a GitHub Pages site&lt;/h3&gt; &#xA;&lt;p&gt;For the GitHub Pages site &lt;code&gt;langchain-ai.github.io/langgraph&lt;/code&gt;, add &lt;code&gt;https://langchain-ai.gitmcp.io/langgraph&lt;/code&gt; as an MCP server to Cursor.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt to Cursor:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;Add memory to my LangGraph agent&#34;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Cursor will pull the relevant documentation and code from GitMCP to correctly implement the memory feature.&lt;/p&gt; &#xA;&lt;h3&gt;Example 3: Using Claude Desktop with the dynamic endpoint&lt;/h3&gt; &#xA;&lt;p&gt;You don&#39;t have to pick specific repositories. The generic &lt;code&gt;gitmcp.io/docs&lt;/code&gt; endpoint allows AI to pick the GitHub project on the fly!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prompt to any AI assistant:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&#34;I want to learn about the OpenAI Whisper speech recognition model. Explain how it works.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Claude will pull the data from GitMCP and answer the question.&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Tools&lt;/h2&gt; &#xA;&lt;p&gt;GitMCP provides AI assistants with several valuable tools to help them access, understand, and query GitHub repositories.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;fetch_&amp;lt;repo-name&amp;gt;_documentation&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This tool gets the primary documentation from a GitHub repository. It works by retrieving relevant documentation (e.g., &lt;code&gt;llms.txt&lt;/code&gt;). This gives the AI a good overview of what the project is about&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;When it&#39;s useful:&lt;/strong&gt; For general questions about a project&#39;s purpose, features, or how to get started&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;search_&amp;lt;repo-name&amp;gt;_documentation&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This tool lets the AI search through a repository&#39;s documentation by providing a specific search query. Instead of loading all the documentation (which could be very large), it uses intelligent search to find just the relevant parts.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;When it&#39;s useful:&lt;/strong&gt; For specific questions about particular features, functions, or concepts within a project&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;fetch_url_content&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This tool helps the AI get information from links mentioned in the documentation. It retrieves the content from those links and converts it to a format the AI can easily read.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;When it&#39;s useful:&lt;/strong&gt; When documentation references external information that would help answer your question&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;search_&amp;lt;repo-name&amp;gt;_code&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This tool searches through the actual code in the repository using GitHub&#39;s code search. It helps AI find specific code examples or implementation details.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;When it&#39;s useful:&lt;/strong&gt; When you want examples of how something is implemented or need technical details not covered in documentation&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; When using the dynamic endpoint (&lt;code&gt;gitmcp.io/docs&lt;/code&gt;), these tools are named slightly differently (&lt;code&gt;fetch_generic_documentation&lt;/code&gt;, &lt;code&gt;search_generic_code&lt;/code&gt;, and &lt;code&gt;search_generic_documentation&lt;/code&gt;) and need additional information about which repository to access.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üìä Badge&lt;/h2&gt; &#xA;&lt;p&gt;GitMCP has a badge to your repository&#39;s README. It allows users to quickly access your documentation through their IDE or browser (using the embedded chat). It also showcases how many times your documentation has been accessed through GitMCP.&lt;/p&gt; &#xA;&lt;p&gt;Example (&lt;code&gt;idosal/git-mcp&lt;/code&gt;): &lt;a href=&#34;https://gitmcp.io/idosal/git-mcp&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https://gitmcp.io/badge/idosal/git-mcp&#34; alt=&#34;GitMCP&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Adding the Badge to Your Repository&lt;/h3&gt; &#xA;&lt;p&gt;Add the following to your &lt;code&gt;README.md&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;[![GitMCP](https://img.shields.io/endpoint?url=https://gitmcp.io/badge/OWNER/REPO)](https://gitmcp.io/OWNER/REPO)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;OWNER&lt;/code&gt; with your GitHub username or organization, and &lt;code&gt;REPO&lt;/code&gt; with your repository name.&lt;/p&gt; &#xA;&lt;h3&gt;How We Count Views&lt;/h3&gt; &#xA;&lt;p&gt;Increment for each tool call on the specific repository.&lt;/p&gt; &#xA;&lt;h3&gt;Customizing the Badge&lt;/h3&gt; &#xA;&lt;p&gt;You can customize the badge&#39;s appearance with parameters:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Parameter&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;   &lt;th&gt;Default&lt;/th&gt; &#xA;   &lt;th&gt;Example&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;color&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Color for the badge value&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;aquamarine&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;?color=green&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;label&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Badge label&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;GitMCP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;Documentation&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please reach out!&lt;/p&gt; &#xA;&lt;h2&gt;‚ùì FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;What is the Model Context Protocol?&lt;/h3&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://modelcontextprotocol.io/introduction&#34;&gt;Model Context Protocol&lt;/a&gt; is a standard that allows AI assistants to request and receive additional context from external sources in a structured manner, enhancing their understanding and performance.&lt;/p&gt; &#xA;&lt;h3&gt;Does GitMCP work with any AI assistant?&lt;/h3&gt; &#xA;&lt;p&gt;Yes, GitMCP is compatible with any AI assistant supporting the Model Context Protocol, including tools like Cursor, VSCode, Claude, etc.&lt;/p&gt; &#xA;&lt;h3&gt;Is GitMCP compatible with all GitHub projects?&lt;/h3&gt; &#xA;&lt;p&gt;Absolutely! GitMCP works with any public GitHub repository without requiring any modifications. It prioritizes the &lt;code&gt;llms.txt&lt;/code&gt; file and falls back to &lt;code&gt;README.md&lt;/code&gt; or other pages if the former is unavailable. Future updates aim to support additional documentation methods and even generate content dynamically.&lt;/p&gt; &#xA;&lt;h3&gt;Does GitMCP cost money?&lt;/h3&gt; &#xA;&lt;p&gt;No, GitMCP is a free service to the community with no associated costs.&lt;/p&gt; &#xA;&lt;h2&gt;üîí Privacy&lt;/h2&gt; &#xA;&lt;p&gt;GitMCP is deeply committed to its users&#39; privacy. The service doesn&#39;t have access to or store any personally identifiable information as it doesn&#39;t require authentication. In addition, it doesn&#39;t store any queries sent by the agents. Moreover, as GitMCP is an open-source project, it can be deployed independently in your environment.&lt;/p&gt; &#xA;&lt;p&gt;GitMCP only accesses content that is already publicly available and only when queried by a user. GitMCP does not automatically scrape repositories. Before accessing any GitHub Pages site, the code checks for &lt;code&gt;robots.txt&lt;/code&gt; rules and follows the directives set by site owners, allowing them to opt out. Please note that GitMCP doesn&#39;t permanently store data regarding the GitHub projects or their content.&lt;/p&gt; &#xA;&lt;h2&gt;üë• Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions, feedback, and ideas! Please review our &lt;a href=&#34;https://github.com/idosal/git-mcp/raw/main/.github/CONTRIBUTING.md&#34;&gt;contribution&lt;/a&gt; guidelines.&lt;/p&gt; &#xA;&lt;h3&gt;Local Development Setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/idosal/git-mcp.git&#xA;cd git-mcp&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pnpm install&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run locally for development&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev&#xA;# or&#xA;pnpm dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;Using MCP Inspector for Testing&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the MCP Inspector tool:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npx @modelcontextprotocol/inspector&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the inspector interface:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Set Transport Type to &lt;code&gt;SSE&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Enter your GitMCP URL (e.g., &lt;code&gt;http://localhost:5173/docs&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;Click &#34;Connect&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üìÑ License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/idosal/git-mcp/main/LICENSE&#34;&gt;Apache License 2.0&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;GitMCP is provided &#34;as is&#34; without warranty of any kind. While we strive to ensure the reliability and security of our service, we are not responsible for any damages or issues that may arise from its use. GitHub projects accessed through GitMCP are subject to their respective owners&#39; terms and conditions. GitMCP is not affiliated with GitHub or any of the mentioned AI tools.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#idosal/git-mcp&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=idosal/git-mcp&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34; /&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openai/openai-node</title>
    <updated>2025-08-11T01:30:55Z</updated>
    <id>tag:github.com,2025-08-11:/openai/openai-node</id>
    <link href="https://github.com/openai/openai-node" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official JavaScript / TypeScript library for the OpenAI API&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenAI TypeScript and JavaScript API Library&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://npmjs.org/package/openai&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/openai.svg?label=npm%20(stable)&#34; alt=&#34;NPM version&#34; /&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/bundlephobia/minzip/openai&#34; alt=&#34;npm bundle size&#34; /&gt; &lt;a href=&#34;https://jsr.io/@openai/openai&#34;&gt;&lt;img src=&#34;https://jsr.io/badges/@openai/openai&#34; alt=&#34;JSR Version&#34; /&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This library provides convenient access to the OpenAI REST API from TypeScript or JavaScript.&lt;/p&gt; &#xA;&lt;p&gt;It is generated from our &lt;a href=&#34;https://github.com/openai/openai-openapi&#34;&gt;OpenAPI specification&lt;/a&gt; with &lt;a href=&#34;https://stainlessapi.com/&#34;&gt;Stainless&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To learn how to use the OpenAI API, check out our &lt;a href=&#34;https://platform.openai.com/docs/api-reference&#34;&gt;API Reference&lt;/a&gt; and &lt;a href=&#34;https://platform.openai.com/docs&#34;&gt;Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm install openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Installation from JSR&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;deno add jsr:@openai/openai&#xA;npx jsr add @openai/openai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;These commands will make the module importable from the &lt;code&gt;@openai/openai&lt;/code&gt; scope. You can also &lt;a href=&#34;https://jsr.io/docs/using-packages#importing-with-jsr-specifiers&#34;&gt;import directly from JSR&lt;/a&gt; without an install step if you&#39;re using the Deno JavaScript runtime:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;jsr:@openai/openai&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;The full API of this library can be found in &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/api.md&#34;&gt;api.md file&lt;/a&gt; along with many &lt;a href=&#34;https://github.com/openai/openai-node/tree/master/examples&#34;&gt;code examples&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The primary API for interacting with OpenAI models is the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/responses&#34;&gt;Responses API&lt;/a&gt;. You can generate text from the model with the code below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  apiKey: process.env[&#39;OPENAI_API_KEY&#39;], // This is the default and can be omitted&#xA;});&#xA;&#xA;const response = await client.responses.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  instructions: &#39;You are a coding assistant that talks like a pirate&#39;,&#xA;  input: &#39;Are semicolons optional in JavaScript?&#39;,&#xA;});&#xA;&#xA;console.log(response.output_text);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The previous standard (supported indefinitely) for generating text is the &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat&#34;&gt;Chat Completions API&lt;/a&gt;. You can use that API to generate text from the model with the code below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  apiKey: process.env[&#39;OPENAI_API_KEY&#39;], // This is the default and can be omitted&#xA;});&#xA;&#xA;const completion = await client.chat.completions.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  messages: [&#xA;    { role: &#39;developer&#39;, content: &#39;Talk like a pirate.&#39; },&#xA;    { role: &#39;user&#39;, content: &#39;Are semicolons optional in JavaScript?&#39; },&#xA;  ],&#xA;});&#xA;&#xA;console.log(completion.choices[0].message.content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Streaming responses&lt;/h2&gt; &#xA;&lt;p&gt;We provide support for streaming responses using Server Sent Events (SSE).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI();&#xA;&#xA;const stream = await client.responses.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  input: &#39;Say &#34;Sheep sleep deep&#34; ten times fast!&#39;,&#xA;  stream: true,&#xA;});&#xA;&#xA;for await (const event of stream) {&#xA;  console.log(event);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;File uploads&lt;/h2&gt; &#xA;&lt;p&gt;Request parameters that correspond to file uploads can be passed in many different forms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;File&lt;/code&gt; (or an object with the same structure)&lt;/li&gt; &#xA; &lt;li&gt;a &lt;code&gt;fetch&lt;/code&gt; &lt;code&gt;Response&lt;/code&gt; (or an object with the same structure)&lt;/li&gt; &#xA; &lt;li&gt;an &lt;code&gt;fs.ReadStream&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;the return value of our &lt;code&gt;toFile&lt;/code&gt; helper&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import fs from &#39;fs&#39;;&#xA;import OpenAI, { toFile } from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI();&#xA;&#xA;// If you have access to Node `fs` we recommend using `fs.createReadStream()`:&#xA;await client.files.create({ file: fs.createReadStream(&#39;input.jsonl&#39;), purpose: &#39;fine-tune&#39; });&#xA;&#xA;// Or if you have the web `File` API you can pass a `File` instance:&#xA;await client.files.create({ file: new File([&#39;my bytes&#39;], &#39;input.jsonl&#39;), purpose: &#39;fine-tune&#39; });&#xA;&#xA;// You can also pass a `fetch` `Response`:&#xA;await client.files.create({ file: await fetch(&#39;https://somesite/input.jsonl&#39;), purpose: &#39;fine-tune&#39; });&#xA;&#xA;// Finally, if none of the above are convenient, you can use our `toFile` helper:&#xA;await client.files.create({&#xA;  file: await toFile(Buffer.from(&#39;my bytes&#39;), &#39;input.jsonl&#39;),&#xA;  purpose: &#39;fine-tune&#39;,&#xA;});&#xA;await client.files.create({&#xA;  file: await toFile(new Uint8Array([0, 1, 2]), &#39;input.jsonl&#39;),&#xA;  purpose: &#39;fine-tune&#39;,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Webhook Verification&lt;/h2&gt; &#xA;&lt;p&gt;Verifying webhook signatures is &lt;em&gt;optional but encouraged&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more information about webhooks, see &lt;a href=&#34;https://platform.openai.com/docs/guides/webhooks&#34;&gt;the API docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Parsing webhook payloads&lt;/h3&gt; &#xA;&lt;p&gt;For most use cases, you will likely want to verify the webhook and parse the payload at the same time. To achieve this, we provide the method &lt;code&gt;client.webhooks.unwrap()&lt;/code&gt;, which parses a webhook request and verifies that it was sent by OpenAI. This method will throw an error if the signature is invalid.&lt;/p&gt; &#xA;&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). The &lt;code&gt;.unwrap()&lt;/code&gt; method will parse this JSON for you into an event object after verifying the webhook was sent from OpenAI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { headers } from &#39;next/headers&#39;;&#xA;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  webhookSecret: process.env.OPENAI_WEBHOOK_SECRET, // env var used by default; explicit here.&#xA;});&#xA;&#xA;export async function webhook(request: Request) {&#xA;  const headersList = headers();&#xA;  const body = await request.text();&#xA;&#xA;  try {&#xA;    const event = client.webhooks.unwrap(body, headersList);&#xA;&#xA;    switch (event.type) {&#xA;      case &#39;response.completed&#39;:&#xA;        console.log(&#39;Response completed:&#39;, event.data);&#xA;        break;&#xA;      case &#39;response.failed&#39;:&#xA;        console.log(&#39;Response failed:&#39;, event.data);&#xA;        break;&#xA;      default:&#xA;        console.log(&#39;Unhandled event type:&#39;, event.type);&#xA;    }&#xA;&#xA;    return Response.json({ message: &#39;ok&#39; });&#xA;  } catch (error) {&#xA;    console.error(&#39;Invalid webhook signature:&#39;, error);&#xA;    return new Response(&#39;Invalid signature&#39;, { status: 400 });&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Verifying webhook payloads directly&lt;/h3&gt; &#xA;&lt;p&gt;In some cases, you may want to verify the webhook separately from parsing the payload. If you prefer to handle these steps separately, we provide the method &lt;code&gt;client.webhooks.verifySignature()&lt;/code&gt; to &lt;em&gt;only verify&lt;/em&gt; the signature of a webhook request. Like &lt;code&gt;.unwrap()&lt;/code&gt;, this method will throw an error if the signature is invalid.&lt;/p&gt; &#xA;&lt;p&gt;Note that the &lt;code&gt;body&lt;/code&gt; parameter must be the raw JSON string sent from the server (do not parse it first). You will then need to parse the body after verifying the signature.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { headers } from &#39;next/headers&#39;;&#xA;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  webhookSecret: process.env.OPENAI_WEBHOOK_SECRET, // env var used by default; explicit here.&#xA;});&#xA;&#xA;export async function webhook(request: Request) {&#xA;  const headersList = headers();&#xA;  const body = await request.text();&#xA;&#xA;  try {&#xA;    client.webhooks.verifySignature(body, headersList);&#xA;&#xA;    // Parse the body after verification&#xA;    const event = JSON.parse(body);&#xA;    console.log(&#39;Verified event:&#39;, event);&#xA;&#xA;    return Response.json({ message: &#39;ok&#39; });&#xA;  } catch (error) {&#xA;    console.error(&#39;Invalid webhook signature:&#39;, error);&#xA;    return new Response(&#39;Invalid signature&#39;, { status: 400 });&#xA;  }&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Handling errors&lt;/h2&gt; &#xA;&lt;p&gt;When the library is unable to connect to the API, or if the API returns a non-success status code (i.e., 4xx or 5xx response), a subclass of &lt;code&gt;APIError&lt;/code&gt; will be thrown:&lt;/p&gt; &#xA;&lt;!-- prettier-ignore --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const job = await client.fineTuning.jobs&#xA;  .create({ model: &#39;gpt-4o&#39;, training_file: &#39;file-abc123&#39; })&#xA;  .catch(async (err) =&amp;gt; {&#xA;    if (err instanceof OpenAI.APIError) {&#xA;      console.log(err.request_id);&#xA;      console.log(err.status); // 400&#xA;      console.log(err.name); // BadRequestError&#xA;      console.log(err.headers); // {server: &#39;nginx&#39;, ...}&#xA;    } else {&#xA;      throw err;&#xA;    }&#xA;  });&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Error codes are as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Status Code&lt;/th&gt; &#xA;   &lt;th&gt;Error Type&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;400&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;BadRequestError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;401&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;AuthenticationError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;403&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;PermissionDeniedError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;404&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;NotFoundError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;422&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;UnprocessableEntityError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;429&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;RateLimitError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&amp;gt;=500&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;InternalServerError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;N/A&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;APIConnectionError&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Request IDs&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For more information on debugging requests, see &lt;a href=&#34;https://platform.openai.com/docs/api-reference/debugging-requests&#34;&gt;these docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;x-request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to OpenAI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const completion = await client.chat.completions.create({&#xA;  messages: [{ role: &#39;user&#39;, content: &#39;Say this is a test&#39; }],&#xA;  model: &#39;gpt-4o&#39;,&#xA;});&#xA;console.log(completion._request_id); // req_123&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also access the Request ID using the &lt;code&gt;.withResponse()&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const { data: stream, request_id } = await openai.chat.completions&#xA;  .create({&#xA;    model: &#39;gpt-4&#39;,&#xA;    messages: [{ role: &#39;user&#39;, content: &#39;Say this is a test&#39; }],&#xA;    stream: true,&#xA;  })&#xA;  .withResponse();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Realtime API Beta&lt;/h2&gt; &#xA;&lt;p&gt;The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling&#34;&gt;function calling&lt;/a&gt; through a &lt;code&gt;WebSocket&lt;/code&gt; connection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { OpenAIRealtimeWebSocket } from &#39;openai/beta/realtime/websocket&#39;;&#xA;&#xA;const rt = new OpenAIRealtimeWebSocket({ model: &#39;gpt-4o-realtime-preview-2024-12-17&#39; });&#xA;&#xA;rt.on(&#39;response.text.delta&#39;, (event) =&amp;gt; process.stdout.write(event.delta));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information see &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/realtime.md&#34;&gt;realtime.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;To use this library with &lt;a href=&#34;https://learn.microsoft.com/azure/ai-services/openai/overview&#34;&gt;Azure OpenAI&lt;/a&gt;, use the &lt;code&gt;AzureOpenAI&lt;/code&gt; class instead of the &lt;code&gt;OpenAI&lt;/code&gt; class.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The Azure API shape slightly differs from the core API shape which means that the static types for responses / params won&#39;t always be correct.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { AzureOpenAI } from &#39;openai&#39;;&#xA;import { getBearerTokenProvider, DefaultAzureCredential } from &#39;@azure/identity&#39;;&#xA;&#xA;const credential = new DefaultAzureCredential();&#xA;const scope = &#39;https://cognitiveservices.azure.com/.default&#39;;&#xA;const azureADTokenProvider = getBearerTokenProvider(credential, scope);&#xA;&#xA;const openai = new AzureOpenAI({ azureADTokenProvider });&#xA;&#xA;const result = await openai.chat.completions.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  messages: [{ role: &#39;user&#39;, content: &#39;Say hello!&#39; }],&#xA;});&#xA;&#xA;console.log(result.choices[0]!.message?.content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Retries&lt;/h3&gt; &#xA;&lt;p&gt;Certain errors will be automatically retried 2 times by default, with a short exponential backoff. Connection errors (for example, due to a network connectivity problem), 408 Request Timeout, 409 Conflict, 429 Rate Limit, and &amp;gt;=500 Internal errors will all be retried by default.&lt;/p&gt; &#xA;&lt;p&gt;You can use the &lt;code&gt;maxRetries&lt;/code&gt; option to configure or disable this:&lt;/p&gt; &#xA;&lt;!-- prettier-ignore --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-js&#34;&gt;// Configure the default for all requests:&#xA;const client = new OpenAI({&#xA;  maxRetries: 0, // default is 2&#xA;});&#xA;&#xA;// Or, configure per-request:&#xA;await client.chat.completions.create({ messages: [{ role: &#39;user&#39;, content: &#39;How can I get the name of the current day in JavaScript?&#39; }], model: &#39;gpt-4o&#39; }, {&#xA;  maxRetries: 5,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Timeouts&lt;/h3&gt; &#xA;&lt;p&gt;Requests time out after 10 minutes by default. You can configure this with a &lt;code&gt;timeout&lt;/code&gt; option:&lt;/p&gt; &#xA;&lt;!-- prettier-ignore --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;// Configure the default for all requests:&#xA;const client = new OpenAI({&#xA;  timeout: 20 * 1000, // 20 seconds (default is 10 minutes)&#xA;});&#xA;&#xA;// Override per-request:&#xA;await client.chat.completions.create({ messages: [{ role: &#39;user&#39;, content: &#39;How can I list all files in a directory using Python?&#39; }], model: &#39;gpt-4o&#39; }, {&#xA;  timeout: 5 * 1000,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On timeout, an &lt;code&gt;APIConnectionTimeoutError&lt;/code&gt; is thrown.&lt;/p&gt; &#xA;&lt;p&gt;Note that requests which time out will be &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/#retries&#34;&gt;retried twice by default&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Request IDs&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;For more information on debugging requests, see &lt;a href=&#34;https://platform.openai.com/docs/api-reference/debugging-requests&#34;&gt;these docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;All object responses in the SDK provide a &lt;code&gt;_request_id&lt;/code&gt; property which is added from the &lt;code&gt;x-request-id&lt;/code&gt; response header so that you can quickly log failing requests and report them back to OpenAI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const response = await client.responses.create({ model: &#39;gpt-4o&#39;, input: &#39;testing 123&#39; });&#xA;console.log(response._request_id); // req_123&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also access the Request ID using the &lt;code&gt;.withResponse()&lt;/code&gt; method:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const { data: stream, request_id } = await openai.responses&#xA;  .create({&#xA;    model: &#39;gpt-4o&#39;,&#xA;    input: &#39;Say this is a test&#39;,&#xA;    stream: true,&#xA;  })&#xA;  .withResponse();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Auto-pagination&lt;/h2&gt; &#xA;&lt;p&gt;List methods in the OpenAI API are paginated. You can use the &lt;code&gt;for await ‚Ä¶ of&lt;/code&gt; syntax to iterate through items across all pages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;async function fetchAllFineTuningJobs(params) {&#xA;  const allFineTuningJobs = [];&#xA;  // Automatically fetches more pages as needed.&#xA;  for await (const fineTuningJob of client.fineTuning.jobs.list({ limit: 20 })) {&#xA;    allFineTuningJobs.push(fineTuningJob);&#xA;  }&#xA;  return allFineTuningJobs;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can request a single page at a time:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;let page = await client.fineTuning.jobs.list({ limit: 20 });&#xA;for (const fineTuningJob of page.data) {&#xA;  console.log(fineTuningJob);&#xA;}&#xA;&#xA;// Convenience methods are provided for manually paginating:&#xA;while (page.hasNextPage()) {&#xA;  page = await page.getNextPage();&#xA;  // ...&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Realtime API Beta&lt;/h2&gt; &#xA;&lt;p&gt;The Realtime API enables you to build low-latency, multi-modal conversational experiences. It currently supports text and audio as both input and output, as well as &lt;a href=&#34;https://platform.openai.com/docs/guides/function-calling&#34;&gt;function calling&lt;/a&gt; through a &lt;code&gt;WebSocket&lt;/code&gt; connection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { OpenAIRealtimeWebSocket } from &#39;openai/beta/realtime/websocket&#39;;&#xA;&#xA;const rt = new OpenAIRealtimeWebSocket({ model: &#39;gpt-4o-realtime-preview-2024-12-17&#39; });&#xA;&#xA;rt.on(&#39;response.text.delta&#39;, (event) =&amp;gt; process.stdout.write(event.delta));&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information see &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/realtime.md&#34;&gt;realtime.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Microsoft Azure OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;To use this library with &lt;a href=&#34;https://learn.microsoft.com/azure/ai-services/openai/overview&#34;&gt;Azure OpenAI&lt;/a&gt;, use the &lt;code&gt;AzureOpenAI&lt;/code&gt; class instead of the &lt;code&gt;OpenAI&lt;/code&gt; class.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] The Azure API shape slightly differs from the core API shape which means that the static types for responses / params won&#39;t always be correct.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { AzureOpenAI } from &#39;openai&#39;;&#xA;import { getBearerTokenProvider, DefaultAzureCredential } from &#39;@azure/identity&#39;;&#xA;&#xA;const credential = new DefaultAzureCredential();&#xA;const scope = &#39;https://cognitiveservices.azure.com/.default&#39;;&#xA;const azureADTokenProvider = getBearerTokenProvider(credential, scope);&#xA;&#xA;const openai = new AzureOpenAI({&#xA;  azureADTokenProvider,&#xA;  apiVersion: &#39;&amp;lt;The API version, e.g. 2024-10-01-preview&amp;gt;&#39;,&#xA;});&#xA;&#xA;const result = await openai.chat.completions.create({&#xA;  model: &#39;gpt-4o&#39;,&#xA;  messages: [{ role: &#39;user&#39;, content: &#39;Say hello!&#39; }],&#xA;});&#xA;&#xA;console.log(result.choices[0]!.message?.content);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more information on support for the Azure API, see &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/azure.md&#34;&gt;azure.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Advanced Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Accessing raw Response data (e.g., headers)&lt;/h3&gt; &#xA;&lt;p&gt;The &#34;raw&#34; &lt;code&gt;Response&lt;/code&gt; returned by &lt;code&gt;fetch()&lt;/code&gt; can be accessed through the &lt;code&gt;.asResponse()&lt;/code&gt; method on the &lt;code&gt;APIPromise&lt;/code&gt; type that all methods return. This method returns as soon as the headers for a successful response are received and does not consume the response body, so you are free to write custom parsing or streaming logic.&lt;/p&gt; &#xA;&lt;p&gt;You can also use the &lt;code&gt;.withResponse()&lt;/code&gt; method to get the raw &lt;code&gt;Response&lt;/code&gt; along with the parsed data. Unlike &lt;code&gt;.asResponse()&lt;/code&gt; this method consumes the body, returning once it is parsed.&lt;/p&gt; &#xA;&lt;!-- prettier-ignore --&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;const client = new OpenAI();&#xA;&#xA;const httpResponse = await client.responses&#xA;  .create({ model: &#39;gpt-4o&#39;, input: &#39;say this is a test.&#39; })&#xA;  .asResponse();&#xA;&#xA;// access the underlying web standard Response object&#xA;console.log(httpResponse.headers.get(&#39;X-My-Header&#39;));&#xA;console.log(httpResponse.statusText);&#xA;&#xA;const { data: modelResponse, response: raw } = await client.responses&#xA;  .create({ model: &#39;gpt-4o&#39;, input: &#39;say this is a test.&#39; })&#xA;  .withResponse();&#xA;console.log(raw.headers.get(&#39;X-My-Header&#39;));&#xA;console.log(modelResponse);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Logging&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] All log messages are intended for debugging only. The format and content of log messages may change between releases.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h4&gt;Log levels&lt;/h4&gt; &#xA;&lt;p&gt;The log level can be configured in two ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Via the &lt;code&gt;OPENAI_LOG&lt;/code&gt; environment variable&lt;/li&gt; &#xA; &lt;li&gt;Using the &lt;code&gt;logLevel&lt;/code&gt; client option (overrides the environment variable if set)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  logLevel: &#39;debug&#39;, // Show all log messages&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Available log levels, from most to least verbose:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;debug&#39;&lt;/code&gt; - Show debug messages, info, warnings, and errors&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;info&#39;&lt;/code&gt; - Show info messages, warnings, and errors&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;warn&#39;&lt;/code&gt; - Show warnings and errors (default)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;error&#39;&lt;/code&gt; - Show only errors&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;&#39;off&#39;&lt;/code&gt; - Disable all logging&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;At the &lt;code&gt;&#39;debug&#39;&lt;/code&gt; level, all HTTP requests and responses are logged, including headers and bodies. Some authentication-related headers are redacted, but sensitive data in request and response bodies may still be visible.&lt;/p&gt; &#xA;&lt;h4&gt;Custom logger&lt;/h4&gt; &#xA;&lt;p&gt;By default, this library logs to &lt;code&gt;globalThis.console&lt;/code&gt;. You can also provide a custom logger. Most logging libraries are supported, including &lt;a href=&#34;https://www.npmjs.com/package/pino&#34;&gt;pino&lt;/a&gt;, &lt;a href=&#34;https://www.npmjs.com/package/winston&#34;&gt;winston&lt;/a&gt;, &lt;a href=&#34;https://www.npmjs.com/package/bunyan&#34;&gt;bunyan&lt;/a&gt;, &lt;a href=&#34;https://www.npmjs.com/package/consola&#34;&gt;consola&lt;/a&gt;, &lt;a href=&#34;https://www.npmjs.com/package/signale&#34;&gt;signale&lt;/a&gt;, and &lt;a href=&#34;https://jsr.io/@std/log&#34;&gt;@std/log&lt;/a&gt;. If your logger doesn&#39;t work, please open an issue.&lt;/p&gt; &#xA;&lt;p&gt;When providing a custom logger, the &lt;code&gt;logLevel&lt;/code&gt; option still controls which messages are emitted, messages below the configured level will not be sent to your logger.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;import pino from &#39;pino&#39;;&#xA;&#xA;const logger = pino();&#xA;&#xA;const client = new OpenAI({&#xA;  logger: logger.child({ name: &#39;OpenAI&#39; }),&#xA;  logLevel: &#39;debug&#39;, // Send all messages to pino, allowing it to filter&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Making custom/undocumented requests&lt;/h3&gt; &#xA;&lt;p&gt;This library is typed for convenient access to the documented API. If you need to access undocumented endpoints, params, or response properties, the library can still be used.&lt;/p&gt; &#xA;&lt;h4&gt;Undocumented endpoints&lt;/h4&gt; &#xA;&lt;p&gt;To make requests to undocumented endpoints, you can use &lt;code&gt;client.get&lt;/code&gt;, &lt;code&gt;client.post&lt;/code&gt;, and other HTTP verbs. Options on the client, such as retries, will be respected when making these requests.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;await client.post(&#39;/some/path&#39;, {&#xA;  body: { some_prop: &#39;foo&#39; },&#xA;  query: { some_query_arg: &#39;bar&#39; },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Undocumented request params&lt;/h4&gt; &#xA;&lt;p&gt;To make requests using undocumented parameters, you may use &lt;code&gt;// @ts-expect-error&lt;/code&gt; on the undocumented parameter. This library doesn&#39;t validate at runtime that the request matches the type, so any extra values you send will be sent as-is.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;client.chat.completions.create({&#xA;  // ...&#xA;  // @ts-expect-error baz is not yet public&#xA;  baz: &#39;undocumented option&#39;,&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For requests with the &lt;code&gt;GET&lt;/code&gt; verb, any extra params will be in the query, all other requests will send the extra param in the body.&lt;/p&gt; &#xA;&lt;p&gt;If you want to explicitly send an extra argument, you can do so with the &lt;code&gt;query&lt;/code&gt;, &lt;code&gt;body&lt;/code&gt;, and &lt;code&gt;headers&lt;/code&gt; request options.&lt;/p&gt; &#xA;&lt;h4&gt;Undocumented response properties&lt;/h4&gt; &#xA;&lt;p&gt;To access undocumented response properties, you may access the response object with &lt;code&gt;// @ts-expect-error&lt;/code&gt; on the response object, or cast the response object to the requisite type. Like the request params, we do not validate or strip extra properties from the response from the API.&lt;/p&gt; &#xA;&lt;h3&gt;Customizing the fetch client&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use a different &lt;code&gt;fetch&lt;/code&gt; function, you can either polyfill the global:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import fetch from &#39;my-fetch&#39;;&#xA;&#xA;globalThis.fetch = fetch;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or pass it to the client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;import fetch from &#39;my-fetch&#39;;&#xA;&#xA;const client = new OpenAI({ fetch });&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fetch options&lt;/h3&gt; &#xA;&lt;p&gt;If you want to set custom &lt;code&gt;fetch&lt;/code&gt; options without overriding the &lt;code&gt;fetch&lt;/code&gt; function, you can provide a &lt;code&gt;fetchOptions&lt;/code&gt; object when instantiating the client or making a request. (Request-specific options override client options.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  fetchOptions: {&#xA;    // `RequestInit` options&#xA;  },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Configuring proxies&lt;/h4&gt; &#xA;&lt;p&gt;To modify proxy behavior, you can provide custom &lt;code&gt;fetchOptions&lt;/code&gt; that add runtime-specific proxy options to requests:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/stainless-api/sdk-assets/refs/heads/main/node.svg?sanitize=true&#34; align=&#34;top&#34; width=&#34;18&#34; height=&#34;21&#34; /&gt; &lt;strong&gt;Node&lt;/strong&gt; &lt;sup&gt;[&lt;a href=&#34;https://github.com/nodejs/undici/raw/main/docs/docs/api/ProxyAgent.md#example---proxyagent-with-fetch&#34;&gt;docs&lt;/a&gt;]&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;import * as undici from &#39;undici&#39;;&#xA;&#xA;const proxyAgent = new undici.ProxyAgent(&#39;http://localhost:8888&#39;);&#xA;const client = new OpenAI({&#xA;  fetchOptions: {&#xA;    dispatcher: proxyAgent,&#xA;  },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/stainless-api/sdk-assets/refs/heads/main/bun.svg?sanitize=true&#34; align=&#34;top&#34; width=&#34;18&#34; height=&#34;21&#34; /&gt; &lt;strong&gt;Bun&lt;/strong&gt; &lt;sup&gt;[&lt;a href=&#34;https://bun.sh/guides/http/proxy&#34;&gt;docs&lt;/a&gt;]&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;openai&#39;;&#xA;&#xA;const client = new OpenAI({&#xA;  fetchOptions: {&#xA;    proxy: &#39;http://localhost:8888&#39;,&#xA;  },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/stainless-api/sdk-assets/refs/heads/main/deno.svg?sanitize=true&#34; align=&#34;top&#34; width=&#34;18&#34; height=&#34;21&#34; /&gt; &lt;strong&gt;Deno&lt;/strong&gt; &lt;sup&gt;[&lt;a href=&#34;https://docs.deno.com/api/deno/~/Deno.createHttpClient&#34;&gt;docs&lt;/a&gt;]&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import OpenAI from &#39;npm:openai&#39;;&#xA;&#xA;const httpClient = Deno.createHttpClient({ proxy: { url: &#39;http://localhost:8888&#39; } });&#xA;const client = new OpenAI({&#xA;  fetchOptions: {&#xA;    client: httpClient,&#xA;  },&#xA;});&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Frequently Asked Questions&lt;/h2&gt; &#xA;&lt;h2&gt;Semantic versioning&lt;/h2&gt; &#xA;&lt;p&gt;This package generally follows &lt;a href=&#34;https://semver.org/spec/v2.0.0.html&#34;&gt;SemVer&lt;/a&gt; conventions, though certain backwards-incompatible changes may be released as minor versions:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Changes that only affect static types, without breaking runtime behavior.&lt;/li&gt; &#xA; &lt;li&gt;Changes to library internals which are technically public but not intended or documented for external use. &lt;em&gt;(Please open a GitHub issue to let us know if you are relying on such internals.)&lt;/em&gt;&lt;/li&gt; &#xA; &lt;li&gt;Changes that we do not expect to impact the vast majority of users in practice.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We take backwards-compatibility seriously and work hard to ensure you can rely on a smooth upgrade experience.&lt;/p&gt; &#xA;&lt;p&gt;We are keen for your feedback; please open an &lt;a href=&#34;https://www.github.com/openai/openai-node/issues&#34;&gt;issue&lt;/a&gt; with questions, bugs, or suggestions.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;p&gt;TypeScript &amp;gt;= 4.9 is supported.&lt;/p&gt; &#xA;&lt;p&gt;The following runtimes are supported:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Node.js 20 LTS or later (&lt;a href=&#34;https://endoflife.date/nodejs&#34;&gt;non-EOL&lt;/a&gt;) versions.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deno v1.28.0 or higher.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Bun 1.0 or later.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Cloudflare Workers.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Vercel Edge Runtime.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Jest 28 or greater with the &lt;code&gt;&#34;node&#34;&lt;/code&gt; environment (&lt;code&gt;&#34;jsdom&#34;&lt;/code&gt; is not supported at this time).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Nitro v2.6 or greater.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Web browsers: disabled by default to avoid exposing your secret API credentials. Enable browser support by explicitly setting &lt;code&gt;dangerouslyAllowBrowser&lt;/code&gt; to true&#39;.&lt;/p&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;More explanation&lt;/summary&gt; &#xA;   &lt;h3&gt;Why is this dangerous?&lt;/h3&gt; &#xA;   &lt;p&gt;Enabling the &lt;code&gt;dangerouslyAllowBrowser&lt;/code&gt; option can be dangerous because it exposes your secret API credentials in the client-side code. Web browsers are inherently less secure than server environments, any user with access to the browser can potentially inspect, extract, and misuse these credentials. This could lead to unauthorized access using your credentials and potentially compromise sensitive data or functionality.&lt;/p&gt; &#xA;   &lt;h3&gt;When might this not be dangerous?&lt;/h3&gt; &#xA;   &lt;p&gt;In certain scenarios where enabling browser support might not pose significant risks:&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Internal Tools: If the application is used solely within a controlled internal environment where the users are trusted, the risk of credential exposure can be mitigated.&lt;/li&gt; &#xA;    &lt;li&gt;Public APIs with Limited Scope: If your API has very limited scope and the exposed credentials do not grant access to sensitive data or critical operations, the potential impact of exposure is reduced.&lt;/li&gt; &#xA;    &lt;li&gt;Development or debugging purpose: Enabling this feature temporarily might be acceptable, provided the credentials are short-lived, aren&#39;t also used in production environments, or are frequently rotated.&lt;/li&gt; &#xA;   &lt;/ul&gt; &#xA;  &lt;/details&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;  &#xA;&lt;p&gt;Note that React Native is not supported at this time.&lt;/p&gt; &#xA;&lt;p&gt;If you are interested in other runtime environments, please open or upvote an issue on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/openai/openai-node/master/CONTRIBUTING.md&#34;&gt;the contributing documentation&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>