<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-06T01:27:46Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>modularml/mojo</title>
    <updated>2023-05-06T01:27:46Z</updated>
    <id>tag:github.com,2023-05-06:/modularml/mojo</id>
    <link href="https://github.com/modularml/mojo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Mojo Programming Language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Welcome to Mojo üî•&lt;/h1&gt; &#xA;&lt;p&gt;Mojo is a new programming language that bridges the gap between research and production by combining Python syntax and ecosystem with systems programming and metaprogramming features. Mojo is still young, but it is designed to become a superset of Python over time.&lt;/p&gt; &#xA;&lt;p&gt;We plan to open-source Mojo progressively over time, but it&#39;s changing very quickly now. We believe that a small, tight-knit group of engineers with a shared vision can move faster than a community effort, so we will continue to incubate it within Modular until it&#39;s more complete. Please see the &lt;a href=&#34;https://docs.modular.com/mojo/faq.html&#34;&gt;Mojo FAQ&lt;/a&gt; for more information about this and other common questions.&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve opened this repo now because we want to gather issues and engage in feedback from users who have access to the Mojo Playground (our hosted JupyterHub where you can try coding with an early version of Mojo). To get access to the Mojo Playground, &lt;a href=&#34;https://docs.modular.com/mojo/get-started.html&#34;&gt;see here to sign up&lt;/a&gt;. Then, when you want to report issues or request features, &lt;a href=&#34;https://github.com/modularml/mojo/issues&#34;&gt;please create a GitHub issue here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more general questions or to chat with other Mojo developers, check out our &lt;a href=&#34;https://discord.gg/modular&#34;&gt;Discord&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Otherwise, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://docs.modular.com/mojo/why-mojo.html&#34;&gt;inspiration behind Mojo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Check out the &lt;a href=&#34;https://docs.modular.com/mojo/programming-manual.html&#34;&gt;Mojo programming manual&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Read our other docs on &lt;a href=&#34;https://docs.modular.com/mojo&#34;&gt;docs.modular.com/mojo&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>Deci-AI/super-gradients</title>
    <updated>2023-05-06T01:27:46Z</updated>
    <id>tag:github.com,2023-05-06:/Deci-AI/super-gradients</id>
    <link href="https://github.com/Deci-AI/super-gradients" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Easily train or fine-tune SOTA computer vision models with one open source training library. The home of Yolo-NAS.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34; markdown=&#34;1&#34;&gt; &#xA; &lt;img src=&#34;docs/assets/SG_img/SG - Horizontal Glow 2.png&#34; width=&#34;600&#34;&gt; &#xA; &lt;br&gt;&#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;strong&gt;Build, train, and fine-tune production-ready deep learning SOTA vision models&lt;/strong&gt; &lt;a href=&#34;https://twitter.com/intent/tweet?text=Easily%20train%20or%20fine-tune%20SOTA%20computer%20vision%20models%20from%20one%20training%20repository&amp;amp;url=https://github.com/Deci-AI/super-gradients&amp;amp;via=deci_ai&amp;amp;hashtags=AI,deeplearning,computervision,training,opensource&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/http/shields.io.svg?style=social&#34; alt=&#34;Tweet&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4&gt;Version 3 is out! Notebooks have been updated!&lt;/h4&gt; &#xA; &lt;hr&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.supergradients.com/&#34;&gt;Website&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://docs.deci.ai/super-gradients/documentation/source/welcome.html&#34;&gt;Docs&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#implemented-model-architectures&#34;&gt;Pretrained Models&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#community&#34;&gt;Community&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#license&#34;&gt;License&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#deci-platform&#34;&gt;Deci Platform&lt;/a&gt; &lt;/p&gt; &#xA; &lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/Deci-AI/super-gradients#prerequisites&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7%20%7C%203.8%20%7C%203.9-blue&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients#prerequisites&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pytorch-1.9%20%7C%201.10-blue&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://pypi.org/project/super-gradients/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/super-gradients&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients#computer-vision-models-pretrained-checkpoints&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pre--trained%20models-34-brightgreen&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/Deci-AI/super-gradients&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://join.slack.com/t/supergradients-comm52/shared_invite/zt-10vz6o1ia-b_0W5jEPEnuHXm087K~t8Q&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/slack-community-blueviolet&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-Apache%202.0-blue&#34;&gt; &lt;/a&gt;&lt;a href=&#34;https://docs.deci.ai/super-gradients/documentation/source/welcome.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/docs-mkdocs-brightgreen&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Build with SuperGradients&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Support various computer vision tasks&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/Deci-AI/super-gradients/raw/master/docs/assets/SG_img/Segmentation 1500x900 .png&#34; width=&#34;250px&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/Deci-AI/super-gradients/raw/master/docs/assets/SG_img/Object detection 1500X900.png&#34; width=&#34;250px&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/Deci-AI/super-gradients/raw/master/docs/assets/SG_img/Classification 1500x900.png&#34; width=&#34;250px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Ready to deploy pre-trained SOTA models&lt;/h3&gt; &#xA;&lt;p&gt;YOLO-NAS architecture is out! The new YOLO-NAS delivers state-of-the-art performance with the unparalleled accuracy-speed performance, outperforming other models such as YOLOv5, YOLOv6, YOLOv7 and YOLOv8. Check it out here: &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/YOLONAS.md&#34;&gt;YOLO-NAS&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/documentation/source/images/yolo_nas_frontier.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load model with pretrained weights&#xA;from super_gradients.training import models&#xA;from super_gradients.common.object_names import Models&#xA;&#xA;model = models.get(Models.YOLO_NAS_M, pretrained_weights=&#34;coco&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;All Computer Vision Models - Pretrained Checkpoints can be found in the &lt;a href=&#34;http://bit.ly/41dkt89&#34;&gt;Model Zoo&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;h4&gt;Classification&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/Classification@2xDark.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Semantic Segmentation&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;./docs/assets/SG_img/Semantic Segmentation@2xDark.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h4&gt;Object Detection&lt;/h4&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;./docs/assets/SG_img/Object Detection@2xDark.png&#34; width=&#34;800px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;Easy to train SOTA Models&lt;/h3&gt; &#xA;&lt;p&gt;Easily load and fine-tune production-ready, pre-trained SOTA models that incorporate best practices and validated hyper-parameters for achieving best-in-class accuracy. For more information on how to do it go to &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Plug and play recipes&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m super_gradients.train_from_recipe architecture=regnetY800 dataset_interface.data_dir=&amp;lt;YOUR_Imagenet_LOCAL_PATH&amp;gt; ckpt_root_dir=&amp;lt;CHEKPOINT_DIRECTORY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More example on how and why to use recipes can be found in &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#recipes&#34;&gt;Recipes&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Production readiness&lt;/h3&gt; &#xA;&lt;p&gt;All SuperGradients models‚Äô are production ready in the sense that they are compatible with deployment tools such as TensorRT (Nvidia) and OpenVINO (Intel) and can be easily taken into production. With a few lines of code you can easily integrate the models into your codebase.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Load model with pretrained weights&#xA;from super_gradients.training import models&#xA;from super_gradients.common.object_names import Models&#xA;&#xA;model = models.get(Models.YOLO_NAS_M, pretrained_weights=&#34;coco&#34;)&#xA;&#xA;# Prepare model for conversion&#xA;# Input size is in format of [Batch x Channels x Width x Height] where 640 is the standard COCO dataset dimensions&#xA;model.eval()&#xA;model.prep_model_for_conversion(input_size=[1, 3, 640, 640])&#xA;    &#xA;# Create dummy_input&#xA;&#xA;# Convert model to onnx&#xA;torch.onnx.export(model, dummy_input,  &#34;yolo_nas_m.onnx&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More information on how to take your model to production can be found in &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt; notebooks&lt;/p&gt; &#xA;&lt;h2&gt;Quick Installation&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install super-gradients&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;What&#39;s New - Version 3.1.1 (May 3rd)&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bit.ly/41WeNPZ&#34;&gt;YOLO-NAS&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;New &lt;a href=&#34;https://bit.ly/3oZfaea&#34;&gt;predict function&lt;/a&gt; (predict on any image, video, url, path, stream)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bit.ly/40YOJ5z&#34;&gt;RoboFlow100&lt;/a&gt; datasets integration&lt;/li&gt; &#xA; &lt;li&gt;A new &lt;a href=&#34;https://docs.deci.ai/super-gradients/documentation/source/welcome.html&#34;&gt;Documentation Hub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Integration with &lt;a href=&#34;https://bit.ly/3ALFUkQ&#34;&gt;DagsHub for experiment monitoring&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Support &lt;a href=&#34;https://bit.ly/41VX6Qu&#34;&gt;Darknet/Yolo format detection dataset&lt;/a&gt; (used by Yolo v5, v6, v7, v8)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bit.ly/3oYu6Jp&#34;&gt;Segformer&lt;/a&gt; model and recipe&lt;/li&gt; &#xA; &lt;li&gt;Post Training Quantization and Quantization Aware Training - &lt;a href=&#34;http://bit.ly/3KrN6an&#34;&gt;notebooks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Check out SG full &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/releases&#34;&gt;release notes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Coming soon&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Pre-trained pose estimation model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Test Time Augmentations (TTA)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Recipe to train DEKR model(convertable to TRT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Key-points Rescoring for Pose estimation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; LR finder&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Data analysis tools&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Content&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;!-- toc --&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#getting-started&#34;&gt;Getting Started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#advanced-features&#34;&gt;Advanced Features&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#installation-methods&#34;&gt;Installation Methods&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#quick-installation&#34;&gt;Quick Installation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#implemented-model-architectures&#34;&gt;Implemented Model Architectures&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#contributing&#34;&gt;Contributing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#community&#34;&gt;Community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/#deci-platform&#34;&gt;Deci Platform&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- tocstop --&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Start Training with Just 1 Command Line&lt;/h3&gt; &#xA;&lt;p&gt;The most simple and straightforward way to start training SOTA performance models with SuperGradients reproducible recipes. Just define your dataset path and where you want your checkpoints to be saved and you are good to go from your terminal!&lt;/p&gt; &#xA;&lt;p&gt;Just make sure that you &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/Dataset_Setup_Instructions.md&#34;&gt;setup your dataset&lt;/a&gt; according to the data dir specified in the recipe.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m super_gradients.train_from_recipe --config-name=imagenet_regnetY architecture=regnetY800 dataset_interface.data_dir=&amp;lt;YOUR_Imagenet_LOCAL_PATH&amp;gt; ckpt_root_dir=&amp;lt;CHEKPOINT_DIRECTORY&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quickly Load Pre-Trained Weights for Your Desired Model with SOTA Performance&lt;/h3&gt; &#xA;&lt;p&gt;Want to try our pre-trained models on your machine? Import SuperGradients, initialize your Trainer, and load your desired architecture and pre-trained weights from our &lt;a href=&#34;http://bit.ly/41dkt89&#34;&gt;SOTA model zoo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# The pretrained_weights argument will load a pre-trained architecture on the provided dataset&#xA;    &#xA;import super_gradients&#xA;&#xA;model = models.get(&#34;model-name&#34;, pretrained_weights=&#34;pretrained-model-name&#34;)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Classification&lt;/h3&gt; &#xA;&lt;h4&gt;Transfer Learning&lt;/h4&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;500&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3xzIutb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; Classification Transfer Learning&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;200&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3xwYEn1&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h3&gt;Semantic Segmentation&lt;/h3&gt; &#xA;&lt;h4&gt;Quick Start&lt;/h4&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;500&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3qKx9m8&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; Segmentation Quick Start&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;200&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3qJjxYq&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source &lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h4&gt;Transfer Learning&lt;/h4&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;500&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3qKwMbe&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; Segmentation Transfer Learning&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;200&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3ShJlXn&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h4&gt;How to Connect Custom Dataset&lt;/h4&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;500&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3QQBVJp&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; Segmentation How to Connect Custom Dataset&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;200&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3Us2WGi&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h3&gt;Object Detection&lt;/h3&gt; &#xA;&lt;h4&gt;Transfer Learning&lt;/h4&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;500&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3SkMohx&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; Detection Transfer Learning&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;200&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3DF8siG&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h4&gt;How to Connect Custom Dataset&lt;/h4&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;500&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3dqDlg3&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; Detection How to Connect Custom Dataset&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;200&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3xBlcmq&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h3&gt;How to Predict Using Pre-trained Model&lt;/h3&gt; &#xA;&lt;h4&gt;Segmentation, Detection and Classification Prediction&lt;/h4&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;500&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3f4mssd&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; How to Predict Using Pre-trained Model&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;200&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3Sf59Tr&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h2&gt;Advanced Features&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Post Training Quantization and Quantization Aware Training&lt;/h3&gt; &#xA;&lt;p&gt;Quantization involves representing weights and biases in lower precision, resulting in reduced memory and computational requirements, making it useful for deploying models on devices with limited resources. The process can be done during training, called Quantization aware training, or after training, called post-training quantization. A full tutorial can be found &lt;a href=&#34;http://bit.ly/41hC8uI&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table class=&#34;‚Äútfo-notebook-buttons‚Äù&#34; align=&#34;‚Äúleft‚Äù&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;‚Äú500‚Äù&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;http://bit.ly/3KrN6an&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; Post Training Quantization and Quantization Aware Training&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;‚Äú200‚Äù&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;http://bit.ly/3nUGzxb&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Knowledge Distillation Training&lt;/h3&gt; &#xA;&lt;p&gt;Knowledge Distillation is a training technique that uses a large model, teacher model, to improve the performance of a smaller model, the student model. Learn more about SuperGradients knowledge distillation training with our pre-trained BEiT base teacher model and Resnet18 student model on CIFAR10 example notebook on Google Colab for an easy to use tutorial using free GPU hardware&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;500&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3BLA5oR&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; Knowledge Distillation Training&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;200&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3S9UlG4&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;h3&gt;Recipes&lt;/h3&gt; &#xA;&lt;p&gt;To train a model, it is necessary to configure 4 main components. These components are aggregated into a single &#34;main&#34; recipe &lt;code&gt;.yaml&lt;/code&gt; file that inherits the aforementioned dataset, architecture, raining and checkpoint params. It is also possible (and recomended for flexibility) to override default settings with custom ones. All recipes can be found &lt;a href=&#34;http://bit.ly/3gfLw07&#34;&gt;here&lt;/a&gt; &lt;br&gt; Recipes support out of the box every model, metric or loss that is implemented in SuperGradients, but you can easily extend this to any custom object that you need by &#34;registering it&#34;. Check out &lt;a href=&#34;http://bit.ly/3TQ4iZB&#34;&gt;this&lt;/a&gt; tutorial for more information.&lt;/p&gt; &#xA;&lt;table class=&#34;tfo-notebook-buttons&#34; align=&#34;left&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt;&#xA;   &lt;td width=&#34;500&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3UiY5ab&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/colab_logo.png&#34;&gt; How to Use Recipes&lt;/a&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;200&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bit.ly/3QSrHbm&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/docs/assets/SG_img/GitHub_logo.png&#34;&gt; GitHub source&lt;/a&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt;&#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;br&gt;&#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;&lt;h3&gt;Using Distributed Data Parallel (DDP) &lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;h4&gt;Why use DDP ?&lt;/h4&gt; &#xA; &lt;p&gt;Recent Deep Learning models are growing larger and larger to an extent that training on a single GPU can take weeks. In order to train models in a timely fashion, it is necessary to train them with multiple GPUs. Using 100s GPUs can reduce training time of a model from a week to less than an hour.&lt;/p&gt; &#xA; &lt;h4&gt;How does it work ?&lt;/h4&gt; &#xA; &lt;p&gt;Each GPU has its own process, which controls a copy of the model and which loads its own mini-batch from disk and sends it to its GPU during training. After the forward pass is completed on every GPU, the gradient is reduced across all GPUs, yielding to all the GPUs having the same gradient locally. This leads to the model weights to stay synchronized across all GPUs after the backward pass.&lt;/p&gt; &#xA; &lt;h4&gt;How to use it ?&lt;/h4&gt; &#xA; &lt;p&gt;You can use SuperGradients to train your model with DDP in just a few lines.&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;main.py&lt;/em&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from super_gradients import init_trainer, Trainer&#xA;from super_gradients.common import MultiGPUMode&#xA;from super_gradients.training.utils.distributed_training_utils import setup_device&#xA;&#xA;# Initialize the environment&#xA;init_trainer()&#xA;&#xA;# Launch DDP on 4 GPUs&#39;&#xA;setup_device(multi_gpu=MultiGPUMode.DISTRIBUTED_DATA_PARALLEL, num_gpus=4)&#xA;&#xA;# Call the trainer&#xA;Trainer(expriment_name=...)&#xA;&#xA;# Everything you do below will run on 4 gpus&#xA;&#xA;...&#xA;&#xA;Trainer.train(...)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Finally, you can launch your distributed training with a simple python call.&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Please note that if you work with &lt;code&gt;torch&amp;lt;1.9.0&lt;/code&gt; (deprecated), you will have to launch your training with either &lt;code&gt;torch.distributed.launch&lt;/code&gt; or &lt;code&gt;torchrun&lt;/code&gt;, in which case &lt;code&gt;nproc_per_node&lt;/code&gt; will overwrite the value set with &lt;code&gt;gpu_mode&lt;/code&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m torch.distributed.launch --nproc_per_node=4 main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=4 main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h4&gt;Calling functions on a single node&lt;/h4&gt; &#xA; &lt;p&gt;It is often in DDP training that we want to execute code on the master rank (i.e rank 0). In SG, users usually execute their own code by triggering &#34;Phase Callbacks&#34; (see &#34;Using phase callbacks&#34; section below). One can make sure the desired code will only be ran on rank 0, using ddp_silent_mode or the multi_process_safe decorator. For example, consider the simple phase callback below, that uploads the first 3 images of every batch during training to the Tensorboard:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from super_gradients.training.utils.callbacks import PhaseCallback, PhaseContext, Phase&#xA;from super_gradients.common.environment.env_helpers import multi_process_safe&#xA;&#xA;class Upload3TrainImagesCalbback(PhaseCallback):&#xA;    def __init__(&#xA;        self,&#xA;    ):&#xA;        super().__init__(phase=Phase.TRAIN_BATCH_END)&#xA;    &#xA;    @multi_process_safe&#xA;    def __call__(self, context: PhaseContext):&#xA;        batch_imgs = context.inputs.cpu().detach().numpy()&#xA;        tag = &#34;batch_&#34; + str(context.batch_idx) + &#34;_images&#34;&#xA;        context.sg_logger.add_images(tag=tag, images=batch_imgs[: 3], global_step=context.epoch)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The @multi_process_safe decorator ensures that the callback will only be triggered by rank 0. Alternatively, this can also be done by the SG trainer boolean attribute (which the phase context has access to), ddp_silent_mode, which is set to False iff the current process rank is zero (even after the process group has been killed):&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from super_gradients.training.utils.callbacks import PhaseCallback, PhaseContext, Phase&#xA;&#xA;class Upload3TrainImagesCalbback(PhaseCallback):&#xA;    def __init__(&#xA;        self,&#xA;    ):&#xA;        super().__init__(phase=Phase.TRAIN_BATCH_END)&#xA;&#xA;    def __call__(self, context: PhaseContext):&#xA;        if not context.ddp_silent_mode:&#xA;            batch_imgs = context.inputs.cpu().detach().numpy()&#xA;            tag = &#34;batch_&#34; + str(context.batch_idx) + &#34;_images&#34;&#xA;            context.sg_logger.add_images(tag=tag, images=batch_imgs[: 3], global_step=context.epoch)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Note that ddp_silent_mode can be accessed through SgTrainer.ddp_silent_mode. Hence, it can be used in scripts after calling SgTrainer.train() when some part of it should be ran on rank 0 only.&lt;/p&gt; &#xA; &lt;h4&gt;Good to know&lt;/h4&gt; &#xA; &lt;p&gt;Your total batch size will be (number of gpus x batch size), so you might want to increase your learning rate. There is no clear rule, but a rule of thumb seems to be to &lt;a href=&#34;https://arxiv.org/pdf/1706.02677.pdf&#34;&gt;linearly increase the learning rate with the number of gpus&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;&lt;h3&gt; Easily change architectures parameters &lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from super_gradients.training import models&#xA;&#xA;# instantiate default pretrained resnet18&#xA;default_resnet18 = models.get(model_name=&#34;resnet18&#34;, num_classes=100, pretrained_weights=&#34;imagenet&#34;)&#xA;&#xA;# instantiate pretrained resnet18, turning DropPath on with probability 0.5&#xA;droppath_resnet18 = models.get(model_name=&#34;resnet18&#34;, arch_params={&#34;droppath_prob&#34;: 0.5}, num_classes=100, pretrained_weights=&#34;imagenet&#34;)&#xA;&#xA;# instantiate pretrained resnet18, without classifier head. Output will be from the last stage before global pooling&#xA;backbone_resnet18 = models.get(model_name=&#34;resnet18&#34;, arch_params={&#34;backbone_mode&#34;: True}, pretrained_weights=&#34;imagenet&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;&lt;h3&gt; Using phase callbacks &lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from super_gradients import Trainer&#xA;from torch.optim.lr_scheduler import ReduceLROnPlateau&#xA;from super_gradients.training.utils.callbacks import Phase, LRSchedulerCallback&#xA;from super_gradients.training.metrics.classification_metrics import Accuracy&#xA;&#xA;# define PyTorch train and validation loaders and optimizer&#xA;&#xA;# define what to be called in the callback&#xA;rop_lr_scheduler = ReduceLROnPlateau(optimizer, mode=&#34;max&#34;, patience=10, verbose=True)&#xA;&#xA;# define phase callbacks, they will fire as defined in Phase&#xA;phase_callbacks = [LRSchedulerCallback(scheduler=rop_lr_scheduler,&#xA;                                       phase=Phase.VALIDATION_EPOCH_END,&#xA;                                       metric_name=&#34;Accuracy&#34;)]&#xA;&#xA;# create a trainer object, look the declaration for more parameters&#xA;trainer = Trainer(&#34;experiment_name&#34;)&#xA;&#xA;# define phase_callbacks as part of the training parameters&#xA;train_params = {&#34;phase_callbacks&#34;: phase_callbacks}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;&lt;h3&gt; Integration to DagsHub &lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/11fW56pMpwOMHQSbQW6xxMRYvw1mEC-t-?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from super_gradients import Trainer&#xA;&#xA;trainer = Trainer(&#34;experiment_name&#34;)&#xA;model = ...&#xA;&#xA;training_params = { ...  # Your training params&#xA;                   &#34;sg_logger&#34;: &#34;dagshub_sg_logger&#34;,  # DagsHub Logger, see class super_gradients.common.sg_loggers.dagshub_sg_logger.DagsHubSGLogger for details&#xA;                   &#34;sg_logger_params&#34;:  # Params that will be passes to __init__ of the logger super_gradients.common.sg_loggers.dagshub_sg_logger.DagsHubSGLogger&#xA;                     {&#xA;                       &#34;dagshub_repository&#34;: &#34;&amp;lt;REPO_OWNER&amp;gt;/&amp;lt;REPO_NAME&amp;gt;&#34;, # Optional: Your DagsHub project name, consisting of the owner name, followed by &#39;/&#39;, and the repo name. If this is left empty, you&#39;ll be prompted in your run to fill it in manually.&#xA;                       &#34;log_mlflow_only&#34;: False, # Optional: Change to true to bypass logging to DVC, and log all artifacts only to MLflow  &#xA;                       &#34;save_checkpoints_remote&#34;: True,&#xA;                       &#34;save_tensorboard_remote&#34;: True,&#xA;                       &#34;save_logs_remote&#34;: True,&#xA;                     }&#xA;                   }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;h3&gt; Integration to Weights and Biases &lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from super_gradients import Trainer&#xA;&#xA;# create a trainer object, look the declaration for more parameters&#xA;trainer = Trainer(&#34;experiment_name&#34;)&#xA;&#xA;train_params = { ... # training parameters&#xA;                &#34;sg_logger&#34;: &#34;wandb_sg_logger&#34;, # Weights&amp;amp;Biases Logger, see class WandBSGLogger for details&#xA;                &#34;sg_logger_params&#34;: # paramenters that will be passes to __init__ of the logger &#xA;                  {&#xA;                    &#34;project_name&#34;: &#34;project_name&#34;, # W&amp;amp;B project name&#xA;                    &#34;save_checkpoints_remote&#34;: True&#xA;                    &#34;save_tensorboard_remote&#34;: True&#xA;                    &#34;save_logs_remote&#34;: True&#xA;                  } &#xA;               }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;&lt;h3&gt; Integration to ClearML &lt;/h3&gt;&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from super_gradients import Trainer&#xA;&#xA;# create a trainer object, look the declaration for more parameters&#xA;trainer = Trainer(&#34;experiment_name&#34;)&#xA;&#xA;train_params = { ... # training parameters&#xA;                &#34;sg_logger&#34;: &#34;clearml_sg_logger&#34;, # ClearML Logger, see class ClearMLSGLogger for details&#xA;                &#34;sg_logger_params&#34;: # paramenters that will be passes to __init__ of the logger &#xA;                  {&#xA;                    &#34;project_name&#34;: &#34;project_name&#34;, # ClearML project name&#xA;                    &#34;save_checkpoints_remote&#34;: True,&#xA;                    &#34;save_tensorboard_remote&#34;: True,&#xA;                    &#34;save_logs_remote&#34;: True,&#xA;                  } &#xA;               }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Installation Methods&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;General requirements&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Python 3.7, 3.8 or 3.9 installed.&lt;/li&gt; &#xA;  &lt;li&gt;1.9.0 &amp;lt;= torch &amp;lt; 1.14 &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;https://pytorch.org/get-started/locally/&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;The python packages that are specified in requirements.txt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;To train on nvidia GPUs&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/cuda-11.2.0-download-archive?target_os=Linux&amp;amp;target_arch=x86_64&amp;amp;target_distro=Ubuntu&#34;&gt;Nvidia CUDA Toolkit &amp;gt;= 11.2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;CuDNN &amp;gt;= 8.1.x&lt;/li&gt; &#xA;  &lt;li&gt;Nvidia Driver with CUDA &amp;gt;= 11.2 support (‚â•460.x)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Quick Installation&lt;/h3&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;Install stable version using PyPi&lt;/summary&gt; &#xA; &lt;p&gt;See in &lt;a href=&#34;https://pypi.org/project/super-gradients/&#34;&gt;PyPi&lt;/a&gt;&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install super-gradients&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;That&#39;s it !&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;details markdown=&#34;1&#34;&gt; &#xA; &lt;summary&gt;Install using GitHub&lt;/summary&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/Deci-AI/super-gradients.git@stable&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Implemented Model Architectures&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;All Computer Vision Models - Pretrained Checkpoints can be found in the &lt;a href=&#34;http://bit.ly/41dkt89&#34;&gt;Model Zoo&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Detailed list can be found &lt;a href=&#34;http://bit.ly/3GnJwgZ&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Classification&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/densenet.py&#34;&gt;DensNet (Densely Connected Convolutional Networks)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/dpn.py&#34;&gt;DPN&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/efficientnet.py&#34;&gt;EfficientNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/lenet.py&#34;&gt;LeNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/mobilenet.py&#34;&gt;MobileNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/mobilenetv2.py&#34;&gt;MobileNet v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/mobilenetv3.py&#34;&gt;MobileNet v3&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/pnasnet.py&#34;&gt;PNASNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/preact_resnet.py&#34;&gt;Pre-activation ResNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/regnet.py&#34;&gt;RegNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/repvgg.py&#34;&gt;RepVGG&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/resnet.py&#34;&gt;ResNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/resnext.py&#34;&gt;ResNeXt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/senet.py&#34;&gt;SENet &lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/shufflenet.py&#34;&gt;ShuffleNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/shufflenetv2.py&#34;&gt;ShuffleNet v2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/classification_models/vgg.py&#34;&gt;VGG&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Semantic Segmentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bit.ly/3RrtMMO&#34;&gt;PP-LiteSeg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/ddrnet.py&#34;&gt;DDRNet (Deep Dual-resolution Networks)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/laddernet.py&#34;&gt;LadderNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/regseg.py&#34;&gt;RegSeg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/shelfnet.py&#34;&gt;ShelfNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/segmentation_models/stdc.py&#34;&gt;STDC&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Object Detection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/detection_models/csp_darknet53.py&#34;&gt;CSP DarkNet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/detection_models/darknet53.py&#34;&gt;DarkNet-53&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/detection_models/ssd.py&#34;&gt;SSD (Single Shot Detector)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/models/detection_models/yolox.py&#34;&gt;YOLOX&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Implemented Datasets&lt;/h2&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Deci provides implementation for various datasets. If you need to download any of the dataset, you can &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/Dataset_Setup_Instructions.md&#34;&gt;find instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Image Classification&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/classification_datasets/cifar.py&#34;&gt;Cifar10&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/classification_datasets/imagenet_dataset.py&#34;&gt;ImageNet&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Semantic Segmentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/segmentation_datasets/cityscape_segmentation.py&#34;&gt;Cityscapes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/segmentation_datasets/coco_segmentation.py&#34;&gt;Coco&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/segmentation_datasets/pascal_voc_segmentation.py&#34;&gt;PascalVOC 2012 / PascalAUG 2012&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/segmentation_datasets/supervisely_persons_segmentation.py&#34;&gt;SuperviselyPersons&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/segmentation_datasets/mapillary_dataset.py&#34;&gt;Mapillary Vistas Dataset&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Object Detection&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/detection_datasets/coco_detection.py&#34;&gt;Coco&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/raw/master/src/super_gradients/training/datasets/detection_datasets/pascal_voc_detection.py&#34;&gt;PascalVOC 2007 &amp;amp; 2012&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Check SuperGradients &lt;a href=&#34;https://docs.deci.ai/super-gradients/documentation/source/welcome.html&#34;&gt;Docs&lt;/a&gt; for full documentation, user guide, and examples.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;To learn about making a contribution to SuperGradients, please see our &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/CONTRIBUTING.md&#34;&gt;Contribution page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Our awesome contributors:&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/Deci-AI/super-gradients/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=Deci-AI/super-gradients&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;br&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you are using SuperGradients library or benchmarks in your research, please cite SuperGradients deep learning training library.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;If you want to be a part of SuperGradients growing community, hear about all the exciting news and updates, need help, request for advanced features, or want to file a bug or issue report, we would love to welcome you aboard!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Discord is the place to be and ask questions about SuperGradients and get support. &lt;a href=&#34;https://discord.gg/2v6cEGMREN&#34;&gt;Click here to join our Discord Community&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;To report a bug, &lt;a href=&#34;https://github.com/Deci-AI/super-gradients/issues&#34;&gt;file an issue&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Join the &lt;a href=&#34;https://www.supergradients.com/#Newsletter&#34;&gt;SG Newsletter&lt;/a&gt; for staying up to date with new features and models, important announcements, and upcoming events.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For a short meeting with us, use this &lt;a href=&#34;https://calendly.com/ofer-baratz-deci/15min&#34;&gt;link&lt;/a&gt; and choose your preferred time.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is released under the &lt;a href=&#34;https://raw.githubusercontent.com/Deci-AI/super-gradients/master/LICENSE&#34;&gt;Apache 2.0 license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;h3&gt;BibTeX&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;&#xA;@misc{supergradients,&#xA;  doi = {10.5281/ZENODO.7789328},&#xA;  url = {https://zenodo.org/record/7789328},&#xA;  author = {Aharon,  Shay and {Louis-Dupont} and {Ofri Masad} and Yurkova,  Kate and {Lotem Fridman} and {Lkdci} and Khvedchenya,  Eugene and Rubin,  Ran and Bagrov,  Natan and Tymchenko,  Borys and Keren,  Tomer and Zhilko,  Alexander and {Eran-Deci}},&#xA;  title = {Super-Gradients},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  year = {2021},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Latest DOI&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://doi.org/10.5281/zenodo.7789328&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/DOI/10.5281/zenodo.7789328.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Deci Platform&lt;/h2&gt; &#xA;&lt;p&gt;Deci Platform is our end to end platform for building, optimizing and deploying deep learning models to production.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://bit.ly/3qO3icq&#34;&gt;Request free trial&lt;/a&gt; to enjoy immediate improvement in throughput, latency, memory footprint and model size.&lt;/p&gt; &#xA;&lt;p&gt;Features&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Automatically compile and quantize your models with just a few clicks (TensorRT, OpenVINO).&lt;/li&gt; &#xA; &lt;li&gt;Gain up to 10X improvement in throughput, latency, memory and model size.&lt;/li&gt; &#xA; &lt;li&gt;Easily benchmark your models‚Äô performance on different hardware and batch sizes.&lt;/li&gt; &#xA; &lt;li&gt;Invite co-workers to collaborate on models and communicate your progress.&lt;/li&gt; &#xA; &lt;li&gt;Deci supports all common frameworks and Hardware, from Intel CPUs to Nvidia&#39;s GPUs and Jetsons. ÷ø&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Request free trial &lt;a href=&#34;https://bit.ly/3qO3icq&#34;&gt;here&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>openlm-research/open_llama</title>
    <updated>2023-05-06T01:27:46Z</updated>
    <id>tag:github.com,2023-05-06:/openlm-research/open_llama</id>
    <link href="https://github.com/openlm-research/open_llama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenLLaMA: An Open Reproduction of LLaMA&lt;/h1&gt; &#xA;&lt;p&gt;In this repo, we release a permissively licensed open source reproduction of Meta AI&#39;s &lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;LLaMA&lt;/a&gt; large language model. In this release, we&#39;re releasing a public preview of the 7B OpenLLaMA model that has been trained with 200 billion tokens. We provide PyTorch and Jax weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. Stay tuned for our updates.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;JAX and PyTorch Weights on Huggingface Hub&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/openlm-research/open_llama_7b_preview_200bt&#34;&gt;200B Tokens Checkpoint&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/openlm-research/open_llama_7b_preview_300bt&#34;&gt;300B Tokens Checkpoint&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Update 5/3/2023&lt;/h2&gt; &#xA;&lt;p&gt;We have released a new checkpoint of OpenLLaMA 7B trained on 300B tokens. In communicating with our users, we have realized that many existing implementations of LLaMA does not prepend the BOS token (id=1) at generation time. Our 200B checkpoint is sensitive to this and may produce degraded results without BOS token at the beginning. Hence, we recommend always prepending the BOS token when using our 200B checkpoint.&lt;/p&gt; &#xA;&lt;p&gt;In an effort to make our model broadly compatible with existing implementations, we have now released a new 300B checkpoint, which is less sensitive to BOS token and can be used either way.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset and Training&lt;/h2&gt; &#xA;&lt;p&gt;We train our models on the &lt;a href=&#34;https://www.together.xyz/blog/redpajama&#34;&gt;RedPajama&lt;/a&gt; dataset released by &lt;a href=&#34;https://www.together.xyz/&#34;&gt;Together&lt;/a&gt;, which is a reproduction of the LLaMA training dataset containing over 1.2 trillion tokens. We follow the exactly same preprocessing steps and training hyperparameters as the original LLaMA paper, including model architecture, context length, training steps, learning rate schedule, and optimizer. The only difference between our setting and the original one is the dataset used: OpenLLaMA employs the RedPajama dataset rather than the one utilized by the original LLaMA.&lt;/p&gt; &#xA;&lt;p&gt;We train the models on cloud TPU-v4s using &lt;a href=&#34;https://github.com/young-geng/EasyLM&#34;&gt;EasyLM&lt;/a&gt;, a JAX based training pipeline we developed for training and fine-tuning language model. We employ a combination of normal data parallelism and &lt;a href=&#34;https://engineering.fb.com/2021/07/15/open-source/fsdp/&#34;&gt;fully sharded data parallelism (also know as ZeRO stage 3)&lt;/a&gt; to balance the training throughput and memory usage. Overall we reach a throughput of over 1900 tokens / second / TPU-v4 chip in our training run. The training loss can be seen in the figure below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openlm-research/open_llama/main/media/loss_200bt.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;We evaluated OpenLLaMA on a wide range of tasks using &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm-evaluation-harness&lt;/a&gt;. The LLaMA results are generated by running the original LLaMA model on the same evaluation metrics. We note that our results for the LLaMA model differ slightly from the original LLaMA paper, which we believe is a result of different evaluation protocols. Similar differences have been reported in &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness/issues/443&#34;&gt;this issue of lm-evaluation-harness&lt;/a&gt;. Additionally, we present the results of GPT-J, a 6B parameter model trained on the &lt;a href=&#34;https://pile.eleuther.ai/&#34;&gt;Pile&lt;/a&gt; dataset by &lt;a href=&#34;https://www.eleuther.ai/&#34;&gt;EleutherAI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The original LLaMA model was trained for 1 trillion tokens and GPT-J was trained for 500 billion tokens, whereas OpenLLaMA was trained on 200 billion tokens. We present the results in the table below. OpenLLaMA exhibits comparable performance to the original LLaMA and GPT-J across a majority of tasks, and outperforms them in some tasks. We expect that the performance of OpenLLaMA, after completing its training on 1 trillion tokens, will be enhanced even further.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Task/Metric&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;GPT-J 6B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;LLaMA 7B&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Open LLaMA 7B Preview 200B Tokens&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;anli_r1/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.32&lt;/td&gt; &#xA;   &lt;td&gt;0.35&lt;/td&gt; &#xA;   &lt;td&gt;0.34&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;anli_r2/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.34&lt;/td&gt; &#xA;   &lt;td&gt;0.34&lt;/td&gt; &#xA;   &lt;td&gt;0.35&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;anli_r3/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.35&lt;/td&gt; &#xA;   &lt;td&gt;0.37&lt;/td&gt; &#xA;   &lt;td&gt;0.34&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_challenge/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.34&lt;/td&gt; &#xA;   &lt;td&gt;0.39&lt;/td&gt; &#xA;   &lt;td&gt;0.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_challenge/acc_norm&lt;/td&gt; &#xA;   &lt;td&gt;0.37&lt;/td&gt; &#xA;   &lt;td&gt;0.41&lt;/td&gt; &#xA;   &lt;td&gt;0.34&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_easy/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.67&lt;/td&gt; &#xA;   &lt;td&gt;0.68&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;arc_easy/acc_norm&lt;/td&gt; &#xA;   &lt;td&gt;0.62&lt;/td&gt; &#xA;   &lt;td&gt;0.52&lt;/td&gt; &#xA;   &lt;td&gt;0.59&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;boolq/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.75&lt;/td&gt; &#xA;   &lt;td&gt;0.67&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cb/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.36&lt;/td&gt; &#xA;   &lt;td&gt;0.36&lt;/td&gt; &#xA;   &lt;td&gt;0.38&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;cb/f1&lt;/td&gt; &#xA;   &lt;td&gt;0.26&lt;/td&gt; &#xA;   &lt;td&gt;0.24&lt;/td&gt; &#xA;   &lt;td&gt;0.29&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hellaswag/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;0.56&lt;/td&gt; &#xA;   &lt;td&gt;0.47&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;hellaswag/acc_norm&lt;/td&gt; &#xA;   &lt;td&gt;0.66&lt;/td&gt; &#xA;   &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;td&gt;0.63&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openbookqa/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.29&lt;/td&gt; &#xA;   &lt;td&gt;0.29&lt;/td&gt; &#xA;   &lt;td&gt;0.26&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openbookqa/acc_norm&lt;/td&gt; &#xA;   &lt;td&gt;0.38&lt;/td&gt; &#xA;   &lt;td&gt;0.41&lt;/td&gt; &#xA;   &lt;td&gt;0.37&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;piqa/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.75&lt;/td&gt; &#xA;   &lt;td&gt;0.78&lt;/td&gt; &#xA;   &lt;td&gt;0.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;piqa/acc_norm&lt;/td&gt; &#xA;   &lt;td&gt;0.76&lt;/td&gt; &#xA;   &lt;td&gt;0.78&lt;/td&gt; &#xA;   &lt;td&gt;0.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;record/em&lt;/td&gt; &#xA;   &lt;td&gt;0.88&lt;/td&gt; &#xA;   &lt;td&gt;0.91&lt;/td&gt; &#xA;   &lt;td&gt;0.87&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;record/f1&lt;/td&gt; &#xA;   &lt;td&gt;0.89&lt;/td&gt; &#xA;   &lt;td&gt;0.91&lt;/td&gt; &#xA;   &lt;td&gt;0.88&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;rte/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.54&lt;/td&gt; &#xA;   &lt;td&gt;0.56&lt;/td&gt; &#xA;   &lt;td&gt;0.53&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;truthfulqa_mc/mc1&lt;/td&gt; &#xA;   &lt;td&gt;0.20&lt;/td&gt; &#xA;   &lt;td&gt;0.21&lt;/td&gt; &#xA;   &lt;td&gt;0.21&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;truthfulqa_mc/mc2&lt;/td&gt; &#xA;   &lt;td&gt;0.36&lt;/td&gt; &#xA;   &lt;td&gt;0.34&lt;/td&gt; &#xA;   &lt;td&gt;0.34&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wic/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;winogrande/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.64&lt;/td&gt; &#xA;   &lt;td&gt;0.68&lt;/td&gt; &#xA;   &lt;td&gt;0.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;wsc/acc&lt;/td&gt; &#xA;   &lt;td&gt;0.37&lt;/td&gt; &#xA;   &lt;td&gt;0.35&lt;/td&gt; &#xA;   &lt;td&gt;0.57&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Average&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;td&gt;0.52&lt;/td&gt; &#xA;   &lt;td&gt;0.50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Preview Weights Release and Usage&lt;/h2&gt; &#xA;&lt;p&gt;To encourage the feedback from the community, we release a preview checkpoint of our weights. The checkpoint can be downloaded from &lt;a href=&#34;https://huggingface.co/openlm-research/open_llama_7b_preview_200bt&#34;&gt;HuggingFace Hub&lt;/a&gt;. We release the weights in two formats: an EasyLM format to be use with our &lt;a href=&#34;https://github.com/young-geng/EasyLM&#34;&gt;EasyLM framework&lt;/a&gt;, and a PyTorch format to be used with the &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;Huggingface Transformers&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;p&gt;For using the weights in our EasyLM framework, please refer to the &lt;a href=&#34;https://github.com/young-geng/EasyLM/raw/main/docs/llama.md&#34;&gt;LLaMA documentation of EasyLM&lt;/a&gt;. Note that unlike the original LLaMA model, our OpenLLaMA tokenizer and weights are trained completely from scratch so it is no longer needed to obtain the original LLaMA tokenizer and weights. For using the weights in the transformers library, please follow the &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;transformers LLaMA documentation&lt;/a&gt;. Note that we use BOS (beginning of sentence) token (id=1) during training, so it is important to prepend this token for best performance during few-shot evaluation.&lt;/p&gt; &#xA;&lt;p&gt;Both our training framework EasyLM and the preview checkpoint weights are licensed permissively under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Future Plans&lt;/h2&gt; &#xA;&lt;p&gt;The current release is only a preview of what the complete OpenLLaMA release will offer. We are currently focused on completing the training process on the entire RedPajama dataset. This can gives us a good apple-to-apple comparison between the original LLaMA and our OpenLLaMA. Other than the 7B model, we are also training a smaller 3B model in hope of facilitating language model usage in low resource use cases. Please stay tuned for our upcoming releases.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;We would love to get feedback from the community. If you have any questions, please open an issue or contact us.&lt;/p&gt; &#xA;&lt;p&gt;OpenLLaMA is developed by: &lt;a href=&#34;https://young-geng.xyz/&#34;&gt;Xinyang Geng&lt;/a&gt;* and &lt;a href=&#34;https://www.haoliu.site/&#34;&gt;Hao Liu&lt;/a&gt;* from Berkeley AI Research. *Equal Contribution&lt;/p&gt; &#xA;&lt;h2&gt;Reference&lt;/h2&gt; &#xA;&lt;p&gt;If you found OpenLLaMA useful in your research or applications, please cite using the following BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{openlm2023openllama,&#xA;  author = {Geng, Xinyang and Liu, Hao},&#xA;  title = {OpenLLaMA: An Open Reproduction of LLaMA},&#xA;  month = May,&#xA;  year = 2023,&#xA;  url = {https://github.com/openlm-research/open_llama}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{together2023redpajama,&#xA;  author = {Together Computer},&#xA;  title = {RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset},&#xA;  month = April,&#xA;  year = 2023,&#xA;  url = {https://github.com/togethercomputer/RedPajama-Data}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{touvron2023llama,&#xA;  title={Llama: Open and efficient foundation language models},&#xA;  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\&#39;e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},&#xA;  journal={arXiv preprint arXiv:2302.13971},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>