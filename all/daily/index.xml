<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-06T01:23:24Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>allenai/OLMo</title>
    <updated>2024-02-06T01:23:24Z</updated>
    <id>tag:github.com,2024-02-06:/allenai/OLMo</id>
    <link href="https://github.com/allenai/OLMo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Modeling, training, eval, and inference code for OLMo&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;!-- &lt;img src=&#34;https://github.com/allenai/OLMo/assets/8812459/774ac485-a535-4768-8f7c-db7be20f5cc3&#34; width=&#34;300&#34;/&gt; --&gt; &#xA; &lt;img src=&#34;https://allenai.org/olmo/olmo-7b-animation.gif&#34; alt=&#34;OLMo Logo&#34; width=&#34;800&#34; style=&#34;margin-left:&#39;auto&#39; margin-right:&#39;auto&#39; display:&#39;block&#39;&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;br&gt; &#xA; &lt;h1&gt;OLMo: Open Language Model&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/LICENSE&#34;&gt; &lt;img alt=&#34;GitHub License&#34; src=&#34;https://img.shields.io/github/license/allenai/OLMo&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/allenai/OLMo/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/allenai/OLMo.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/pdf/2402.00838.pdf&#34;&gt; &lt;img alt=&#34;Paper URL&#34; src=&#34;https://img.shields.io/badge/arxiv-2402.00838-blue&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;OLMo is a repository for training and using AI2&#39;s state-of-the-art open language models. It is built by scientists, for scientists.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First install &lt;a href=&#34;https://pytorch.org&#34;&gt;PyTorch&lt;/a&gt; according to the instructions specific to your operating system.&lt;/p&gt; &#xA;&lt;p&gt;To install from source (recommended for training/fine-tuning) run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/allenai/OLMo.git&#xA;cd OLMo&#xA;pip install -e .[all]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Otherwise you can install the model code by itself directly from PyPI with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install ai2-olmo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Models overview&lt;/h2&gt; &#xA;&lt;p&gt;The core models in the OLMo family released so far are (all trained on the &lt;a href=&#34;https://huggingface.co/datasets/allenai/dolma&#34;&gt;Dolma dataset&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Training Tokens&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Context Length&lt;/th&gt; &#xA;   &lt;th&gt;Training Config&lt;/th&gt; &#xA;   &lt;th&gt;W&amp;amp;B Logs&lt;/th&gt; &#xA;   &lt;th&gt;Data Order File(s) ☨&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-1B&#34;&gt;OLMo 1B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3 Trillion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs/official/OLMo-1B.yaml&#34;&gt;configs/official/OLMo-1B.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-small/46zc5fly/train_data/global_indices.npy&#34;&gt;Epoch 1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-7B&#34;&gt;OLMo 7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.5 Trillion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs/official/OLMo-7B.yaml&#34;&gt;configs/official/OLMo-7B.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5&#34;&gt;wandb.ai/ai2-llm/OLMo-7B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wvc30anm/train_data/global_indices.npy&#34;&gt;Epoch 1&lt;/a&gt;, &lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wd2gxrza/train_data/global_indices.npy&#34;&gt;Epoch 2&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-7B-Twin-2T&#34;&gt;OLMo 7B Twin 2T&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2 Trillion&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs/official/OLMo-7B.yaml&#34;&gt;configs/official/OLMo-7B.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wvc30anm/train_data/global_indices.npy&#34;&gt;Epoch 1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;☨ &lt;em&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo/main/#inspecting-training-data&#34;&gt;Inspecting training data&lt;/a&gt; below for usage.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;You can utilize our Hugging Face integration to run inference on the olmo checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from hf_olmo import * # registers the Auto* classes&#xA;&#xA;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;olmo = AutoModelForCausalLM.from_pretrained(&#34;allenai/OLMo-7B&#34;)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;allenai/OLMo-7B&#34;)&#xA;&#xA;message = [&#34;Language modeling is &#34;]&#xA;inputs = tokenizer(message, return_tensors=&#39;pt&#39;, return_token_type_ids=False)&#xA;response = olmo.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)&#xA;print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, with the Hugging Face pipeline abstraction:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import pipeline&#xA;olmo_pipe = pipeline(&#34;text-generation&#34;, model=&#34;allenai/OLMo-7B&#34;)&#xA;print(olmo_pipe(&#34;Language modeling is&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference on finetuned checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;If you finetune the model using the code above, you can use the conversion script to convert a native OLMo checkpoint to a Hugging Face-compatible checkpoint&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python hf_olmo/convert_olmo_to_hf.py --checkpoint-dir /path/to/checkpoint&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quantization&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;olmo = AutoModelForCausalLM.from_pretrained(&#34;allenai/OLMo-7B&#34;, torch_dtype=torch.float16, load_in_8bit=True)  # requires bitsandbytes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The quantized model is more sensitive to typing / cuda, so it is recommended to pass the inputs as inputs.input_ids.to(&#39;cuda&#39;) to avoid potential issues.&lt;/p&gt; &#xA;&lt;h2&gt;Reproducibility&lt;/h2&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;The configs used to train the official OLMo models are provided in the &lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs/official&#34;&gt;&lt;code&gt;configs/official/&lt;/code&gt;&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;Note that while the training and validation data is public and free to download, the paths to the data within those configs are pointed at a CloudFlare R2 bucket, which requires an API key for programmatic access. So in order to use any of these configs to reproduce a training run you&#39;ll first have to download the corresponding data to a location of your choosing and then update the paths in the config accordingly.&lt;/p&gt; &#xA;&lt;p&gt;You can derive the public HTTP URL from an R2 URL by replacing &lt;code&gt;r2://olmo-data&lt;/code&gt; with &lt;code&gt;https://olmo-data.org&lt;/code&gt;. For example, if the R2 data URL is:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;r2://olmo-data/preprocessed/olmo-mix/v1_5/gpt-neox-20b-pii-special/part-000-00000.npy&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;then the corresponding public URL is:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;https://olmo-data.org/preprocessed/olmo-mix/v1_5/gpt-neox-20b-pii-special/part-000-00000.npy&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve updated the data paths in the config you can launch a training run via &lt;code&gt;torchrun&lt;/code&gt;. For example, to launch the 1B model training on a single 8x GPU node, you would run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --nproc_per_node=8 scripts/train.py configs/official/OLMo-1B.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use the same method to launch multi-node jobs as well. See &lt;a href=&#34;https://pytorch.org/docs/stable/elastic/run.html&#34;&gt;the documentation&lt;/a&gt; for &lt;code&gt;torchrun&lt;/code&gt; to understand the additional arguments you&#39;ll need to configure the rendezvous backend / endpoint.&lt;/p&gt; &#xA;&lt;h3&gt;Inspecting training data&lt;/h3&gt; &#xA;&lt;p&gt;You may be interesting in inspecting the exact tokens that composed a particular batch during the training of one of the OLMo models. We provide tools to do this, but first you&#39;ll need to download the data as above (unless you have an R2 API key) and update the corresponding config accordingly.&lt;/p&gt; &#xA;&lt;p&gt;Then take note of the URL of the data order file you want, which can be found in the &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo/main/#models-overview&#34;&gt;Models Overview&lt;/a&gt; table. For example, the data order file for the first epoch of the OLMo-7B model is &lt;a href=&#34;https://olmo-checkpoints.org/ai2-llm/olmo-small/46zc5fly/train_data/global_indices.npy&#34;&gt;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wvc30anm/train_data/global_indices.npy&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you have that you can use this snippet to inspect the data within a particular batch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np&#xA;from cached_path import cached_path&#xA;&#xA;from olmo.config import TrainConfig&#xA;from olmo.data import build_memmap_dataset&#xA;&#xA;# Update these paths to what you want:&#xA;data_order_file_path = cached_path(&#34;https://olmo-checkpoints.org/ai2-llm/olmo-medium/wvc30anm/train_data/global_indices.npy&#34;)&#xA;train_config_path = &#34;configs/official/OLMo-7B.yaml&#34;&#xA;&#xA;&#xA;cfg = TrainConfig.load(train_config_path)&#xA;dataset = build_memmap_dataset(cfg, cfg.data)&#xA;batch_size = cfg.global_train_batch_size&#xA;global_indices = np.memmap(data_order_file_path, mode=&#34;r+&#34;, dtype=np.uint32)&#xA;&#xA;&#xA;def get_batch_instances(batch_idx: int) -&amp;gt; list[list[int]]:&#xA;    batch_start = batch_idx * batch_size&#xA;    batch_end = (batch_idx + 1) * batch_size&#xA;    batch_indices = global_indices[batch_start:batch_end]&#xA;    batch_instances = []&#xA;    for index in batch_indices:&#xA;        token_ids = dataset[index][&#34;input_ids&#34;].tolist()&#xA;        batch_instances.append(token_ids)&#xA;    return batch_instances&#xA;&#xA;&#xA;# Get all 2048 x 2048 token IDs in the first batch.&#xA;get_batch_instances(0)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fine-tuning&lt;/h2&gt; &#xA;&lt;p&gt;To fine-tune an OLMo model using our trainer you&#39;ll first need to prepare your dataset by tokenizing it and saving the tokens IDs to a flat numpy memory-mapped array. See &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo/main/scripts/prepare_tulu_data.py&#34;&gt;&lt;code&gt;scripts/prepare_tulu_data.py&lt;/code&gt;&lt;/a&gt; for an example with the Tulu V2 dataset, which can be easily modified for other datasets.&lt;/p&gt; &#xA;&lt;p&gt;Next, prepare your training config. There are many examples in the &lt;a href=&#34;https://github.com/allenai/OLMo/raw/main/configs&#34;&gt;&lt;code&gt;configs/&lt;/code&gt;&lt;/a&gt; directory that you can use as a starting point. The most important thing is to make sure the model parameters (the &lt;code&gt;model&lt;/code&gt; field in the config) match up with the checkpoint you&#39;re starting from. To be safe you can always start from the config that comes with the model checkpoint. At a minimum you&#39;ll need to make the following changes to the config or provide the corresponding overrides from the command line:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Update &lt;code&gt;load_path&lt;/code&gt; to point to the checkpoint you want to start from.&lt;/li&gt; &#xA; &lt;li&gt;Set &lt;code&gt;reset_trainer_state&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Update &lt;code&gt;data.paths&lt;/code&gt; to point to the &lt;code&gt;token_ids.npy&lt;/code&gt; file you generated.&lt;/li&gt; &#xA; &lt;li&gt;Optionally update &lt;code&gt;data.label_mask_paths&lt;/code&gt; to point to the &lt;code&gt;label_mask.npy&lt;/code&gt; file you generated, unless you don&#39;t need special masking for the loss.&lt;/li&gt; &#xA; &lt;li&gt;Update &lt;code&gt;evaluators&lt;/code&gt; to add/remove in-loop evaluations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you&#39;re satisfied with your training config, you can launch the training job via &lt;code&gt;torchrun&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;torchrun --nproc_per_node=8 scripts/train.py {path_to_train_config} \&#xA;    --data.paths=[{path_to_data}/input_ids.npy] \&#xA;    --data.label_mask_paths=[{path_to_data}/label_mask.npy] \&#xA;    --load_path={path_to_checkpoint} \&#xA;    --reset_trainer_state&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: passing CLI overrides like &lt;code&gt;--reset_trainer_state&lt;/code&gt; is only necessary if you didn&#39;t update those fields in your config.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;Additional tools for evaluating OLMo models are available at the &lt;a href=&#34;https://github.com/allenai/ai2-olmo-eval&#34;&gt;OLMo Eval&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h2&gt;Citing&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{OLMo,&#xA;  title={OLMo: Accelerating the Science of Language Models},&#xA;  author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and A. Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Daniel Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hanna Hajishirzi},&#xA;  year={2024},&#xA;  url={https://api.semanticscholar.org/CorpusID:267365485},&#xA;  journal={arXiv preprint},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>allenai/OLMo-Eval</title>
    <updated>2024-02-06T01:23:24Z</updated>
    <id>tag:github.com,2024-02-06:/allenai/OLMo-Eval</id>
    <link href="https://github.com/allenai/OLMo-Eval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Evaluation suite for LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OLMo-Eval&lt;/h1&gt; &#xA;&lt;p&gt;OLMo-Eval is a repository for evaluating open language models.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;code&gt;olmo_eval&lt;/code&gt; framework is a way to run evaluation pipelines for language models on NLP tasks. The codebase is extensible and contains &lt;code&gt;task_sets&lt;/code&gt; and example configurations, which run a series of &lt;a href=&#34;https://github.com/allenai/tango&#34;&gt;&lt;code&gt;tango&lt;/code&gt;&lt;/a&gt; steps for computating the model outputs and metrics.&lt;/p&gt; &#xA;&lt;p&gt;Using this pipeline, you can evaluate &lt;em&gt;m&lt;/em&gt; models on &lt;em&gt;t&lt;/em&gt; task_sets, where each task_set consists of one or more individual tasks. Using task_sets allows you to compute aggregate metrics for multiple tasks. The optional &lt;code&gt;google-sheet&lt;/code&gt; integration can be used for reporting.&lt;/p&gt; &#xA;&lt;p&gt;The pipeline is built using &lt;a href=&#34;https://github.com/allenai/tango&#34;&gt;ai2-tango&lt;/a&gt; and &lt;a href=&#34;https://github.com/allenai/catwalk&#34;&gt;ai2-catwalk&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;After cloning the repository, please run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;conda create -n eval-pipeline python=3.10&#xA;conda activate eval-pipeline&#xA;cd OLMo-Eval&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;p&gt;The current &lt;code&gt;task_sets&lt;/code&gt; can be found at &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/configs/task_sets&#34;&gt;configs/task_sets&lt;/a&gt;. In this example, we run &lt;code&gt;gen_tasks&lt;/code&gt; on &lt;code&gt;EleutherAI/pythia-1b&lt;/code&gt;. The example config is &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/configs/example_config.jsonnet&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The configuration can be run as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;tango --settings tango.yml run configs/example_config.jsonnet --workspace my-eval-workspace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This executes all the steps defined in the config, and saves them in a local &lt;code&gt;tango&lt;/code&gt; workspace called &lt;code&gt;my-eval-workspace&lt;/code&gt;. If you add a new task_set or model to your config and run the same command again, it will reuse the previous outputs, and only compute the new outputs.&lt;/p&gt; &#xA;&lt;p&gt;The output should look like this:&lt;/p&gt; &#xA;&lt;img width=&#34;1886&#34; alt=&#34;Screen Shot 2023-12-04 at 9 22 35 PM&#34; src=&#34;https://github.com/allenai/ai2-llm-eval/assets/6500683/14a74e61-75d8-470c-8bde-12e35c38c44a&#34;&gt; &#xA;&lt;p&gt;New models and datasets can be added by modifying the &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/configs/example_config.jsonnet&#34;&gt;example configuration&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Load pipeline output&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from tango import Workspace&#xA;workspace = Workspace.from_url(&#34;local://my-eval-workspace&#34;)&#xA;result = workspace.step_result(&#34;combine-all-outputs&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Load individual task results with per instance outputs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result = workspace.step_result(&#34;outputs_pythia-1bstep140000_gen_tasks_drop&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Evaluating common models on standard benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/configs/eval_table.jsonnet&#34;&gt;eval_table&lt;/a&gt; config evaluates &lt;code&gt;falcon-7b&lt;/code&gt;, &lt;code&gt;mpt-7b&lt;/code&gt;, &lt;code&gt;llama2-7b&lt;/code&gt;, and &lt;code&gt;llama2-13b&lt;/code&gt;, on &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/configs/task_sets/standard_benchmarks.libsonnet&#34;&gt;&lt;code&gt;standard_benchmarks&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/configs/task_sets/mmlu_tasks.libsonnet&#34;&gt;&lt;code&gt;MMLU&lt;/code&gt;&lt;/a&gt;. Run as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-commandline&#34;&gt;tango --settings tango.yml run configs/eval_table.jsonnet --workspace my-eval-workspace&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;PALOMA&lt;/h2&gt; &#xA;&lt;p&gt;This repository was also used to run evaluations for the &lt;a href=&#34;https://www.semanticscholar.org/paper/Paloma%3A-A-Benchmark-for-Evaluating-Language-Model-Magnusson-Bhagia/1a3f7e23ef8f0bf06d0efa0dc174e4e361226ead?utm_source=direct_link&#34;&gt;PALOMA paper&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Details on running the evaluation on PALOMA can be found &lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/paloma/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Advanced&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/ADVANCED.md#save-output-to-google-sheet&#34;&gt;Save output to google sheet&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/ADVANCED.md#use-a-remote-workspace&#34;&gt;Use a remote workspace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/ADVANCED.md#run-without-tango&#34;&gt;Run without Tango (useful for debugging)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/allenai/OLMo-Eval/main/BEAKER.md&#34;&gt;Run on Beaker&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>X-PLUG/MobileAgent</title>
    <updated>2024-02-06T01:23:24Z</updated>
    <id>tag:github.com,2024-02-06:/X-PLUG/MobileAgent</id>
    <link href="https://github.com/X-PLUG/MobileAgent" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/assets/logo.png?v=1&amp;amp;type=image&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2401.16158&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2401.16158-b31b1b.svg?logo=arXiv&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/papers/2401.16158&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/🤗-Paper%20In%20HF-red.svg&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Junyang Wang&#xA; &lt;sup&gt;1&lt;/sup&gt;, Haiyang Xu&#xA; &lt;sup&gt;2†&lt;/sup&gt;, Jiabo Ye&#xA; &lt;sup&gt;2&lt;/sup&gt;, Ming Yan&#xA; &lt;sup&gt;2†&lt;/sup&gt;, &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Weizhou Shen&#xA; &lt;sup&gt;2&lt;/sup&gt;, Ji Zhang&#xA; &lt;sup&gt;2&lt;/sup&gt;, Fei Huang&#xA; &lt;sup&gt;2&lt;/sup&gt;, Jitao Sang&#xA; &lt;sup&gt;1†&lt;/sup&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  {junyangwang, jtsang}@bjtu.edu.cn, {shuofeng.xhy, ym119608}@alibaba-inc.com &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;1&lt;/sup&gt;Beijing Jiaotong University &#xA; &lt;sup&gt;2&lt;/sup&gt;Alibaba Group &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;sup&gt;†&lt;/sup&gt;Corresponding author &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;📋Introduction&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/assets/example.png?v=1&amp;amp;type=image&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Pure visual solution, independent of XML and system metadata.&lt;/li&gt; &#xA; &lt;li&gt;Unrestricted operation scope, capable of multi-app operations.&lt;/li&gt; &#xA; &lt;li&gt;Multiple visual perception tools for operation localization.&lt;/li&gt; &#xA; &lt;li&gt;No need for exploration and training, plug and play.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📢News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2.5] 🔥🔥We provide a &lt;strong&gt;free&lt;/strong&gt; API and deploy the entire process for experiencing Mobile Agent, even if &lt;strong&gt;you don&#39;t have an OpenAI API Key&lt;/strong&gt;. Check out &lt;a href=&#34;https://raw.githubusercontent.com/X-PLUG/MobileAgent/main/#quick_start&#34;&gt;Quick Start&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2.2] 🔥We are deploying the demo based on Gradio and users will be able to upload the screenshots.&lt;/li&gt; &#xA; &lt;li&gt;[1.31] 🔥Our code is available! Welcome to try Mobile-Agent.&lt;/li&gt; &#xA; &lt;li&gt;[1.31] 🔥Human-operated data in Mobile-Eval is in preparation and will be open-sourced soon.&lt;/li&gt; &#xA; &lt;li&gt;[1.30] Our paper is available at &lt;a href=&#34;https://arxiv.org/abs/2401.16158&#34;&gt;LINK&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[1.30] Our evaluation results on Mobile-Eval are available.&lt;/li&gt; &#xA; &lt;li&gt;[1.30] The code and Mobile-Eval benchmark are coming soon!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📺Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31&#34;&gt;https://github.com/X-PLUG/MobileAgent/assets/127390760/26c48fb0-67ed-4df6-97b2-aa0c18386d31&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔧Preparation&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/X-PLUG/MobileAgent.git&#xA;cd MobileAgent&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Preparation for Connecting Mobile Device&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the &lt;a href=&#34;https://developer.android.com/tools/releases/platform-tools?hl=en&#34;&gt;Android Debug Bridge&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Turn on the ADB debugging switch on your Android phone, it needs to be turned on in the developer options first.&lt;/li&gt; &#xA; &lt;li&gt;Connect your phone to the computer with a data cable and select &#34;Transfer files&#34;.&lt;/li&gt; &#xA; &lt;li&gt;Test your ADB environment as follow: &lt;code&gt;/path/to/adb devices&lt;/code&gt;. If the connected devices are displayed, the preparation is complete.&lt;/li&gt; &#xA; &lt;li&gt;If you are using a MAC or Linux system, make sure to turn on adb permissions as follow: &lt;code&gt;sudo chmod +x /path/to/adb&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;If you are using Windows system, the path will be &lt;code&gt;xx/xx/adb.exe&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;a id=&#34;quick_start&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🔧Quick Start&lt;/h2&gt; &#xA;&lt;h3&gt;Note&lt;/h3&gt; &#xA;&lt;p&gt;❗Since the GPT-4V will have severe hallucinations when perceiving non-English screenshots, we strongly recommend using Mobile-Agent under English-only systems and apps to ensure the performance. ❗Due to current limited resources, please contact us to get a free API Key consisting of a &lt;strong&gt;url&lt;/strong&gt; and a &lt;strong&gt;token&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Email: &lt;a href=&#34;mailto:junyangwang@bjtu.edu.cn&#34;&gt;junyangwang@bjtu.edu.cn&lt;/a&gt;, &lt;a href=&#34;mailto:junyangwang287@gmail.com&#34;&gt;junyangwang287@gmail.com&lt;/a&gt;(If the former cannot be reached)&lt;/li&gt; &#xA; &lt;li&gt;WeChat: Wangjunyang0410&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run_api.py --adb_path /path/to/adb --url &#34;The url you got&#34; --token &#34;The token you got&#34; --instruction &#34;your instruction&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🔧Getting Started with your own API Key&lt;/h2&gt; &#xA;&lt;h3&gt;Preparation for Visual Perception Tools&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Download the icon detection model &lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&#34;&gt;Grounding DION&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The text detection model will be automatically downloaded from modelscope after you run Mobile-Agent.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py --grounding_ckpt /path/to/GroundingDION --adb_path /path/to/adb --api &#34;your API_TOKEN&#34; --instruction &#34;your instruction&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;API_TOKEN is an API Key from OpenAI with the permission to access &lt;code&gt;gpt-4-vision-preview&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;📱Mobile-Eval&lt;/h2&gt; &#xA;&lt;p&gt;Mobile-Eval is a benchmark designed for evaluating the performance of mobile device agents. This benchmark includes 10 mainstream single-app scenarios and 1 multi-app scenario.&lt;/p&gt; &#xA;&lt;p&gt;For each scenario, we have designed three instructions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Instruction 1: relatively simple and basic task&lt;/li&gt; &#xA; &lt;li&gt;Instruction 2: additional requirements added on top of the difficulty of Instruction 1&lt;/li&gt; &#xA; &lt;li&gt;Instruction 3: user demands with no explicit task indication&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The detailed content of Mobile-Eval is as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Application&lt;/th&gt; &#xA;   &lt;th&gt;Instruction&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alibaba.com&lt;/td&gt; &#xA;   &lt;td&gt;1. Help me find caps in Alibaba.com.&lt;br&gt;2. Help me find caps in Alibaba.com. If the &#34;Add to cart&#34; is available in the item information page, please add the item to my cart.&lt;br&gt;3. I want to buy a cap. I&#39;ve heard things are cheap on Alibaba.com. Maybe you can find it for me.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Amazon Music&lt;/td&gt; &#xA;   &lt;td&gt;1. Search singer Jay Chou in Amazon Music.&lt;br&gt;2. Search a music about &#34;agent&#34; in Amazon Music and play it.&lt;br&gt;3. I want to listen music to relax. Find an App to help me.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chrome&lt;/td&gt; &#xA;   &lt;td&gt;1. Search result for today&#39;s Lakers game.&lt;br&gt;2. Search the information about Taylor Swift.&lt;br&gt;3. I want to know the result for today&#39;s Lakers game. Find an App to help me.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gmail&lt;/td&gt; &#xA;   &lt;td&gt;1. Send an empty email to to {address}.&lt;br&gt;2. Send an email to {address}n to tell my new work.&lt;br&gt;3. I want to let my friend know my new work, and his address is {address}. Find an App to help me.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google Maps&lt;/td&gt; &#xA;   &lt;td&gt;1. Navigate to Hangzhou West Lake.&lt;br&gt;2. Navigate to a nearby gas station.&lt;br&gt;3. I want to go to Hangzhou West Lake, but I don&#39;t know the way. Find an App to help me.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google Play&lt;/td&gt; &#xA;   &lt;td&gt;1. Download WhatsApp in Play Store.&lt;br&gt;2. Download Instagram in Play Store.&lt;br&gt;3. I want WhatsApp on my phone. Find an App to help me.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Notes&lt;/td&gt; &#xA;   &lt;td&gt;1. Create a new note in Notes.&lt;br&gt;2. Create a new note in Notes and write &#34;Hello, this is a note&#34;, then save it.&lt;br&gt;3. I suddenly have something to record, so help me find an App and write down the following content: meeting at 3pm.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Settings&lt;/td&gt; &#xA;   &lt;td&gt;1. Turn on the dark mode.&lt;br&gt;2. Turn on the airplane mode.&lt;br&gt;3. I want to see the real time internet speed at the battery level, please turn on this setting for me.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TikTok&lt;/td&gt; &#xA;   &lt;td&gt;1. Swipe a video about pet cat in TikTok and click a &#34;like&#34; for this video.&lt;br&gt;2. Swipe a video about pet cat in TikTok and comment &#34;Ohhhh, so cute cat!&#34;.&lt;br&gt;3. Swipe videos in TikTok. Click &#34;like&#34; for 3 pet video cat.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;YouTube&lt;/td&gt; &#xA;   &lt;td&gt;1. Search for videos about Stephen Curry on YouTube.&lt;br&gt;2. Search for videos about Stephen Curry on YouTube and open &#34;Comments&#34; to comment &#34;Oh, chef, your basketball spirit has always inspired me&#34;.&lt;br&gt;3. I need you to help me show my love for Stephen Curry on YouTube.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi-App&lt;/td&gt; &#xA;   &lt;td&gt;1. Open the calendar and look at today&#39;s date, then go to Notes and create a new note to write &#34;Today is {today&#39;s data}&#34;.&lt;br&gt;2. Check the temperature in the next 5 days, and then create a new note in Notes and write a temperature analysis.&lt;br&gt;3. Search the result for today&#39;s Lakers game, and then create a note in Notes to write a sport news for this result.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;📝Evaluation results&lt;/h2&gt; &#xA;&lt;p&gt;We evaluated Mobile-Agent on Mobile-Eval. The evaluation results are available at &lt;a href=&#34;https://github.com/X-PLUG/MobileAgent/tree/main/results&#34;&gt;LINK&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We have stored the evaluation results for the 10 apps and the multi-app scenario in folders named after each app.&lt;/li&gt; &#xA; &lt;li&gt;The numbers within each app&#39;s folder represent the results for different types of instruction within that app.&lt;/li&gt; &#xA; &lt;li&gt;For example, if you want to view the results of Mobile-Agent for the second instruction in Google Maps, you should go to the following path:&lt;code&gt;results/Google Maps/2&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;If the last action of Mobile-Agent is not &#34;stop&#34;, it indicates that Mobile-Agent did not complete the corresponding instruction. During the evaluation, we manually terminated these cases where completion was not possible.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📄To-do List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Development of Mobile-Agent app on Android platform.&lt;/li&gt; &#xA; &lt;li&gt;Adaptation to other mobile device platforms.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📑Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find Mobile-Agent useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{wang2024mobile,&#xA;  title={Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception},&#xA;  author={Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},&#xA;  journal={arXiv preprint arXiv:2401.16158},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;📦Related Projects&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;GroundingDINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;CLIP: Contrastive Language-Image Pretraining&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>