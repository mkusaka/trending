<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-17T01:28:57Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>grishka/NearDrop</title>
    <updated>2023-04-17T01:28:57Z</updated>
    <id>tag:github.com,2023-04-17:/grishka/NearDrop</id>
    <link href="https://github.com/grishka/NearDrop" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An unofficial Google Nearby Share app for macOS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;NearDrop&lt;/strong&gt; is a partial implementation of &lt;a href=&#34;https://blog.google/products/android/nearby-share/&#34;&gt;Google&#39;s Nearby Share&lt;/a&gt; for macOS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/grishka/NearDrop/master/PROTOCOL.md&#34;&gt;Protocol documentation&lt;/a&gt; is available separately.&lt;/p&gt; &#xA;&lt;p&gt;The app lives in your menu bar and saves files to your downloads folder. It&#39;s that simple, really.&lt;/p&gt; &#xA;&lt;h2&gt;Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Receive only&lt;/strong&gt;. For now. I haven&#39;t yet figured out how to make Android turn on the MDNS service and/or show the &#34;a device nearby is sharing&#34; notification.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Wi-Fi LAN only&lt;/strong&gt;. Your Android device and your Mac need to be on the same network for this app to work. Google&#39;s implementation supports multiple mediums, including Wi-Fi Direct, Wi-Fi hotspot, Bluetooth, some kind of 5G peer-to-peer connection, and even a WebRTC-based protocol that goes over the internet through Google servers. Wi-Fi direct isn&#39;t supported on macOS (Apple has their own, incompatible, AWDL thing, used in AirDrop). Bluetooth needs further reverse engineering.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Visible to everyone on your network at all times&lt;/strong&gt; while the app is running. Limited visibility (contacts etc) requires talking to Google servers, and becoming temporarily visible requires listening for whatever triggers the &#34;device nearby is sharing&#34; notification.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download the latest build from the releases section, unzip, move to your applications folder. When running for the first time, right-click the app and select &#34;Open&#34;, then confirm running an app from unidentified developer.&lt;/p&gt; &#xA;&lt;p&gt;If you want the app to start on boot, add it manually to login objects in System Preferences.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests that change the readme will not be accepted.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h4&gt;Why is the app not notarized?&lt;/h4&gt; &#xA;&lt;p&gt;Because I don&#39;t want to pay Apple $99 a year for the privilege of developing macOS apps and oppose their idea of security.&lt;/p&gt; &#xA;&lt;h4&gt;Why is this not on the app store?&lt;/h4&gt; &#xA;&lt;p&gt;Because I don&#39;t want to pay Apple $99 a year for the privilege of developing macOS apps. I also don&#39;t want to have to go through the review process.&lt;/p&gt; &#xA;&lt;h4&gt;Why not the other way around, i.e. AirDrop on Android?&lt;/h4&gt; &#xA;&lt;p&gt;While I am an Android developer, and I have looked into this, this is nigh-impossible. AirDrop uses &lt;a href=&#34;https://stackoverflow.com/questions/19587701/what-is-awdl-apple-wireless-direct-link-and-how-does-it-work&#34;&gt;AWDL&lt;/a&gt;, Apple&#39;s own proprietary take on peer-to-peer Wi-Fi. This works on top of 802.11 itself, the low-level Wi-Fi protocol, and thus can not be implemented without messing around with the Wi-Fi adapter drivers and raw packets and all that. It might be possible on Android, but it would at the very least require root and possibly a custom kernel. There is &lt;a href=&#34;https://owlink.org/code/&#34;&gt;an open-source implementation of AWDL and AirDrop for Linux&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/AnimatedDrawings</title>
    <updated>2023-04-17T01:28:57Z</updated>
    <id>tag:github.com,2023-04-17:/facebookresearch/AnimatedDrawings</id>
    <link href="https://github.com/facebookresearch/AnimatedDrawings" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code to accompany &#34;A Method for Animating Children&#39;s Drawings of the Human Figure&#34;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Animated Drawings&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/219223438-2c93f9cb-d4b5-45e9-a433-149ed76affa6.gif&#34; alt=&#34;Sequence 02&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repo contains an implementation of the algorithm described in the paper, `A Method for Animating Children&#39;s Drawings of the Human Figure&#39; (to appear in Transactions on Graphics and to be presented at SIGGRAPH 2023).&lt;/p&gt; &#xA;&lt;p&gt;In addition, this repo aims to be a useful creative tool in its own right, allowing you to flexibly create animations starring your own drawn characters. If you do create something fun with this, let us know! Use hashtag &lt;strong&gt;#FAIRAnimatedDrawings&lt;/strong&gt;, or tag me on twitter: &lt;a href=&#34;https://twitter.com/hjessmith/&#34;&gt;@hjessmith&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Project website: &lt;a href=&#34;http://www.fairanimateddrawings.com&#34;&gt;http://www.fairanimateddrawings.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Video overview of &lt;a href=&#34;https://www.youtube.com/watch?v=WsMUKQLVsOI&#34;&gt;Animated Drawings OS Project&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;This project has been tested with macOS Ventura 13.2.1 and Ubuntu 18.04. If you&#39;re installing on another operating sytem, you may encounter issues.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;We &lt;em&gt;strongly&lt;/em&gt; recommend activating a Python virtual environment prior to installing Animated Drawings. Conda&#39;s Miniconda is a great choice. Follow &lt;a href=&#34;https://conda.io/projects/conda/en/stable/user-guide/install/index.html&#34;&gt;these steps&lt;/a&gt; to download and install it. Then run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    # create and activate the virtual environment&#xA;    conda create --name animated_drawings python=3.8.13&#xA;    conda activate animated_drawings&#xA;&#xA;    # clone AnimatedDrawings and use pip to install&#xA;    git clone https://github.com/facebookresearch/AnimatedDrawings.git&#xA;    cd AnimatedDrawings&#xA;    pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Mac M1/M2 users: if you get architecture errors, make sure your &lt;code&gt;~/.condarc&lt;/code&gt; does not have &lt;code&gt;osx-64&lt;/code&gt;, but only &lt;code&gt;osx-arm64&lt;/code&gt; and &lt;code&gt;noarch&lt;/code&gt; in its subdirs listing. You can see that it&#39;s going to go sideways as early as &lt;code&gt;conda create&lt;/code&gt; because it will show &lt;code&gt;osx-64&lt;/code&gt; instead of &lt;code&gt;osx-arm64&lt;/code&gt; versions of libraries under &#34;The following NEW packages will be INSTALLED&#34;.&lt;/p&gt; &#xA;&lt;h2&gt;Using Animated Drawings&lt;/h2&gt; &#xA;&lt;h3&gt;Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;Now that everything&#39;s set up, let&#39;s animate some drawings! To get started, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Open a terminal and activate the animated_drawings conda environment:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;~ % conda activate animated_drawings&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Ensure you&#39;re in the root directory of AnimatedDrawings:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(animated_drawings) ~ % cd {location of AnimatedDrawings on your computer}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Start up a Python interpreter:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(animated_drawings) AnimatedDrawings % python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Copy and paste the follow two lines into the interpreter:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/interactive_window_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If everything is installed correctly, an interactive window should appear on your screen. (Use spacebar to pause/unpause the scene, arrow keys to move back and forth in time, and q to close the screen.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/interactive_window_example.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt; &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;There&#39;s a lot happening behind the scenes here. Characters, motions, scenes, and more are all controlled by configuration files, such as &lt;code&gt;interactive_window_example.yaml&lt;/code&gt;. Below, we show how different effects can be achieved by varying the config files. You can learn more about the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/config/README.md&#34;&gt;config files here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Export MP4 video&lt;/h3&gt; &#xA;&lt;p&gt;Suppose you&#39;d like to save the animation as a video file instead of viewing it directly in a window. Specify a different example config by copying these lines into the Python interpreter:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/export_mp4_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instead of an interactive window, the animation was saved to a file, video.mp4, located in the same directory as your script.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/mp4_export_video.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt; &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Export transparent .gif&lt;/h3&gt; &#xA;&lt;p&gt;Perhaps you&#39;d like a transparent .gif instead of an .mp4? Copy these lines in the Python interpreter intead:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/export_gif_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Instead of an interactive window, the animation was saved to a file, video.gif, located in the same directory as your script.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/gif_export_video.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt; &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Headless Rendering&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;d like to generate a video headlessly (e.g. on a remote server accessed via ssh), you&#39;ll need to specify &lt;code&gt;USE_MESA: True&lt;/code&gt; within the &lt;code&gt;view&lt;/code&gt; section of the config file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;    view:&#xA;      USE_MESA: True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Animating Your Own Drawing&lt;/h3&gt; &#xA;&lt;p&gt;All of the examples above use drawings with pre-existing annotations. To understand what we mean by &lt;em&gt;annotations&lt;/em&gt; here, look at one of the &#39;pre-rigged&#39; character&#39;s &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char1/&#34;&gt;annotation files&lt;/a&gt;. You can use whatever process you&#39;d like to create those annotations files and, as long as they are valid, AnimatedDrawings will give you an animation.&lt;/p&gt; &#xA;&lt;p&gt;So you&#39;d like to animate your own drawn character. I wouldn&#39;t want to you to create those annotation files manually. That would be tedious. To make it fast and easy, we&#39;ve trained a drawn humanoid figure detector and pose estimator and provided scripts to automatically generate annotation files from the model predictions.&lt;/p&gt; &#xA;&lt;p&gt;To get it working, you&#39;ll need to set up a Docker container that runs TorchServe. This allows us to quickly show your image to our machine learning models and receive their predictions.&lt;/p&gt; &#xA;&lt;p&gt;To set up the container, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;Install Docker Desktop&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ensure Docker Desktop is running.&lt;/li&gt; &#xA; &lt;li&gt;Run the following commands, starting from the Animated Drawings root directory:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    (animated_drawings) AnimatedDrawings % cd torchserve&#xA;&#xA;    # build the docker image... this takes a while (~5-7 minutes on Macbook Pro 2021)&#xA;    (animated_drawings) torchserve % docker build -t docker_torchserve .&#xA;&#xA;    # start the docker container and expose the necessary ports&#xA;    (animated_drawings) torchserve % docker run -d --name docker_torchserve -p 8080:8080 -p 8081:8081 docker_torchserve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Wait ~10 seconds, then ensure Docker and TorchServe are working by pinging the server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    (animated_drawings) torchserve % curl http://localhost:8080/ping&#xA;&#xA;    # should return:&#xA;    # {&#xA;    #   &#34;status&#34;: &#34;Healthy&#34;&#xA;    # }&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If, after waiting, the response is &lt;code&gt;curl: (52) Empty reply from server&lt;/code&gt;, one of two things is likely happening.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Torchserve hasn&#39;t finished initializing yet, so wait another 10 seconds and try again.&lt;/li&gt; &#xA; &lt;li&gt;Torchserve is failing because it doesn&#39;t have enough RAM. Try &lt;a href=&#34;https://docs.docker.com/desktop/settings/mac/#advanced&#34;&gt;increasing the amount of memory available to your Docker containers&lt;/a&gt; to 16GB by modifying Docker Desktop&#39;s settings.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;With that set up, you can now go directly from image -&amp;gt; animation with a single command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    (animated_drawings) torchserve % cd ../examples&#xA;    (animated_drawings) examples % python image_to_animation.py drawings/garlic.png garlic_out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As you waited, the image located at &lt;code&gt;drawings/garlic.png&lt;/code&gt; was analyzed, the character detected, segmented, and rigged, and it was animated using BVH motion data from a human actor. The resulting animation was saved as &lt;code&gt;./garlic_out/video.gif&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/drawings/garlic.png&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/garlic.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Fixing bad predictions&lt;/h3&gt; &#xA;&lt;p&gt;You may notice that, when you ran &lt;code&gt;python image_to_animation.py drawings/garlic.png garlic_out&lt;/code&gt;, there were additional non-video files within &lt;code&gt;garlic_out&lt;/code&gt;. &lt;code&gt;mask.png&lt;/code&gt;, &lt;code&gt;texture.png&lt;/code&gt;, and &lt;code&gt;char_cfg.yaml&lt;/code&gt; contain annotation results of the image character analysis step. These annotations were created from our model predictions. If the mask predictions are incorrect, you can edit the mask with an image editing program like Paint or Photoshop. If the joint predictions are incorrect, you can run &lt;code&gt;python fix_annotations.py&lt;/code&gt; to launch a web interface to visualize, correct, and update the annotations. Pass it the location of the folder containing incorrect joint predictions (here we use &lt;code&gt;garlic_out/&lt;/code&gt; as an example):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    (animated_drawings) examples % python fix_annotations.py garlic_out/&#xA;    ...&#xA;     * Running on http://127.0.0.1:5050&#xA;    Press CTRL+C to quit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Navigate to &lt;code&gt;http://127.0.0.1:5050&lt;/code&gt; in your browser to access the web interface. Drag the joints into the appropriate positions, and hit &lt;code&gt;Submit&lt;/code&gt; to save your edits.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve modified the annotations, you can render an animation using them like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    # specify the folder where the fixed annoations are located&#xA;    (animated_drawings) examples % python annotations_to_animation.py garlic_out&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Adding multiple characters to scene&lt;/h3&gt; &#xA;&lt;p&gt;Multiple characters can be added to a video by specifying multiple entries within the config scene&#39;s &#39;ANIMATED_CHARACTERS&#39; list. To see for yourself, run the following commands from a Python interpreter within the AnimatedDrawings root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/multiple_characters_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char1/texture.png&#34; height=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char2/texture.png&#34; height=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/multiple_characters_example.gif&#34; height=&#34;256&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Adding a background image&lt;/h3&gt; &#xA;&lt;p&gt;Suppose you&#39;d like to add a background to the animation. You can do so by specifying the image path within the config. Run the following commands from a Python interpreter within the AnimatedDrawings root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/background_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char4/texture.png&#34; height=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/characters/char4/background.png&#34; height=&#34;256&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/background_example.gif&#34; height=&#34;256&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Using BVH Files with Different Skeletons&lt;/h3&gt; &#xA;&lt;p&gt;You can use any motion clip you&#39;d like, as long as it is in BVH format.&lt;/p&gt; &#xA;&lt;p&gt;If the BVH&#39;s skeleton differs from the examples used in this project, you&#39;ll need to create a new motion config file and retarget config file. Once you&#39;ve done that, you should be good to go. The following code and resulting clip uses a BVH with completely different skeleton. Run the following commands from a Python interpreter within the AnimatedDrawings root directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/different_bvh_skeleton_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/media/different_bvh_skeleton_example.gif&#34; height=&#34;256&#34;&gt; &#xA;&lt;h3&gt;Creating Your Own BVH Files&lt;/h3&gt; &#xA;&lt;p&gt;You may be wondering how you can create BVH files of your own. You used to need a motion capture studio. But now, thankfully, there are simple and accessible options for getting 3D motion data from a single RGB video. For example, I created this Readme&#39;s banner animation by:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Recording myself doing a silly dance with my phone&#39;s camera.&lt;/li&gt; &#xA; &lt;li&gt;Using &lt;a href=&#34;https://www.rokoko.com/&#34;&gt;Rokoko&lt;/a&gt; to export a BVH from my video.&lt;/li&gt; &#xA; &lt;li&gt;Creating a new &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/config/README.md#motion&#34;&gt;motion config file&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/config/README.md#retarget&#34;&gt;retarget config file&lt;/a&gt; to fit the skeleton exported by Rokoko.&lt;/li&gt; &#xA; &lt;li&gt;Using AnimatedDrawings to animate the characters and export a transparent animated gif.&lt;/li&gt; &#xA; &lt;li&gt;Combining the animated gif, original video, and original drawings in Adobe Premiere.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/219223438-2c93f9cb-d4b5-45e9-a433-149ed76affa6.gif&#34; height=&#34;256&#34;&gt; &#xA;&lt;h3&gt;Adding Addition Character Skeletons&lt;/h3&gt; &#xA;&lt;p&gt;All of the example animations above depict &#34;human-like&#34; characters; they have two arms and two legs. Our method is primarily designed with these human-like characters is mind, and the provided pose estimation model assumes a human-like skeleton is present. But you can manually specify a different skeletons within the &lt;code&gt;character config&lt;/code&gt; and modify the specified &lt;code&gt;retarget config&lt;/code&gt; to support it. If you&#39;re interested, look at the configuration files specified in the two examples below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/six_arms_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/223584962-925ee5aa-11de-47e5-ace2-a6d5940b34ae.png&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/223585000-dc8acf4e-974d-4cae-998b-94543f5f42c8.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from animated_drawings import render&#xA;render.start(&#39;./examples/config/mvc/four_legs_example.yaml&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/223585033-f11e4e66-0443-405a-80e5-09b6aa0e335d.png&#34; height=&#34;256&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/6675724/223585043-7ce9eac0-bb4c-4547-b038-c63ca2852ef2.gif&#34; width=&#34;256&#34; height=&#34;256&#34;&gt;&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Creating Your Own Config Files&lt;/h3&gt; &#xA;&lt;p&gt;If you want to create your own config files, see the &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/AnimatedDrawings/main/examples/config/README.md&#34;&gt;configuration file documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Browser-Based Demo&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;d like to animate a drawing of your own, but don&#39;t want to deal with downloading code and using the command line, check out our browser-based demo:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://sketch.metademolab.com/&#34;&gt;www.sketch.metademolab.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Animated Drawings is released under the &lt;a href=&#34;https://github.com/fairinternal/AnimatedDrawings/raw/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;i&gt;The accompanying paper has been accepted to Transactions on Graphics and will be presented at SIGGRAPH 2023. Official citation will be added later, but for now you can access the paper on arxiv: &lt;a href=&#34;https://arxiv.org/abs/2303.12741&#34;&gt;A Method for Animating Children&#39;s Drawings of The Human Figure&lt;/a&gt; &lt;/i&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you find this repo helpful in your own work, please consider citing the accompanying paper:&lt;/p&gt; &#xA;&lt;p&gt;(citation information to be added later)&lt;/p&gt; &#xA;&lt;h2&gt;Amateur Drawings Dataset&lt;/h2&gt; &#xA;&lt;p&gt;To obtain the Amateur Drawings Dataset, run the following two commands from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download annotations (~275Mb)&#xA;wget https://dl.fbaipublicfiles.com/amateur_drawings/amateur_drawings_annotations.json&#xA;&#xA;# download images (~50Gb)&#xA;wget https://dl.fbaipublicfiles.com/amateur_drawings/amateur_drawings.tar&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you have feedback about the dataset, please fill out &lt;a href=&#34;https://forms.gle/kE66yskh9uhtLbFz9&#34;&gt;this form&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Trained Model Weights&lt;/h2&gt; &#xA;&lt;p&gt;Trained model weights for human-like figure detection and pose estimation are included in the &lt;a href=&#34;https://github.com/facebookresearch/AnimatedDrawings/releases&#34;&gt;repo releases&lt;/a&gt;. Model weights are released under &lt;a href=&#34;https://github.com/facebookresearch/AnimatedDrawings/raw/main/LICENSE&#34;&gt;MIT license&lt;/a&gt;. The .mar files were generated using the OpenMMLab framework (&lt;a href=&#34;https://github.com/open-mmlab/mmdetection/raw/main/LICENSE&#34;&gt;OpenMMDet Apache 2.0 License&lt;/a&gt;, &lt;a href=&#34;https://github.com/open-mmlab/mmpose/raw/main/LICENSE&#34;&gt;OpenMMPose Apache 2.0 License&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;h2&gt;As-Rigid-As-Possible Shape Manipulation&lt;/h2&gt; &#xA;&lt;p&gt;These characters are deformed using &lt;a href=&#34;https://www-ui.is.s.u-tokyo.ac.jp/~takeo/papers/takeo_jgt09_arapFlattening.pdf&#34;&gt;As-Rigid-As-Possible (ARAP) shape manipulation&lt;/a&gt;. We have a Python implementation of the algorithm, located &lt;a href=&#34;https://github.com/fairinternal/AnimatedDrawings/raw/main/animated_drawings/model/arap.py&#34;&gt;here&lt;/a&gt;, that might be of use to other developers.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ericciarla/babyagijs</title>
    <updated>2023-04-17T01:28:57Z</updated>
    <id>tag:github.com,2023-04-17:/ericciarla/babyagijs</id>
    <link href="https://github.com/ericciarla/babyagijs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI-powered task management system in Javascript&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;BabyAGI JS&lt;/h1&gt; &#xA;&lt;p&gt;BabyAGI JS is a JavaScript-based AI agent that creates, prioritizes, and executes tasks using the GPT 3.5 or GPT 4 architecture. It integrates with OpenAI&#39;s language model to create a powerful AI that can handle a wide range of tasks.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Task creation: Generates new tasks based on the current context and objectives.&lt;/li&gt; &#xA; &lt;li&gt;Task prioritization: Reorders tasks according to their importance and relevance to the main objective.&lt;/li&gt; &#xA; &lt;li&gt;Task execution: Performs tasks and returns results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to use&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repository.&lt;/li&gt; &#xA; &lt;li&gt;Add API keys to your .env&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install the required dependencies using &lt;code&gt;npm install&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Write your code in the &lt;code&gt;src&lt;/code&gt; directory.&lt;/li&gt; &#xA; &lt;li&gt;Run your program with &lt;code&gt;npm run start&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Main Files&lt;/h2&gt; &#xA;&lt;h3&gt;&lt;code&gt;src/index.ts&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This file initializes the BabyAGI agent with the required configurations, including the language model and objective. It imports the &lt;code&gt;BabyAGI&lt;/code&gt; class from &lt;code&gt;babyagi.js&lt;/code&gt; and creates a new instance to perform tasks based on the given objective.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;src/babyagi.ts&lt;/code&gt;&lt;/h3&gt; &#xA;&lt;p&gt;This file contains the core implementation of the BabyAGI agent. It defines three main classes, &lt;code&gt;TaskCreationChain&lt;/code&gt;, &lt;code&gt;TaskPrioritizationChain&lt;/code&gt;, and &lt;code&gt;ExecutionChain&lt;/code&gt;, which are responsible for creating, prioritizing, and executing tasks, respectively.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;BabyAGI&lt;/code&gt; class combines these three classes and provides methods to add tasks, print tasks, and execute tasks. The &lt;code&gt;call&lt;/code&gt; method is the main entry point to start the agent&#39;s task processing loop.&lt;/p&gt; &#xA;&lt;h2&gt;Example&lt;/h2&gt; &#xA;&lt;p&gt;The following is an example of how to use the BabyAGI agent:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set the objective in &lt;code&gt;src/index.ts&lt;/code&gt;:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-javascript&#34;&gt;const OBJECTIVE = &#39;Integrate stripe in typescript&#39;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Run the program with &lt;code&gt;npm run start&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The BabyAGI agent will create, prioritize, and execute tasks based on the given objective.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions to improve BabyAGI JS. Feel free to open an issue or submit a pull request.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License.&lt;/p&gt;</summary>
  </entry>
</feed>