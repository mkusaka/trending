<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-28T01:28:02Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Pierian-Data/Complete-Python-3-Bootcamp</title>
    <updated>2024-08-28T01:28:02Z</updated>
    <id>tag:github.com,2024-08-28:/Pierian-Data/Complete-Python-3-Bootcamp</id>
    <link href="https://github.com/Pierian-Data/Complete-Python-3-Bootcamp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Complete-Python-3-Bootcamp&lt;/h1&gt; &#xA;&lt;p&gt;Course Files for Complete Python 3 Bootcamp Course on Udemy&lt;/p&gt; &#xA;&lt;p&gt;Copyright(¬©) by Pierian Data Inc.&lt;/p&gt; &#xA;&lt;p&gt;Get it now for 95% off with the link: &lt;a href=&#34;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&#34;&gt;https://www.udemy.com/complete-python-bootcamp/?couponCode=COMPLETE_GITHUB&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PromtEngineer/Verbi</title>
    <updated>2024-08-28T01:28:02Z</updated>
    <id>tag:github.com,2024-08-28:/PromtEngineer/Verbi</id>
    <link href="https://github.com/PromtEngineer/Verbi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modular voice assistant application for experimenting with state-of-the-art transcription, response generation, and text-to-speech models. Supports OpenAI, Groq, Elevanlabs, CartesiaAI, and Deepgram APIs, plus local models via Ollama. Ideal for research and development in voice technology.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;VERBI - Voice Assistant üéôÔ∏è&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/PromtEngineer/Verbi/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PromtEngineer/Verbi?style=social&#34; alt=&#34;GitHub Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PromtEngineer/Verbi/network/members&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/forks/PromtEngineer/Verbi?style=social&#34; alt=&#34;GitHub Forks&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PromtEngineer/Verbi/issues&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues/PromtEngineer/Verbi&#34; alt=&#34;GitHub Issues&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PromtEngineer/Verbi/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/PromtEngineer/Verbi&#34; alt=&#34;GitHub Pull Requests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PromtEngineer/Verbi/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/PromtEngineer/Verbi&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Motivation ‚ú®‚ú®‚ú®&lt;/h2&gt; &#xA;&lt;p&gt;Welcome to the Voice Assistant project! üéôÔ∏è Our goal is to create a modular voice assistant application that allows you to experiment with state-of-the-art (SOTA) models for various components. The modular structure provides flexibility, enabling you to pick and choose between different SOTA models for transcription, response generation, and text-to-speech (TTS). This approach facilitates easy testing and comparison of different models, making it an ideal platform for research and development in voice assistant technologies. Whether you&#39;re a developer, researcher, or enthusiast, this project is for you!&lt;/p&gt; &#xA;&lt;h2&gt;Features üß∞&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modular Design&lt;/strong&gt;: Easily switch between different models for transcription, response generation, and TTS.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support for Multiple APIs&lt;/strong&gt;: Integrates with OpenAI, Groq, and Deepgram APIs, along with placeholders for local models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Audio Recording and Playback&lt;/strong&gt;: Record audio from the microphone and play generated speech.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Configuration Management&lt;/strong&gt;: Centralized configuration in &lt;code&gt;config.py&lt;/code&gt; for easy setup and management.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Project Structure üìÇ&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-plaintext&#34;&gt;voice_assistant/&#xA;‚îú‚îÄ‚îÄ voice_assistant/&#xA;‚îÇ   ‚îú‚îÄ‚îÄ __init__.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ audio.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ api_key_manager.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ config.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ transcription.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ response_generation.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ text_to_speech.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ utils.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ local_tts_api.py&#xA;‚îÇ   ‚îú‚îÄ‚îÄ local_tts_generation.py&#xA;‚îú‚îÄ‚îÄ .env&#xA;‚îú‚îÄ‚îÄ run_voice_assistant.py&#xA;‚îú‚îÄ‚îÄ setup.py&#xA;‚îú‚îÄ‚îÄ requirements.txt&#xA;‚îî‚îÄ‚îÄ README.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Setup Instructions üìã&lt;/h2&gt; &#xA;&lt;h4&gt;Prerequisites ‚úÖ&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.10 or higher&lt;/li&gt; &#xA; &lt;li&gt;Virtual environment (recommended)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Step-by-Step Instructions üî¢&lt;/h4&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;üì• &lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   git clone https://github.com/PromtEngineer/Verbi.git&#xA;   cd Verbi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;üêç &lt;strong&gt;Set up a virtual environment&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Using &lt;code&gt;venv&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;    python -m venv venv&#xA;    source venv/bin/activate  # On Windows use `venv\Scripts\activate`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Using &lt;code&gt;conda&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;    conda create --name verbi python=3.10&#xA;    conda activate verbi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;üì¶ &lt;strong&gt;Install the required packages&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;üõ†Ô∏è &lt;strong&gt;Set up the environment variables&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root directory and add your API keys:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;    OPENAI_API_KEY=your_openai_api_key&#xA;    GROQ_API_KEY=your_groq_api_key&#xA;    DEEPGRAM_API_KEY=your_deepgram_api_key&#xA;    LOCAL_MODEL_PATH=path/to/local/model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;üß© &lt;strong&gt;Configure the models&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Edit config.py to select the models you want to use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;    class Config:&#xA;        # Model selection&#xA;        TRANSCRIPTION_MODEL = &#39;groq&#39;  # Options: &#39;openai&#39;, &#39;groq&#39;, &#39;deepgram&#39;, &#39;fastwhisperapi&#39; &#39;local&#39;&#xA;        RESPONSE_MODEL = &#39;groq&#39;       # Options: &#39;openai&#39;, &#39;groq&#39;, &#39;ollama&#39;, &#39;local&#39;&#xA;        TTS_MODEL = &#39;deepgram&#39;        # Options: &#39;openai&#39;, &#39;deepgram&#39;, &#39;elevenlabs&#39;, &#39;local&#39;, &#39;melotts&#39;&#xA;&#xA;        # API keys and paths&#xA;        OPENAI_API_KEY = os.getenv(&#34;OPENAI_API_KEY&#34;)&#xA;        GROQ_API_KEY = os.getenv(&#34;GROQ_API_KEY&#34;)&#xA;        DEEPGRAM_API_KEY = os.getenv(&#34;DEEPGRAM_API_KEY&#34;)&#xA;        LOCAL_MODEL_PATH = os.getenv(&#34;LOCAL_MODEL_PATH&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are running LLM locally via &lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt;, make sure the Ollama server is runnig before starting verbi.&lt;/p&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;üîä &lt;strong&gt;Configure ElevenLabs Jarvis&#39; Voice&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Voice samples &lt;a href=&#34;https://github.com/PromtEngineer/Verbi/tree/main/voice_samples&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Follow this &lt;a href=&#34;https://elevenlabs.io/app/voice-lab/share/de3746fa51a09e771604d74b5d1ff6797b6b96a5958f9de95cef544dde31dad9/WArWzu0z4mbSyy5BfRKM&#34;&gt;link&lt;/a&gt; to add the Jarvis voice to your ElevenLabs account.&lt;/li&gt; &#xA; &lt;li&gt;Name the voice &#39;Paul J.&#39; or, if you prefer a different name, ensure it matches the ELEVENLABS_VOICE_ID variable in the text_to_speech.py file.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;üèÉ &lt;strong&gt;Run the voice assistant&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   python run_voice_assistant.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;üé§ &lt;strong&gt;Install FastWhisperAPI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Optional step if you need a local transcription model&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Clone the repository&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   cd..&#xA;   git clone https://github.com/3choff/FastWhisperAPI.git&#xA;   cd FastWhisperAPI&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Install the required packages:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Run the API&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   fastapi run main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Alternative Setup and Run Methods&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The API can also run directly on a Docker container or in Google Colab.&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Docker:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Build a Docker container:&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   docker build -t fastwhisperapi .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Run the container&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   docker run -p 8000:8000 fastwhisperapi&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Refer to the repository documentation for the Google Colab method: &lt;a href=&#34;https://github.com/3choff/FastWhisperAPI/raw/main/README.md&#34;&gt;https://github.com/3choff/FastWhisperAPI/blob/main/README.md&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üé§ &lt;strong&gt;Install Local TTS - MeloTTS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Optional step if you need a local Text to Speech model&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Install MeloTTS from Github&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Use the following &lt;a href=&#34;https://github.com/myshell-ai/MeloTTS/raw/main/docs/install.md#linux-and-macos-install&#34;&gt;link&lt;/a&gt; to install MeloTTS for your operating system.&lt;/p&gt; &lt;p&gt;Once the package is installed on your local virtual environment, you can start the api server using the following command.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   python voice_assistant/local_tts_api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;local_tts_api.py&lt;/code&gt; file implements as fastapi server that will listen to incoming text and will generate audio using MeloTTS model. In order to use the local TTS model, you will need to update the &lt;code&gt;config.py&lt;/code&gt; file by setting:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;   TTS_MODEL = &#39;melotts&#39;        # Options: &#39;openai&#39;, &#39;deepgram&#39;, &#39;elevenlabs&#39;, &#39;local&#39;, &#39;melotts&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can run the main file to start using verbi with local models.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Model Options ‚öôÔ∏è&lt;/h2&gt; &#xA;&lt;h4&gt;Transcription Models üé§&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: Uses OpenAI&#39;s Whisper model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Groq&lt;/strong&gt;: Uses Groq&#39;s Whisper-large-v3 model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deepgram&lt;/strong&gt;: Uses Deepgram&#39;s transcription model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;FastWhisperAPI&lt;/strong&gt;: Uses FastWhisperAPI, a local transcription API powered by Faster Whisper.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local&lt;/strong&gt;: Placeholder for a local speech-to-text (STT) model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Response Generation Models üí¨&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: Uses OpenAI&#39;s GPT-4 model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Groq&lt;/strong&gt;: Uses Groq&#39;s LLaMA model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt;: Uses any model served via Ollama.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local&lt;/strong&gt;: Placeholder for a local language model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Text-to-Speech (TTS) Models üîä&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;: Uses OpenAI&#39;s TTS model with the &#39;fable&#39; voice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deepgram&lt;/strong&gt;: Uses Deepgram&#39;s TTS model with the &#39;aura-angus-en&#39; voice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;ElevenLabs&lt;/strong&gt;: Uses ElevenLabs&#39; TTS model with the &#39;Paul J.&#39; voice.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Local&lt;/strong&gt;: Placeholder for a local TTS model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Detailed Module Descriptions üìò&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;run_verbi.py&lt;/code&gt;&lt;/strong&gt;: Main script to run the voice assistant.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/config.py&lt;/code&gt;&lt;/strong&gt;: Manages configuration settings and API keys.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/api_key_manager.py&lt;/code&gt;&lt;/strong&gt;: Handles retrieval of API keys based on configured models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/audio.py&lt;/code&gt;&lt;/strong&gt;: Functions for recording and playing audio.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/transcription.py&lt;/code&gt;&lt;/strong&gt;: Manages audio transcription using various APIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/response_generation.py&lt;/code&gt;&lt;/strong&gt;: Handles generating responses using various language models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/text_to_speech.py&lt;/code&gt;&lt;/strong&gt;: Manages converting text responses into speech.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/utils.py&lt;/code&gt;&lt;/strong&gt;: Contains utility functions like deleting files.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/local_tts_api.py&lt;/code&gt;&lt;/strong&gt;: Contains the api implementation to run the MeloTTS model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/local_tts_generation.py&lt;/code&gt;&lt;/strong&gt;: Contains the code to use the MeloTTS api to generated audio.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;voice_assistant/__init__.py&lt;/code&gt;&lt;/strong&gt;: Initializes the &lt;code&gt;voice_assistant&lt;/code&gt; package.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap üõ§Ô∏èüõ§Ô∏èüõ§Ô∏è&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s what&#39;s next for the Voice Assistant project:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Add Support for Streaming&lt;/strong&gt;: Enable real-time streaming of audio input and output.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Add Support for ElevenLabs and Enhanced Deepgram for TTS&lt;/strong&gt;: Integrate additional TTS options for higher quality and variety.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Add Filler Audios&lt;/strong&gt;: Include background or filler audios while waiting for model responses to enhance user experience.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Add Support for Local Models Across the Board&lt;/strong&gt;: Expand support for local models in transcription, response generation, and TTS.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing ü§ù&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the community! If you&#39;d like to help improve this project, please follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fork the repository.&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch (&lt;code&gt;git checkout -b feature-branch&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Make your changes and commit them (&lt;code&gt;git commit -m &#39;Add new feature&#39;&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Push to the branch (&lt;code&gt;git push origin feature-branch&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Open a pull request detailing your changes.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Star History ‚ú®‚ú®‚ú®&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#PromtEngineer/Verbi&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=PromtEngineer/Verbi&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>khcrysalis/Feather</title>
    <updated>2024-08-28T01:28:02Z</updated>
    <id>tag:github.com,2024-08-28:/khcrysalis/Feather</id>
    <link href="https://github.com/khcrysalis/Feather" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Feather is a free on-device iOS application manager/installer built with UIKit for quality.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img width=&#34;100&#34; height=&#34;100&#34; src=&#34;https://raw.githubusercontent.com/khcrysalis/Feather/main/Images/512@2x.png&#34; style=&#34;margin-right: -15px;&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h1&gt;Feather&lt;/h1&gt; &#xA;&lt;p&gt; Feather is a free on-device iOS application manager/installer built with UIKit for quality. &lt;/p&gt; &#xA;&lt;h4&gt;README In Other Languages&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/khcrysalis/Feather/raw/main/README_ru.md&#34;&gt;–†—É—Å—Å–∫–∏–πüá∑üá∫&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Altstore repo support&lt;/strong&gt;. &lt;em&gt;Supporting Legacy and 2.0 repo structures&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Import your own &lt;code&gt;.ipa&lt;/code&gt;&#39;s&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Inject tweaks when signing apps&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install applications straight to your device seamlessly over the air&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Allows multiple certificate imports for easy switching&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Configurable signing options&lt;/strong&gt;. &lt;em&gt;(name, bundleid, version, other plist options)&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Meant to be used with Apple Accounts that are apart of &lt;code&gt;ADP&lt;/code&gt; (Apple Developer Program)&lt;/strong&gt;. &lt;em&gt;however other certificates can also work!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Easy resigning&lt;/strong&gt;! &lt;em&gt;If you have another certificate you would like to use on an app you may resign and reinstall that same app!&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;No tracking, analytics, or any of the sort&lt;/strong&gt;. &lt;em&gt;Your information such as udid and certificates will never leave the device.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;Tweak support is in beta&lt;/strong&gt;, make sure your tweaks work on the &lt;a href=&#34;https://theapplewiki.com/wiki/ElleKit&#34;&gt;Ellekit&lt;/a&gt; hooking platform, and built with the latest version of theos.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Some tweaks, not all, should work with Feather.&lt;/strong&gt; However, don&#39;t expect tweaks to work out of the box. As we will not change any dylib load command that isn&#39;t CydiaSubstrate.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/khcrysalis/Feather/issues/26&#34;&gt;Visit the roadmap here!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;p align=&#34;center&#34;&gt;&#xA;     &lt;picture&gt;&#xA;      &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;Images/Repos.png&#34;&gt;&#xA;      &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;Images/Repos_L.png&#34;&gt;&#xA;      &lt;img alt=&#34;Pointercrate-pocket.&#34; src=&#34;https://raw.githubusercontent.com/khcrysalis/Feather/main/Images/Repos_L.png&#34; width=&#34;200&#34;&gt;&#xA;     &lt;/picture&gt;&lt;/p&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;p align=&#34;center&#34;&gt;&#xA;     &lt;picture&gt;&#xA;      &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;Images/Store.png&#34;&gt;&#xA;      &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;Images/Store_L.png&#34;&gt;&#xA;      &lt;img alt=&#34;Pointercrate-pocket.&#34; src=&#34;https://raw.githubusercontent.com/khcrysalis/Feather/main/Images/Store_L.png&#34; width=&#34;200&#34;&gt;&#xA;     &lt;/picture&gt;&lt;/p&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;p align=&#34;center&#34;&gt;&#xA;     &lt;picture&gt;&#xA;      &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;Images/Library.png&#34;&gt;&#xA;      &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;Images/Library_L.png&#34;&gt;&#xA;      &lt;img alt=&#34;Pointercrate-pocket.&#34; src=&#34;https://raw.githubusercontent.com/khcrysalis/Feather/main/Images/Library_L.png&#34; width=&#34;200&#34;&gt;&#xA;     &lt;/picture&gt;&lt;/p&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;p align=&#34;center&#34;&gt;&#xA;     &lt;picture&gt;&#xA;      &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;Images/Sign.png&#34;&gt;&#xA;      &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;Images/Sign_L.png&#34;&gt;&#xA;      &lt;img alt=&#34;Pointercrate-pocket.&#34; src=&#34;https://raw.githubusercontent.com/khcrysalis/Feather/main/Images/Sign_L.png&#34; width=&#34;200&#34;&gt;&#xA;     &lt;/picture&gt;&lt;/p&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Sources&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Store&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Library&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Signing&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Tip: Go into lightmode to see lightmode screenshots!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;How it Works&lt;/h2&gt; &#xA;&lt;p&gt;Feather allows you to import a &lt;code&gt;.p12&lt;/code&gt; and a &lt;code&gt;.mobileprovision&lt;/code&gt; pair to sign the application with (you will need a correct password to the p12 before importing). &lt;a href=&#34;https://github.com/zhlynn/zsign&#34;&gt;Zsign&lt;/a&gt; is used for the signing aspect, feather feeds it the certificates you have selected in its certificates tab and will sign the app on your device - after its finished it will now be added to your signed applications tab. When selected, it will take awhile as its compressing and will prompt you to install it.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;What does feather use for its server?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;It uses the &lt;a href=&#34;https://github.com/Upinel/localhost.direct&#34;&gt;localhost.direct&lt;/a&gt; certificate and &lt;a href=&#34;https://github.com/vapor/vapor&#34;&gt;Vapor&lt;/a&gt; to self host an HTTPS server on your device - all itms services really needs is a valid certificate and a valid HTTPS server. Which allows iOS to accept the request and install the application.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Why does Feather append a random string on the bundle ID?&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;New ADP (Apple Developer Program) memberships created after June 6, 2021, require development and ad-hoc signed apps for iOS, iPadOS, and tvOS to check with a PPQ (Provisioning Profile Query Check) service when the app is first launched. The device must be connected to the internet to verify.&lt;/p&gt; &#xA;&lt;p&gt;PPQCheck checks for a similar bundle identifier on the App Store, if said identifier matches the app you&#39;re launching and is happened to be signed with a non-appstore certificate, your Apple ID may be flagged and even banned from using the program for any longer.&lt;/p&gt; &#xA;&lt;p&gt;This is why we prepend the random string before each identifier, its done as a safety meassure - however you can disable it if you &lt;em&gt;really&lt;/em&gt; want to in Feathers settings page.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;NOTE: IF YOU WANT TO KEEP APPLICATION DATA THROUGH REINSTALLS, MAKE SURE YOU HAVE THE SAME BUNDLEID.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Building&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/khcrysalis/feather # Clone&#xA;cd feather&#xA;make package SCHEME=&#34;&#39;feather (Release)&#39;&#34; # Build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Use &lt;code&gt;SCHEME=&#34;&#39;feather (Debug)&#39;&#34;&lt;/code&gt; for debug build&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Upinel/localhost.direct&#34;&gt;localhost.direct&lt;/a&gt; - localhost with public CA signed SSL certificate&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/vapor/vapor&#34;&gt;Vapor&lt;/a&gt; - A server-side Swift HTTP web framework.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/zhlynn/zsign&#34;&gt;Zsign&lt;/a&gt; - Allowing to sign on-device, reimplimented to work on other platforms such as iOS.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kean/Nuke&#34;&gt;Nuke&lt;/a&gt; - Image caching.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lakr233/Asspp&#34;&gt;Asspp&lt;/a&gt; - Some code for setting up the http server.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - [plistserver](https://github.com/QuickSign-Team/plistserver) - Hosted on https://api.palera.in&#xA;&gt; NOTE: The original license to plistserver is [GPL](https://github.com/nekohaxx/plistserver/commit/b207a76a9071a695d8b498db029db5d63a954e53), so changing the license is NOT viable as technically it&#39;s irrevocable. We are allowed to host it on our own server for use in Feather by technicality.  --&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;a href=&#34;https://star-history.com/#khcrysalis/feather&amp;amp;Date&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=khcrysalis/feather&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=khcrysalis/feather&amp;amp;type=Date&#34;&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=khcrysalis/feather&amp;amp;type=Date&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Contributions&lt;/h2&gt; &#xA;&lt;p&gt;They are welcome! :)&lt;/p&gt; &#xA;&lt;h2&gt;History&lt;/h2&gt; &#xA;&lt;p&gt;There was a tool called ESign (Easy Sign) that would allow you to sideload applications seamlessly on device, however it was discovered it sadly sends analytics over to some other location. There were stuff that supposedly removed the analytics but it&#39;s hard to decipher if it actually removed the problem at hand.&lt;/p&gt; &#xA;&lt;p&gt;So I decided to make an alternative with similar features so I don&#39;t need to use that tool, along with me an others. A lot of research has been done to get this working, and originally got it working a few months ago for the first time! Of course without the help with Dhinakg in discovering you can actually use a local server to deploy an app on your device!&lt;/p&gt; &#xA;&lt;p&gt;And now we&#39;re here! Hopefully this satisfies most people that want to sideload with their developer account or in general!&lt;/p&gt;</summary>
  </entry>
</feed>