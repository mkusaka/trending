<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-06-29T01:30:23Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>embedchain/embedchain</title>
    <updated>2023-06-29T01:30:23Z</updated>
    <id>tag:github.com,2023-06-29:/embedchain/embedchain</id>
    <link href="https://github.com/embedchain/embedchain" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Framework to easily create LLM powered bots over any dataset.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;embedchain&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/nhvCbCtKV&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/nhvCbCtKV?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/pypi/v/embedchain&#34; alt=&#34;PyPI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;embedchain is a framework to easily create LLM powered bots over any dataset.&lt;/p&gt; &#xA;&lt;p&gt;It abstracts the entire process of loading a dataset, chunking it, creating embeddings and then storing in a vector database.&lt;/p&gt; &#xA;&lt;p&gt;You can add a single or multiple dataset using &lt;code&gt;.add&lt;/code&gt; and &lt;code&gt;.add_local&lt;/code&gt; function and then use &lt;code&gt;.query&lt;/code&gt; function to find an answer from the added datasets.&lt;/p&gt; &#xA;&lt;p&gt;If you want to create a Naval Ravikant bot which has 1 youtube video, 1 book as pdf and 2 of his blog posts, as well as a question and answer pair you supply, all you need to do is add the links to the videos, pdf and blog posts and the QnA pair and embedchain will create a bot for you.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from embedchain import App&#xA;&#xA;naval_chat_bot = App()&#xA;&#xA;# Embed Online Resources&#xA;naval_chat_bot.add(&#34;youtube_video&#34;, &#34;https://www.youtube.com/watch?v=3qHkcs3kG44&#34;)&#xA;naval_chat_bot.add(&#34;pdf_file&#34;, &#34;https://navalmanack.s3.amazonaws.com/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_Final.pdf&#34;)&#xA;naval_chat_bot.add(&#34;web_page&#34;, &#34;https://nav.al/feedback&#34;)&#xA;naval_chat_bot.add(&#34;web_page&#34;, &#34;https://nav.al/agi&#34;)&#xA;&#xA;# Embed Local Resources&#xA;naval_chat_bot.add_local(&#34;qna_pair&#34;, (&#34;Who is Naval Ravikant?&#34;, &#34;Naval Ravikant is an Indian-American entrepreneur and investor.&#34;))&#xA;&#xA;naval_chat_bot.query(&#34;What unique capacity does Naval argue humans possess when it comes to understanding explanations or concepts?&#34;)&#xA;# answer: Naval argues that humans possess the unique capacity to understand explanations or concepts to the maximum extent possible in this physical reality.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Getting Started&lt;/h1&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First make sure that you have the package installed. If not, then install it using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install embedchain&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;We use OpenAI&#39;s embedding model to create embeddings for chunks and ChatGPT API as LLM to get answer given the relevant docs. Make sure that you have an OpenAI account and an API key. If you have dont have an API key, you can create one by visiting &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;this link&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Once you have the API key, set it in an environment variable called &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;sk-xxxx&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Next import the &lt;code&gt;App&lt;/code&gt; class from embedchain and use &lt;code&gt;.add&lt;/code&gt; function to add any dataset.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#xA;from embedchain import App&#xA;&#xA;naval_chat_bot = App()&#xA;&#xA;# Embed Online Resources&#xA;naval_chat_bot.add(&#34;youtube_video&#34;, &#34;https://www.youtube.com/watch?v=3qHkcs3kG44&#34;)&#xA;naval_chat_bot.add(&#34;pdf_file&#34;, &#34;https://navalmanack.s3.amazonaws.com/Eric-Jorgenson_The-Almanack-of-Naval-Ravikant_Final.pdf&#34;)&#xA;naval_chat_bot.add(&#34;web_page&#34;, &#34;https://nav.al/feedback&#34;)&#xA;naval_chat_bot.add(&#34;web_page&#34;, &#34;https://nav.al/agi&#34;)&#xA;&#xA;# Embed Local Resources&#xA;naval_chat_bot.add_local(&#34;qna_pair&#34;, (&#34;Who is Naval Ravikant?&#34;, &#34;Naval Ravikant is an Indian-American entrepreneur and investor.&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If there is any other app instance in your script or app, you can change the import as&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from embedchain import App as EmbedChainApp&#xA;&#xA;# or&#xA;&#xA;from embedchain import App as ECApp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Now your app is created. You can use &lt;code&gt;.query&lt;/code&gt; function to get the answer for any query.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(naval_chat_bot.query(&#34;What unique capacity does Naval argue humans possess when it comes to understanding explanations or concepts?&#34;))&#xA;# answer: Naval argues that humans possess the unique capacity to understand explanations or concepts to the maximum extent possible in this physical reality.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Format supported&lt;/h2&gt; &#xA;&lt;p&gt;We support the following formats:&lt;/p&gt; &#xA;&lt;h3&gt;Youtube Video&lt;/h3&gt; &#xA;&lt;p&gt;To add any youtube video to your app, use the data_type (first argument to &lt;code&gt;.add&lt;/code&gt;) as &lt;code&gt;youtube_video&lt;/code&gt;. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add(&#39;youtube_video&#39;, &#39;a_valid_youtube_url_here&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;PDF File&lt;/h3&gt; &#xA;&lt;p&gt;To add any pdf file, use the data_type as &lt;code&gt;pdf_file&lt;/code&gt;. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add(&#39;pdf_file&#39;, &#39;a_valid_url_where_pdf_file_can_be_accessed&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that we do not support password protected pdfs.&lt;/p&gt; &#xA;&lt;h3&gt;Web Page&lt;/h3&gt; &#xA;&lt;p&gt;To add any web page, use the data_type as &lt;code&gt;web_page&lt;/code&gt;. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add(&#39;web_page&#39;, &#39;a_valid_web_page_url&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Text&lt;/h3&gt; &#xA;&lt;p&gt;To supply your own text, use the data_type as &lt;code&gt;text&lt;/code&gt; and enter a string. The text is not processed, this can be very versatile. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add_local(&#39;text&#39;, &#39;Seek wealth, not money or status. Wealth is having assets that earn while you sleep. Money is how we transfer time and wealth. Status is your place in the social hierarchy.&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: This is not used in the examples because in most cases you will supply a whole paragraph or file, which did not fit.&lt;/p&gt; &#xA;&lt;h3&gt;QnA Pair&lt;/h3&gt; &#xA;&lt;p&gt;To supply your own QnA pair, use the data_type as &lt;code&gt;qna_pair&lt;/code&gt; and enter a tuple. Eg:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;app.add_local(&#39;qna_pair&#39;, (&#34;Question&#34;, &#34;Answer&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;More Formats coming soon&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you want to add any other format, please create an &lt;a href=&#34;https://github.com/embedchain/embedchain/issues&#34;&gt;issue&lt;/a&gt; and we will add it to the list of supported formats.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How does it work?&lt;/h1&gt; &#xA;&lt;p&gt;Creating a chat bot over any dataset needs the following steps to happen&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;load the data&lt;/li&gt; &#xA; &lt;li&gt;create meaningful chunks&lt;/li&gt; &#xA; &lt;li&gt;create embeddings for each chunk&lt;/li&gt; &#xA; &lt;li&gt;store the chunks in vector database&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Whenever a user asks any query, following process happens to find the answer for the query&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;create the embedding for query&lt;/li&gt; &#xA; &lt;li&gt;find similar documents for this query from vector database&lt;/li&gt; &#xA; &lt;li&gt;pass similar documents as context to LLM to get the final answer.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The process of loading the dataset and then querying involves multiple steps and each steps has nuances of it is own.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How should I chunk the data? What is a meaningful chunk size?&lt;/li&gt; &#xA; &lt;li&gt;How should I create embeddings for each chunk? Which embedding model should I use?&lt;/li&gt; &#xA; &lt;li&gt;How should I store the chunks in vector database? Which vector database should I use?&lt;/li&gt; &#xA; &lt;li&gt;Should I store meta data along with the embeddings?&lt;/li&gt; &#xA; &lt;li&gt;How should I find similar documents for a query? Which ranking model should I use?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;These questions may be trivial for some but for a lot of us, it needs research, experimentation and time to find out the accurate answers.&lt;/p&gt; &#xA;&lt;p&gt;embedchain is a framework which takes care of all these nuances and provides a simple interface to create bots over any dataset.&lt;/p&gt; &#xA;&lt;p&gt;In the first release, we are making it easier for anyone to get a chatbot over any dataset up and running in less than a minute. All you need to do is create an app instance, add the data sets using &lt;code&gt;.add&lt;/code&gt; function and then use &lt;code&gt;.query&lt;/code&gt; function to get the relevant answer.&lt;/p&gt; &#xA;&lt;h1&gt;Tech Stack&lt;/h1&gt; &#xA;&lt;p&gt;embedchain is built on the following stack:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hwchase17/langchain&#34;&gt;Langchain&lt;/a&gt; as an LLM framework to load, chunk and index data&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings&#34;&gt;OpenAI&#39;s Ada embedding model&lt;/a&gt; to create embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/chat-completions-api&#34;&gt;OpenAI&#39;s ChatGPT API&lt;/a&gt; as LLM to get answers given the context&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/chroma-core/chroma&#34;&gt;Chroma&lt;/a&gt; as the vector database to store embeddings&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Author&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Taranjeet Singh (&lt;a href=&#34;https://twitter.com/taranjeetio&#34;&gt;@taranjeetio&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>steven-tey/chathn</title>
    <updated>2023-06-29T01:30:23Z</updated>
    <id>tag:github.com,2023-06-29:/steven-tey/chathn</id>
    <link href="https://github.com/steven-tey/chathn" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Chat with Hacker News using natural language. Built with OpenAI Functions and Vercel AI SDK.&lt;/p&gt;&lt;hr&gt;&lt;a href=&#34;https://chathn.vercel.app&#34;&gt; &lt;img alt=&#34;Chat with Hacker News using natural language.&#34; src=&#34;https://raw.githubusercontent.com/steven-tey/chathn/main/app/opengraph-image.png&#34;&gt; &lt;h1 align=&#34;center&#34;&gt;ChatHN&lt;/h1&gt; &lt;/a&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Chat with Hacker News using natural language. Built with OpenAI Functions and Vercel AI SDK. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://news.ycombinator.com/item?id=36480570&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hacker%20News-210-%23FF6600&#34; alt=&#34;Hacker News&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/steven-tey/chathn/raw/main/LICENSE&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/steven-tey/chathn?label=license&amp;amp;logo=github&amp;amp;color=f80&amp;amp;logoColor=fff&#34; alt=&#34;License&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/steven-tey/chathn&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/steven-tey/chathn?style=social&#34; alt=&#34;ChatHN&#39;s GitHub repo&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/steven-tey/chathn/main/#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/steven-tey/chathn/main/#deploy-your-own&#34;&gt;&lt;strong&gt;Deploy Your Own&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/steven-tey/chathn/main/#setting-up-locally&#34;&gt;&lt;strong&gt;Setting Up Locally&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/steven-tey/chathn/main/#tech-stack&#34;&gt;&lt;strong&gt;Tech Stack&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/steven-tey/chathn/main/#contributing&#34;&gt;&lt;strong&gt;Contributing&lt;/strong&gt;&lt;/a&gt; · &lt;a href=&#34;https://raw.githubusercontent.com/steven-tey/chathn/main/#license&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;ChatHN is an open-source AI chatbot that uses &lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/function-calling&#34;&gt;OpenAI Functions&lt;/a&gt; and the &lt;a href=&#34;https://sdk.vercel.ai/docs&#34;&gt;Vercel AI SDK&lt;/a&gt; to interact with the &lt;a href=&#34;https://github.com/HackerNews/API&#34;&gt;Hacker News API&lt;/a&gt; with natural language.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/steven-tey/chathn/assets/28986134/9c0ad554-f4e5-4e98-8771-5999ddf79235&#34;&gt;https://github.com/steven-tey/chathn/assets/28986134/9c0ad554-f4e5-4e98-8771-5999ddf79235&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Deploy your own&lt;/h2&gt; &#xA;&lt;p&gt;You can deploy your own version of ChatHN with 1-click:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://vercel.com/new/clone?demo-title=ChatHN%20%E2%80%93%20Chat%20with%20Hacker%20News&amp;amp;demo-description=AI%20chatbot%20that%20uses%20OpenAI%20Functions%20and%20Vercel%20AI%20SDK%20to%20interact%20with%20the%20Hacker%20News%20API%20with%20natural%20language.&amp;amp;demo-url=https%3A%2F%2Fchathn.vercel.app%2F&amp;amp;demo-image=%2F%2Fimages.ctfassets.net%2Fe5382hct74si%2F2lviJwxaFNmmqdNynfoUvi%2Fbc4eee4291e05f34c8e3691b3bd5d48d%2FCleanShot_2023-06-25_at_12.47.17.png&amp;amp;project-name=ChatHN%20%E2%80%93%20Chat%20with%20Hacker%20News&amp;amp;repository-name=chathn&amp;amp;repository-url=https%3A%2F%2Fgithub.com%2Fsteven-tey%2Fchathn&amp;amp;from=templates&amp;amp;skippable-integrations=1&amp;amp;env=OPENAI_API_KEY&amp;amp;envDescription=Get%20your%20OpenAI%20API%20key%20here%3A&amp;amp;envLink=https%3A%2F%2Fplatform.openai.com%2Faccount%2Fapi-keys&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Setting Up Locally&lt;/h2&gt; &#xA;&lt;p&gt;To set up ChatHN locally, you&#39;ll need to clone the repository and set up the following environment variables:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt; –&amp;nbsp;your OpenAI API key (you can get one &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;here&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tech Stack&lt;/h2&gt; &#xA;&lt;p&gt;ChatH is built on the following stack:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt; –&amp;nbsp;framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://platform.openai.com/docs/guides/gpt/function-calling&#34;&gt;OpenAI Functions&lt;/a&gt; - AI completions&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sdk.vercel.ai/docs&#34;&gt;Vercel AI SDK&lt;/a&gt; – AI streaming library&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vercel.com&#34;&gt;Vercel&lt;/a&gt; –&amp;nbsp;deployments&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tailwindcss.com/&#34;&gt;TailwindCSS&lt;/a&gt; – styles&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s how you can contribute:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/steven-tey/chathn/issues&#34;&gt;Open an issue&lt;/a&gt; if you believe you&#39;ve encountered a bug.&lt;/li&gt; &#xA; &lt;li&gt;Make a &lt;a href=&#34;https://github.com/steven-tey/chathn/pull&#34;&gt;pull request&lt;/a&gt; to add new features/make quality-of-life improvements/fix bugs.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Author&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Steven Tey (&lt;a href=&#34;https://twitter.com/steventey&#34;&gt;@steventey&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Licensed under the &lt;a href=&#34;https://github.com/steven-tey/chathn/raw/main/LICENSE.md&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>turboderp/exllama</title>
    <updated>2023-06-29T01:30:23Z</updated>
    <id>tag:github.com,2023-06-29:/turboderp/exllama</id>
    <link href="https://github.com/turboderp/exllama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ExLlama&lt;/h1&gt; &#xA;&lt;p&gt;A standalone Python/C++/CUDA implementation of Llama for use with 4-bit GPTQ weights, designed to be fast and memory-efficient on modern GPUs.&lt;/p&gt; &#xA;&lt;p&gt;Disclaimer: The project is coming along, but it&#39;s still a work in progress!&lt;/p&gt; &#xA;&lt;h2&gt;Hardware requirements&lt;/h2&gt; &#xA;&lt;p&gt;I am developing on an RTX 4090 and an RTX 3090-Ti. Both cards support the CUDA kernels, but there might be incompatibilities with older cards.&lt;/p&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python 3.9 or newer&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;torch&lt;/code&gt; tested on 2.0.1 and 2.1.0 (nightly) with cu118&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;safetensors&lt;/code&gt; 0.3.1&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;sentencepiece&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ninja&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Additionally, only for the web UI:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;flask&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;waitress&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Linux/WSL prerequisites&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu118&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Windows prerequisites&lt;/h2&gt; &#xA;&lt;p&gt;To run on Windows (without WSL):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://visualstudio.microsoft.com/downloads/&#34;&gt;MSVC 2022&lt;/a&gt;. You can choose to install the whole &lt;code&gt;Visual Studio 2022&lt;/code&gt; IDE, or alternatively just the &lt;code&gt;Build Tools for Visual Studio 2022&lt;/code&gt; package (make sure &lt;code&gt;Desktop development with C++&lt;/code&gt; is ticked in the installer), it doesn&#39;t really matter which.&lt;/li&gt; &#xA; &lt;li&gt;Install the appropriate version of &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;PyTorch&lt;/a&gt;, choosing one of the CUDA versions. I am developing on the nightly build, but the stable version (2.0.1) should also work.&lt;/li&gt; &#xA; &lt;li&gt;Install CUDA Toolkit, (&lt;a href=&#34;https://developer.nvidia.com/cuda-11-7-0-download-archive&#34;&gt;11.7&lt;/a&gt; and &lt;a href=&#34;https://developer.nvidia.com/cuda-11-8-0-download-archive&#34;&gt;11.8&lt;/a&gt; both seem to work, just make sure to match PyTorch&#39;s Compute Platform version).&lt;/li&gt; &#xA; &lt;li&gt;For best performance, enable Hardware Accelerated GPU Scheduling.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;How to&lt;/h2&gt; &#xA;&lt;p&gt;Install dependencies, clone repo and run benchmark:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&#xA;git clone https://github.com/turboderp/exllama&#xA;cd exllama&#xA;&#xA;python test_benchmark_inference.py -d &amp;lt;path_to_model_files&amp;gt; -p -ppl&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The CUDA extension is loaded at runtime so there&#39;s no need to install it separately. It will be compiled on the first run and cached to &lt;code&gt;~/.cache/torch_extensions/&lt;/code&gt; which could take a little while. If nothing happens at first, give it a minute to compile.&lt;/p&gt; &#xA;&lt;p&gt;Chatbot example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python example_chatbot.py -d &amp;lt;path_to_model_files&amp;gt; -un &#34;Jeff&#34; -p prompt_chatbort.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Web UI&lt;/h2&gt; &#xA;&lt;p&gt;I made a simple web UI for it. Like the rest of the project, it&#39;s a work in progress. Don&#39;t look at the JavaScript, it was mostly written by ChatGPT and it will haunt your dreams. But it sort of works, and it&#39;s kinda fun, especially multibot mode:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/turboderp/exllama/master/doc/_screenshot.jpg&#34; alt=&#34;_screenshot.jpg&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements-web.txt&#xA;&#xA;python webui/app.py -d &amp;lt;path_to_model_files&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that sessions are stored in &lt;code&gt;~/exllama_sessions/&lt;/code&gt;. You can change the location of the sessions storage with &lt;code&gt;-sd&lt;/code&gt; if you want.&lt;/p&gt; &#xA;&lt;h2&gt;Docker&lt;/h2&gt; &#xA;&lt;p&gt;For security benefits and easier deployment, it is also possible to run the web UI in an isolated docker container. Note: the docker image currently only supports NVIDIA GPUs.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html&#34;&gt;NVIDIA Container Toolkit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It is recommended to run docker in &lt;a href=&#34;https://docs.docker.com/engine/security/rootless/&#34;&gt;rootless mode&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Build&lt;/h3&gt; &#xA;&lt;p&gt;The easiest way to build the docker image is using docker compose. First, set the &lt;code&gt;MODEL_PATH&lt;/code&gt; and &lt;code&gt;SESSIONS_PATH&lt;/code&gt; variables in the &lt;code&gt;.env&lt;/code&gt; file to the actual directories on the host. Then run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is also possible to manually build the image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t exllama-web .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: by default, the service inside the docker container is run by a non-root user. Hence, the ownership of bind-mounted directories (&lt;code&gt;/data/model&lt;/code&gt; and &lt;code&gt;/data/exllama_sessions&lt;/code&gt; in the default &lt;code&gt;docker-compose.yml&lt;/code&gt; file) is changed to this non-root user in the container entrypoint (&lt;code&gt;entrypoint.sh&lt;/code&gt;). To disable this, set &lt;code&gt;RUN_UID=0&lt;/code&gt; in the &lt;code&gt;.env&lt;/code&gt; file if using &lt;code&gt;docker compose&lt;/code&gt;, or the following command if you manually build the image:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t exllama-web --build-arg RUN_UID=0 .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;p&gt;Using docker compose:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The web UI can now be accessed on the host at &lt;a href=&#34;http://localhost:5000&#34;&gt;http://localhost:5000&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The configuration can be viewed in &lt;code&gt;docker-compose.yml&lt;/code&gt; and changed by creating a &lt;code&gt;docker-compose.override.yml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Run manually:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run --gpus all -p 5000:5000 -v &amp;lt;path_to_model_dir&amp;gt;:/data/model/ -v &amp;lt;path_to_session_dir&amp;gt;:/data/exllama_sessions --rm -it exllama-web --host 0.0.0.0:5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Results so far&lt;/h2&gt; &#xA;&lt;h3&gt;New implementation&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;grpsz&lt;/th&gt; &#xA;   &lt;th&gt;act&lt;/th&gt; &#xA;   &lt;th&gt;Seq. len.&lt;/th&gt; &#xA;   &lt;th&gt;VRAM&lt;/th&gt; &#xA;   &lt;th&gt;Prompt&lt;/th&gt; &#xA;   &lt;th&gt;Best&lt;/th&gt; &#xA;   &lt;th&gt;Worst&lt;/th&gt; &#xA;   &lt;th&gt;Ppl&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;5,194 MB&lt;/td&gt; &#xA;   &lt;td&gt;13,918 t/s&lt;/td&gt; &#xA;   &lt;td&gt;173 t/s&lt;/td&gt; &#xA;   &lt;td&gt;140 t/s&lt;/td&gt; &#xA;   &lt;td&gt;6.45&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;9,127 MB&lt;/td&gt; &#xA;   &lt;td&gt;7,507 t/s&lt;/td&gt; &#xA;   &lt;td&gt;102 t/s&lt;/td&gt; &#xA;   &lt;td&gt;86 t/s&lt;/td&gt; &#xA;   &lt;td&gt;5.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;no&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;20,795 MB&lt;/td&gt; &#xA;   &lt;td&gt;2,959 t/s&lt;/td&gt; &#xA;   &lt;td&gt;47 t/s&lt;/td&gt; &#xA;   &lt;td&gt;40 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.60&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;20,795 MB&lt;/td&gt; &#xA;   &lt;td&gt;2,784 t/s&lt;/td&gt; &#xA;   &lt;td&gt;45 t/s&lt;/td&gt; &#xA;   &lt;td&gt;37 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.55&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;1,550 t &lt;sup&gt;1&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;21,486 MB&lt;/td&gt; &#xA;   &lt;td&gt;2,636 t/s&lt;/td&gt; &#xA;   &lt;td&gt;41 t/s&lt;/td&gt; &#xA;   &lt;td&gt;37 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.52&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Koala&lt;/td&gt; &#xA;   &lt;td&gt;13B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;9,127 MB&lt;/td&gt; &#xA;   &lt;td&gt;5,529 t/s&lt;/td&gt; &#xA;   &lt;td&gt;93 t/s&lt;/td&gt; &#xA;   &lt;td&gt;79 t/s&lt;/td&gt; &#xA;   &lt;td&gt;6.73&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WizardLM&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;td&gt;no &lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;20,199 MB&lt;/td&gt; &#xA;   &lt;td&gt;2,313 t/s&lt;/td&gt; &#xA;   &lt;td&gt;47 t/s&lt;/td&gt; &#xA;   &lt;td&gt;40 t/s&lt;/td&gt; &#xA;   &lt;td&gt;5.75&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; Can not achieve full sequence length without OoM (yet)&lt;br&gt; &lt;sup&gt;2&lt;/sup&gt; Not quite sure if this is act-order or not. Weights have no group index, at least&lt;/p&gt; &#xA;&lt;p&gt;All tests done on stock RTX 4090 / 12900K, running with a desktop environment, with a few other apps also using VRAM.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&#34;Prompt&#34;&lt;/strong&gt; speed is inference over the sequence length listed minus 128 tokens. &lt;strong&gt;&#34;Worst&#34;&lt;/strong&gt; is the average speed for the last 128 tokens of the full context (worst case) and &lt;strong&gt;&#34;Best&#34;&lt;/strong&gt; lists the speed for the first 128 tokens in an empty sequence (best case.)&lt;/p&gt; &#xA;&lt;p&gt;VRAM usage is as reported by PyTorch and does not include PyTorch&#39;s own overhead (CUDA kernels, internal buffers etc.) This is somewhat unpredictable anyway. Best bet is to just optimize VRAM usage by the model, probably aiming for 20 GB on a 24 GB GPU to ensure there is room for a desktop environment and all of Torch&#39;s internals.&lt;/p&gt; &#xA;&lt;p&gt;Perplexity is measured only to verify that the models are working. The dataset used is a particular, small sample from WikiText, so scores are not necessarily comparable to other Llama benchmarks.&lt;/p&gt; &#xA;&lt;h3&gt;Dual GPU results&lt;/h3&gt; &#xA;&lt;p&gt;Since many seem to be interested in running 65B models, I can confirm that this works with two 24 GB GPUs. The following benchmarks are from a 4090 + 3090-Ti with &lt;code&gt;-gs 17.2,24&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;groupsize&lt;/th&gt; &#xA;   &lt;th&gt;act&lt;/th&gt; &#xA;   &lt;th&gt;Seq. len.&lt;/th&gt; &#xA;   &lt;th&gt;VRAM&lt;/th&gt; &#xA;   &lt;th&gt;Prompt&lt;/th&gt; &#xA;   &lt;th&gt;Best&lt;/th&gt; &#xA;   &lt;th&gt;Worst&lt;/th&gt; &#xA;   &lt;th&gt;Ppl&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;39,804 MB&lt;/td&gt; &#xA;   &lt;td&gt;1,109 t/s&lt;/td&gt; &#xA;   &lt;td&gt;20 t/s&lt;/td&gt; &#xA;   &lt;td&gt;18 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;32&lt;/td&gt; &#xA;   &lt;td&gt;yes&lt;/td&gt; &#xA;   &lt;td&gt;2,048 t&lt;/td&gt; &#xA;   &lt;td&gt;43,424 MB&lt;/td&gt; &#xA;   &lt;td&gt;1,037 t/s&lt;/td&gt; &#xA;   &lt;td&gt;17 t/s&lt;/td&gt; &#xA;   &lt;td&gt;16 t/s&lt;/td&gt; &#xA;   &lt;td&gt;4.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Testing long sequences&lt;/h3&gt; &#xA;&lt;p&gt;The following tests were all done on &lt;strong&gt;33B/65B, 4bit 128g&lt;/strong&gt; with various settings, just to test the max sequence length and get a sense of what can be achieved with different or multiple GPUs right now. Llama goes incoherent generating past 2048 tokens anyway, but with some fine-tuning, who knows? Note that these tests were run a while ago and the speeds are no longer current.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;Size&lt;/th&gt; &#xA;   &lt;th&gt;Seq. len.&lt;/th&gt; &#xA;   &lt;th&gt;VRAM&lt;/th&gt; &#xA;   &lt;th&gt;Long seq.&lt;/th&gt; &#xA;   &lt;th&gt;Ind.&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4090/24GB&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;2,516 t&lt;/td&gt; &#xA;   &lt;td&gt;22,145 MB&lt;/td&gt; &#xA;   &lt;td&gt;1140 t/s&lt;/td&gt; &#xA;   &lt;td&gt;28 t/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;4090/24GB + 3070Ti/8GB&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;3,932 t&lt;/td&gt; &#xA;   &lt;td&gt;22,055 MB + 7,377 MB&lt;/td&gt; &#xA;   &lt;td&gt;840 t/s&lt;/td&gt; &#xA;   &lt;td&gt;22 t/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A6000/48GB (headless)&lt;/td&gt; &#xA;   &lt;td&gt;33B&lt;/td&gt; &#xA;   &lt;td&gt;9,032 t&lt;/td&gt; &#xA;   &lt;td&gt;46,863 MB&lt;/td&gt; &#xA;   &lt;td&gt;645 t/s&lt;/td&gt; &#xA;   &lt;td&gt;12 t/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A100/80GB (headless)&lt;/td&gt; &#xA;   &lt;td&gt;65B&lt;/td&gt; &#xA;   &lt;td&gt;9,520 t&lt;/td&gt; &#xA;   &lt;td&gt;79,009 MB&lt;/td&gt; &#xA;   &lt;td&gt;650 t/s&lt;/td&gt; &#xA;   &lt;td&gt;9 t/s&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;p&gt;Moved the todo list &lt;a href=&#34;https://raw.githubusercontent.com/turboderp/exllama/master/doc/TODO.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Compatibility&lt;/h2&gt; &#xA;&lt;p&gt;I downloaded a whole bunch of GPTQ models to test compatibility. &lt;a href=&#34;https://raw.githubusercontent.com/turboderp/exllama/master/doc/model_compatibility.md&#34;&gt;Here&lt;/a&gt; is the list of models confirmed to be working right now.&lt;/p&gt; &#xA;&lt;h2&gt;Recent updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-02&lt;/strong&gt;: Web UI is now in a fairly working state. Expect it to be a little scuffed in places. There will be a rewrite at some point to make the client-side code less seizure-inducing. It has multibot mode, chat rewind and editing features, sessions, and more. I&#39;m going to build it out with support for instruct prompting and such, in time.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-04&lt;/strong&gt;: Refactored a whole bunch to move more of the work into the extension, setting up for more tuning options to come soon and eventually auto tuning. Also optimized a little, for about a 5% speedup.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-06&lt;/strong&gt;: Some minor optimizations. Also it should now compile the extension more easily and run more seamlessly on Windows.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-09&lt;/strong&gt;: Fused most of the self-attention step. More to come. Slight speedup already, but more importantly went from 69% actual CPU utilization to 37%. This should do a lot to address the bottleneck on CPUs with lower single-threaded performance.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-10&lt;/strong&gt;: Docker support now! And some minor optimizations. Cleaned up the project a bit.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-11&lt;/strong&gt;: Added some concurrency a couple of places. It&#39;s only beneficial on the 4090, on small models where the cores are somewhat underutilized and the L2 cache can keep up. For the 3090 it&#39;s detrimental to performance, so it&#39;s disabled by default. YMMV. Use &lt;code&gt;-cs&lt;/code&gt; to try it out.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-17&lt;/strong&gt;: Fixed a nasty bug in the fused attention that was causing slightly incorrect cache states on 13B and 33B models. You definitely want to update.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023-06-18&lt;/strong&gt;: LoRA support now. Still needs a lot of testing and some optimization, and currently you can&#39;t stack multiple LoRAs during the same inference. There&#39;s also no support in the web UI yet.&lt;/p&gt;</summary>
  </entry>
</feed>