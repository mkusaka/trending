<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-08-23T01:29:16Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/pyright</title>
    <updated>2024-08-23T01:29:16Z</updated>
    <id>tag:github.com,2024-08-23:/microsoft/pyright</id>
    <link href="https://github.com/microsoft/pyright" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Static Type Checker for Python&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/microsoft/pyright/raw/main/docs/img/PyrightLarge.png&#34; alt=&#34;Pyright&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Static Type Checker for Python&lt;/h1&gt; &#xA;&lt;p&gt;Pyright is a full-featured, standards-based static type checker for Python. It is designed for high performance and can be used with large Python source bases.&lt;/p&gt; &#xA;&lt;p&gt;Pyright includes both a &lt;a href=&#34;https://microsoft.github.io/pyright/#/command-line&#34;&gt;command-line tool&lt;/a&gt; and an &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-pyright.pyright&#34;&gt;extension for Visual Studio Code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Pyright Playground&lt;/h2&gt; &#xA;&lt;p&gt;Try Pyright in your browser using the &lt;a href=&#34;https://pyright-play.net/?code=MQAgKgFglgziMEMC2AHANgUxAEw0g9gHYwAuATgiRnBPgO4gDG%2BSBhIGZZ%2BZcjC7AEZZcVRlWzwSlKPzRoAniEFKUCslADmEEgDoAUPtwAzEAmzYAFAA8AXCGNp8lADQgF9x85IBKW-pBAkDIMEgBXMnZrEABqd0NQAAUEGBgoQk0zKTIQdNIBRiwUkBIILBgMZkJJBDJNMKQMQhJg6jC0Ejh0rLIw5qhGjmtClBIoIgNzKwBGNwAiOZ99IA&#34;&gt;Pyright Playground&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://microsoft.github.io/pyright&#34;&gt;the documentation&lt;/a&gt; for installation, configuration, and usage details.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Do you have questions about Pyright or Python type annotations in general? Post your questions in &lt;a href=&#34;https://github.com/microsoft/pyright/discussions&#34;&gt;the discussion section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you would like to report a bug or request an enhancement, file a new issue in either the &lt;a href=&#34;https://github.com/microsoft/pyright/issues&#34;&gt;pyright&lt;/a&gt; or &lt;a href=&#34;https://github.com/microsoft/pylance-release/issues&#34;&gt;pylance-release&lt;/a&gt; issue tracker. In general, core type checking functionality is associated with Pyright while language service functionality is associated with Pylance, but the same contributors monitor both repos. For best results, provide the information requested in the issue template.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. For feature and complex bug fix contributions, it is recommended that you first discuss the proposed change with Pyright’s maintainers before submitting the pull request. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.microsoft.com&#34;&gt;https://cla.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA-bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fmtlib/fmt</title>
    <updated>2024-08-23T01:29:16Z</updated>
    <id>tag:github.com,2024-08-23:/fmtlib/fmt</id>
    <link href="https://github.com/fmtlib/fmt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A modern formatting library&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/576385/156254208-f5b743a9-88cf-439d-b0c0-923d53e8d551.png&#34; alt=&#34;{fmt}&#34; width=&#34;25%&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/fmtlib/fmt/actions?query=workflow%3Alinux&#34;&gt;&lt;img src=&#34;https://github.com/fmtlib/fmt/workflows/linux/badge.svg?sanitize=true&#34; alt=&#34;image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/fmtlib/fmt/actions?query=workflow%3Amacos&#34;&gt;&lt;img src=&#34;https://github.com/fmtlib/fmt/workflows/macos/badge.svg?sanitize=true&#34; alt=&#34;image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/fmtlib/fmt/actions?query=workflow%3Awindows&#34;&gt;&lt;img src=&#34;https://github.com/fmtlib/fmt/workflows/windows/badge.svg?sanitize=true&#34; alt=&#34;image&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?%0Acolspec=ID%20Type%20Component%20Status%20Proj%20Reported%20Owner%20%0ASummary&amp;amp;q=proj%3Dfmt&amp;amp;can=1&#34;&gt;&lt;img src=&#34;https://oss-fuzz-build-logs.storage.googleapis.com/badges/fmt.svg?sanitize=true&#34; alt=&#34;fmt is continuously fuzzed at oss-fuzz&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://stackoverflow.com/questions/tagged/fmt&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/stackoverflow-fmt-blue.svg?sanitize=true&#34; alt=&#34;Ask questions at StackOverflow with the tag fmt&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://securityscorecards.dev/viewer/?uri=github.com/fmtlib/fmt&#34;&gt;&lt;img src=&#34;https://api.securityscorecards.dev/projects/github.com/fmtlib/fmt/badge&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;{fmt}&lt;/strong&gt; is an open-source formatting library providing a fast and safe alternative to C stdio and C++ iostreams.&lt;/p&gt; &#xA;&lt;p&gt;If you like this project, please consider donating to one of the funds that help victims of the war in Ukraine: &lt;a href=&#34;https://www.stopputin.net/&#34;&gt;https://www.stopputin.net/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fmt.dev&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hackingcpp.com/cpp/libs/fmt.html&#34;&gt;Cheat Sheets&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Q&amp;amp;A: ask questions on &lt;a href=&#34;https://stackoverflow.com/questions/tagged/fmt&#34;&gt;StackOverflow with the tag fmt&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Try {fmt} in &lt;a href=&#34;https://godbolt.org/z/8Mx1EW73v&#34;&gt;Compiler Explorer&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Features&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple &lt;a href=&#34;https://fmt.dev/latest/api/&#34;&gt;format API&lt;/a&gt; with positional arguments for localization&lt;/li&gt; &#xA; &lt;li&gt;Implementation of &lt;a href=&#34;https://en.cppreference.com/w/cpp/utility/format&#34;&gt;C++20 std::format&lt;/a&gt; and &lt;a href=&#34;https://en.cppreference.com/w/cpp/io/print&#34;&gt;C++23 std::print&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fmt.dev/latest/syntax/&#34;&gt;Format string syntax&lt;/a&gt; similar to Python&#39;s &lt;a href=&#34;https://docs.python.org/3/library/stdtypes.html#str.format&#34;&gt;format&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fast IEEE 754 floating-point formatter with correct rounding, shortness and round-trip guarantees using the &lt;a href=&#34;https://github.com/jk-jeon/dragonbox&#34;&gt;Dragonbox&lt;/a&gt; algorithm&lt;/li&gt; &#xA; &lt;li&gt;Portable Unicode support&lt;/li&gt; &#xA; &lt;li&gt;Safe &lt;a href=&#34;https://fmt.dev/latest/api/#printf-formatting&#34;&gt;printf implementation&lt;/a&gt; including the POSIX extension for positional arguments&lt;/li&gt; &#xA; &lt;li&gt;Extensibility: &lt;a href=&#34;https://fmt.dev/latest/api/#formatting-user-defined-types&#34;&gt;support for user-defined types&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;High performance: faster than common standard library implementations of &lt;code&gt;(s)printf&lt;/code&gt;, iostreams, &lt;code&gt;to_string&lt;/code&gt; and &lt;code&gt;to_chars&lt;/code&gt;, see &lt;a href=&#34;https://raw.githubusercontent.com/fmtlib/fmt/master/#speed-tests&#34;&gt;Speed tests&lt;/a&gt; and &lt;a href=&#34;http://www.zverovich.net/2020/06/13/fast-int-to-string-revisited.html&#34;&gt;Converting a hundred million integers to strings per second&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Small code size both in terms of source code with the minimum configuration consisting of just three files, &lt;code&gt;core.h&lt;/code&gt;, &lt;code&gt;format.h&lt;/code&gt; and &lt;code&gt;format-inl.h&lt;/code&gt;, and compiled code; see &lt;a href=&#34;https://raw.githubusercontent.com/fmtlib/fmt/master/#compile-time-and-code-bloat&#34;&gt;Compile time and code bloat&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reliability: the library has an extensive set of &lt;a href=&#34;https://github.com/fmtlib/fmt/tree/master/test&#34;&gt;tests&lt;/a&gt; and is &lt;a href=&#34;https://bugs.chromium.org/p/oss-fuzz/issues/list?colspec=ID%20Type%20Component%20Status%20Proj%20Reported%20Owner%20Summary&amp;amp;q=proj%3Dfmt&amp;amp;can=1&#34;&gt;continuously fuzzed&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Safety: the library is fully type-safe, errors in format strings can be reported at compile time, automatic memory management prevents buffer overflow errors&lt;/li&gt; &#xA; &lt;li&gt;Ease of use: small self-contained code base, no external dependencies, permissive MIT &lt;a href=&#34;https://github.com/fmtlib/fmt/raw/master/LICENSE&#34;&gt;license&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fmt.dev/latest/#portability&#34;&gt;Portability&lt;/a&gt; with consistent output across platforms and support for older compilers&lt;/li&gt; &#xA; &lt;li&gt;Clean warning-free codebase even on high warning levels such as &lt;code&gt;-Wall -Wextra -pedantic&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Locale independence by default&lt;/li&gt; &#xA; &lt;li&gt;Optional header-only configuration enabled with the &lt;code&gt;FMT_HEADER_ONLY&lt;/code&gt; macro&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See the &lt;a href=&#34;https://fmt.dev&#34;&gt;documentation&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;h1&gt;Examples&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Print to stdout&lt;/strong&gt; (&lt;a href=&#34;https://godbolt.org/z/Tevcjh&#34;&gt;run&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;fmt/core.h&amp;gt;&#xA;&#xA;int main() {&#xA;  fmt::print(&#34;Hello, world!\n&#34;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format a string&lt;/strong&gt; (&lt;a href=&#34;https://godbolt.org/z/oK8h33&#34;&gt;run&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;std::string s = fmt::format(&#34;The answer is {}.&#34;, 42);&#xA;// s == &#34;The answer is 42.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Format a string using positional arguments&lt;/strong&gt; (&lt;a href=&#34;https://godbolt.org/z/Yn7Txe&#34;&gt;run&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;std::string s = fmt::format(&#34;I&#39;d rather be {1} than {0}.&#34;, &#34;right&#34;, &#34;happy&#34;);&#xA;// s == &#34;I&#39;d rather be happy than right.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Print dates and times&lt;/strong&gt; (&lt;a href=&#34;https://godbolt.org/z/c31ExdY3W&#34;&gt;run&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;fmt/chrono.h&amp;gt;&#xA;&#xA;int main() {&#xA;  auto now = std::chrono::system_clock::now();&#xA;  fmt::print(&#34;Date and time: {}\n&#34;, now);&#xA;  fmt::print(&#34;Time: {:%H:%M}\n&#34;, now);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Date and time: 2023-12-26 19:10:31.557195597&#xA;Time: 19:10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Print a container&lt;/strong&gt; (&lt;a href=&#34;https://godbolt.org/z/MxM1YqjE7&#34;&gt;run&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;vector&amp;gt;&#xA;#include &amp;lt;fmt/ranges.h&amp;gt;&#xA;&#xA;int main() {&#xA;  std::vector&amp;lt;int&amp;gt; v = {1, 2, 3};&#xA;  fmt::print(&#34;{}\n&#34;, v);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[1, 2, 3]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Check a format string at compile time&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;std::string s = fmt::format(&#34;{:d}&#34;, &#34;I am not a number&#34;);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This gives a compile-time error in C++20 because &lt;code&gt;d&lt;/code&gt; is an invalid format specifier for a string.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Write a file from a single thread&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;fmt/os.h&amp;gt;&#xA;&#xA;int main() {&#xA;  auto out = fmt::output_file(&#34;guide.txt&#34;);&#xA;  out.print(&#34;Don&#39;t {}&#34;, &#34;Panic&#34;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This can be &lt;a href=&#34;http://www.zverovich.net/2020/08/04/optimal-file-buffer-size.html&#34;&gt;5 to 9 times faster than fprintf&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Print with colors and text styles&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;#include &amp;lt;fmt/color.h&amp;gt;&#xA;&#xA;int main() {&#xA;  fmt::print(fg(fmt::color::crimson) | fmt::emphasis::bold,&#xA;             &#34;Hello, {}!\n&#34;, &#34;world&#34;);&#xA;  fmt::print(fg(fmt::color::floral_white) | bg(fmt::color::slate_gray) |&#xA;             fmt::emphasis::underline, &#34;Olá, {}!\n&#34;, &#34;Mundo&#34;);&#xA;  fmt::print(fg(fmt::color::steel_blue) | fmt::emphasis::italic,&#xA;             &#34;你好{}！\n&#34;, &#34;世界&#34;);&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Output on a modern terminal with Unicode support:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/fmtlib/fmt/assets/%0A576385/2a93c904-d6fa-4aa6-b453-2618e1c327d7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;h2&gt;Speed tests&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Run Time, s&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;libc&lt;/td&gt; &#xA;   &lt;td&gt;printf&lt;/td&gt; &#xA;   &lt;td&gt;0.91&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;libc++&lt;/td&gt; &#xA;   &lt;td&gt;std::ostream&lt;/td&gt; &#xA;   &lt;td&gt;2.49&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;{fmt} 9.1&lt;/td&gt; &#xA;   &lt;td&gt;fmt::print&lt;/td&gt; &#xA;   &lt;td&gt;0.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Boost Format 1.80&lt;/td&gt; &#xA;   &lt;td&gt;boost::format&lt;/td&gt; &#xA;   &lt;td&gt;6.26&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Folly Format&lt;/td&gt; &#xA;   &lt;td&gt;folly::format&lt;/td&gt; &#xA;   &lt;td&gt;1.87&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;{fmt} is the fastest of the benchmarked methods, ~20% faster than &lt;code&gt;printf&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The above results were generated by building &lt;code&gt;tinyformat_test.cpp&lt;/code&gt; on macOS 12.6.1 with &lt;code&gt;clang++ -O3 -DNDEBUG -DSPEED_TEST -DHAVE_FORMAT&lt;/code&gt;, and taking the best of three runs. In the test, the format string &lt;code&gt;&#34;%0.10f:%04d:%+g:%s:%p:%c:%%\n&#34;&lt;/code&gt; or equivalent is filled 2,000,000 times with output sent to &lt;code&gt;/dev/null&lt;/code&gt;; for further details refer to the &lt;a href=&#34;https://github.com/fmtlib/format-benchmark/raw/master/src/tinyformat-test.cc&#34;&gt;source&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;{fmt} is up to 20-30x faster than &lt;code&gt;std::ostringstream&lt;/code&gt; and &lt;code&gt;sprintf&lt;/code&gt; on IEEE754 &lt;code&gt;float&lt;/code&gt; and &lt;code&gt;double&lt;/code&gt; formatting (&lt;a href=&#34;https://github.com/fmtlib/dtoa-benchmark&#34;&gt;dtoa-benchmark&lt;/a&gt;) and faster than &lt;a href=&#34;https://github.com/google/double-conversion&#34;&gt;double-conversion&lt;/a&gt; and &lt;a href=&#34;https://github.com/ulfjack/ryu&#34;&gt;ryu&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fmt.dev/unknown_mac64_clang12.0.html&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/576385/95684665-11719600-0ba8-11eb-8e5b-972ff4e49428.png&#34; alt=&#34;image&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Compile time and code bloat&lt;/h2&gt; &#xA;&lt;p&gt;The script &lt;a href=&#34;https://github.com/fmtlib/format-benchmark/raw/master/bloat-test.py&#34;&gt;bloat-test.py&lt;/a&gt; from &lt;a href=&#34;https://github.com/fmtlib/format-benchmark&#34;&gt;format-benchmark&lt;/a&gt; tests compile time and code bloat for nontrivial projects. It generates 100 translation units and uses &lt;code&gt;printf()&lt;/code&gt; or its alternative five times in each to simulate a medium-sized project. The resulting executable size and compile time (Apple clang version 15.0.0 (clang-1500.1.0.2.5), macOS Sonoma, best of three) is shown in the following tables.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Optimized build (-O3)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Compile Time, s&lt;/th&gt; &#xA;   &lt;th&gt;Executable size, KiB&lt;/th&gt; &#xA;   &lt;th&gt;Stripped size, KiB&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;printf&lt;/td&gt; &#xA;   &lt;td&gt;1.6&lt;/td&gt; &#xA;   &lt;td&gt;54&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IOStreams&lt;/td&gt; &#xA;   &lt;td&gt;25.9&lt;/td&gt; &#xA;   &lt;td&gt;98&lt;/td&gt; &#xA;   &lt;td&gt;84&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;fmt 83652df&lt;/td&gt; &#xA;   &lt;td&gt;4.8&lt;/td&gt; &#xA;   &lt;td&gt;54&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tinyformat&lt;/td&gt; &#xA;   &lt;td&gt;29.1&lt;/td&gt; &#xA;   &lt;td&gt;161&lt;/td&gt; &#xA;   &lt;td&gt;136&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Boost Format&lt;/td&gt; &#xA;   &lt;td&gt;55.0&lt;/td&gt; &#xA;   &lt;td&gt;530&lt;/td&gt; &#xA;   &lt;td&gt;317&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;{fmt} is fast to compile and is comparable to &lt;code&gt;printf&lt;/code&gt; in terms of per-call binary size (within a rounding error on this system).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Non-optimized build&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Method&lt;/th&gt; &#xA;   &lt;th&gt;Compile Time, s&lt;/th&gt; &#xA;   &lt;th&gt;Executable size, KiB&lt;/th&gt; &#xA;   &lt;th&gt;Stripped size, KiB&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;printf&lt;/td&gt; &#xA;   &lt;td&gt;1.4&lt;/td&gt; &#xA;   &lt;td&gt;54&lt;/td&gt; &#xA;   &lt;td&gt;50&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;IOStreams&lt;/td&gt; &#xA;   &lt;td&gt;23.4&lt;/td&gt; &#xA;   &lt;td&gt;92&lt;/td&gt; &#xA;   &lt;td&gt;68&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;{fmt} 83652df&lt;/td&gt; &#xA;   &lt;td&gt;4.4&lt;/td&gt; &#xA;   &lt;td&gt;89&lt;/td&gt; &#xA;   &lt;td&gt;85&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tinyformat&lt;/td&gt; &#xA;   &lt;td&gt;24.5&lt;/td&gt; &#xA;   &lt;td&gt;204&lt;/td&gt; &#xA;   &lt;td&gt;161&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Boost Format&lt;/td&gt; &#xA;   &lt;td&gt;36.4&lt;/td&gt; &#xA;   &lt;td&gt;831&lt;/td&gt; &#xA;   &lt;td&gt;462&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;code&gt;libc&lt;/code&gt;, &lt;code&gt;lib(std)c++&lt;/code&gt;, and &lt;code&gt;libfmt&lt;/code&gt; are all linked as shared libraries to compare formatting function overhead only. Boost Format is a header-only library so it doesn&#39;t provide any linkage options.&lt;/p&gt; &#xA;&lt;h2&gt;Running the tests&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://fmt.dev/latest/get-started/#building-from-source&#34;&gt;Building the library&lt;/a&gt; for instructions on how to build the library and run the unit tests.&lt;/p&gt; &#xA;&lt;p&gt;Benchmarks reside in a separate repository, &lt;a href=&#34;https://github.com/fmtlib/format-benchmark&#34;&gt;format-benchmarks&lt;/a&gt;, so to run the benchmarks you first need to clone this repository and generate Makefiles with CMake:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ git clone --recursive https://github.com/fmtlib/format-benchmark.git&#xA;$ cd format-benchmark&#xA;$ cmake .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can run the speed test:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ make speed-test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or the bloat test:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ make bloat-test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Migrating code&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://clang.llvm.org/extra/clang-tidy/&#34;&gt;clang-tidy&lt;/a&gt; v18 provides the &lt;a href=&#34;https://clang.llvm.org/extra/clang-tidy/checks/modernize/use-std-print.html&#34;&gt;modernize-use-std-print&lt;/a&gt; check that is capable of converting occurrences of &lt;code&gt;printf&lt;/code&gt; and &lt;code&gt;fprintf&lt;/code&gt; to &lt;code&gt;fmt::print&lt;/code&gt; if configured to do so. (By default it converts to &lt;code&gt;std::print&lt;/code&gt;.)&lt;/p&gt; &#xA;&lt;h1&gt;Notable projects using this library&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://play0ad.com/&#34;&gt;0 A.D.&lt;/a&gt;: a free, open-source, cross-platform real-time strategy game&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ampl/mp&#34;&gt;AMPL/MP&lt;/a&gt;: an open-source library for mathematical programming&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/apple/foundationdb&#34;&gt;Apple&#39;s FoundationDB&lt;/a&gt;: an open-source, distributed, transactional key-value store&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aseprite/aseprite&#34;&gt;Aseprite&lt;/a&gt;: animated sprite editor &amp;amp; pixel art tool&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.aviobook.aero/en&#34;&gt;AvioBook&lt;/a&gt;: a comprehensive aircraft operations suite&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://battle.net/&#34;&gt;Blizzard Battle.net&lt;/a&gt;: an online gaming platform&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://celestia.space/&#34;&gt;Celestia&lt;/a&gt;: real-time 3D visualization of space&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ceph.com/&#34;&gt;Ceph&lt;/a&gt;: a scalable distributed storage system&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ccache.dev/&#34;&gt;ccache&lt;/a&gt;: a compiler cache&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ClickHouse/ClickHouse&#34;&gt;ClickHouse&lt;/a&gt;: an analytical database management system&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/contour-terminal/contour/&#34;&gt;Contour&lt;/a&gt;: a modern terminal emulator&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cuauv.org/&#34;&gt;CUAUV&lt;/a&gt;: Cornell University&#39;s autonomous underwater vehicle&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://drake.mit.edu/&#34;&gt;Drake&lt;/a&gt;: a planning, control, and analysis toolbox for nonlinear dynamical systems (MIT)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/envoyproxy/envoy&#34;&gt;Envoy&lt;/a&gt;: C++ L7 proxy and communication bus (Lyft)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fivem.net/&#34;&gt;FiveM&lt;/a&gt;: a modification framework for GTA V&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/MengRao/fmtlog&#34;&gt;fmtlog&lt;/a&gt;: a performant fmtlib-style logging library with latency in nanoseconds&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebook/folly&#34;&gt;Folly&lt;/a&gt;: Facebook open-source library&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://gemrb.org/&#34;&gt;GemRB&lt;/a&gt;: a portable open-source implementation of Bioware&#39;s Infinity Engine&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://store.steampowered.com/app/1247360/Grand_Mountain_Adventure/&#34;&gt;Grand Mountain Adventure&lt;/a&gt;: a beautiful open-world ski &amp;amp; snowboarding game&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pvpgn/pvpgn-server&#34;&gt;HarpyWar/pvpgn&lt;/a&gt;: Player vs Player Gaming Network with tweaks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/kbengine/kbengine&#34;&gt;KBEngine&lt;/a&gt;: an open-source MMOG server engine&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://keypirinha.com/&#34;&gt;Keypirinha&lt;/a&gt;: a semantic launcher for Windows&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kodi.tv/&#34;&gt;Kodi&lt;/a&gt; (formerly xbmc): home theater software&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://kth.cash/&#34;&gt;Knuth&lt;/a&gt;: high-performance Bitcoin full-node&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/contour-terminal/libunicode/&#34;&gt;libunicode&lt;/a&gt;: a modern C++17 Unicode library&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mariadb.org/&#34;&gt;MariaDB&lt;/a&gt;: relational database management system&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/verona&#34;&gt;Microsoft Verona&lt;/a&gt;: research programming language for concurrent ownership&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://mongodb.com/&#34;&gt;MongoDB&lt;/a&gt;: distributed document database&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/duckie/mongo_smasher&#34;&gt;MongoDB Smasher&lt;/a&gt;: a small tool to generate randomized datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://openspaceproject.com/&#34;&gt;OpenSpace&lt;/a&gt;: an open-source astrovisualization framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.polserver.com/&#34;&gt;PenUltima Online (POL)&lt;/a&gt;: an MMO server, compatible with most Ultima Online clients&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/pytorch&#34;&gt;PyTorch&lt;/a&gt;: an open-source machine learning library&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.quasardb.net/&#34;&gt;quasardb&lt;/a&gt;: a distributed, high-performance, associative database&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/odygrd/quill&#34;&gt;Quill&lt;/a&gt;: asynchronous low-latency logging library&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ravijanjam/qkw&#34;&gt;QKW&lt;/a&gt;: generalizing aliasing to simplify navigation, and execute complex multi-line terminal command sequences&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/HunanTV/redis-cerberus&#34;&gt;redis-cerberus&lt;/a&gt;: a Redis cluster proxy&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://vectorized.io/redpanda&#34;&gt;redpanda&lt;/a&gt;: a 10x faster Kafka® replacement for mission-critical systems written in C++&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://rpclib.net/&#34;&gt;rpclib&lt;/a&gt;: a modern C++ msgpack-RPC server and client library&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.salesforce.com/analytics-cloud/overview/&#34;&gt;Salesforce Analytics Cloud&lt;/a&gt;: business intelligence software&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.scylladb.com/&#34;&gt;Scylla&lt;/a&gt;: a Cassandra-compatible NoSQL data store that can handle 1 million transactions per second on a single server&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.seastar-project.org/&#34;&gt;Seastar&lt;/a&gt;: an advanced, open-source C++ framework for high-performance server applications on modern hardware&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/gabime/spdlog&#34;&gt;spdlog&lt;/a&gt;: super fast C++ logging library&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.stellar.org/&#34;&gt;Stellar&lt;/a&gt;: financial platform&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.touchsurgery.com/&#34;&gt;Touch Surgery&lt;/a&gt;: surgery simulator&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TrinityCore/TrinityCore&#34;&gt;TrinityCore&lt;/a&gt;: open-source MMORPG framework&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://userver.tech/&#34;&gt;🐙 userver framework&lt;/a&gt;: open-source asynchronous framework with a rich set of abstractions and database drivers&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/terminal&#34;&gt;Windows Terminal&lt;/a&gt;: the new Windows terminal&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/search?q=fmtlib&amp;amp;type=Code&#34;&gt;More...&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you are aware of other projects using this library, please let me know by &lt;a href=&#34;mailto:victor.zverovich@gmail.com&#34;&gt;email&lt;/a&gt; or by submitting an &lt;a href=&#34;https://github.com/fmtlib/fmt/issues&#34;&gt;issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Motivation&lt;/h1&gt; &#xA;&lt;p&gt;So why yet another formatting library?&lt;/p&gt; &#xA;&lt;p&gt;There are plenty of methods for doing this task, from standard ones like the printf family of function and iostreams to Boost Format and FastFormat libraries. The reason for creating a new library is that every existing solution that I found either had serious issues or didn&#39;t provide all the features I needed.&lt;/p&gt; &#xA;&lt;h2&gt;printf&lt;/h2&gt; &#xA;&lt;p&gt;The good thing about &lt;code&gt;printf&lt;/code&gt; is that it is pretty fast and readily available being a part of the C standard library. The main drawback is that it doesn&#39;t support user-defined types. &lt;code&gt;printf&lt;/code&gt; also has safety issues although they are somewhat mitigated with &lt;a href=&#34;https://gcc.gnu.org/onlinedocs/gcc/Function-Attributes.html&#34;&gt;__attribute__ ((format (printf, ...))&lt;/a&gt; in GCC. There is a POSIX extension that adds positional arguments required for &lt;a href=&#34;https://en.wikipedia.org/wiki/Internationalization_and_localization&#34;&gt;i18n&lt;/a&gt; to &lt;code&gt;printf&lt;/code&gt; but it is not a part of C99 and may not be available on some platforms.&lt;/p&gt; &#xA;&lt;h2&gt;iostreams&lt;/h2&gt; &#xA;&lt;p&gt;The main issue with iostreams is best illustrated with an example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;std::cout &amp;lt;&amp;lt; std::setprecision(2) &amp;lt;&amp;lt; std::fixed &amp;lt;&amp;lt; 1.23456 &amp;lt;&amp;lt; &#34;\n&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which is a lot of typing compared to printf:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-c++&#34;&gt;printf(&#34;%.2f\n&#34;, 1.23456);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Matthew Wilson, the author of FastFormat, called this &#34;chevron hell&#34;. iostreams don&#39;t support positional arguments by design.&lt;/p&gt; &#xA;&lt;p&gt;The good part is that iostreams support user-defined types and are safe although error handling is awkward.&lt;/p&gt; &#xA;&lt;h2&gt;Boost Format&lt;/h2&gt; &#xA;&lt;p&gt;This is a very powerful library that supports both &lt;code&gt;printf&lt;/code&gt;-like format strings and positional arguments. Its main drawback is performance. According to various benchmarks, it is much slower than other methods considered here. Boost Format also has excessive build times and severe code bloat issues (see &lt;a href=&#34;https://raw.githubusercontent.com/fmtlib/fmt/master/#benchmarks&#34;&gt;Benchmarks&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;FastFormat&lt;/h2&gt; &#xA;&lt;p&gt;This is an interesting library that is fast, safe and has positional arguments. However, it has significant limitations, citing its author:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Three features that have no hope of being accommodated within the current design are:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Leading zeros (or any other non-space padding)&lt;/li&gt; &#xA;  &lt;li&gt;Octal/hexadecimal encoding&lt;/li&gt; &#xA;  &lt;li&gt;Runtime width/alignment specification&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;It is also quite big and has a heavy dependency, on STLSoft, which might be too restrictive for use in some projects.&lt;/p&gt; &#xA;&lt;h2&gt;Boost Spirit.Karma&lt;/h2&gt; &#xA;&lt;p&gt;This is not a formatting library but I decided to include it here for completeness. As iostreams, it suffers from the problem of mixing verbatim text with arguments. The library is pretty fast, but slower on integer formatting than &lt;code&gt;fmt::format_to&lt;/code&gt; with format string compilation on Karma&#39;s own benchmark, see &lt;a href=&#34;http://www.zverovich.net/2020/06/13/fast-int-to-string-revisited.html&#34;&gt;Converting a hundred million integers to strings per second&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;{fmt} is distributed under the MIT &lt;a href=&#34;https://github.com/fmtlib/fmt/raw/master/LICENSE&#34;&gt;license&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Documentation License&lt;/h1&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://fmt.dev/latest/syntax/&#34;&gt;Format String Syntax&lt;/a&gt; section in the documentation is based on the one from Python &lt;a href=&#34;https://docs.python.org/3/library/string.html#module-string&#34;&gt;string module documentation&lt;/a&gt;. For this reason, the documentation is distributed under the Python Software Foundation license available in &lt;a href=&#34;https://raw.github.com/fmtlib/fmt/master/doc/python-license.txt&#34;&gt;doc/python-license.txt&lt;/a&gt;. It only applies if you distribute the documentation of {fmt}.&lt;/p&gt; &#xA;&lt;h1&gt;Maintainers&lt;/h1&gt; &#xA;&lt;p&gt;The {fmt} library is maintained by Victor Zverovich (&lt;a href=&#34;https://github.com/vitaut&#34;&gt;vitaut&lt;/a&gt;) with contributions from many other people. See &lt;a href=&#34;https://github.com/fmtlib/fmt/graphs/contributors&#34;&gt;Contributors&lt;/a&gt; and &lt;a href=&#34;https://github.com/fmtlib/fmt/releases&#34;&gt;Releases&lt;/a&gt; for some of the names. Let us know if your contribution is not listed or mentioned incorrectly and we&#39;ll make it right.&lt;/p&gt; &#xA;&lt;h1&gt;Security Policy&lt;/h1&gt; &#xA;&lt;p&gt;To report a security issue, please disclose it at &lt;a href=&#34;https://github.com/fmtlib/fmt/security/advisories/new&#34;&gt;security advisory&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This project is maintained by a team of volunteers on a reasonable-effort basis. As such, please give us at least &lt;em&gt;90&lt;/em&gt; days to work on a fix before public exposure.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>NVlabs/VILA</title>
    <updated>2024-08-23T01:29:16Z</updated>
    <id>tag:github.com,2024-08-23:/NVlabs/VILA</id>
    <link href="https://github.com/NVlabs/VILA" rel="alternate"></link>
    <summary type="html">&lt;p&gt;VILA - a multi-image visual language model with training, inference and evaluation recipe, deployable from cloud to edge (Jetson Orin and laptops)&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/vila-logo.jpg&#34; width=&#34;20%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;VILA: On Pre-training for Visual Language Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/CODE_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/MODEL_LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/MODEL%20License-CC%20By%20NC%204.0-red.svg?sanitize=true&#34; alt=&#34;Model License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-3100/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.10+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.10+&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2312.07533&#34;&gt;VILA arxiv&lt;/a&gt; / &lt;a href=&#34;https://vila-demo.hanlab.ai/&#34;&gt;VILA Demo&lt;/a&gt; / &lt;a href=&#34;https://huggingface.co/collections/Efficient-Large-Model/vila-on-pre-training-for-visual-language-models-65d8022a3a52cd9bcd62698e&#34;&gt;VILA Huggingface&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💡 Introduction&lt;/h2&gt; &#xA;&lt;p&gt;VILA is a visual language model (VLM) pretrained with interleaved image-text data at scale, enabling &lt;strong&gt;video understanding&lt;/strong&gt; and &lt;strong&gt;multi-image understanding&lt;/strong&gt; capabilities. VILA is deployable on the edge by &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt; 4bit quantization and &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; framework. We find: (1) image-text pairs are not enough, interleaved image-text is essential; (2) unfreezing LLM during interleaved image-text pre-training enables in-context learning; (3)re-blending text-only instruction data is crucial to boost both VLM and text-only performance; (4) token compression extends #video frames. VILA unveils appealing capabilities, including: video reasoning, in-context learning, visual chain-of-thought, and better world knowledge.&lt;/p&gt; &#xA;&lt;h2&gt;💡 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2024/08] We release &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/LongVILA.md&#34;&gt;LongVILA&lt;/a&gt; that supports long video understanding (Captioning, QA, Needle-in-a-Haystack) up to 1024 frames.&lt;/li&gt; &#xA; &lt;li&gt;[2024/07] VILA1.5 also ranks 1st place (OSS model) on &lt;a href=&#34;https://github.com/JUNJIE99/MLVU&#34;&gt;MLVU test leaderboard&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/06] VILA1.5 is now the best open sourced VLM on &lt;a href=&#34;https://mmmu-benchmark.github.io/#leaderboard&#34;&gt;MMMU leaderboard&lt;/a&gt; and &lt;a href=&#34;https://video-mme.github.io/home_page.html#leaderboard&#34;&gt;Video-MME&lt;/a&gt; leaderboard!&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] We release VILA-1.5, which offers &lt;strong&gt;video understanding capability&lt;/strong&gt;. VILA-1.5 comes with four model sizes: 3B/8B/13B/40B.&lt;/li&gt; &#xA; &lt;li&gt;[2024/05] We release &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt;-quantized 4bit VILA-1.5 models. VILA-1.5 is efficiently deployable on diverse NVIDIA GPUs (A100, 4090, 4070 Laptop, Orin, Orin Nano) by &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_trt_llm&#34;&gt;TensorRT-LLM&lt;/a&gt; backends.&lt;/li&gt; &#xA; &lt;li&gt;[2024/03] VILA has been accepted by CVPR 2024!&lt;/li&gt; &#xA; &lt;li&gt;[2024/02] We release &lt;a href=&#34;https://arxiv.org/pdf/2306.00978.pdf&#34;&gt;AWQ&lt;/a&gt;-quantized 4bit VILA models, deployable on Jetson Orin and laptops through &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt; and &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine&#34;&gt;TinyChatEngine&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2024/02] VILA is released. We propose interleaved image-text pretraining that enables &lt;strong&gt;multi-image&lt;/strong&gt; VLM. VILA comes with impressive in-context learning capabilities. We open source everything: including training code, evaluation code, datasets, model ckpts.&lt;/li&gt; &#xA; &lt;li&gt;[2023/12] &lt;a href=&#34;https://arxiv.org/abs/2312.07533&#34;&gt;Paper&lt;/a&gt; is on Arxiv!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;h3&gt;Image QA Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Prec.&lt;/th&gt; &#xA;   &lt;th&gt;VQAv2&lt;/th&gt; &#xA;   &lt;th&gt;GQA&lt;/th&gt; &#xA;   &lt;th&gt;VizWiz&lt;/th&gt; &#xA;   &lt;th&gt;SQA-I&lt;/th&gt; &#xA;   &lt;th&gt;VQA-T&lt;/th&gt; &#xA;   &lt;th&gt;POPE&lt;/th&gt; &#xA;   &lt;th&gt;MME&lt;/th&gt; &#xA;   &lt;th&gt;MMB&lt;/th&gt; &#xA;   &lt;th&gt;MMB-CN&lt;/th&gt; &#xA;   &lt;th&gt;SEED&lt;/th&gt; &#xA;   &lt;th&gt;SEED-I&lt;/th&gt; &#xA;   &lt;th&gt;MMMU (val)&lt;/th&gt; &#xA;   &lt;th&gt;MMMU (test)&lt;/th&gt; &#xA;   &lt;th&gt;llava-bench&lt;/th&gt; &#xA;   &lt;th&gt;MM-Vet&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;80.4&lt;/td&gt; &#xA;   &lt;td&gt;61.5&lt;/td&gt; &#xA;   &lt;td&gt;53.5&lt;/td&gt; &#xA;   &lt;td&gt;69.0&lt;/td&gt; &#xA;   &lt;td&gt;60.4&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;1442.44&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;52.7&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;   &lt;td&gt;67.9&lt;/td&gt; &#xA;   &lt;td&gt;33.3&lt;/td&gt; &#xA;   &lt;td&gt;30.8&lt;/td&gt; &#xA;   &lt;td&gt;75.9&lt;/td&gt; &#xA;   &lt;td&gt;35.4&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;80.0&lt;/td&gt; &#xA;   &lt;td&gt;61.1&lt;/td&gt; &#xA;   &lt;td&gt;53.8&lt;/td&gt; &#xA;   &lt;td&gt;67.8&lt;/td&gt; &#xA;   &lt;td&gt;60.4&lt;/td&gt; &#xA;   &lt;td&gt;85.9&lt;/td&gt; &#xA;   &lt;td&gt;1437.34&lt;/td&gt; &#xA;   &lt;td&gt;63.3&lt;/td&gt; &#xA;   &lt;td&gt;51.4&lt;/td&gt; &#xA;   &lt;td&gt;59.8&lt;/td&gt; &#xA;   &lt;td&gt;66.6&lt;/td&gt; &#xA;   &lt;td&gt;32.7&lt;/td&gt; &#xA;   &lt;td&gt;31.1&lt;/td&gt; &#xA;   &lt;td&gt;75.0&lt;/td&gt; &#xA;   &lt;td&gt;37.3&lt;/td&gt; &#xA;   &lt;td&gt;59.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;79.8&lt;/td&gt; &#xA;   &lt;td&gt;61.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;69.6&lt;/td&gt; &#xA;   &lt;td&gt;63.4&lt;/td&gt; &#xA;   &lt;td&gt;85.3&lt;/td&gt; &#xA;   &lt;td&gt;1431.65&lt;/td&gt; &#xA;   &lt;td&gt;62.8&lt;/td&gt; &#xA;   &lt;td&gt;52.2&lt;/td&gt; &#xA;   &lt;td&gt;60.0&lt;/td&gt; &#xA;   &lt;td&gt;66.4&lt;/td&gt; &#xA;   &lt;td&gt;32.8&lt;/td&gt; &#xA;   &lt;td&gt;31.3&lt;/td&gt; &#xA;   &lt;td&gt;76.7&lt;/td&gt; &#xA;   &lt;td&gt;38.6&lt;/td&gt; &#xA;   &lt;td&gt;60.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;79.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;62.3&lt;/td&gt; &#xA;   &lt;td&gt;69.2&lt;/td&gt; &#xA;   &lt;td&gt;63.0&lt;/td&gt; &#xA;   &lt;td&gt;85.8&lt;/td&gt; &#xA;   &lt;td&gt;1417.06&lt;/td&gt; &#xA;   &lt;td&gt;61.6&lt;/td&gt; &#xA;   &lt;td&gt;51.5&lt;/td&gt; &#xA;   &lt;td&gt;59.1&lt;/td&gt; &#xA;   &lt;td&gt;65.7&lt;/td&gt; &#xA;   &lt;td&gt;33.4&lt;/td&gt; &#xA;   &lt;td&gt;30.4&lt;/td&gt; &#xA;   &lt;td&gt;77.1&lt;/td&gt; &#xA;   &lt;td&gt;36.7&lt;/td&gt; &#xA;   &lt;td&gt;60.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;80.9&lt;/td&gt; &#xA;   &lt;td&gt;61.9&lt;/td&gt; &#xA;   &lt;td&gt;58.7&lt;/td&gt; &#xA;   &lt;td&gt;79.9&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;84.4&lt;/td&gt; &#xA;   &lt;td&gt;1577.01&lt;/td&gt; &#xA;   &lt;td&gt;72.3&lt;/td&gt; &#xA;   &lt;td&gt;66.2&lt;/td&gt; &#xA;   &lt;td&gt;64.2&lt;/td&gt; &#xA;   &lt;td&gt;71.4&lt;/td&gt; &#xA;   &lt;td&gt;36.9&lt;/td&gt; &#xA;   &lt;td&gt;36.0&lt;/td&gt; &#xA;   &lt;td&gt;80.0&lt;/td&gt; &#xA;   &lt;td&gt;38.3&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;80.3&lt;/td&gt; &#xA;   &lt;td&gt;61.7&lt;/td&gt; &#xA;   &lt;td&gt;59.3&lt;/td&gt; &#xA;   &lt;td&gt;79.0&lt;/td&gt; &#xA;   &lt;td&gt;65.4&lt;/td&gt; &#xA;   &lt;td&gt;82.9&lt;/td&gt; &#xA;   &lt;td&gt;1593.65&lt;/td&gt; &#xA;   &lt;td&gt;71.0&lt;/td&gt; &#xA;   &lt;td&gt;64.9&lt;/td&gt; &#xA;   &lt;td&gt;64.0&lt;/td&gt; &#xA;   &lt;td&gt;71.1&lt;/td&gt; &#xA;   &lt;td&gt;36.0&lt;/td&gt; &#xA;   &lt;td&gt;36.1&lt;/td&gt; &#xA;   &lt;td&gt;79.0&lt;/td&gt; &#xA;   &lt;td&gt;37.2&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;82.8&lt;/td&gt; &#xA;   &lt;td&gt;64.3&lt;/td&gt; &#xA;   &lt;td&gt;62.6&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;65.0&lt;/td&gt; &#xA;   &lt;td&gt;86.3&lt;/td&gt; &#xA;   &lt;td&gt;1569.55&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;   &lt;td&gt;33.6&lt;/td&gt; &#xA;   &lt;td&gt;80.8&lt;/td&gt; &#xA;   &lt;td&gt;44.3&lt;/td&gt; &#xA;   &lt;td&gt;66.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;82.7&lt;/td&gt; &#xA;   &lt;td&gt;64.5&lt;/td&gt; &#xA;   &lt;td&gt;63.3&lt;/td&gt; &#xA;   &lt;td&gt;79.7&lt;/td&gt; &#xA;   &lt;td&gt;64.7&lt;/td&gt; &#xA;   &lt;td&gt;86.7&lt;/td&gt; &#xA;   &lt;td&gt;1531.35&lt;/td&gt; &#xA;   &lt;td&gt;74.7&lt;/td&gt; &#xA;   &lt;td&gt;66.7&lt;/td&gt; &#xA;   &lt;td&gt;65.1&lt;/td&gt; &#xA;   &lt;td&gt;72.6&lt;/td&gt; &#xA;   &lt;td&gt;37.8&lt;/td&gt; &#xA;   &lt;td&gt;34.0&lt;/td&gt; &#xA;   &lt;td&gt;81.9&lt;/td&gt; &#xA;   &lt;td&gt;46.4&lt;/td&gt; &#xA;   &lt;td&gt;66.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;84.3&lt;/td&gt; &#xA;   &lt;td&gt;64.6&lt;/td&gt; &#xA;   &lt;td&gt;62.2&lt;/td&gt; &#xA;   &lt;td&gt;87.2&lt;/td&gt; &#xA;   &lt;td&gt;73.6&lt;/td&gt; &#xA;   &lt;td&gt;87.3&lt;/td&gt; &#xA;   &lt;td&gt;1726.82&lt;/td&gt; &#xA;   &lt;td&gt;82.4&lt;/td&gt; &#xA;   &lt;td&gt;80.2&lt;/td&gt; &#xA;   &lt;td&gt;69.1&lt;/td&gt; &#xA;   &lt;td&gt;75.8&lt;/td&gt; &#xA;   &lt;td&gt;51.9&lt;/td&gt; &#xA;   &lt;td&gt;46.9&lt;/td&gt; &#xA;   &lt;td&gt;81.3&lt;/td&gt; &#xA;   &lt;td&gt;53.0&lt;/td&gt; &#xA;   &lt;td&gt;72.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;84.1&lt;/td&gt; &#xA;   &lt;td&gt;64.4&lt;/td&gt; &#xA;   &lt;td&gt;61.3&lt;/td&gt; &#xA;   &lt;td&gt;86.7&lt;/td&gt; &#xA;   &lt;td&gt;73.2&lt;/td&gt; &#xA;   &lt;td&gt;88.2&lt;/td&gt; &#xA;   &lt;td&gt;1714.79&lt;/td&gt; &#xA;   &lt;td&gt;83.2&lt;/td&gt; &#xA;   &lt;td&gt;79.6&lt;/td&gt; &#xA;   &lt;td&gt;68.9&lt;/td&gt; &#xA;   &lt;td&gt;75.6&lt;/td&gt; &#xA;   &lt;td&gt;49.3&lt;/td&gt; &#xA;   &lt;td&gt;46.2&lt;/td&gt; &#xA;   &lt;td&gt;83.0&lt;/td&gt; &#xA;   &lt;td&gt;51.4&lt;/td&gt; &#xA;   &lt;td&gt;72.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: VQAV2 and VizWiz are test-dev, the average accuracy is calculated over all datasets and MME numbers are divided by 20.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Video QA Benchmarks&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Prec.&lt;/th&gt; &#xA;   &lt;th&gt;Perception Test&lt;/th&gt; &#xA;   &lt;th&gt;ActivityNet&lt;/th&gt; &#xA;   &lt;th&gt;MSVD&lt;/th&gt; &#xA;   &lt;th&gt;MSRVTT&lt;/th&gt; &#xA;   &lt;th&gt;TGIF&lt;/th&gt; &#xA;   &lt;th&gt;EgoSchema (test)&lt;/th&gt; &#xA;   &lt;th&gt;CinePile&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;47&lt;/td&gt; &#xA;   &lt;td&gt;50.2&lt;/td&gt; &#xA;   &lt;td&gt;76.6&lt;/td&gt; &#xA;   &lt;td&gt;57.5&lt;/td&gt; &#xA;   &lt;td&gt;51.7&lt;/td&gt; &#xA;   &lt;td&gt;42.6&lt;/td&gt; &#xA;   &lt;td&gt;37.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;49.7&lt;/td&gt; &#xA;   &lt;td&gt;50.7&lt;/td&gt; &#xA;   &lt;td&gt;76.9&lt;/td&gt; &#xA;   &lt;td&gt;57.6&lt;/td&gt; &#xA;   &lt;td&gt;51.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;54.1&lt;/td&gt; &#xA;   &lt;td&gt;54.3&lt;/td&gt; &#xA;   &lt;td&gt;78.3&lt;/td&gt; &#xA;   &lt;td&gt;60.1&lt;/td&gt; &#xA;   &lt;td&gt;54.1&lt;/td&gt; &#xA;   &lt;td&gt;50.4&lt;/td&gt; &#xA;   &lt;td&gt;48.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;53.6&lt;/td&gt; &#xA;   &lt;td&gt;54.7&lt;/td&gt; &#xA;   &lt;td&gt;77.9&lt;/td&gt; &#xA;   &lt;td&gt;60.2&lt;/td&gt; &#xA;   &lt;td&gt;56&lt;/td&gt; &#xA;   &lt;td&gt;52.2&lt;/td&gt; &#xA;   &lt;td&gt;50.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;54&lt;/td&gt; &#xA;   &lt;td&gt;58&lt;/td&gt; &#xA;   &lt;td&gt;80.1&lt;/td&gt; &#xA;   &lt;td&gt;63&lt;/td&gt; &#xA;   &lt;td&gt;58.2&lt;/td&gt; &#xA;   &lt;td&gt;58.7&lt;/td&gt; &#xA;   &lt;td&gt;51.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference speed ( Token/sec )&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;$~~~~~~$&lt;/th&gt; &#xA;   &lt;th&gt;Precision&lt;/th&gt; &#xA;   &lt;th&gt;A100&lt;/th&gt; &#xA;   &lt;th&gt;4090&lt;/th&gt; &#xA;   &lt;th&gt;Orin&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;104.6&lt;/td&gt; &#xA;   &lt;td&gt;137.6&lt;/td&gt; &#xA;   &lt;td&gt;25.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;182.8&lt;/td&gt; &#xA;   &lt;td&gt;215.5&lt;/td&gt; &#xA;   &lt;td&gt;42.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;104.3&lt;/td&gt; &#xA;   &lt;td&gt;137.2&lt;/td&gt; &#xA;   &lt;td&gt;24.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-3B-S2-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;180.2&lt;/td&gt; &#xA;   &lt;td&gt;219.3&lt;/td&gt; &#xA;   &lt;td&gt;40.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;74.9&lt;/td&gt; &#xA;   &lt;td&gt;57.4&lt;/td&gt; &#xA;   &lt;td&gt;10.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-3-VILA1.5-8B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;168.9&lt;/td&gt; &#xA;   &lt;td&gt;150.2&lt;/td&gt; &#xA;   &lt;td&gt;28.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;50.9&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;6.1&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-13B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;115.9&lt;/td&gt; &#xA;   &lt;td&gt;105.7&lt;/td&gt; &#xA;   &lt;td&gt;20.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B&lt;/td&gt; &#xA;   &lt;td&gt;fp16&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VILA1.5-40B-AWQ&lt;/td&gt; &#xA;   &lt;td&gt;int4&lt;/td&gt; &#xA;   &lt;td&gt;57.0&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;--&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: Measured using the &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tinychat&#34;&gt;TinyChat&lt;/a&gt; backend at batch size = 1.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h2&gt;VILA Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Video captioning&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/156256291/c9520943-2478-4f97-bc95-121d625018a6&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/156256291/c9520943-2478-4f97-bc95-121d625018a6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: Elaborate on the visual and narrative elements of the video in detail.&lt;/p&gt; &#xA;&lt;p&gt;Caption: The video shows a person&#39;s hands working on a white surface. They are folding a piece of fabric with a checkered pattern in shades of blue and white. The fabric is being folded into a smaller, more compact shape. The person&#39;s fingernails are painted red, and they are wearing a black and red garment. There are also a ruler and a pencil on the surface, suggesting that measurements and precision are involved in the process.&lt;/p&gt; &#xA;&lt;h3&gt;In context learning&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/demo_img_1.png&#34; height=&#34;239&#34;&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/demo_img_2.png&#34; height=&#34;250&#34;&gt; &#xA;&lt;h3&gt;Multi-image reasoning&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/demo_images/demo_img_3.png&#34; height=&#34;193&#34;&gt; &#xA;&lt;h3&gt;VILA on Jetson Orin&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/7783214/6079374c-0787-4bc4-b9c6-e1524b4c9dc4&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/7783214/6079374c-0787-4bc4-b9c6-e1524b4c9dc4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;VILA on RTX 4090&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Efficient-Large-Model/VILA/assets/7783214/80c47742-e873-4080-ad7d-d17c4700539f&#34;&gt;https://github.com/Efficient-Large-Model/VILA/assets/7783214/80c47742-e873-4080-ad7d-d17c4700539f&lt;/a&gt;&lt;/p&gt;  &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./environment_setup.sh vila&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;VILA training contains three steps, for specific hyperparameters, please check out the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/scripts/v1_5&#34;&gt;scripts/v1_5&lt;/a&gt; folder:&lt;/p&gt; &#xA;&lt;h3&gt;Step-1: Alignment&lt;/h3&gt; &#xA;&lt;p&gt;We utilize LLaVA-CC3M-Pretrain-595K dataset to align the textual and visual modalities.&lt;/p&gt; &#xA;&lt;p&gt;The stage 1 script takes in two parameters and it can run on a single 8xA100 node. &lt;code&gt;BASE_MODEL_PATH&lt;/code&gt; points to a online or local huggingface repository, such as &lt;code&gt;NousResearch/Llama-2-7b-hf&lt;/code&gt;. &lt;code&gt;OUTPUT_NAME&lt;/code&gt; points to a target directory under &lt;code&gt;checkpoints&lt;/code&gt;, which will save the trained multimodal projector afterwards.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/1_mm_align.sh [BASE_MODEL_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step-2: Pretraining&lt;/h3&gt; &#xA;&lt;p&gt;We use MMC4 and Coyo dataset to train VLM with interleaved image-text pairs.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/2_pretrain_mmc4_coyo.sh [CODE_PATH] [BASE_MODEL_PATH] [STAGE1_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The stage 2 script takes in four arguments. &lt;code&gt;CODE_PATH&lt;/code&gt; is the absolute path to our VILA codebase, &lt;code&gt;BASE_MODEL_PATH&lt;/code&gt; has similar meaning to what is presented in the stage 1 script. &lt;code&gt;STAGE1_PATH&lt;/code&gt; points to the &lt;code&gt;OUTPUT_NAME&lt;/code&gt; of stage 1 (i.e. where the stage 1 checkpoint is stored). &lt;code&gt;OUTPUT_NAME&lt;/code&gt; is the desired folder name under &lt;code&gt;checkpoints&lt;/code&gt; that saves the pretraining checkpoint. The script we provided for this stage is executed on slurm, and we expect it to execute on 16 nodes (128 GPUs).&lt;/p&gt; &#xA;&lt;h3&gt;Step-3: Supervised fine-tuning&lt;/h3&gt; &#xA;&lt;p&gt;This is the last stage of VILA training, in which we tune the model to follow multimodal instructions on a subset of M3IT, FLAN and ShareGPT4V. This stage runs on a 8xA100 node.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/v1_5/paper/3_sft.sh [STAGE2_PATH] [OUTPUT_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The stage 3 script takes in two arguments. &lt;code&gt;STAGE2_PATH&lt;/code&gt; points to the &lt;code&gt;OUTPUT_NAME&lt;/code&gt; of the stage 2 script (i.e. where the stage 2 checkpoint is stored). &lt;code&gt;OUTPUT_NAME&lt;/code&gt; is the desired folder name under &lt;code&gt;checkpoints&lt;/code&gt; that stores the final checkpoint.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluations&lt;/h2&gt; &#xA;&lt;h3&gt;Image Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;You can follow &lt;a href=&#34;https://github.com/haotian-liu/LLaVA/raw/main/docs/Evaluation.md&#34;&gt;Llava1.5 eval&lt;/a&gt; to download all datasets. After downloading all datasets, please put them under &lt;code&gt;playground/data/eval&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Please make the following changes to the MME evaluation script. Please search for:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_path = &#34;MME_Benchmark_release_version&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and replace it with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;data_path = os.path.join(script_dir, &#34;MME_Benchmark_release_version&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We provide a push-the-button script to perform evaluation on all 10 datasets that do not require GPT-assisted evaluation:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/v1_5/eval/eval_all.sh [CHECKPOINT_PATH] [MODEL_NAME] [CONV_MODE]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This script takes in two parameters, &lt;code&gt;CHECKPOINT_PATH&lt;/code&gt; points to the stage 3 model checkpoint, and &lt;code&gt;MODEL_NAME&lt;/code&gt; will be the name of evaluation results.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/830/my-submission&#34;&gt;VQAv2&lt;/a&gt; and &lt;a href=&#34;https://eval.ai/web/challenges/challenge-page/2185/my-submission&#34;&gt;Vizwiz&lt;/a&gt; evaluations are hosted on eval.ai. You need to register an account and create a team to be able to submit eval.&lt;/p&gt; &#xA;&lt;p&gt;MMBench and MMBench_CN eval are hosted on another &lt;a href=&#34;https://opencompass.org.cn/leaderboard-multimodal&#34;&gt;evaluation server&lt;/a&gt;. Make sure you change the name of the file before submitting, otherwise the server caches results and will always return wrong result to you.&lt;/p&gt; &#xA;&lt;p&gt;We provide a quick script to automatically organize the prediction files that need to be submitted to servers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python scripts/v1_5/eval/copy_predictions.py [MODEL_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will be able to find the predictions under &lt;code&gt;playground/data/predictions_upload/[MODEL_NAME]&lt;/code&gt; after executing this script.&lt;/p&gt; &#xA;&lt;h3&gt;Video Benchmarks&lt;/h3&gt; &#xA;&lt;p&gt;Please follow the evaluation steps in &lt;a href=&#34;https://github.com/PKU-YuanGroup/Video-LLaVA/raw/main/TRAIN_AND_VALIDATE.md#data-for-validating&#34;&gt;Video-LLaVA&lt;/a&gt; for dataset preparation.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./scripts/v1_5/eval/video_chatgpt/run_all.sh [CHECKPOINT_PATH] [MODEL_NAME] [CONV_MODE]&#xA;./scripts/v1_5/eval/video_chatgpt/eval_all.sh [MODEL_NAME]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;We provide snippets for quick inference with user prompts and images.&lt;/p&gt; &#xA;&lt;p&gt;Llama-3-VILA1.5-8B inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/Llama-3-VILA1.5-8b \&#xA;    --conv-mode llama_3 \&#xA;    --query &#34;&amp;lt;image&amp;gt;\n Please describe the traffic condition.&#34; \&#xA;    --image-file &#34;av.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;VILA1.5-40B inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/VILA1.5-40b \&#xA;    --conv-mode hermes-2 \&#xA;    --query &#34;&amp;lt;image&amp;gt;\n Please describe the traffic condition.&#34; \&#xA;    --image-file &#34;av.png&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;VILA1.5-3B video inference:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore llava/eval/run_vila.py \&#xA;    --model-path Efficient-Large-Model/VILA1.5-3b \&#xA;    --conv-mode vicuna_v1 \&#xA;    --query &#34;&amp;lt;video&amp;gt;\n Please describe this video.&#34; \&#xA;    --video-file &#34;demo.mp4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quantization and Deployment&lt;/h2&gt; &#xA;&lt;p&gt;Our VILA models are quantized by &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt; into 4 bits for efficient inference on the edge. We provide a push-the-button &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/raw/main/scripts/vila_example.sh&#34;&gt;script&lt;/a&gt; to quantize VILA with AWQ.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA on desktop GPUs and edge GPUs&lt;/h3&gt; &#xA;&lt;p&gt;We support AWQ-quantized 4bit VILA on GPU platforms via &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat&#34;&gt;TinyChat&lt;/a&gt;. We provide a &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat#support-vlm-models-vila--llava&#34;&gt;tutorial&lt;/a&gt; to run the model with TinyChat after quantization. We also provide an &lt;a href=&#34;https://github.com/mit-han-lab/llm-awq/tree/main/tinychat/serve&#34;&gt;instruction&lt;/a&gt; to launch a Gradio server (powered by TinyChat and AWQ) to serve 4-bit quantized VILA models.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA on laptops&lt;/h3&gt; &#xA;&lt;p&gt;We further support our AWQ-quantized 4bit VILA models on various CPU platforms with both x86 and ARM architectures with our &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine&#34;&gt;TinyChatEngine&lt;/a&gt;. We also provide a detailed &lt;a href=&#34;https://github.com/mit-han-lab/TinyChatEngine/tree/main?tab=readme-ov-file#deploy-vision-language-model-vlm-chatbot-with-tinychatengine&#34;&gt;tutorial&lt;/a&gt; to help the users deploy VILA on different CPUs.&lt;/p&gt; &#xA;&lt;h3&gt;Running VILA API server&lt;/h3&gt; &#xA;&lt;p&gt;A simple API server has been provided to serve VILA models. The server is built on top of &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/transformers/&#34;&gt;Huggingface Transformers&lt;/a&gt;. The server can be run with the following command:&lt;/p&gt; &#xA;&lt;h4&gt;With CLI&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -W ignore server.py \&#xA;    --port 8000 \&#xA;    --model-path Efficient-Large-Model/VILA1.5-3B \&#xA;    --conv-mode vicuna_v1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;With Docker&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker build -t vila-server:latest .&#xA;docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \&#xA;    -v ./hub:/root/.cache/huggingface/hub \&#xA;    -it --rm -p 8000:8000 \&#xA;    -e VILA_MODEL_PATH=Efficient-Large-Model/VILA1.5-3B \&#xA;    -e VILA_CONV_MODE=vicuna_v1 \&#xA;    vila-server:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can call the endpoint with the OpenAI SDK as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;&#xA;client = OpenAI(&#xA;    base_url=&#34;http://localhost:8000&#34;,&#xA;    api_key=&#34;fake-key&#34;,&#xA;)&#xA;response = client.chat.completions.create(&#xA;    messages=[&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: [&#xA;                {&#34;type&#34;: &#34;text&#34;, &#34;text&#34;: &#34;What’s in this image?&#34;},&#xA;                {&#xA;                    &#34;type&#34;: &#34;image_url&#34;,&#xA;                    &#34;image_url&#34;: {&#xA;                        &#34;url&#34;: &#34;https://blog.logomyway.com/wp-content/uploads/2022/01/NVIDIA-logo.jpg&#34;,&#xA;                        # Or you can pass in a base64 encoded image&#xA;                        # &#34;url&#34;: &#34;data:image/png;base64,&amp;lt;base64_encoded_image&amp;gt;&#34;,&#xA;                    },&#xA;                },&#xA;            ],&#xA;        }&#xA;    ],&#xA;    max_tokens=300,&#xA;    model=&#34;VILA1.5-3B&#34;,&#xA;    # You can pass in extra parameters as follows&#xA;    extra_body={&#34;num_beams&#34;: 1, &#34;use_cache&#34;: False},&#xA;)&#xA;print(response.choices[0].message.content)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;sup&gt;NOTE: This API server is intended for evaluation purposes only and has not been optimized for production use. It has only been tested on A100 and H100 GPUs.&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Checkpoints&lt;/h2&gt; &#xA;&lt;p&gt;We release &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b&#34;&gt;VILA1.5-3B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-s2&#34;&gt;VILA1.5-3B-S2&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/Llama-3-VILA1.5-8b&#34;&gt;Llama-3-VILA1.5-8B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-13b&#34;&gt;VILA1.5-13B&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-40b&#34;&gt;VILA1.5-40B&lt;/a&gt; and the 4-bit &lt;a href=&#34;https://arxiv.org/abs/2306.00978&#34;&gt;AWQ&lt;/a&gt;-quantized models &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-AWQ&#34;&gt;VILA1.5-3B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-3b-s2-AWQ&#34;&gt;VILA1.5-3B-S2-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/Llama-3-VILA1.5-8b-AWQ&#34;&gt;Llama-3-VILA1.5-8B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-13b-AWQ&#34;&gt;VILA1.5-13B-AWQ&lt;/a&gt;, &lt;a href=&#34;https://hf.co/Efficient-Large-Model/VILA1.5-40b-AWQ&#34;&gt;VILA1.5-40B-AWQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;🔒 License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The code is released under the Apache 2.0 license as found in the &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;The pretrained weights are released under the &lt;a href=&#34;https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en&#34;&gt;CC-BY-NC-SA-4.0 license&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;The service is a research preview intended for non-commercial use only, and is subject to the following licenses and terms: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;Model License&lt;/a&gt; of LLaMA. For LLAMA3-VILA checkpoints terms of use, please refer to the &lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;LLAMA3 License&lt;/a&gt; for additional details.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://openai.com/policies/terms-of-use&#34;&gt;Terms of Use&lt;/a&gt; of the data generated by OpenAI&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/data_prepare/LICENSE&#34;&gt;Dataset Licenses&lt;/a&gt; for each one used during training.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Team&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=OI7zFmwAAAAJ&amp;amp;hl=en&#34;&gt;*Yao Lu&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hongxu-yin.github.io/&#34;&gt;*Hongxu Yin&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linji.me/&#34;&gt;*Ji Lin&lt;/a&gt;: OpenAI (work done at Nvidia and MIT)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=6gKEYRgAAAAJ&amp;amp;hl=en&#34;&gt;Wei Ping&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.pmolchanov.com/&#34;&gt;Pavlo Molchanov&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=Wel9l1wAAAAJ&amp;amp;hl=en&#34;&gt;Andrew Tao&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://kentang.net/&#34;&gt;Haotian Tang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ys-2020.github.io/&#34;&gt;Shang Yang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://lzhu.me/&#34;&gt;Ligeng Zhu&lt;/a&gt;: Nvidia, MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://weichenwang.me/&#34;&gt;Wei-Chen Wang&lt;/a&gt;: MIT&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://xuefuzhao.github.io/&#34;&gt;Fuzhao Xue&lt;/a&gt;: Nvidia, NUS&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://seerkfang.github.io/&#34;&gt;Yunhao Fang&lt;/a&gt;: Nvidia, UCSD&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://yukangchen.com/&#34;&gt;Yukang Chen&lt;/a&gt;: Nvidia, CUHK&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://openreview.net/profile?id=~Zhuoyang_Zhang1&#34;&gt;Zhuoyang Zhang&lt;/a&gt;: Nvidia, Tsinghua Univ.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.linkedin.com/in/yue-james-shen/&#34;&gt;Yue Shen&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=6xFvyJwAAAAJ&amp;amp;hl=en&#34;&gt;Wei-Ming Chen&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=r5WezOYAAAAJ&amp;amp;hl=zh-CN&#34;&gt;Huizi Mao&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://bfshi.github.io/&#34;&gt;Baifeng Shi&lt;/a&gt;: Nvidia, UC Berkeley&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://jankautz.com/&#34;&gt;Jan Kautz&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=62ElavIAAAAJ&amp;amp;hl=en&#34;&gt;Mohammad Shoeybi&lt;/a&gt;: Nvidia&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;http://songhan.mit.edu/&#34;&gt;Song Han&lt;/a&gt;: Nvidia, MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lin2023vila,&#xA;      title={VILA: On Pre-training for Visual Language Models},&#xA;      author={Ji Lin and Hongxu Yin and Wei Ping and Yao Lu and Pavlo Molchanov and Andrew Tao and Huizi Mao and Jan Kautz and Mohammad Shoeybi and Song Han},&#xA;      year={2023},&#xA;      eprint={2312.07533},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;: the codebase we built upon. Thanks for their wonderful work.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OpenGVLab/InternVL&#34;&gt;InternVL&lt;/a&gt;: for open-sourcing InternViT (used in VILA1.5-40b) and the &lt;a href=&#34;https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat#prepare-training-datasets&#34;&gt;InternVL-SFT&lt;/a&gt; data blend (inspired by LLaVA-1.6) used in all VILA1.5 models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lm-sys/FastChat&#34;&gt;Vicuna&lt;/a&gt;: the amazing open-sourced large language model!&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/mbzuai-oryx/Video-ChatGPT&#34;&gt;Video-ChatGPT&lt;/a&gt;: we borrowed video evaluation script from this repository.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/allenai/mmc4&#34;&gt;MMC4&lt;/a&gt;, &lt;a href=&#34;https://github.com/kakaobrain/coyo-dataset&#34;&gt;COYO-700M&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/MMInstruction/M3IT&#34;&gt;M3IT&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/Open-Orca/FLAN&#34;&gt;OpenORCA/FLAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/NVlabs/VILA/main/google-research-datasets/wit&#34;&gt;WIT&lt;/a&gt;, &lt;a href=&#34;https://github.com/OFA-Sys/gsm8k-ScRel/raw/main/data/train_use.jsonl&#34;&gt;GSM8K-ScRel&lt;/a&gt;, &lt;a href=&#34;https://visualgenome.org/api/v0/api_home.html&#34;&gt;VisualGenome&lt;/a&gt;, &lt;a href=&#34;https://visualcommonsense.com/download/&#34;&gt;VCR&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/derek-thomas/ScienceQA&#34;&gt;ScienceQA&lt;/a&gt;, &lt;a href=&#34;https://github.com/bytedance/Shot2Story/raw/master/DATA.md&#34;&gt;Shot2Story&lt;/a&gt;, &lt;a href=&#34;http://youcook2.eecs.umich.edu/&#34;&gt;Youcook2&lt;/a&gt;, &lt;a href=&#34;https://eric-xw.github.io/vatex-website/download.html&#34;&gt;Vatex&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/datasets/ShareGPTVideo/train_video_and_instruction&#34;&gt;ShareGPT-Video&lt;/a&gt; for providing datasets used in this research.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>