<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-18T01:31:30Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>webdevcody/code-racer</title>
    <updated>2023-07-18T01:31:30Z</updated>
    <id>tag:github.com,2023-07-18:/webdevcody/code-racer</id>
    <link href="https://github.com/webdevcody/code-racer" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/webdevcody/code-racer/main/public/static/logo.png&#34; width=&#34;60&#34;&gt; &#xA; &lt;br&gt; &#xA; &lt;h1&gt;Code Racer&lt;/h1&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  Welcome to Code Racer, a community project built with &#xA; &lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt;, &#xA; &lt;a href=&#34;https://tailwindcss.com&#34;&gt;Tailwind CSS&lt;/a&gt;, and TypeScript. Code Racer is a multiplayer coding game where developers can compete against each other to solve programming challenges in real-time. Sharpen your coding skills, challenge your peers, and have fun while racing against the clock! &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Code snippet games&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Technologies Used&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nextjs.org/&#34;&gt;Next.js&lt;/a&gt; : A React framework for building server-side rendered and statically generated applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://next-auth.js.org/&#34;&gt;NextAuth&lt;/a&gt; : For user authentication.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.prisma.io/&#34;&gt;Prisma&lt;/a&gt; : next-generation ORM, it provide a clean and type-safe API for submitting database queries&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tailwindcss.com&#34;&gt;Tailwind CSS&lt;/a&gt; : A utility-first CSS framework for rapid UI development.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.typescriptlang.org/&#34;&gt;TypeScript&lt;/a&gt;: A typed superset of JavaScript that provides enhanced tooling and developer productivity.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions from the community! If you&#39;d like to contribute to Code Racer, please follow refer to &lt;a href=&#34;https://raw.githubusercontent.com/webdevcody/code-racer/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;, but we have these base guidelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork the repository.&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch for your feature or bug fix.&lt;/li&gt; &#xA; &lt;li&gt;Make your changes and test thoroughly.&lt;/li&gt; &#xA; &lt;li&gt;Commit your changes with clear commit messages.&lt;/li&gt; &#xA; &lt;li&gt;Push your branch to your forked repository. Submit a pull request detailing your changes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please ensure that your code adheres to the project&#39;s coding standards and conventions.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The Code Racer project is licensed under the MIT License. Feel free to use, modify, and distribute the code as per the terms of the license. Acknowledgements&lt;/p&gt; &#xA;&lt;p&gt;Code Racer wouldn&#39;t be possible without the valuable contributions and support from the open-source community. We would like to express our gratitude to all the contributors and acknowledge the following libraries and resources used in this project.&lt;/p&gt; &#xA;&lt;p&gt;A big thank you to all the developers who have helped shape Code Racer into what it is today!&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, suggestions, or feedback regarding Code Racer, please feel free to reach out to us at in the WebDevCody &lt;a href=&#34;https://discord.gg/4kGbBaa&#34;&gt;discord&lt;/a&gt; server&lt;/p&gt; &#xA;&lt;p&gt;Happy coding and enjoy the race!&lt;/p&gt; &#xA;&lt;h2&gt;Related Youtube Videos &amp;amp; Progress&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=-n6tV3RPjGc&#34;&gt;Community Project Announcement Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BQXXBsHXfak&#34;&gt;First Q&amp;amp;A Livestream and Community Project Live Coding Session&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;h2&gt;How To(s) - A newbie section&lt;/h2&gt; &lt;/summary&gt; &#xA; &lt;h3&gt;Run the applications locally&lt;/h3&gt; &#xA; &lt;p&gt;Pre-requisites - &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt; and &lt;a href=&#34;https://nodejs.org/&#34;&gt;Node&lt;/a&gt; installed.&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Create a copy of &lt;code&gt;.env.example&lt;/code&gt; with name &lt;code&gt;.env&lt;/code&gt; \ &lt;code&gt;.env.local&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Get the Postgres running - &lt;code&gt;docker compose up -d&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Run the dev env - &lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA; &lt;p&gt;Don&#39;t forget to turn down the postgres post devlopment - &lt;code&gt;docker compose down&lt;/code&gt;&lt;/p&gt; &#xA; &lt;h3&gt;Check the logs of Post&lt;/h3&gt; &#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;Run the command - &lt;code&gt;docker logs --follow code-racer-postgres&lt;/code&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>guoyww/AnimateDiff</title>
    <updated>2023-07-18T01:31:30Z</updated>
    <id>tag:github.com,2023-07-18:/guoyww/AnimateDiff</id>
    <link href="https://github.com/guoyww/AnimateDiff" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official implementation of AnimateDiff.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AnimateDiff&lt;/h1&gt; &#xA;&lt;p&gt;This repository is the official implementation of &lt;a href=&#34;https://arxiv.org/abs/2307.04725&#34;&gt;AnimateDiff&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.04725&#34;&gt;AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning&lt;/a&gt;&lt;/strong&gt; &lt;br&gt; Yuwei Guo, Ceyuan Yang*, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, Bo Dai&lt;/p&gt; &#xA;&lt;p style=&#34;font-size: 0.8em; margin-top: -1em&#34;&gt;*Corresponding Author&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.04725&#34;&gt;Arxiv Report&lt;/a&gt; | &lt;a href=&#34;https://animatediff.github.io/&#34;&gt;Project Page&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Code Release&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Arxiv Report&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; GPU Memory Optimization&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Gradio Interface&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Common Issues&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Installation&lt;/summary&gt; Please ensure the installation of [xformer](https://github.com/facebookresearch/xformers) that is applied to reduce the inference memory. &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Various resolution or number of frames&lt;/summary&gt; Currently, we recommend users to generate animation with 16 frames and 512 resolution that are aligned with our training settings. Notably, various resolution/frames may affect the quality more or less. &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Animating a given image&lt;/summary&gt; We totally agree that animating a given image is an appealing feature, which we would try to support officially in future. For now, you may enjoy other efforts from the [talesofai](https://github.com/talesofai/AnimateDiff). &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Contributions from community&lt;/summary&gt; Contributions are always welcome!! We will create another branch which community could contribute to. As for the main branch, we would like to align it with the original technical report:) &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Setup for Inference&lt;/h2&gt; &#xA;&lt;h3&gt;Prepare Environment&lt;/h3&gt; &#xA;&lt;p&gt;&lt;del&gt;Our approach takes around 60 GB GPU memory to inference. NVIDIA A100 is recommanded.&lt;/del&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;We updated our inference code with xformers and a sequential decoding trick. Now AnimateDiff takes only ~12GB VRAM to inference, and run on a single RTX3090 !!&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/guoyww/AnimateDiff.git&#xA;cd AnimateDiff&#xA;&#xA;conda env create -f environment.yaml&#xA;conda activate animatediff&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Base T2I &amp;amp; Motion Module Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;We provide two versions of our Motion Module, which are trained on stable-diffusion-v1-4 and finetuned on v1-5 seperately. It&#39;s recommanded to try both of them for best results.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git lfs install&#xA;git clone https://huggingface.co/runwayml/stable-diffusion-v1-5 models/StableDiffusion/&#xA;&#xA;bash download_bashscripts/0-MotionModule.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may also directly download the motion module checkpoints from &lt;a href=&#34;https://drive.google.com/drive/folders/1EqLC65eR1-W-sGD0Im7fkED6c8GkiNFI?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;, then put them in &lt;code&gt;models/Motion_Module/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h3&gt;Prepare Personalize T2I&lt;/h3&gt; &#xA;&lt;p&gt;Here we provide inference configs for 6 demo T2I on CivitAI. You may run the following bash scripts to download these checkpoints.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash download_bashscripts/1-ToonYou.sh&#xA;bash download_bashscripts/2-Lyriel.sh&#xA;bash download_bashscripts/3-RcnzCartoon.sh&#xA;bash download_bashscripts/4-MajicMix.sh&#xA;bash download_bashscripts/5-RealisticVision.sh&#xA;bash download_bashscripts/6-Tusun.sh&#xA;bash download_bashscripts/7-FilmVelvia.sh&#xA;bash download_bashscripts/8-GhibliBackground.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;After downloading the above peronalized T2I checkpoints, run the following commands to generate animations. The results will automatically be saved to &lt;code&gt;samples/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m scripts.animate --config configs/prompts/1-ToonYou.yaml&#xA;python -m scripts.animate --config configs/prompts/2-Lyriel.yaml&#xA;python -m scripts.animate --config configs/prompts/3-RcnzCartoon.yaml&#xA;python -m scripts.animate --config configs/prompts/4-MajicMix.yaml&#xA;python -m scripts.animate --config configs/prompts/5-RealisticVision.yaml&#xA;python -m scripts.animate --config configs/prompts/6-Tusun.yaml&#xA;python -m scripts.animate --config configs/prompts/7-FilmVelvia.yaml&#xA;python -m scripts.animate --config configs/prompts/8-GhibliBackground.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To generate animations with a new DreamBooth/LoRA model, you may create a new config &lt;code&gt;.yaml&lt;/code&gt; file in the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;NewModel:&#xA;  path: &#34;[path to your DreamBooth/LoRA model .safetensors file]&#34;&#xA;  base: &#34;[path to LoRA base model .safetensors file, leave it empty string if not needed]&#34;&#xA;&#xA;  motion_module:&#xA;    - &#34;models/Motion_Module/mm_sd_v14.ckpt&#34;&#xA;    - &#34;models/Motion_Module/mm_sd_v15.ckpt&#34;&#xA;    &#xA;  steps:          25&#xA;  guidance_scale: 7.5&#xA;&#xA;  prompt:&#xA;    - &#34;[positive prompt]&#34;&#xA;&#xA;  n_prompt:&#xA;    - &#34;[negative prompt]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the following commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m scripts.animate --config [path to the config file]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Gallery&lt;/h2&gt; &#xA;&lt;p&gt;Here we demonstrate several best results we found in our experiments.&lt;/p&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_01/01.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_01/02.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_01/03.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_01/04.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p style=&#34;margin-left: 2em; margin-top: -1em&#34;&gt;ModelÔºö&lt;a href=&#34;https://civitai.com/models/30240/toonyou&#34;&gt;ToonYou&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_02/01.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_02/02.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_02/03.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_02/04.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p style=&#34;margin-left: 2em; margin-top: -1em&#34;&gt;ModelÔºö&lt;a href=&#34;https://civitai.com/models/4468/counterfeit-v30&#34;&gt;Counterfeit V3.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_03/01.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_03/02.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_03/03.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_03/04.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p style=&#34;margin-left: 2em; margin-top: -1em&#34;&gt;ModelÔºö&lt;a href=&#34;https://civitai.com/models/4201/realistic-vision-v20&#34;&gt;Realistic Vision V2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_04/01.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_04/02.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_04/03.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_04/04.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p style=&#34;margin-left: 2em; margin-top: -1em&#34;&gt;ModelÔºö &lt;a href=&#34;https://civitai.com/models/43331/majicmix-realistic&#34;&gt;majicMIX Realistic&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_05/01.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_05/02.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_05/03.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_05/04.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p style=&#34;margin-left: 2em; margin-top: -1em&#34;&gt;ModelÔºö&lt;a href=&#34;https://civitai.com/models/66347/rcnz-cartoon-3d&#34;&gt;RCNZ Cartoon&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_06/01.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_06/02.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_06/03.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_06/04.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p style=&#34;margin-left: 2em; margin-top: -1em&#34;&gt;ModelÔºö&lt;a href=&#34;https://civitai.com/models/33208/filmgirl-film-grain-lora-and-loha&#34;&gt;FilmVelvia&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Community Cases&lt;/h4&gt; &#xA;&lt;p&gt;Here are some samples contributed by the community artists. Create a Pull Request if you would like to show your results hereüòö.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_07/init.jpg&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_07/01.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_07/02.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_07/03.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_07/04.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p style=&#34;margin-left: 2em; margin-top: -1em&#34;&gt; Character ModelÔºö&lt;a href=&#34;https://civitai.com/models/13237/genshen-impact-yoimiya&#34;&gt;Yoimiya&lt;/a&gt; (with an initial reference image, see &lt;a href=&#34;https://github.com/talesofai/AnimateDiff&#34;&gt;WIP fork&lt;/a&gt; for the extended implementation.) &lt;/p&gt;&#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_08/01.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_08/02.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_08/03.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/guoyww/AnimateDiff/main/__assets__/animations/model_08/04.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p style=&#34;margin-left: 2em; margin-top: -1em&#34;&gt; Character ModelÔºö&lt;a href=&#34;https://civitai.com/models/9850/paimon-genshin-impact&#34;&gt;Paimon&lt;/a&gt;; Pose ModelÔºö&lt;a href=&#34;https://civitai.com/models/107295/or-holdingsign&#34;&gt;Hold Sign&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{guo2023animatediff,&#xA;  title={AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning},&#xA;  author={Guo, Yuwei and Yang, Ceyuan and Rao, Anyi and Wang, Yaohui and Qiao, Yu and Lin, Dahua and Dai, Bo},&#xA;  journal={arXiv preprint arXiv:2307.04725},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Yuwei Guo&lt;/strong&gt;: &lt;a href=&#34;mailto:guoyuwei@pjlab.org.cn&#34;&gt;guoyuwei@pjlab.org.cn&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Ceyuan Yang&lt;/strong&gt;: &lt;a href=&#34;mailto:yangceyuan@pjlab.org.cn&#34;&gt;yangceyuan@pjlab.org.cn&lt;/a&gt;&lt;br&gt; &lt;strong&gt;Bo Dai&lt;/strong&gt;: &lt;a href=&#34;mailto:daibo@pjlab.org.cn&#34;&gt;daibo@pjlab.org.cn&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Codebase built upon &lt;a href=&#34;https://github.com/showlab/Tune-A-Video&#34;&gt;Tune-a-Video&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bazingagin/npc_gzip</title>
    <updated>2023-07-18T01:31:30Z</updated>
    <id>tag:github.com,2023-07-18:/bazingagin/npc_gzip</id>
    <link href="https://github.com/bazingagin/npc_gzip" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h3&gt;Code for Paper: ‚ÄúLow-Resource‚Äù Text Classification: A Parameter-Free Classification Method with Compressors&lt;/h3&gt; &#xA;&lt;p&gt;This paper is accepted to Findings of &lt;a href=&#34;https://aclanthology.org/2023.findings-acl.426/&#34;&gt;ACL2023&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Require&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch&#xA;torchdata&#xA;torchtext==0.11 (for dataset)&#xA;numpy&#xA;pathos (if need multiprocessing)&#xA;scikit-learn&#xA;tqdm&#xA;unidecode&#xA;datasets&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main_text.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, this will only use 100 test and training samples per class as a quick demo. They can be changed by &lt;code&gt;--num_test&lt;/code&gt;, &lt;code&gt;--num_train&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--compressor &amp;lt;gzip, lzma, bz2&amp;gt;&#xA;--dataset &amp;lt;AG_NEWS, SogouNews, DBpedia, YahooAnswers, 20News, Ohsumed_single, R8, R52, kinnews, kirnews, swahili, filipino&amp;gt; [Note that for small datasets like kinnews, default 100-shot is too big, need to set --num_test and --num_train.]&#xA;--num_train &amp;lt;INT&amp;gt;&#xA;--num_test &amp;lt;INT&amp;gt;&#xA;--data_dir &amp;lt;DIR&amp;gt; [This needs to be specified for R8, R52 and Ohsumed.]&#xA;--all_test [This will use the whole test dataset.]&#xA;--all_train&#xA;--record [This will record the distance matrix in order to save for the future use. It&#39;s helpful when you when to run on the whole dataset.]&#xA;--test_idx_start &amp;lt;INT&amp;gt;&#xA;--test_idx_end &amp;lt;INT&amp;gt; [These two args help us to run on a certain range of test set. Also helpful for calculating the distance matrix on the whole dataset.]&#xA;--para [This will use multiprocessing to accelerate.]&#xA;--output_dir &amp;lt;DIR&amp;gt; [The output directory to save information of tested indicies or distance matrix.]&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Calculate Accuracy (Optional)&lt;/h3&gt; &#xA;&lt;p&gt;If we want to calculate accuracy from recorded distance file &#xA; &lt;distance dir&gt;&#xA;  , use&#xA; &lt;/distance&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python main_text.py --record --score --distance_fn &amp;lt;DISTANCE DIR&amp;gt; &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to calculate accuracy. Otherwise, the accuracy will be calculated automatically using the command in the last section.&lt;/p&gt;</summary>
  </entry>
</feed>