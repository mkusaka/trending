<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-10-20T01:28:39Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/openvmm</title>
    <updated>2024-10-20T01:28:39Z</updated>
    <id>tag:github.com,2024-10-20:/microsoft/openvmm</id>
    <link href="https://github.com/microsoft/openvmm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Home of OpenVMM and OpenHCL.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OpenVMM&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/openvmm/actions/workflows/openvmm-ci.yaml&#34;&gt;&lt;img src=&#34;https://github.com/microsoft/openvmm/actions/workflows/openvmm-ci.yaml/badge.svg?branch=main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenVMM is a modular, cross-platform Virtual Machine Monitor (VMM), written in Rust.&lt;/p&gt; &#xA;&lt;p&gt;Although it can function as a traditional VMM, OpenVMM&#39;s development is currently focused on its role in the OpenHCL paravisor.&lt;/p&gt; &#xA;&lt;p&gt;This repo is the home for both projects.&lt;/p&gt; &#xA;&lt;p&gt;For more information, read our &lt;a href=&#34;https://openvmm.dev/&#34;&gt;guide&lt;/a&gt; or our &lt;a href=&#34;https://techcommunity.microsoft.com/t5/windows-os-platform-blog/openhcl-the-new-open-source-paravisor/ba-p/4273172&#34;&gt;introductory blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;For info on how to run, build, and use OpenVMM, check out the &lt;a href=&#34;https://aka.ms/openvmmguide&#34;&gt;The OpenVMM Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The guide is published out of this repo via &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/openvmm/main/Guide/src/SUMMARY.md&#34;&gt;Markdown files&lt;/a&gt;. Please keep them up-to-date.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OHIF/Viewers</title>
    <updated>2024-10-20T01:28:39Z</updated>
    <id>tag:github.com,2024-10-20:/OHIF/Viewers</id>
    <link href="https://github.com/OHIF/Viewers" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OHIF zero-footprint DICOM viewer and oncology specific Lesion Tracker, plus shared extension packages&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;OHIF Medical Imaging Viewer&lt;/h1&gt; &#xA; &lt;p&gt;&lt;strong&gt;The OHIF Viewer&lt;/strong&gt; is a zero-footprint medical image viewer provided by the &lt;a href=&#34;https://ohif.org/&#34;&gt;Open Health Imaging Foundation (OHIF)&lt;/a&gt;. It is a configurable and extensible progressive web application with out-of-the-box support for image archives which support &lt;a href=&#34;https://www.dicomstandard.org/using/dicomweb/&#34;&gt;DICOMweb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://docs.ohif.org/&#34;&gt;&lt;strong&gt;Read The Docs&lt;/strong&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://viewer.ohif.org/&#34;&gt;Live Demo&lt;/a&gt; | &#xA; &lt;a href=&#34;https://ui.ohif.org/&#34;&gt;Component Library&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  üì∞ &#xA; &lt;a href=&#34;https://ohif.org/news/&#34;&gt;&lt;strong&gt;Join OHIF Newsletter&lt;/strong&gt;&lt;/a&gt; üì∞ &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  üì∞ &#xA; &lt;a href=&#34;https://ohif.org/news/&#34;&gt;&lt;strong&gt;Join OHIF Newsletter&lt;/strong&gt;&lt;/a&gt; üì∞ &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://npmjs.org/package/@ohif/app&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@ohif/app.svg?style=flat-square&#34; alt=&#34;NPM version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OHIF/Viewers/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square&#34; alt=&#34;MIT License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/OHIF/Viewers/master/percy-url&#34;&gt;&lt;img src=&#34;https://percy.io/static/images/percy-badge.svg?sanitize=true&#34; alt=&#34;This project is using Percy.io for visual regression testing.&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- [![NPM downloads][npm-downloads-image]][npm-url] --&gt; &#xA;&lt;!-- [![Pulls][docker-pulls-img]][docker-image-url] --&gt; &#xA;&lt;!-- [![FOSSA Status](https://app.fossa.io/api/projects/git%2Bgithub.com%2FOHIF%2FViewers.svg?type=shield)](https://app.fossa.io/projects/git%2Bgithub.com%2FOHIF%2FViewers?ref=badge_shield) --&gt; &#xA;&lt;!-- [![Netlify Status][netlify-image]][netlify-url] --&gt; &#xA;&lt;!-- [![CircleCI][circleci-image]][circleci-url] --&gt; &#xA;&lt;!-- [![codecov][codecov-image]][codecov-url] --&gt; &#xA;&lt;!-- [![All Contributors](https://img.shields.io/badge/all_contributors-10-orange.svg?style=flat-square)](#contributors) --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/OHIF/Viewers/raw/master/platform/docs/docs/assets/img/demo-measurements.webp?raw=true&#34; alt=&#34;Measurement tracking&#34; width=&#34;350&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Measurement Tracking&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://viewer.ohif.org/viewer?StudyInstanceUIDs=1.3.6.1.4.1.25403.345050719074.3824.20170125095438.5&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/OHIF/Viewers/raw/master/platform/docs/docs/assets/img/demo-segmentation.webp?raw=true&#34; alt=&#34;Segmentations&#34; width=&#34;350&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Labelmap Segmentations&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://viewer.ohif.org/viewer?StudyInstanceUIDs=1.3.12.2.1107.5.2.32.35162.30000015050317233592200000046&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/OHIF/Viewers/raw/master/platform/docs/docs/assets/img/demo-ptct.webp?raw=true&#34; alt=&#34;Hanging Protocols&#34; width=&#34;350&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Fusion and Custom Hanging protocols&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://viewer.ohif.org/tmtv?StudyInstanceUIDs=1.3.6.1.4.1.14519.5.2.1.7009.2403.334240657131972136850343327463&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/OHIF/Viewers/raw/master/platform/docs/docs/assets/img/demo-volume-rendering.webp?raw=true&#34; alt=&#34;Volume Rendering&#34; width=&#34;350&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Volume Rendering&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://viewer.ohif.org/viewer?StudyInstanceUIDs=1.3.6.1.4.1.25403.345050719074.3824.20170125095438.5&amp;amp;hangingprotocolId=mprAnd3DVolumeViewport&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/OHIF/Viewers/raw/master/platform/docs/docs/assets/img/demo-pdf.webp?raw=true&#34; alt=&#34;PDF&#34; width=&#34;350&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;PDF&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://viewer.ohif.org/viewer?StudyInstanceUIDs=2.25.317377619501274872606137091638706705333&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/OHIF/Viewers/raw/master/platform/docs/docs/assets/img/demo-rtstruct.webp?raw=true&#34; alt=&#34;RTSTRUCT&#34; width=&#34;350&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;RT STRUCT&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://viewer.ohif.org/viewer?StudyInstanceUIDs=1.3.6.1.4.1.5962.99.1.2968617883.1314880426.1493322302363.3.0&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/OHIF/Viewers/raw/master/platform/docs/docs/assets/img/demo-4d.webp?raw=true&#34; alt=&#34;4D&#34; width=&#34;350&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;4D&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://viewer.ohif.org/dynamic-volume?StudyInstanceUIDs=2.25.232704420736447710317909004159492840763&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/OHIF/Viewers/raw/master/platform/docs/docs/assets/img/demo-video.webp?raw=true&#34; alt=&#34;VIDEO&#34; width=&#34;350&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Video&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://viewer.ohif.org/viewer?StudyInstanceUIDs=2.25.96975534054447904995905761963464388233&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/OHIF/Viewers/raw/master/platform/docs/docs/assets/img/microscopy.webp?raw=true&#34; alt=&#34;microscopy&#34; width=&#34;350&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Slide Microscopy&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://viewer.ohif.org/microscopy?StudyInstanceUIDs=2.25.141277760791347900862109212450152067508&#34;&gt;Demo&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;The OHIF Viewer can retrieve and load images from most sources and formats; render sets in 2D, 3D, and reconstructed representations; allows for the manipulation, annotation, and serialization of observations; supports internationalization, OpenID Connect, offline use, hotkeys, and many more features.&lt;/p&gt; &#xA;&lt;p&gt;Almost everything offers some degree of customization and configuration. If it doesn&#39;t support something you need, we accept pull requests and have an ever improving Extension System.&lt;/p&gt; &#xA;&lt;h2&gt;Why Choose Us&lt;/h2&gt; &#xA;&lt;h3&gt;Community &amp;amp; Experience&lt;/h3&gt; &#xA;&lt;p&gt;The OHIF Viewer is a collaborative effort that has served as the basis for many active, production, and FDA Cleared medical imaging viewers. It benefits from our extensive community&#39;s collective experience, and from the sponsored contributions of individuals, research groups, and commercial organizations.&lt;/p&gt; &#xA;&lt;h3&gt;Built to Adapt&lt;/h3&gt; &#xA;&lt;p&gt;After more than 8-years of integrating with many companies and organizations, The OHIF Viewer has been rebuilt from the ground up to better address the varying workflow and configuration needs of its many users. All of the Viewer&#39;s core features are built using it&#39;s own extension system. The same extensibility that allows us to offer:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2D and 3D medical image viewing&lt;/li&gt; &#xA; &lt;li&gt;Multiplanar Reconstruction (MPR)&lt;/li&gt; &#xA; &lt;li&gt;Maximum Intensity Project (MIP)&lt;/li&gt; &#xA; &lt;li&gt;Whole slide microscopy viewing&lt;/li&gt; &#xA; &lt;li&gt;PDF and Dicom Structured Report rendering&lt;/li&gt; &#xA; &lt;li&gt;Segmentation rendering as labelmaps and contours&lt;/li&gt; &#xA; &lt;li&gt;User Access Control (UAC)&lt;/li&gt; &#xA; &lt;li&gt;Context specific toolbar and side panel content&lt;/li&gt; &#xA; &lt;li&gt;and many others&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Can be leveraged by you to customize the viewer for your workflow, and to add any new functionality you may need (and wish to maintain privately without forking).&lt;/p&gt; &#xA;&lt;h3&gt;Support&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OHIF/Viewers/issues/new?assignees=&amp;amp;labels=Community%3A+Report+%3Abug%3A%2CAwaiting+Reproduction&amp;amp;projects=&amp;amp;template=bug-report.yml&amp;amp;title=%5BBug%5D+&#34;&gt;Report a Bug üêõ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/OHIF/Viewers/issues/new?assignees=&amp;amp;labels=Community%3A+Request+%3Ahand%3A&amp;amp;projects=&amp;amp;template=feature-request.yml&amp;amp;title=%5BFeature+Request%5D+&#34;&gt;Request a Feature üöÄ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/OHIF/Viewers/master/community.ohif.org&#34;&gt;Ask a Question ü§ó&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://join.slack.com/t/cornerstonejs/shared_invite/zt-1r8xb2zau-dOxlD6jit3TN0Uwf928w9Q&#34;&gt;Slack Channel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For commercial support, academic collaborations, and answers to common questions; please use &lt;a href=&#34;https://ohif.org/get-support/&#34;&gt;Get Support&lt;/a&gt; to contact us.&lt;/p&gt; &#xA;&lt;h2&gt;Developing&lt;/h2&gt; &#xA;&lt;h3&gt;Branches&lt;/h3&gt; &#xA;&lt;h4&gt;&lt;code&gt;master&lt;/code&gt; branch - The latest dev (beta) release&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;master&lt;/code&gt; - The latest dev release&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is typically where the latest development happens. Code that is in the master branch has passed code reviews and automated tests, but it may not be deemed ready for production. This branch usually contains the most recent changes and features being worked on by the development team. It&#39;s often the starting point for creating feature branches (where new features are developed) and hotfix branches (for urgent fixes).&lt;/p&gt; &#xA;&lt;p&gt;Each package is tagged with beta version numbers, and published to npm such as &lt;code&gt;@ohif/ui@3.6.0-beta.1&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;release/*&lt;/code&gt; branches - The latest stable releases&lt;/h3&gt; &#xA;&lt;p&gt;Once the &lt;code&gt;master&lt;/code&gt; branch code reaches a stable, release-ready state, we conduct a comprehensive code review and QA testing. Upon approval, we create a new release branch from &lt;code&gt;master&lt;/code&gt;. These branches represent the latest stable version considered ready for production.&lt;/p&gt; &#xA;&lt;p&gt;For example, &lt;code&gt;release/3.5&lt;/code&gt; is the branch for version 3.5.0, and &lt;code&gt;release/3.6&lt;/code&gt; is for version 3.6.0. After each release, we wait a few days to ensure no critical bugs. If any are found, we fix them in the release branch and create a new release with a minor version bump, e.g., 3.5.1 in the &lt;code&gt;release/3.5&lt;/code&gt; branch.&lt;/p&gt; &#xA;&lt;p&gt;Each package is tagged with version numbers and published to npm, such as &lt;code&gt;@ohif/ui@3.5.0&lt;/code&gt;. Note that &lt;code&gt;master&lt;/code&gt; is always ahead of the &lt;code&gt;release&lt;/code&gt; branch. We publish docker builds for both beta and stable releases.&lt;/p&gt; &#xA;&lt;p&gt;Here is a schematic representation of our development workflow:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/OHIF/Viewers/master/platform/docs/docs/assets/img/github-readme-branches-Jun2024.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://yarnpkg.com/en/docs/install&#34;&gt;Yarn 1.17.3+&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nodejs.org/en/&#34;&gt;Node 18+&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Yarn Workspaces should be enabled on your machine: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;yarn config set workspaces-experimental true&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/articles/fork-a-repo&#34;&gt;Fork this repository&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://help.github.com/en/articles/fork-a-repo#step-2-create-a-local-clone-of-your-fork&#34;&gt;Clone your forked repository&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;git clone https://github.com/YOUR-USERNAME/Viewers.git&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Navigate to the cloned project&#39;s directory&lt;/li&gt; &#xA; &lt;li&gt;Add this repo as a &lt;code&gt;remote&lt;/code&gt; named &lt;code&gt;upstream&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;git remote add upstream https://github.com/OHIF/Viewers.git&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;yarn install&lt;/code&gt; to restore dependencies and link projects&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;To Develop&lt;/h4&gt; &#xA;&lt;p&gt;&lt;em&gt;From this repository&#39;s root directory:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Enable Yarn Workspaces&#xA;yarn config set workspaces-experimental true&#xA;&#xA;# Restore dependencies&#xA;yarn install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Commands&lt;/h2&gt; &#xA;&lt;p&gt;These commands are available from the root directory. Each project directory also supports a number of commands that can be found in their respective &lt;code&gt;README.md&lt;/code&gt; and &lt;code&gt;package.json&lt;/code&gt; files.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Yarn Commands&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Develop&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;dev&lt;/code&gt; or &lt;code&gt;start&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Default development experience for Viewer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;test:unit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Jest multi-project test runner; overall coverage&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;Deploy&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;build&lt;/code&gt;*&lt;/td&gt; &#xA;   &lt;td&gt;Builds production output for our PWA Viewer&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;* - For more information on our different builds, check out our &lt;a href=&#34;https://docs.ohif.org/deployment/&#34;&gt;Deploy Docs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Project&lt;/h2&gt; &#xA;&lt;p&gt;The OHIF Medical Image Viewing Platform is maintained as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Monorepo&#34;&gt;&lt;code&gt;monorepo&lt;/code&gt;&lt;/a&gt;. This means that this repository, instead of containing a single project, contains many projects. If you explore our project structure, you&#39;ll see the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.&#xA;‚îú‚îÄ‚îÄ extensions               #&#xA;‚îÇ   ‚îú‚îÄ‚îÄ _example             # Skeleton of example extension&#xA;‚îÇ   ‚îú‚îÄ‚îÄ default              # basic set of useful functionalities (datasources, panels, etc)&#xA;‚îÇ   ‚îú‚îÄ‚îÄ cornerstone       # image rendering and tools w/ Cornerstone3D&#xA;‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-dicom-sr # DICOM Structured Report rendering and export&#xA;‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-dicom-sr # DICOM Structured Report rendering and export&#xA;‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-dicom-seg # DICOM Segmentation rendering and export&#xA;‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-dicom-rt # DICOM RTSTRUCT rendering&#xA;‚îÇ   ‚îú‚îÄ‚îÄ cornerstone-microscopy # Whole Slide Microscopy rendering&#xA;‚îÇ   ‚îú‚îÄ‚îÄ dicom-pdf # PDF rendering&#xA;‚îÇ   ‚îú‚îÄ‚îÄ dicom-video # DICOM RESTful Services&#xA;‚îÇ   ‚îú‚îÄ‚îÄ measurement-tracking # Longitudinal measurement tracking&#xA;‚îÇ   ‚îú‚îÄ‚îÄ tmtv # Total Metabolic Tumor Volume (TMTV) calculation&#xA;|&#xA;&#xA;‚îÇ&#xA;‚îú‚îÄ‚îÄ modes                    #&#xA;‚îÇ   ‚îú‚îÄ‚îÄ _example             # Skeleton of example mode&#xA;‚îÇ   ‚îú‚îÄ‚îÄ basic-dev-mode       # Basic development mode&#xA;‚îÇ   ‚îú‚îÄ‚îÄ longitudinal         # Longitudinal mode (measurement tracking)&#xA;‚îÇ   ‚îú‚îÄ‚îÄ tmtv       # Total Metabolic Tumor Volume (TMTV) calculation mode&#xA;‚îÇ   ‚îî‚îÄ‚îÄ microscopy          # Whole Slide Microscopy mode&#xA;‚îÇ&#xA;‚îú‚îÄ‚îÄ platform                 #&#xA;‚îÇ   ‚îú‚îÄ‚îÄ core                 # Business Logic&#xA;‚îÇ   ‚îú‚îÄ‚îÄ i18n                 # Internationalization Support&#xA;‚îÇ   ‚îú‚îÄ‚îÄ ui                   # React component library&#xA;‚îÇ   ‚îú‚îÄ‚îÄ docs                 # Documentation&#xA;‚îÇ   ‚îî‚îÄ‚îÄ viewer               # Connects platform and extension projects&#xA;‚îÇ&#xA;‚îú‚îÄ‚îÄ ...                      # misc. shared configuration&#xA;‚îú‚îÄ‚îÄ lerna.json               # MonoRepo (Lerna) settings&#xA;‚îú‚îÄ‚îÄ package.json             # Shared devDependencies and commands&#xA;‚îî‚îÄ‚îÄ README.md                # This file&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;To acknowledge the OHIF Viewer in an academic publication, please cite&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Open Health Imaging Foundation Viewer: An Extensible Open-Source Framework for Building Web-Based Imaging Applications to Support Cancer Research&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;Erik Ziegler, Trinity Urban, Danny Brown, James Petts, Steve D. Pieper, Rob Lewis, Chris Hafey, and Gordon J. Harris&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;JCO Clinical Cancer Informatics&lt;/em&gt;, no. 4 (2020), 336-345, DOI: &lt;a href=&#34;https://www.doi.org/10.1200/CCI.19.00131&#34;&gt;10.1200/CCI.19.00131&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;Open-Access on Pubmed Central: &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7259879/&#34;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7259879/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;or, for v1, please cite:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;LesionTracker: Extensible Open-Source Zero-Footprint Web Viewer for Cancer Imaging Research and Clinical Trials&lt;/em&gt;&lt;/p&gt; &#xA; &lt;p&gt;Trinity Urban, Erik Ziegler, Rob Lewis, Chris Hafey, Cheryl Sadow, Annick D. Van den Abbeele and Gordon J. Harris&lt;/p&gt; &#xA; &lt;p&gt;&lt;em&gt;Cancer Research&lt;/em&gt;, November 1 2017 (77) (21) e119-e122 DOI: &lt;a href=&#34;https://www.doi.org/10.1158/0008-5472.CAN-17-0334&#34;&gt;10.1158/0008-5472.CAN-17-0334&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you use or find this repository helpful, please take the time to star this repository on GitHub. This is an easy way for us to assess adoption and it can help us obtain future funding for the project.&lt;/p&gt; &#xA;&lt;p&gt;This work is supported primarily by the National Institutes of Health, National Cancer Institute, Informatics Technology for Cancer Research (ITCR) program, under a &lt;a href=&#34;https://projectreporter.nih.gov/project_info_description.cfm?aid=8971104&#34;&gt;grant to Dr. Gordon Harris at Massachusetts General Hospital (U24 CA199460)&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://imaging.datacommons.cancer.gov/&#34;&gt;NCI Imaging Data Commons (IDC) project&lt;/a&gt; supported the development of new features and bug fixes marked with &lt;a href=&#34;https://github.com/OHIF/Viewers/issues?q=is%3Aissue+is%3Aopen+label%3AIDC%3Apriority&#34;&gt;&#34;IDC:priority&#34;&lt;/a&gt;, &lt;a href=&#34;https://github.com/OHIF/Viewers/issues?q=is%3Aissue+is%3Aopen+label%3AIDC%3Acandidate&#34;&gt;&#34;IDC:candidate&#34;&lt;/a&gt; or &lt;a href=&#34;https://github.com/OHIF/Viewers/issues?q=is%3Aissue+is%3Aopen+label%3AIDC%3Acollaboration&#34;&gt;&#34;IDC:collaboration&#34;&lt;/a&gt;. NCI Imaging Data Commons is supported by contract number 19X037Q from Leidos Biomedical Research under Task Order HHSN26100071 from NCI. &lt;a href=&#34;https://learn.canceridc.dev/portal/visualization&#34;&gt;IDC Viewer&lt;/a&gt; is a customized version of the OHIF Viewer.&lt;/p&gt; &#xA;&lt;p&gt;This project is tested with BrowserStack. Thank you for supporting open-source!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT ¬© &lt;a href=&#34;https://github.com/OHIF&#34;&gt;OHIF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!--&#xA;  Links&#xA;  --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- Badges --&gt; &#xA;&lt;!-- ROW --&gt; &#xA;&lt;!-- Links --&gt; &#xA;&lt;!-- Platform --&gt; &#xA;&lt;!-- Extensions --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://app.fossa.com/projects/git%2Bgithub.com%2FOHIF%2FViewers?ref=badge_large&amp;amp;issueType=license&#34;&gt;&lt;img src=&#34;https://app.fossa.com/api/projects/git%2Bgithub.com%2FOHIF%2FViewers.svg?type=large&amp;amp;issueType=license&#34; alt=&#34;FOSSA Status&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>facebookresearch/co-tracker</title>
    <updated>2024-10-20T01:28:39Z</updated>
    <id>tag:github.com,2024-10-20:/facebookresearch/co-tracker</id>
    <link href="https://github.com/facebookresearch/co-tracker" rel="alternate"></link>
    <summary type="html">&lt;p&gt;CoTracker is a model for tracking any point (pixel) on a video.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://ai.facebook.com/research/&#34;&gt;Meta AI Research, GenAI&lt;/a&gt;&lt;/strong&gt;; &lt;strong&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/&#34;&gt;University of Oxford, VGG&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nikitakaraevv.github.io/&#34;&gt;Nikita Karaev&lt;/a&gt;, &lt;a href=&#34;https://linkedin.com/in/lvoursl&#34;&gt;Iurii Makarov&lt;/a&gt;, &lt;a href=&#34;https://jytime.github.io/&#34;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&#34;https://www.irocco.info/&#34;&gt;Ignacio Rocco&lt;/a&gt;, &lt;a href=&#34;https://ai.facebook.com/people/benjamin-graham/&#34;&gt;Benjamin Graham&lt;/a&gt;, &lt;a href=&#34;https://nneverova.github.io/&#34;&gt;Natalia Neverova&lt;/a&gt;, &lt;a href=&#34;https://www.robots.ox.ac.uk/~vedaldi/&#34;&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href=&#34;https://chrirupp.github.io/&#34;&gt;Christian Rupprecht&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://cotracker3.github.io/&#34;&gt;Project Page&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2307.07635&#34;&gt;Paper #1&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2410.11831&#34;&gt;Paper #2&lt;/a&gt; | &lt;a href=&#34;https://twitter.com/n_karaev/status/1742638906355470772&#34;&gt;X Thread&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#citing-cotracker&#34;&gt;BibTeX&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/main/notebooks/demo.ipynb&#34;&gt; &lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt; &lt;/a&gt; &#xA;&lt;a href=&#34;https://huggingface.co/spaces/facebook/cotracker&#34;&gt; &lt;img alt=&#34;Spaces&#34; src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34;&gt; &lt;/a&gt; &#xA;&lt;img width=&#34;1100&#34; src=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/assets/teaser.png&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;CoTracker&lt;/strong&gt; is a fast transformer-based model that can track any point in a video. It brings to tracking some of the benefits of Optical Flow.&lt;/p&gt; &#xA;&lt;p&gt;CoTracker can track:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Any pixel&lt;/strong&gt; in a video&lt;/li&gt; &#xA; &lt;li&gt;A &lt;strong&gt;quasi-dense&lt;/strong&gt; set of pixels together&lt;/li&gt; &#xA; &lt;li&gt;Points can be manually selected or sampled on a grid in any video frame&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Try these tracking modes for yourself with our &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb&#34;&gt;Colab demo&lt;/a&gt; or in the &lt;a href=&#34;https://huggingface.co/spaces/facebook/cotracker&#34;&gt;Hugging Face Space ü§ó&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Updates:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;[October 15, 2024] üì£ We&#39;re releasing CoTracker3! State-of-the-art point tracking with a lightweight architecture trained with 1000x less data than previous top-performing models. Code for baseline models and the pseudo-labeling pipeline are available in the repo, as well as model checkpoints. Check out our &lt;a href=&#34;https://arxiv.org/abs/2410.11831&#34;&gt;paper&lt;/a&gt; for more details.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[September 25, 2024] CoTracker2.1 is now available! This model has better performance on TAP-Vid benchmarks and follows the architecture of the original CoTracker. Try it out!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[June 14, 2024] We have released the code for &lt;a href=&#34;https://github.com/facebookresearch/vggsfm&#34;&gt;VGGSfM&lt;/a&gt;, a model for recovering camera poses and 3D structure from any image sequences based on point tracking! VGGSfM is the first fully differentiable SfM framework that unlocks scalability and outperforms conventional SfM methods on standard benchmarks.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[December 27, 2023] CoTracker2 is now available! It can now track many more (up to &lt;strong&gt;265*265&lt;/strong&gt;!) points jointly and it has a cleaner and more memory-efficient implementation. It also supports online processing. See the &lt;a href=&#34;https://arxiv.org/abs/2307.07635&#34;&gt;updated paper&lt;/a&gt; for more details. The old version remains available &lt;a href=&#34;https://github.com/facebookresearch/co-tracker/tree/8d364031971f6b3efec945dd15c468a183e58212&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;[September 5, 2023] You can now run our Gradio demo &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/gradio_demo/app.py&#34;&gt;locally&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick start&lt;/h2&gt; &#xA;&lt;p&gt;The easiest way to use CoTracker is to load a pretrained model from &lt;code&gt;torch.hub&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;h3&gt;Offline mode:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install imageio[ffmpeg]&lt;/code&gt;, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;# Download the video&#xA;url = &#39;https://github.com/facebookresearch/co-tracker/raw/refs/heads/main/assets/apple.mp4&#39;&#xA;&#xA;import imageio.v3 as iio&#xA;frames = iio.imread(url, plugin=&#34;FFMPEG&#34;)  # plugin=&#34;pyav&#34;&#xA;&#xA;device = &#39;cuda&#39;&#xA;grid_size = 10&#xA;video = torch.tensor(frames).permute(0, 3, 1, 2)[None].float().to(device)  # B T C H W&#xA;&#xA;# Run Offline CoTracker:&#xA;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker&#34;, &#34;cotracker3_offline&#34;).to(device)&#xA;pred_tracks, pred_visibility = cotracker(video, grid_size=grid_size) # B T N 2,  B T N 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Online mode:&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker&#34;, &#34;cotracker3_online&#34;).to(device)&#xA;&#xA;# Run Online CoTracker, the same model with a different API:&#xA;# Initialize online processing&#xA;cotracker(video_chunk=video, is_first_step=True, grid_size=grid_size)  &#xA;&#xA;# Process the video&#xA;for ind in range(0, video.shape[1] - cotracker.step, cotracker.step):&#xA;    pred_tracks, pred_visibility = cotracker(&#xA;        video_chunk=video[:, ind : ind + cotracker.step * 2]&#xA;    )  # B T N 2,  B T N 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Online processing is more memory-efficient and allows for the processing of longer videos. However, in the example provided above, the video length is known! See &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/online_demo.py&#34;&gt;the online demo&lt;/a&gt; for an example of tracking from an online stream with an unknown video length.&lt;/p&gt; &#xA;&lt;h3&gt;Visualize predicted tracks:&lt;/h3&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#installation-instructions&#34;&gt;installing&lt;/a&gt; CoTracker, you can visualize tracks with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from cotracker.utils.visualizer import Visualizer&#xA;&#xA;vis = Visualizer(save_dir=&#34;./saved_videos&#34;, pad_value=120, linewidth=3)&#xA;vis.visualize(video, pred_tracks, pred_visibility)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We offer a number of other ways to interact with CoTracker:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Interactive Gradio demo: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;A demo is available in the &lt;a href=&#34;https://huggingface.co/spaces/facebook/cotracker&#34;&gt;&lt;code&gt;facebook/cotracker&lt;/code&gt; Hugging Face Space ü§ó&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;You can use the gradio demo locally by running &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/gradio_demo/app.py&#34;&gt;&lt;code&gt;python -m gradio_demo.app&lt;/code&gt;&lt;/a&gt; after installing the required packages: &lt;code&gt;pip install -r gradio_demo/requirements.txt&lt;/code&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Jupyter notebook: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can run the notebook in &lt;a href=&#34;https://colab.research.google.com/github/facebookresearch/co-tracker/blob/master/notebooks/demo.ipynb&#34;&gt;Google Colab&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Or explore the notebook located at &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/notebooks/demo.ipynb&#34;&gt;&lt;code&gt;notebooks/demo.ipynb&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;You can &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#installation-instructions&#34;&gt;install&lt;/a&gt; CoTracker &lt;em&gt;locally&lt;/em&gt; and then: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Run an &lt;em&gt;offline&lt;/em&gt; demo with 10 ‚®â 10 points sampled on a grid on the first frame of a video (results will be saved to &lt;code&gt;./saved_videos/demo.mp4&lt;/code&gt;)):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python demo.py --grid_size 10&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Run an &lt;em&gt;online&lt;/em&gt; demo:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python online_demo.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;A GPU is strongly recommended for using CoTracker locally.&lt;/p&gt; &#xA;&lt;img width=&#34;500&#34; src=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/assets/bmx-bumps.gif&#34;&gt; &#xA;&lt;h2&gt;Installation Instructions&lt;/h2&gt; &#xA;&lt;p&gt;You can use a Pretrained Model via PyTorch Hub, as described above, or install CoTracker from this GitHub repo. This is the best way if you need to run our local demo or evaluate/train CoTracker.&lt;/p&gt; &#xA;&lt;p&gt;Ensure you have both &lt;em&gt;PyTorch&lt;/em&gt; and &lt;em&gt;TorchVision&lt;/em&gt; installed on your system. Follow the instructions &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for the installation. We strongly recommend installing both PyTorch and TorchVision with CUDA support, although for small tasks CoTracker can be run on CPU.&lt;/p&gt; &#xA;&lt;h3&gt;Install a Development Version&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/facebookresearch/co-tracker&#xA;cd co-tracker&#xA;pip install -e .&#xA;pip install matplotlib flow_vis tqdm tensorboard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can manually download all CoTracker3 checkpoints (baseline and scaled models, as well as single and sliding window architectures) from the links below and place them in the &lt;code&gt;checkpoints&lt;/code&gt; folder as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p checkpoints&#xA;cd checkpoints&#xA;# download the online (multi window) model&#xA;wget https://huggingface.co/facebook/cotracker3/resolve/main/scaled_online.pth&#xA;# download the offline (single window) model&#xA;wget https://huggingface.co/facebook/cotracker3/resolve/main/scaled_offline.pth&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also download CoTracker3 checkpoints trained only on Kubric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# download the online (sliding window) model&#xA;wget https://huggingface.co/facebook/cotracker3/resolve/main/baseline_online.pth&#xA;# download the offline (single window) model&#xA;wget https://huggingface.co/facebook/cotracker3/resolve/main/baseline_offline.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For old checkpoints, see &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/#previous-version&#34;&gt;this section&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation&lt;/h2&gt; &#xA;&lt;p&gt;To reproduce the results presented in the paper, download the following datasets:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;TAP-Vid&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dynamic-stereo.github.io/&#34;&gt;Dynamic Replica&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;And install the necessary dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install hydra-core==1.1.0 mediapy&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, execute the following command to evaluate the online model on TAP-Vid DAVIS:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./cotracker/evaluation/evaluate.py --config-name eval_tapvid_davis_first exp_dir=./eval_outputs dataset_root=your/tapvid/path&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And the offline model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./cotracker/evaluation/evaluate.py --config-name eval_tapvid_davis_first exp_dir=./eval_outputs dataset_root=/fsx-repligen/shared/datasets/tapvid offline_model=True window_len=60 checkpoint=./checkpoints/scaled_offline.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We run evaluations jointly on all the target points at a time for faster inference. With such evaluations, the numbers are similar to those presented in the paper. If you want to reproduce the exact numbers from the paper, add the flag &lt;code&gt;single_point=True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;These are the numbers that you should be able to reproduce using the released checkpoint and the current version of the codebase:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Kinetics, $\delta_\text{avg}^\text{vis}$&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;DAVIS, $\delta_\text{avg}^\text{vis}$&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RoboTAP, $\delta_\text{avg}^\text{vis}$&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;RGB-S, $\delta_\text{avg}^\text{vis}$&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CoTracker2, 27.12.23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;61.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;69.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CoTracker2.1, 25.09.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;70.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;79.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CoTracker3 offline, 15.10.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;67.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;76.9&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;85.0&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;CoTracker3 online, 15.10.24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;68.3&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;76.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;78.8&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;82.7&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;h3&gt;Baseline&lt;/h3&gt; &#xA;&lt;p&gt;To train the CoTracker as described in our paper, you first need to generate annotations for &lt;a href=&#34;https://github.com/google-research/kubric&#34;&gt;Google Kubric&lt;/a&gt; MOVI-f dataset. Instructions for annotation generation can be found &lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;here&lt;/a&gt;. You can also find a discussion on dataset generation in &lt;a href=&#34;https://github.com/facebookresearch/co-tracker/issues/8&#34;&gt;this issue&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you have the annotated dataset, you need to make sure you followed the steps for evaluation setup and install the training dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install pip==24.0&#xA;pip install pytorch_lightning==1.6.0 tensorboard opencv-python&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now you can launch training on Kubric. Our model was trained for 50000 iterations on 32 GPUs (4 nodes with 8 GPUs). Modify &lt;em&gt;dataset_root&lt;/em&gt; and &lt;em&gt;ckpt_path&lt;/em&gt; accordingly before running this command. For training on 4 nodes, add &lt;code&gt;--num_nodes 4&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here is an example of how to launch training of the online model on Kubric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; python train_on_kubric.py --batch_size 1 --num_steps 50000 \&#xA; --ckpt_path ./ --model_name cotracker_three --save_freq 200 --sequence_len 64 \&#xA;  --eval_datasets tapvid_davis_first tapvid_stacking --traj_per_sample 384 \&#xA;  --sliding_window_len 16 --train_datasets kubric --save_every_n_epoch 5 \&#xA;  --evaluate_every_n_epoch 5 --model_stride 4 --dataset_root ${path_to_your_dataset} \&#xA;   --num_nodes 4 --num_virtual_tracks 64 --mixed_precision --corr_radius 3 \ &#xA;   --wdecay 0.0005 --linear_layer_for_vis_conf --validate_at_start --add_huber_loss&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Training the offline model on Kubric:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_on_kubric.py --batch_size 1 --num_steps 50000 \&#xA; --ckpt_path ./ --model_name cotracker_three --save_freq 200 --sequence_len 60 \&#xA; --eval_datasets tapvid_davis_first tapvid_stacking --traj_per_sample 512 \&#xA; --sliding_window_len 60 --train_datasets kubric --save_every_n_epoch 5 \&#xA; --evaluate_every_n_epoch 5 --model_stride 4 --dataset_root ${path_to_your_dataset} \&#xA; --num_nodes 4 --num_virtual_tracks 64 --mixed_precision --offline_model \&#xA; --random_frame_rate --query_sampling_method random --corr_radius 3 \&#xA; --wdecay 0.0005 --random_seq_len --linear_layer_for_vis_conf \&#xA; --validate_at_start --add_huber_loss&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Fine-tuning with pseudo labels&lt;/h3&gt; &#xA;&lt;p&gt;In order to launch training with pseudo-labelling, you need to collect your own dataset of real videos. There is a sample class available in &lt;a href=&#34;https://raw.githubusercontent.com/facebookresearch/co-tracker/main/cotracker/datasets/real_dataset.py&#34;&gt;&lt;code&gt;cotracker/datasets/real_dataset.py&lt;/code&gt;&lt;/a&gt; with keyword-based filtering that we used for training. Your class should implement loading a video and storing it in the &lt;code&gt;CoTrackerData&lt;/code&gt; class as a field, while pseudo labels will be generated in &lt;code&gt;train_on_real_data.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You should have an existing Kubric-trained model for fine-tuning with pseudo labels. Here is an example of how you can launch fine-tuning of the online model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python ./train_on_real_data.py --batch_size 1 --num_steps 15000 \&#xA; --ckpt_path ./ --model_name cotracker_three --save_freq 200 --sequence_len 64 \&#xA; --eval_datasets tapvid_stacking tapvid_davis_first --traj_per_sample 384 \&#xA; --save_every_n_epoch 15 --evaluate_every_n_epoch 15 --model_stride 4 \&#xA; --dataset_root ${path_to_your_dataset} --num_nodes 4 --real_data_splits 0 \&#xA; --num_virtual_tracks 64 --mixed_precision --random_frame_rate \&#xA; --restore_ckpt ./checkpoints/baseline_online.pth \&#xA; --lr 0.00005 --real_data_filter_sift --validate_at_start \&#xA; --sliding_window_len 16 --limit_samples 15000&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And the offline model:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_on_real_data.py --batch_size 1 --num_steps 15000 \&#xA; --ckpt_path ./ --model_name cotracker_three --save_freq 200 --sequence_len 80 \&#xA; --eval_datasets tapvid_stacking tapvid_davis_first --traj_per_sample 384 --save_every_n_epoch 15 \&#xA; --evaluate_every_n_epoch 15 --model_stride 4 --dataset_root ${path_to_your_dataset} \&#xA;  --num_nodes 4 --real_data_splits 0 --num_virtual_tracks 64 --mixed_precision \&#xA;  --random_frame_rate --restore_ckpt ./checkpoints/baseline_offline.pth --lr 0.00005 \&#xA;  --real_data_filter_sift --validate_at_start --offline_model --limit_samples 15000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;h3&gt;Building the documentation&lt;/h3&gt; &#xA;&lt;p&gt;To build CoTracker documentation, first install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install sphinx&#xA;pip install sphinxcontrib-bibtex&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you can use this command to generate the documentation in the &lt;code&gt;docs/_build/html&lt;/code&gt; folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;make -C docs html&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Previous versions&lt;/h2&gt; &#xA;&lt;h3&gt;CoTracker v2&lt;/h3&gt; &#xA;&lt;p&gt;You could use CoTracker v2 with torch.hub in both offline and online modes.&lt;/p&gt; &#xA;&lt;h4&gt;Offline mode:&lt;/h4&gt; &#xA;&lt;p&gt;&lt;code&gt;pip install imageio[ffmpeg]&lt;/code&gt;, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;# Download the video&#xA;url = &#39;https://github.com/facebookresearch/co-tracker/blob/main/assets/apple.mp4&#39;&#xA;&#xA;import imageio.v3 as iio&#xA;frames = iio.imread(url, plugin=&#34;FFMPEG&#34;)  # plugin=&#34;pyav&#34;&#xA;&#xA;device = &#39;cuda&#39;&#xA;grid_size = 10&#xA;video = torch.tensor(frames).permute(0, 3, 1, 2)[None].float().to(device)  # B T C H W&#xA;&#xA;# Run Offline CoTracker:&#xA;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker&#34;, &#34;cotracker2&#34;).to(device)&#xA;pred_tracks, pred_visibility = cotracker(video, grid_size=grid_size) # B T N 2,  B T N 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Online mode:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker&#34;, &#34;cotracker2_online&#34;).to(device)&#xA;&#xA;# Run Online CoTracker, the same model with a different API:&#xA;# Initialize online processing&#xA;cotracker(video_chunk=video, is_first_step=True, grid_size=grid_size)  &#xA;&#xA;# Process the video&#xA;for ind in range(0, video.shape[1] - cotracker.step, cotracker.step):&#xA;    pred_tracks, pred_visibility = cotracker(&#xA;        video_chunk=video[:, ind : ind + cotracker.step * 2]&#xA;    )  # B T N 2,  B T N 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Checkpoint for v2 could be downloaded with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://huggingface.co/facebook/cotracker/resolve/main/cotracker2.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CoTracker v1&lt;/h3&gt; &#xA;&lt;p&gt;It is directly available via pytorch hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import einops&#xA;import timm&#xA;import tqdm&#xA;&#xA;cotracker = torch.hub.load(&#34;facebookresearch/co-tracker:v1.0&#34;, &#34;cotracker_w8&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The old version of the code is available &lt;a href=&#34;https://github.com/facebookresearch/co-tracker/tree/8d364031971f6b3efec945dd15c468a183e58212&#34;&gt;here&lt;/a&gt;. You can also download the corresponding checkpoints:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_4_wind_8.pth&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_4_wind_12.pth&#xA;wget https://dl.fbaipublicfiles.com/cotracker/cotracker_stride_8_wind_16.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The majority of CoTracker is licensed under CC-BY-NC, however portions of the project are available under separate license terms: Particle Video Revisited is licensed under the MIT license, TAP-Vid and LocoTrack are licensed under the Apache 2.0 license.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank &lt;a href=&#34;https://github.com/aharley/pips&#34;&gt;PIPs&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepmind/tapnet&#34;&gt;TAP-Vid&lt;/a&gt;, &lt;a href=&#34;https://github.com/cvlab-kaist/locotrack&#34;&gt;LocoTrack&lt;/a&gt; for publicly releasing their code and data. We also want to thank &lt;a href=&#34;https://lukemelas.github.io/&#34;&gt;Luke Melas-Kyriazi&lt;/a&gt; for proofreading the paper, &lt;a href=&#34;https://jytime.github.io/&#34;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&#34;https://shapovalov.ro/&#34;&gt;Roman Shapovalov&lt;/a&gt; and &lt;a href=&#34;https://adamharley.com/&#34;&gt;Adam W. Harley&lt;/a&gt; for the insightful discussions.&lt;/p&gt; &#xA;&lt;h2&gt;Citing CoTracker&lt;/h2&gt; &#xA;&lt;p&gt;If you find our repository useful, please consider giving it a star ‚≠ê and citing our research papers in your work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{karaev23cotracker,&#xA;  title     = {CoTracker: It is Better to Track Together},&#xA;  author    = {Nikita Karaev and Ignacio Rocco and Benjamin Graham and Natalia Neverova and Andrea Vedaldi and Christian Rupprecht},&#xA;  booktitle = {Proc. {ECCV}},&#xA;  year      = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{karaev24cotracker3,&#xA;  title     = {CoTracker3: Simpler and Better Point Tracking by Pseudo-Labelling Real Videos},&#xA;  author    = {Nikita Karaev and Iurii Makarov and Jianyuan Wang and Natalia Neverova and Andrea Vedaldi and Christian Rupprecht},&#xA;  booktitle = {Proc. {arXiv:2410.11831}},&#xA;  year      = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>