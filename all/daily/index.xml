<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-12T01:28:30Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>tailwindlabs/tailwindcss</title>
    <updated>2024-06-12T01:28:30Z</updated>
    <id>tag:github.com,2024-06-12:/tailwindlabs/tailwindcss</id>
    <link href="https://github.com/tailwindlabs/tailwindcss" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A utility-first CSS framework for rapid UI development.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://tailwindcss.com&#34; target=&#34;_blank&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://raw.githubusercontent.com/tailwindlabs/tailwindcss/HEAD/.github/logo-dark.svg&#34;&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://raw.githubusercontent.com/tailwindlabs/tailwindcss/HEAD/.github/logo-light.svg&#34;&gt; &#xA;   &lt;img alt=&#34;Tailwind CSS&#34; src=&#34;https://raw.githubusercontent.com/tailwindlabs/tailwindcss/HEAD/.github/logo-light.svg?sanitize=true&#34; width=&#34;350&#34; height=&#34;70&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; A utility-first CSS framework for rapidly building custom user interfaces. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/tailwindlabs/tailwindcss/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/tailwindlabs/tailwindcss/ci.yml?branch=next&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/tailwindcss&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/dt/tailwindcss.svg?sanitize=true&#34; alt=&#34;Total Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tailwindcss/tailwindcss/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/tailwindcss.svg?sanitize=true&#34; alt=&#34;Latest Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/tailwindcss/tailwindcss/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/l/tailwindcss.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;For full documentation, visit &lt;a href=&#34;https://tailwindcss.com&#34;&gt;tailwindcss.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;For help, discussion about best practices, or any other conversation that would benefit from being searchable:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tailwindcss/tailwindcss/discussions&#34;&gt;Discuss Tailwind CSS on GitHub&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For chatting with others using the framework:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/7NF8GNe&#34;&gt;Join the Tailwind CSS Discord Server&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you&#39;re interested in contributing to Tailwind CSS, please read our &lt;a href=&#34;https://github.com/tailwindcss/tailwindcss/raw/next/.github/CONTRIBUTING.md&#34;&gt;contributing docs&lt;/a&gt; &lt;strong&gt;before submitting a pull request&lt;/strong&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>piku/piku</title>
    <updated>2024-06-12T01:28:30Z</updated>
    <id>tag:github.com,2024-06-12:/piku/piku</id>
    <link href="https://github.com/piku/piku" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The tiniest PaaS you&#39;ve ever seen. Piku allows you to do git push deployments to your own servers.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/piku/piku/master/img/logo.png&#34; alt=&#34;piku logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;piku&lt;/code&gt;, inspired by &lt;code&gt;dokku&lt;/code&gt;, allows you do &lt;code&gt;git push&lt;/code&gt; deployments to your own servers, no matter how small they are.&lt;/p&gt; &#xA;&lt;h2&gt;Demo&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://asciinema.org/a/Ar31IoTkzsZmWWvlJll6p7haS&#34;&gt;&lt;img src=&#34;https://asciinema.org/a/Ar31IoTkzsZmWWvlJll6p7haS.svg?sanitize=true&#34; alt=&#34;asciicast&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-MIT-yellow.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Documentation: &lt;a href=&#34;https://raw.githubusercontent.com/piku/piku/master/#install&#34;&gt;Install&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/piku/piku/master/#workflow&#34;&gt;Using&lt;/a&gt; | &lt;a href=&#34;https://piku.github.io/configuration/procfile.html&#34;&gt;Procfile&lt;/a&gt; | &lt;a href=&#34;https://piku.github.io/configuration/env.html&#34;&gt;ENV&lt;/a&gt; | &lt;a href=&#34;https://piku.github.io/community/examples.html&#34;&gt;Examples&lt;/a&gt; | &lt;a href=&#34;https://github.com/orgs/piku/projects/2&#34;&gt;Roadmap&lt;/a&gt; | &lt;a href=&#34;https://piku.github.io/community/contributing.html&#34;&gt;Contributing&lt;/a&gt; | &lt;a href=&#34;https://www.youtube.com/watch?v=ec-GoDukHWk&#34;&gt;LinuxConf Talk&lt;/a&gt; | &lt;a href=&#34;https://github.com/piku/webapp-tutorial&#34;&gt;Fast Web App Tutorial&lt;/a&gt; | &lt;a href=&#34;https://github.com/piku/piku/discussions&#34;&gt;Discussion Forum&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://piku.github.io/install&#34;&gt;Install&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;p&gt;TL;DR:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl https://piku.github.io/get | sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;There are also &lt;a href=&#34;https://piku.github.io/install&#34;&gt;other installation methods&lt;/a&gt; available, including &lt;a href=&#34;https://github.com/piku/cloud-init&#34;&gt;&lt;code&gt;cloud-init&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://piku.github.io/install&#34;&gt;manual installation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Project Activity&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;code&gt;piku&lt;/code&gt; is considered STABLE&lt;/strong&gt;. It is actively maintained, but &#34;actively&#34; here means the feature set is pretty much done, so it is only updated when new language runtimes are added or reproducible bugs crop up.&lt;/p&gt; &#xA;&lt;p&gt;It currently requires Python 3.7 or above, since even though 3.8+ is now the baseline Python 3 version in Ubuntu LTS 20.04 and Debian 11 has already moved on to 3.9, there are no substantial differences between those versions.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;We wanted an Heroku/CloudFoundry-like way to deploy stuff on a few &lt;code&gt;ARM&lt;/code&gt; boards, but since &lt;code&gt;dokku&lt;/code&gt; didn&#39;t work on &lt;code&gt;ARM&lt;/code&gt; at the time and even &lt;code&gt;docker&lt;/code&gt; can be overkill sometimes, a simpler solution was needed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;piku&lt;/code&gt; is currently able to deploy, manage and independently scale multiple applications per host on both ARM and Intel architectures, and works on any cloud provider (as well as bare metal) that can run Python, &lt;code&gt;nginx&lt;/code&gt; and &lt;code&gt;uwsgi&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Workflow&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;piku&lt;/code&gt; supports a Heroku-like workflow:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;git&lt;/code&gt; SSH remote pointing to your &lt;code&gt;piku&lt;/code&gt; server with the app name as repo name: &lt;code&gt;git remote add piku piku@yourserver:appname&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Push your code: &lt;code&gt;git push piku master&lt;/code&gt; (or if you want to push a different branch than the current one use &lt;code&gt;git push piku release-branch-name&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;piku&lt;/code&gt; determines the runtime and installs the dependencies for your app (building whatever&#39;s required). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;For Python, it segregates each app&#39;s dependencies into a &lt;code&gt;virtualenv&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;For Go, it defines a separate &lt;code&gt;GOPATH&lt;/code&gt; for each app.&lt;/li&gt; &#xA;   &lt;li&gt;For Node, it installs whatever is in &lt;code&gt;package.json&lt;/code&gt; into &lt;code&gt;node_modules&lt;/code&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;For Java, it builds your app depending on either &lt;code&gt;pom.xml&lt;/code&gt; or &lt;code&gt;build.gradle&lt;/code&gt; file.&lt;/li&gt; &#xA;   &lt;li&gt;For Clojure, it can use either &lt;code&gt;leiningen&lt;/code&gt; or the Clojure CLI and a &lt;code&gt;deps.edn&lt;/code&gt; file.&lt;/li&gt; &#xA;   &lt;li&gt;For Ruby, it does &lt;code&gt;bundle install&lt;/code&gt; of your gems in an isolated folder.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;It then looks at a &lt;a href=&#34;https://piku.github.io/configuration/procfile.html&#34;&gt;&lt;code&gt;Procfile&lt;/code&gt;&lt;/a&gt; and starts the relevant workers using &lt;code&gt;uwsgi&lt;/code&gt; as a generic process manager.&lt;/li&gt; &#xA; &lt;li&gt;You can optionally also specify a &lt;code&gt;release&lt;/code&gt; worker which is run once when the app is deployed.&lt;/li&gt; &#xA; &lt;li&gt;You can then remotely change application settings (&lt;code&gt;config:set&lt;/code&gt;) or scale up/down worker processes (&lt;code&gt;ps:scale&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;You can also bake application and &lt;code&gt;nginx&lt;/code&gt; settings into an &lt;a href=&#34;https://piku.github.io/configuration/env.html&#34;&gt;&lt;code&gt;ENV&lt;/code&gt;&lt;/a&gt; file. You can also deploy a &lt;code&gt;gh-pages&lt;/code&gt; style static site using a &lt;code&gt;static&lt;/code&gt; worker type, with the root path as the argument, and run a &lt;code&gt;release&lt;/code&gt; task to do some processing on the server after &lt;code&gt;git push&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Virtual Hosts and SSL&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;piku&lt;/code&gt; has full virtual host support - i.e., you can host multiple apps on the same VPS and use DNS aliases to access them via different hostnames.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;piku&lt;/code&gt; will also set up either a private certificate or obtain one via &lt;a href=&#34;https://letsencrypt.org/&#34;&gt;Let&#39;s Encrypt&lt;/a&gt; to enable SSL.&lt;/p&gt; &#xA;&lt;p&gt;If you are on a LAN and are accessing &lt;code&gt;piku&lt;/code&gt; from macOS/iOS/Linux clients, you can try using &lt;a href=&#34;https://github.com/piku/avahi-aliases&#34;&gt;&lt;code&gt;piku/avahi-aliases&lt;/code&gt;&lt;/a&gt; to announce different hosts for the same IP address via Avahi/mDNS/Bonjour.&lt;/p&gt; &#xA;&lt;h3&gt;Caching and Static Paths&lt;/h3&gt; &#xA;&lt;p&gt;Besides static sites, &lt;code&gt;piku&lt;/code&gt; also supports directly mapping specific URL prefixes to filesystem paths (to serve static assets) or caching back-end responses (to remove load from applications).&lt;/p&gt; &#xA;&lt;p&gt;These features are configured by setting appropriate values in the &lt;a href=&#34;https://piku.github.io/configuration/env.html&#34;&gt;&lt;code&gt;ENV&lt;/code&gt;&lt;/a&gt; file.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Platforms&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;piku&lt;/code&gt; is intended to work in any POSIX-like environment where you have Python, &lt;code&gt;nginx&lt;/code&gt;, &lt;code&gt;uwsgi&lt;/code&gt; and SSH: it has been deployed on Linux, FreeBSD, &lt;a href=&#34;http://www.cygwin.com&#34;&gt;Cygwin&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux&#34;&gt;Windows Subsystem for Linux&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;As a baseline, it began its development on an original 256MB Rasbperry Pi Model B, and still runs reliably on it.&lt;/p&gt; &#xA;&lt;p&gt;But its main use is as a micro-PaaS to run applications on cloud servers with both Intel and ARM CPUs, with Debian and Ubuntu Linux as target platforms.&lt;/p&gt; &#xA;&lt;h3&gt;Supported Runtimes&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;piku&lt;/code&gt; currently supports apps written in Python, Node, Clojure, Java and a few other languages (like Go) in the works.&lt;/p&gt; &#xA;&lt;p&gt;But as a general rule, if it can be invoked from a shell, it can be run inside &lt;code&gt;piku&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Core values&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run on low end devices.&lt;/li&gt; &#xA; &lt;li&gt;Accessible to hobbyists and K-12 schools.&lt;/li&gt; &#xA; &lt;li&gt;~1500 lines readable code.&lt;/li&gt; &#xA; &lt;li&gt;Functional code style.&lt;/li&gt; &#xA; &lt;li&gt;Few (single?) dependencies&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://12factor.net&#34;&gt;12 factor app&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Simplify user experience.&lt;/li&gt; &#xA; &lt;li&gt;Cover 80% of common use cases.&lt;/li&gt; &#xA; &lt;li&gt;Sensible defaults for all features.&lt;/li&gt; &#xA; &lt;li&gt;Leverage distro packages in Raspbian/Debian/Ubuntu (Alpine and RHEL support is WIP)&lt;/li&gt; &#xA; &lt;li&gt;Leverage standard tooling (&lt;code&gt;git&lt;/code&gt;, &lt;code&gt;ssh&lt;/code&gt;, &lt;code&gt;uwsgi&lt;/code&gt;, &lt;code&gt;nginx&lt;/code&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Preserve backwards compatibility where possible&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>lllyasviel/stable-diffusion-webui-forge</title>
    <updated>2024-06-12T01:28:30Z</updated>
    <id>tag:github.com,2024-06-12:/lllyasviel/stable-diffusion-webui-forge</id>
    <link href="https://github.com/lllyasviel/stable-diffusion-webui-forge" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Stable Diffusion WebUI Forge&lt;/h1&gt; &#xA;&lt;p&gt;Stable Diffusion WebUI Forge is a platform on top of &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;Stable Diffusion WebUI&lt;/a&gt; (based on &lt;a href=&#34;https://www.gradio.app/&#34;&gt;Gradio&lt;/a&gt;) to make development easier, optimize resource management, speed up inference, and study experimental features.&lt;/p&gt; &#xA;&lt;p&gt;The name &#34;Forge&#34; is inspired from &#34;Minecraft Forge&#34;. This project is aimed at becoming SD WebUI&#39;s Forge.&lt;/p&gt; &#xA;&lt;p&gt;This repo will undergo major change very recently. See also the &lt;a href=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/801&#34;&gt;Announcement&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Installing Forge&lt;/h1&gt; &#xA;&lt;p&gt;If you are proficient in Git and you want to install Forge as another branch of SD-WebUI, please see &lt;a href=&#34;https://github.com/continue-revolution/sd-webui-animatediff/raw/forge/master/docs/how-to-use.md#you-have-a1111-and-you-know-git&#34;&gt;here&lt;/a&gt;. In this way, you can reuse all SD checkpoints and all extensions you installed previously in your OG SD-WebUI, but you should know what you are doing.&lt;/p&gt; &#xA;&lt;p&gt;If you know what you are doing, you can install Forge using same method as SD-WebUI. (Install Git, Python, Git Clone the forge repo &lt;code&gt;https://github.com/lllyasviel/stable-diffusion-webui-forge.git&lt;/code&gt; and then run webui-user.bat).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Or you can just use this one-click installation package (with git and python included).&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/releases/download/latest/webui_forge_cu121_torch21.7z&#34;&gt;&amp;gt;&amp;gt;&amp;gt; Click Here to Download One-Click Package&amp;lt;&amp;lt;&amp;lt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;After you download, you uncompress, use &lt;code&gt;update.bat&lt;/code&gt; to update, and use &lt;code&gt;run.bat&lt;/code&gt; to run.&lt;/p&gt; &#xA;&lt;p&gt;Note that running &lt;code&gt;update.bat&lt;/code&gt; is important, otherwise you may be using a previous version with potential bugs unfixed.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/c49bd60d-82bd-4086-9859-88d472582b94&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Screenshots of Comparison&lt;/h1&gt; &#xA;&lt;p&gt;I tested with several devices, and this is a typical result from 8GB VRAM (3070ti laptop) with SDXL.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is original WebUI:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/16893937-9ed9-4f8e-b960-70cd5d1e288f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/7bbc16fe-64ef-49e2-a595-d91bb658bd94&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/de1747fd-47bc-482d-a5c6-0728dd475943&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/96e5e171-2d74-41ba-9dcc-11bf68be7e16&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(average about 7.4GB/8GB, peak at about 7.9GB/8GB)&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;This is WebUI Forge:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/ca5e05ed-bd86-4ced-8662-f41034648e8c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/3629ee36-4a99-4d9b-b371-12efb260a283&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/6d13ebb7-c30d-4aa8-9242-c0b5a1af8c95&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/c4f723c3-6ea7-4539-980b-0708ed2a69aa&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(average and peak are all 6.3GB/8GB)&lt;/p&gt; &#xA;&lt;p&gt;You can see that Forge does not change WebUI results. Installing Forge is not a seed breaking change.&lt;/p&gt; &#xA;&lt;p&gt;Forge can perfectly keep WebUI unchanged even for most complicated prompts like &lt;code&gt;fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][ in background:0.25] [shoddy:masterful:0.5]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All your previous works still work in Forge!&lt;/p&gt; &#xA;&lt;h1&gt;Forge Backend&lt;/h1&gt; &#xA;&lt;p&gt;Forge backend removes all WebUI&#39;s codes related to resource management and reworked everything. All previous CMD flags like &lt;code&gt;medvram, lowvram, medvram-sdxl, precision full, no half, no half vae, attention_xxx, upcast unet&lt;/code&gt;, ... are all &lt;strong&gt;REMOVED&lt;/strong&gt;. Adding these flags will not cause error but they will not do anything now. &lt;strong&gt;We highly encourage Forge users to remove all cmd flags and let Forge to decide how to load models.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Without any cmd flag, Forge can run SDXL with 4GB vram and SD1.5 with 2GB vram.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Some flags that you may still pay attention to:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--always-offload-from-vram&lt;/code&gt; (This flag will make things &lt;strong&gt;slower&lt;/strong&gt; but less risky). This option will let Forge always unload models from VRAM. This can be useful if you use multiple software together and want Forge to use less VRAM and give some VRAM to other software, or when you are using some old extensions that will compete vram with Forge, or (very rarely) when you get OOM.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--cuda-malloc&lt;/code&gt; (This flag will make things &lt;strong&gt;faster&lt;/strong&gt; but more risky). This will ask pytorch to use &lt;em&gt;cudaMallocAsync&lt;/em&gt; for tensor malloc. On some profilers I can observe performance gain at millisecond level, but the real speed up on most my devices are often unnoticed (about or less than 0.1 second per image). This cannot be set as default because many users reported issues that the async malloc will crash the program. Users need to enable this cmd flag at their own risk.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--cuda-stream&lt;/code&gt; (This flag will make things &lt;strong&gt;faster&lt;/strong&gt; but more risky). This will use pytorch CUDA streams (a special type of thread on GPU) to move models and compute tensors simultaneously. This can almost eliminate all model moving time, and speed up SDXL on 30XX/40XX devices with small VRAM (eg, RTX 4050 6GB, RTX 3060 Laptop 6GB, etc) by about 15% to 25%. However, this unfortunately cannot be set as default because I observe higher possibility of pure black images (Nan outputs) on 2060, and higher chance of OOM on 1080 and 2060. When the resolution is large, there is a chance that the computation time of one single attention layer is longer than the time for moving entire model to GPU. When that happens, the next attention layer will OOM since the GPU is filled with the entire model, and no remaining space is available for computing another attention layer. Most overhead detecting methods are not robust enough to be reliable on old devices (in my tests). Users need to enable this cmd flag at their own risk.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;code&gt;--pin-shared-memory&lt;/code&gt; (This flag will make things &lt;strong&gt;faster&lt;/strong&gt; but more risky). Effective only when used together with &lt;code&gt;--cuda-stream&lt;/code&gt;. This will offload modules to Shared GPU Memory instead of system RAM when offloading models. On some 30XX/40XX devices with small VRAM (eg, RTX 4050 6GB, RTX 3060 Laptop 6GB, etc), I can observe significant (at least 20%) speed-up for SDXL. However, this unfortunately cannot be set as default because the OOM of Shared GPU Memory is a much more severe problem than common GPU memory OOM. Pytorch does not provide any robust method to unload or detect Shared GPU Memory. Once the Shared GPU Memory OOM, the entire program will crash (observed with SDXL on GTX 1060/1050/1066), and there is no dynamic method to prevent or recover from the crash. Users need to enable this cmd flag at their own risk.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;If you really want to play with cmd flags, you can additionally control the GPU with:&lt;/p&gt; &#xA;&lt;p&gt;(extreme VRAM cases)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--always-gpu&#xA;--always-cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(rare attention cases)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--attention-split&#xA;--attention-quad&#xA;--attention-pytorch&#xA;--disable-xformers&#xA;--disable-attention-upcast&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(float point type)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--all-in-fp32&#xA;--all-in-fp16&#xA;--unet-in-bf16&#xA;--unet-in-fp16&#xA;--unet-in-fp8-e4m3fn&#xA;--unet-in-fp8-e5m2&#xA;--vae-in-fp16&#xA;--vae-in-fp32&#xA;--vae-in-bf16&#xA;--clip-in-fp8-e4m3fn&#xA;--clip-in-fp8-e5m2&#xA;--clip-in-fp16&#xA;--clip-in-fp32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(rare platforms)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;--directml&#xA;--disable-ipex-hijack&#xA;--pytorch-deterministic&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Again, Forge do not recommend users to use any cmd flags unless you are very sure that you really need these.&lt;/p&gt; &#xA;&lt;h1&gt;UNet Patcher&lt;/h1&gt; &#xA;&lt;p&gt;Note that &lt;a href=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/discussions/169&#34;&gt;Forge does not use any other software as backend&lt;/a&gt;. The full name of the backend is &lt;code&gt;Stable Diffusion WebUI with Forge backend&lt;/code&gt;, or for simplicity, the &lt;code&gt;Forge backend&lt;/code&gt;. The API and python symbols are made similar to previous software only for reducing the learning cost of developers.&lt;/p&gt; &#xA;&lt;p&gt;Now developing an extension is super simple. We finally have a patchable UNet.&lt;/p&gt; &#xA;&lt;p&gt;Below is using one single file with 80 lines of codes to support FreeU:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;extensions-builtin/sd_forge_freeu/scripts/forge_freeu.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import gradio as gr&#xA;from modules import scripts&#xA;&#xA;&#xA;def Fourier_filter(x, threshold, scale):&#xA;    x_freq = torch.fft.fftn(x.float(), dim=(-2, -1))&#xA;    x_freq = torch.fft.fftshift(x_freq, dim=(-2, -1))&#xA;    B, C, H, W = x_freq.shape&#xA;    mask = torch.ones((B, C, H, W), device=x.device)&#xA;    crow, ccol = H // 2, W //2&#xA;    mask[..., crow - threshold:crow + threshold, ccol - threshold:ccol + threshold] = scale&#xA;    x_freq = x_freq * mask&#xA;    x_freq = torch.fft.ifftshift(x_freq, dim=(-2, -1))&#xA;    x_filtered = torch.fft.ifftn(x_freq, dim=(-2, -1)).real&#xA;    return x_filtered.to(x.dtype)&#xA;&#xA;&#xA;def set_freeu_v2_patch(model, b1, b2, s1, s2):&#xA;    model_channels = model.model.model_config.unet_config[&#34;model_channels&#34;]&#xA;    scale_dict = {model_channels * 4: (b1, s1), model_channels * 2: (b2, s2)}&#xA;&#xA;    def output_block_patch(h, hsp, *args, **kwargs):&#xA;        scale = scale_dict.get(h.shape[1], None)&#xA;        if scale is not None:&#xA;            hidden_mean = h.mean(1).unsqueeze(1)&#xA;            B = hidden_mean.shape[0]&#xA;            hidden_max, _ = torch.max(hidden_mean.view(B, -1), dim=-1, keepdim=True)&#xA;            hidden_min, _ = torch.min(hidden_mean.view(B, -1), dim=-1, keepdim=True)&#xA;            hidden_mean = (hidden_mean - hidden_min.unsqueeze(2).unsqueeze(3)) / \&#xA;                          (hidden_max - hidden_min).unsqueeze(2).unsqueeze(3)&#xA;            h[:, :h.shape[1] // 2] = h[:, :h.shape[1] // 2] * ((scale[0] - 1) * hidden_mean + 1)&#xA;            hsp = Fourier_filter(hsp, threshold=1, scale=scale[1])&#xA;        return h, hsp&#xA;&#xA;    m = model.clone()&#xA;    m.set_model_output_block_patch(output_block_patch)&#xA;    return m&#xA;&#xA;&#xA;class FreeUForForge(scripts.Script):&#xA;    def title(self):&#xA;        return &#34;FreeU Integrated&#34;&#xA;&#xA;    def show(self, is_img2img):&#xA;        # make this extension visible in both txt2img and img2img tab.&#xA;        return scripts.AlwaysVisible&#xA;&#xA;    def ui(self, *args, **kwargs):&#xA;        with gr.Accordion(open=False, label=self.title()):&#xA;            freeu_enabled = gr.Checkbox(label=&#39;Enabled&#39;, value=False)&#xA;            freeu_b1 = gr.Slider(label=&#39;B1&#39;, minimum=0, maximum=2, step=0.01, value=1.01)&#xA;            freeu_b2 = gr.Slider(label=&#39;B2&#39;, minimum=0, maximum=2, step=0.01, value=1.02)&#xA;            freeu_s1 = gr.Slider(label=&#39;S1&#39;, minimum=0, maximum=4, step=0.01, value=0.99)&#xA;            freeu_s2 = gr.Slider(label=&#39;S2&#39;, minimum=0, maximum=4, step=0.01, value=0.95)&#xA;&#xA;        return freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2&#xA;&#xA;    def process_before_every_sampling(self, p, *script_args, **kwargs):&#xA;        # This will be called before every sampling.&#xA;        # If you use highres fix, this will be called twice.&#xA;        &#xA;        freeu_enabled, freeu_b1, freeu_b2, freeu_s1, freeu_s2 = script_args&#xA;&#xA;        if not freeu_enabled:&#xA;            return&#xA;&#xA;        unet = p.sd_model.forge_objects.unet&#xA;&#xA;        unet = set_freeu_v2_patch(unet, freeu_b1, freeu_b2, freeu_s1, freeu_s2)&#xA;&#xA;        p.sd_model.forge_objects.unet = unet&#xA;&#xA;        # Below codes will add some logs to the texts below the image outputs on UI.&#xA;        # The extra_generation_params does not influence results.&#xA;        p.extra_generation_params.update(dict(&#xA;            freeu_enabled=freeu_enabled,&#xA;            freeu_b1=freeu_b1,&#xA;            freeu_b2=freeu_b2,&#xA;            freeu_s1=freeu_s1,&#xA;            freeu_s2=freeu_s2,&#xA;        ))&#xA;&#xA;        return&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It looks like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/277bac6e-5ea7-4bff-b71a-e55a60cfc03c&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Similar components like HyperTile, KohyaHighResFix, SAG, can all be implemented within 100 lines of codes (see also the codes).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/06472b03-b833-4816-ab47-70712ac024d3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;ControlNets can finally be called by different extensions.&lt;/p&gt; &#xA;&lt;p&gt;Implementing Stable Video Diffusion and Zero123 are also super simple now (see also the codes).&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Stable Video Diffusion:&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;extensions-builtin/sd_forge_svd/scripts/forge_svd.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;import gradio as gr&#xA;import os&#xA;import pathlib&#xA;&#xA;from modules import script_callbacks&#xA;from modules.paths import models_path&#xA;from modules.ui_common import ToolButton, refresh_symbol&#xA;from modules import shared&#xA;&#xA;from modules_forge.forge_util import numpy_to_pytorch, pytorch_to_numpy&#xA;from ldm_patched.modules.sd import load_checkpoint_guess_config&#xA;from ldm_patched.contrib.external_video_model import VideoLinearCFGGuidance, SVD_img2vid_Conditioning&#xA;from ldm_patched.contrib.external import KSampler, VAEDecode&#xA;&#xA;&#xA;opVideoLinearCFGGuidance = VideoLinearCFGGuidance()&#xA;opSVD_img2vid_Conditioning = SVD_img2vid_Conditioning()&#xA;opKSampler = KSampler()&#xA;opVAEDecode = VAEDecode()&#xA;&#xA;svd_root = os.path.join(models_path, &#39;svd&#39;)&#xA;os.makedirs(svd_root, exist_ok=True)&#xA;svd_filenames = []&#xA;&#xA;&#xA;def update_svd_filenames():&#xA;    global svd_filenames&#xA;    svd_filenames = [&#xA;        pathlib.Path(x).name for x in&#xA;        shared.walk_files(svd_root, allowed_extensions=[&#34;.pt&#34;, &#34;.ckpt&#34;, &#34;.safetensors&#34;])&#xA;    ]&#xA;    return svd_filenames&#xA;&#xA;&#xA;@torch.inference_mode()&#xA;@torch.no_grad()&#xA;def predict(filename, width, height, video_frames, motion_bucket_id, fps, augmentation_level,&#xA;            sampling_seed, sampling_steps, sampling_cfg, sampling_sampler_name, sampling_scheduler,&#xA;            sampling_denoise, guidance_min_cfg, input_image):&#xA;    filename = os.path.join(svd_root, filename)&#xA;    model_raw, _, vae, clip_vision = \&#xA;        load_checkpoint_guess_config(filename, output_vae=True, output_clip=False, output_clipvision=True)&#xA;    model = opVideoLinearCFGGuidance.patch(model_raw, guidance_min_cfg)[0]&#xA;    init_image = numpy_to_pytorch(input_image)&#xA;    positive, negative, latent_image = opSVD_img2vid_Conditioning.encode(&#xA;        clip_vision, init_image, vae, width, height, video_frames, motion_bucket_id, fps, augmentation_level)&#xA;    output_latent = opKSampler.sample(model, sampling_seed, sampling_steps, sampling_cfg,&#xA;                                      sampling_sampler_name, sampling_scheduler, positive,&#xA;                                      negative, latent_image, sampling_denoise)[0]&#xA;    output_pixels = opVAEDecode.decode(vae, output_latent)[0]&#xA;    outputs = pytorch_to_numpy(output_pixels)&#xA;    return outputs&#xA;&#xA;&#xA;def on_ui_tabs():&#xA;    with gr.Blocks() as svd_block:&#xA;        with gr.Row():&#xA;            with gr.Column():&#xA;                input_image = gr.Image(label=&#39;Input Image&#39;, source=&#39;upload&#39;, type=&#39;numpy&#39;, height=400)&#xA;&#xA;                with gr.Row():&#xA;                    filename = gr.Dropdown(label=&#34;SVD Checkpoint Filename&#34;,&#xA;                                           choices=svd_filenames,&#xA;                                           value=svd_filenames[0] if len(svd_filenames) &amp;gt; 0 else None)&#xA;                    refresh_button = ToolButton(value=refresh_symbol, tooltip=&#34;Refresh&#34;)&#xA;                    refresh_button.click(&#xA;                        fn=lambda: gr.update(choices=update_svd_filenames),&#xA;                        inputs=[], outputs=filename)&#xA;&#xA;                width = gr.Slider(label=&#39;Width&#39;, minimum=16, maximum=8192, step=8, value=1024)&#xA;                height = gr.Slider(label=&#39;Height&#39;, minimum=16, maximum=8192, step=8, value=576)&#xA;                video_frames = gr.Slider(label=&#39;Video Frames&#39;, minimum=1, maximum=4096, step=1, value=14)&#xA;                motion_bucket_id = gr.Slider(label=&#39;Motion Bucket Id&#39;, minimum=1, maximum=1023, step=1, value=127)&#xA;                fps = gr.Slider(label=&#39;Fps&#39;, minimum=1, maximum=1024, step=1, value=6)&#xA;                augmentation_level = gr.Slider(label=&#39;Augmentation Level&#39;, minimum=0.0, maximum=10.0, step=0.01,&#xA;                                               value=0.0)&#xA;                sampling_steps = gr.Slider(label=&#39;Sampling Steps&#39;, minimum=1, maximum=200, step=1, value=20)&#xA;                sampling_cfg = gr.Slider(label=&#39;CFG Scale&#39;, minimum=0.0, maximum=50.0, step=0.1, value=2.5)&#xA;                sampling_denoise = gr.Slider(label=&#39;Sampling Denoise&#39;, minimum=0.0, maximum=1.0, step=0.01, value=1.0)&#xA;                guidance_min_cfg = gr.Slider(label=&#39;Guidance Min Cfg&#39;, minimum=0.0, maximum=100.0, step=0.5, value=1.0)&#xA;                sampling_sampler_name = gr.Radio(label=&#39;Sampler Name&#39;,&#xA;                                                 choices=[&#39;euler&#39;, &#39;euler_ancestral&#39;, &#39;heun&#39;, &#39;heunpp2&#39;, &#39;dpm_2&#39;,&#xA;                                                          &#39;dpm_2_ancestral&#39;, &#39;lms&#39;, &#39;dpm_fast&#39;, &#39;dpm_adaptive&#39;,&#xA;                                                          &#39;dpmpp_2s_ancestral&#39;, &#39;dpmpp_sde&#39;, &#39;dpmpp_sde_gpu&#39;,&#xA;                                                          &#39;dpmpp_2m&#39;, &#39;dpmpp_2m_sde&#39;, &#39;dpmpp_2m_sde_gpu&#39;,&#xA;                                                          &#39;dpmpp_3m_sde&#39;, &#39;dpmpp_3m_sde_gpu&#39;, &#39;ddpm&#39;, &#39;lcm&#39;, &#39;ddim&#39;,&#xA;                                                          &#39;uni_pc&#39;, &#39;uni_pc_bh2&#39;], value=&#39;euler&#39;)&#xA;                sampling_scheduler = gr.Radio(label=&#39;Scheduler&#39;,&#xA;                                              choices=[&#39;normal&#39;, &#39;karras&#39;, &#39;exponential&#39;, &#39;sgm_uniform&#39;, &#39;simple&#39;,&#xA;                                                       &#39;ddim_uniform&#39;], value=&#39;karras&#39;)&#xA;                sampling_seed = gr.Number(label=&#39;Seed&#39;, value=12345, precision=0)&#xA;&#xA;                generate_button = gr.Button(value=&#34;Generate&#34;)&#xA;&#xA;                ctrls = [filename, width, height, video_frames, motion_bucket_id, fps, augmentation_level,&#xA;                         sampling_seed, sampling_steps, sampling_cfg, sampling_sampler_name, sampling_scheduler,&#xA;                         sampling_denoise, guidance_min_cfg, input_image]&#xA;&#xA;            with gr.Column():&#xA;                output_gallery = gr.Gallery(label=&#39;Gallery&#39;, show_label=False, object_fit=&#39;contain&#39;,&#xA;                                            visible=True, height=1024, columns=4)&#xA;&#xA;        generate_button.click(predict, inputs=ctrls, outputs=[output_gallery])&#xA;    return [(svd_block, &#34;SVD&#34;, &#34;svd&#34;)]&#xA;&#xA;&#xA;update_svd_filenames()&#xA;script_callbacks.on_ui_tabs(on_ui_tabs)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that although the above codes look like independent codes, they actually will automatically offload/unload any other models. For example, below is me opening webui, load SDXL, generated an image, then go to SVD, then generated image frames. You can see that the GPU memory is perfectly managed and the SDXL is moved to RAM then SVD is moved to GPU.&lt;/p&gt; &#xA;&lt;p&gt;Note that this management is fully automatic. This makes writing extensions super simple.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/de1a2d05-344a-44d7-bab8-9ecc0a58a8d3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/14bcefcf-599f-42c3-bce9-3fd5e428dd91&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Similarly, Zero123:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/7685019c-7239-47fb-9cb5-2b7b33943285&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Write a simple ControlNet:&lt;/h3&gt; &#xA;&lt;p&gt;Below is a simple extension to have a completely independent pass of ControlNet that never conflicts any other extensions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;extensions-builtin/sd_forge_controlnet_example/scripts/sd_forge_controlnet_example.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note that this extension is hidden because it is only for developers. To see it in UI, use &lt;code&gt;--show-controlnet-example&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The memory optimization in this example is fully automatic. You do not need to care about memory and inference speed, but you may want to cache objects if you wish.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Use --show-controlnet-example to see this extension.&#xA;&#xA;import cv2&#xA;import gradio as gr&#xA;import torch&#xA;&#xA;from modules import scripts&#xA;from modules.shared_cmd_options import cmd_opts&#xA;from modules_forge.shared import supported_preprocessors&#xA;from modules.modelloader import load_file_from_url&#xA;from ldm_patched.modules.controlnet import load_controlnet&#xA;from modules_forge.controlnet import apply_controlnet_advanced&#xA;from modules_forge.forge_util import numpy_to_pytorch&#xA;from modules_forge.shared import controlnet_dir&#xA;&#xA;&#xA;class ControlNetExampleForge(scripts.Script):&#xA;    model = None&#xA;&#xA;    def title(self):&#xA;        return &#34;ControlNet Example for Developers&#34;&#xA;&#xA;    def show(self, is_img2img):&#xA;        # make this extension visible in both txt2img and img2img tab.&#xA;        return scripts.AlwaysVisible&#xA;&#xA;    def ui(self, *args, **kwargs):&#xA;        with gr.Accordion(open=False, label=self.title()):&#xA;            gr.HTML(&#39;This is an example controlnet extension for developers.&#39;)&#xA;            gr.HTML(&#39;You see this extension because you used --show-controlnet-example&#39;)&#xA;            input_image = gr.Image(source=&#39;upload&#39;, type=&#39;numpy&#39;)&#xA;            funny_slider = gr.Slider(label=&#39;This slider does nothing. It just shows you how to transfer parameters.&#39;,&#xA;                                     minimum=0.0, maximum=1.0, value=0.5)&#xA;&#xA;        return input_image, funny_slider&#xA;&#xA;    def process(self, p, *script_args, **kwargs):&#xA;        input_image, funny_slider = script_args&#xA;&#xA;        # This slider does nothing. It just shows you how to transfer parameters.&#xA;        del funny_slider&#xA;&#xA;        if input_image is None:&#xA;            return&#xA;&#xA;        # controlnet_canny_path = load_file_from_url(&#xA;        #     url=&#39;https://huggingface.co/lllyasviel/sd_control_collection/resolve/main/sai_xl_canny_256lora.safetensors&#39;,&#xA;        #     model_dir=model_dir,&#xA;        #     file_name=&#39;sai_xl_canny_256lora.safetensors&#39;&#xA;        # )&#xA;        controlnet_canny_path = load_file_from_url(&#xA;            url=&#39;https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/control_v11p_sd15_canny_fp16.safetensors&#39;,&#xA;            model_dir=controlnet_dir,&#xA;            file_name=&#39;control_v11p_sd15_canny_fp16.safetensors&#39;&#xA;        )&#xA;        print(&#39;The model [control_v11p_sd15_canny_fp16.safetensors] download finished.&#39;)&#xA;&#xA;        self.model = load_controlnet(controlnet_canny_path)&#xA;        print(&#39;Controlnet loaded.&#39;)&#xA;&#xA;        return&#xA;&#xA;    def process_before_every_sampling(self, p, *script_args, **kwargs):&#xA;        # This will be called before every sampling.&#xA;        # If you use highres fix, this will be called twice.&#xA;&#xA;        input_image, funny_slider = script_args&#xA;&#xA;        if input_image is None or self.model is None:&#xA;            return&#xA;&#xA;        B, C, H, W = kwargs[&#39;noise&#39;].shape  # latent_shape&#xA;        height = H * 8&#xA;        width = W * 8&#xA;        batch_size = p.batch_size&#xA;&#xA;        preprocessor = supported_preprocessors[&#39;canny&#39;]&#xA;&#xA;        # detect control at certain resolution&#xA;        control_image = preprocessor(&#xA;            input_image, resolution=512, slider_1=100, slider_2=200, slider_3=None)&#xA;&#xA;        # here we just use nearest neighbour to align input shape.&#xA;        # You may want crop and resize, or crop and fill, or others.&#xA;        control_image = cv2.resize(&#xA;            control_image, (width, height), interpolation=cv2.INTER_NEAREST)&#xA;&#xA;        # Output preprocessor result. Now called every sampling. Cache in your own way.&#xA;        p.extra_result_images.append(control_image)&#xA;&#xA;        print(&#39;Preprocessor Canny finished.&#39;)&#xA;&#xA;        control_image_bchw = numpy_to_pytorch(control_image).movedim(-1, 1)&#xA;&#xA;        unet = p.sd_model.forge_objects.unet&#xA;&#xA;        # Unet has input, middle, output blocks, and we can give different weights&#xA;        # to each layers in all blocks.&#xA;        # Below is an example for stronger control in middle block.&#xA;        # This is helpful for some high-res fix passes. (p.is_hr_pass)&#xA;        positive_advanced_weighting = {&#xA;            &#39;input&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2],&#xA;            &#39;middle&#39;: [1.0],&#xA;            &#39;output&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2]&#xA;        }&#xA;        negative_advanced_weighting = {&#xA;            &#39;input&#39;: [0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, 1.05, 1.15, 1.25],&#xA;            &#39;middle&#39;: [1.05],&#xA;            &#39;output&#39;: [0.15, 0.25, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, 1.05, 1.15, 1.25]&#xA;        }&#xA;&#xA;        # The advanced_frame_weighting is a weight applied to each image in a batch.&#xA;        # The length of this list must be same with batch size&#xA;        # For example, if batch size is 5, the below list is [0.2, 0.4, 0.6, 0.8, 1.0]&#xA;        # If you view the 5 images as 5 frames in a video, this will lead to&#xA;        # progressively stronger control over time.&#xA;        advanced_frame_weighting = [float(i + 1) / float(batch_size) for i in range(batch_size)]&#xA;&#xA;        # The advanced_sigma_weighting allows you to dynamically compute control&#xA;        # weights given diffusion timestep (sigma).&#xA;        # For example below code can softly make beginning steps stronger than ending steps.&#xA;        sigma_max = unet.model.model_sampling.sigma_max&#xA;        sigma_min = unet.model.model_sampling.sigma_min&#xA;        advanced_sigma_weighting = lambda s: (s - sigma_min) / (sigma_max - sigma_min)&#xA;&#xA;        # You can even input a tensor to mask all control injections&#xA;        # The mask will be automatically resized during inference in UNet.&#xA;        # The size should be B 1 H W and the H and W are not important&#xA;        # because they will be resized automatically&#xA;        advanced_mask_weighting = torch.ones(size=(1, 1, 512, 512))&#xA;&#xA;        # But in this simple example we do not use them&#xA;        positive_advanced_weighting = None&#xA;        negative_advanced_weighting = None&#xA;        advanced_frame_weighting = None&#xA;        advanced_sigma_weighting = None&#xA;        advanced_mask_weighting = None&#xA;&#xA;        unet = apply_controlnet_advanced(unet=unet, controlnet=self.model, image_bchw=control_image_bchw,&#xA;                                         strength=0.6, start_percent=0.0, end_percent=0.8,&#xA;                                         positive_advanced_weighting=positive_advanced_weighting,&#xA;                                         negative_advanced_weighting=negative_advanced_weighting,&#xA;                                         advanced_frame_weighting=advanced_frame_weighting,&#xA;                                         advanced_sigma_weighting=advanced_sigma_weighting,&#xA;                                         advanced_mask_weighting=advanced_mask_weighting)&#xA;&#xA;        p.sd_model.forge_objects.unet = unet&#xA;&#xA;        # Below codes will add some logs to the texts below the image outputs on UI.&#xA;        # The extra_generation_params does not influence results.&#xA;        p.extra_generation_params.update(dict(&#xA;            controlnet_info=&#39;You should see these texts below output images!&#39;,&#xA;        ))&#xA;&#xA;        return&#xA;&#xA;&#xA;# Use --show-controlnet-example to see this extension.&#xA;if not cmd_opts.show_controlnet_example:&#xA;    del ControlNetExampleForge&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/822fa2fc-c9f4-4f58-8669-4b6680b91063&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Add a preprocessor&lt;/h3&gt; &#xA;&lt;p&gt;Below is the full codes to add a normalbae preprocessor with perfect memory managements.&lt;/p&gt; &#xA;&lt;p&gt;You can use arbitrary independent extensions to add a preprocessor.&lt;/p&gt; &#xA;&lt;p&gt;Your preprocessor will be read by all other extensions using &lt;code&gt;modules_forge.shared.preprocessors&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below codes are in &lt;code&gt;extensions-builtin\forge_preprocessor_normalbae\scripts\preprocessor_normalbae.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from modules_forge.supported_preprocessor import Preprocessor, PreprocessorParameter&#xA;from modules_forge.shared import preprocessor_dir, add_supported_preprocessor&#xA;from modules_forge.forge_util import resize_image_with_pad&#xA;from modules.modelloader import load_file_from_url&#xA;&#xA;import types&#xA;import torch&#xA;import numpy as np&#xA;&#xA;from einops import rearrange&#xA;from annotator.normalbae.models.NNET import NNET&#xA;from annotator.normalbae import load_checkpoint&#xA;from torchvision import transforms&#xA;&#xA;&#xA;class PreprocessorNormalBae(Preprocessor):&#xA;    def __init__(self):&#xA;        super().__init__()&#xA;        self.name = &#39;normalbae&#39;&#xA;        self.tags = [&#39;NormalMap&#39;]&#xA;        self.model_filename_filters = [&#39;normal&#39;]&#xA;        self.slider_resolution = PreprocessorParameter(&#xA;            label=&#39;Resolution&#39;, minimum=128, maximum=2048, value=512, step=8, visible=True)&#xA;        self.slider_1 = PreprocessorParameter(visible=False)&#xA;        self.slider_2 = PreprocessorParameter(visible=False)&#xA;        self.slider_3 = PreprocessorParameter(visible=False)&#xA;        self.show_control_mode = True&#xA;        self.do_not_need_model = False&#xA;        self.sorting_priority = 100  # higher goes to top in the list&#xA;&#xA;    def load_model(self):&#xA;        if self.model_patcher is not None:&#xA;            return&#xA;&#xA;        model_path = load_file_from_url(&#xA;            &#34;https://huggingface.co/lllyasviel/Annotators/resolve/main/scannet.pt&#34;,&#xA;            model_dir=preprocessor_dir)&#xA;&#xA;        args = types.SimpleNamespace()&#xA;        args.mode = &#39;client&#39;&#xA;        args.architecture = &#39;BN&#39;&#xA;        args.pretrained = &#39;scannet&#39;&#xA;        args.sampling_ratio = 0.4&#xA;        args.importance_ratio = 0.7&#xA;        model = NNET(args)&#xA;        model = load_checkpoint(model_path, model)&#xA;        self.norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])&#xA;&#xA;        self.model_patcher = self.setup_model_patcher(model)&#xA;&#xA;    def __call__(self, input_image, resolution, slider_1=None, slider_2=None, slider_3=None, **kwargs):&#xA;        input_image, remove_pad = resize_image_with_pad(input_image, resolution)&#xA;&#xA;        self.load_model()&#xA;&#xA;        self.move_all_model_patchers_to_gpu()&#xA;&#xA;        assert input_image.ndim == 3&#xA;        image_normal = input_image&#xA;&#xA;        with torch.no_grad():&#xA;            image_normal = self.send_tensor_to_model_device(torch.from_numpy(image_normal))&#xA;            image_normal = image_normal / 255.0&#xA;            image_normal = rearrange(image_normal, &#39;h w c -&amp;gt; 1 c h w&#39;)&#xA;            image_normal = self.norm(image_normal)&#xA;&#xA;            normal = self.model_patcher.model(image_normal)&#xA;            normal = normal[0][-1][:, :3]&#xA;            normal = ((normal + 1) * 0.5).clip(0, 1)&#xA;&#xA;            normal = rearrange(normal[0], &#39;c h w -&amp;gt; h w c&#39;).cpu().numpy()&#xA;            normal_image = (normal * 255.0).clip(0, 255).astype(np.uint8)&#xA;&#xA;        return remove_pad(normal_image)&#xA;&#xA;&#xA;add_supported_preprocessor(PreprocessorNormalBae())&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;New features (that are not available in original WebUI)&lt;/h1&gt; &#xA;&lt;p&gt;Thanks to Unet Patcher, many new things are possible now and supported in Forge, including SVD, Z123, masked Ip-adapter, masked controlnet, photomaker, etc.&lt;/p&gt; &#xA;&lt;p&gt;Masked Ip-Adapter&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/d26630f9-922d-4483-8bf9-f364dca5fd50&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/03580ef7-235c-4b03-9ca6-a27677a5a175&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/d9ed4a01-70d4-45b4-a6a7-2f765f158fae&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Masked ControlNet&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/872d4785-60e4-4431-85c7-665c781dddaa&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/335a3b33-1ef8-46ff-a462-9f1b4f2c49fc&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/b3684a15-8895-414e-8188-487269dfcada&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;PhotoMaker&lt;/p&gt; &#xA;&lt;p&gt;(Note that photomaker is a special control that need you to add the trigger word &#34;photomaker&#34;. Your prompt should be like &#34;a photo of photomaker&#34;)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/07b0b626-05b5-473b-9d69-3657624d59be&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Marigold Depth&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge/assets/19834515/bdf54148-892d-410d-8ed9-70b4b121b6e7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;New Samplers (that are not in origin)&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code&gt;DDPM&#xA;DDPM Karras&#xA;DPM++ 2M Turbo&#xA;DPM++ 2M SDE Turbo&#xA;LCM Karras&#xA;Euler A Turbo&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;About Extensions&lt;/h1&gt; &#xA;&lt;p&gt;ControlNet and TiledVAE are integrated, and you should uninstall these two extensions:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sd-webui-controlnet&#xA;multidiffusion-upscaler-for-automatic1111&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that &lt;strong&gt;AnimateDiff&lt;/strong&gt; is under construction by &lt;a href=&#34;https://github.com/continue-revolution&#34;&gt;continue-revolution&lt;/a&gt; at &lt;a href=&#34;https://github.com/continue-revolution/sd-webui-animatediff/tree/forge/master&#34;&gt;sd-webui-animatediff forge/master branch&lt;/a&gt; and &lt;a href=&#34;https://github.com/continue-revolution/sd-forge-animatediff&#34;&gt;sd-forge-animatediff&lt;/a&gt; (they are in sync). (continue-revolution original words: prompt travel, inf t2v, controlnet v2v have been proven to work well; motion lora, i2i batch still under construction and may be finished in a week&#34;)&lt;/p&gt; &#xA;&lt;p&gt;Other extensions should work without problems, like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;canvas-zoom&#xA;translations/localizations&#xA;Dynamic Prompts&#xA;Adetailer&#xA;Ultimate SD Upscale&#xA;Reactor&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;However, if newer extensions use Forge, their codes can be much shorter.&lt;/p&gt; &#xA;&lt;p&gt;Usually if an old extension rework using Forge&#39;s unet patcher, 80% codes can be removed, especially when they need to call controlnet.&lt;/p&gt; &#xA;&lt;h1&gt;Contribution&lt;/h1&gt; &#xA;&lt;p&gt;Forge uses a bot to get commits and codes from &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev&lt;/a&gt; every afternoon (if merge is automatically successful by a git bot, or by my compiler, or by my ChatGPT bot) or mid-night (if my compiler and my ChatGPT bot both failed to merge and I review it manually).&lt;/p&gt; &#xA;&lt;p&gt;All PRs that can be implemented in &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui/tree/dev&lt;/a&gt; should submit PRs there.&lt;/p&gt; &#xA;&lt;p&gt;Feel free to submit PRs related to the functionality of Forge here.&lt;/p&gt;</summary>
  </entry>
</feed>