<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2022-12-04T01:30:15Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hehonghui/awesome-english-ebooks</title>
    <updated>2022-12-04T01:30:15Z</updated>
    <id>tag:github.com,2022-12-04:/hehonghui/awesome-english-ebooks</id>
    <link href="https://github.com/hehonghui/awesome-english-ebooks" rel="alternate"></link>
    <summary type="html">&lt;p&gt;经济学人(含音频)、纽约客、卫报、连线、大西洋月刊等英语杂志免费下载,支持epub、mobi、pdf格式, 每周更新&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;经济学人、纽约客等英语外刊杂志下载&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;一、优质App推荐&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;img align=&#34;center&#34; src=&#34;https://ereader.link/images/ereader.png&#34; width=&#34;32px&#34;&gt; 英阅阅读器 - 超好用的英语阅读神器, &lt;font color=&#34;#e3120b&#34;&gt;让您轻松读懂英文小说、外刊杂志&lt;/font&gt;,支持点击查词、句子翻译、mdict英汉-英英词典、阅读笔记等功能,&lt;a href=&#34;https://apps.apple.com/cn/app/ereader-%E8%8B%B1%E9%98%85%E9%98%85%E8%AF%BB%E5%99%A8/id1558805880&#34;&gt;iOS版下载&lt;/a&gt;、&lt;a href=&#34;https://www.coolapk.com/apk/283424&#34;&gt;Android版下载&lt;/a&gt;;&lt;/li&gt; &#xA; &lt;li&gt;&lt;img align=&#34;center&#34; src=&#34;https://img-blog.csdnimg.cn/909634eacf664a8f8f5fc41ec648bb1c.png&#34; width=&#34;32px&#34;&gt; 英语考级君 - &lt;font color=&#34;#e3120b&#34;&gt;考研英语、四六级一站式备考App&lt;/font&gt;, 真题刷题、听力提升(有声书、听力素材)、背单词、语法学习全覆盖, 助力您的英语考试。&lt;a href=&#34;https://apps.apple.com/id/app/%E8%8B%B1%E8%AF%AD%E8%80%83%E7%BA%A7%E5%90%9B-%E8%8B%B1%E8%AF%AD%E8%80%83%E8%AF%95%E5%A4%87%E8%80%83-%E6%82%A8%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%9C%9F%E9%A2%98%E4%BC%B4%E4%BE%A3/id1585354395&#34;&gt;点击这里下载iOS版&lt;/a&gt; ;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;二、内容分类&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hehonghui/awesome-english-ebooks/master/01_economist/te_2022.12.03&#34;&gt;经济学人 - 周刊, 点击这里下载最新一期&lt;/a&gt; , 每周五十一点更新&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hehonghui/awesome-english-ebooks/master/02_new_yorker/2022.12.05&#34;&gt;纽约客 - 周刊, 点击这里下载最新一期&lt;/a&gt; , 每周六上午更新&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hehonghui/awesome-english-ebooks/master/09_guardian/&#34;&gt;卫报 - 每周两期&lt;/a&gt;, 每周三、周日更新&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hehonghui/awesome-english-ebooks/master/04_atlantic&#34;&gt;The Atlantic - 月刊&lt;/a&gt;, 每月2号更新&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hehonghui/awesome-english-ebooks/master/05_wired&#34;&gt;Wired - 月刊&lt;/a&gt;, 每月2号更新&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;如何选择杂志 ? 请参考下面两篇文章&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/25051680&#34;&gt;考研英语题源分析，看看题目来自于哪里&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/54181221&#34;&gt;2018英语外刊大合集&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;三、其他阅读器&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;epub 格式的电子书可以安装 &lt;a href=&#34;https://www.duokan.com/product&#34;&gt;多看阅读&lt;/a&gt; , 通过 &lt;code&gt;wifi传书功能&lt;/code&gt; 通过浏览器将电子书传入到阅读器中, 然后就可以进行阅读;&lt;/li&gt; &#xA; &lt;li&gt;mobi 格式的电子书需要使用 &lt;code&gt;kindle设备&lt;/code&gt; 或者在电脑、手机上安装 &lt;a href=&#34;https://www.amazon.cn/kindle-dbs/fd/kcp/ref=sv_kinc_0&#34;&gt;kindle 阅读app&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt;</summary>
  </entry>
  <entry>
    <title>apple/ml-stable-diffusion</title>
    <updated>2022-12-04T01:30:15Z</updated>
    <id>tag:github.com,2022-12-04:/apple/ml-stable-diffusion</id>
    <link href="https://github.com/apple/ml-stable-diffusion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Stable Diffusion with Core ML on Apple Silicon&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Core ML Stable Diffusion&lt;/h1&gt; &#xA;&lt;p&gt;Run Stable Diffusion on Apple Silicon with Core ML&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/readme_reel.png&#34;&gt; &#xA;&lt;p&gt;This repository comprises:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;python_coreml_stable_diffusion&lt;/code&gt;, a Python package for converting PyTorch models to Core ML format and performing image generation with Hugging Face &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; in Python&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;StableDiffusion&lt;/code&gt;, a Swift package that developers can add to their Xcode projects as a dependency to deploy image generation capabilities in their apps. The Swift package relies on the Core ML model files generated by &lt;code&gt;python_coreml_stable_diffusion&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you run into issues during installation or runtime, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#FAQ&#34;&gt;FAQ&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;example-results&#34;&gt;&lt;/a&gt; Example Results&lt;/h2&gt; &#xA;&lt;p&gt;There are numerous versions of Stable Diffusion available on the &lt;a href=&#34;https://huggingface.co/models?search=stable-diffusion&#34;&gt;Hugging Face Hub&lt;/a&gt;. Here are example results from three of those models:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;code&gt;--model-version&lt;/code&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-base&#34;&gt;stabilityai/stable-diffusion-2-base&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;CompVis/stable-diffusion-v1-4&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Output&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_11_computeUnit_CPU_AND_GPU_modelVersion_stabilityai_stable-diffusion-2-base.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_13_computeUnit_CPU_AND_NE_modelVersion_CompVis_stable-diffusion-v1-4.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_CPU_AND_NE_modelVersion_runwayml_stable-diffusion-v1-5.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;M1 iPad Pro 8GB Latency (s)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;29&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;38&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;M1 MacBook Pro 16GB Latency (s)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;35&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;M2 MacBook Air 8GB Latency (s)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#important-notes-on-performance-benchmarks&#34;&gt;Important Notes on Performance Benchmarks&lt;/a&gt; section for details.&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;converting-models-to-coreml&#34;&gt;&lt;/a&gt; Converting Models to Core ML&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Create a Python environment and install dependencies:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n coreml_stable_diffusion python=3.8 -y&#xA;conda activate coreml_stable_diffusion&#xA;cd /path/to/cloned/ml-stable-diffusion/repository&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Log in to or register for your &lt;a href=&#34;https://huggingface.co&#34;&gt;Hugging Face account&lt;/a&gt;, generate a &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;User Access Token&lt;/a&gt; and use this token to set up Hugging Face API access by running &lt;code&gt;huggingface-cli login&lt;/code&gt; in a Terminal window.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Navigate to the version of Stable Diffusion that you would like to use on &lt;a href=&#34;https://huggingface.co/models?search=stable-diffusion&#34;&gt;Hugging Face Hub&lt;/a&gt; and accept its Terms of Use. The default model version is &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;CompVis/stable-diffusion-v1-4&lt;/a&gt;. The model version may be changed by the user as described in the next step.&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Execute the following command from the Terminal to generate Core ML model files (&lt;code&gt;.mlpackage&lt;/code&gt;)&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --convert-text-encoder --convert-vae-decoder --convert-safety-checker -o &amp;lt;output-mlpackages-directory&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;strong&gt;WARNING:&lt;/strong&gt; This command will download several GB worth of PyTorch checkpoints from Hugging Face.&lt;/p&gt; &#xA; &lt;p&gt;This generally takes 15-20 minutes on an M1 MacBook Pro. Upon successful execution, the 4 neural network models that comprise Stable Diffusion will have been converted from PyTorch to Core ML (&lt;code&gt;.mlpackage&lt;/code&gt;) and saved into the specified &lt;code&gt;&amp;lt;output-mlpackages-directory&amp;gt;&lt;/code&gt;. Some additional notable arguments:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--model-version&lt;/code&gt;: The model version defaults to &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;CompVis/stable-diffusion-v1-4&lt;/a&gt;. Developers may specify other versions that are available on &lt;a href=&#34;https://huggingface.co/models?search=stable-diffusion&#34;&gt;Hugging Face Hub&lt;/a&gt;, e.g. &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-2-base&#34;&gt;stabilityai/stable-diffusion-2-base&lt;/a&gt; &amp;amp; &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--bundle-resources-for-swift-cli&lt;/code&gt;: Compiles all 4 models and bundles them along with necessary resources for text tokenization into &lt;code&gt;&amp;lt;output-mlpackages-directory&amp;gt;/Resources&lt;/code&gt; which should provided as input to the Swift package. This flag is not necessary for the diffusers-based Python pipeline.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--chunk-unet&lt;/code&gt;: Splits the Unet model in two approximately equal chunks (each with less than 1GB of weights) for mobile-friendly deployment. This is &lt;strong&gt;required&lt;/strong&gt; for ANE deployment on iOS and iPadOS. This is not required for macOS. Swift CLI is able to consume both the chunked and regular versions of the Unet model but prioritizes the former. Note that chunked unet is not compatible with the Python pipeline because Python pipeline is intended for macOS only. Chunking is for on-device deployment with Swift only.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--attention-implementation&lt;/code&gt;: Defaults to &lt;code&gt;SPLIT_EINSUM&lt;/code&gt; which is the implementation described in &lt;a href=&#34;https://machinelearning.apple.com/research/neural-engine-transformers&#34;&gt;Deploying Transformers on the Apple Neural Engine&lt;/a&gt;. &lt;code&gt;--attention-implementation ORIGINAL&lt;/code&gt; will switch to an alternative that should be used for non-ANE deployment. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#performance-benchmark&#34;&gt;Performance Benchmark&lt;/a&gt; section for further guidance.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;code&gt;--check-output-correctness&lt;/code&gt;: Compares original PyTorch model&#39;s outputs to final Core ML model&#39;s outputs. This flag increases RAM consumption significantly so it is recommended only for debugging purposes.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;image-generation-with-python&#34;&gt;&lt;/a&gt; Image Generation with Python&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;Run text-to-image generation using the example Python pipeline based on &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt;:&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m python_coreml_stable_diffusion.pipeline --prompt &#34;a photo of an astronaut riding a horse on mars&#34; -i &amp;lt;output-mlpackages-directory&amp;gt; -o &amp;lt;/path/to/output/image&amp;gt; --compute-unit ALL --seed 93&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Please refer to the help menu for all available arguments: &lt;code&gt;python -m python_coreml_stable_diffusion.pipeline -h&lt;/code&gt;. Some notable arguments:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;-i&lt;/code&gt;: Should point to the &lt;code&gt;-o&lt;/code&gt; directory from Step 4 of &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#converting-models-to-coreml&#34;&gt;Converting Models to Core ML&lt;/a&gt; section from above.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--model-version&lt;/code&gt;: If you overrode the default model version while converting models to Core ML, you will need to specify the same model version here.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--compute-unit&lt;/code&gt;: Note that the most performant compute unit for this particular implementation may differ across different hardware. &lt;code&gt;CPU_AND_GPU&lt;/code&gt; or &lt;code&gt;CPU_AND_NE&lt;/code&gt; may be faster than &lt;code&gt;ALL&lt;/code&gt;. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#performance-benchmark&#34;&gt;Performance Benchmark&lt;/a&gt; section for further guidance.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--scheduler&lt;/code&gt;: If you would like to experiment with different schedulers, you may specify it here. For available options, please see the help menu. You may also specify a custom number of inference steps by &lt;code&gt;--num-inference-steps&lt;/code&gt; which defaults to 50.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Image Generation with Swift&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;h3&gt;&lt;a name=&#34;swift-requirements&#34;&gt;&lt;/a&gt; System Requirements&lt;/h3&gt; &#xA; &lt;p&gt;Building the Swift projects require:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;macOS 13 or newer&lt;/li&gt; &#xA;  &lt;li&gt;Xcode 14.1 or newer with command line tools installed. Please check &lt;a href=&#34;https://developer.apple.com/download/all/?q=xcode&#34;&gt;developer.apple.com&lt;/a&gt; for the latest version.&lt;/li&gt; &#xA;  &lt;li&gt;Core ML models and tokenization resources. Please see &lt;code&gt;--bundle-resources-for-swift-cli&lt;/code&gt; from the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#converting-models-to-coreml&#34;&gt;Converting Models to Core ML&lt;/a&gt; section above&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;If deploying this model to:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;iPhone &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;iOS 16.2 or newer&lt;/li&gt; &#xA;    &lt;li&gt;iPhone 12 or newer&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;iPad &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;iPadOS 16.2 or newer&lt;/li&gt; &#xA;    &lt;li&gt;M1 or newer&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Mac &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;macOS 13.1 or newer&lt;/li&gt; &#xA;    &lt;li&gt;M1 or newer&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h3&gt;Example CLI Usage&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;swift run StableDiffusionSample &#34;a photo of an astronaut riding a horse on mars&#34; --resource-path &amp;lt;output-mlpackages-directory&amp;gt;/Resources/ --seed 93 --output-path &amp;lt;/path/to/output/image&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;The output will be named based on the prompt and random seed: e.g. &lt;code&gt;&amp;lt;/path/to/output/image&amp;gt;/a_photo_of_an_astronaut_riding_a_horse_on_mars.93.final.png&lt;/code&gt;&lt;/p&gt; &#xA; &lt;p&gt;Please use the &lt;code&gt;--help&lt;/code&gt; flag to learn about batched generation and more.&lt;/p&gt; &#xA; &lt;h3&gt;Example Library Usage&lt;/h3&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;import StableDiffusion&#xA;...&#xA;let pipeline = try StableDiffusionPipeline(resourcesAt: resourceURL)&#xA;let image = try pipeline.generateImages(prompt: prompt, seed: seed).first&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;h3&gt;Swift Package Details&lt;/h3&gt; &#xA; &lt;p&gt;This Swift package contains two products:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;StableDiffusion&lt;/code&gt; library&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;StableDiffusionSample&lt;/code&gt; command-line tool&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Both of these products require the Core ML models and tokenization resources to be supplied. When specifying resources via a directory path that directory must contain the following:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;TextEncoder.mlmodelc&lt;/code&gt; (text embedding model)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;Unet.mlmodelc&lt;/code&gt; or &lt;code&gt;UnetChunk1.mlmodelc&lt;/code&gt; &amp;amp; &lt;code&gt;UnetChunk2.mlmodelc&lt;/code&gt; (denoising autoencoder model)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;VAEDecoder.mlmodelc&lt;/code&gt; (image decoder model)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;vocab.json&lt;/code&gt; (tokenizer vocabulary file)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;merges.text&lt;/code&gt; (merges for byte pair encoding file)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Optionally, it may also include the safety checker model that some versions of Stable Diffusion include:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;SafetyChecker.mlmodelc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;p&gt;Note that the chunked version of Unet is checked for first. Only if it is not present will the full &lt;code&gt;Unet.mlmodelc&lt;/code&gt; be loaded. Chunking is required for iOS and iPadOS and not necessary for macOS.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;performance-benchmark&#34;&gt;&lt;/a&gt; Performance Benchmark&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;Standard &lt;a href=&#34;https://huggingface.co/CompVis/stable-diffusion-v1-4&#34;&gt;CompVis/stable-diffusion-v1-4&lt;/a&gt; Benchmark&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Device&lt;/th&gt; &#xA;    &lt;th&gt;&lt;code&gt;--compute-unit&lt;/code&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;code&gt;--attention-implementation&lt;/code&gt;&lt;/th&gt; &#xA;    &lt;th&gt;Latency (seconds)&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mac Studio (M1 Ultra, 64-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_GPU&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ORIGINAL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;9&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mac Studio (M1 Ultra, 48-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_GPU&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ORIGINAL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;13&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M1 Max, 32-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_GPU&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ORIGINAL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;18&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M1 Max, 24-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_GPU&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ORIGINAL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M1 Pro, 16-core GPU)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;ALL&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SPLIT_EINSUM (default)&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;26&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M2)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_NE&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SPLIT_EINSUM (default)&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;23&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MacBook Pro (M1)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_NE&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SPLIT_EINSUM (default)&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;35&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;iPad Pro (5th gen, M1)&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;CPU_AND_NE&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;code&gt;SPLIT_EINSUM (default)&lt;/code&gt;&lt;/td&gt; &#xA;    &lt;td&gt;38&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#important-notes-on-performance-benchmarks&#34;&gt;Important Notes on Performance Benchmarks&lt;/a&gt; section for details.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;important-notes-on-performance-benchmarks&#34;&gt;&lt;/a&gt; Important Notes on Performance Benchmarks&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;This benchmark was conducted by Apple using public beta versions of iOS 16.2, iPadOS 16.2 and macOS 13.1 in November 2022.&lt;/li&gt; &#xA;  &lt;li&gt;The executed program is &lt;code&gt;python_coreml_stable_diffusion.pipeline&lt;/code&gt; for macOS devices and a minimal Swift test app built on the &lt;code&gt;StableDiffusion&lt;/code&gt; Swift package for iOS and iPadOS devices.&lt;/li&gt; &#xA;  &lt;li&gt;The median value across 3 end-to-end executions is reported.&lt;/li&gt; &#xA;  &lt;li&gt;Performance may materially differ across different versions of Stable Diffusion due to architecture changes in the model itself. Each reported number is specific to the model version mentioned in that context.&lt;/li&gt; &#xA;  &lt;li&gt;The image generation procedure follows the standard configuration: 50 inference steps, 512x512 output image resolution, 77 text token sequence length, classifier-free guidance (batch size of 2 for unet).&lt;/li&gt; &#xA;  &lt;li&gt;The actual prompt length does not impact performance because the Core ML model is converted with a static shape that computes the forward pass for all of the 77 elements (&lt;code&gt;tokenizer.model_max_length&lt;/code&gt;) in the text token sequence regardless of the actual length of the input text.&lt;/li&gt; &#xA;  &lt;li&gt;Pipelining across the 4 models is not optimized and these performance numbers are subject to variance under increased system load from other applications. Given these factors, we do not report sub-second variance in latency.&lt;/li&gt; &#xA;  &lt;li&gt;Weights and activations are in float16 precision for both the GPU and the ANE.&lt;/li&gt; &#xA;  &lt;li&gt;The Swift CLI program consumes a peak memory of approximately 2.6GB (without the safety checker), 2.1GB of which is model weights in float16 precision. We applied &lt;a href=&#34;https://coremltools.readme.io/docs/compressing-ml-program-weights#use-affine-quantization&#34;&gt;8-bit weight quantization&lt;/a&gt; to reduce peak memory consumption by approximately 1GB. However, we observed that it had an adverse effect on generated image quality and we rolled it back. We encourage developers to experiment with other advanced weight compression techniques such as &lt;a href=&#34;https://coremltools.readme.io/docs/compressing-ml-program-weights#use-a-lookup-table&#34;&gt;palettization&lt;/a&gt; and/or &lt;a href=&#34;https://coremltools.readme.io/docs/compressing-ml-program-weights#use-sparse-representation&#34;&gt;pruning&lt;/a&gt; which may yield better results.&lt;/li&gt; &#xA;  &lt;li&gt;In the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/performance-benchmark&#34;&gt;benchmark table&lt;/a&gt;, we report the best performing &lt;code&gt;--compute-unit&lt;/code&gt; and &lt;code&gt;--attention-implementation&lt;/code&gt; values per device. The former does not modify the Core ML model and can be applied during runtime. The latter modifies the Core ML model. Note that the best performing compute unit is model version and hardware-specific.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;results-with-different-compute-units&#34;&gt;&lt;/a&gt; Results with Different Compute Units&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;p&gt;It is highly probable that there will be slight differences across generated images using different compute units.&lt;/p&gt; &#xA; &lt;p&gt;The following images were generated on an M1 MacBook Pro and macOS 13.1 with the prompt &lt;em&gt;&#34;a photo of an astronaut riding a horse on mars&#34;&lt;/em&gt; using the &lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt; model version. The random seed was set to 93:&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;CPU_AND_NE&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;CPU_AND_GPU&lt;/th&gt; &#xA;    &lt;th align=&#34;center&#34;&gt;ALL&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_CPU_AND_NE_modelVersion_runwayml_stable-diffusion-v1-5.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_CPU_AND_GPU_modelVersion_runwayml_stable-diffusion-v1-5.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;    &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/assets/a_high_quality_photo_of_an_astronaut_riding_a_horse_in_space/randomSeed_93_computeUnit_ALL_modelVersion_runwayml_stable-diffusion-v1-5.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Differences may be less or more pronounced for different inputs. Please see the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#faq&#34;&gt;FAQ&lt;/a&gt; Q8 for a detailed explanation.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; Click to expand &lt;/summary&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q1: &lt;/b&gt; &lt;code&gt; ERROR: Failed building wheel for tokenizers or error: can&#39;t find Rust compiler &lt;/code&gt; &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A1: &lt;/b&gt; Please review this &lt;a href=&#34;https://github.com/huggingface/transformers/issues/2831#issuecomment-592724471&#34;&gt;potential solution&lt;/a&gt;.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q2: &lt;/b&gt; &lt;code&gt; RuntimeError: {NSLocalizedDescription = &#34;Error computing NN outputs.&#34; &lt;/code&gt; &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A2: &lt;/b&gt; There are many potential causes for this error. In this context, it is highly likely to be encountered when your system is under increased memory pressure from other applications. Reducing memory utilization of other applications is likely to help alleviate the issue.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q3: &lt;/b&gt; My Mac has 8GB RAM and I am converting models to Core ML using the example command. The process is geting killed because of memory issues. How do I fix this issue? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A3: &lt;/b&gt; In order to minimize the memory impact of the model conversion process, please execute the following command instead:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m python_coreml_stable_diffusion.torch2coreml --convert-vae-decoder -o &amp;lt;output-mlpackages-directory&amp;gt; &amp;amp;&amp;amp; \&#xA;python -m python_coreml_stable_diffusion.torch2coreml --convert-unet -o &amp;lt;output-mlpackages-directory&amp;gt; &amp;amp;&amp;amp; \&#xA;python -m python_coreml_stable_diffusion.torch2coreml --convert-text-encoder -o &amp;lt;output-mlpackages-directory&amp;gt; &amp;amp;&amp;amp; \&#xA;python -m python_coreml_stable_diffusion.torch2coreml --convert-safety-checker -o &amp;lt;output-mlpackages-directory&amp;gt; &amp;amp;&amp;amp;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;p&gt;If you need &lt;code&gt;--chunk-unet&lt;/code&gt;, you may do so in yet another independent command which will reuse the previously exported Unet model and simply chunk it in place:&lt;/p&gt; &#xA;  &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m python_coreml_stable_diffusion.torch2coreml --convert-unet --chunk-unet -o &amp;lt;output-mlpackages-directory&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q4: &lt;/b&gt; My Mac has 8GB RAM, should image generation work on my machine? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A4: &lt;/b&gt; Yes! Especially the &lt;code&gt;--compute-unit CPU_AND_NE&lt;/code&gt; option should work under reasonable system load from other applications. Note that part of the &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#example-results&#34;&gt;Example Results&lt;/a&gt; were generated using an M2 MacBook Air with 8GB RAM.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q5: &lt;/b&gt; Every time I generate an image using the Python pipeline, loading all the Core ML models takes 2-3 minutes. Is this expected? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A5: &lt;/b&gt; Yes and using the Swift library reduces this to just a few seconds. The reason is that &lt;code&gt;coremltools&lt;/code&gt; loads Core ML models (&lt;code&gt;.mlpackage&lt;/code&gt;) and each model is compiled to be run on the requested compute unit during load time. Because of the size and number of operations of the unet model, it takes around 2-3 minutes to compile it for Neural Engine execution. Other models should take at most a few seconds. Note that &lt;code&gt;coremltools&lt;/code&gt; does not cache the compiled model for later loads so each load takes equally long. In order to benefit from compilation caching, &lt;code&gt;StableDiffusion&lt;/code&gt; Swift package by default relies on compiled Core ML models (&lt;code&gt;.mlmodelc&lt;/code&gt;) which will be compiled down for the requested compute unit upon first load but then the cache will be reused on subsequent loads until it is purged due to lack of use.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q6: &lt;/b&gt; I want to deploy &lt;code&gt;StableDiffusion&lt;/code&gt;, the Swift package, in my mobile app. What should I be aware of?&#34; &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A6: &lt;/b&gt; &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#swift-requirements&#34;&gt;This section&lt;/a&gt; describes the minimum SDK and OS versions as well as the device models supported by this package. In addition to these requirements, for best practice, we recommend testing the package on the device with the least amount of RAM available among your deployment targets. This is due to the fact that &lt;code&gt;StableDiffusion&lt;/code&gt; consumes approximately 2.6GB of peak memory during runtime while using &lt;code&gt;.cpuAndNeuralEngine&lt;/code&gt; (the Swift equivalent of &lt;code&gt;coremltools.ComputeUnit.CPU_AND_NE&lt;/code&gt;). Other compute units may have a higher peak memory consumption so &lt;code&gt;.cpuAndNeuralEngine&lt;/code&gt; is recommended for iOS and iPadOS deployment (Please refer to this &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#swift-requirements&#34;&gt;section&lt;/a&gt; for minimum device model requirements). If your app crashes during image generation, please try adding the &lt;a href=&#34;https://developer.apple.com/documentation/bundleresources/entitlements/com_apple_developer_kernel_increased-memory-limit&#34;&gt;Increased Memory Limit&lt;/a&gt; capability to your Xcode project which should significantly increase your app&#39;s memory limit.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q7: &lt;/b&gt; How do I generate images with different resolutions using the same Core ML models? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A7: &lt;/b&gt; The current version of &lt;code&gt;python_coreml_stable_diffusion&lt;/code&gt; does not support single-model multi-resolution out of the box. However, developers may fork this project and leverage the &lt;a href=&#34;https://coremltools.readme.io/docs/flexible-inputs&#34;&gt;flexible shapes&lt;/a&gt; support from coremltools to extend the &lt;code&gt;torch2coreml&lt;/code&gt; script by using &lt;code&gt;coremltools.EnumeratedShapes&lt;/code&gt;. Note that, while the &lt;code&gt;text_encoder&lt;/code&gt; is agnostic to the image resolution, the inputs and outputs of &lt;code&gt;vae_decoder&lt;/code&gt; and &lt;code&gt;unet&lt;/code&gt; models are dependent on the desired image resolution.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q8: &lt;/b&gt; Are the Core ML and PyTorch generated images going to be identical? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A8: &lt;/b&gt; If desired, the generated images across PyTorch and Core ML can be made approximately identical. However, it is not guaranteed by default. There are several factors that might lead to different images across PyTorch and Core ML:&lt;/p&gt; &#xA;  &lt;p&gt;&lt;b&gt; 1. Random Number Generator Behavior &lt;/b&gt;&lt;/p&gt; &#xA;  &lt;p&gt;The main source of potentially different results across PyTorch and Core ML is the Random Number Generator (&lt;a href=&#34;https://en.wikipedia.org/wiki/Random_number_generation&#34;&gt;RNG&lt;/a&gt;) behavior. PyTorch and Numpy have different sources of randomness. &lt;code&gt;python_coreml_stable_diffusion&lt;/code&gt; generally relies on Numpy for RNG (e.g. latents initialization) and &lt;code&gt;StableDiffusion&lt;/code&gt; Swift Library reproduces this RNG behavior. However, PyTorch-based pipelines such as Hugging Face &lt;code&gt;diffusers&lt;/code&gt; relies on PyTorch&#39;s RNG behavior.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;b&gt; 2. PyTorch &lt;/b&gt;&lt;/p&gt; &#xA;  &lt;p&gt;&lt;em&gt;&#34;Completely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.&#34;&lt;/em&gt; (&lt;a href=&#34;https://pytorch.org/docs/stable/notes/randomness.html#reproducibility&#34;&gt;source&lt;/a&gt;).&lt;/p&gt; &#xA;  &lt;p&gt;&lt;b&gt; 3. Model Function Drift During Conversion &lt;/b&gt;&lt;/p&gt; &#xA;  &lt;p&gt;The difference in outputs across corresponding PyTorch and Core ML models is a potential cause. The signal integrity is tested during the conversion process (enabled via &lt;code&gt;--check-output-correctness&lt;/code&gt; argument to &lt;code&gt;python_coreml_stable_diffusion.torch2coreml&lt;/code&gt;) and it is verified to be above a minimum &lt;a href=&#34;https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio&#34;&gt;PSNR&lt;/a&gt; value as tested on random inputs. Note that this is simply a sanity check and does not guarantee this minimum PSNR across all possible inputs. Furthermore, the results are not guaranteed to be identical when executing the same Core ML models across different compute units. This is not expected to be a major source of difference as the sample visual results indicate in &lt;a href=&#34;https://raw.githubusercontent.com/apple/ml-stable-diffusion/main/#results-with-different-compute-units&#34;&gt;this section&lt;/a&gt;.&lt;/p&gt; &#xA;  &lt;p&gt;&lt;b&gt; 4. Weights and Activations Data Type &lt;/b&gt;&lt;/p&gt; &#xA;  &lt;p&gt;When quantizing models from float32 to lower-precision data types such as float16, the generated images are &lt;a href=&#34;https://lambdalabs.com/blog/inference-benchmark-stable-diffusion&#34;&gt;known to vary slightly&lt;/a&gt; in semantics even when using the same PyTorch model. Core ML models generated by coremltools have float16 weights and activations by default &lt;a href=&#34;https://github.com/apple/coremltools/raw/main/coremltools/converters/_converters_entry.py#L256&#34;&gt;unless explicitly overriden&lt;/a&gt;. This is not expected to be a major source of difference.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA; &lt;details&gt; &#xA;  &lt;summary&gt; &lt;b&gt; Q9: &lt;/b&gt; The model files are very large, how do I avoid a large binary for my App? &lt;/summary&gt; &#xA;  &lt;p&gt;&lt;b&gt; A9: &lt;/b&gt; The recommended option is to prompt the user to download these assets upon first launch of the app. This keeps the app binary size independent of the Core ML models being deployed. Disclosing the size of the download to the user is extremely important as there could be data charges or storage impact that the user might not be comfortable with.&lt;/p&gt; &#xA; &lt;/details&gt; &#xA;&lt;/details&gt;</summary>
  </entry>
  <entry>
    <title>AleoHQ/snarkOS</title>
    <updated>2022-12-04T01:30:15Z</updated>
    <id>tag:github.com,2022-12-04:/AleoHQ/snarkOS</id>
    <link href="https://github.com/AleoHQ/snarkOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A Decentralized Operating System for ZK Applications&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;snarkOS&#34; width=&#34;1412&#34; src=&#34;https://cdn.aleo.org/snarkos/banner.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://circleci.com/gh/AleoHQ/snarkOS&#34;&gt;&lt;img src=&#34;https://circleci.com/gh/AleoHQ/snarkOS.svg?style=svg&amp;amp;circle-token=6e9ad6d39d95350544f352d34e0e5c62ef54db26&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/AleoHQ/snarkOS&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/AleoHQ/snarkOS/branch/master/graph/badge.svg?token=cck8tS9HpO&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.aleo.org/discord&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/700454073459015690?logo=discord&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;&lt;a name=&#34;TableofContents&#34;&gt;&lt;/a&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#1-overview&#34;&gt;1. Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#2-build-guide&#34;&gt;2. Build Guide&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#21-requirements&#34;&gt;2.1 Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#22-installation&#34;&gt;2.2 Installation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#3-run-an-aleo-node&#34;&gt;3. Run an Aleo Node&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#3a-run-an-aleo-client&#34;&gt;3a. Run an Aleo Client&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#3a-run-an-aleo-prover&#34;&gt;3b. Run an Aleo Prover&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#4-faqs&#34;&gt;4. FAQs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#5-configuration-file&#34;&gt;5. Command Line Interface&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#6-development-guide&#34;&gt;6. Development Guide&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#61-quick-start&#34;&gt;6.1 Quick Start&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#61-operations&#34;&gt;6.2 Operations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#7-license&#34;&gt;7. License&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;1. Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;snarkOS&lt;/strong&gt; is a decentralized operating system for zero-knowledge applications. This code forms the backbone the &lt;a href=&#34;https://aleo.org/&#34;&gt;Aleo&lt;/a&gt; network, which verifies transactions and stores the encrypted state applications in a publicly-verifiable manner.&lt;/p&gt; &#xA;&lt;h2&gt;2. Build Guide&lt;/h2&gt; &#xA;&lt;h3&gt;2.1 Requirements&lt;/h3&gt; &#xA;&lt;p&gt;The following are &lt;strong&gt;minimum&lt;/strong&gt; requirements to run an Aleo node:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;CPU&lt;/strong&gt;: 16-cores (32-cores preferred)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt;: 16GB of memory (32GB preferred)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Storage&lt;/strong&gt;: 128GB of disk space&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Network&lt;/strong&gt;: 10 Mbps of upload &lt;strong&gt;and&lt;/strong&gt; download bandwidth&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note to run an Aleo Prover that is &lt;strong&gt;competitive&lt;/strong&gt;, the machine will require more than these requirements.&lt;/p&gt; &#xA;&lt;h3&gt;2.2 Installation&lt;/h3&gt; &#xA;&lt;p&gt;Before beginning, please ensure your machine has &lt;code&gt;Rust v1.65+&lt;/code&gt; installed. Instructions to &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;install Rust can be found here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Start by cloning this Github repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/AleoHQ/snarkOS.git --depth 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, move into the &lt;code&gt;snarkOS&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd snarkOS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;[For Ubuntu users]&lt;/strong&gt; A helper script to install dependencies is available. From the &lt;code&gt;snarkOS&lt;/code&gt; directory, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./build_ubuntu.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Lastly, install &lt;code&gt;snarkOS&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo install --path .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Run an Aleo Node&lt;/h2&gt; &#xA;&lt;h2&gt;3a. Run an Aleo Client&lt;/h2&gt; &#xA;&lt;p&gt;Start by following the instructions in the &lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#2-build-guide&#34;&gt;Build Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Next, to start a client node, from the &lt;code&gt;snarkOS&lt;/code&gt; directory, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./run-client.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3b. Run an Aleo Prover&lt;/h2&gt; &#xA;&lt;p&gt;Start by following the instructions in the &lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#2-build-guide&#34;&gt;Build Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Next, generate an Aleo account address:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;snarkos account new&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will output a new Aleo account in the terminal.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please remember to save the account private key and view key.&lt;/strong&gt; The following is an example output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; Attention - Remember to store this account private key and view key.&#xA;&#xA;  Private Key  APrivateKey1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  &amp;lt;-- Save Me And Use In The Next Step&#xA;     View Key  AViewKey1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  &amp;lt;-- Save Me&#xA;      Address  aleo1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  &amp;lt;-- Save Me&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, to start a proving node, from the &lt;code&gt;snarkOS&lt;/code&gt; directory, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./run-prover.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When prompted, enter your Aleo private key:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Enter the Aleo Prover account private key:&#xA;APrivateKey1xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;4. FAQs&lt;/h2&gt; &#xA;&lt;h3&gt;1. My node is unable to compile.&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure your machine has &lt;code&gt;Rust v1.65+&lt;/code&gt; installed. Instructions to &lt;a href=&#34;https://www.rust-lang.org/tools/install&#34;&gt;install Rust can be found here.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;If large errors appear during compilation, try running &lt;code&gt;cargo clean&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Ensure &lt;code&gt;snarkOS&lt;/code&gt; is started using &lt;code&gt;./run-client.sh&lt;/code&gt; or &lt;code&gt;./run-prover.sh&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;2. My node is unable to connect to peers on the network.&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure ports &lt;code&gt;4133/tcp&lt;/code&gt; and &lt;code&gt;3033/tcp&lt;/code&gt; are open on your router and OS firewall.&lt;/li&gt; &#xA; &lt;li&gt;Ensure &lt;code&gt;snarkOS&lt;/code&gt; is started using &lt;code&gt;./run-client.sh&lt;/code&gt; or &lt;code&gt;./run-prover.sh&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;3. I can&#39;t generate a new address&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Before running the command above (&lt;code&gt;snarkos account new&lt;/code&gt;) try &lt;code&gt;source ~/.bashrc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Also double-check the spelling of &lt;code&gt;snarkos&lt;/code&gt;. Note the directory is &lt;code&gt;/snarkOS&lt;/code&gt;, the command is &lt;code&gt;snarkos&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;5. Command Line Interface&lt;/h2&gt; &#xA;&lt;p&gt;To run a node with custom settings, refer to the full list of options and flags available in the &lt;code&gt;snarkOS&lt;/code&gt; CLI.&lt;/p&gt; &#xA;&lt;p&gt;The full list of CLI flags and options can be viewed with &lt;code&gt;snarkos --help&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;snarkOS &#xA;The Aleo Team &amp;lt;hello@aleo.org&amp;gt;&#xA;&#xA;USAGE:&#xA;    snarkos [OPTIONS] &amp;lt;SUBCOMMAND&amp;gt;&#xA;&#xA;OPTIONS:&#xA;    -h, --help                     Print help information&#xA;    -v, --verbosity &amp;lt;VERBOSITY&amp;gt;    Specify the verbosity [options: 0, 1, 2, 3] [default: 2]&#xA;&#xA;SUBCOMMANDS:&#xA;    account    Commands to manage Aleo accounts&#xA;    clean      Cleans the snarkOS node storage&#xA;    help       Print this message or the help of the given subcommand(s)&#xA;    start      Starts the snarkOS node&#xA;    update     Update snarkOS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following are the options for the &lt;code&gt;snarkos start&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;snarkos-start &#xA;Starts the snarkOS node&#xA;&#xA;USAGE:&#xA;    snarkos start [OPTIONS]&#xA;&#xA;OPTIONS:&#xA;        --beacon &amp;lt;BEACON&amp;gt;          Specify this as a beacon, with the given account private key for this node&#xA;        --client &amp;lt;CLIENT&amp;gt;          Specify this as a client, with an optional account private key for this node&#xA;        --connect &amp;lt;CONNECT&amp;gt;        Specify the IP address and port of a peer to connect to [default: ]&#xA;        --dev &amp;lt;DEV&amp;gt;                Enables development mode, specify a unique ID for this node&#xA;    -h, --help                     Print help information&#xA;        --logfile &amp;lt;LOGFILE&amp;gt;        Specify the path to the file where logs will be stored [default: /tmp/snarkos.log]&#xA;        --network &amp;lt;NETWORK&amp;gt;        Specify the network of this node [default: 3]&#xA;        --node &amp;lt;NODE&amp;gt;              Specify the IP address and port for the node server [default: 0.0.0.0:4133]&#xA;        --nodisplay                If the flag is set, the node will not render the display&#xA;        --norest                   If the flag is set, the node will not initialize the REST server&#xA;        --prover &amp;lt;PROVER&amp;gt;          Specify this as a prover, with the given account private key for this node&#xA;        --rest &amp;lt;REST&amp;gt;              Specify the IP address and port for the REST server [default: 0.0.0.0:3033]&#xA;        --validator &amp;lt;VALIDATOR&amp;gt;    Specify this as a validator, with the given account private key for this node&#xA;        --verbosity &amp;lt;VERBOSITY&amp;gt;    Specify the verbosity of the node [options: 0, 1, 2, 3] [default: 2]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;6. Development&lt;/h2&gt; &#xA;&lt;h3&gt;6.1 Quick Start&lt;/h3&gt; &#xA;&lt;p&gt;In one terminal, start the beacon by running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --release -- start --nodisplay --dev 0 --beacon &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In a second terminal, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --release -- start --nodisplay --dev 1 --prover &#34;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This procedure can be repeated to start more nodes.&lt;/p&gt; &#xA;&lt;h3&gt;6.2 Operations&lt;/h3&gt; &#xA;&lt;p&gt;It is important to initialize the nodes starting from &lt;code&gt;0&lt;/code&gt; and incrementing by &lt;code&gt;1&lt;/code&gt; for each new node.&lt;/p&gt; &#xA;&lt;p&gt;The following is a list of options to initialize a node (replace &lt;code&gt;XX&lt;/code&gt; with a number starting from &lt;code&gt;0&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --release -- start --nodisplay --dev XX --beacon &#34;&#34;&#xA;cargo run --release -- start --nodisplay --dev XX --validator &#34;&#34;&#xA;cargo run --release -- start --nodisplay --dev XX --prover &#34;&#34;&#xA;cargo run --release -- start --nodisplay --dev XX --client &#34;&#34;&#xA;cargo run --release -- start --nodisplay --dev XX&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When no node type is specified, the node will default to &lt;code&gt;--client&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h5&gt;Clean Up&lt;/h5&gt; &#xA;&lt;p&gt;To clean up the node storage, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cargo run --release -- clean --dev XX&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;7. License&lt;/h2&gt; &#xA;&lt;p&gt;We welcome all contributions to &lt;code&gt;snarkOS&lt;/code&gt;. Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/#7-license&#34;&gt;license&lt;/a&gt; for the terms of contributions.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/AleoHQ/snarkOS/testnet3/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-GPLv3-blue.svg?sanitize=true&#34; alt=&#34;License: GPL v3&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>