<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-31T01:28:36Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>QwenLM/Qwen2.5</title>
    <updated>2025-01-31T01:28:36Z</updated>
    <id>tag:github.com,2025-01-31:/QwenLM/Qwen2.5</id>
    <link href="https://github.com/QwenLM/Qwen2.5" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Qwen2.5 is the large language model series developed by Qwen team, Alibaba Cloud.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Qwen2.5&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/logo/qwen2.5_logo.png&#34; width=&#34;400&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; üíú &lt;a href=&#34;https://chat.qwenlm.ai/&#34;&gt;&lt;b&gt;Qwen Chat&lt;/b&gt;&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ó &lt;a href=&#34;https://huggingface.co/Qwen&#34;&gt;Hugging Face&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü§ñ &lt;a href=&#34;https://modelscope.cn/organization/qwen&#34;&gt;ModelScope&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href=&#34;https://arxiv.org/abs/2412.15115&#34;&gt;Paper&lt;/a&gt; &amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp; üìë &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5/&#34;&gt;Blog&lt;/a&gt; &amp;nbsp;&amp;nbsp; ÔΩú &amp;nbsp;&amp;nbsp;üìñ &lt;a href=&#34;https://qwen.readthedocs.io/&#34;&gt;Documentation&lt;/a&gt; &lt;br&gt; üíª &lt;a href=&#34;https://gallery.pai-ml.com/#/preview/deepLearning/nlp/qwen2-5_7b&#34;&gt;PAI-DSW&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üñ•Ô∏è &lt;a href=&#34;https://huggingface.co/spaces/Qwen/Qwen2.5-72B-Instruct&#34;&gt;Demo&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;üí¨ &lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/assets/wechat.png&#34;&gt;WeChat (ÂæÆ‰ø°)&lt;/a&gt;&amp;nbsp;&amp;nbsp; | &amp;nbsp;&amp;nbsp;ü´® &lt;a href=&#34;https://discord.gg/CV4E9rpNSD&#34;&gt;Discord&lt;/a&gt;&amp;nbsp;&amp;nbsp; &lt;/p&gt; &#xA;&lt;p&gt;Visit our Hugging Face or ModelScope organization (click links above), search checkpoints with names starting with &lt;code&gt;Qwen2.5-&lt;/code&gt; or visit the &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e&#34;&gt;Qwen2.5 collection&lt;/a&gt;, and you will find all you need! Enjoy!&lt;/p&gt; &#xA;&lt;p&gt;To learn more about Qwen2.5, feel free to read our documentation [&lt;a href=&#34;https://qwen.readthedocs.io/en/latest/&#34;&gt;EN&lt;/a&gt;|&lt;a href=&#34;https://qwen.readthedocs.io/zh-cn/latest/&#34;&gt;ZH&lt;/a&gt;]. Our documentation consists of the following sections:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Quickstart: the basic usages and demonstrations;&lt;/li&gt; &#xA; &lt;li&gt;Inference: the guidance for the inference with transformers, including batch inference, streaming, etc.;&lt;/li&gt; &#xA; &lt;li&gt;Run Locally: the instructions for running LLM locally on CPU and GPU, with frameworks like &lt;code&gt;llama.cpp&lt;/code&gt; and &lt;code&gt;Ollama&lt;/code&gt;;&lt;/li&gt; &#xA; &lt;li&gt;Deployment: the demonstration of how to deploy Qwen for large-scale inference with frameworks like &lt;code&gt;vLLM&lt;/code&gt;, &lt;code&gt;TGI&lt;/code&gt;, etc.;&lt;/li&gt; &#xA; &lt;li&gt;Quantization: the practice of quantizing LLMs with GPTQ, AWQ, as well as the guidance for how to make high-quality quantized GGUF files;&lt;/li&gt; &#xA; &lt;li&gt;Training: the instructions for post-training, including SFT and RLHF (TODO) with frameworks like Axolotl, LLaMA-Factory, etc.&lt;/li&gt; &#xA; &lt;li&gt;Framework: the usage of Qwen with frameworks for application, e.g., RAG, Agent, etc.&lt;/li&gt; &#xA; &lt;li&gt;Benchmark: the statistics about inference speed and memory footprint (Available for Qwen2.5).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;In the past three months since Qwen2&#39;s release, numerous developers have built new models on the Qwen2 language models, providing us with valuable feedback. During this period, we have focused on creating smarter and more knowledgeable language models. Today, we are excited to introduce the latest addition to the Qwen family: &lt;strong&gt;Qwen2.5&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dense, easy-to-use, decoder-only language models, available in &lt;strong&gt;0.5B&lt;/strong&gt;, &lt;strong&gt;1.5B&lt;/strong&gt;, &lt;strong&gt;3B&lt;/strong&gt;, &lt;strong&gt;7B&lt;/strong&gt;, &lt;strong&gt;14B&lt;/strong&gt;, &lt;strong&gt;32B&lt;/strong&gt;, and &lt;strong&gt;72B&lt;/strong&gt; sizes, and base and instruct variants.&lt;/li&gt; &#xA; &lt;li&gt;Pretrained on our latest large-scale dataset, encompassing up to &lt;strong&gt;18T&lt;/strong&gt; tokens.&lt;/li&gt; &#xA; &lt;li&gt;Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.&lt;/li&gt; &#xA; &lt;li&gt;More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.&lt;/li&gt; &#xA; &lt;li&gt;Context length support up to &lt;strong&gt;128K&lt;/strong&gt; tokens and can generate up to &lt;strong&gt;8K&lt;/strong&gt; tokens.&lt;/li&gt; &#xA; &lt;li&gt;Multilingual support for over &lt;strong&gt;29&lt;/strong&gt; languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024.09.19: We released the Qwen2.5 series. This time there are 3 extra model sizes: 3B, 14B, and 32B for more possibilities. Check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5&#34;&gt;blog&lt;/a&gt; for more!&lt;/li&gt; &#xA; &lt;li&gt;2024.06.06: We released the Qwen2 series. Check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2/&#34;&gt;blog&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;2024.03.28: We released the first MoE model of Qwen: Qwen1.5-MoE-A2.7B! Temporarily, only HF transformers and vLLM support the model. We will soon add the support of llama.cpp, mlx-lm, etc. Check our &lt;a href=&#34;https://qwenlm.github.io/blog/qwen-moe/&#34;&gt;blog&lt;/a&gt; for more information!&lt;/li&gt; &#xA; &lt;li&gt;2024.02.05: We released the Qwen1.5 series.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;Detailed evaluation results are reported in this &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5/&#34;&gt; üìë blog&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For requirements on GPU memory and the respective throughput, see results &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html&#34;&gt;here&lt;/a&gt; .&lt;/p&gt; &#xA;&lt;h2&gt;Quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;ü§ó Hugging Face Transformers&lt;/h3&gt; &#xA;&lt;p&gt;The latest version of &lt;code&gt;transformers&lt;/code&gt; is recommended (at least 4.37.0). Here we show a code snippet to show you how to use the chat model with &lt;code&gt;transformers&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;model_name = &#34;Qwen/Qwen2.5-7B-Instruct&#34;&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    torch_dtype=&#34;auto&#34;,&#xA;    device_map=&#34;auto&#34;&#xA;)&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;&#xA;prompt = &#34;Give me a short introduction to large language model.&#34;&#xA;messages = [&#xA;    {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&#34;},&#xA;    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt}&#xA;]&#xA;text = tokenizer.apply_chat_template(&#xA;    messages,&#xA;    tokenize=False,&#xA;    add_generation_prompt=True&#xA;)&#xA;model_inputs = tokenizer([text], return_tensors=&#34;pt&#34;).to(model.device)&#xA;&#xA;generated_ids = model.generate(&#xA;    **model_inputs,&#xA;    max_new_tokens=512&#xA;)&#xA;generated_ids = [&#xA;    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)&#xA;]&#xA;&#xA;response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For quantized models, we advise you to use the GPTQ and AWQ correspondents, namely &lt;code&gt;Qwen2.5-7B-Instruct-GPTQ-Int8&lt;/code&gt; and &lt;code&gt;Qwen2.5-7B-Instruct-AWQ&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;ü§ñ ModelScope&lt;/h3&gt; &#xA;&lt;p&gt;We strongly advise users especially those in mainland China to use ModelScope. &lt;code&gt;snapshot_download&lt;/code&gt; can help you solve issues concerning downloading checkpoints.&lt;/p&gt; &#xA;&lt;h3&gt;üíª Run locally&lt;/h3&gt; &#xA;&lt;h4&gt;Ollama&lt;/h4&gt; &#xA;&lt;p&gt;After &lt;a href=&#34;https://github.com/ollama/ollama/raw/main/README.md&#34;&gt;installing ollama&lt;/a&gt;, you can initiate the ollama service with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama serve&#xA;# You need to keep this service running whenever you are using ollama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To pull a model checkpoint and run the model, use the &lt;code&gt;ollama run&lt;/code&gt; command. You can specify a model size by adding a suffix to &lt;code&gt;qwen2.5&lt;/code&gt;, such as &lt;code&gt;:0.5b&lt;/code&gt;, &lt;code&gt;:1.5b&lt;/code&gt;, &lt;code&gt;:7b&lt;/code&gt;, or &lt;code&gt;:72b&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;ollama run qwen2.5:7b&#xA;# To exit, type &#34;/bye&#34; and press ENTER&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also access the ollama service via its OpenAI-compatible API. Please note that you need to (1) keep &lt;code&gt;ollama serve&lt;/code&gt; running while using the API, and (2) execute &lt;code&gt;ollama run qwen2.5:7b&lt;/code&gt; before utilizing this API to ensure that the model checkpoint is prepared.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from openai import OpenAI&#xA;client = OpenAI(&#xA;    base_url=&#39;http://localhost:11434/v1/&#39;,&#xA;    api_key=&#39;ollama&#39;,  # required but ignored&#xA;)&#xA;chat_completion = client.chat.completions.create(&#xA;    messages=[&#xA;        {&#xA;            &#39;role&#39;: &#39;user&#39;,&#xA;            &#39;content&#39;: &#39;Say this is a test&#39;,&#xA;        }&#xA;    ],&#xA;    model=&#39;qwen2.5:7b&#39;,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional details, please visit &lt;a href=&#34;https://ollama.ai/&#34;&gt;ollama.ai&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;llama.cpp&lt;/h4&gt; &#xA;&lt;p&gt;Download our provided GGUF files or create them by yourself, and you can directly use them with the latest &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;&lt;code&gt;llama.cpp&lt;/code&gt;&lt;/a&gt; with a one-line command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;./llama-cli -m &amp;lt;path-to-file&amp;gt; -n 512 -co -sp -cnv -p &#34;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For additional guides, please refer to &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html&#34;&gt;our documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;MLX-LM&lt;/h4&gt; &#xA;&lt;p&gt;If you are running on Apple Silicon, we have also provided checkpoints compatible with &lt;a href=&#34;https://github.com/ml-explore/mlx-examples/raw/main/llms/README.md&#34;&gt;&lt;code&gt;mlx-lm&lt;/code&gt;&lt;/a&gt;. Look for models ending with MLX on HuggingFace Hub, like &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-7B-Instruct-MLX&#34;&gt;Qwen2.5-7B-Instruct-MLX&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;LMStudio&lt;/h4&gt; &#xA;&lt;p&gt;Qwen2.5 has already been supported by &lt;a href=&#34;https://lmstudio.ai/&#34;&gt;lmstudio.ai&lt;/a&gt;. You can directly use LMStudio with our GGUF files.&lt;/p&gt; &#xA;&lt;h4&gt;OpenVINO&lt;/h4&gt; &#xA;&lt;p&gt;Qwen2.5 has already been supported by &lt;a href=&#34;https://github.com/openvinotoolkit&#34;&gt;OpenVINO toolkit&lt;/a&gt;. You can install and run this &lt;a href=&#34;https://github.com/OpenVINO-dev-contest/Qwen2.openvino&#34;&gt;chatbot example&lt;/a&gt; with Intel CPU, integrated GPU or discrete GPU.&lt;/p&gt; &#xA;&lt;h2&gt;Web UI&lt;/h2&gt; &#xA;&lt;h4&gt;Text generation web UI&lt;/h4&gt; &#xA;&lt;p&gt;You can directly use &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;&lt;code&gt;text-generation-webui&lt;/code&gt;&lt;/a&gt; for creating a web UI demo. If you use GGUF, remember to install the latest wheel of &lt;code&gt;llama.cpp&lt;/code&gt; with the support of Qwen2.5.&lt;/p&gt; &#xA;&lt;h4&gt;llamafile&lt;/h4&gt; &#xA;&lt;p&gt;Clone &lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile&#34;&gt;&lt;code&gt;llamafile&lt;/code&gt;&lt;/a&gt;, run source install, and then create your own llamafile with the GGUF file following the guide &lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#creating-llamafiles&#34;&gt;here&lt;/a&gt;. You are able to run one line of command, say &lt;code&gt;./qwen.llamafile&lt;/code&gt;, to create a demo.&lt;/p&gt; &#xA;&lt;h2&gt;Deployment&lt;/h2&gt; &#xA;&lt;p&gt;Qwen2.5 is supported by multiple inference frameworks. Here we demonstrate the usage of &lt;code&gt;vLLM&lt;/code&gt;, &lt;code&gt;SGLang&lt;/code&gt; and &lt;code&gt;OpenLLM&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;vLLM&lt;/h3&gt; &#xA;&lt;p&gt;We advise you to use the latest version of vLLM to build OpenAI-compatible API service, including tool use support. Start the server with a chat model, e.g. &lt;code&gt;Qwen2.5-7B-Instruct&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;vllm serve Qwen/Qwen2.5-7B-Instruct&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then use the chat API as demonstrated below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;curl http://localhost:8000/v1/chat/completions -H &#34;Content-Type: application/json&#34; -d &#39;{&#xA;    &#34;model&#34;: &#34;Qwen/Qwen2.5-7B-Instruct&#34;,&#xA;    &#34;messages&#34;: [&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;}&#xA;    ],&#xA;    &#34;temperature&#34;: 0.7,&#xA;    &#34;top_p&#34;: 0.8,&#xA;    &#34;repetition_penalty&#34;: 1.05,&#xA;    &#34;max_tokens&#34;: 512&#xA;}&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from openai import OpenAI&#xA;# Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server.&#xA;openai_api_key = &#34;EMPTY&#34;&#xA;openai_api_base = &#34;http://localhost:8000/v1&#34;&#xA;&#xA;client = OpenAI(&#xA;    api_key=openai_api_key,&#xA;    base_url=openai_api_base,&#xA;)&#xA;&#xA;chat_response = client.chat.completions.create(&#xA;    model=&#34;Qwen2.5-7B-Instruct&#34;,&#xA;    messages=[&#xA;        {&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&#34;},&#xA;        {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;Tell me something about large language models.&#34;},&#xA;    ],&#xA;    temperature=0.7,&#xA;    top_p=0.8,&#xA;    max_tokens=512,&#xA;    extra_body={&#xA;        &#34;repetition_penalty&#34;: 1.05,&#xA;    },&#xA;)&#xA;print(&#34;Chat response:&#34;, chat_response)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;SGLang&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Warning] The OpenAI-compatible APIs provided by SGLang currently do NOT support &lt;strong&gt;tool use&lt;/strong&gt; or &lt;strong&gt;function calling&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Please install &lt;code&gt;SGLang&lt;/code&gt; from source. Similar to &lt;code&gt;vLLM&lt;/code&gt;, you need to launch a server and use OpenAI-compatible API service. Start the server first:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m sglang.launch_server --model-path Qwen/Qwen2.5-7B-Instruct --port 30000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use it in Python as shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sglang import function, system, user, assistant, gen, set_default_backend, RuntimeEndpoint&#xA;&#xA;@function&#xA;def multi_turn_question(s, question_1, question_2):&#xA;    s += system(&#34;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&#34;)&#xA;    s += user(question_1)&#xA;    s += assistant(gen(&#34;answer_1&#34;, max_tokens=256))&#xA;    s += user(question_2)&#xA;    s += assistant(gen(&#34;answer_2&#34;, max_tokens=256))&#xA;&#xA;set_default_backend(RuntimeEndpoint(&#34;http://localhost:30000&#34;))&#xA;&#xA;state = multi_turn_question.run(&#xA;    question_1=&#34;What is the capital of China?&#34;,&#xA;    question_2=&#34;List two local attractions.&#34;,&#xA;)&#xA;&#xA;for m in state.messages():&#xA;    print(m[&#34;role&#34;], &#34;:&#34;, m[&#34;content&#34;])&#xA;&#xA;print(state[&#34;answer_1&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;OpenLLM&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/bentoml/OpenLLM&#34;&gt;OpenLLM&lt;/a&gt; allows you to easily run&amp;nbsp;Qwen2.5 as OpenAI-compatible APIs. You can start a model server using &lt;code&gt;openllm serve&lt;/code&gt;. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;openllm serve qwen2.5:7b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The server is active at &lt;code&gt;http://localhost:3000/&lt;/code&gt;, providing OpenAI-compatible APIs. You can create an OpenAI client to call its chat API. For more information, refer to &lt;a href=&#34;https://qwen.readthedocs.io/en/latest/deployment/openllm.html&#34;&gt;our documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Tool Use&lt;/h3&gt; &#xA;&lt;p&gt;For tool use capabilities, we recommend taking a look at &lt;a href=&#34;https://github.com/QwenLM/Qwen-Agent&#34;&gt;Qwen-Agent&lt;/a&gt;, which provides a wrapper around these APIs to support tool use or function calling. Tool use with Qwen2.5 can also be conducted with Hugging Face &lt;code&gt;transformers&lt;/code&gt;, Ollama, and vLLM. Follow guides in our documentation to see how to enable the support.&lt;/p&gt; &#xA;&lt;h2&gt;Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;We advise you to use training frameworks, including &lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl&#34;&gt;Axolotl&lt;/a&gt;, &lt;a href=&#34;https://github.com/hiyouga/LLaMA-Factory&#34;&gt;Llama-Factory&lt;/a&gt;, &lt;a href=&#34;https://github.com/unslothai/unsloth&#34;&gt;unsloth&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope/swift&#34;&gt;Swift&lt;/a&gt;, etc., to finetune your models with SFT, DPO, PPO, etc.&lt;/p&gt; &#xA;&lt;h2&gt;License Agreement&lt;/h2&gt; &#xA;&lt;p&gt;All our open-source models, except for the 3B and 72B variants, are licensed under Apache 2.0. You can find the license files in the respective Hugging Face repositories.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our work helpful, feel free to give us a cite.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{qwen2.5,&#xA;    title   = {Qwen2.5 Technical Report}, &#xA;    author  = {An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},&#xA;    journal = {arXiv preprint arXiv:2412.15115},&#xA;    year    = {2024}&#xA;}&#xA;&#xA;@article{qwen2,&#xA;    title   = {Qwen2 Technical Report}, &#xA;    author  = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},&#xA;    journal = {arXiv preprint arXiv:2407.10671},&#xA;    year    = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contact Us&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested to leave a message to either our research team or product team, join our &lt;a href=&#34;https://discord.gg/z3GAxXZ9Ce&#34;&gt;Discord&lt;/a&gt; or &lt;a href=&#34;https://raw.githubusercontent.com/QwenLM/Qwen2.5/main/assets/wechat.png&#34;&gt;WeChat groups&lt;/a&gt;!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>block/goose</title>
    <updated>2025-01-31T01:28:36Z</updated>
    <id>tag:github.com,2025-01-31:/block/goose</id>
    <link href="https://github.com/block/goose" rel="alternate"></link>
    <summary type="html">&lt;p&gt;an open-source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; codename goose &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;strong&gt;an open-source, extensible AI agent that goes beyond code suggestions&lt;br&gt;install, execute, edit, and test with any LLM&lt;/strong&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://discord.gg/7GaTvbDwga&#34;&gt; &lt;img src=&#34;https://img.shields.io/discord/1287729918100246654?logo=discord&amp;amp;logoColor=white&amp;amp;label=Join+Us&amp;amp;color=blueviolet&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/block/goose/actions/workflows/ci.yml&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/block/goose/ci.yml?branch=main&#34; alt=&#34;CI&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://block.github.io/goose&#34;&gt;documentation&lt;/a&gt;, or to try it out head to the &lt;a href=&#34;https://block.github.io/goose/docs/getting-started/installation&#34;&gt;installation&lt;/a&gt; instructions!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>n4ze3m/page-assist</title>
    <updated>2025-01-31T01:28:36Z</updated>
    <id>tag:github.com,2025-01-31:/n4ze3m/page-assist</id>
    <link href="https://github.com/n4ze3m/page-assist" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Use your locally running AI models to assist you in your web browsing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Page Assist&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/bu54382uBd&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-join%20chat-blue.svg?sanitize=true&#34; alt=&#34;Join dialoqbase #welcome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Page Assist is an open-source browser extension that provides a sidebar and web UI for your local AI model. It allows you to interact with your model from any webpage.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Page Assist supports Chromium-based browsers like Chrome, Brave, and Edge, as well as Firefox.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chrome.google.com/webstore/detail/page-assist/jfgfiigpkhlkbnfnbobbkinehhfdhndo&#34;&gt;&lt;img src=&#34;https://pub-35424b4473484be483c0afa08c69e7da.r2.dev/UV4C4ybeBTsZt43U4xis.png&#34; alt=&#34;Chrome Web Store&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://addons.mozilla.org/en-US/firefox/addon/page-assist/&#34;&gt;&lt;img src=&#34;https://pub-35424b4473484be483c0afa08c69e7da.r2.dev/get-the-addon.png&#34; alt=&#34;Firefox Add-on&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Checkout the Demo (v1.0.0):&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=8VTjlLGXA4s&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/8VTjlLGXA4s/0.jpg&#34; alt=&#34;Page Assist Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Sidebar&lt;/strong&gt;: A sidebar that can be opened on any webpage. It allows you to interact with your model and see the results.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Web UI&lt;/strong&gt;: A web UI that allows you to interact with your model like a ChatGPT Website.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chat With Webpage&lt;/strong&gt;: You can chat with the webpage and ask questions about the content.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;want more features? Create an issue and let me know.&lt;/p&gt; &#xA;&lt;h3&gt;Manual Installation&lt;/h3&gt; &#xA;&lt;h4&gt;Pre-requisites&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bun - &lt;a href=&#34;https://bun.sh/&#34;&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ollama (Local AI Provider) - &lt;a href=&#34;https://ollama.com&#34;&gt;Installation Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Any OpenAI API Compatible Endpoint (like LM Studio, llamafile etc.)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/n4ze3m/page-assist.git&#xA;cd page-assist&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install the dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Build the extension (by default it will build for Chrome)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or you can build for Firefox&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun build:firefox&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Load the extension (chrome)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Open the Extension Management page by navigating to &lt;code&gt;chrome://extensions&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enable Developer Mode by clicking the toggle switch next to Developer mode.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Click the &lt;code&gt;Load unpacked&lt;/code&gt; button and select the &lt;code&gt;build&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Load the extension (firefox)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open the Add-ons page by navigating to &lt;code&gt;about:addons&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Extensions&lt;/code&gt; tab.&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Manage Your Extensions&lt;/code&gt; button.&lt;/li&gt; &#xA; &lt;li&gt;Click the &lt;code&gt;Load Temporary Add-on&lt;/code&gt; button and select the &lt;code&gt;manifest.json&lt;/code&gt; file from the &lt;code&gt;build&lt;/code&gt; directory.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Sidebar&lt;/h3&gt; &#xA;&lt;p&gt;Once the extension is installed, you can open the sidebar via context menu or keyboard shortcut.&lt;/p&gt; &#xA;&lt;p&gt;Default Keyboard Shortcut: &lt;code&gt;Ctrl+Shift+Y&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Web UI&lt;/h3&gt; &#xA;&lt;p&gt;You can open the Web UI by clicking on the extension icon which will open a new tab with the Web UI.&lt;/p&gt; &#xA;&lt;p&gt;Default Keyboard Shortcut: &lt;code&gt;Ctrl+Shift+L&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: You can change the keyboard shortcuts from the extension settings on the Chrome Extension Management page.&lt;/p&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;You can run the extension in development mode to make changes and test them.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will start a development server and watch for changes in the source files. You can load the extension in your browser and test the changes.&lt;/p&gt; &#xA;&lt;h2&gt;Browser Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Browser&lt;/th&gt; &#xA;   &lt;th&gt;Sidebar&lt;/th&gt; &#xA;   &lt;th&gt;Chat With Webpage&lt;/th&gt; &#xA;   &lt;th&gt;Web UI&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chrome&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Brave&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Firefox&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Vivaldi&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Edge&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LibreWolf&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Zen Browser&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Opera&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Arc&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚ùå&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Local AI Provider&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;Ollama&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Chrome AI (Gemini Nano)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;OpenAI API Compatible endpoints (like LM Studio, llamafile etc.)&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Firefox Support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; More Local AI Providers&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; More Customization Options&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Better UI/UX&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Privacy&lt;/h2&gt; &#xA;&lt;p&gt;Page Assist does not collect any personal data. The only time the extension communicates with the server is when you are using the share feature, which can be disabled from the settings.&lt;/p&gt; &#xA;&lt;p&gt;All the data is stored locally in the browser storage. You can view the source code and verify it yourself.&lt;/p&gt; &#xA;&lt;p&gt;You learn more about the privacy policy &lt;a href=&#34;https://raw.githubusercontent.com/n4ze3m/page-assist/main/PRIVACY.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome. If you have any feature requests, bug reports, or questions, feel free to create an issue.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you like the project and want to support it, you can buy me a coffee. It will help me to keep working on the project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ko-fi.com/M4M3EMCLL&#34; target=&#34;_blank&#34;&gt;&lt;img height=&#34;36&#34; style=&#34;border:0px;height:36px;&#34; src=&#34;https://storage.ko-fi.com/cdn/kofi2.png?v=3&#34; border=&#34;0&#34; alt=&#34;Buy Me a Coffee at ko-fi.com&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;or you can sponsor me on GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Blogs and Videos About Page Assist&lt;/h2&gt; &#xA;&lt;p&gt;This are some of the blogs and videos about Page Assist. If you have written a blog or made a video about Page Assist, feel free to create a PR and add it here.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://note.com/lucas_san/n/nf00d01a02c3a&#34;&gt;Ollama„ÇíChromeAddon„ÅÆPage Assist„ÅßÁ∞°ÂçòÊìç‰Ωú&lt;/a&gt; by &lt;a href=&#34;https://twitter.com/LucasChatGPT&#34;&gt;LucasChatGPT&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=IvLTlDy9G8c&#34;&gt;This Chrome Extension Surprised Me&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@technovangelist&#34;&gt;Matt Williams&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=61uN5jtj2wo&#34;&gt;Ollama With 1 Click&lt;/a&gt; by &lt;a href=&#34;https://www.youtube.com/@ecomxfactor-YaronBeen&#34;&gt;Yaron Been From EcomXFactor&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt; &#xA;&lt;h2&gt;Last but not least&lt;/h2&gt; &#xA;&lt;p&gt;Made in &lt;a href=&#34;https://en.wikipedia.org/wiki/Alappuzha&#34;&gt;Alappuzha&lt;/a&gt; with ‚ù§Ô∏è&lt;/p&gt;</summary>
  </entry>
</feed>