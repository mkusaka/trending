<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-05T01:23:17Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>igorbrigadir/awesome-twitter-algo</title>
    <updated>2023-04-05T01:23:17Z</updated>
    <id>tag:github.com,2023-04-05:/igorbrigadir/awesome-twitter-algo</id>
    <link href="https://github.com/igorbrigadir/awesome-twitter-algo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The release of the Twitter algorithm, annotated for recsys&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome Twitter Algo &lt;span&gt;üê¶&lt;/span&gt;&lt;/h1&gt; &#xA;&lt;p&gt;Curated by &lt;a href=&#34;https://github.com/igorbrigadir&#34;&gt;Igor Brigadir&lt;/a&gt; and &lt;a href=&#34;https://github.com/veekaybee&#34;&gt;Vicki Boykis&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;An annotated look through the release of the Twitter algorithm, through the context of engineering and recsys, with notes from repo creators on significance of specific parts of the code. Since it can be hard to parse through so much code and derive meaning and context, we do it for you!&lt;/p&gt; &#xA;&lt;p&gt;This code focuses on the services used to build the Home timeline &lt;code&gt;For You&lt;/code&gt; feed, the algorithmic tab that is now served first on both web and mobile next to the &lt;code&gt;Following&lt;/code&gt; feed.&lt;/p&gt; &#xA;&lt;img width=&#34;591&#34; alt=&#34;Screenshot 2023-03-31 at 9 36 04 PM&#34; src=&#34;https://user-images.githubusercontent.com/3837836/229259504-fd08c5f5-a346-4e6a-b7d0-2f5514e02915.png&#34;&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;We&#39;re happy to take changes that add and contextualize Twitter&#39;s recommendations algorithm as it&#39;s been released over the past week. To contribute, please submit a PR with good formatting and grammar and lots of links to references where relevant. We&#39;re especially happy for feedback from tweeps or former tweeps who can tell us where we got it wrong.&lt;/p&gt; &#xA;&lt;h1&gt;High-level Context and Summary&lt;/h1&gt; &#xA;&lt;p&gt;One thing that&#39;s immediately obvious is that this is not the entire codebase or even a working majority of it. Missing from this codebase are&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;many flows that process,enrich, and refine model input data&lt;/li&gt; &#xA; &lt;li&gt;YAML configuration metafiles which could tell us quite a bit about how the code actually works. There are only 7 of them, the rest have been redacted.&lt;/li&gt; &#xA; &lt;li&gt;Most code related to spinning up the actual infrastructure&lt;/li&gt; &#xA; &lt;li&gt;Git commit history that shows us how some of this code has evolved&lt;/li&gt; &#xA; &lt;li&gt;A large portion of the trust and safety codebase, which Twitter &lt;a href=&#34;https://github.com/twitter/the-algorithm/tree/main/trust_and_safety_models#trust-and-safety-models&#34;&gt;has noted they&#39;ve left out for now&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;An important high-level concept discussed in the Spaces releasing this code was in-network and out-of-network. In-network tweets are those from people you follow, out-of-network is everyone else. A blend of 50%/50% are offered in the daily ~1500 tweets run through rankers.&lt;/p&gt; &#xA;&lt;h1&gt;Code Links&lt;/h1&gt; &#xA;&lt;p&gt;What was released? The majority of the code and algorithms, but not the data or parameters or configurations or build tools of the Recommneder Systems behind &#34;For You&#34; timeline recommendations. The Candidate Retrieval code was also not released, and neither was the Trust and Safety components, and the Ads components - those remain closed off. No User Data or credentials were inside the repositories and code comments were sanitized (or at least, none were obviously there on first look).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/twitter/the-algorithm&#34;&gt;Twitter Algo Repo&lt;/a&gt; || &lt;a href=&#34;https://github.com/twitter/the-algorithm-ml&#34;&gt;Twitter ML Algo Repo&lt;/a&gt; || &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm&#34;&gt;Blog Post&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Recsys Architecture Diagram&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3837836/229310537-620628de-7a81-4ced-b34f-94eb3f3e2370.png&#34; alt=&#34;twitter architecture@2x (1)&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://whimsical.com/twitter-archtecture-PoR7TJb1eac2UofLVSY28e&#34;&gt;Link to update here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Twitter Data-Centric Historical Architecture Context&lt;/h1&gt; &#xA;&lt;p&gt;There is a very, very old post from 2013 on &lt;a href=&#34;http://highscalability.com/blog/2013/7/8/the-architecture-twitter-uses-to-deal-with-150m-active-users.html&#34;&gt;High Scalability&lt;/a&gt; which gives some context to how these systems were initially constructed.&lt;/p&gt; &#xA;&lt;p&gt;As context, Twitter initially ran all workloads on-prem but has been &lt;a href=&#34;https://cloud.google.com/blog/products/data-analytics/how-twitter-modernized-its-data-processing-with-google-cloud&#34;&gt;moving to Google Cloud.&lt;/a&gt;. In 2019, Twitter began by migrating to BigQuery and DataFlow from a data and analytics perspective. Before the move to BigQuery, &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/topics/infrastructure/2022/scaling-data-access-by-moving-an-exabyte-of-data-to-google-cloud&#34;&gt;much of the data was stored in HDFS using Thrift&lt;/a&gt;. It currently lives in BigQuery and is processed for many of the pipelines described below using &lt;a href=&#34;https://cloud.google.com/dataflow&#34;&gt;DataFlow&lt;/a&gt;, GCP&#39;s Spark/Scalding-processing equivaent platform.&lt;/p&gt; &#xA;&lt;h1&gt;Programming Languages and Frameworks&lt;/h1&gt; &#xA;&lt;p&gt;The released code comes in a variety of languages. The most common languages used at Twitter are:&lt;/p&gt; &#xA;&lt;h2&gt;Java&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Used in Lucene for search indexing&lt;/li&gt; &#xA; &lt;li&gt;Used in the &lt;a href=&#34;https://github.com/twitter/GraphJet&#34;&gt;GraphJet&lt;/a&gt; library&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Scala&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/twitter/scalding&#34;&gt;Scalding&lt;/a&gt;, written at Twitter, pre-cursor to Spark&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/spotify/scio&#34;&gt;Scio&lt;/a&gt; for parallel cluster computing computations&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Python&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python for the machine learning models in the stack, includes both PyTorch and Tensorflow (legacy) code&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Frameworks and Metalanguages&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Bazel for &lt;a href=&#34;https://bazel.build/&#34;&gt;building Scala and Java services&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Starlark for &lt;a href=&#34;https://github.com/bazelbuild/starlark&#34;&gt;bazel configuration&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://thrift.apache.org/&#34;&gt;Thrift&lt;/a&gt;, a cross-platform framework for RPC calls originally developed at Facebook(Meta)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.twitter.com/engineering/en_us/topics/infrastructure/2023/kerberizing-hadoop-clusters-at-twitter&#34;&gt;Hadoop&lt;/a&gt; - Twitter still runs one of the largest installs of Hadoop out there&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Internal Libraries&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.github.io/finagle/&#34;&gt;Finagle&lt;/a&gt; is a service written in Scala with Java and Scala APIs used to manage RPCs&lt;/li&gt; &#xA; &lt;li&gt;Snowflake is &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/a/2010/announcing-snowflake&#34;&gt;a service that generates unique identifiers&lt;/a&gt; for each tweet based on timestamp, worker number, and sequence number&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Recsys&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/3837836/229260535-27c3bcc6-403b-4d71-b301-f381b0b1be33.png&#34; alt=&#34;recsys&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;The typical recommender system pipeline has four steps: candidate generation, ranking, filtering, and serving. Twitter has many pipelines for performing verious parts of this this across the overall released codebase.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Candidate generation&lt;/strong&gt; occurs when you have millions or billions of potential items in your source data based on user-item interactions. This piece usually includes collaborative filtering or neural algorithms to reduce the size of the candiate dataset for downstream tasks.&lt;/li&gt; &#xA; &lt;li&gt;These need to then be &lt;strong&gt;ranked&lt;/strong&gt; against each other, &lt;strong&gt;filtered&lt;/strong&gt; against business logic and blended and served to the user in a given surface area, in this case the &lt;code&gt;For You&lt;/code&gt; feed in the Twitter timeline.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Input Data&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The system starts with &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm&#34;&gt;500 million tweets posted on a daily basis&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/topics/infrastructure/2021/processing-billions-of-events-in-real-time-at-twitter-&#34;&gt;input data&lt;/a&gt; comes from:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Kafka&lt;/li&gt; &#xA; &lt;li&gt;Twitter Eventbus&lt;/li&gt; &#xA; &lt;li&gt;GCS&lt;/li&gt; &#xA; &lt;li&gt;Vertica&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.twitter.com/engineering/en_us/a/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale&#34;&gt;Manhattan&lt;/a&gt;, a real-time multitenant distributed database that was initially developed as a serving layer on top of Hadoop and includes both observability and other metrics.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;841&#34; alt=&#34;Screenshot 2023-04-01 at 3 26 24 PM&#34; src=&#34;https://user-images.githubusercontent.com/3837836/229310405-a4839079-bb77-427e-bfe8-4cebd1d1a6af.png&#34;&gt; &#xA;&lt;p&gt;In migrating to GCP, the current data ingest looks something like this:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Streaming Dataflow jobs to apply deduping&lt;/li&gt; &#xA; &lt;li&gt;Perform real-time aggregation and sink data into BigTable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;That data is then made available to the candidate generation phase. There is not much about the actual data, even what a schema might look like, in the repo.&lt;/p&gt; &#xA;&lt;h2&gt;Candidate Generators&lt;/h2&gt; &#xA;&lt;p&gt;(also called &#34;features&#34; in the chart)&lt;/p&gt; &#xA;&lt;h3&gt;GraphJet&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/twitter/GraphJet&#34;&gt;GraphJet&lt;/a&gt; - A realtime Java graph processing library that allows for in-memory processing on a single server and focuses on providing content recommendations. &lt;a href=&#34;http://www.vldb.org/pvldb/vol9/p1281-sharma.pdf&#34;&gt;Paper here.&lt;/a&gt; Recommendations are provided based on shared interests, correlated activities, and a number of other input signals. GraphJet maintains a &lt;a href=&#34;https://mathworld.wolfram.com/BipartiteGraph.html&#34;&gt;realtime bipartite interaction graph&lt;/a&gt; that keeps track of user‚Äìtweet interactions over the most recent n hours and reads from Kafka. Each individual GraphJet serever can ingest one million graph edges per second and compute 500 recommendations/second.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;They describe the reasons specifically for creating an in-memory DB in the GraphJet paper:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In terms of recommendation algorithms, we have found that random walks, particularly over bipartite graphs, work well for generating high-engagement recommendations. Although conceptually simple, random-walk algorithms define a large design space that supports customization for a wide range of application scenarios, for recommendations in different contexts (web, mobile, email digests, etc.) as well as entirely unrelated applications (e.g., social search). The output of our random-walk algorithms can serve as input to machine-learned models that further increase the quality of recommendations, but in many cases, the output is sufficiently relevant for direct user consumption.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In terms of production infrastructure for generating graph recommendations, the deployed systems at Twitter have always gone &#34;against the grain&#34; of conventional wisdom. When many in the community were focused on building distributed graph stores, we built a solution (circa 2010) based on retaining the entire graph in memory on a single machine (i.e., no partitioning). This unorthodox design decision enabled Twitter to rapidly develop and deploy a missing feature in the service (see Section 2.1). Later, when there was much activity in the space of graph processing frameworks rushing to replace MapReduce, we abandoned the in-memory system and reimplemented our algorithms in Hadoop MapReduce (circa 2012). Once again, this might seem like another strange design decision (see Section 2.2). Most recently, we have supplemented Hadoop-based recommendations with custom infrastructure, first with a system called MagicRecs (see Section 2.3) and culminating in GraphJet, the focus of this paper.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;The precursor to GraphJet was WTF, Who to Follow, which focused &lt;a href=&#34;https://web.stanford.edu/~rezab/papers/wtf_overview.pdf&#34;&gt;only on recommending users to other users.&lt;/a&gt;, using &lt;a href=&#34;https://github.com/twitter/cassovary&#34;&gt;Cassovary, an in-memory graph processing engine&lt;/a&gt; built specifically for WTF, also built on the JVM.&lt;/p&gt; &#xA;&lt;img width=&#34;447&#34; alt=&#34;Screenshot 2023-04-02 at 12 22 15 PM&#34; src=&#34;https://user-images.githubusercontent.com/3837836/229365667-96855c3a-6238-4138-bdd7-faf396d8397e.png&#34;&gt; &#xA;&lt;p&gt;GraphJet implements two random walk algorithms:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Circle of Trust (internal to Twitter) and&lt;/li&gt; &#xA; &lt;li&gt;SALSA (Stochastic Approach for Link-Structure Analysis).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img width=&#34;423&#34; alt=&#34;Screenshot 2023-04-02 at 12 38 16 PM&#34; src=&#34;https://user-images.githubusercontent.com/3837836/229366503-3f860693-6dfd-4755-90c4-e67c85964700.png&#34;&gt; GraphJet Architecture &#xA;&lt;p&gt;+A large portion of the traffic to GraphJet comes from clients who request content recommendations for a partic- ular user.&lt;/p&gt; &#xA;&lt;p&gt;Graphjet includes &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/ec83d01dcaebf369444d75ed04b3625a0a645eb9/src/scala/com/twitter/recos/graph_common/NodeInfoHandler.scala#L22&#34;&gt;CLICK, FAVORITE, RETWEET, REPLY, AND TWEET as input node types&lt;/a&gt; and keeps track of left (input) and right(output) nodes.&lt;/p&gt; &#xA;&lt;h3&gt;SimClusters&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;TwHIN&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;RealGraph&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;TweepCred&lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Earlybird&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The largest candidate generator is &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/a/2011/the-engineering-behind-twitter-s-new-search-experience&#34;&gt;Earlybird&lt;/a&gt;, a Lucene based real-time retrieval engine. There is an &lt;a href=&#34;http://notes.stephenholiday.com/Earlybird.pdf&#34;&gt;Earlybird paper.&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Rankers&lt;/h2&gt; &#xA;&lt;p&gt;Based on the blog, a total of 1500 candidates are retrieved. However, only some of them will be served to your Twitter feed.&lt;/p&gt; &#xA;&lt;p&gt;Twitter would want to show the tweets that you are most likely to positively engage with. Therefore Twitter will predict probabilities of whether you will engage with the tweet, and use these probabilities to score the tweets.&lt;/p&gt; &#xA;&lt;p&gt;To reduce computation cost, tweets are first ranked with a light ranker (which is just a logistic regression) and then a heavy ranking (a neural network model).&lt;/p&gt; &#xA;&lt;h3&gt;Light Ranker&lt;/h3&gt; &#xA;&lt;p&gt;This is their &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/src/python/twitter/deepbird/projects/timelines/scripts/models/earlybird/README.md&#34;&gt;documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚ÄúThe Earlybird light ranker is a which predicts the likelihood that the user will engage with a tweet. It is intended to be a simplified version of the heavy ranker which can run on a greater amount of tweets.‚Äù&lt;/li&gt; &#xA; &lt;li&gt;‚ÄúThe current model was last trained several years ago, and uses some very strange features. We are working on training a new model, and eventually rebuilding this part of the stack entirely.‚Äù&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Twitter has separate models for ranking in-network and out-network tweets, with different features&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/src/python/twitter/deepbird/projects/timelines/configs/recap_earlybird/feature_config.py&#34;&gt;In-network model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/src/python/twitter/deepbird/projects/timelines/configs/rectweet_earlybird/feature_config.py&#34;&gt;Out-of-network model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The model for the Light Ranker TensorFlow model is trained using &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/twml/README.md&#34;&gt;TWML&lt;/a&gt; which is said to be deprecated, but the code is in &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/ec83d01dcaebf369444d75ed04b3625a0a645eb9/src/python/twitter/deepbird/projects/timelines/scripts/models/earlybird/README.md&#34;&gt;deepbird&lt;/a&gt; project.&lt;/p&gt; &#xA;&lt;p&gt;The Earlybird Light Ranker has some &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/ec83d01dcaebf369444d75ed04b3625a0a645eb9/src/python/twitter/deepbird/projects/timelines/scripts/models/earlybird/example_weights.py#L6&#34;&gt;feature weights&lt;/a&gt; but as suggested in the code, they are read in as run time parameters and these are most likely different in practice.&lt;/p&gt; &#xA;&lt;h3&gt;Heavy Ranker&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recap&lt;/strong&gt; The &#34;Heavy Ranker&#34; is a &lt;a href=&#34;https://arxiv.org/abs/2102.07619&#34;&gt;parallel masknet&lt;/a&gt;. Majority of the code for this is in the &lt;a href=&#34;https://github.com/twitter/the-algorithm-ml/raw/main/projects/home/recap/README.md&#34;&gt;ML repo&lt;/a&gt;. The &lt;a href=&#34;https://github.com/twitter/the-algorithm-ml/raw/main/projects/home/recap/README.md&#34;&gt;ranker itself&lt;/a&gt; is run after the candidate generators.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/twitter/the-algorithm-ml/raw/main/projects/home/recap/FEATURES.md&#34;&gt;Input features&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;All of these are combined and weighted into a score. Hyperparameters for the model &lt;a href=&#34;https://github.com/twitter/the-algorithm-ml/raw/78c3235eee5b4e111ccacb7d48e80eca019e480c/projects/home/recap/config/local_prod.yaml#L1&#34;&gt;and weighting are here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;For more details on the model, see the &lt;a href=&#34;https://raw.githubusercontent.com/igorbrigadir/awesome-twitter-algo/main/masknet.md&#34;&gt;Architecture overview&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Scoring Plan&lt;/h3&gt; &#xA;&lt;p&gt;After the model predicts the probability of the actions, weights are assigned to the probability. The tweet with the highest score is likely to appear at the top of your feed.&lt;/p&gt; &#xA;&lt;p&gt;These are the actions predicted, and &lt;a href=&#34;https://raw.githubusercontent.com/twitter/the-algorithm-ml/main/projects/home/recap/README.md&#34;&gt;their corresponding weights&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;feature&lt;/th&gt; &#xA;   &lt;th&gt;weight&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability the user will favorite the Tweet&lt;/td&gt; &#xA;   &lt;td&gt;(0.5)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability the user will click into the conversation of this tweet and reply or like a Tweet&lt;/td&gt; &#xA;   &lt;td&gt;(11*)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability the user will click into the conversation of this Tweet and stay there for at least 2 minutes&lt;/td&gt; &#xA;   &lt;td&gt;(11*)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability the user will react negatively (requesting &#34;show less often&#34; on the Tweet or author, block or mute the Tweet author)&lt;/td&gt; &#xA;   &lt;td&gt;(-74)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability the user opens the Tweet author profile and Likes or replies to a Tweet&lt;/td&gt; &#xA;   &lt;td&gt;(12)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability the user replies to the Tweet&lt;/td&gt; &#xA;   &lt;td&gt;(27)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability the user replies to the Tweet and this reply is engaged by the Tweet author&lt;/td&gt; &#xA;   &lt;td&gt;(75)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability the user will click Report Tweet&lt;/td&gt; &#xA;   &lt;td&gt;(-369)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability the user will ReTweet the Tweet&lt;/td&gt; &#xA;   &lt;td&gt;(1)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;probability (for a video Tweet) that the user will watch at least half of the video&lt;/td&gt; &#xA;   &lt;td&gt;(0.005)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The score of the tweet is equal to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;P(favorite) * 0.5 + max( P(click and reply), P(click and stay two minutes) ) * 11 + P(hide or block or mute) * -74 + ... etc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The tweet with the highest score is likely to appear at the top of your feed. (There is still a part on boost where multipliers will be applied to the score). However, filtering is applied afterwards, and this could change what tweets you actually see.&lt;/p&gt; &#xA;&lt;p&gt;There are some interpretations we can make from the scoring plan&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;They combine the negative feedback actions (hide/mute/block) even though they have different produce consequences. By combining the predictions I think they hope to generalize the signal. However, the report prediction is by itself and has a much larger negative weight.&lt;/li&gt; &#xA; &lt;li&gt;There is very limited implicit action in the scoring plan. This is unlike short video recommendation systems like TikTok where the system learns from how long you stay on the video. The weight for the video completion prediction is insignificant.&lt;/li&gt; &#xA; &lt;li&gt;The only implicit action being predicted is when you click into the conversation of this Tweet and stay there for at least 2 minutes. 2 minutes is quite a large number. This can be viewed as a defense against comment bait, where the author entices you to click on the comments but leave you disappointed. If you exit the comment section soon after clicking, it is not considered a positive signal to engagement.&lt;/li&gt; &#xA; &lt;li&gt;The scoring plan encourages participation in the conversation. The weight for the probability of you replying is high. The weight for the probability of the author replying to your reply is even higher. We can view this as Twitter&#39;s intention to be the &#34;town square&#34; of the Internet. However, this signal does not differentiate whether the conversation is friendly or otherwise (unless you also hide/mute/block/report).&lt;/li&gt; &#xA; &lt;li&gt;We should also note that the score of Blue Verified authors will be given a &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/d1cab28a1044a147a107ae067890850041956777/home-mixer/server/src/main/scala/com/twitter/home_mixer/param/HomeGlobalParams.scala#L89,L103&#34;&gt;multiplier of 4 or 2&lt;/a&gt;, which overrides many of the weights in the scoring plan.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The release does not describe how the weights are chosen. We expect the weights to be tuned with A/B testing. We are also curious about what Twitter measures and optimizes when they tune the weights.&lt;/p&gt; &#xA;&lt;h2&gt;Filters&lt;/h2&gt; &#xA;&lt;p&gt;Usually, filtering happens before ranking to avoid the need to rank candidates that will be filtered later. However, on Twitter, the blog implies that filtering happens after ranking.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/visibilitylib/README.md&#34;&gt;visibility-filters&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(From the blog) &#34;Filter out Tweets based on their content and your preferences. For instance, remove Tweets from accounts you block or mute.&#34;&lt;/li&gt; &#xA;   &lt;li&gt;‚ÄúVisibility Filtering library is currently being reviewed and rebuilt, and part of the code has been removed and is not ready to be shared yet. The remaining part of the code needs further review and will be shared once it‚Äôs ready. Also code comments have been sanitized.‚Äù&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Remove &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/home-mixer/server/src/main/scala/com/twitter/home_mixer/functional_component/filter/OutOfNetworkCompetitorURLFilter.scala&#34;&gt;out-of-network competitor site URLs&lt;/a&gt; from potential offered candidate Tweets&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Ordering&lt;/h2&gt; &#xA;&lt;p&gt;There are some reasons why we might not want to order the tweets strictly by the scoring plan. The scoring plan scores tweets independent of other Tweets. However, we might want to consider other tweets when presenting the tweets on the feed, for example, avoid showing tweets from the same author consecutively or maintain some other form of diversity in the tweets.&lt;/p&gt; &#xA;&lt;p&gt;These are the heuristics mentioned in the &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/topics/open-source/2023/twitter-recommendation-algorithm&#34;&gt;blog&lt;/a&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Author Diversity&lt;/strong&gt;: Avoid too many consecutive Tweets from a single author.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/home-mixer/server/src/main/scala/com/twitter/home_mixer/product/scored_tweets/scorer/DiversityDiscountProvider.scala&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;score * ((1 - 0.25) * Math.pow(0.5, position) + 0.25)&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;If you have seen the author is the same feed refresh, the score of the tweet from the author havled (but with a floor)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Content Balance&lt;/strong&gt;: Ensure we are delivering a fair balance of In-Network and Out-of-Network Tweets.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;(Contributions needed)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Feedback-based Fatigue&lt;/strong&gt;: Lower the score of certain Tweets if the viewer has provided negative feedback around it.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/home-mixer/server/src/main/scala/com/twitter/home_mixer/functional_component/scorer/FeedbackFatigueScorer.scala&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;The multiplier will be less than one if you &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Provided negative feedback on the author of the tweet&lt;/li&gt; &#xA;     &lt;li&gt;Provided negative feedback to the users who like the tweet&lt;/li&gt; &#xA;     &lt;li&gt;Provided negative feedback on users who follow the author of the tweet (?)&lt;/li&gt; &#xA;     &lt;li&gt;Provided negative feedback on users who retweeted the tweet&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;Recent negative feedback will have a greater weight &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;If the negative feedback is provided more than 14 + 140 days ago, the negative feedback will not be considered.&lt;/li&gt; &#xA;     &lt;li&gt;If the negative feedback was provided less than 14 days ago, the tweet will be filtered. See &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/home-mixer/server/src/main/scala/com/twitter/home_mixer/functional_component/filter/FeedbackFatigueFilter.scala&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Social Proof&lt;/strong&gt;: Exclude Out-of-Network Tweets without a second degree connection to the Tweet as a quality safeguard. In other words, ensure someone you follow engaged with the Tweet or follows the Tweet‚Äôs author.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;What is described above is a filter, not a discount. However, we can find the discount, see &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/home-mixer/server/src/main/scala/com/twitter/home_mixer/functional_component/scorer/OONTweetScalingScorer.scala&#34;&gt;code&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;ScaleFactor = 0.75&lt;/code&gt; is applied to out-of-network tweets (exactly second degree connection?), in-network retweets of out-of-network tweets should not have this multiplier applied&lt;/li&gt; &#xA;   &lt;li&gt;We might have a filter that removes all content with more than two degrees of connection.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Twitter Blue boost&lt;/strong&gt;: (Not listed in blog)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;See &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/home-mixer/server/src/main/scala/com/twitter/home_mixer/functional_component/scorer/VerifiedAuthorScalingScorer.scala&#34;&gt;code&lt;/a&gt; and &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/home-mixer/server/src/main/scala/com/twitter/home_mixer/param/HomeGlobalParams.scala#L89&#34;&gt;default parameters&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;If the author of the candidate tweet is a Blue Verified and in the network of the user (i.e. user follows author?), the score of the tweet is multiplied by 4&lt;/li&gt; &#xA;     &lt;li&gt;If the author of the candidate tweet is a Blue Verified and out of the network of the user (i.e. does not follow author an within two degrees of connection), the score of the tweet from is multiplied by 2.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;This means that Blue Verified authors that the user does not follow is given a greater boost than the authors the user explictly follows.&lt;/li&gt; &#xA;   &lt;li&gt;Note that Twitter Blue is launched shortly after Elon Musk&#39;s takeover.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Business Terms and Logic&lt;/h2&gt; &#xA;&lt;p&gt;These are Twitter specific terms and names that keep coming up across different code bases and blog posts.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Twepoch - A &#34;magic number&#34; &lt;code&gt;1288834974657L&lt;/code&gt;, which is a timestamp for &lt;code&gt;2010-11-04T01:42:54Z&lt;/code&gt; the date that Twitter introduced the Snowflake ID system, used as Twitter&#39;s own &lt;a href=&#34;https://en.wikipedia.org/wiki/Epoch_(computing)&#34;&gt;&#34;Unix Epoch&#34;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Snowflake - Twitter&#39;s system for &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/a/2010/announcing-snowflake&#34;&gt;assigning unique IDs&lt;/a&gt; to tweets, users, lists, DMs, media etc.&lt;/li&gt; &#xA; &lt;li&gt;WTF - &lt;a href=&#34;https://web.stanford.edu/~rezab/papers/wtf_overview.pdf&#34;&gt;Who to follow&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;DDG - Duck Duck Goose, Twitter&#39;s &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/a/2015/twitter-experimentation-technical-overview&#34;&gt;A/B Testing Platform&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Earlybird - Twitter&#39;s Lucene based real-time search index. &lt;a href=&#34;https://stephenholiday.com/notes/earlybird/&#34;&gt;Notes&lt;/a&gt; and a &lt;a href=&#34;https://blog.twitter.com/engineering/en_us/a/2011/the-engineering-behind-twitter-s-new-search-experience&#34;&gt;blog post here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&#34;Unregretted user minutes&#34; - the metric Twitter publicly states is the thing they are optimizing for. It is unknown how exactly they measure this.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Bias and Manipulation&lt;/h2&gt; &#xA;&lt;p&gt;Cases of potential bias, manipulation, favouritism, hacks, etc. The focus on this repository is on the concrete, techincal aspects of the code, not speculating on anything twitter may or may not have done. That exercise is left to the reader, however, there are some technical aspects that should still be described about these popular accusations, this is a section for those. Unfortunately, much of the configuration that would contain specific instances of interventions is not in the code.&lt;/p&gt; &#xA;&lt;h3&gt;Deboosting Rival Sites&lt;/h3&gt; &#xA;&lt;p&gt;It was long speculated youtube links get massively deboosted, and Spaces links massively boost Tweets in recommendations. There are no specific references to this in the code. However, there are filters that could be configured for this, referencing &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/ec83d01dcaebf369444d75ed04b3625a0a645eb9/home-mixer/server/src/main/scala/com/twitter/home_mixer/product/scored_tweets/ScoredTweetsRecommendationPipelineConfig.scala#L231&#34;&gt;OutOfNetworkCompetitorURLFilter&lt;/a&gt; for example.&lt;/p&gt; &#xA;&lt;h3&gt;Elon Musk feature&lt;/h3&gt; &#xA;&lt;p&gt;The Elon Musk / Democrat / Republican Code: &lt;a href=&#34;https://github.com/twitter/the-algorithm/commit/ec83d01dcaebf369444d75ed04b3625a0a645eb9&#34;&gt;Now Removed&lt;/a&gt;. One of the first widely shared cases, falsely assuming this is something that directly affects Recommendations when it was actually for internal A/B testing, to monitor for effects (DDG is Duck Duck Goose, the A/B Testing Platfom). It was also mentioned in &lt;a href=&#34;https://twitter.com/elonmusk/status/1641880448061120513&#34;&gt;the space&lt;/a&gt; and denied there. However, a former twitter employee also offered an &lt;a href=&#34;https://twitter.com/igb/status/1641910616939167745&#34;&gt;alternative explanation&lt;/a&gt; (A/B Testing measures behavior, so one way or another Twitter is tuning your TL, indirectly).&lt;/p&gt; &#xA;&lt;h3&gt;Ukraine&lt;/h3&gt; &#xA;&lt;p&gt;There are two mentions related to Ukraine in the Twiter Algo repo. Whereas &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/7f90d0ca342b928b479b512ec51ac2c3821f5922/visibilitylib/src/main/scala/com/twitter/visibility/rules/PublicInterestRules.scala#L54&#34;&gt;one of them&lt;/a&gt; is a flag for Ukraine-related misinformation used for moderation, or warning labels, there is another &lt;em&gt;safety label&lt;/em&gt; for Twitter Spaces called &lt;em&gt;&lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/7f90d0ca342b928b479b512ec51ac2c3821f5922/visibilitylib/src/main/scala/com/twitter/visibility/models/SpaceSafetyLabelType.scala#L39&#34;&gt;UkraineCrisisTopic&lt;/a&gt;&lt;/em&gt;. Here are some facts about these labels and their function:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Each safety label &#34;&lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/main/visibilitylib/README.md&#34;&gt;describes a particular policy violation, and usually leads to reduced visibility of the labeled entity in product surfaces&lt;/a&gt;&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;SafetyLabel&lt;/code&gt; results in tweet interstitial or notice, are publicly &lt;a href=&#34;https://help.twitter.com/en/resources/addressing-misleading-info&#34;&gt;documented here previously&lt;/a&gt; and specifically for &lt;a href=&#34;https://help.twitter.com/en/rules-and-policies/crisis-misinformation&#34;&gt;Armed Conflicts here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;All &lt;a href=&#34;https://github.com/twitter/the-algorithm/raw/7f90d0ca342b928b479b512ec51ac2c3821f5922/visibilitylib/src/main/scala/com/twitter/visibility/models/SpaceSafetyLabelType.scala#L26&#34;&gt;other Twitter Spaces safety labels&lt;/a&gt; are related to misinformation, NSFW, toxic or harmful content, DMCA takedowns, etc.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Changes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2 hours after it was released, &lt;a href=&#34;https://github.com/twitter/the-algorithm/compare/ef4c5eb65e6e04fac4f0e1fa8bbeff56b75c1f98...ec83d01dcaebf369444d75ed04b3625a0a645eb9&#34;&gt;Twitter removed&lt;/a&gt; feature flags that specifically higlighted Elon&#39;s account&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Discussions about The Algorithm Elsewhere&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://solomonmg.github.io/post/twitter-the-algorithm/&#34;&gt;What can we learn from `The Algorithm,&#39; Twitter&#39;s partial open-sourcing of it&#39;s feed-ranking recommendation system?&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources for Learning More about Recsys&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;@karlhigley&#39;s &lt;a href=&#34;https://practicalrecs.com/the-rest-of-the-owl.html&#34;&gt;blog&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/karlhigley/status/1138122648934916097&#34;&gt;thread of threads&lt;/a&gt; are very accessible things about Recommender Systems in practice.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;A good Recommender Systems entry point is the Google Machine Learning for &lt;a href=&#34;https://developers.google.com/machine-learning/recommendation&#34;&gt;Recommender Systems course&lt;/a&gt;, it also has a good &lt;a href=&#34;https://developers.google.com/machine-learning/glossary/recsystems&#34;&gt;glossary&lt;/a&gt; of terms.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The biggest academic recsys community is &lt;a href=&#34;https://recsys.acm.org/&#34;&gt;ACM Recsys&lt;/a&gt; and state-of-the-art recommender systems research is usually openly available in the &lt;a href=&#34;https://dl.acm.org/conference/recsys&#34;&gt;proceedings&lt;/a&gt;. A lot of the presentations are on &lt;a href=&#34;https://www.youtube.com/@acmrecsys/videos&#34;&gt;youtube&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Admittedly out of date, but still useful, is the &lt;a href=&#34;https://www.recsyswiki.com/wiki/Main_Page&#34;&gt;RecSys Wiki&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The latest edition of the &lt;a href=&#34;https://link.springer.com/book/10.1007/978-1-0716-2197-4&#34;&gt;Recommender Systems Handbook&lt;/a&gt; is also a good book that covers the field well.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;I find it very helpful to break down recommendation systems into four stages - &lt;a href=&#34;https://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e&#34;&gt;retrieval, filtering, scoring, and ordering&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Although a &lt;a href=&#34;http://patrickhalina.com/posts/ml-systems-design-interview-guide/&#34;&gt;systems design interview guide&lt;/a&gt;, it introduces the most important design consideration for a recommendation system.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>twitter/opensource-website</title>
    <updated>2023-04-05T01:23:17Z</updated>
    <id>tag:github.com,2023-04-05:/twitter/opensource-website</id>
    <link href="https://github.com/twitter/opensource-website" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Twitter&#39;s open source website, identifying projects we&#39;ve released, organizations we support, and the work we do to support open source.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;opensource.twitter.dev&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.twitter.dev/status/#active&#34;&gt;&lt;img src=&#34;https://opensource.twitter.dev/status/active.svg?sanitize=true&#34; alt=&#34;status: active&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the Twitter Open Source website at &lt;a href=&#34;https://opensource.twitter.dev&#34;&gt;https://opensource.twitter.dev&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This site is built with &lt;a href=&#34;https://gohugo.io/&#34;&gt;hugo&lt;/a&gt;, using a custom built-in theme. Follow the standard instructions on the hugo website to &lt;a href=&#34;https://gohugo.io/getting-started/installing/&#34;&gt;install&lt;/a&gt; and &lt;a href=&#34;https://gohugo.io/getting-started/usage/&#34;&gt;run&lt;/a&gt; hugo. Just make sure to use hugo-extended, which has added support for Sass/SCSS stylesheets.&lt;/p&gt; &#xA;&lt;p&gt;There are also some python scripts that run periodically to &lt;a href=&#34;https://raw.githubusercontent.com/twitter/opensource-website/main/.github/workflows/update-data.yml&#34;&gt;update some repo data&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>OptimalScale/LMFlow</title>
    <updated>2023-04-05T01:23:17Z</updated>
    <id>tag:github.com,2023-04-05:/OptimalScale/LMFlow</id>
    <link href="https://github.com/OptimalScale/LMFlow" rel="alternate"></link>
    <summary type="html">&lt;p&gt;An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Language Model for All.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/logo.png&#34; alt=&#34;LMFlow&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto; background-color: transparent;&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;LMFlow&lt;/h1&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;b&gt;English&lt;/b&gt; | &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/README_zh-hans.md&#34;&gt;ÁÆÄ‰Ωì‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/README_es.md&#34;&gt;Espa√±ol&lt;/a&gt; | &lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/README_jp.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/OptimalScale/LMFlow/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Website-Doc-ff69b4.svg?sanitize=true&#34; alt=&#34;Doc&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/srGxyazbNs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-LMFlow-%237289da.svg?logo=discord&#34; alt=&#34;Embark&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/lmflow/shared_invite/zt-1s6egx12s-THlwHuCjF6~JGKmx7JoJPA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp;amp;amp&#34; alt=&#34;slack badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://i.328888.xyz/2023/04/04/ibvpAk.jpeg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WeChat-Join-brightgreen?logo=wechat&amp;amp;amp&#34; alt=&#34;WeChat badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community.&lt;/p&gt; &#xA;&lt;p&gt;Large Language Model for All. See our &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#vision&#34;&gt;vision&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/features.png&#34; alt=&#34;LMFlow-features&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Latest News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023-04-02] &lt;a href=&#34;https://lmflow.com/&#34;&gt;Web service is online!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-04-01] &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#model-zoo&#34;&gt;Release Chinese checkpoints in model zoo: LLaMA-7B-tuned, LLaMA-13B-tuned, LLaMA-33B-tuned.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-04-01] &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#model-zoo&#34;&gt;Release English checkpoints in model zoo: LLaMA-7B-medical, LLaMA-13B-medical, and LLaMA-33B-medical.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-03-27] &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#supported-models&#34;&gt;Support full tuning and lora tuning for all decoder models.&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-03-27] &lt;a href=&#34;https://github.com/OptimalScale/LMFlow#model-performance&#34;&gt;Tasked tuned model beats ChatGPT on medical domain&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023-03-27] &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Release code and checkpoints - version 0.0.1&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Demos&lt;/h2&gt; &#xA;&lt;h3&gt;Currently our checkpoint download service is at capacity. We have allocated one more server to support that. If you encounter error &#34;&lt;em&gt;too many HTTP requests&lt;/em&gt;&#34;, please wait for several minutes and try again. Thanks for your understanding.&lt;span&gt;üôè&lt;/span&gt;&lt;/h3&gt; &#xA;&lt;p&gt;We provide four kinds of demos which include&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Online Service: If you don&#39;t want to run any code and just want to try our models, we deploy our instruction-tuned LLaMA-7B and LLaMA-33B for you to have a try.&lt;/li&gt; &#xA; &lt;li&gt;Colab Chatbot (shell): An interactive shell-based chatbot for you to easily deploy a chatbot on colab.&lt;/li&gt; &#xA; &lt;li&gt;Colab Chatbot (web): An interactive web-based chatbot for you to easily deploy your own chatbot on colab.&lt;/li&gt; &#xA; &lt;li&gt;Local Deploy: We also provide a way for you to deploy your model/chatbot locally, which means you can deploy much larger model than previous three methods if you have enough resource.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://lmflow.com&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Online%20Service-Web-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1P9Hf6_mLE7WHH92pw73j9D5kz6GTdkow?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-(shell)%20%20chatbot:%20gpt--neo-orange?logo=google-colab&amp;amp;amp&#34; alt=&#34;colab badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1LLtiiQO-ZIIFsTKxYzGWYX9BDRc-v8dq?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Colab-(web)%20%20chatbot:%20gpt--neo-blue?logo=google-colab&amp;amp;amp&#34; alt=&#34;colab badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Online Service&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Welcome to visit our &lt;a href=&#34;https://lmflow.com/&#34;&gt;web service&lt;/a&gt;. We deploy LLaMA-7B-tuned, and LLaMA-33B-tuned online for preview. Due to the high website traffic, sometimes the website may fail to respond. You can also deploy the chatbot referto &lt;code&gt;Local Deploy&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Colab chatbot(shell)&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/OptimalScale/LMFlow/main/assets/colab-shell-chatbot-demo.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;We provide a simple shell demo of chatbot with Google Colab&#39;s T4/P100/V100 GPU. Notice that the provided gpt-neo-2.7b model is &lt;strong&gt;a rather weak model&lt;/strong&gt;, which only supports English and may sometimes generate unsatisfactory responses. To improve the performance, users can use their own dataset to finetune and obtain a better model with LMFlow. One can also try other available decoder-only models provided in ü§ó &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;huggingface&lt;/a&gt;, by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_chatbot.sh {another-model-name}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Colab chatbot(web)&lt;/h3&gt; &#xA;&lt;p&gt;We provide a simple web demo of chatbot with Google Colab&#39;s T4/P100/V100 GPU. Notice that the provided gpt-neo-2.7b model is &lt;strong&gt;a rather weak model&lt;/strong&gt;, which only supports English and may sometimes generate unsatisfactory responses.&lt;/p&gt; &#xA;&lt;h3&gt;Local Deploy&lt;/h3&gt; &#xA;&lt;p&gt;If you have resources and want to deploy your own model locally. We provide you an easy way to run a flask server to launch a backend (to further provide services to other frontend) and an interactive web frontend (to let you communicate directly) by&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd ./service&#xA;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Medical Performance&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;PubMedQA (ID)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MedQA-USMLE (OOD)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;MedMCQA (ID)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Average&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Human (pass)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Human (expert)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;78.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;87.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;90.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;85.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;InstructGPT 175B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;73.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;46.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;54.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ChatGPT&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;63.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;57.0&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.7&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;55.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA 7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.9&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LLaMA 33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;43.4&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Task-tuned LLaMA 7B (Full)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;75.1&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;49.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;56.5&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Task-tuned LLaMA 33B (LoRA)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;74.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.3&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;50.2&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;58.5&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The LLaMA 33B (LoRA) performance is achieved with only &lt;strong&gt;~16h&lt;/strong&gt; finetuning on the training split of PubMedQA and MedMCQA with a single 8 * A100 server. For more performance, including instruction tuning results, please refer to our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Model Zoo&lt;/h2&gt; &#xA;&lt;p&gt;We open-sourced the trained checkpoints to everyone for further training and inference.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Instruct-tuned Models&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;Base Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Download&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-7B-tuned&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1x5JLae3akVkfFeDhSe3TEyUbPn_GNFyb/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-13B-tuned&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1m_rpe6rNpN59kWvjJ3GfKeEmS-68TRYr/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-33B-tuned&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1IqgqLHwNkWQ7BffheZnqD6a-8Zul1bk6/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA-65B-tuned&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/65&#34; alt=&#34;training&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-65B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google Drive&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA7B-medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-7B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1Z44tsrRvfDFvucbNGFjHC_vbPcBvg3x-/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA13B-medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-13B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1uoTAXTMyYQkP6N4ummx7tj-c4v1p91ap/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA33B-medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/100&#34; alt=&#34;completed&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-33B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/14N9o_1pwHmVuSikQ3orMVzZDrLYJC0iM/view?usp=share_link&#34;&gt;Google Drive&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaMA65B-medical&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://geps.dev/progress/90&#34; alt=&#34;training&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;LLaMA-65B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Google Drive&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Pipelines&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Pipelines&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Task Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Instruction Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Parameter-Efficient Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Large Model Inference&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;‚úÖ&lt;/span&gt; Supported&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Alignment Tuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;üîß&lt;/span&gt; Developing&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Supported Models&lt;/h2&gt; &#xA;&lt;p&gt;Seamlessly supported all the &lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;decoder models&lt;/a&gt; in ü§ó huggingface. LLaMA, GPT2, GPT-Neo, Galactica, have been fully tested. We will support encoder models soon.&lt;/p&gt; &#xA;&lt;h2&gt;1.Setup&lt;/h2&gt; &#xA;&lt;p&gt;Our package has been full tested on Linux OS (Ubuntu 20.04). Other OS platforms (MacOS, Windows) are not fully tested.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/OptimalScale/LMFlow.git&#xA;cd LMFlow&#xA;conda create -n lmflow python=3.9 -y&#xA;conda activate lmflow&#xA;conda install mpi4py&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2.Prepare Dataset&lt;/h2&gt; &#xA;&lt;p&gt;You can easily download the example training dataset and test dataset by running&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd data&#xA;bash download.sh all&#xA;cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also use your own dataset by simply convert to the following format:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;type&#34;: &#34;text2text&#34;,&#xA;  &#34;instances&#34;: [&#xA;    {&#xA;      &#34;input&#34;: &#34;Question: The Transformer architecture [START_REF]&#34;,&#xA;      &#34;output&#34;: &#34;N/A&#34;&#xA;    },&#xA;    ...&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;type&#34;: &#34;text_only&#34;,&#xA;  &#34;instances&#34;: [&#xA;    {&#xA;      &#34;text&#34;: &#34;Defintion: In this task, we ask you to write an answer to a question that involves events that may be stationary (not changing over time) or transient (changing over time). For example, the sentence \&#34;he was born in the U.S.\&#34; contains a stationary event since it will last forever; however, \&#34;he is hungry\&#34; contains a transient event since it will remain true for a short period of time. Note that a lot of the questions could have more than one correct answer. We only need a single most-likely answer. Please try to keep your \&#34;answer\&#34; as simple as possible. Concise and simple \&#34;answer\&#34; is preferred over those complex and verbose ones. \n Input: Question: Sentence: It&#39;s hail crackled across the comm, and Tara spun to retake her seat at the helm. \nQuestion: Will the hail storm ever end? \n Output: NA \n\n&#34;&#xA;    },&#xA;    ...&#xA;  ]&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;3. Run Scripts&lt;/h2&gt; &#xA;&lt;h3&gt;3.1 Run Finetuning&lt;/h3&gt; &#xA;&lt;p&gt;You can run &lt;code&gt;scripts/run_finetune.sh&lt;/code&gt; to finetune a GPT-2 base model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_finetune.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you would like to provide arguments for deepspeed to reflect your machine settings, you may pass the corresponding deepspeed arguments to the script. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_finetune.sh &#34;--num_gpus=8 --master_port 10001&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable LoRA finetuning, you may refer to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_finetune_with_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;which can be run in similar manner.&lt;/p&gt; &#xA;&lt;p&gt;For detailed configurations, one may modify these scripts directly. These scripts actually just call python script &lt;code&gt;examples/finetune.py&lt;/code&gt;, which can be run in following manner,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;deepspeed ${deepspeed_args} \&#xA;  examples/finetune.py \&#xA;    --deepspeed configs/ds_config_zero3.json \&#xA;    --bf16 \&#xA;    --run_name finetune_with_lora \&#xA;    --model_name_or_path facebook/galactica-1.3b \&#xA;    --num_train_epochs 0.01 \&#xA;    --learning_rate 2e-5 \&#xA;    --dataset_path ${dataset_path} \&#xA;    --per_device_train_batch_size 1 \&#xA;    --per_device_eval_batch_size 1 \&#xA;    --validation_split_percentage 0 \&#xA;    --logging_steps 20 \&#xA;    --block_size 512 \&#xA;    --do_train \&#xA;    --output_dir output_models/finetune \&#xA;    --overwrite_output_dir \&#xA;    --ddp_timeout 72000 \&#xA;    --save_steps 5000 \&#xA;    --dataloader_num_workers 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Here we set number of epochs &lt;code&gt;--num_train_epochs&lt;/code&gt; to &lt;code&gt;0.01&lt;/code&gt; so that the finetuning process can be finished quickly. If you wish to obtain a model with better performance, feel free to adjust those hyperparameters. You may run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python examples/finetune.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to view all possible finetuning arguments. The finetuned model checkpoint will be saved in the argument specified by &lt;code&gt;--output_dir&lt;/code&gt;, which is &lt;code&gt;output_models/finetune&lt;/code&gt; in the above example.&lt;/p&gt; &#xA;&lt;h3&gt;3.2 Run Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;One can directly run evaluation with an existing huggingface model, e.g. to run GPT2 large, one may execute&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_evaluation.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or run the corresponding python script&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;CUDA_VISIBLE_DEVICES=0 \&#xA;    deepspeed examples/evaluate.py \&#xA;    --answer_type medmcqa \&#xA;    --model_name_or_path gpt2-large \&#xA;    --dataset_path data/MedQA-USMLE/validation \&#xA;    --deepspeed examples/ds_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To load the finetuned model, specify &lt;code&gt;--model_name_or_path&lt;/code&gt; with the saved model checkpoint directory path.&lt;/p&gt; &#xA;&lt;p&gt;For LoRA finetuned models, one may refer to&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./scripts/run_evaluation_with_lora.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Those scripts invoke the examples &lt;code&gt;examples/*.py&lt;/code&gt; built based on our APIs. For more API-related examples, one may refer to the methods in the unittest &lt;code&gt;tests&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;4. Additional Notes&lt;/h2&gt; &#xA;&lt;h3&gt;4.1 LLaMA Checkpoint&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;First, you need to get the access of LLaMA model from &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;facebookresearch/llama&lt;/a&gt;. Download the official checkpoints and save them into &lt;code&gt;${llama-path}&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Second, convert the official checkpoints &lt;code&gt;${llama-path}&lt;/code&gt; to HuggingFace supported checkpoints &lt;code&gt;${llama-hf-path}&lt;/code&gt; by running&lt;/p&gt; &lt;p&gt;&lt;code&gt;python ./scripts/convert_llama_weights_to_hf.py --input_dir ${llama-path} --model_size 7B --output_dir ${llama-hf-path}/llama-7b-hf&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Then you are good to go by setting the checkpoint path to &lt;code&gt;${llama-hf-path}/llama-7b-hf&lt;/code&gt;. Enjoy it!&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;(optional) Now you have the original llama-7b-hf pretrained model. With&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd output_models &amp;amp;&amp;amp; ./download.sh all &amp;amp;&amp;amp; cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can obtain the model difference finetuned by ours. By a way similar to &lt;code&gt;./scripts/run_evaluation_with_lora.sh&lt;/code&gt;,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;CUDA_VISIBLE_DEVICES=0 \&#xA;    deepspeed examples/evaluate.py \&#xA;    --answer_type text \&#xA;    --model_name_or_path ${llama-hf-path}/llama-7b-hf \&#xA;    --lora_model_path output_models/${llama-model-diff-path} \&#xA;    --dataset_path data/alpaca/test \&#xA;    --prompt_structure &#34;Input: {input}&#34; \&#xA;    --deepspeed examples/ds_config.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can now evaluate with the finetuned llama model.&lt;/p&gt; &#xA;&lt;h3&gt;4.2 DeepSpeed Config&lt;/h3&gt; &#xA;&lt;p&gt;You can config the deepspeed under configs. Details can be referred at &lt;a href=&#34;https://www.deepspeed.ai/docs/config-json/&#34;&gt;DeepSpeed Configuration&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;5. Model Release&lt;/h2&gt; &#xA;&lt;h3&gt;5.1 Medical Model Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;You can run following script to download our medical model checkpoints :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd output_models&#xA;bash download.sh medical_ckpt&#xA;cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also directly download our model via google drive link : &lt;a href=&#34;https://drive.google.com/file/d/1bnsQGNGNYchsOfiNyRAmL2fNiowbmFNw/view?usp=share_link&#34;&gt;medical_ckpt.tar.gz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5.2 Instruction Model Checkpoints&lt;/h3&gt; &#xA;&lt;p&gt;Similarly, you can run following script to download our instruction model checkpoints :&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd output_models&#xA;bash download.sh instruction_ckpt&#xA;cd -&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also directly download our model via google drive link : &lt;a href=&#34;https://drive.google.com/file/d/1d_ioQ-ViVweeifbsFSO4pczc3UORFHZO/view?usp=share_link&#34;&gt;instruction_ckpt.tar.gz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;5.3 Begin Reproduce&lt;/h3&gt; &#xA;&lt;p&gt;After downloading the model checkpoints. You can replace the &lt;code&gt;--lora_model_path&lt;/code&gt; with &lt;code&gt;output_models/instruction_ckpt/llama7b-lora&lt;/code&gt; (example for llama-7b for instruction) and replace &lt;code&gt;--model_name_or_path&lt;/code&gt; with your converted llama model inside &lt;code&gt;LMFlow/scripts/run_evaluation_with_lora.sh&lt;/code&gt; and run this shell script to reproduce the result.&lt;/p&gt; &#xA;&lt;p&gt;Then you can check the model performance at our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://optimalscale.github.io/LMFlow/&#34;&gt;Documentation&lt;/a&gt; for more API reference and experimental results.&lt;/p&gt; &#xA;&lt;h2&gt;Vision&lt;/h2&gt; &#xA;&lt;p&gt;Hello there! We are excited to announce the upcoming release of our code repository that includes a complete LLM training process, enabling users to quickly build their own language models and train them effectively.&lt;/p&gt; &#xA;&lt;p&gt;Our code repository is not just a simple model; it includes the complete training workflow, model optimization, and testing tools. You can use it to build various types of language models, including conversation models, question-answering models, and text generation models, among others.&lt;/p&gt; &#xA;&lt;p&gt;Moreover, we aim to create an open and democratic LLM sharing platform where people can share their checkpoints and experiences to collectively improve the skills of the community. We welcome anyone who is interested in LLM to participate and join us in building an open and friendly community!&lt;/p&gt; &#xA;&lt;p&gt;Whether you are a beginner or an expert, we believe that you can benefit from this platform. Let&#39;s work together to build a vibrant and innovative LLM community!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/srGxyazbNs&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-LMFlow-%237289da.svg?logo=discord&#34; alt=&#34;Embark&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://join.slack.com/t/lmflow/shared_invite/zt-1s6egx12s-THlwHuCjF6~JGKmx7JoJPA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp;amp;amp&#34; alt=&#34;slack badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://i.328888.xyz/2023/04/04/ibvpAk.jpeg&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/WeChat-Join-brightgreen?logo=wechat&amp;amp;amp&#34; alt=&#34;WeChat badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This package aims to provide a streamlined and user-friendly pipeline for large model tuning. Its functionalities serve as a reference and are intended for use by the user. However, it is important to note that the responsibility for the preparation of the data and pretrained models lies solely with the user. This package does not guarantee the accuracy, completeness, applicability, or legality of the components from the user&#39;s preparation. Users must be aware of and assume all risks and liabilities associated with the preparation of the models and data, and obtain legal, commercial, and technical advice before utilizing this package. The pipeline shall not be held responsible for any direct, indirect, special, incidental, or consequential damages resulting from the user&#39;s improper preparation of the data and pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;Our checkpoints, which include both English and Chinese versions, are provided solely for research purposes. The training data contained within these checkpoints includes generated results from the ChatGPT language model. We do not endorse or encourage the distribution or usage of these checkpoints for commercial purposes. Users of these checkpoints are solely responsible for ensuring that they are used correctly and appropriately.&lt;/p&gt; &#xA;&lt;p&gt;It is also crucial to highlight that the results generated by the model are based on probabilistic models and not directly related to this pipeline. The accuracy, reliability, applicability, and legality of the results are not guaranteed by this pipeline. Therefore, users must also be aware of the risks and liabilities associated with the results and seek legal, commercial, and technical advice before relying on the model-generated outcomes. This pipeline shall not be accountable for any direct, indirect, special, incidental, or consequential damages resulting from the user&#39;s reliance on the model-generated results.&lt;/p&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;If you need any help, please submit a &lt;a href=&#34;https://github.com/OptimalScale/LMFlow&#34;&gt;Github&lt;/a&gt; issue.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/OptimalScale/LMFlow/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=OptimalScale/LMFlow&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this repository useful, please consider giving ‚≠ê and citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lmflow,&#xA;  author = {Shizhe Diao and Rui Pan and Hanze Dong and KaShun Shum and Jipeng Zhang and Wei Xiong and Tong Zhang},&#xA;  title = {LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models},&#xA;  year = {2023},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://optimalscale.github.io/LMFlow/}},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>