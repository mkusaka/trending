<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-23T01:26:35Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>weaviate/Verba</title>
    <updated>2024-05-23T01:26:35Z</updated>
    <id>tag:github.com,2024-05-23:/weaviate/Verba</id>
    <link href="https://github.com/weaviate/Verba" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Retrieval Augmented Generation (RAG) chatbot powered by Weaviate&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Verba&lt;/h1&gt; &#xA;&lt;h2&gt;The Golden RAGtriever&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://weaviate.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=powered%20by&amp;amp;message=Weaviate%20%E2%9D%A4&amp;amp;color=green&amp;amp;style=flat-square&#34; alt=&#34;Weaviate&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/goldenverba/&#34;&gt;&lt;img src=&#34;https://static.pepy.tech/personalized-badge/goldenverba?period=total&amp;amp;units=international_system&amp;amp;left_color=grey&amp;amp;right_color=orange&amp;amp;left_text=pip%20downloads&#34; alt=&#34;PyPi downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://docs.docker.com/get-started/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Docker_support-%E2%9C%93-4c1?style=flat-square&amp;amp;logo=docker&amp;amp;logoColor=white&#34; alt=&#34;Docker support&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://verba.weaviate.io/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Check%20out%20the%20demo!-yellow?&amp;amp;style=flat-square&amp;amp;logo=react&amp;amp;logoColor=white&#34; alt=&#34;Demo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Welcome to Verba: The Golden RAGtriever, an open-source application designed to offer an end-to-end, streamlined, and user-friendly interface for Retrieval-Augmented Generation (RAG) out of the box. In just a few easy steps, explore your datasets and extract insights with ease, either locally with HuggingFace and Ollama or through LLM providers such as OpenAI, Cohere, and Google.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install goldenverba&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/weaviate/Verba/raw/1.0.0/img/verba.gif&#34; alt=&#34;Demo of Verba&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#verba&#34;&gt;Verba&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#what-is-verba&#34;&gt;üéØ What Is Verba?&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#feature-lists&#34;&gt;‚ú® Features&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#getting-started-with-verba&#34;&gt;‚ú® Getting Started with Verba&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#api-keys&#34;&gt;üîë API Keys&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#weaviate&#34;&gt;Weaviate&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#ollama&#34;&gt;Ollama&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#google&#34;&gt;Google&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#unstructured&#34;&gt;Unstructured&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#openai&#34;&gt;OpenAI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#huggingface&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#how-to-deploy-with-pip&#34;&gt;Quickstart: Deploy with pip&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#how-to-build-from-source&#34;&gt;Quickstart: Build from Source&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#how-to-install-verba-with-docker&#34;&gt;Quickstart: Deploy with Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#%EF%B8%8Fverba-walkthrough&#34;&gt;üíæ Verba Walkthrough&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#open-source-contribution&#34;&gt;üíñ Open Source Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#known-issues&#34;&gt;üö© Known Issues&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#faq&#34;&gt;‚ùîFAQ&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;What Is Verba?&lt;/h2&gt; &#xA;&lt;p&gt;Verba is a fully-customizable personal assistant for querying and interacting with your data, &lt;strong&gt;either locally or deployed via cloud&lt;/strong&gt;. Resolve questions around your documents, cross-reference multiple data points or gain insights from existing knowledge bases. Verba combines state-of-the-art RAG techniques with Weaviate&#39;s context-aware database. Choose between different RAG frameworks, data types, chunking &amp;amp; retrieving techniques, and LLM providers based on your individual use-case.&lt;/p&gt; &#xA;&lt;h2&gt;Feature Lists&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ü§ñ Model Support&lt;/th&gt; &#xA;   &lt;th&gt;Implemented&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ollama (e.g. Llama3)&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Local Embedding and Generation Models powered by Ollama&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HuggingFace (e.g. MiniLMEmbedder)&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Local Embedding Models powered by HuggingFace&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cohere (e.g. Command R+)&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Embedding and Generation Models by Cohere&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Google (e.g. Gemini)&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Embedding and Generation Models by Google&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAI (e.g. GPT4)&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Embedding and Generation Models by OpenAI&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;üìÅ Data Support&lt;/th&gt; &#xA;   &lt;th&gt;Implemented&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;PDF Ingestion&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Import PDF into Verba&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CSV/XLSX Ingestion&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Import Table Data into Verba&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Multi-Modal&lt;/td&gt; &#xA;   &lt;td&gt;planned ‚è±Ô∏è&lt;/td&gt; &#xA;   &lt;td&gt;Import Multi-Modal Data into Verba&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UnstructuredIO&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Import Data through Unstructured&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;‚ú® RAG Features&lt;/th&gt; &#xA;   &lt;th&gt;Implemented&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hybrid Search&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Semantic Search combined with Keyword Search&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Semantic Caching&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Results saved and retrieved based on semantic meaning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Autocomplete Suggestion&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Verba suggests autocompletion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Filtering&lt;/td&gt; &#xA;   &lt;td&gt;planned ‚è±Ô∏è&lt;/td&gt; &#xA;   &lt;td&gt;Apply Filters (e.g. documents, document types etc.) before performing RAG&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Advanced Querying&lt;/td&gt; &#xA;   &lt;td&gt;planned ‚è±Ô∏è&lt;/td&gt; &#xA;   &lt;td&gt;Task Delegation Based on LLM Evaluation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Reranking&lt;/td&gt; &#xA;   &lt;td&gt;planned ‚è±Ô∏è&lt;/td&gt; &#xA;   &lt;td&gt;Rerank results based on context for improved results&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RAG Evaluation&lt;/td&gt; &#xA;   &lt;td&gt;planned ‚è±Ô∏è&lt;/td&gt; &#xA;   &lt;td&gt;Interface for Evaluating RAG pipelines&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Customizable Metadata&lt;/td&gt; &#xA;   &lt;td&gt;planned ‚è±Ô∏è&lt;/td&gt; &#xA;   &lt;td&gt;Free control over Metadata&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;üÜí Cool Bonus&lt;/th&gt; &#xA;   &lt;th&gt;Implemented&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Docker Support&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Verba is deployable via Docker&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Customizable Frontend&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;   &lt;td&gt;Verba&#39;s frontend is fully-customizable via the frontend&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;ü§ù RAG Libraries&lt;/th&gt; &#xA;   &lt;th&gt;Implemented&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Haystack&lt;/td&gt; &#xA;   &lt;td&gt;planned ‚è±Ô∏è&lt;/td&gt; &#xA;   &lt;td&gt;Implement Haystack RAG pipelines&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LlamaIndex&lt;/td&gt; &#xA;   &lt;td&gt;planned ‚è±Ô∏è&lt;/td&gt; &#xA;   &lt;td&gt;Implement LlamaIndex RAG pipelines&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LangChain&lt;/td&gt; &#xA;   &lt;td&gt;planned ‚è±Ô∏è&lt;/td&gt; &#xA;   &lt;td&gt;Implement LangChain RAG pipelines&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Something is missing? Feel free to create a new issue or discussion with your idea!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/weaviate/Verba/raw/1.0.0/img/verba_screen.png&#34; alt=&#34;Showcase of Verba&#34;&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;Getting Started with Verba&lt;/h1&gt; &#xA;&lt;p&gt;You have three deployment options for Verba:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install via pip&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install goldenverba&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Build from Source&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/weaviate/Verba&#xA;&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use Docker for Deployment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;: If you&#39;re not using Docker, ensure that you have &lt;code&gt;Python &amp;gt;=3.10.0&lt;/code&gt; installed on your system.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re unfamiliar with Python and Virtual Environments, please read the &lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/PYTHON_TUTORIAL.md&#34;&gt;python tutorial guidelines&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;API Keys&lt;/h1&gt; &#xA;&lt;p&gt;Before starting Verba you&#39;ll need to configure access to various components depending on your chosen technologies, such as OpenAI, Cohere, and HuggingFace via an &lt;code&gt;.env&lt;/code&gt; file. Create this &lt;code&gt;.env&lt;/code&gt; in the same directory you want to start Verba in. You can find an &lt;code&gt;.env.example&lt;/code&gt; file in the &lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/goldenverba/.env.example&#34;&gt;goldenverba&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure to only set environment variables you intend to use, environment variables with missing or incorrect values may lead to errors.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Below is a comprehensive list of the API keys and variables you may require:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Environment Variable&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WEAVIATE_URL_VERBA&lt;/td&gt; &#xA;   &lt;td&gt;URL to your hosted Weaviate Cluster&lt;/td&gt; &#xA;   &lt;td&gt;Connect to your &lt;a href=&#34;https://console.weaviate.cloud/&#34;&gt;WCS&lt;/a&gt; Cluster&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;WEAVIATE_API_KEY_VERBA&lt;/td&gt; &#xA;   &lt;td&gt;API Credentials to your hosted Weaviate Cluster&lt;/td&gt; &#xA;   &lt;td&gt;Connect to your &lt;a href=&#34;https://console.weaviate.cloud/&#34;&gt;WCS&lt;/a&gt; Cluster&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPENAI_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;Your API Key&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt; Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OPENAI_BASE_URL&lt;/td&gt; &#xA;   &lt;td&gt;URL to OpenAI instance&lt;/td&gt; &#xA;   &lt;td&gt;Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;COHERE_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;Your API Key&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to &lt;a href=&#34;https://cohere.com/&#34;&gt;Cohere&lt;/a&gt; Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OLLAMA_URL&lt;/td&gt; &#xA;   &lt;td&gt;URL to your Ollama instance (e.g. &lt;a href=&#34;http://localhost:11434&#34;&gt;http://localhost:11434&lt;/a&gt; )&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to &lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt; Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OLLAMA_MODEL&lt;/td&gt; &#xA;   &lt;td&gt;Model Name (e.g. llama)&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to a specific &lt;a href=&#34;https://ollama.com/&#34;&gt;Ollama&lt;/a&gt; Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UNSTRUCTURED_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;Your API Key&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to &lt;a href=&#34;https://docs.unstructured.io/welcome&#34;&gt;Unstructured&lt;/a&gt; Data Ingestion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UNSTRUCTURED_API_URL&lt;/td&gt; &#xA;   &lt;td&gt;URL to Unstructured Instance&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to &lt;a href=&#34;https://docs.unstructured.io/welcome&#34;&gt;Unstructured&lt;/a&gt; Data Ingestion&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GITHUB_TOKEN&lt;/td&gt; &#xA;   &lt;td&gt;Your GitHub Token&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to Data Ingestion via GitHub&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/td&gt; &#xA;   &lt;td&gt;Google Credentials&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to Google Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GOOGLE_CLOUD_PROJECT&lt;/td&gt; &#xA;   &lt;td&gt;Google Cloud Project&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to Google Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GOOGLE_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;Your API Key&lt;/td&gt; &#xA;   &lt;td&gt;Get Access to Google Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VERBA_PRODUCTION&lt;/td&gt; &#xA;   &lt;td&gt;True&lt;/td&gt; &#xA;   &lt;td&gt;Run Verba in Production Mode&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Weaviate&lt;/h2&gt; &#xA;&lt;p&gt;Verba provides flexibility in connecting to Weaviate instances based on your needs. By default, Verba opts for &lt;a href=&#34;https://weaviate.io/developers/weaviate/installation/embedded&#34;&gt;Weaviate Embedded&lt;/a&gt; if it doesn&#39;t detect the &lt;code&gt;WEAVIATE_URL_VERBA&lt;/code&gt; and &lt;code&gt;WEAVIATE_API_KEY_VERBA&lt;/code&gt; environment variables. This local deployment is the most straightforward way to launch your Weaviate database for prototyping and testing.&lt;/p&gt; &#xA;&lt;p&gt;However, you also have other options:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üå©Ô∏è Weaviate Cloud Service (WCS)&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you prefer a cloud-based solution, Weaviate Cloud Service (WCS) offers a scalable, managed environment. Learn how to set up a cloud cluster and get the API keys by following the &lt;a href=&#34;https://weaviate.io/developers/wcs/guides/create-instance&#34;&gt;Weaviate Cluster Setup Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;üê≥ Docker Deployment&lt;/strong&gt; Another robust local alternative is deploying Weaviate using Docker. For more details, consult the &lt;a href=&#34;https://weaviate.io/developers/weaviate/installation/docker-compose&#34;&gt;Weaviate Docker Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Ollama&lt;/h2&gt; &#xA;&lt;p&gt;Verba supports Ollama models. Download and Install Ollama on your device (&lt;a href=&#34;https://ollama.com/download&#34;&gt;https://ollama.com/download&lt;/a&gt;). Make sure to install your preferred LLM using &lt;code&gt;ollama run &amp;lt;model&amp;gt;&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Tested with &lt;code&gt;llama3&lt;/code&gt;, &lt;code&gt;llama3:70b&lt;/code&gt; and &lt;code&gt;mistral&lt;/code&gt;. The bigger models generally perform better, but need more computational power.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Make sure Ollama Server runs in the background and that you don&#39;t ingest documents with different ollama models since their vector dimension can vary that will lead to errors&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Google&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use the Google Features, make sure to install the Google Verba package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install goldenverba[google]&#xA;&#xA;or&#xA;&#xA;pip install `.[google]`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you&#39;re using Docker, modify the Dockerfile accordingly&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Google Embeddings&lt;/h3&gt; &#xA;&lt;p&gt;For the Google Embeddings, Verba is using Vertex AI Studio inside Google Cloud. You can find instructions for obtaining a key &lt;a href=&#34;https://cloud.google.com/iam/docs/create-short-lived-credentials-direct&#34;&gt;here&lt;/a&gt;. If you have the &lt;code&gt;gcloud&lt;/code&gt; CLI installed, you can run the following command: &lt;code&gt;gcloud auth print-access-token&lt;/code&gt;. &lt;strong&gt;At the moment, this access token must be renewed every hour.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You also need to set the &lt;code&gt;GOOGLE_CLOUD_PROJECT&lt;/code&gt; environment variable to the name of your project.&lt;/p&gt; &#xA;&lt;h3&gt;Google Gemini&lt;/h3&gt; &#xA;&lt;p&gt;To use Google Gemini, you need a service account key, which is a JSON file. To obtain this, go to &#34;project settings&#34; in your Google Cloud console, then to &#34;service accounts&#34;. Create a new service account, then create a new key. Download this key and place it in the route of Verba. Name it &lt;code&gt;gemini_secrets.json&lt;/code&gt; to have it excluded from git automatically. Set the environment variable &lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt; to the location of this file, e.g. &lt;code&gt;gemini_secrets.json&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You also need to set the &lt;code&gt;GOOGLE_CLOUD_PROJECT&lt;/code&gt; environment variable to the name of your project.&lt;/p&gt; &#xA;&lt;h2&gt;Unstructured&lt;/h2&gt; &#xA;&lt;p&gt;Verba supports importing documents through Unstructured IO (e.g plain text, .pdf, .csv, and more). To use them you need the &lt;code&gt;UNSTRUCTURED_API_KEY&lt;/code&gt; and &lt;code&gt;UNSTRUCTURED_API_URL&lt;/code&gt; environment variable. You can get it from &lt;a href=&#34;https://unstructured.io/&#34;&gt;Unstructured&lt;/a&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;UNSTRUCTURED_API_URL is set to &lt;code&gt;https://api.unstructured.io/general/v0/general&lt;/code&gt; by default&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;OpenAI&lt;/h2&gt; &#xA;&lt;p&gt;Verba supports OpenAI Models such as Ada, GPT3, and GPT4. To use them, you need to specify the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable. You can get it from &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also add a &lt;code&gt;OPENAI_BASE_URL&lt;/code&gt; to use proxies such as LiteLLM (&lt;a href=&#34;https://github.com/BerriAI/litellm&#34;&gt;https://github.com/BerriAI/litellm&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_BASE_URL=YOUR-OPENAI_BASE_URL&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Azure OpenAI&lt;/h3&gt; &#xA;&lt;p&gt;To use Azure OpenAI, you need to set&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The API type:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_TYPE=&#34;azure&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The key and the endpoint:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=&amp;lt;YOUR_KEY&amp;gt;&#xA;OPENAI_BASE_URL=http://XXX.openai.azure.com&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Azure OpenAI resource name, which is XXX if your endpoint is XXX.openai.azure.com&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;AZURE_OPENAI_RESOURCE_NAME=&amp;lt;YOUR_AZURE_RESOURCE_NAME&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You need to set the models, for the embeddings and for the query.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;AZURE_OPENAI_EMBEDDING_MODEL=&#34;text-embedding-ada-002&#34;&#xA;OPENAI_MODEL=&#34;gpt-4&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Finally, as Azure is using per-minute quota, you might need to add a waiting time between each chunk upload. For example, if you have a limit of 240k tokens per minute, if your chunks are 400 tokens max, then 100ms between queries should be fine. If you get error 429 from weaviate, then increase this value.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;WAIT_TIME_BETWEEN_INGESTION_QUERIES_MS=&#34;100&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;HuggingFace&lt;/h2&gt; &#xA;&lt;p&gt;If you want to use the HuggingFace Features, make sure to install the correct Verba package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install goldenverba[huggingface]&#xA;&#xA;or&#xA;&#xA;pip install `.[huggingface]`&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you&#39;re using Docker, modify the Dockerfile accordingly&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h1&gt;How to deploy with pip&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;Python &amp;gt;=3.10.0&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Initialize a new Python Environment&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m virtualenv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install Verba&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install goldenverba&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Launch Verba&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;verba start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can specify the --port and --host via flags&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Access Verba&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;Visit localhost:8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create .env file and add environment variables&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;How to build from Source&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Clone the Verba repos&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/weaviate/Verba.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Initialize a new Python Environment&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 -m virtualenv venv&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Install Verba&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Launch Verba&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;verba start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;You can specify the --port and --host via flags&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Access Verba&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;Visit localhost:8000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Create .env file and add environment variables&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;How to install Verba with Docker&lt;/h1&gt; &#xA;&lt;p&gt;Docker is a set of platform-as-a-service products that use OS-level virtualization to deliver software in packages called containers. To get started with deploying Verba using Docker, follow the steps below. If you need more detailed instructions on Docker usage, check out the &lt;a href=&#34;https://docker-curriculum.com/&#34;&gt;Docker Curriculum&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Clone the Verba repos&lt;/strong&gt; Ensure you have Git installed on your system. Then, open a terminal or command prompt and run the following command to clone the Verba repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/weaviate/Verba.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set neccessary environment variables&lt;/strong&gt; Make sure to set your required environment variables in the &lt;code&gt;.env&lt;/code&gt; file. You can read more about how to set them up in the &lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/#api-keys&#34;&gt;API Keys Section&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Adjust the docker-compose file&lt;/strong&gt; You can use the &lt;code&gt;docker-compose.yml&lt;/code&gt; to add required environment variables under the &lt;code&gt;verba&lt;/code&gt; service and can also adjust the Weaviate Docker settings to enable Authentification or change other settings of your database instance. You can read more about the Weaviate configuration in our &lt;a href=&#34;https://weaviate.io/developers/weaviate/installation/docker-compose&#34;&gt;docker-compose documentation&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Please make sure to only add environment variables that you really need. If you have no authentifcation enabled in your Weaviate Cluster, make sure to not include the &lt;code&gt;WEAVIATE_API_KEY_VERBA&lt;/code&gt; enviroment variable&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Deploy using Docker&lt;/strong&gt; With Docker installed and the Verba repository cloned, navigate to the directory containing the Docker Compose file in your terminal or command prompt. Run the following command to start the Verba application in detached mode, which allows it to run in the background:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;docker compose up -d&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;docker compose --env-file .env up -d&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This command will download the necessary Docker images, create containers, and start Verba. Remember, Docker must be installed on your system to use this method. For installation instructions and more details about Docker, visit the official Docker documentation.&lt;/p&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;&lt;strong&gt;Access Verba&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;You can access your local Weaviate instance at &lt;code&gt;localhost:8080&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can access the Verba frontend at &lt;code&gt;localhost:8000&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want your Docker Instance to install a specific version of Verba you can edit the &lt;code&gt;Dockerfile&lt;/code&gt; and change the installation line.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;RUN pip install -e &#39;.&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Verba Walkthrough&lt;/h2&gt; &#xA;&lt;h3&gt;Overview Page&lt;/h3&gt; &#xA;&lt;p&gt;Once you have access to Verba, you can use the &lt;code&gt;Overview Page&lt;/code&gt; to validate if all environments and libraries were correctly set and installed. You can use the Admin Console, to see all data stored in the Weaviate Collections and reset certain parts of Verba (e.g. Documents, Cache, Configuration, etc.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/weaviate/Verba/raw/1.0.0/img/verba_status.png&#34; alt=&#34;Demo of Verba&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Import Your Data&lt;/h3&gt; &#xA;&lt;p&gt;With Verba configured, you&#39;re ready to import your data and start exploring. Use the &lt;code&gt;Add Documents&lt;/code&gt; Page to ingest your data. You can choose between Readers that support different data types, chunking techniques, and embedding model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/weaviate/Verba/raw/1.0.0/img/verba_data.png&#34; alt=&#34;Demo of Verba&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Query Your Data&lt;/h3&gt; &#xA;&lt;p&gt;With Data imported, you can use the &lt;code&gt;Chat&lt;/code&gt; page to ask any related questions. You will receive relevant chunks that are semantically relevant to your question and an answer generated by your choosen model. You can configure the RAG pipeline under the &lt;code&gt;RAG&lt;/code&gt; page.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/weaviate/Verba/raw/1.0.0/img/verba_rag.png&#34; alt=&#34;Demo of Verba&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Open Source Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Your contributions are always welcome! Feel free to contribute ideas, feedback, or create issues and bug reports if you find any! Before contributing, please read the &lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;. Visit our &lt;a href=&#34;https://forum.weaviate.io/&#34;&gt;Weaviate Community Forum&lt;/a&gt; if you need any help!&lt;/p&gt; &#xA;&lt;h3&gt;Project Architecture&lt;/h3&gt; &#xA;&lt;p&gt;You can learn more about Verba&#39;s architecture and implementation in its &lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/TECHNICAL.md&#34;&gt;technical documentation&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/weaviate/Verba/main/FRONTEND.md&#34;&gt;frontend documentation&lt;/a&gt;. It&#39;s recommended to have a look at them before making any contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Known Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Weaviate Embeeded&lt;/strong&gt; currently not working on Windows yet &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Will be fixed in future versions, until then please use the Docker or WCS Deployment&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Is Verba Multi-Lingual?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;This depends on your choosen Embedding and Generation Model whether they support multi-lingual data.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Can I use my Ollama Server with the Verba Docker?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Yes, you can! Make sure the URL is set to: &lt;code&gt;OLLAMA_URL=http://host.docker.internal:11434&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;How to clear Weaviate Embedded Storage?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Remove the directory &lt;code&gt;rm ~/.local/share/weaviate&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;How can I specify the port?&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;You can use the port and host flag &lt;code&gt;verba start --port 9000 -host 0.0.0.0&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>naklecha/llama3-from-scratch</title>
    <updated>2024-05-23T01:26:35Z</updated>
    <id>tag:github.com,2024-05-23:/naklecha/llama3-from-scratch</id>
    <link href="https://github.com/naklecha/llama3-from-scratch" rel="alternate"></link>
    <summary type="html">&lt;p&gt;llama3 implementation one matrix multiplication at a time&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama3 implemented from scratch&lt;/h1&gt; &#xA;&lt;p&gt;in this file, i implemented llama3 from scratch, one tensor and matrix multiplication at a time. &lt;br&gt; also, im going to load tensors directly from the model file that meta provided for llama3, you need to download the weights before running this file. here is the offical link to download the weights: &lt;a href=&#34;https://llama.meta.com/llama-downloads/&#34;&gt;https://llama.meta.com/llama-downloads/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/archi.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;tokenizer&lt;/h2&gt; &#xA;&lt;p&gt;im not going to implement a bpe tokenizer (but andrej karpathy has a really clean implementation) &lt;br&gt; link to his implementation: &lt;a href=&#34;https://github.com/karpathy/minbpe&#34;&gt;https://github.com/karpathy/minbpe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/karpathyminbpe.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from pathlib import Path&#xA;import tiktoken&#xA;from tiktoken.load import load_tiktoken_bpe&#xA;import torch&#xA;import json&#xA;import matplotlib.pyplot as plt&#xA;&#xA;tokenizer_path = &#34;Meta-Llama-3-8B/tokenizer.model&#34;&#xA;special_tokens = [&#xA;            &#34;&amp;lt;|begin_of_text|&amp;gt;&#34;,&#xA;            &#34;&amp;lt;|end_of_text|&amp;gt;&#34;,&#xA;            &#34;&amp;lt;|reserved_special_token_0|&amp;gt;&#34;,&#xA;            &#34;&amp;lt;|reserved_special_token_1|&amp;gt;&#34;,&#xA;            &#34;&amp;lt;|reserved_special_token_2|&amp;gt;&#34;,&#xA;            &#34;&amp;lt;|reserved_special_token_3|&amp;gt;&#34;,&#xA;            &#34;&amp;lt;|start_header_id|&amp;gt;&#34;,&#xA;            &#34;&amp;lt;|end_header_id|&amp;gt;&#34;,&#xA;            &#34;&amp;lt;|reserved_special_token_4|&amp;gt;&#34;,&#xA;            &#34;&amp;lt;|eot_id|&amp;gt;&#34;,  # end of turn&#xA;        ] + [f&#34;&amp;lt;|reserved_special_token_{i}|&amp;gt;&#34; for i in range(5, 256 - 5)]&#xA;mergeable_ranks = load_tiktoken_bpe(tokenizer_path)&#xA;tokenizer = tiktoken.Encoding(&#xA;    name=Path(tokenizer_path).name,&#xA;    pat_str=r&#34;(?i:&#39;s|&#39;t|&#39;re|&#39;ve|&#39;m|&#39;ll|&#39;d)|[^\r\n\p{L}\p{N}]?\p{L}+|\p{N}{1,3}| ?[^\s\p{L}\p{N}]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&#34;,&#xA;    mergeable_ranks=mergeable_ranks,&#xA;    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},&#xA;)&#xA;&#xA;tokenizer.decode(tokenizer.encode(&#34;hello world!&#34;))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;hello world!&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;reading the model file&lt;/h2&gt; &#xA;&lt;p&gt;normally, reading this depends on how the model classes are written and the variable names inside them. &lt;br&gt; but since we are implementing llama3 from scratch we will read the file one tensor at a time.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/model.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model = torch.load(&#34;Meta-Llama-3-8B/consolidated.00.pth&#34;)&#xA;print(json.dumps(list(model.keys())[:20], indent=4))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#xA;    &#34;tok_embeddings.weight&#34;,&#xA;    &#34;layers.0.attention.wq.weight&#34;,&#xA;    &#34;layers.0.attention.wk.weight&#34;,&#xA;    &#34;layers.0.attention.wv.weight&#34;,&#xA;    &#34;layers.0.attention.wo.weight&#34;,&#xA;    &#34;layers.0.feed_forward.w1.weight&#34;,&#xA;    &#34;layers.0.feed_forward.w3.weight&#34;,&#xA;    &#34;layers.0.feed_forward.w2.weight&#34;,&#xA;    &#34;layers.0.attention_norm.weight&#34;,&#xA;    &#34;layers.0.ffn_norm.weight&#34;,&#xA;    &#34;layers.1.attention.wq.weight&#34;,&#xA;    &#34;layers.1.attention.wk.weight&#34;,&#xA;    &#34;layers.1.attention.wv.weight&#34;,&#xA;    &#34;layers.1.attention.wo.weight&#34;,&#xA;    &#34;layers.1.feed_forward.w1.weight&#34;,&#xA;    &#34;layers.1.feed_forward.w3.weight&#34;,&#xA;    &#34;layers.1.feed_forward.w2.weight&#34;,&#xA;    &#34;layers.1.attention_norm.weight&#34;,&#xA;    &#34;layers.1.ffn_norm.weight&#34;,&#xA;    &#34;layers.2.attention.wq.weight&#34;&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;with open(&#34;Meta-Llama-3-8B/params.json&#34;, &#34;r&#34;) as f:&#xA;    config = json.load(f)&#xA;config&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;{&#39;dim&#39;: 4096,&#xA; &#39;n_layers&#39;: 32,&#xA; &#39;n_heads&#39;: 32,&#xA; &#39;n_kv_heads&#39;: 8,&#xA; &#39;vocab_size&#39;: 128256,&#xA; &#39;multiple_of&#39;: 1024,&#xA; &#39;ffn_dim_multiplier&#39;: 1.3,&#xA; &#39;norm_eps&#39;: 1e-05,&#xA; &#39;rope_theta&#39;: 500000.0}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;we use this config to infer details about the model like&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;the model has 32 transformer layers&lt;/li&gt; &#xA; &lt;li&gt;each multi-head attention block has 32 heads&lt;/li&gt; &#xA; &lt;li&gt;the vocab size and so on&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dim = config[&#34;dim&#34;]&#xA;n_layers = config[&#34;n_layers&#34;]&#xA;n_heads = config[&#34;n_heads&#34;]&#xA;n_kv_heads = config[&#34;n_kv_heads&#34;]&#xA;vocab_size = config[&#34;vocab_size&#34;]&#xA;multiple_of = config[&#34;multiple_of&#34;]&#xA;ffn_dim_multiplier = config[&#34;ffn_dim_multiplier&#34;]&#xA;norm_eps = config[&#34;norm_eps&#34;]&#xA;rope_theta = torch.tensor(config[&#34;rope_theta&#34;])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;converting text to tokens&lt;/h2&gt; &#xA;&lt;p&gt;here we use tiktoken (i think an openai library) as the tokenizer&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/tokens.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#34;the answer to the ultimate question of life, the universe, and everything is &#34;&#xA;tokens = [128000] + tokenizer.encode(prompt)&#xA;print(tokens)&#xA;tokens = torch.tensor(tokens)&#xA;prompt_split_as_tokens = [tokenizer.decode([token.item()]) for token in tokens]&#xA;print(prompt_split_as_tokens)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]&#xA;[&#39;&amp;lt;|begin_of_text|&amp;gt;&#39;, &#39;the&#39;, &#39; answer&#39;, &#39; to&#39;, &#39; the&#39;, &#39; ultimate&#39;, &#39; question&#39;, &#39; of&#39;, &#39; life&#39;, &#39;,&#39;, &#39; the&#39;, &#39; universe&#39;, &#39;,&#39;, &#39; and&#39;, &#39; everything&#39;, &#39; is&#39;, &#39; &#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;converting tokens to their embedding&lt;/h2&gt; &#xA;&lt;p&gt;IM SORRY but this is the only part of the codebase where i use an inbuilt neural network module &lt;br&gt; anyway, so our [17x1] tokens are now [17x4096], i.e. 17 embeddings (one for each token) of length 4096 &lt;br&gt; &lt;br&gt; note: keep track of the shapes, it makes it much easier to understand everything&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/embeddings.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;embedding_layer = torch.nn.Embedding(vocab_size, dim)&#xA;embedding_layer.weight.data.copy_(model[&#34;tok_embeddings.weight&#34;])&#xA;token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)&#xA;token_embeddings_unnormalized.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;we then normalize the embedding using rms normalization&lt;/h2&gt; &#xA;&lt;p&gt;please, note after this step the shapes dont change, the values are just normalized &lt;br&gt; things to keep in mind, we need a norm_eps (from config) because we dont want to accidently set rms to 0 and divide by 0 &lt;br&gt; here is the formula:&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/rms.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# def rms_norm(tensor, norm_weights):&#xA;#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5&#xA;#     return tensor * (norm_weights / rms)&#xA;def rms_norm(tensor, norm_weights):&#xA;    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;building the first first layer of the transformer&lt;/h1&gt; &#xA;&lt;h3&gt;normalization&lt;/h3&gt; &#xA;&lt;p&gt;you will see me accessing layer.0 from the model dict (this is the first layer) &lt;br&gt; anyway, so after normalizing our shapes are still [17x4096] same as embedding but normalized&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/norm.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;token_embeddings = rms_norm(token_embeddings_unnormalized, model[&#34;layers.0.attention_norm.weight&#34;])&#xA;token_embeddings.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;attention implemented from scratch&lt;/h3&gt; &#xA;&lt;p&gt;let&#39;s load the attention heads of the first layer of the transformer&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/qkv.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&amp;gt; when we load the query, key, value and output vectors from the model we notice the shapes to be [4096x4096], [1024x4096], [1024x4096], [4096x4096] &lt;br&gt; &amp;gt; at first glance this is weird because ideally we want each q,k,v and o for each head individually &lt;br&gt; &amp;gt; the authors of the code bundled them togeather because its easy it helps parallize attention head multiplication. &lt;br&gt; &amp;gt; im going to unwrap everything...&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&#xA;    model[&#34;layers.0.attention.wq.weight&#34;].shape,&#xA;    model[&#34;layers.0.attention.wk.weight&#34;].shape,&#xA;    model[&#34;layers.0.attention.wv.weight&#34;].shape,&#xA;    model[&#34;layers.0.attention.wo.weight&#34;].shape&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;unwrapping query&lt;/h3&gt; &#xA;&lt;p&gt;in the next section we will unwrap the queries from multiple attention heads, the resulting shape is [32x128x4096] &lt;br&gt;&lt;br&gt; here, 32 is the number of attention heads in llama3, 128 is the size of the query vector and 4096 is the size of the token embedding&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;q_layer0 = model[&#34;layers.0.attention.wq.weight&#34;]&#xA;head_dim = q_layer0.shape[0] // n_heads&#xA;q_layer0 = q_layer0.view(n_heads, head_dim, dim)&#xA;q_layer0.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([32, 128, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;im going to implement the first head of the first layer&lt;/h3&gt; &#xA;&lt;p&gt;here i access the query weight matrix first head of the first layer, the size of this query weight matrix is [128x4096]&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;q_layer0_head0 = q_layer0[0]&#xA;q_layer0_head0.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([128, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;we now multiply the query weights with the token embedding, to recive a query for the token&lt;/h3&gt; &#xA;&lt;p&gt;here you can see the resulting shape is [17x128], this is because we have 17 tokens and for each token there is a 128 length query.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/q_per_token.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)&#xA;q_per_token.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 128])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;positioning encoding&lt;/h2&gt; &#xA;&lt;p&gt;we are now at a stage where we have a query vector for each token in our prompt, but if you think about it -- the indivitually query vector has no idea about the position in the prompt. &lt;br&gt;&lt;br&gt; query: &#34;the answer to the ultimate question of life, the universe, and everything is &#34; &lt;br&gt;&lt;br&gt; in our prompt we have used &#34;the&#34; three times, we need the query vectors of all 3 &#34;the&#34; tokens to have different query vectors (each of size [1x128]) based on their positions in the query. we perform these rotations using RoPE (rotory positional embedding). &lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h3&gt;RoPE&lt;/h3&gt; &#xA;&lt;p&gt;watch this video (this is what i watched) to understand the math. &lt;a href=&#34;https://www.youtube.com/watch?v=o29P0Kpobz0&amp;amp;t=530s&#34;&gt;https://www.youtube.com/watch?v=o29P0Kpobz0&amp;amp;t=530s&lt;/a&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/rope.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)&#xA;q_per_token_split_into_pairs.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 64, 2])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;in the above step, we split the query vectors into pairs, we apply a rotational angle shift to each pair! &lt;br&gt;&lt;br&gt; we now have a vector of size [17x64x2], this is the 128 length queries split into 64 pairs for each token in the prompt! each of those 64 pairs will be rotated by m*(theta) where m is the position of the token for which we are rotating the query!&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/qsplit.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;using dot product of complex numbers to rotate a vector&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/freq_cis.png&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;zero_to_one_split_into_64_parts = torch.tensor(range(64))/64&#xA;zero_to_one_split_into_64_parts&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,&#xA;        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,&#xA;        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,&#xA;        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,&#xA;        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,&#xA;        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,&#xA;        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,&#xA;        0.9844])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;freqs = 1.0 / (rope_theta ** zero_to_one_split_into_64_parts)&#xA;freqs&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,&#xA;        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,&#xA;        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,&#xA;        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,&#xA;        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,&#xA;        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,&#xA;        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,&#xA;        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,&#xA;        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,&#xA;        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,&#xA;        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;freqs_for_each_token = torch.outer(torch.arange(17), freqs)&#xA;freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)&#xA;freqs_cis.shape&#xA;&#xA;# viewing tjhe third row of freqs_cis&#xA;value = freqs_cis[3]&#xA;plt.figure()&#xA;for i, element in enumerate(value[:17]):&#xA;    plt.plot([0, element.real], [0, element.imag], color=&#39;blue&#39;, linewidth=1, label=f&#34;Index: {i}&#34;)&#xA;    plt.annotate(f&#34;{i}&#34;, xy=(element.real, element.imag), color=&#39;red&#39;)&#xA;plt.xlabel(&#39;Real&#39;)&#xA;plt.ylabel(&#39;Imaginary&#39;)&#xA;plt.title(&#39;Plot of one row of freqs_cis&#39;)&#xA;plt.show()&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/implllama3_30_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;now that we have a complex number (the angle change vector) for every token&#39;s query element&lt;/h3&gt; &#xA;&lt;p&gt;we can convert our queries (the one we split into pairs) as complex numbers and then dot product to rotate the query based on the position &lt;br&gt; honeslty this is beautiful to think about :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)&#xA;q_per_token_as_complex_numbers.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 64])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis&#xA;q_per_token_as_complex_numbers_rotated.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 64])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;after rotated vector is obtained&lt;/h3&gt; &#xA;&lt;p&gt;we can get back our the queries as pairs by viewing the complex numbers as real numbers again&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)&#xA;q_per_token_split_into_pairs_rotated.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 64, 2])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the rotated pairs are now merged, we now have a new query vector (rotated query vector) that is of the shape [17x128] where 17 is the number of tokens and the 128 is the dim of the query vector&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)&#xA;q_per_token_rotated.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 128])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;keys (almost the same as queries)&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/keys.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; im lazy as fuck, so im not going to go through the math for keys, the only things you need to keep in mind are: &#xA;&lt;br&gt; &amp;gt; keys generate key vectors also of dimention 128 &#xA;&lt;br&gt; &amp;gt; keys have only 1/4th the number of the weights as queries, this is because the weights for keys are shared across 4 heads at a time, to reduce the number of computations need &#xA;&lt;br&gt; &amp;gt; keys are also rotated to add positional info, just like queries because of the same reasons &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k_layer0 = model[&#34;layers.0.attention.wk.weight&#34;]&#xA;k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[0] // n_kv_heads, dim)&#xA;k_layer0.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([8, 128, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k_layer0_head0 = k_layer0[0]&#xA;k_layer0_head0.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([128, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)&#xA;k_per_token.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 128])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)&#xA;k_per_token_split_into_pairs.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 64, 2])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)&#xA;k_per_token_as_complex_numbers.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 64])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)&#xA;k_per_token_split_into_pairs_rotated.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 64, 2])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)&#xA;k_per_token_rotated.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 128])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;at this stage now have both the rotated values of queries and keys, for each token.&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/keys0.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; each of the queries and keys are now of shape [17x128]. &#xA;&lt;h2&gt;in the next step we will multiply the queries and key matrices&lt;/h2&gt; &#xA;&lt;p&gt;doing this will give us a score mapping each token with one another &lt;br&gt; this score describes how well each token&#39;s query relates to the each tokens&#39;s key. THIS IS SELF ATTENTION :) &lt;br&gt; the shape of the attention score matrix (qk_per_token) is [17x17] where 17 is the number of tokens in the prompt&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/qkmatmul.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**0.5&#xA;qk_per_token.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 17])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;we now have to mask query key scores&lt;/h1&gt; &#xA;&lt;p&gt;during the training process of llama3, the future token qk scores are masked. &lt;br&gt; why? because during training we only learn to predict tokens using past tokens. &lt;br&gt; as a result, during inference we set the future tokens to zero.&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/mask.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def display_qk_heatmap(qk_per_token):&#xA;    _, ax = plt.subplots()&#xA;    im = ax.imshow(qk_per_token.to(float).detach(), cmap=&#39;viridis&#39;)&#xA;    ax.set_xticks(range(len(prompt_split_as_tokens)))&#xA;    ax.set_yticks(range(len(prompt_split_as_tokens)))&#xA;    ax.set_xticklabels(prompt_split_as_tokens)&#xA;    ax.set_yticklabels(prompt_split_as_tokens)&#xA;    ax.figure.colorbar(im, ax=ax)&#xA;    &#xA;display_qk_heatmap(qk_per_token)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/implllama3_50_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mask = torch.full((len(tokens), len(tokens)), float(&#34;-inf&#34;), device=tokens.device)&#xA;mask = torch.triu(mask, diagonal=1)&#xA;mask&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],&#xA;        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;qk_per_token_after_masking = qk_per_token + mask&#xA;display_qk_heatmap(qk_per_token_after_masking)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/implllama3_52_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/softmax.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)&#xA;display_qk_heatmap(qk_per_token_after_masking_after_softmax)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/implllama3_54_0.png&#34; alt=&#34;png&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;values (almost the end of attention)&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/value.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; these scores (0-1) are used to determine how much of value matrix is used per token &#xA;&lt;br&gt; &amp;gt; just like keys, value weights are also shared acorss every 4 attention heads (to save computation) &#xA;&lt;br&gt; &amp;gt; as a result, the shape of the value weight matrix below is [8x128x4096] &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;v_layer0 = model[&#34;layers.0.attention.wv.weight&#34;]&#xA;v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[0] // n_kv_heads, dim)&#xA;v_layer0.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([8, 128, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the first layer, first head value weight matrix is given below&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;v_layer0_head0 = v_layer0[0]&#xA;v_layer0_head0.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([128, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;value vectors&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/v0.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; we now use the value weghts to get the attention values per token, this is of size [17x128] where 17 is the number of tokens in the prompt and 128 is the dim of the value vector per token &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)&#xA;v_per_token.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 128])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;attention&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/attention.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; the resultant attention vector after multipying with the values per token is of shape [17*128] &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)&#xA;qkv_attention.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 128])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;multi head attention&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/heads.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; WE NOW HAVE THE ATTENTION VALUE OF THE FIRST LAYER AND FIRST HEAD &#xA;&lt;br&gt; now im going to run a loop and perform the exact same math as the cells above but for every head in the first layer &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;qkv_attention_store = []&#xA;&#xA;for head in range(n_heads):&#xA;    q_layer0_head = q_layer0[head]&#xA;    k_layer0_head = k_layer0[head//4] # key weights are shared across 4 heads&#xA;    v_layer0_head = v_layer0[head//4] # value weights are shared across 4 heads&#xA;    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)&#xA;    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)&#xA;    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)&#xA;&#xA;    q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)&#xA;    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)&#xA;    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis[:len(tokens)])&#xA;    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)&#xA;&#xA;    k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)&#xA;    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)&#xA;    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis[:len(tokens)])&#xA;    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)&#xA;&#xA;    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5&#xA;    mask = torch.full((len(tokens), len(tokens)), float(&#34;-inf&#34;), device=tokens.device)&#xA;    mask = torch.triu(mask, diagonal=1)&#xA;    qk_per_token_after_masking = qk_per_token + mask&#xA;    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)&#xA;    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)&#xA;    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)&#xA;    qkv_attention_store.append(qkv_attention)&#xA;&#xA;len(qkv_attention_store)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/stacked.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; we now have a the qkv_attention matrix for all 32 heads on the first layer, next im going to merge all attention scores into one large matrix of size [17x4096] &#xA;&lt;br&gt; we are almost at the end :) &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)&#xA;stacked_qkv_attention.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;weight matrix, one of the final steps&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/weightmatrix.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; one of the last things to do for a layer 0 attention is, is to multiply the weight matrix of the &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w_layer0 = model[&#34;layers.0.attention.wo.weight&#34;]&#xA;w_layer0.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([4096, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;this is a simple linear layer, so we just matmul&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;embedding_delta = torch.matmul(stacked_qkv_attention, w_layer0.T)&#xA;embedding_delta.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/afterattention.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; we now have the change in the embedding value after attention, that should be adding to the original token embeddings &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;embedding_after_edit = token_embeddings_unnormalized + embedding_delta&#xA;embedding_after_edit.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;we normalize and then run a feed forward neural network through the embedding delta&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/norm_after.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[&#34;layers.0.ffn_norm.weight&#34;])&#xA;embedding_after_edit_normalized.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;loading the ff weights and implementing the feed forward network&lt;/h2&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/swiglu.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; in llama3, they used a SwiGLU feedforward network, this network architecture is really good at adding non linearity when needed by the model. &#xA;&lt;br&gt; its pretty standard to use this feed forward network architecture in llms these days &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;w1 = model[&#34;layers.0.feed_forward.w1.weight&#34;]&#xA;w2 = model[&#34;layers.0.feed_forward.w2.weight&#34;]&#xA;w3 = model[&#34;layers.0.feed_forward.w3.weight&#34;]&#xA;output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)&#xA;output_after_feedforward.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;WE FINALLY HAVE NEW EDITED EMBEDDINGS FOR EACH TOKEN AFTER THE FIRST LAYER&lt;/h1&gt; &#xA;&lt;p&gt;just 31 more layers to go before we are done (one for loop away) &lt;br&gt; you can imagine this edited embedding as having information about all queries asked on the first layer &lt;br&gt; now each layer will encode more and more complex queries on the quesions asked, until we have an embedding that knows everything about the next token that we need.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;layer_0_embedding = embedding_after_edit+output_after_feedforward&#xA;layer_0_embedding.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;god, everything all at once&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/god.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; yep, this is it. everything we did before, all at once, for every single layer. &#xA;&lt;br&gt; &#xA;&lt;h1&gt;have fun reading :)&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;final_embedding = token_embeddings_unnormalized&#xA;for layer in range(n_layers):&#xA;    qkv_attention_store = []&#xA;    layer_embedding_norm = rms_norm(final_embedding, model[f&#34;layers.{layer}.attention_norm.weight&#34;])&#xA;    q_layer = model[f&#34;layers.{layer}.attention.wq.weight&#34;]&#xA;    q_layer = q_layer.view(n_heads, q_layer.shape[0] // n_heads, dim)&#xA;    k_layer = model[f&#34;layers.{layer}.attention.wk.weight&#34;]&#xA;    k_layer = k_layer.view(n_kv_heads, k_layer.shape[0] // n_kv_heads, dim)&#xA;    v_layer = model[f&#34;layers.{layer}.attention.wv.weight&#34;]&#xA;    v_layer = v_layer.view(n_kv_heads, v_layer.shape[0] // n_kv_heads, dim)&#xA;    w_layer = model[f&#34;layers.{layer}.attention.wo.weight&#34;]&#xA;    for head in range(n_heads):&#xA;        q_layer_head = q_layer[head]&#xA;        k_layer_head = k_layer[head//4]&#xA;        v_layer_head = v_layer[head//4]&#xA;        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)&#xA;        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)&#xA;        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)&#xA;        q_per_token_split_into_pairs = q_per_token.float().view(q_per_token.shape[0], -1, 2)&#xA;        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)&#xA;        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)&#xA;        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)&#xA;        k_per_token_split_into_pairs = k_per_token.float().view(k_per_token.shape[0], -1, 2)&#xA;        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)&#xA;        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)&#xA;        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)&#xA;        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(128)**0.5&#xA;        mask = torch.full((len(token_embeddings_unnormalized), len(token_embeddings_unnormalized)), float(&#34;-inf&#34;))&#xA;        mask = torch.triu(mask, diagonal=1)&#xA;        qk_per_token_after_masking = qk_per_token + mask&#xA;        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=1).to(torch.bfloat16)&#xA;        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)&#xA;        qkv_attention_store.append(qkv_attention)&#xA;&#xA;    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-1)&#xA;    w_layer = model[f&#34;layers.{layer}.attention.wo.weight&#34;]&#xA;    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)&#xA;    embedding_after_edit = final_embedding + embedding_delta&#xA;    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[f&#34;layers.{layer}.ffn_norm.weight&#34;])&#xA;    w1 = model[f&#34;layers.{layer}.feed_forward.w1.weight&#34;]&#xA;    w2 = model[f&#34;layers.{layer}.feed_forward.w2.weight&#34;]&#xA;    w3 = model[f&#34;layers.{layer}.feed_forward.w3.weight&#34;]&#xA;    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)&#xA;    final_embedding = embedding_after_edit+output_after_feedforward&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;we now have the final embedding, the best guess the model could make about the next token&lt;/h1&gt; &#xA;&lt;p&gt;the shape of the embedding is the same as regular token embeddings [17x4096] where 17 is the number of tokens and 4096 is the embedding dim&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/last_norm.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;final_embedding = rms_norm(final_embedding, model[&#34;norm.weight&#34;])&#xA;final_embedding.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([17, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;finally, lets decode the embedding into the token value&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/finallayer.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; we will use the output decoder to convert the final embedding into a token &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model[&#34;output.weight&#34;].shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([128256, 4096])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;we use the embedding of the last token to predict the next value&lt;/h1&gt; &#xA;&lt;p&gt;hopefully in our case, 42 :) note: 42 is the answer to &#34;the answer to the ultimate question of life, the universe, and everything is &#34;, according to the book &#34;hitchhiker&#39;s guide to the galaxy&#34;, most mordern llms would answer with 42 here, which should validate our entire code! wish me luck :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;logits = torch.matmul(final_embedding[-1], model[&#34;output.weight&#34;].T)&#xA;logits.shape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;torch.Size([128256])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;the model predicted token number 2983 as the next token, is this the token number for 42?&lt;/h3&gt; &#xA;&lt;p&gt;IM HYPING YOU UP, this is the last cell of code, hopefully you had fun :)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;next_token = torch.argmax(logits, dim=-1)&#xA;next_token&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;tensor(2983)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;lets fucking go&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/42.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tokenizer.decode([next_token.item()])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#39;42&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;thank you, i love you :)&lt;/h1&gt; &#xA;&lt;p&gt;This is the end. Hopefully you enjoyed reading it!&lt;/p&gt; &#xA;&lt;p&gt;If you want to support my work&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;follow me on twitter &lt;a href=&#34;https://twitter.com/naklecha&#34;&gt;https://twitter.com/naklecha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;or, buy me a coffee &lt;a href=&#34;https://www.buymeacoffee.com/naklecha&#34;&gt;https://www.buymeacoffee.com/naklecha&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Honestly, if you made it this far you already made my day :)&lt;/p&gt; &#xA;&lt;h2&gt;what motivates me?&lt;/h2&gt; &#xA;&lt;p&gt;My friends and I are on a mission - to make research more accessible! We created a research lab called A10 - &lt;a href=&#34;http://aaaaaaaaaa.org/&#34;&gt;AAAAAAAAAA.org&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;A10 twitter - &lt;a href=&#34;https://twitter.com/aaaaaaaaaaorg&#34;&gt;https://twitter.com/aaaaaaaaaaorg&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;our thesis:&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/naklecha/llama3-from-scratch/main/images/a10.png&#34; width=&#34;600px&#34;&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
</feed>