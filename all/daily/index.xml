<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-22T01:27:23Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>roc-lang/roc</title>
    <updated>2023-11-22T01:27:23Z</updated>
    <id>tag:github.com,2023-11-22:/roc-lang/roc</id>
    <link href="https://github.com/roc-lang/roc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast, friendly, functional language. Work in progress!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Work in progress!&lt;/h1&gt; &#xA;&lt;p&gt;Roc is not ready for a 0.1 release yet, but we do have:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/roc-lang/roc/tree/main/getting_started&#34;&gt;&lt;strong&gt;installation&lt;/strong&gt; guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roc-lang.org/tutorial&#34;&gt;&lt;strong&gt;tutorial&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.roc-lang.org/builtins&#34;&gt;&lt;strong&gt;docs&lt;/strong&gt; for the standard library&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/roc-lang/examples/tree/main/examples&#34;&gt;&lt;strong&gt;examples&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/roc-lang/roc/raw/main/FAQ.md&#34;&gt;&lt;strong&gt;faq&lt;/strong&gt;: frequently asked questions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://roc.zulipchat.com&#34;&gt;&lt;strong&gt;group chat&lt;/strong&gt;&lt;/a&gt; for help, questions and discussions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute, check out &lt;a href=&#34;https://github.com/roc-lang/roc/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22&#34;&gt;good first issues&lt;/a&gt;. Don&#39;t hesitate to ask for help on our &lt;a href=&#34;https://roc.zulipchat.com&#34;&gt;group chat&lt;/a&gt;, we&#39;re friendly!&lt;/p&gt; &#xA;&lt;h2&gt;Sponsors&lt;/h2&gt; &#xA;&lt;p&gt;You can ðŸ’œ &lt;strong&gt;sponsor&lt;/strong&gt; ðŸ’œ Roc on:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/sponsors/roc-lang&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://liberapay.com/roc_lang&#34;&gt;Liberapay&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are very grateful for our corporate sponsors &lt;a href=&#34;https://www.vendr.com/&#34;&gt;Vendr&lt;/a&gt;, &lt;a href=&#34;https://www.rwx.com&#34;&gt;RWX&lt;/a&gt;, and &lt;a href=&#34;https://tweedegolf.nl/en&#34;&gt;Tweede golf&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.vendr.com&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1094080/223597445-81755626-a080-4299-a38c-3c92e7548489.png&#34; height=&#34;60&#34; alt=&#34;Vendr logo&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://www.rwx.com&#34;&gt;&lt;img src=&#34;https://github.com/roc-lang/roc/assets/1094080/82c0868e-d23f-42a0-ac2d-c6e6b2e16575&#34; height=&#34;60&#34; alt=&#34;RWX logo&#34;&gt;&lt;/a&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://tweedegolf.nl/en&#34;&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1094080/183123052-856815b1-8cc9-410a-83b0-589f03613188.svg?sanitize=true&#34; height=&#34;60&#34; alt=&#34;tweede golf logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you would like your company to become a corporate sponsor of Roc&#39;s development, please &lt;a href=&#34;https://roc.zulipchat.com/#narrow/pm-with/281383-user281383&#34;&gt;DM Richard Feldman on Zulip&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;p&gt;We&#39;d also like to express our gratitude to our generous &lt;a href=&#34;https://github.com/sponsors/roc-lang/&#34;&gt;individual sponsors&lt;/a&gt;! A special thanks to those sponsoring $25/month or more:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jamesbirtles&#34;&gt;James Birtles&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ivo-Balbaert&#34;&gt;Ivo Balbaert&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rvcas&#34;&gt;Lucas Rosa&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Ocupe&#34;&gt;Jonas Schell&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/cdolan&#34;&gt;Christopher Dolan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nickgravgaard&#34;&gt;Nick Gravgaard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/aaronwhite&#34;&gt;Aaron White&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/popara&#34;&gt;Zeljko Nesic&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/shritesh&#34;&gt;Shritesh Bhattarai&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rtfeldman&#34;&gt;Richard Feldman&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ayazhafiz&#34;&gt;Ayaz Hafiz&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thank you all so much for helping Roc progress!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>blakeblackshear/frigate</title>
    <updated>2023-11-22T01:27:23Z</updated>
    <id>tag:github.com,2023-11-22:/blakeblackshear/frigate</id>
    <link href="https://github.com/blakeblackshear/frigate" rel="alternate"></link>
    <summary type="html">&lt;p&gt;NVR with realtime local object detection for IP cameras&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; alt=&#34;logo&#34; src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/frigate.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Frigate - NVR With Realtime Object Detection for IP Cameras&lt;/h1&gt; &#xA;&lt;p&gt;A complete and local NVR designed for &lt;a href=&#34;https://www.home-assistant.io&#34;&gt;Home Assistant&lt;/a&gt; with AI object detection. Uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras.&lt;/p&gt; &#xA;&lt;p&gt;Use of a &lt;a href=&#34;https://coral.ai/products/&#34;&gt;Google Coral Accelerator&lt;/a&gt; is optional, but highly recommended. The Coral will outperform even the best CPUs and can process 100+ FPS with very little overhead.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Tight integration with Home Assistant via a &lt;a href=&#34;https://github.com/blakeblackshear/frigate-hass-integration&#34;&gt;custom component&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Designed to minimize resource use and maximize performance by only looking for objects when and where it is necessary&lt;/li&gt; &#xA; &lt;li&gt;Leverages multiprocessing heavily with an emphasis on realtime over processing every frame&lt;/li&gt; &#xA; &lt;li&gt;Uses a very low overhead motion detection to determine where to run object detection&lt;/li&gt; &#xA; &lt;li&gt;Object detection with TensorFlow runs in separate processes for maximum FPS&lt;/li&gt; &#xA; &lt;li&gt;Communicates over MQTT for easy integration into other systems&lt;/li&gt; &#xA; &lt;li&gt;Records video with retention settings based on detected objects&lt;/li&gt; &#xA; &lt;li&gt;24/7 recording&lt;/li&gt; &#xA; &lt;li&gt;Re-streaming via RTSP to reduce the number of connections to your camera&lt;/li&gt; &#xA; &lt;li&gt;WebRTC &amp;amp; MSE support for low-latency live view&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;View the documentation at &lt;a href=&#34;https://docs.frigate.video&#34;&gt;https://docs.frigate.video&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Donations&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to make a donation to support development, please use &lt;a href=&#34;https://github.com/sponsors/blakeblackshear&#34;&gt;Github Sponsors&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots&lt;/h2&gt; &#xA;&lt;p&gt;Integration into Home Assistant&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/media_browser.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/media_browser.png&#34; height=&#34;400&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/notification.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/notification.png&#34; height=&#34;400&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;Also comes with a builtin UI:&lt;/p&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/home-ui.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/home-ui.png&#34; height=&#34;400&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/camera-ui.png&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/camera-ui.png&#34; height=&#34;400&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/blakeblackshear/frigate/dev/docs/static/img/events-ui.png&#34; alt=&#34;Events&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>yl4579/StyleTTS2</title>
    <updated>2023-11-22T01:27:23Z</updated>
    <id>tag:github.com,2023-11-22:/yl4579/StyleTTS2</id>
    <link href="https://github.com/yl4579/StyleTTS2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models&lt;/h1&gt; &#xA;&lt;h3&gt;Yinghao Aaron Li, Cong Han, Vinay S. Raghavan, Gavin Mischler, Nima Mesgarani&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS synthesis on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Paper: &lt;a href=&#34;https://arxiv.org/abs/2306.07691&#34;&gt;https://arxiv.org/abs/2306.07691&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Audio samples: &lt;a href=&#34;https://styletts2.github.io/&#34;&gt;https://styletts2.github.io/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Training and inference demo code for single-speaker models (LJSpeech)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Test training code for multi-speaker models (VCTK and LibriTTS)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Finish demo code for multispeaker model and upload pre-trained models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add a finetuning script for new speakers with base pre-trained multispeaker models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Fix DDP (accelerator) for &lt;code&gt;train_second.py&lt;/code&gt; &lt;strong&gt;(I have tried everything I could to fix this but had no success, so if you are willing to help, please see &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/7&#34;&gt;#7&lt;/a&gt;)&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pre-requisites&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.7&lt;/li&gt; &#xA; &lt;li&gt;Clone this repository:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/yl4579/StyleTTS2.git&#xA;cd StyleTTS2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Install python requirements:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;On Windows add:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -U&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Also install phonemizer and espeak if you want to run the demo:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install phonemizer&#xA;sudo apt-get install espeak-ng&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Download and extract the &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34;&gt;LJSpeech dataset&lt;/a&gt;, unzip to the data folder and upsample the data to 24 kHz. The text aligner and pitch extractor are pre-trained on 24 kHz data, but you can easily change the preprocessing and re-train them using your own preprocessing. For LibriTTS, you will need to combine train-clean-360 with train-clean-100 and rename the folder train-clean-460 (see &lt;a href=&#34;https://github.com/yl4579/StyleTTS/raw/main/Data/val_list_libritts.txt&#34;&gt;val_list_libritts.txt&lt;/a&gt; as an example).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;First stage training:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;accelerate launch train_first.py --config_path ./Configs/config.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Second stage training &lt;strong&gt;(DDP version not working, so the current version uses DP, again see &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/7&#34;&gt;#7&lt;/a&gt; if you want to help)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_second.py --config_path ./Configs/config.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can run both consecutively and it will train both the first and second stages. The model will be saved in the format &#34;epoch_1st_%05d.pth&#34; and &#34;epoch_2nd_%05d.pth&#34;. Checkpoints and Tensorboard logs will be saved at &lt;code&gt;log_dir&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The data list format needs to be &lt;code&gt;filename.wav|transcription|speaker&lt;/code&gt;, see &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/raw/main/Data/val_list.txt&#34;&gt;val_list.txt&lt;/a&gt; as an example. The speaker labels are needed for multi-speaker models because we need to sample reference audio for style diffusion model training.&lt;/p&gt; &#xA;&lt;h3&gt;Important Configurations&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/raw/main/Configs/config.yml&#34;&gt;config.yml&lt;/a&gt;, there are a few important configurations to take care of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;OOD_data&lt;/code&gt;: The path for out-of-distribution texts for SLM adversarial training. The format should be &lt;code&gt;text|anything&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;min_length&lt;/code&gt;: Minimum length of OOD texts for training. This is to make sure the synthesized speech has a minimum length.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;max_len&lt;/code&gt;: Maximum length of audio for training. The unit is frame. Since the default hop size is 300, one frame is approximately &lt;code&gt;300 / 24000&lt;/code&gt; (0.125) second. Lowering this if you encounter the out-of-memory issue.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;multispeaker&lt;/code&gt;: Set to true if you want to train a multispeaker model. This is needed because the architecture of the denoiser is different for single and multispeaker models.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;batch_percentage&lt;/code&gt;: This is to make sure during SLM adversarial training there are no out-of-memory (OOM) issues. If you encounter OOM problem, please set a lower number for this.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Pre-trained modules&lt;/h3&gt; &#xA;&lt;p&gt;In &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/tree/main/Utils&#34;&gt;Utils&lt;/a&gt; folder, there are three pre-trained models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/yl4579/StyleTTS2/tree/main/Utils/ASR&#34;&gt;ASR&lt;/a&gt; folder&lt;/strong&gt;: It contains the pre-trained text aligner, which was pre-trained on English (LibriTTS), Japanese (JVS), and Chinese (AiShell) corpus. It works well for most other languages without fine-tuning, but you can always train your own text aligner with the code here: &lt;a href=&#34;https://github.com/yl4579/AuxiliaryASR&#34;&gt;yl4579/AuxiliaryASR&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/yl4579/StyleTTS2/tree/main/Utils/JDC&#34;&gt;JDC&lt;/a&gt; folder&lt;/strong&gt;: It contains the pre-trained pitch extractor, which was pre-trained on English (LibriTTS) corpus only. However, it works well for other languages too because F0 is independent of language. If you want to train on singing corpus, it is recommended to train a new pitch extractor with the code here: &lt;a href=&#34;https://github.com/yl4579/PitchExtractor&#34;&gt;yl4579/PitchExtractor&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/yl4579/StyleTTS2/tree/main/Utils/PLBERT&#34;&gt;PLBERT&lt;/a&gt; folder&lt;/strong&gt;: It contains the pre-trained &lt;a href=&#34;https://arxiv.org/abs/2301.08810&#34;&gt;PL-BERT&lt;/a&gt; model, which was pre-trained on English (Wikipedia) corpus only. It probably does not work very well on other languages, so you will need to train a different PL-BERT for different languages using the repo here: &lt;a href=&#34;https://github.com/yl4579/PL-BERT&#34;&gt;yl4579/PL-BERT&lt;/a&gt;. You can also replace this module with other phoneme BERT models like &lt;a href=&#34;https://arxiv.org/abs/2305.19709&#34;&gt;XPhoneBERT&lt;/a&gt; which is pre-trained on more than 100 languages.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Common Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Loss becomes NaN&lt;/strong&gt;: If it is the first stage, please make sure you do not use mixed precision, as it can cause loss becoming NaN for some particular datasets when the batch size is not set properly (need to be more than 16 to work well). For the second stage, please also experiment with different batch sizes, with higher batch sizes being more likely to cause NaN loss values. We recommend the batch size to be 16. You can refer to issues &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/10&#34;&gt;#10&lt;/a&gt; and &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/11&#34;&gt;#11&lt;/a&gt; for more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Out of memory&lt;/strong&gt;: Please either use lower &lt;code&gt;batch_size&lt;/code&gt; or &lt;code&gt;max_len&lt;/code&gt;. You may refer to issue &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/10&#34;&gt;#10&lt;/a&gt; for more information.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Finetuning&lt;/h2&gt; &#xA;&lt;p&gt;The script is modified from &lt;code&gt;train_second.py&lt;/code&gt; which uses DP, as DDP does not work for &lt;code&gt;train_second.py&lt;/code&gt;. Please see the bold section above if you are willing to help with this problem.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python train_finetune.py --config_path ./Configs/config_ft.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Please make sure you have the LibriTTS checkpoint downloaded and unzipped under the folder. The default configuration &lt;code&gt;config_ft.yml&lt;/code&gt; finetunes on LJSpeech with 1 hour of speech data (around 1k samples) for 50 epochs. This took about 4 hours to finish on four NVidia A100. The quality is slightly worse (similar to NaturalSpeech on LJSpeech) than LJSpeech model trained from scratch with 24 hours of speech data, which took around 2.5 days to finish on four A100.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Finetune_Demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/raw/main/Demo/Inference_LJSpeech.ipynb&#34;&gt;Inference_LJSpeech.ipynb&lt;/a&gt; (single-speaker) and &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/raw/main/Demo/Inference_LibriTTS.ipynb&#34;&gt;Inference_LibriTTS.ipynb&lt;/a&gt; (multi-speaker) for details. For LibriTTS, you will also need to download &lt;a href=&#34;https://huggingface.co/yl4579/StyleTTS2-LibriTTS/resolve/main/reference_audio.zip&#34;&gt;reference_audio.zip&lt;/a&gt; and unzip it under the &lt;code&gt;demo&lt;/code&gt; before running the demo.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The pretrained StyleTTS 2 on LJSpeech corpus in 24 kHz can be downloaded at &lt;a href=&#34;https://huggingface.co/yl4579/StyleTTS2-LJSpeech/tree/main&#34;&gt;https://huggingface.co/yl4579/StyleTTS2-LJSpeech/tree/main&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Demo_LJSpeech.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The pretrained StyleTTS 2 model on LibriTTS can be downloaded at &lt;a href=&#34;https://huggingface.co/yl4579/StyleTTS2-LibriTTS/tree/main&#34;&gt;https://huggingface.co/yl4579/StyleTTS2-LibriTTS/tree/main&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/yl4579/StyleTTS2/blob/main/Colab/StyleTTS2_Demo_LibriTTS.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;em&gt;&lt;strong&gt;Before using these pre-trained models, you agree to inform the listeners that the speech samples are synthesized by the pre-trained models, unless you have the permission to use the voice you synthesize. That is, you agree to only use voices whose speakers grant the permission to have their voice cloned, either directly or by license before making synthesized voices public, or you have to publicly announce that these voices are synthesized if you do not have the permission to use these voices.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Common Issues&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;High-pitched background noise&lt;/strong&gt;: This is caused by numerical float differences in older GPUs. For more details, please refer to issue &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/13&#34;&gt;#13&lt;/a&gt;. Basically, you will need to use more modern GPUs or do inference on CPUs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pre-trained model license&lt;/strong&gt;: You only need to abide by the above rules if you use &lt;strong&gt;the pre-trained models&lt;/strong&gt; and the voices are &lt;strong&gt;NOT&lt;/strong&gt; in the training set, i.e., your reference speakers are not from any open access dataset. For more details of rules to use the pre-trained models, please see &lt;a href=&#34;https://github.com/yl4579/StyleTTS2/issues/37&#34;&gt;#37&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/archinetai/audio-diffusion-pytorch&#34;&gt;archinetai/audio-diffusion-pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jik876/hifi-gan&#34;&gt;jik876/hifi-gan&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rishikksh20/iSTFTNet-pytorch&#34;&gt;rishikksh20/iSTFTNet-pytorch&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/project/01-nsf&#34;&gt;nii-yamagishilab/project-NN-Pytorch-scripts/project/01-nsf&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>