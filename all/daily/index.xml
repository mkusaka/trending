<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-24T01:25:58Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>LLM-Red-Team/kimi-free-api</title>
    <updated>2024-04-24T01:25:58Z</updated>
    <id>tag:github.com,2024-04-24:/LLM-Red-Team/kimi-free-api</id>
    <link href="https://github.com/LLM-Red-Team/kimi-free-api" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🚀 KIMI AI 长文本大模型逆向API白嫖服务【特长：长文本解读整理】，支持高速流式输出、联网搜索、长文档解读、图像解析、多轮对话，零配置部署，多路token支持，自动清理会话痕迹。&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;KIMI AI Free 服务&lt;/h1&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;span&gt;[ 中文 | &lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/README_EN.md&#34;&gt;English&lt;/a&gt; ]&lt;/span&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.shields.io/github/license/llm-red-team/kimi-free-api.svg?sanitize=true&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/llm-red-team/kimi-free-api.svg?sanitize=true&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/forks/llm-red-team/kimi-free-api.svg?sanitize=true&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://img.shields.io/docker/pulls/vinlic/kimi-free-api.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;支持高速流式输出、支持多轮对话、支持联网搜索、支持长文档解读、支持图像解析，零配置部署，多路token支持，自动清理会话痕迹。&lt;/p&gt; &#xA;&lt;p&gt;与ChatGPT接口完全兼容。&lt;/p&gt; &#xA;&lt;p&gt;还有以下五个free-api欢迎关注：&lt;/p&gt; &#xA;&lt;p&gt;阶跃星辰 (跃问StepChat) 接口转API &lt;a href=&#34;https://github.com/LLM-Red-Team/step-free-api&#34;&gt;step-free-api&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;阿里通义 (Qwen) 接口转API &lt;a href=&#34;https://github.com/LLM-Red-Team/qwen-free-api&#34;&gt;qwen-free-api&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ZhipuAI (智谱清言) 接口转API &lt;a href=&#34;https://github.com/LLM-Red-Team/glm-free-api&#34;&gt;glm-free-api&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;秘塔AI (metaso) 接口转API &lt;a href=&#34;https://github.com/LLM-Red-Team/metaso-free-api&#34;&gt;metaso-free-api&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;聆心智能 (Emohaa) 接口转API &lt;a href=&#34;https://github.com/LLM-Red-Team/emohaa-free-api&#34;&gt;emohaa-free-api&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;目录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E5%85%8D%E8%B4%A3%E5%A3%B0%E6%98%8E&#34;&gt;免责声明&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E5%9C%A8%E7%BA%BF%E4%BD%93%E9%AA%8C&#34;&gt;在线体验&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E6%95%88%E6%9E%9C%E7%A4%BA%E4%BE%8B&#34;&gt;效果示例&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E6%8E%A5%E5%85%A5%E5%87%86%E5%A4%87&#34;&gt;接入准备&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E5%A4%9A%E8%B4%A6%E5%8F%B7%E6%8E%A5%E5%85%A5&#34;&gt;多账号接入&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#Docker%E9%83%A8%E7%BD%B2&#34;&gt;Docker部署&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#Docker-compose%E9%83%A8%E7%BD%B2&#34;&gt;Docker-compose部署&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#Render%E9%83%A8%E7%BD%B2&#34;&gt;Render部署&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#Vercel%E9%83%A8%E7%BD%B2&#34;&gt;Vercel部署&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E5%8E%9F%E7%94%9F%E9%83%A8%E7%BD%B2&#34;&gt;原生部署&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E6%8E%A5%E5%8F%A3%E5%88%97%E8%A1%A8&#34;&gt;接口列表&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E5%AF%B9%E8%AF%9D%E8%A1%A5%E5%85%A8&#34;&gt;对话补全&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E6%96%87%E6%A1%A3%E8%A7%A3%E8%AF%BB&#34;&gt;文档解读&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E5%9B%BE%E5%83%8F%E8%A7%A3%E6%9E%90&#34;&gt;图像解析&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#refresh_token%E5%AD%98%E6%B4%BB%E6%A3%80%E6%B5%8B&#34;&gt;refresh_token存活检测&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9&#34;&gt;注意事项&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/#Nginx%E5%8F%8D%E4%BB%A3%E4%BC%98%E5%8C%96&#34;&gt;Nginx反代优化&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;逆向API是不稳定的，建议前往MoonshotAI官方 &lt;a href=&#34;https://platform.moonshot.cn/&#34;&gt;https://platform.moonshot.cn/&lt;/a&gt; 付费使用API，避免封禁的风险。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;本组织和个人不接受任何资金捐助和交易，此项目是纯粹研究交流学习性质！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;仅限自用，禁止对外提供服务或商用，避免对官方造成服务压力，否则风险自担！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;仅限自用，禁止对外提供服务或商用，避免对官方造成服务压力，否则风险自担！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;仅限自用，禁止对外提供服务或商用，避免对官方造成服务压力，否则风险自担！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;在线体验&lt;/h2&gt; &#xA;&lt;p&gt;此链接仅临时测试功能，不可长期使用，长期使用请自行部署。&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://udify.app/chat/Po0F6BMJ15q5vu2P&#34;&gt;https://udify.app/chat/Po0F6BMJ15q5vu2P&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;效果示例&lt;/h2&gt; &#xA;&lt;h3&gt;验明正身Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/doc/example-1.png&#34; alt=&#34;验明正身&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;多轮对话Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/doc/example-6.png&#34; alt=&#34;多轮对话&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;联网搜索Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/doc/example-2.png&#34; alt=&#34;联网搜索&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;长文档解读Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/doc/example-5.png&#34; alt=&#34;长文档解读&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;图像解析Demo&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/doc/example-3.png&#34; alt=&#34;图像解析&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;响应流畅度一致&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/LLM-Red-Team/kimi-free-api/assets/20235341/48c7ec00-2b03-46c4-95d0-452d3075219b&#34; alt=&#34;响应流畅度一致&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;接入准备&lt;/h2&gt; &#xA;&lt;p&gt;从 &lt;a href=&#34;https://kimi.moonshot.cn&#34;&gt;kimi.moonshot.cn&lt;/a&gt; 获取refresh_token&lt;/p&gt; &#xA;&lt;p&gt;进入kimi随便发起一个对话，然后F12打开开发者工具，从Application &amp;gt; Local Storage中找到&lt;code&gt;refresh_token&lt;/code&gt;的值，这将作为Authorization的Bearer Token值：&lt;code&gt;Authorization: Bearer TOKEN&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/doc/example-0.png&#34; alt=&#34;example0&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果你看到的&lt;code&gt;refresh_token&lt;/code&gt;是一个数组，请使用&lt;code&gt;.&lt;/code&gt;拼接起来再使用。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/LLM-Red-Team/kimi-free-api/master/doc/example-8.jpg&#34; alt=&#34;example8&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;多账号接入&lt;/h3&gt; &#xA;&lt;p&gt;目前kimi限制普通账号每3小时内只能进行30轮长文本的问答（短文本不限），你可以通过提供多个账号的refresh_token并使用&lt;code&gt;,&lt;/code&gt;拼接提供：&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Authorization: Bearer TOKEN1,TOKEN2,TOKEN3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;每次请求服务会从中挑选一个。&lt;/p&gt; &#xA;&lt;h2&gt;Docker部署&lt;/h2&gt; &#xA;&lt;p&gt;请准备一台具有公网IP的服务器并将8000端口开放。&lt;/p&gt; &#xA;&lt;p&gt;拉取镜像并启动服务&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -it -d --init --name kimi-free-api -p 8000:8000 -e TZ=Asia/Shanghai vinlic/kimi-free-api:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;查看服务实时日志&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker logs -f kimi-free-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;重启服务&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker restart kimi-free-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;停止服务&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker stop kimi-free-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Docker-compose部署&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;version: &#39;3&#39;&#xA;&#xA;services:&#xA;  kimi-free-api:&#xA;    container_name: kimi-free-api&#xA;    image: vinlic/kimi-free-api:latest&#xA;    restart: always&#xA;    ports:&#xA;      - &#34;8000:8000&#34;&#xA;    environment:&#xA;      - TZ=Asia/Shanghai&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Render部署&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意：部分部署区域可能无法连接kimi，如容器日志出现请求超时或无法连接（新加坡实测不可用）请切换其他区域部署！&lt;/strong&gt; &lt;strong&gt;注意：免费账户的容器实例将在一段时间不活动时自动停止运行，这会导致下次请求时遇到50秒或更长的延迟，建议查看&lt;a href=&#34;https://github.com/LLM-Red-Team/free-api-hub/#Render%E5%AE%B9%E5%99%A8%E4%BF%9D%E6%B4%BB&#34;&gt;Render容器保活&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;fork本项目到你的github账号下。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;访问 &lt;a href=&#34;https://dashboard.render.com/&#34;&gt;Render&lt;/a&gt; 并登录你的github账号。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;构建你的 Web Service（New+ -&amp;gt; Build and deploy from a Git repository -&amp;gt; Connect你fork的项目 -&amp;gt; 选择部署区域 -&amp;gt; 选择实例类型为Free -&amp;gt; Create Web Service）。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;等待构建完成后，复制分配的域名并拼接URL访问即可。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Vercel部署&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意：Vercel免费账户的请求响应超时时间为10秒，但接口响应通常较久，可能会遇到Vercel返回的504超时错误！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;请先确保安装了Node.js环境。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm i -g vercel --registry http://registry.npmmirror.com&#xA;vercel login&#xA;git clone https://github.com/LLM-Red-Team/kimi-free-api&#xA;cd kimi-free-api&#xA;vercel --prod&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Zeabur部署&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;注意：免费账户的容器实例可能无法稳定运行&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://zeabur.com/templates/GRFYBP&#34;&gt;&lt;img src=&#34;https://zeabur.com/button.svg?sanitize=true&#34; alt=&#34;Deploy on Zeabur&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;原生部署&lt;/h2&gt; &#xA;&lt;p&gt;请准备一台具有公网IP的服务器并将8000端口开放。&lt;/p&gt; &#xA;&lt;p&gt;请先安装好Node.js环境并且配置好环境变量，确认node命令可用。&lt;/p&gt; &#xA;&lt;p&gt;安装依赖&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm i&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;安装PM2进行进程守护&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm i -g pm2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;编译构建，看到dist目录就是构建完成&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;启动服务&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pm2 start dist/index.js --name &#34;kimi-free-api&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;查看服务实时日志&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pm2 logs kimi-free-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;重启服务&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pm2 reload kimi-free-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;停止服务&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pm2 stop kimi-free-api&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;接口列表&lt;/h2&gt; &#xA;&lt;p&gt;目前支持与openai兼容的 &lt;code&gt;/v1/chat/completions&lt;/code&gt; 接口，可自行使用与openai或其他兼容的客户端接入接口，或者使用 &lt;a href=&#34;https://dify.ai/&#34;&gt;dify&lt;/a&gt; 等线上服务接入使用。&lt;/p&gt; &#xA;&lt;h3&gt;对话补全&lt;/h3&gt; &#xA;&lt;p&gt;对话补全接口，与openai的 &lt;a href=&#34;https://platform.openai.com/docs/guides/text-generation/chat-completions-api&#34;&gt;chat-completions-api&lt;/a&gt; 兼容。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;POST /v1/chat/completions&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;header 需要设置 Authorization 头部：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Authorization: Bearer [refresh_token]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;请求数据：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    // 模型名称随意填写，如果不希望输出检索过程模型名称请包含silent_search&#xA;    &#34;model&#34;: &#34;kimi&#34;,&#xA;    &#34;messages&#34;: [&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: &#34;测试&#34;&#xA;        }&#xA;    ],&#xA;    // 是否开启联网搜索，默认false&#xA;    &#34;use_search&#34;: true,&#xA;    // 如果使用SSE流请设置为true，默认false&#xA;    &#34;stream&#34;: false&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;响应数据：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;id&#34;: &#34;cnndivilnl96vah411dg&#34;,&#xA;    &#34;model&#34;: &#34;kimi&#34;,&#xA;    &#34;object&#34;: &#34;chat.completion&#34;,&#xA;    &#34;choices&#34;: [&#xA;        {&#xA;            &#34;index&#34;: 0,&#xA;            &#34;message&#34;: {&#xA;                &#34;role&#34;: &#34;assistant&#34;,&#xA;                &#34;content&#34;: &#34;你好！我是Kimi，由月之暗面科技有限公司开发的人工智能助手。我擅长中英文对话，可以帮助你获取信息、解答疑问，还能阅读和理解你提供的文件和网页内容。如果你有任何问题或需要帮助，随时告诉我！&#34;&#xA;            },&#xA;            &#34;finish_reason&#34;: &#34;stop&#34;&#xA;        }&#xA;    ],&#xA;    &#34;usage&#34;: {&#xA;        &#34;prompt_tokens&#34;: 1,&#xA;        &#34;completion_tokens&#34;: 1,&#xA;        &#34;total_tokens&#34;: 2&#xA;    },&#xA;    &#34;created&#34;: 1710152062&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;文档解读&lt;/h3&gt; &#xA;&lt;p&gt;提供一个可访问的文件URL或者BASE64_URL进行解析。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;POST /v1/chat/completions&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;header 需要设置 Authorization 头部：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Authorization: Bearer [refresh_token]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;请求数据：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    // 模型名称随意填写，如果不希望输出检索过程模型名称请包含silent_search&#xA;    &#34;model&#34;: &#34;kimi&#34;,&#xA;    &#34;messages&#34;: [&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: [&#xA;                {&#xA;                    &#34;type&#34;: &#34;file&#34;,&#xA;                    &#34;file_url&#34;: {&#xA;                        &#34;url&#34;: &#34;https://mj101-1317487292.cos.ap-shanghai.myqcloud.com/ai/test.pdf&#34;&#xA;                    }&#xA;                },&#xA;                {&#xA;                    &#34;type&#34;: &#34;text&#34;,&#xA;                    &#34;text&#34;: &#34;文档里说了什么？&#34;&#xA;                }&#xA;            ]&#xA;        }&#xA;    ],&#xA;    // 建议关闭联网搜索，防止干扰解读结果&#xA;    &#34;use_search&#34;: false&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;响应数据：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;id&#34;: &#34;cnmuo7mcp7f9hjcmihn0&#34;,&#xA;    &#34;model&#34;: &#34;kimi&#34;,&#xA;    &#34;object&#34;: &#34;chat.completion&#34;,&#xA;    &#34;choices&#34;: [&#xA;        {&#xA;            &#34;index&#34;: 0,&#xA;            &#34;message&#34;: {&#xA;                &#34;role&#34;: &#34;assistant&#34;,&#xA;                &#34;content&#34;: &#34;文档中包含了几个古代魔法咒语的例子，这些咒语来自古希腊和罗马时期的魔法文本，被称为PGM（Papyri Graecae Magicae）。以下是文档中提到的几个咒语的内容：\n\n1. 第一个咒语（PMG 4.1390 – 1495）描述了一个仪式，要求留下一些你吃剩的面包，将其分成七块小片，然后去到英雄、角斗士和那些死于非命的人被杀的地方。对面包片念咒并扔出去，然后从仪式地点捡起一些被污染的泥土扔进你心仪的女人的家中，之后去睡觉。咒语的内容是向命运女神（Moirai）、罗马的命运女神（Fates）和自然力量（Daemons）祈求，希望他们帮助实现愿望。\n\n2. 第二个咒语（PMG 4.1342 – 57）是一个召唤咒语，通过念出一系列神秘的名字和词语来召唤一个名为Daemon的存在，以使一个名为Tereous的人（由Apia所生）受到精神和情感上的折磨，直到她来到施法者Didymos（由Taipiam所生）的身边。\n\n3. 第三个咒语（PGM 4.1265 – 74）提到了一个名为NEPHERIĒRI的神秘名字，这个名字与爱神阿佛洛狄忒（Aphrodite）有关。为了赢得一个美丽女人的心，需要保持三天的纯洁，献上乳香，并在献祭时念出这个名字。然后，在接近那位女士时，心中默念这个名字七次，连续七天这样做，以期成功。\n\n4. 第四个咒语（PGM 4.1496 – 1）描述了在燃烧没药（myrrh）时念诵的咒语。这个咒语是向没药祈祷，希望它能够像“肉食者”和“心灵点燃者”一样，吸引一个名为[名字]的女人（她的母亲名为[名字]），让她无法安坐、饮食、注视或亲吻其他人，而是让她的心中只有施法者，直到她来到施法者身边。\n\n这些咒语反映了古代人们对魔法和超自然力量的信仰，以及他们试图通过这些咒语来影响他人情感和行为的方式。&#34;&#xA;            },&#xA;            &#34;finish_reason&#34;: &#34;stop&#34;&#xA;        }&#xA;    ],&#xA;    &#34;usage&#34;: {&#xA;        &#34;prompt_tokens&#34;: 1,&#xA;        &#34;completion_tokens&#34;: 1,&#xA;        &#34;total_tokens&#34;: 2&#xA;    },&#xA;    &#34;created&#34;: 100920&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;图像解析&lt;/h3&gt; &#xA;&lt;p&gt;提供一个可访问的图像URL或者BASE64_URL进行解析。&lt;/p&gt; &#xA;&lt;p&gt;此格式兼容 &lt;a href=&#34;https://platform.openai.com/docs/guides/vision&#34;&gt;gpt-4-vision-preview&lt;/a&gt; API格式，您也可以用这个格式传送文档进行解析。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;POST /v1/chat/completions&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;header 需要设置 Authorization 头部：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Authorization: Bearer [refresh_token]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;请求数据：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    // 模型名称随意填写，如果不希望输出检索过程模型名称请包含silent_search&#xA;    &#34;model&#34;: &#34;kimi&#34;,&#xA;    &#34;messages&#34;: [&#xA;        {&#xA;            &#34;role&#34;: &#34;user&#34;,&#xA;            &#34;content&#34;: [&#xA;                {&#xA;                    &#34;type&#34;: &#34;image_url&#34;,&#xA;                    &#34;image_url&#34;: {&#xA;                        &#34;url&#34;: &#34;https://www.moonshot.cn/assets/logo/normal-dark.png&#34;&#xA;                    }&#xA;                },&#xA;                {&#xA;                    &#34;type&#34;: &#34;text&#34;,&#xA;                    &#34;text&#34;: &#34;图像描述了什么？&#34;&#xA;                }&#xA;            ]&#xA;        }&#xA;    ],&#xA;    // 建议关闭联网搜索，防止干扰解读结果&#xA;    &#34;use_search&#34;: false&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;响应数据：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;id&#34;: &#34;cnn6l8ilnl92l36tu8ag&#34;,&#xA;    &#34;model&#34;: &#34;kimi&#34;,&#xA;    &#34;object&#34;: &#34;chat.completion&#34;,&#xA;    &#34;choices&#34;: [&#xA;        {&#xA;            &#34;index&#34;: 0,&#xA;            &#34;message&#34;: {&#xA;                &#34;role&#34;: &#34;assistant&#34;,&#xA;                &#34;content&#34;: &#34;图像中展示了“Moonshot AI”的字样，这可能是月之暗面科技有限公司（Moonshot AI）的标志或者品牌标识。通常这样的图像用于代表公司或产品，传达品牌信息。由于图像是PNG格式，它可能是一个透明背景的logo，用于网站、应用程序或其他视觉材料中。&#34;&#xA;            },&#xA;            &#34;finish_reason&#34;: &#34;stop&#34;&#xA;        }&#xA;    ],&#xA;    &#34;usage&#34;: {&#xA;        &#34;prompt_tokens&#34;: 1,&#xA;        &#34;completion_tokens&#34;: 1,&#xA;        &#34;total_tokens&#34;: 2&#xA;    },&#xA;    &#34;created&#34;: 1710123627&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;refresh_token存活检测&lt;/h3&gt; &#xA;&lt;p&gt;检测refresh_token是否存活，如果存活live为true，否则为false，请不要频繁（小于10分钟）调用此接口。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;POST /token/check&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;请求数据：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;token&#34;: &#34;eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9...&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;响应数据：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;    &#34;live&#34;: true&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;注意事项&lt;/h2&gt; &#xA;&lt;h3&gt;Nginx反代优化&lt;/h3&gt; &#xA;&lt;p&gt;如果您正在使用Nginx反向代理kimi-free-api，请添加以下配置项优化流的输出效果，优化体验感。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-nginx&#34;&gt;# 关闭代理缓冲。当设置为off时，Nginx会立即将客户端请求发送到后端服务器，并立即将从后端服务器接收到的响应发送回客户端。&#xA;proxy_buffering off;&#xA;# 启用分块传输编码。分块传输编码允许服务器为动态生成的内容分块发送数据，而不需要预先知道内容的大小。&#xA;chunked_transfer_encoding on;&#xA;# 开启TCP_NOPUSH，这告诉Nginx在数据包发送到客户端之前，尽可能地发送数据。这通常在sendfile使用时配合使用，可以提高网络效率。&#xA;tcp_nopush on;&#xA;# 开启TCP_NODELAY，这告诉Nginx不延迟发送数据，立即发送小数据包。在某些情况下，这可以减少网络的延迟。&#xA;tcp_nodelay on;&#xA;# 设置保持连接的超时时间，这里设置为120秒。如果在这段时间内，客户端和服务器之间没有进一步的通信，连接将被关闭。&#xA;keepalive_timeout 120;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Token统计&lt;/h3&gt; &#xA;&lt;p&gt;由于推理侧不在kimi-free-api，因此token不可统计，将以固定数字返回!!!!!&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#LLM-Red-Team/kimi-free-api&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=LLM-Red-Team/kimi-free-api&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>LlamaFamily/Llama-Chinese</title>
    <updated>2024-04-24T01:25:58Z</updated>
    <id>tag:github.com,2024-04-24:/LlamaFamily/Llama-Chinese</id>
    <link href="https://github.com/LlamaFamily/Llama-Chinese" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Llama中文社区，Llama3在线体验和微调模型已开放，实时汇总最新Llama3学习资料，已将所有代码更新适配Llama3，构建最好的中文Llama大模型，完全开源可商用&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/README_EN.md&#34;&gt;English&lt;/a&gt; ｜ 中文 &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt; Llama中文社区 &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/assets/llama.jpg&#34; alt=&#34;Llama&#34; style=&#34;width: 20%; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;font face=&#34;黑体&#34; color=&#34;orange&#34; size=&#34;6&#34;&gt; Llama3体验和微调已开放，最好的中文Llama大模型 &lt;/font&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 🤗 &lt;a href=&#34;https://huggingface.co/FlagAlpha&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt; • 🤖 &lt;a href=&#34;https://www.modelscope.cn/organization/FlagAlpha/&#34; target=&#34;_blank&#34;&gt;ModelScope&lt;/a&gt; • ✡️ &lt;a href=&#34;https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat&#34; target=&#34;_blank&#34;&gt;WiseModel&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://llama.family&#34;&gt;Llama3 在线体验（包含Llama2）：https://llama.family&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://huggingface.co/FlagAlpha/Atom-7B-Chat&#34;&gt;基于Llama2的开源中文预训练大模型Atom-7B&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🗂️ 目录&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-llama%E4%B8%AD%E6%96%87%E7%A4%BE%E5%8C%BA&#34;&gt;📌 Llama中文社区&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E7%A4%BE%E5%8C%BA%E4%BB%8B%E7%BB%8Dllama%E4%B8%AD%E6%96%87%E7%A4%BE%E5%8C%BA&#34;&gt;🔥 社区介绍：Llama中文社区&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%9C%80%E6%96%B0%E5%8A%A8%E6%80%81&#34;&gt;📢 最新动态&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%A8%A1%E5%9E%8B&#34;&gt;🤗 模型&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8Batom&#34;&gt;🤗 中文预训练模型Atom-7B&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama3%E5%AE%98%E6%96%B9%E6%A8%A1%E5%9E%8B&#34;&gt;🤗 Llama3官方模型&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama3%E4%B8%AD%E6%96%87%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B&#34;&gt;🤗 Llama3中文微调模型&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama2%E5%AE%98%E6%96%B9%E6%A8%A1%E5%9E%8B&#34;&gt;🤗 Llama2官方模型&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama2%E4%B8%AD%E6%96%87%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B&#34;&gt;🤗 Llama2中文微调模型&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E7%A4%BE%E5%8C%BA%E8%B5%84%E6%BA%90&#34;&gt;🌟 社区资源&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8llama%E6%A8%A1%E5%9E%8B&#34;&gt;📌 如何使用Llama模型?&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B-%E4%BD%BF%E7%94%A8anaconda&#34;&gt;快速上手-使用Anaconda&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B-%E4%BD%BF%E7%94%A8docker&#34;&gt;快速上手-使用Docker&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B-%E4%BD%BF%E7%94%A8llamacpp&#34;&gt;快速上手-使用llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B-%E4%BD%BF%E7%94%A8gradio&#34;&gt;快速上手-使用gradio&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B-%E6%9E%84%E5%BB%BAapi%E6%9C%8D%E5%8A%A1&#34;&gt;快速上手-构建API服务&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83&#34;&gt;🤖 模型预训练&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83&#34;&gt;💡 模型微调&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#step1-%E7%8E%AF%E5%A2%83%E5%87%86%E5%A4%87&#34;&gt;Step1: 环境准备&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#step2-%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87&#34;&gt;Step2: 数据准备&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#step3-%E5%BE%AE%E8%B0%83%E8%84%9A%E6%9C%AC&#34;&gt;Step3: 微调脚本&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#lora%E5%BE%AE%E8%B0%83&#34;&gt;LoRA微调&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E5%85%A8%E9%87%8F%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83&#34;&gt;全量参数微调&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#step4-%E5%8A%A0%E8%BD%BD%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B&#34;&gt;Step4: 加载微调模型&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#lora%E5%BE%AE%E8%B0%83-1&#34;&gt;LoRA微调&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E5%85%A8%E9%87%8F%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83-1&#34;&gt;全量参数微调&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96&#34;&gt;🍄 模型量化&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E9%83%A8%E7%BD%B2%E5%8A%A0%E9%80%9F&#34;&gt;🚀 部署加速&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#tensorrt-llm&#34;&gt;TensorRT-LLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#vllm&#34;&gt;vLLM&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#jittorllms&#34;&gt;JittorLLMs&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#lmdeploy&#34;&gt;lmdeploy&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E5%A4%96%E5%BB%B6%E8%83%BD%E5%8A%9B&#34;&gt;💪 外延能力&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#langchain&#34;&gt;LangChain&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B&#34;&gt;🥇 模型评测&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama2%E5%92%8Cllama3%E5%AF%B9%E6%AF%94%E8%AF%84%E6%B5%8B&#34;&gt;Llama2和Llama3对比评测&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama3%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B&#34;&gt;Llama3模型评测&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama2%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B&#34;&gt;Llama2模型评测&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%BF%83&#34;&gt;📖 学习中心&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama3&#34;&gt;Llama3&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama2&#34;&gt;Llama2&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#meta%E5%AE%98%E6%96%B9%E5%AF%B9%E4%BA%8Ellama2%E7%9A%84%E4%BB%8B%E7%BB%8D&#34;&gt;Meta官方对于Llama2的介绍&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama%E7%9B%B8%E5%85%B3%E8%AE%BA%E6%96%87&#34;&gt;Llama相关论文&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E5%85%B6%E5%AE%83&#34;&gt;📌 其它&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E8%87%B4%E8%B0%A2&#34;&gt;🎉 致谢&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E9%97%AE%E9%A2%98%E5%8F%8D%E9%A6%88&#34;&gt;🤔 问题反馈&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📌 Llama中文社区&lt;/h2&gt; &#xA;&lt;h3&gt;🔥 社区介绍：llama中文社区&lt;/h3&gt; &#xA;&lt;p&gt;欢迎来到Llama中文社区！我们是一个专注于Llama模型在中文方面的优化和上层建设的高级技术社区。 &lt;strong&gt;已经基于大规模中文数据，从预训练开始对Llama2模型进行中文能力的持续迭代升级【Done】&lt;/strong&gt;。&lt;strong&gt;正在对Llama3模型进行中文能力的持续迭代升级【Doing】&lt;/strong&gt; 我们热忱欢迎对大模型LLM充满热情的开发者和研究者加入我们的行列。&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;h4&gt;为什么选择Llama中文社区？&lt;/h4&gt; &#xA; &lt;p&gt;🚀 &lt;strong&gt;高级工程师团队支持&lt;/strong&gt;：社区有一批专注为大家服务的NLP高级工程师，我们有着强大的技术支持和丰富的经验，为您提供专业的指导和帮助。&lt;/p&gt; &#xA; &lt;p&gt;🎯 &lt;strong&gt;中文优化&lt;/strong&gt;：我们致力于在Llama模型的中文处理方面进行优化，探索适用于中文的最佳实践，以提升其性能和适应性【支持Llama2、Llama3】。&lt;/p&gt; &#xA; &lt;p&gt;💡 &lt;strong&gt;创新交流&lt;/strong&gt;：我们拥有一支富有创造力和经验的社区成员团队，定期组织线上活动、技术研讨和经验分享，促进成员间的创新交流。&lt;/p&gt; &#xA; &lt;p&gt;🌐 &lt;strong&gt;全球联结&lt;/strong&gt;：我们欢迎来自世界各地的开发者加入社区，构建一个开放、多元化的学习和交流平台。&lt;/p&gt; &#xA; &lt;p&gt;🤝 &lt;strong&gt;开放共享&lt;/strong&gt;：我们鼓励社区成员开源分享代码和模型，推动合作共赢，共同促进中文NLP技术的发展。&lt;/p&gt; &#xA; &lt;h4&gt;社区活动&lt;/h4&gt; &#xA; &lt;p&gt;🗓️ &lt;strong&gt;线上讲座&lt;/strong&gt;：邀请行业内专家进行线上讲座，分享Llama在中文NLP领域的最新技术和应用，探讨前沿研究成果。&lt;/p&gt; &#xA; &lt;p&gt;💻 &lt;strong&gt;项目展示&lt;/strong&gt;：成员可展示自己在Llama中文优化方面的项目成果，获得反馈和建议，促进项目协作。&lt;/p&gt; &#xA; &lt;p&gt;📚 &lt;strong&gt;学习资源&lt;/strong&gt;：社区维护丰富的学习资料库，包括教程、文档和论文解读，为成员提供全面的学习支持。&lt;/p&gt; &#xA; &lt;p&gt;📝 &lt;strong&gt;论文解读&lt;/strong&gt;：社区成员共同解读与Llama相关的最新研究论文，深入理解前沿算法和方法。&lt;/p&gt; &#xA; &lt;p&gt;🎉 &lt;strong&gt;主题活动&lt;/strong&gt;：定期举办各类主题活动，包括挑战赛、黑客马拉松和技术沙龙，让社区成员在轻松愉快的氛围中交流和学习。&lt;/p&gt; &#xA; &lt;p&gt;🌟 &lt;strong&gt;奖励计划&lt;/strong&gt;：我们设立奖励计划，对社区中积极参与、贡献优秀的成员给予荣誉和奖励，激励更多优秀人才的加入。&lt;/p&gt; &#xA; &lt;p&gt;📈 &lt;strong&gt;技术咨询&lt;/strong&gt;：我们提供技术咨询服务，解答您在Llama开发和优化过程中遇到的问题，助您快速攻克难关。&lt;/p&gt; &#xA; &lt;p&gt;🚀 &lt;strong&gt;项目合作&lt;/strong&gt;：鼓励成员间的项目合作，共同探索Llama在实际应用中的潜力，打造创新解决方案。&lt;/p&gt; &#xA; &lt;h4&gt;立即加入我们！&lt;/h4&gt; &#xA; &lt;p&gt;📚 &lt;strong&gt;愿景&lt;/strong&gt;：无论您是对Llama已有研究和应用经验的专业开发者，还是对Llama中文优化感兴趣并希望深入探索的新手，我们都热切期待您的加入。在Llama中文社区，您将有机会与行业内顶尖人才共同交流，携手推动中文NLP技术的进步，开创更加美好的技术未来！&lt;/p&gt; &#xA; &lt;p&gt;🔗 &lt;strong&gt;温馨提示&lt;/strong&gt;：本社区为专业技术交流平台，我们热切期望志同道合的开发者和研究者加入。请遵守社区准则，共同维护积极向上的学习氛围。感谢您的理解和支持！&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;📢 最新动态&lt;/h3&gt; &#xA;&lt;p&gt;【最新】2024年04月23日：社区增加了llama3 8B中文微调模型&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese?tab=readme-ov-file#llama3%E4%B8%AD%E6%96%87%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B&#34;&gt;Llama3-Chinese-8B-Instruct&lt;/a&gt;以及对应的&lt;a href=&#34;https://llama.family/docs/chat-completion-v1&#34;&gt;免费API调用&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年04月19日：社区增加了llama3 8B、llama3 70B&lt;a href=&#34;https://llama.family/chat/#/&#34;&gt;在线体验链接&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年04月14日：社区更新了四个专家角色：心理咨询师、羊驼夸夸 、律师、医生。链接：&lt;a href=&#34;https://llama.family/tools/#/agent&#34;&gt;角色role&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年04月10日：Atom-7B-Chat 模型回答内容相较之前更为丰富、增强了模型的指令遵循能力和回答稳定性、优化了ppo的奖励模型。下载链接&lt;a href=&#34;https://modelscope.cn/models/FlagAlpha/Atom-7B-Chat&#34;&gt;modelscope&lt;/a&gt;、&lt;a href=&#34;https://huggingface.co/FlagAlpha/Atom-7B-Chat&#34;&gt;Huggingface&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年04月01日：社区上线了Llama中文&lt;a href=&#34;https://llama.family/store&#34;&gt;应用平台&lt;/a&gt;；同时如果你有优秀的的应用需要推广可以填写&lt;a href=&#34;https://atomecho.feishu.cn/share/base/form/shrcnFqpN71OmBoXDCT6y0TQgIc&#34;&gt;申请表&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年03月08日：开放了免费API供大家使用，包含（Atom-1B,7B,13B 3种中文大模型）&lt;a href=&#34;https://llama.family/docs/chat-completion-v1&#34;&gt;API使用链接&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年04月14日：社区更新了四个专家角色：心理咨询师、羊驼夸夸 、律师、医生。链接：&lt;a href=&#34;https://llama.family/tools/#/agent&#34;&gt;角色role&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年04月10日：Atom-7B-Chat 模型回答内容相较之前更为丰富、增强了模型的指令遵循能力和回答稳定性、优化了ppo的奖励模型。下载链接&lt;a href=&#34;https://modelscope.cn/models/FlagAlpha/Atom-7B-Chat&#34;&gt;modelscope&lt;/a&gt;、&lt;a href=&#34;https://huggingface.co/FlagAlpha/Atom-7B-Chat&#34;&gt;Huggingface&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年04月01日：社区上线了Llama中文&lt;a href=&#34;https://llama.family/store&#34;&gt;应用平台&lt;/a&gt;；同时如果你有优秀的的应用需要推广可以填写&lt;a href=&#34;https://atomecho.feishu.cn/share/base/form/shrcnFqpN71OmBoXDCT6y0TQgIc&#34;&gt;申请表&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年03月28日：&lt;a href=&#34;https://mp.weixin.qq.com/s/CsturoU1pOX11CqVnZgu2A&#34;&gt;社区免费公开课&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;【最新】2024年03月08日：开放了免费API供大家使用，包含（Atom-1B,7B,13B 3种中文大模型）&lt;a href=&#34;https://llama.family/docs/chat-completion-v1&#34;&gt;API使用链接&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;【最新】2023年10月8日：新增清华大学JittorLLMs的推理加速功能&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#jittorllms&#34;&gt;JittorLLMs&lt;/a&gt;！&lt;/p&gt; &#xA;&lt;p&gt;【最新】2023年9月12日：更新预训练版本&lt;a href=&#34;https://huggingface.co/FlagAlpha/Atom-7B&#34;&gt;Atom-7B&lt;/a&gt;和对话版本&lt;a href=&#34;https://huggingface.co/FlagAlpha/Atom-7B-Chat&#34;&gt;Atom-7B-Chat&lt;/a&gt;模型参数，最新的中文预训练数据量为2.7TB token，训练进程见&lt;a href=&#34;https://llama.family/&#34;&gt;llama.family&lt;/a&gt;！&lt;/p&gt; &#xA;&lt;p&gt;【最新】2023年9月2日：新增模型&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83&#34;&gt;预训练代码&lt;/a&gt;和&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83&#34;&gt;全量参数微调代码&lt;/a&gt;！&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年8月28日：发布基于Llama2进行中文预训练的开源大模型&lt;a href=&#34;https://huggingface.co/FlagAlpha/Atom-7B&#34;&gt;Atom-7B&lt;/a&gt;，并将持续更新，详情参考&lt;a href=&#34;https://mp.weixin.qq.com/s/Bdx0JTVh1kgPn5ydYxIkEw&#34;&gt;社区公众号文章&lt;/a&gt;！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年8月26日：提供&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#fastapi%E6%8E%A5%E5%8F%A3%E6%90%AD%E5%BB%BA&#34;&gt;FastAPI&lt;/a&gt;接口搭建脚本！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年8月26日：提供将Meta原始模型参数转换为兼容Hugging Face的&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/scripts/convert2hf/README.md&#34;&gt;格式转化脚本&lt;/a&gt;！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年8月26日：新增&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E4%BB%A3%E7%A0%81%E6%A8%A1%E5%9E%8B&#34;&gt;Code Llama&lt;/a&gt;模型！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年8月15日：新增&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E5%8A%A0%E8%BD%BD%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B&#34;&gt;PEFT加载微调模型参数&lt;/a&gt;的代码示例！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年8月14日：&lt;a href=&#34;https://llama.family&#34;&gt;大模型数据共享训练平台&lt;/a&gt;上线，没有算力也能参与大模型训练，社区每位成员贡献的数据都将决定模型能力的未来走向！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年8月3日：新增FasterTransformer和vLLM的GPU&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F&#34;&gt;推理加速&lt;/a&gt;支持！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月31日：【重磅】国内首个真正意义上的Llama2中文大模型发布！详情参见&lt;a href=&#34;https://mp.weixin.qq.com/s/lExUU7z_MvgJ7tzQPF8tUQ&#34;&gt;社区公众号文章&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月28日：通过&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#docker%E9%83%A8%E7%BD%B2%E9%97%AE%E7%AD%94%E6%8E%A5%E5%8F%A3&#34;&gt;Docker部署&lt;/a&gt;问答接口！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月27日：新增&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#langchain&#34;&gt;LangChain&lt;/a&gt;支持！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月26日：新增Llama2-13B中文微调参数的&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96&#34;&gt;4bit量化压缩版本&lt;/a&gt;！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月25日：社区微信公众号“Llama中文社区”欢迎大家关注，获取最新分享和动态！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月24日：&lt;a href=&#34;https://huggingface.co/FlagAlpha&#34;&gt;FlagAlpha&lt;/a&gt;新增Llama2-13B中文微调参数！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月24日：&lt;a href=&#34;https://llama.family/&#34;&gt;llama.family&lt;/a&gt;新增Llama2-70B在线体验！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月23日：Llama2中文微调参数发布至Hugging Face仓库&lt;a href=&#34;https://huggingface.co/FlagAlpha&#34;&gt;FlagAlpha&lt;/a&gt;！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月22日：Llama2在线体验链接&lt;a href=&#34;https://llama.family/&#34;&gt;llama.family&lt;/a&gt;上线，同时包含Meta原版和中文微调版本！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月21日：评测了Meta原始版Llama2 Chat模型的&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B&#34;&gt;中文问答能力&lt;/a&gt;！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月21日：新增Llama2模型的Hugging Face版本国内下载地址！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月20日：新增&lt;a href=&#34;https://chinesellama.feishu.cn/wiki/space/7257824476874768388?ccm_open_type=lark_wiki_spaceLink&#34;&gt;飞书知识库文档&lt;/a&gt;，欢迎大家一起共建！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月20日：国内Llama2最新下载地址上线！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月19日：正式启动Llama2模型的中文预训练，关注我们获取实时动态！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月19日：Llama2国内下载地址正在启动，敬请期待！&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;2023年7月19日：开启Llama2中文社区，欢迎大家加入！&lt;/p&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;🤗 模型&lt;/h3&gt; &#xA;&lt;h4&gt;🔵 中文预训练模型Atom&lt;/h4&gt; &#xA;&lt;p&gt;&lt;strong&gt;原子大模型Atom&lt;/strong&gt;由Llama中文社区和原子回声联合打造。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;类别&lt;/th&gt; &#xA;   &lt;th&gt;模型名称&lt;/th&gt; &#xA;   &lt;th&gt;🤗模型加载名称&lt;/th&gt; &#xA;   &lt;th&gt;下载地址&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;预训练&lt;/td&gt; &#xA;   &lt;td&gt;Atom-7B&lt;/td&gt; &#xA;   &lt;td&gt;FlagAlpha/Atom-7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FlagAlpha/Atom-7B&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://modelscope.cn/models/FlagAlpha/Atom-7B&#34;&gt;ModelScope&lt;/a&gt; | &lt;a href=&#34;https://wisemodel.cn/models/FlagAlpha/Atom-7B&#34;&gt;WiseModel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chat&lt;/td&gt; &#xA;   &lt;td&gt;Atom-7B-Chat&lt;/td&gt; &#xA;   &lt;td&gt;FlagAlpha/Atom-7B-Chat&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FlagAlpha/Atom-7B-Chat&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://modelscope.cn/models/FlagAlpha/Atom-7B-Chat&#34;&gt;ModelScope&lt;/a&gt; | &lt;a href=&#34;https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat&#34;&gt;WiseModel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Atom系列模型包含Atom-13B、Atom-7B和Atom-1B，基于Llama2做了中文能力的持续优化。Atom-7B和Atom-7B-Chat目前已完全开源，支持商用，可在&lt;a href=&#34;https://huggingface.co/FlagAlpha&#34;&gt;Hugging Face&lt;/a&gt;仓库获取模型，详情见&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E5%9F%BA%E4%BA%8Ellama2%E7%9A%84%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8Batom&#34;&gt;Atom-7B下载&lt;/a&gt;。Atom大模型针对中文做了以下优化：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;大规模的中文数据预训练&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;原子大模型Atom在Llama2的基础上，采用大规模的中文数据进行持续预训练，包含百科、书籍、博客、新闻、公告、小说、金融数据、法律数据、医疗数据、代码数据、专业论文数据、中文自然语言处理竞赛数据集等，详见&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E6%95%B0%E6%8D%AE%E6%9D%A5%E6%BA%90&#34;&gt;📝 数据来源&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;同时对庞大的数据进行了过滤、打分、去重，筛选出超过1T token的高质量中文数据，持续不断加入训练迭代中。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;更高效的中文词表 为了提高中文文本处理的效率，我们针对Llama2模型的词表进行了深度优化。首先，我们基于数百G的中文文本，在该模型词表的基础上扩展词库至65,000个单词。经过测试，我们的改进使得中文编码/解码速度提高了约350％。此外，我们还扩大了中文字符集的覆盖范围，包括所有emoji符号😊。这使得生成带有表情符号的文章更加高效。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;自适应上下文扩展 Atom大模型默认支持4K上下文，利用位置插值PI和Neural Tangent Kernel （NTK）方法，经过微调可以将上下文长度扩增到32K。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;📝 中文数据&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;我们通过以下数据来优化Llama2的中文能力:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;类型&lt;/th&gt; &#xA;   &lt;th&gt;描述&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;网络数据&lt;/td&gt; &#xA;   &lt;td&gt;互联网上公开的网络数据，挑选出去重后的高质量中文数据，涉及到百科、书籍、博客、新闻、公告、小说等高质量长文本数据。&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/goldsmith/Wikipedia&#34;&gt;Wikipedia&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;中文Wikipedia的数据&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BAAI-WuDao/Model&#34;&gt;悟道&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;中文悟道开源的200G数据&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/CLUEbenchmark/CLUEDatasetSearch&#34;&gt;Clue&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Clue开放的中文预训练数据，进行清洗后的高质量中文长文本数据&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;竞赛数据集&lt;/td&gt; &#xA;   &lt;td&gt;近年来中文自然语言处理多任务竞赛数据集，约150个&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/esbatmop/MNBVC&#34;&gt;MNBVC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MNBVC 中清洗出来的部分数据集&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;社区提供预训练版本Atom-7B和基于Atom-7B进行对话微调的模型参数供开放下载，关于模型的进展详见社区官网&lt;a href=&#34;https://llama.family&#34;&gt;llama.family&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h4&gt;Llama3官方模型&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;类别&lt;/th&gt; &#xA;   &lt;th&gt;模型名称&lt;/th&gt; &#xA;   &lt;th&gt;🤗模型加载名称&lt;/th&gt; &#xA;   &lt;th&gt;下载地址&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;预训练&lt;/td&gt; &#xA;   &lt;td&gt;Llama3-8B&lt;/td&gt; &#xA;   &lt;td&gt;meta-llama/Meta-Llama-3-8B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://pan.baidu.com/s/1gBZ7wEn3gC8VRok0Onh9BQ?pwd=8frq&#34;&gt;百度网盘&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;预训练&lt;/td&gt; &#xA;   &lt;td&gt;Llama3-70B&lt;/td&gt; &#xA;   &lt;td&gt;meta-llama/Meta-Llama-3-70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-7B&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://pan.baidu.com/s/1gBZ7wEn3gC8VRok0Onh9BQ?pwd=8frq&#34;&gt;百度网盘&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话模型&lt;/td&gt; &#xA;   &lt;td&gt;Llama3-8B-Chat&lt;/td&gt; &#xA;   &lt;td&gt;meta-llama/Meta-Llama-3-8B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://pan.baidu.com/s/1gBZ7wEn3gC8VRok0Onh9BQ?pwd=8frq&#34;&gt;百度网盘&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话模型&lt;/td&gt; &#xA;   &lt;td&gt;Llama3-70B-Chat&lt;/td&gt; &#xA;   &lt;td&gt;meta-llama/Meta-Llama-3-70B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://pan.baidu.com/s/1gBZ7wEn3gC8VRok0Onh9BQ?pwd=8frq&#34;&gt;百度网盘&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Llama3中文微调模型&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;类别&lt;/th&gt; &#xA;   &lt;th&gt;模型名称&lt;/th&gt; &#xA;   &lt;th&gt;🤗模型加载名称&lt;/th&gt; &#xA;   &lt;th&gt;下载地址&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;对话模型&lt;/td&gt; &#xA;   &lt;td&gt;Llama3-Chinese-8B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;FlagAlpha/Llama3-Chinese-8B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FlagAlpha/Llama3-Chinese-8B-Instruct&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://modelscope.cn/models/FlagAlpha/Llama3-Chinese-8B-Instruct/summary&#34;&gt;modelscope&lt;/a&gt; | &lt;a href=&#34;https://wisemodel.cn/models/FlagAlpha/Llama3-Chinese-8B-Instruct/file&#34;&gt;wisemodel&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Llama2官方模型&lt;/h4&gt; &#xA;&lt;details&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;类别&lt;/th&gt; &#xA;    &lt;th&gt;模型名称&lt;/th&gt; &#xA;    &lt;th&gt;🤗模型加载名称&lt;/th&gt; &#xA;    &lt;th&gt;下载地址&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;预训练&lt;/td&gt; &#xA;    &lt;td&gt;Llama2-7B&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-7b-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-hf&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://pan.xunlei.com/s/VN_t0dUikZqOwt-5DZWHuMvqA1?pwd=66ep&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;预训练&lt;/td&gt; &#xA;    &lt;td&gt;Llama2-13B&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-13b-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-hf&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://pan.xunlei.com/s/VN_yT_9G8xNOz0SDWQ7Mb_GZA1?pwd=yvgf&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;预训练&lt;/td&gt; &#xA;    &lt;td&gt;Llama2-70B&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-70b-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-70b-hf&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Chat&lt;/td&gt; &#xA;    &lt;td&gt;Llama2-7B-Chat&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-7b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://pan.xunlei.com/s/VN_oaV4BpKFgKLto4KgOhBcaA1?pwd=ufir&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Chat&lt;/td&gt; &#xA;    &lt;td&gt;Llama2-13B-Chat&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-13b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-13b-chat-hf&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://pan.xunlei.com/s/VN_yA-9G34NGL9B79b3OQZZGA1?pwd=xqrg&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Chat&lt;/td&gt; &#xA;    &lt;td&gt;Llama2-70B-Chat&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-70b-chat-hf&#34;&gt;HuggingFace&lt;/a&gt; | &lt;a href=&#34;https://pan.xunlei.com/s/VNa_vCGzCy3h3N7oeFXs2W1hA1?pwd=uhxh#&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-7b&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1cIPzdNywWLvQI7_2QanOEQ?pwd=zfwi&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-7b-Python&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1liY8klGoDagYbpw-g-oFag?pwd=i952&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-7b-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/108o9_DT2E_vfSGtOnDCQVw?pwd=zkt9&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-13b&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1lLaeHv0XEBv0iiZzI1dpnw?pwd=qn99&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-13b-Python&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1OLVfvZS_oqL3oqMKwsI87w?pwd=a78k&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-13b-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1HyxJl4w8wElgkZRh2ATrXQ?pwd=seg6&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-34b&lt;/td&gt; &#xA;    &lt;td&gt;meta-llama/Llama-2-70b-chat-hf&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://pan.baidu.com/s/1vEw0pFgIkctPUN4_5_6pIQ?pwd=q8eu&#34;&gt;迅雷网盘&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;Meta官方在2023年8月24日发布了Code Llama，基于代码数据对Llama2进行了微调，提供三个不同功能的版本：基础模型（Code Llama）、Python专用模型（Code Llama - Python）和指令跟随模型（Code Llama - Instruct），包含7B、13B、34B三种不同参数规模。不同模型能力区别如下表所示：&lt;/p&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;模型类别&lt;/th&gt; &#xA;    &lt;th&gt;模型名称&lt;/th&gt; &#xA;    &lt;th&gt;代码续写&lt;/th&gt; &#xA;    &lt;th&gt;代码填充&lt;/th&gt; &#xA;    &lt;th&gt;指令编程&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code Llama&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-7b&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-13b&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-34b&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code Llama - Python&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-7b-Python&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-13b-Python&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-34b-Python&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Code Llama - Instruct&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-7b-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-13b-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;    &lt;td&gt;CodeLlama-34b-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;td&gt;❌&lt;/td&gt; &#xA;    &lt;td&gt;✅&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;p&gt;关于Code Llama的详细信息可以参考官方Github仓库&lt;a href=&#34;https://github.com/facebookresearch/codellama&#34;&gt;codellama&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h4&gt;Llama2中文微调模型&lt;/h4&gt; &#xA;&lt;p&gt;我们基于中文指令数据集对Llama2-Chat模型进行了微调，使得Llama2模型有着更强的中文对话能力。LoRA参数以及与基础模型合并的参数均已上传至&lt;a href=&#34;https://huggingface.co/FlagAlpha&#34;&gt;Hugging Face&lt;/a&gt;，目前包含7B和13B的模型。&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;类别&lt;/th&gt; &#xA;   &lt;th&gt;模型名称&lt;/th&gt; &#xA;   &lt;th&gt;🤗模型加载名称&lt;/th&gt; &#xA;   &lt;th&gt;基础模型版本&lt;/th&gt; &#xA;   &lt;th&gt;下载地址&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;合并参数&lt;/td&gt; &#xA;   &lt;td&gt;Llama2-Chinese-7b-Chat&lt;/td&gt; &#xA;   &lt;td&gt;FlagAlpha/Llama2-Chinese-7b-Chat&lt;/td&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-7b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;合并参数&lt;/td&gt; &#xA;   &lt;td&gt;Llama2-Chinese-13b-Chat&lt;/td&gt; &#xA;   &lt;td&gt;FlagAlpha/Llama2-Chinese-13b-Chat&lt;/td&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-13b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA参数&lt;/td&gt; &#xA;   &lt;td&gt;Llama2-Chinese-7b-Chat-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;FlagAlpha/Llama2-Chinese-7b-Chat-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-7b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FlagAlpha/Llama2-Chinese-7b-Chat-LoRA&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LoRA参数&lt;/td&gt; &#xA;   &lt;td&gt;Llama2-Chinese-13b-Chat-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;FlagAlpha/Llama2-Chinese-13b-Chat-LoRA&lt;/td&gt; &#xA;   &lt;td&gt;meta-llama/Llama-2-13b-chat-hf&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-LoRA&#34;&gt;HuggingFace&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;社区资源&lt;/h3&gt; &#xA;&lt;p&gt;社区资源的丰富性是社区发展的重要保障，它涵盖了各种方面，其中包括但不限于以下四个方面：算力、数据、论坛和应用。在这些方面的积极发展与充分利用，将为社区成员提供更多的机会和支持，推动整个社区向着更加繁荣的方向发展。更多的内容请看&lt;a href=&#34;https://llama.family/&#34;&gt;llama.family&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;h4&gt;💻 算力&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;提供低于市场价格的算力资源，可用于各类计算任务，如深度学习模型的训练、推理等。&lt;/li&gt; &#xA;  &lt;li&gt;为社区成员提供专属的在线推理服务，让用户可以快速有效地对模型进行推理操作。&lt;/li&gt; &#xA;  &lt;li&gt;提供一键在线微调服务，使用户可以方便地对模型进行微调，以适应不同的任务和数据。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;📊 数据&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;开放丰富的训练数据资源，覆盖多个领域和行业，为模型训练提供充足的数据支持。&lt;/li&gt; &#xA;  &lt;li&gt;提供高质量、多样化的数据集，以满足不同用户的需求，并支持数据共享和交流，促进数据资源的充分利用。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;💬 论坛&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;社区论坛为社区成员提供了一个在线交流和讨论技术问题的平台。&lt;/li&gt; &#xA;  &lt;li&gt;在论坛上，用户可以分享经验、提出问题、解答疑惑，促进技术交流和合作。&lt;/li&gt; &#xA;  &lt;li&gt;论坛还可以定期举办线上活动、研讨会等，增进社区成员之间的联系和了解。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;📱 应用&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;免费提供应用推广展示位，让开发者可以将他们的应用充分展示给社区成员。&lt;/li&gt; &#xA;  &lt;li&gt;提供推广的帮助，包括但不限于宣传推广、用户引导等服务，帮助应用获得更多的曝光和用户。&lt;/li&gt; &#xA;  &lt;li&gt;通过社区平台，为优秀的应用提供合作机会，促进应用开发者之间的合作和交流，共同推动应用的发展和壮大。&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;📌 如何使用Llama模型?&lt;/h2&gt; &#xA;&lt;p&gt;你可以选择下面的快速上手的任一种方式，开始使用 Llama 系列模型。推荐使用&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama2%E4%B8%AD%E6%96%87%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8Batom-7b&#34;&gt;中文预训练对话模型&lt;/a&gt;进行使用，对中文的效果支持更好。&lt;/p&gt; &#xA;&lt;h3&gt;快速上手-使用Anaconda&lt;/h3&gt; &#xA;&lt;p&gt;第 0 步：前提条件&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;确保安装了 Python 3.10 以上版本。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;第 1 步：准备环境&lt;/p&gt; &#xA;&lt;p&gt;如需设置环境，安装所需要的软件包，运行下面的命令。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/LlamaFamily/Llama-Chinese.git&#xA;cd Llama-Chinese&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;第 2 步：下载模型&lt;/p&gt; &#xA;&lt;p&gt;你可以从以下来源下载Atom-7B-Chat模型。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/FlagAlpha&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://modelscope.cn/organization/FlagAlpha&#34;&gt;ModelScope&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wisemodel.cn/models/FlagAlpha/Atom-7B-Chat&#34;&gt;WideModel&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;第 3 步：进行推理&lt;/p&gt; &#xA;&lt;p&gt;使用Atom-7B-Chat模型进行推理 创建一个名为 quick_start.py 的文件，并将以下内容复制到该文件中。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;device_map = &#34;cuda:0&#34; if torch.cuda.is_available() else &#34;auto&#34;&#xA;model = AutoModelForCausalLM.from_pretrained(&#39;FlagAlpha/Atom-7B-Chat&#39;,device_map=device_map,torch_dtype=torch.float16,load_in_8bit=True,trust_remote_code=True,use_flash_attention_2=True)&#xA;model =model.eval()&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;FlagAlpha/Atom-7B-Chat&#39;,use_fast=False)&#xA;tokenizer.pad_token = tokenizer.eos_token&#xA;input_ids = tokenizer([&#39;&amp;lt;s&amp;gt;Human: 介绍一下中国\n&amp;lt;/s&amp;gt;&amp;lt;s&amp;gt;Assistant: &#39;], return_tensors=&#34;pt&#34;,add_special_tokens=False).input_ids&#xA;if torch.cuda.is_available():&#xA;  input_ids = input_ids.to(&#39;cuda&#39;)&#xA;generate_input = {&#xA;    &#34;input_ids&#34;:input_ids,&#xA;    &#34;max_new_tokens&#34;:512,&#xA;    &#34;do_sample&#34;:True,&#xA;    &#34;top_k&#34;:50,&#xA;    &#34;top_p&#34;:0.95,&#xA;    &#34;temperature&#34;:0.3,&#xA;    &#34;repetition_penalty&#34;:1.3,&#xA;    &#34;eos_token_id&#34;:tokenizer.eos_token_id,&#xA;    &#34;bos_token_id&#34;:tokenizer.bos_token_id,&#xA;    &#34;pad_token_id&#34;:tokenizer.pad_token_id&#xA;}&#xA;generate_ids  = model.generate(**generate_input)&#xA;text = tokenizer.decode(generate_ids[0])&#xA;print(text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行 quick_start.py 代码。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python quick_start.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;快速上手-使用Docker&lt;/h3&gt; &#xA;&lt;p&gt;详情参见：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/docs/chat_gradio_guide.md&#34;&gt;Docker部署&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;第 1 步：准备docker镜像，通过docker容器启动&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/examples/chat_gradio.py&#34;&gt;chat_gradio.py&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/LlamaFamily/Llama-Chinese.git&#xA;&#xA;cd Llama-Chinese&#xA;&#xA;docker build -f docker/Dockerfile -t flagalpha/llama2-chinese:gradio .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;第 2 步：通过docker-compose启动chat_gradio&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd Llama-Chinese/docker&#xA;docker-compose up -d --build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;快速上手-使用llama.cpp&lt;/h3&gt; &#xA;&lt;p&gt;详情参见：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/inference-speed/CPU/ggml/README.md&#34;&gt;使用llama.cpp&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;快速上手-使用gradio&lt;/h3&gt; &#xA;&lt;p&gt;基于gradio搭建的问答界面，实现了流式的输出，将下面代码复制到控制台运行，以下代码以Atom-7B-Chat模型为例，不同模型只需修改一下面的model_name_or_path对应的模型名称就好了😊&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python examples/chat_gradio.py --model_name_or_path FlagAlpha/Atom-7B-Chat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;快速上手-构建API服务&lt;/h3&gt; &#xA;&lt;p&gt;使用FastChat构建和OpenAI一致的推理服务接口。&lt;/p&gt; &#xA;&lt;details&gt;&#xA;  第 0 步：前提条件 &#xA; &lt;p&gt;安装fastchat&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip3 install &#34;fschat[model_worker,webui]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;第 1 步：启动Restful API&lt;/p&gt; &#xA; &lt;p&gt;开启三个控制台分别执行下面的三个命令&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;首先启动controler&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m fastchat.serve.controller \&#xA;--host localhost \&#xA;--port 21001&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;启动模型&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=&#34;0&#34; python3 -m fastchat.serve.model_worker --model-path /path/Atom-7B-Chat \&#xA;--host localhost \&#xA;--port 21002 \&#xA;--worker-address &#34;http://localhost:21002&#34; \&#xA;--limit-worker-concurrency 5 \&#xA;--stream-interval 2 \&#xA;--gpus &#34;1&#34; \&#xA;--load-8bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;启动RESTful API 服务&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m fastchat.serve.openai_api_server \&#xA;--host localhost \&#xA;--port 21003 \&#xA;--controller-address http://localhost:21001&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;第 2 步：测试api服务&lt;/p&gt; &#xA; &lt;p&gt;执行下面的python代码测试上面部署的api服务&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# coding=utf-8&#xA;import json&#xA;import time&#xA;import urllib.request&#xA;import sys&#xA;import requests&#xA;&#xA;def test_api_server(input_text):&#xA;    header = {&#39;Content-Type&#39;: &#39;application/json&#39;}&#xA;&#xA;    data = {&#xA;          &#34;messages&#34;: [{&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: &#34;&#34;}, {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: input_text}],&#xA;          &#34;temperature&#34;: 0.3, &#xA;          &#34;top_p&#34; : 0.95, &#xA;          &#34;max_tokens&#34;: 512, &#xA;          &#34;model&#34;: &#34;LLama2-Chinese-13B&#34;,&#xA;          &#34;stream&#34; : False,&#xA;          &#34;n&#34; : 1,&#xA;          &#34;best_of&#34;: 1, &#xA;          &#34;presence_penalty&#34;: 1.2, &#xA;          &#34;frequency_penalty&#34;: 0.2,           &#xA;          &#34;top_k&#34;: 50, &#xA;          &#34;use_beam_search&#34;: False, &#xA;          &#34;stop&#34;: [], &#xA;          &#34;ignore_eos&#34; :False,&#xA;          &#34;logprobs&#34;: None&#xA;    }&#xA;    response = requests.post(&#xA;        url=&#39;http://127.0.0.1:21003/v1/chat/completions&#39;,&#xA;        headers=header,&#xA;        data=json.dumps(data).encode(&#39;utf-8&#39;)&#xA;    )&#xA;&#xA;    result = None&#xA;    try:&#xA;        result = json.loads(response.content)&#xA;        print(json.dumps(data, ensure_ascii=False, indent=2))&#xA;        print(json.dumps(result, ensure_ascii=False, indent=2))&#xA;&#xA;    except Exception as e:&#xA;        print(e)&#xA;&#xA;    return result&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    test_api_server(&#34;如何去北京?&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;🤖 模型预训练&lt;/h2&gt; &#xA;&lt;p&gt;虽然Llama2的预训练数据相对于第一代LLaMA扩大了一倍，但是中文预训练数据的比例依然非常少，仅占0.13%，这也导致了原始Llama2的中文能力较弱。为了能够提升模型的中文能力，可以采用微调和预训练两种路径，其中：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;微调需要的算力资源少，能够快速实现一个中文Llama的雏形。但缺点也显而易见，只能激发基座模型已有的中文能力，由于Llama2的中文训练数据本身较少，所以能够激发的能力也有限，治标不治本。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;基于大规模中文语料进行预训练，成本高，不仅需要大规模高质量的中文数据，也需要大规模的算力资源。但是优点也显而易见，就是能从模型底层优化中文能力，真正达到治本的效果，从内核为大模型注入强大的中文能力。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;我们为社区提供了Llama模型的预训练代码，以及&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/tree/main/data&#34;&gt;中文测试语料&lt;/a&gt;，更多数据可以参考&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#-%E4%B8%AD%E6%96%87%E6%95%B0%E6%8D%AE&#34;&gt;中文语料&lt;/a&gt;。具体代码和配置如下：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;模型预训练脚本：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/train/pretrain/pretrain.sh&#34;&gt;train/pretrain/pretrain.sh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;预训练实现代码：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/train/pretrain/pretrain_clm.py&#34;&gt;train/pretrain/pretrain_clm.py&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/DeepSpeed&#34;&gt;DeepSpeed&lt;/a&gt;加速： &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;对于单卡训练，可以采用ZeRO-2的方式，参数配置见 &lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/train/pretrain/ds_config_zero2.json&#34;&gt;train/pretrain/ds_config_zero2.json&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;对于多卡训练，可以采用ZeRO-3的方式，参数配置见 &lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/train/pretrain/ds_config_zero3.json&#34;&gt;train/pretrain/ds_config_zero3.json&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;训练效果度量指标：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/train/pretrain/accuracy.py&#34;&gt;train/pretrain/accuracy.py&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;💡 模型微调&lt;/h2&gt; &#xA;&lt;p&gt;本仓库中同时提供了LoRA微调和全量参数微调代码，关于LoRA的详细介绍可以参考论文“&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34;&gt;LoRA: Low-Rank Adaptation of Large Language Models&lt;/a&gt;”以及微软Github仓库&lt;a href=&#34;https://github.com/microsoft/LoRA&#34;&gt;LoRA&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;Step1: 环境准备&lt;/h3&gt; &#xA;&lt;p&gt;根据&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/requirements.txt&#34;&gt;requirements.txt&lt;/a&gt;安装对应的环境依赖。&lt;/p&gt; &#xA;&lt;h3&gt;Step2: 数据准备&lt;/h3&gt; &#xA;&lt;p&gt;在data目录下提供了一份用于模型sft的数据样例：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;训练数据：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/data/train_sft.csv&#34;&gt;data/train_sft.csv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;验证数据：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/data/dev_sft.csv&#34;&gt;data/dev_sft.csv&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;每个csv文件中包含一列“text”，每一行为一个训练样例，每个训练样例按照以下格式将问题和答案组织为模型输入，您可以按照以下格式自定义训练和验证数据集：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;&amp;lt;s&amp;gt;Human: &#34;+问题+&#34;\n&amp;lt;/s&amp;gt;&amp;lt;s&amp;gt;Assistant: &#34;+答案+&#34;\n&#34;&amp;lt;/s&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;例如，&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&amp;lt;s&amp;gt;Human: 用一句话描述地球为什么是独一无二的。&amp;lt;/s&amp;gt;&amp;lt;s&amp;gt;Assistant: 因为地球是目前为止唯一已知存在生命的行星。&amp;lt;/s&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step3: 微调脚本&lt;/h3&gt; &#xA;&lt;h4&gt;LoRA微调&lt;/h4&gt; &#xA;&lt;p&gt;LoRA微调脚本见：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/train/sft/finetune_lora.sh&#34;&gt;train/sft/finetune_lora.sh&lt;/a&gt;，关于LoRA微调的具体实现代码见&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/train/sft/finetune_clm_lora.py&#34;&gt;train/sft/finetune_clm_lora.py&lt;/a&gt;，单机多卡的微调可以通过修改脚本中的&lt;code&gt;--include localhost:0&lt;/code&gt;来实现。&lt;/p&gt; &#xA;&lt;h4&gt;全量参数微调&lt;/h4&gt; &#xA;&lt;p&gt;全量参数微调脚本见：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/train/sft/finetune.sh&#34;&gt;train/sft/finetune.sh&lt;/a&gt;，关于全量参数微调的具体实现代码见&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/train/sft/finetune_clm.py&#34;&gt;train/sft/finetune_clm.py&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;Step4: 加载微调模型&lt;/h3&gt; &#xA;&lt;h4&gt;LoRA微调&lt;/h4&gt; &#xA;&lt;p&gt;基于LoRA微调的模型参数见：&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#llama2%E4%B8%AD%E6%96%87%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B&#34;&gt;基于Llama2的中文微调模型&lt;/a&gt;，LoRA参数需要和基础模型参数结合使用。&lt;/p&gt; &#xA;&lt;p&gt;通过&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt;加载预训练模型参数和微调模型参数，以下示例代码中，base_model_name_or_path为预训练模型参数保存路径，finetune_model_path为微调模型参数保存路径。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import AutoTokenizer, AutoModelForCausalLM&#xA;from peft import PeftModel,PeftConfig&#xA;# 例如: finetune_model_path=&#39;FlagAlpha/Llama2-Chinese-7b-Chat-LoRA&#39;&#xA;finetune_model_path=&#39;&#39;  &#xA;config = PeftConfig.from_pretrained(finetune_model_path)&#xA;# 例如: base_model_name_or_path=&#39;meta-llama/Llama-2-7b-chat&#39;&#xA;tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,use_fast=False)&#xA;tokenizer.pad_token = tokenizer.eos_token&#xA;device_map = &#34;cuda:0&#34; if torch.cuda.is_available() else &#34;auto&#34;&#xA;model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,device_map=device_map,torch_dtype=torch.float16,load_in_8bit=True,trust_remote_code=True,use_flash_attention_2=True)&#xA;model = PeftModel.from_pretrained(model, finetune_model_path, device_map={&#34;&#34;: 0})&#xA;model =model.eval()&#xA;input_ids = tokenizer([&#39;&amp;lt;s&amp;gt;Human: 介绍一下北京\n&amp;lt;/s&amp;gt;&amp;lt;s&amp;gt;Assistant: &#39;], return_tensors=&#34;pt&#34;,add_special_tokens=False).input_ids&#xA;if torch.cuda.is_available():&#xA;  input_ids = input_ids.to(&#39;cuda&#39;)&#xA;generate_input = {&#xA;    &#34;input_ids&#34;:input_ids,&#xA;    &#34;max_new_tokens&#34;:512,&#xA;    &#34;do_sample&#34;:True,&#xA;    &#34;top_k&#34;:50,&#xA;    &#34;top_p&#34;:0.95,&#xA;    &#34;temperature&#34;:0.3,&#xA;    &#34;repetition_penalty&#34;:1.3,&#xA;    &#34;eos_token_id&#34;:tokenizer.eos_token_id,&#xA;    &#34;bos_token_id&#34;:tokenizer.bos_token_id,&#xA;    &#34;pad_token_id&#34;:tokenizer.pad_token_id&#xA;}&#xA;generate_ids  = model.generate(**generate_input)&#xA;text = tokenizer.decode(generate_ids[0])&#xA;print(text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;全量参数微调&lt;/h4&gt; &#xA;&lt;p&gt;对于全量参数微调的模型，调用方式同&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/#%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B&#34;&gt;模型调用代码示例&lt;/a&gt;，只需要修改其中的模型名称或者保存路径即可。&lt;/p&gt; &#xA;&lt;h2&gt;🍄 模型量化&lt;/h2&gt; &#xA;&lt;p&gt;我们对中文微调的模型参数进行了量化，方便以更少的计算资源运行。目前已经在&lt;a href=&#34;https://huggingface.co/FlagAlpha&#34;&gt;Hugging Face&lt;/a&gt;上传了13B中文微调模型&lt;a href=&#34;https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat&#34;&gt;FlagAlpha/Llama2-Chinese-13b-Chat&lt;/a&gt;的4bit压缩版本&lt;a href=&#34;https://huggingface.co/FlagAlpha/Llama2-Chinese-13b-Chat-4bit&#34;&gt;FlagAlpha/Llama2-Chinese-13b-Chat-4bit&lt;/a&gt;，具体调用方式如下：&lt;/p&gt; &#xA;&lt;p&gt;环境准备：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install git+https://github.com/PanQiWei/AutoGPTQ.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer&#xA;from auto_gptq import AutoGPTQForCausalLM&#xA;model = AutoGPTQForCausalLM.from_quantized(&#39;FlagAlpha/Llama2-Chinese-13b-Chat-4bit&#39;, device=&#34;cuda:0&#34;)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#39;FlagAlpha/Llama2-Chinese-13b-Chat-4bit&#39;,use_fast=False)&#xA;input_ids = tokenizer([&#39;&amp;lt;s&amp;gt;Human: 怎么登上火星\n&amp;lt;/s&amp;gt;&amp;lt;s&amp;gt;Assistant: &#39;], return_tensors=&#34;pt&#34;,add_special_tokens=False).input_ids.to(&#39;cuda&#39;)        &#xA;generate_input = {&#xA;    &#34;input_ids&#34;:input_ids,&#xA;    &#34;max_new_tokens&#34;:512,&#xA;    &#34;do_sample&#34;:True,&#xA;    &#34;top_k&#34;:50,&#xA;    &#34;top_p&#34;:0.95,&#xA;    &#34;temperature&#34;:0.3,&#xA;    &#34;repetition_penalty&#34;:1.3,&#xA;    &#34;eos_token_id&#34;:tokenizer.eos_token_id,&#xA;    &#34;bos_token_id&#34;:tokenizer.bos_token_id,&#xA;    &#34;pad_token_id&#34;:tokenizer.pad_token_id&#xA;}&#xA;generate_ids  = model.generate(**generate_input)&#xA;text = tokenizer.decode(generate_ids[0])&#xA;print(text)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🚀 部署加速&lt;/h2&gt; &#xA;&lt;p&gt;随着大模型参数规模的不断增长，在有限的算力资源下，提升模型的推理速度逐渐变为一个重要的研究方向。常用的推理加速框架包含 lmdeploy、TensorRT-LLM、vLLM和JittorLLMs 等。&lt;/p&gt; &#xA;&lt;h3&gt;TensorRT-LLM&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/NVIDIA/TensorRT-LLM/tree/main&#34;&gt;TensorRT-LLM&lt;/a&gt;由NVIDIA开发，高性能推理框架&lt;/p&gt; &#xA;&lt;p&gt;详细的推理文档见：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/tree/main/inference-speed/GPU/TensorRT-LLM_example&#34;&gt;inference-speed/GPU/TensorRT-LLM_example&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;vLLM&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/vllm-project/vllm&#34;&gt;vLLM&lt;/a&gt;由加州大学伯克利分校开发，核心技术是PageAttention，吞吐量比HuggingFace Transformers高出24倍。相较与FasterTrainsformer，vLLM更加的简单易用，不需要额外进行模型的转换，支持fp16推理。&lt;/p&gt; &#xA;&lt;p&gt;详细的推理文档见：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/inference-speed/GPU/vllm_example/README.md&#34;&gt;inference-speed/GPU/vllm_example&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;JittorLLMs&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Jittor/JittorLLMs&#34;&gt;JittorLLMs&lt;/a&gt;由非十科技领衔，与清华大学可视媒体研究中心合作研发，通过动态swap机制大幅降低硬件配置要求（减少80%）,并且Jittor框架通过零拷贝技术，大模型加载相比Pytorch开销降低40%，同时，通过元算子自动编译优化，计算性能提升20%以上。&lt;/p&gt; &#xA;&lt;p&gt;详细的推理文档见：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/inference-speed/GPU/JittorLLMs_example/README.md&#34;&gt;inference-speed/GPU/JittorLLMs&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;lmdeploy&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/InternLM/lmdeploy/&#34;&gt;lmdeploy&lt;/a&gt; 由上海人工智能实验室开发，推理使用 C++/CUDA，对外提供 python/gRPC/http 接口和 WebUI 界面，支持 tensor parallel 分布式推理、支持 fp16/weight int4/kv cache int8 量化。&lt;/p&gt; &#xA;&lt;p&gt;详细的推理文档见：&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/tree/main/inference-speed/GPU/lmdeploy_example&#34;&gt;inference-speed/GPU/lmdeploy_example&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;💪 外延能力&lt;/h2&gt; &#xA;&lt;p&gt;除了持续增强大模型内在的知识储备、通用理解、逻辑推理和想象能力等，未来，我们也会不断丰富大模型的外延能力，例如知识库检索、计算工具、WolframAlpha、操作软件等。 我们首先集成了LangChain框架，可以更方便地基于Llama2开发文档检索、问答机器人和智能体应用等，关于LangChain的更多介绍参见&lt;a href=&#34;https://github.com/langchain-ai/langchain&#34;&gt;LangChain&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;LangChain&lt;/h3&gt; &#xA;&lt;p&gt;针对LangChain框架封装的Llama2 LLM类见&lt;a href=&#34;https://github.com/LlamaFamily/Llama-Chinese/raw/main/examples/llama2_for_langchain.py&#34;&gt;examples/llama2_for_langchain.py&lt;/a&gt;，简单的调用代码示例如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from llama2_for_langchain import Llama2&#xA;&#xA;# 这里以调用FlagAlpha/Atom-7B-Chat为例&#xA;llm = Llama2(model_name_or_path=&#39;FlagAlpha/Atom-7B-Chat&#39;)&#xA;&#xA;while True:&#xA;    human_input = input(&#34;Human: &#34;)&#xA;    response = llm(human_input)&#xA;    print(f&#34;Llama2: {response}&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🥇 模型评测&lt;/h2&gt; &#xA;&lt;h3&gt;Llama2和Llama3对比评测&lt;/h3&gt; &#xA;&lt;p&gt;基础模型对比&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/assets/base_eval.png&#34; style=&#34;width: 100%; display: block; margin: auto;&#34;&gt; &lt;/p&gt; 微调模型对比 &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/assets/tuned_eval.png&#34; style=&#34;width: 100%; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Llama3模型评测&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/assets/llama3_eval.png&#34; style=&#34;width: 100%; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;h3&gt;Llama2模型评测&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/assets/llama_eval.jpeg&#34; style=&#34;width: 100%; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;为了能够更加清晰地了解Llama2模型的中文问答能力，我们筛选了一些具有代表性的中文问题，对Llama2模型进行提问。我们测试的模型包含Meta公开的Llama2-7B-Chat和Llama2-13B-Chat两个版本，没有做任何微调和训练。测试问题筛选自&lt;a href=&#34;https://github.com/AtomEcho/AtomBulb&#34;&gt;AtomBulb&lt;/a&gt;，共95个测试问题，包含：通用知识、语言理解、创作能力、逻辑推理、代码编程、工作技能、使用工具、人格特征八个大的类别。&lt;/p&gt; &#xA;&lt;p&gt;测试中使用的Prompt如下，例如对于问题“列出5种可以改善睡眠质量的方法”：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[INST] &#xA;&amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;&#xA;You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. The answer always been translate into Chinese language.&#xA;&#xA;If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don&#39;t know the answer to a question, please don&#39;t share false information.&#xA;&#xA;The answer always been translate into Chinese language.&#xA;&amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt;&#xA;&#xA;列出5种可以改善睡眠质量的方法&#xA;[/INST]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Llama2-7B-Chat的测试结果见&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/assets/meta_eval_7B.md&#34;&gt;meta_eval_7B.md&lt;/a&gt;，Llama2-13B-Chat的测试结果见&lt;a href=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/assets/meta_eval_13B.md&#34;&gt;meta_eval_13B.md&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;通过测试我们发现，Meta原始的Llama2 Chat模型对于中文问答的对齐效果一般，大部分情况下都不能给出中文回答，或者是中英文混杂的形式。因此，基于中文数据对Llama2模型进行训练和微调十分必要。&lt;/p&gt; &#xA;&lt;h2&gt;📖 学习中心&lt;/h2&gt; &#xA;&lt;h3&gt;官方文档&lt;/h3&gt; &#xA;&lt;p&gt;Meta Llama全系列模型官方文档：&lt;a href=&#34;https://llama.meta.com/docs/get-started&#34;&gt;https://llama.meta.com/docs/get-started&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Llama3&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chinesellama.feishu.cn/wiki/XBKPwbhWriWCfrkmJhfcrS9Rnqc?fromScene=spaceOverview&#34;&gt;Llama3全套学习资料&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Llama3官方链接：&lt;a href=&#34;https://llama.meta.com/llama3&#34;&gt;https://llama.meta.com/llama3&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Llama2&lt;/h3&gt; &#xA;&lt;h4&gt;Meta官方对于&lt;a href=&#34;https://ai.meta.com/llama&#34;&gt;Llama2&lt;/a&gt;的介绍&lt;/h4&gt; &#xA;&lt;p&gt;自从Meta公司发布第一代LLaMA模型以来，羊驼模型家族繁荣发展。近期Meta发布了Llama2版本，开源可商用，在模型和效果上有了重大更新。Llama2总共公布了7B、13B和70B三种参数大小的模型。相比于LLaMA，Llama2的训练数据达到了2万亿token，上下文长度也由之前的2048升级到4096，可以理解和生成更长的文本。Llama2 Chat模型基于100万人类标记数据微调得到，在英文对话上达到了接近ChatGPT的效果。&lt;/p&gt; &#xA;&lt;h3&gt;Llama相关论文&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.09288&#34;&gt;Llama 2: Open Foundation and Fine-Tuned Chat Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/&#34;&gt;Code Llama: Open Foundation Models for Code&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;📌 其它&lt;/h2&gt; &#xA;&lt;h3&gt;🎉 致谢&lt;/h3&gt; &#xA;&lt;p&gt;感谢原子回声&lt;a href=&#34;https://github.com/AtomEcho&#34;&gt;AtomEcho&lt;/a&gt;团队的技术和资源支持！&lt;/p&gt; &#xA;&lt;p&gt;感谢芯格&lt;a href=&#34;https://coremesh.net&#34;&gt;Coremesh&lt;/a&gt;团队的技术和资源支持！&lt;/p&gt; &#xA;&lt;p&gt;感谢 @xzsGenius 对Llama2中文社区的贡献！&lt;/p&gt; &#xA;&lt;p&gt;感谢 @Z Potentials社区对Llama2中文社区的支持！&lt;/p&gt; &#xA;&lt;h3&gt;🤔 问题反馈&lt;/h3&gt; &#xA;&lt;p&gt;如有问题，请在GitHub Issue中提交，在提交问题之前，请先查阅以往的issue是否能解决你的问题。&lt;/p&gt; &#xA;&lt;p&gt;礼貌地提出问题，构建和谐的讨论社区。&lt;/p&gt; &#xA;&lt;p&gt;加入&lt;a href=&#34;https://chinesellama.feishu.cn/wiki/space/7257824476874768388?ccm_open_type=lark_wiki_spaceLink&#34;&gt;飞书知识库&lt;/a&gt;，一起共建社区文档。&lt;/p&gt; &#xA;&lt;p&gt;加入微信群讨论😍😍&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/LlamaFamily/Llama-Chinese/main/assets/wechat.jpeg&#34; alt=&#34;Wechat&#34; style=&#34;width: 100%; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://api.star-history.com/svg?repos=LlamaFamily/Llama-Chinese&amp;amp;type=Date&#34; alt=&#34;Wechat&#34; style=&#34;width: 100%; display: block; margin: auto;&#34;&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bia-pain-bache/BPB-Worker-Panel</title>
    <updated>2024-04-24T01:25:58Z</updated>
    <id>tag:github.com,2024-04-24:/bia-pain-bache/BPB-Worker-Panel</id>
    <link href="https://github.com/bia-pain-bache/BPB-Worker-Panel" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A GUI Panel providing Worker subscriptions and Fragment settings and configs, providing configs for cross-platform clients using (singbox-core and xray-core)&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;💦 BPB Panel&lt;/h1&gt; &#xA;&lt;h3&gt;🌏 Readme in &lt;a href=&#34;https://raw.githubusercontent.com/bia-pain-bache/BPB-Worker-Panel/main/README_fa.md&#34;&gt;Farsi&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bia-pain-bache/BPB-Worker-Panel/main/docs/assets/images/Panel.jpg&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This project is dedicated to developing a user panel for the &lt;a href=&#34;https://github.com/yonggekkk/Cloudflare-workers-pages-vless&#34;&gt;Cloudflare-workers/pages proxy script&lt;/a&gt; created by &lt;a href=&#34;https://github.com/yonggekkk&#34;&gt;yonggekkk&lt;/a&gt;. The panel offers two deployment options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Worker&lt;/strong&gt; deployment&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Pages&lt;/strong&gt; deployment&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;🌟 If you found &lt;strong&gt;BPB Panel&lt;/strong&gt; valuable, Your donations make all the difference 🌟&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;USDT (TRC20):&lt;/strong&gt; &lt;code&gt;TUeGCozCNL1s5XqGkZ1DtKuCnugJaDnYcc&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tezos (XTZ):&lt;/strong&gt; &lt;code&gt;tz1RSFQ8jDTZC2UZPwHy55D9XvfnW9KqfZb1&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt;: No cost involved.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;User-Friendly Panel:&lt;/strong&gt; Designed for easy navigation, configuration and usage.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Support Fragment:&lt;/strong&gt; Provides support for fragment functionality.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ad Blocking (Optional):&lt;/strong&gt; Option to block Ads.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Direct Iran (Optional):&lt;/strong&gt; Includes an option for direct access to Iran.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Full routing rules:&lt;/strong&gt; Bypassing Iran, Blocking Ads, Malwares, Phishing... for Sing-box.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Chain Proxy:&lt;/strong&gt; Capable of adding a chain proxy to fix IP.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Supports Wide Range of Clients:&lt;/strong&gt; Offers subscription links for Xray and Sing-box core clients.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Subscription Link (JSON):&lt;/strong&gt; Provides subscription link for JSON configs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Password-Protected Panel:&lt;/strong&gt; Secure your panel with password protection.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Custom Cloudflare Clean IP:&lt;/strong&gt; Ability to use online scanner and setting up clean IP-domains.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;How to use:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bia-pain-bache/BPB-Worker-Panel/main/docs/pages_installation_fa.md&#34;&gt;Installation (Pages)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bia-pain-bache/BPB-Worker-Panel/main/docs/worker_installation_fa.md&#34;&gt;Installation (Worker)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bia-pain-bache/BPB-Worker-Panel/main/docs/configuration_fa.md&#34;&gt;How to use&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bia-pain-bache/BPB-Worker-Panel/main/docs/faq.md&#34;&gt;FAQ&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Supported Clients&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Client&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Version&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Fragment&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;v2rayNG&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.8.19 or higher&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;v2rayN&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.42 or higher&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Nekobox&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Sing-box&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1.8.10 or higher&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Streisand&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;V2Box&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Shadowrocket&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Nekoray&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;✔&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;Hiddify&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;span&gt;❌&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Stargazers Over Time&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://starchart.cc/bia-pain-bache/BPB-Worker-Panel&#34;&gt;&lt;img src=&#34;https://starchart.cc/bia-pain-bache/BPB-Worker-Panel.svg?variant=adaptive&#34; alt=&#34;Stargazers Over Time&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Special Thanks&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CF-vless code author &lt;a href=&#34;https://github.com/3Kmfi6HP/EDtunnel&#34;&gt;3Kmfi6HP&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CF preferred IP program author &lt;a href=&#34;https://github.com/badafans/Cloudflare-IP-SpeedTest&#34;&gt;badafans&lt;/a&gt;, &lt;a href=&#34;https://github.com/XIU2/CloudflareSpeedTest&#34;&gt;XIU2&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;For a detailed tutorial on the core script, please refer to &lt;a href=&#34;https://ygkkk.blogspot.com/2023/07/cfworkers-vless.html&#34;&gt;Yongge’s blog and video tutorials&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>