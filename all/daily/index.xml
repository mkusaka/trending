<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-28T01:28:42Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>microsoft/MS-DOS</title>
    <updated>2024-04-28T01:28:42Z</updated>
    <id>tag:github.com,2024-04-28:/microsoft/MS-DOS</id>
    <link href="https://github.com/microsoft/MS-DOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The original sources of MS-DOS 1.25, 2.0, and 4.0 for reference purposes&lt;/p&gt;&lt;hr&gt;&lt;img width=&#34;150&#34; height=&#34;150&#34; align=&#34;left&#34; style=&#34;float: left; margin: 0 10px 0 0;&#34; alt=&#34;MS-DOS logo&#34; src=&#34;https://github.com/Microsoft/MS-DOS/raw/main/.readmes/msdos-logo.png&#34;&gt; &#xA;&lt;h1&gt;MS-DOS v1.25, v2.0, v4.0 Source Code&lt;/h1&gt; &#xA;&lt;p&gt;This repo contains the original source-code and compiled binaries for MS-DOS v1.25 and MS-DOS v2.0, plus the source-code for MS-DOS v4.00 jointly developed by IBM and Microsoft.&lt;/p&gt; &#xA;&lt;p&gt;The MS-DOS v1.25 and v2.0 files &lt;a href=&#34;http://www.computerhistory.org/atchm/microsoft-ms-dos-early-source-code/&#34;&gt;were originally shared at the Computer History Museum on March 25th, 2014&lt;/a&gt; and are being (re)published in this repo to make them easier to find, reference-to in external writing and works, and to allow exploration and experimentation for those interested in early PC Operating Systems.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;All files within this repo are released under the &lt;a href=&#34;https://en.wikipedia.org/wiki/MIT_License&#34;&gt;MIT License&lt;/a&gt; as per the &lt;a href=&#34;https://github.com/Microsoft/MS-DOS/raw/main/LICENSE&#34;&gt;LICENSE file&lt;/a&gt; stored in the root of this repo.&lt;/p&gt; &#xA;&lt;h1&gt;For historical reference&lt;/h1&gt; &#xA;&lt;p&gt;The source files in this repo are for historical reference and will be kept static, so please &lt;strong&gt;donâ€™t send&lt;/strong&gt; Pull Requests suggesting any modifications to the source files, but feel free to fork this repo and experiment ðŸ˜Š.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h1&gt;Trademarks&lt;/h1&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>EricLBuehler/mistral.rs</title>
    <updated>2024-04-28T01:28:42Z</updated>
    <id>tag:github.com,2024-04-28:/EricLBuehler/mistral.rs</id>
    <link href="https://github.com/EricLBuehler/mistral.rs" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Blazingly fast LLM inference.&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; mistral.rs &lt;/h1&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; Blazingly fast LLM inference. &lt;/h3&gt; &#xA;&lt;p align=&#34;center&#34;&gt; | &lt;a href=&#34;https://ericlbuehler.github.io/mistral.rs/mistralrs/&#34;&gt;&lt;b&gt;Rust Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://github.com/EricLBuehler/mistral.rs/raw/master/mistralrs-pyo3/API.md&#34;&gt;&lt;b&gt;Python Documentation&lt;/b&gt;&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/SZrecqK8qw&#34;&gt;&lt;b&gt;Discord&lt;/b&gt;&lt;/a&gt; | &lt;/p&gt; &#xA;&lt;p&gt;Mistral.rs is a fast LLM inference platform supporting inference on a variety of devices, quantization, and easy-to-use application with an Open-AI API compatible HTTP server and Python bindings.&lt;/p&gt; &#xA;&lt;h2&gt;Upcoming features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;More models: please submit requests &lt;a href=&#34;https://github.com/EricLBuehler/mistral.rs/issues/156&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;X-LoRA: Scalings &lt;code&gt;topk&lt;/code&gt; and softmax &lt;code&gt;topk&lt;/code&gt; (&lt;a href=&#34;https://github.com/EricLBuehler/mistral.rs/issues/48&#34;&gt;#48&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Parallel linear layers (sharding) (&lt;a href=&#34;https://github.com/EricLBuehler/mistral.rs/issues/50&#34;&gt;#50&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Speculative decoding: &lt;a href=&#34;https://arxiv.org/pdf/2211.17192&#34;&gt;https://arxiv.org/pdf/2211.17192&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running the new Llama 3 model&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;cargo run --release --features ... -- -i plain -m meta-llama/Meta-Llama-3-8B-Instruct -a llama&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Running the new Phi 3 model with 128K context window&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;cargo run --release --features ... -- -i plain -m microsoft/Phi-3-mini-128k-instruct -a phi3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Fast&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Quantized model support: 2-bit, 3-bit, 4-bit, 5-bit, 6-bit and 8-bit for faster inference and optimized memory usage.&lt;/li&gt; &#xA; &lt;li&gt;Continuous batching.&lt;/li&gt; &#xA; &lt;li&gt;Prefix caching.&lt;/li&gt; &#xA; &lt;li&gt;Device mapping: load and run some layers on the device and the reset on the CPU.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Accelerator support&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Apple silicon support with the Metal framework.&lt;/li&gt; &#xA; &lt;li&gt;CPU inference with &lt;code&gt;mkl&lt;/code&gt;, &lt;code&gt;accelerate&lt;/code&gt; support and optimized backend.&lt;/li&gt; &#xA; &lt;li&gt;CUDA support with flash attention and cuDNN.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Easy&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lightweight OpenAI API compatible HTTP server.&lt;/li&gt; &#xA; &lt;li&gt;Python API.&lt;/li&gt; &#xA; &lt;li&gt;Grammar support with Regex and Yacc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/ISQ.md&#34;&gt;ISQ&lt;/a&gt; (In situ quantization): run &lt;code&gt;.safetensors&lt;/code&gt; models directly from Huggingface Hub by quantizing them after loading instead of creating a GGUF file. This loads the ISQ-able weights on CPU before quantizing with ISQ and then moving back to the device to avoid memory spikes.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Powerful&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fast LoRA support with weight merging.&lt;/li&gt; &#xA; &lt;li&gt;First X-LoRA inference platform with first class support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is a demo of interactive mode with streaming running Mistral GGUF:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/EricLBuehler/mistral.rs/assets/65165915/3396abcd-8d44-4bf7-95e6-aa532db09415&#34;&gt;https://github.com/EricLBuehler/mistral.rs/assets/65165915/3396abcd-8d44-4bf7-95e6-aa532db09415&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Supported models:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Mistral 7B (v0.1 and v0.2)&lt;/li&gt; &#xA; &lt;li&gt;Gemma&lt;/li&gt; &#xA; &lt;li&gt;Llama, including Llama 3&lt;/li&gt; &#xA; &lt;li&gt;Mixtral 8x7B&lt;/li&gt; &#xA; &lt;li&gt;Phi 2&lt;/li&gt; &#xA; &lt;li&gt;Phi 3&lt;/li&gt; &#xA; &lt;li&gt;Qwen 2&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/#supported-models&#34;&gt;this section&lt;/a&gt; for details on quantization and LoRA support.&lt;/p&gt; &#xA;&lt;h2&gt;APIs and Integrations&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rust Library API&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Rust multithreaded API for easy integration into any application.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ericlbuehler.github.io/mistral.rs/mistralrs/&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/mistralrs/examples/&#34;&gt;Examples&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;To install: Add &lt;code&gt;mistralrs = { git = &#34;https://github.com/EricLBuehler/mistral.rs.git&#34; }&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Python API for mistral.rs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/mistralrs-pyo3/README.md&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/mistralrs-pyo3/API.md&#34;&gt;Docs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/python/python_api.py&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/python/cookbook.ipynb&#34;&gt;Cookbook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mistralrs import Runner, Which, ChatCompletionRequest, Message, Role&#xA;&#xA;runner = Runner(&#xA;    which=Which.GGUF(&#xA;        tok_model_id=&#34;mistralai/Mistral-7B-Instruct-v0.1&#34;,&#xA;        quantized_model_id=&#34;TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;,&#xA;        quantized_filename=&#34;mistral-7b-instruct-v0.1.Q4_K_M.gguf&#34;,&#xA;        tokenizer_json=None,&#xA;        repeat_last_n=64,&#xA;    )&#xA;)&#xA;&#xA;res = runner.send_chat_completion_request(&#xA;    ChatCompletionRequest(&#xA;        model=&#34;mistral&#34;,&#xA;        messages=[Message(Role.User, &#34;Tell me a story about the Rust type system.&#34;)],&#xA;        max_tokens=256,&#xA;        presence_penalty=1.0,&#xA;        top_p=0.1,&#xA;        temperature=0.1,&#xA;    )&#xA;)&#xA;print(res.choices[0].message.content)&#xA;print(res.usage)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;HTTP Server&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;OpenAI API compatible API server&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/http.md&#34;&gt;API Docs&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/README.md#run&#34;&gt;Running&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/server/chat.py&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Llama Index integration&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/integrations/llama_index_integration.py&#34;&gt;Source&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/llama_index/xlora_gguf.py&#34;&gt;Example&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/llama_index/cookbook.ipynb&#34;&gt;Cookbook&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Supported accelerators&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CUDA: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enable with &lt;code&gt;cuda&lt;/code&gt; feature: &lt;code&gt;--features cuda&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Flash attention support with &lt;code&gt;flash-attn&lt;/code&gt; feature, only applicable to non-quantized models: &lt;code&gt;--features flash-attn&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;cuDNNsupport with &lt;code&gt;cudnn&lt;/code&gt; feature: &lt;code&gt;--features cudnn&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Metal: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Enable with &lt;code&gt;metal&lt;/code&gt; feature: &lt;code&gt;--features metal&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;CPU: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Intel MKL with &lt;code&gt;mkl&lt;/code&gt; feature: &lt;code&gt;--features mkl&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Apple Accelerate with &lt;code&gt;accelerate&lt;/code&gt; feature: &lt;code&gt;--features accelerate&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Enabling features is done by passing &lt;code&gt;--features ...&lt;/code&gt; to the build system. When using &lt;code&gt;cargo run&lt;/code&gt; or &lt;code&gt;maturin develop&lt;/code&gt;, pass the &lt;code&gt;--features&lt;/code&gt; flag before the &lt;code&gt;--&lt;/code&gt; separating build flags from runtime flags.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To enable a single feature like &lt;code&gt;metal&lt;/code&gt;: &lt;code&gt;cargo build --release --features metal&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;To enable multiple features, specify them in quotes: &lt;code&gt;cargo build --release --features &#34;cuda flash-attn cudnn&#34;&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Device&lt;/th&gt; &#xA;   &lt;th&gt;Mistral.rs Completion T/s&lt;/th&gt; &#xA;   &lt;th&gt;Llama.cpp Completion T/s&lt;/th&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Quant&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A10 GPU, CUDA&lt;/td&gt; &#xA;   &lt;td&gt;78&lt;/td&gt; &#xA;   &lt;td&gt;78&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;&gt;mistral-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4_K_M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Intel Xeon 8358 CPU, AVX&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;19&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;&gt;mistral-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4_K_M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Raspberry Pi 5 (8GB), Neon&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;em&gt;segfault&lt;/em&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;&gt;mistral-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2_K&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A100 GPU, CUDA&lt;/td&gt; &#xA;   &lt;td&gt;110&lt;/td&gt; &#xA;   &lt;td&gt;119&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/TheBloke/Mistral-7B-Instruct-v0.1-GGUF&#34;&gt;mistral-7b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4_K_M&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Please submit more benchmarks via raising an issue!&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installation and Build&lt;/h3&gt; &#xA;&lt;p&gt;To install mistral.rs, one should ensure they have Rust installed by following &lt;a href=&#34;https://rustup.rs/&#34;&gt;this&lt;/a&gt; link. Additionally, the Huggingface token should be provided in &lt;code&gt;~/.cache/huggingface/token&lt;/code&gt; when using the server to enable automatic download of gated models.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install required packages&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;openssl&lt;/code&gt; (ex., &lt;code&gt;sudo apt install libssl-dev&lt;/code&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;pkg-config&lt;/code&gt; (ex., &lt;code&gt;sudo apt install pkg-config&lt;/code&gt;)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install Rust: &lt;a href=&#34;https://rustup.rs/&#34;&gt;https://rustup.rs/&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;curl --proto &#39;=https&#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh&#xA;source $HOME/.cargo/env&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set HF token correctly (skip if already set or your model is not gated, or if you want to use the &lt;code&gt;token_source&lt;/code&gt; parameters in Python or the command line.)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir ~/.cache/huggingface&#xA;touch ~/.cache/huggingface/token&#xA;echo &amp;lt;HF_TOKEN_HERE&amp;gt; &amp;gt; ~/.cache/huggingface/token&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the code&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/EricLBuehler/mistral.rs.git&#xA;cd mistral.rs&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build or install&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Base build command&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with CUDA support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with CUDA and Flash Attention V2 support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features &#34;cuda flash-attn&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with Metal support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features metal&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with Accelerate support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Build with MKL support&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo build --release --features mkl&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Install with &lt;code&gt;cargo install&lt;/code&gt; for easy command line usage&lt;/p&gt; &lt;p&gt;Pass the same values to &lt;code&gt;--features&lt;/code&gt; as you would for &lt;code&gt;cargo build&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cargo install --path mistralrs-server --features cuda&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The build process will output a binary &lt;code&gt;misralrs-server&lt;/code&gt; at &lt;code&gt;./target/release/mistralrs-server&lt;/code&gt; which may be copied into the working directory with the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cp ./target/release/mistralrs-server ./mistralrs_server&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Installing Python support&lt;/p&gt; &lt;p&gt;You can install Python support by following the guide &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/mistralrs-pyo3/README.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Getting models&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Loading from HF Hub:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Mistral.rs can automatically download models from HF Hub. To access gated models, you should provide a token source. They may be one of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;literal:&amp;lt;value&amp;gt;&lt;/code&gt;: Load from a specified literal&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;env:&amp;lt;value&amp;gt;&lt;/code&gt;: Load from a specified environment variable&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;path:&amp;lt;value&amp;gt;&lt;/code&gt;: Load from a specified file&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cache&lt;/code&gt;: &lt;strong&gt;default&lt;/strong&gt;: Load from the HF token at ~/.cache/huggingface/token or equivalent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;none&lt;/code&gt;: Use no HF token&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;This is passed in the following ways:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Command line:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --token-source none -i plain -m microsoft/Phi-3-mini-128k-instruct -a phi3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/examples/python/token_source.py&#34;&gt;Here&lt;/a&gt; is an example of setting the token source.&lt;/p&gt; &#xA;&lt;p&gt;If token cannot be loaded, no token will be used (i.e. effectively using &lt;code&gt;none&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Loading from local files:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also instruct mistral.rs to load models locally by modifying the &lt;code&gt;*_model_id&lt;/code&gt; arguments or options:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 plain -m . -a mistral&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The following files must be present in the paths for the options below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--model-id&lt;/code&gt; (server) or &lt;code&gt;model_id&lt;/code&gt; (python) or &lt;code&gt;--tok-model-id&lt;/code&gt; (server) or &lt;code&gt;tok_model_id&lt;/code&gt; (python): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;tokenizer_config.json&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;tokenizer.json&lt;/code&gt; (if not specified separately)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;.safetensors&lt;/code&gt; files.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--quantized-model-id&lt;/code&gt; (server) or &lt;code&gt;quantized_model_id&lt;/code&gt; (python): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Specified &lt;code&gt;.gguf&lt;/code&gt; or &lt;code&gt;.ggml&lt;/code&gt; file.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--x-lora-model-id&lt;/code&gt; (server) or &lt;code&gt;xlora_model_id&lt;/code&gt; (python): &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;code&gt;xlora_classifier.safetensors&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;code&gt;xlora_config.json&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Adapters &lt;code&gt;.safetensors&lt;/code&gt; and &lt;code&gt;adapter_config.json&lt;/code&gt; files in their respective directories&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run&lt;/h3&gt; &#xA;&lt;p&gt;To start a server serving Mistral GGUF on &lt;code&gt;localhost:1234&lt;/code&gt;,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 --log output.log gguf -m TheBloke/Mistral-7B-Instruct-v0.1-GGUF -t mistralai/Mistral-7B-Instruct-v0.1 -f mistral-7b-instruct-v0.1.Q4_K_M.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Mistral.rs uses subcommands to control the model type. They are generally of format &lt;code&gt;&amp;lt;XLORA/LORA&amp;gt;-&amp;lt;QUANTIZATION&amp;gt;&lt;/code&gt;. Please run &lt;code&gt;./mistralrs_server --help&lt;/code&gt; to see the subcommands.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, for models without quantization, the model architecture should be provided as the &lt;code&gt;--arch&lt;/code&gt; or &lt;code&gt;-a&lt;/code&gt; argument in contrast to GGUF models which encode the architecture in the file. It should be one of the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;mistral&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;gemma&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mixtral&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;llama&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;phi2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;phi3&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;qwen2&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Interactive mode:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can launch interactive mode, a simple chat application running in the terminal, by passing &lt;code&gt;-i&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server -i gguf -t mistralai/Mistral-7B-Instruct-v0.1 -m TheBloke/Mistral-7B-Instruct-v0.1-GGUF -f mistral-7b-instruct-v0.1.Q4_K_M.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quick examples:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;X-LoRA with no quantization&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start an X-LoRA server with the exactly as presented in &lt;a href=&#34;https://arxiv.org/abs/2402.07148&#34;&gt;the paper&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 x-lora-plain -o orderings/xlora-paper-ordering.json -x lamm-mit/x-lora&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LoRA with a model from GGUF&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start an LoRA server with adapters from the X-LoRA paper (you should modify the ordering file to use only one adapter, as the adapter static scalings are all 1 and so the signal will become distorted):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 lora-gguf -o orderings/xlora-paper-ordering.json -m TheBloke/zephyr-7B-beta-GGUF -f zephyr-7b-beta.Q8_0.gguf -x lamm-mit/x-lora&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Normally with a LoRA model you would use a custom ordering file. However, for this example we use the ordering from the X-LoRA paper because we are using the adapters from the X-LoRA paper.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With a model from GGUF&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start a server running Mistral from GGUF:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 gguf -t mistralai/Mistral-7B-Instruct-v0.1 -m TheBloke/Mistral-7B-Instruct-v0.1-GGUF -f mistral-7b-instruct-v0.1.Q4_K_M.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;With a model from GGML&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start a server running Llama from GGML:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 ggml -t meta-llama/Llama-2-13b-chat-hf -m TheBloke/Llama-2-13B-chat-GGML -f llama-2-13b-chat.ggmlv3.q4_K_M.bin&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain model from safetensors&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To start a server running Mistral from safetensors.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./mistralrs_server --port 1234 gguf -m mistralai/Mistral-7B-Instruct-v0.1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Command line docs&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Command line docs &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/CMD_LINE_DOCS.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Supported models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Quantization support&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;GGUF&lt;/th&gt; &#xA;   &lt;th&gt;GGML&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral 7B&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral 8x7B&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 2&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 3&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen 2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Device mapping support&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Supported&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Normal&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GGUF&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GGML&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;X-LoRA and LoRA support&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;X-LoRA&lt;/th&gt; &#xA;   &lt;th&gt;X-LoRA+GGUF&lt;/th&gt; &#xA;   &lt;th&gt;X-LoRA+GGML&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mistral 7B&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemma&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Mixtral 8x7B&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 2&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi 3&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;âœ…&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen 2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Using derivative models&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To use a derivative model, select the model architecture using the correct subcommand. To see what can be passed for the architecture, pass &lt;code&gt;--help&lt;/code&gt; after the subcommand. For example, when using a different model than the default, specify the following for the following types of models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Normal&lt;/strong&gt;: Model id&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Quantized&lt;/strong&gt;: Quantized model id, quantized filename, and tokenizer id&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;X-LoRA&lt;/strong&gt;: Model id, X-LoRA ordering&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;X-LoRA quantized&lt;/strong&gt;: Quantized model id, quantized filename, tokenizer id, and X-LoRA ordering&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt;: Model id, LoRA ordering&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LoRA quantized&lt;/strong&gt;: Quantized model id, quantized filename, tokenizer id, and LoRA ordering&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/#adapter-ordering-file&#34;&gt;this&lt;/a&gt; section to determine if it is necessary to prepare an X-LoRA/LoRA ordering file, it is always necessary if the target modules or architecture changed, or if the adapter order changed.&lt;/p&gt; &#xA;&lt;p&gt;It is also important to check the chat template style of the model. If the HF hub repo has a &lt;code&gt;tokenizer_config.json&lt;/code&gt; file, it is not necessary to specify. Otherwise, templates can be found in &lt;code&gt;chat_templates&lt;/code&gt; and should be passed before the subcommand. If the model is not instruction tuned, no chat template will be found and the APIs will only accept a prompt, no messages.&lt;/p&gt; &#xA;&lt;p&gt;For example, when using a Zephyr model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;./mistralrs_server --port 1234 --log output.txt gguf -t HuggingFaceH4/zephyr-7b-beta -m TheBloke/zephyr-7B-beta-GGUF -f zephyr-7b-beta.Q5_0.gguf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Adapter model support: X-LoRA and LoRA&lt;/h3&gt; &#xA;&lt;p&gt;An adapter model is a model with X-LoRA or LoRA. X-LoRA support is provided by selecting the &lt;code&gt;x-lora-*&lt;/code&gt; architecture, and LoRA support by selecting the &lt;code&gt;lora-*&lt;/code&gt; architecture. Please find docs for adapter models &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/ADAPTER_MODELS.md&#34;&gt;here&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Chat Templates and Tokenizer&lt;/h3&gt; &#xA;&lt;p&gt;Mistral.rs will attempt to automatically load a chat template and tokenizer. This enables high flexibility across models and ensures accurate and flexible chat templating. However, this behavior can be customized. Please find detailed documentation &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/CHAT_TOK.md&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you have any problems or want to contribute something, please raise an issue or pull request!&lt;/p&gt; &#xA;&lt;p&gt;Consider enabling &lt;code&gt;RUST_LOG=debug&lt;/code&gt; environment variable.&lt;/p&gt; &#xA;&lt;p&gt;If you want to add a new model, please see &lt;a href=&#34;https://raw.githubusercontent.com/EricLBuehler/mistral.rs/master/docs/ADDING_MODELS.md&#34;&gt;our guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;This project would not be possible without the excellent work at &lt;a href=&#34;https://github.com/huggingface/candle&#34;&gt;&lt;code&gt;candle&lt;/code&gt;&lt;/a&gt;. Additionally, thank you to all contributors! Contributing can range from raising an issue or suggesting a feature to adding some new functionality.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>KingsGambitLab/Lecture_Notes</title>
    <updated>2024-04-28T01:28:42Z</updated>
    <id>tag:github.com,2024-04-28:/KingsGambitLab/Lecture_Notes</id>
    <link href="https://github.com/KingsGambitLab/Lecture_Notes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This repository is there to store the combined lecture notes of all the lectures. We are using markdown to write the lecture notes.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lecture_Notes&lt;/h1&gt; &#xA;&lt;p&gt;This repository is there to store the combined lecture notes of all the lectures. We are using markdown to write the lecture notes.&lt;/p&gt; &#xA;&lt;p&gt;To render math in the notes, use &lt;a href=&#34;https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima&#34;&gt;https://chrome.google.com/webstore/detail/mathjax-plugin-for-github/ioemnmodlmafdkllaclgeombjnmnbima&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Guidelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Keep code snippets in Python/Psuedocode&lt;/li&gt; &#xA; &lt;li&gt;For language specific concepts, make sure to contrast it with other languages&lt;/li&gt; &#xA; &lt;li&gt;Use LaTeX math mode when possible. Quick References: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference&#34;&gt;https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://jojozhuang.github.io/blog/2018/09/11/mathjax-cheat-sheet-for-mathematical-notation/&#34;&gt;https://jojozhuang.github.io/blog/2018/09/11/mathjax-cheat-sheet-for-mathematical-notation/&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Soft resize images to appropriate size (use html img tag with inline style)&lt;/li&gt; &#xA; &lt;li&gt;Use spaces instead of tabs (important, to keep indentation in code blocks consistent)&lt;/li&gt; &#xA; &lt;li&gt;Clearly mark the begin and end of each topic / question by using HR. markdown syntax: &lt;code&gt;-- --&lt;/code&gt; on a new line&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;em&gt;Italics&lt;/em&gt; minimally (hard to detect). Use &lt;strong&gt;bold&lt;/strong&gt; and headings instead.&lt;/li&gt; &#xA; &lt;li&gt;Don&#39;t use H1&lt;/li&gt; &#xA; &lt;li&gt;Use the following syntax for H2 instead of the usual &lt;code&gt;# heading 2&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-md&#34;&gt;Heading 2&#xA;---------&#xA;&#xA;content&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>