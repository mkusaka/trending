<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-15T02:45:34Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>datawhalechina/llm-universe</title>
    <updated>2024-04-15T02:45:34Z</updated>
    <id>tag:github.com,2024-04-15:/datawhalechina/llm-universe</id>
    <link href="https://github.com/datawhalechina/llm-universe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;本项目是一个面向小白开发者的大模型应用开发教程，在线阅读地址：https://datawhalechina.github.io/llm-universe/&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;动手学大模型应用开发&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/figures/C0-0-logo.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;项目简介&lt;/h2&gt; &#xA;&lt;p&gt;本项目是一个面向小白开发者的大模型应用开发教程，旨在基于阿里云服务器，结合个人知识库助手项目，通过一个课程完成大模型开发的重点入门，主要内容包括：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;大模型简介&lt;/strong&gt;，何为大模型、大模型特点是什么、LangChain 是什么，如何开发一个 LLM 应用，针对小白开发者的简单介绍；&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;如何调用大模型 API&lt;/strong&gt;，本节介绍了国内外知名大模型产品 API 的多种调用方式，包括调用原生 API、封装为 LangChain LLM、封装为 Fastapi 等调用方式，同时将包括百度文心、讯飞星火、智谱AI等多种大模型 API 进行了统一形式封装；&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;知识库搭建&lt;/strong&gt;，不同类型知识库文档的加载、处理，向量数据库的搭建；&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;构建 RAG 应用&lt;/strong&gt;，包括将 LLM 接入到 LangChain 构建检索问答链，使用 Streamlit 进行应用部署&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;验证迭代&lt;/strong&gt;，大模型开发如何实现验证迭代，一般的评估方法有什么；&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;本项目主要包括三部分内容：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM 开发入门&lt;/strong&gt;。V1 版本的简化版，旨在帮助初学者最快、最便捷地入门 LLM 开发，理解 LLM 开发的一般流程，可以搭建出一个简单的 Demo。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM 开发技巧&lt;/strong&gt;。LLM 开发更进阶的技巧，包括但不限于：Prompt Engineering、多类型源数据的处理、优化检索、召回精排、Agent 框架等&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM 应用实例&lt;/strong&gt;。引入一些成功的开源案例，从本课程的角度出发，解析这些应用范例的 Idea、核心思路、实现框架，帮助初学者明白其可以通过 LLM 开发什么样的应用。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;目前，第一部分已经完稿，欢迎大家阅读学习；第二、三部分正在创作中。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;目录结构说明：&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;  requirements.txt：官方环境下的安装依赖&#xA;  notebook：Notebook 源代码文件&#xA;  docs：Markdown 文档文件&#xA;  figures：图片&#xA;  data_base：所使用的知识库源文件&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;项目意义&lt;/h2&gt; &#xA;&lt;p&gt;LLM 正逐步成为信息世界的新革命力量，其通过强大的自然语言理解、自然语言生成能力，为开发者提供了新的、更强大的应用开发选择。随着国内外井喷式的 LLM API 服务开放，如何基于 LLM API 快速、便捷地开发具备更强能力、集成 LLM 的应用，开始成为开发者的一项重要技能。&lt;/p&gt; &#xA;&lt;p&gt;目前，关于 LLM 的介绍以及零散的 LLM 开发技能课程已有不少，但质量参差不齐，且没有很好地整合，开发者需要搜索大量教程并阅读大量相关性不强、必要性较低的内容，才能初步掌握大模型开发的必备技能，学习效率低，学习门槛也较高。&lt;/p&gt; &#xA;&lt;p&gt;本项目从实践出发，结合最常见、通用的个人知识库助手项目，深入浅出逐步拆解 LLM 开发的一般流程、步骤，旨在帮助没有算法基础的小白通过一个课程完成大模型开发的基础入门。同时，我们也会加入 RAG 开发的进阶技巧以及一些成功的 LLM 应用案例的解读，帮助完成第一部分学习的读者进一步掌握更高阶的 RAG 开发技巧，并能够通过对已有成功项目的借鉴开发自己的、好玩的应用。&lt;/p&gt; &#xA;&lt;h2&gt;项目受众&lt;/h2&gt; &#xA;&lt;p&gt;所有具备基础 Python 能力，想要掌握 LLM 应用开发技能的开发者。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;本项目对学习者的人工智能基础、算法基础没有任何要求，仅需要掌握基本 Python 语法、掌握初级 Python 开发技能即可。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;考虑到环境搭建问题，本项目提供了阿里云服务器学生免费领取方式，学生读者可以免费领取阿里云服务器，并通过阿里云服务器完成本课程的学习；本项目同时也提供了个人电脑及非阿里云服务器的环境搭建指南；本项目对本地硬件基本没有要求，不需要 GPU 环境，个人电脑及服务器均可用于学习。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;注：本项目主要使用各大模型厂商提供的 API 来进行应用开发，如果你想要学习部署应用本地开源 LLM，欢迎学习同样由 Datawhale 出品的 &lt;a href=&#34;https://github.com/datawhalechina/self-llm&#34;&gt;Self LLM ｜ 开源大模型食用指南&lt;/a&gt;，该项目将手把手教你如何速通开源 LLM 部署微调全链路！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;注：考虑到学习难度，本项目主要面向初学者，介绍如何使用 LLM 来搭建应用。如果你想要进一步深入学习 LLM 的理论基础，并在理论的基础上进一步认识、应用 LLM，欢迎学习同样由 Datawhale 出品的 &lt;a href=&#34;https://github.com/datawhalechina/so-large-lm&#34;&gt;So Large LM | 大模型基础&lt;/a&gt;，该项目将为你提供全面而深入的 LLM 理论知识及实践方法！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目亮点&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;充分面向实践，动手学习大模型开发。相较于其他从理论入手、与实践代差较大的类似教程，本教程基于具有通用性的个人知识库助手项目打造，将普适的大模型开发理念融合在项目实践中，帮助学习者通过动手搭建个人项目来掌握大模型开发技能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;从零开始，全面又简短的大模型教程。本项目针对个人知识库助手项目，对相关大模型开发理论、概念和基本技能进行了项目主导的重构，删去不需要理解的底层原理和算法细节，涵盖所有大模型开发的核心技能。教程整体时长在数小时之内，但学习完本教程，可以掌握基础大模型开发的所有核心技能。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;兼具统一性与拓展性。本项目对 GPT、百度文心、讯飞星火、智谱GLM 等国内外主要 LLM API 进行了统一封装，支持一键调用不同的 LLM，帮助开发者将更多的精力放在学习应用与模型本身的优化上，而不需要花时间在繁琐的调用细节上；同时，本教程拟上线 &lt;a href=&#34;https://1aigc.cn/&#34;&gt;奇想星球 | AIGC共创社区平台&lt;/a&gt;，支持学习者自定义项目为本教程增加拓展内容，具备充分的拓展性。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;在线阅读地址&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://datawhalechina.github.io/llm-universe/&#34;&gt;https://datawhalechina.github.io/llm-universe/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;PDF 地址&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/datawhalechina/llm-universe/releases/tag/v1&#34;&gt;https://github.com/datawhalechina/llm-universe/releases/tag/v1&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;内容大纲&lt;/h2&gt; &#xA;&lt;h3&gt;第一部分 LLM 开发入门&lt;/h3&gt; &#xA;&lt;p&gt;负责人：邹雨衡&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/&#34;&gt;LLM 介绍&lt;/a&gt; @高立业 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/1.%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E7%90%86%E8%AE%BA%E7%AE%80%E4%BB%8B.md&#34;&gt;LLM 的理论介绍&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/2.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%20RAG%20%E7%AE%80%E4%BB%8B.md&#34;&gt;什么是 RAG，RAG 的核心优势&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/3.LangChain%20%E7%AE%80%E4%BB%8B.md&#34;&gt;什么是 LangChain&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/4.%E5%BC%80%E5%8F%91%20LLM%20%E5%BA%94%E7%94%A8%E7%9A%84%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B.md&#34;&gt;开发 LLM 应用的整体流程&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/5.%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8.md&#34;&gt;阿里云服务器的基本使用&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C1%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%20LLM%20%E4%BB%8B%E7%BB%8D/6.%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.md&#34;&gt;环境配置&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/&#34;&gt;使用 LLM API 开发应用&lt;/a&gt; @毛雨 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/1.%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5.md&#34;&gt;基本概念&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/2.%20%E4%BD%BF%E7%94%A8%20LLM%20API.ipynb&#34;&gt;使用 LLM API&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;ChatGPT&lt;/li&gt; &#xA;     &lt;li&gt;文心一言&lt;/li&gt; &#xA;     &lt;li&gt;讯飞星火&lt;/li&gt; &#xA;     &lt;li&gt;智谱 GLM&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C2%20%E4%BD%BF%E7%94%A8%20LLM%20API%20%E5%BC%80%E5%8F%91%E5%BA%94%E7%94%A8/3.%20Prompt%20Engineering.ipynb&#34;&gt;Prompt Engineering&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/&#34;&gt;搭建知识库&lt;/a&gt; @娄天奥 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/1.%E8%AF%8D%E5%90%91%E9%87%8F%E5%8F%8A%E5%90%91%E9%87%8F%E7%9F%A5%E8%AF%86%E5%BA%93%E4%BB%8B%E7%BB%8D.md&#34;&gt;词向量及向量知识库介绍&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/2.%E4%BD%BF%E7%94%A8%20Embedding%20API.ipynb&#34;&gt;使用 Embedding API&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/3.%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.ipynb&#34;&gt;数据处理：读取、清洗与切片&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C3%20%E6%90%AD%E5%BB%BA%E7%9F%A5%E8%AF%86%E5%BA%93/4.%E6%90%AD%E5%BB%BA%E5%B9%B6%E4%BD%BF%E7%94%A8%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93.ipynb&#34;&gt;搭建并使用向量数据库&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/&#34;&gt;构建 RAG 应用&lt;/a&gt; @徐虎 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/1.LLM%20%E6%8E%A5%E5%85%A5%20LangChain.ipynb&#34;&gt;将 LLM 接入 LangChain&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;ChatGPT&lt;/li&gt; &#xA;     &lt;li&gt;文心一言&lt;/li&gt; &#xA;     &lt;li&gt;讯飞星火&lt;/li&gt; &#xA;     &lt;li&gt;智谱 GLM&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/2.%E6%9E%84%E5%BB%BA%E6%A3%80%E7%B4%A2%E9%97%AE%E7%AD%94%E9%93%BE.ipynb&#34;&gt;基于 LangChain 搭建检索问答链&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C4%20%E6%9E%84%E5%BB%BA%20RAG%20%E5%BA%94%E7%94%A8/3.%E9%83%A8%E7%BD%B2%E7%9F%A5%E8%AF%86%E5%BA%93%E5%8A%A9%E6%89%8B.ipynb&#34;&gt;基于 Streamlit 部署知识库助手&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C5%20%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96/&#34;&gt;系统评估与优化&lt;/a&gt; @邹雨衡 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C5%20%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96/1.%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0%20LLM%20%E5%BA%94%E7%94%A8.ipynb&#34;&gt;如何评估 LLM 应用&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C5%20%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96/2.%E8%AF%84%E4%BC%B0%E5%B9%B6%E4%BC%98%E5%8C%96%E7%94%9F%E6%88%90%E9%83%A8%E5%88%86.ipynb&#34;&gt;评估并优化生成部分&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/datawhalechina/llm-universe/main/notebook/C5%20%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E4%B8%8E%E4%BC%98%E5%8C%96/3.%E8%AF%84%E4%BC%B0%E5%B9%B6%E4%BC%98%E5%8C%96%E6%A3%80%E7%B4%A2%E9%83%A8%E5%88%86.md&#34;&gt;评估并优化检索部分&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;第二部分 进阶 RAG 技巧（正在创作）&lt;/h3&gt; &#xA;&lt;p&gt;负责人：高立业&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;背景 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;架构概览&lt;/li&gt; &#xA;   &lt;li&gt;存在的问题&lt;/li&gt; &#xA;   &lt;li&gt;解决方法&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;数据处理 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;多类型文档处理&lt;/li&gt; &#xA;   &lt;li&gt;分块优化&lt;/li&gt; &#xA;   &lt;li&gt;向量模型的选择&lt;/li&gt; &#xA;   &lt;li&gt;微调向量模型（进阶）&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;索引层面 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;索引结构&lt;/li&gt; &#xA;   &lt;li&gt;混合检索&lt;/li&gt; &#xA;   &lt;li&gt;假设性问题&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;检索阶段 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;query 过滤&lt;/li&gt; &#xA;   &lt;li&gt;对齐 query 和 文档&lt;/li&gt; &#xA;   &lt;li&gt;对齐检索和 LLM&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;生成阶段 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;后处理&lt;/li&gt; &#xA;   &lt;li&gt;微调 LLM（进阶）&lt;/li&gt; &#xA;   &lt;li&gt;参考引用&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;增强阶段 &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;上下文增强&lt;/li&gt; &#xA;   &lt;li&gt;增强流程&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt;RAG 工程化评估&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;第三部分 开源 LLM 应用解读&lt;/h3&gt; &#xA;&lt;p&gt;负责人：徐虎&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;ChatWithDatawhale——个人知识库助手解读&lt;/li&gt; &#xA; &lt;li&gt;天机——人情世故大模型解读&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;核心贡献者&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/logan-zou&#34;&gt;邹雨衡-项目负责人&lt;/a&gt;（Datawhale成员-对外经济贸易大学研究生）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/0-yy-0&#34;&gt;高立业-第二部分负责人&lt;/a&gt;（DataWhale成员-算法工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/xuhu0115&#34;&gt;徐虎-第三部分负责人&lt;/a&gt;（Datawhale成员-算法工程师）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;主要贡献者&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Myoungs&#34;&gt;毛雨-内容创作者&lt;/a&gt;（后端开发工程师）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lta155&#34;&gt;娄天奥-内容创作者&lt;/a&gt;（Datawhale鲸英助教-中国科学院大学研究生）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/2951121599&#34;&gt;崔腾松-项目支持者&lt;/a&gt;（Datawhale成员-奇想星球联合发起人）&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/JuneYaooo&#34;&gt;June-项目支持者&lt;/a&gt;（Datawhale成员-奇想星球联合发起人）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;其他&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;特别感谢 &lt;a href=&#34;https://github.com/Sm1les&#34;&gt;@Sm1les&lt;/a&gt;、&lt;a href=&#34;https://github.com/LSGOMYP&#34;&gt;@LSGOMYP&lt;/a&gt; 对本项目的帮助与支持；&lt;/li&gt; &#xA; &lt;li&gt;特别感谢&lt;a href=&#34;https://1aigc.cn/&#34;&gt;奇想星球 | AIGC共创社区平台&lt;/a&gt;提供的支持，欢迎大家关注；&lt;/li&gt; &#xA; &lt;li&gt;如果有任何想法可以联系我们 DataWhale 也欢迎大家多多提出 issue；&lt;/li&gt; &#xA; &lt;li&gt;特别感谢以下为教程做出贡献的同学！&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;a href=&#34;https://github.com/datawhalechina/llm-universe/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=datawhalechina/llm-universe&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#datawhalechina/llm-universe&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=datawhalechina/llm-universe&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>PKU-YuanGroup/MagicTime</title>
    <updated>2024-04-15T02:45:34Z</updated>
    <id>tag:github.com,2024-04-15:/PKU-YuanGroup/MagicTime</id>
    <link href="https://github.com/PKU-YuanGroup/MagicTime" rel="alternate"></link>
    <summary type="html">&lt;p&gt;MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/MagicTime/main/__assets__/magictime_logo.png&#34; width=&#34;150px&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;MagicTime: Time-lapse Video Generation Models &lt;/a&gt;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;as Metamorphic Simulators&lt;/a&gt;&lt;/p&gt;&lt;/h2&gt;&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; If you like our project, please give us a star ⭐ on GitHub for the latest update. &lt;/h5&gt; &#xA;&lt;h5 align=&#34;center&#34;&gt; &lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/BestWishYsh/MagicTime?logs=build&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Open%20In%20Spaces-blue.svg?sanitize=true&#34; alt=&#34;hf_space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://replicate.com/camenduru/magictime&#34;&gt;&lt;img src=&#34;https://replicate.com/camenduru/magictime/badge&#34; alt=&#34;Replicate demo and cloud API&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/camenduru/MagicTime-jupyter&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/papers/2404.05014&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97-Paper%20In%20HF-red.svg?sanitize=true&#34; alt=&#34;hf_space&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Arxiv-2404.05014-b31b1b.svg?logo=arXiv&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pku-yuangroup.github.io/MagicTime/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-%3CWebsite%3E-blue.svg?sanitize=true&#34; alt=&#34;Home Page&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://drive.google.com/drive/folders/1WsomdkmSp3ql3ImcNsmzFuSQ9Qukuyr8?usp=sharing&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Dataset-%3CGoogle%3E-green&#34; alt=&#34;Dataset&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/_akhaliq/status/1777538468043792473&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Twitter@AK%20-black?logo=twitter&amp;amp;logoColor=1D9BF0&#34; alt=&#34;zhihu&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/vhjf36495872/status/1777525817087553827?s=61&amp;amp;t=r2HzCsU2AnJKbR8yKSprKw&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Twitter@Jinfa%20Huang%20-black?logo=twitter&amp;amp;logoColor=1D9BF0&#34; alt=&#34;zhihu&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://zenodo.org/doi/10.5281/zenodo.10960665&#34;&gt;&lt;img src=&#34;https://zenodo.org/badge/783303222.svg?sanitize=true&#34; alt=&#34;DOI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-yellow&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/MagicTime&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/p&gt; &lt;/h5&gt; &#xA;&lt;div align=&#34;center&#34;&gt;&#xA;  This repository is the official implementation of MagicTime, a metamorphic video generation pipeline based on the given prompts. The main idea is to enhance the capacity of video generation models to accurately depict the real world through our proposed methods and dataset. &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;details open&gt;&#xA; &lt;summary&gt;💡 We also have other video generation project that may interest you ✨. &lt;/summary&gt;&#xA; &lt;p&gt; &#xA;  &lt;!--  may --&gt; &lt;/p&gt;&#xA; &lt;blockquote&gt; &#xA;  &lt;p&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;&lt;strong&gt;Open-Sora-Plan&lt;/strong&gt;&lt;/a&gt; &lt;br&gt; PKU-Yuan Lab and Tuzhan AI etc. &lt;br&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/-Github-black?logo=github&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan.svg?style=social&#34; alt=&#34;github&#34;&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA; &lt;/blockquote&gt; &#xA; &lt;p&gt;&lt;/p&gt;&#xA;&lt;/details&gt; &#xA;&lt;h2&gt;📣 News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;⏳⏳⏳ Training a stronger model with the support of &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;Open-Sora-Plan&lt;/a&gt; (e.g 257 x 512 × 512).&lt;/li&gt; &#xA; &lt;li&gt;⏳⏳⏳ Release the training code of MagicTime.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.14]&lt;/code&gt; Thanks &lt;a href=&#34;https://twitter.com/camenduru&#34;&gt;@camenduru&lt;/a&gt; and &lt;a href=&#34;https://modelslab.com/&#34;&gt;@ModelsLab&lt;/a&gt; for providing Jupyter Notebook &lt;a href=&#34;https://github.com/camenduru/MagicTime-jupyter&#34;&gt;MagicTime-jupyter&lt;/a&gt; and &lt;a href=&#34;https://replicate.com/camenduru/magictime&#34;&gt;Replicate Demo&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.13]&lt;/code&gt; 🔥 We have compressed the size of repo with less than 1.0 MB, so that everyone can clone easier and faster. You can click &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/archive/refs/heads/main.zip&#34;&gt;here&lt;/a&gt; to download, or use &lt;code&gt;git clone --depth=1&lt;/code&gt; command to obtain this repo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.12]&lt;/code&gt; Thanks &lt;a href=&#34;https://github.com/kijai&#34;&gt;@Jukka Seppänen&lt;/a&gt; and &lt;a href=&#34;https://www.bilibili.com/video/BV1wx421U7Gn/?spm_id_from=333.1007.top_right_bar_window_history.content.click&#34;&gt;@Baobao Wang&lt;/a&gt; for providing ComfyUI Extension &lt;a href=&#34;https://github.com/kijai/ComfyUI-MagicTimeWrapper&#34;&gt;ComfyUI-MagicTimeWrapper&lt;/a&gt;. If you find related work, please let us know.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.11]&lt;/code&gt; 🔥 We release the Hugging Face Space of MagicTime, you can &lt;a href=&#34;https://huggingface.co/spaces/BestWishYsh/MagicTime?logs=build&#34;&gt;click&lt;/a&gt; here to have a try.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.10]&lt;/code&gt; 🔥 We release the inference code and model weight of MagicTime.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.09]&lt;/code&gt; 🔥 We release the arXiv paper for MagicTime, and you can click &lt;a href=&#34;https://arxiv.org/abs/2404.05014&#34;&gt;here&lt;/a&gt; to see more details.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.08]&lt;/code&gt; 🔥 We released the subset of ChronoMagic dataset used to train MagicTime. The dataset includes 2,265 metamorphic video-text pairs and can be downloaded at &lt;a href=&#34;https://drive.google.com/drive/folders/1WsomdkmSp3ql3ImcNsmzFuSQ9Qukuyr8?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;[2024.04.08]&lt;/code&gt; 🔥 &lt;strong&gt;All codes &amp;amp; datasets&lt;/strong&gt; are coming soon! Stay tuned 👀!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;😮 Highlights&lt;/h2&gt; &#xA;&lt;p&gt;MagicTime shows excellent performance in &lt;strong&gt;metamorphic video generation&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Metamorphic Videos vs. General Videos&lt;/h3&gt; &#xA;&lt;p&gt;Compared to general videos, metamorphic videos contain physical knowledge, long persistence, and strong variation, making them difficult to generate. We show compressed .gif on github, which loses some quality. The general videos are generated by the &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;Animatediff&lt;/a&gt; and &lt;strong&gt;MagicTime&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     Type&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Bean sprouts grow and mature from seeds&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] construction in a Minecraft virtual environment&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Cupcakes baking in an oven [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] transitioning from a tightly closed bud to a fully bloomed state [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;General Videos&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_0_0.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_0_1.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_0_2.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_0_3.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Metamorphic Videos&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_1_0.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_1_1.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_1_2.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/C_1_3.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Gallery&lt;/h3&gt; &#xA;&lt;p&gt;We showcase some metamorphic videos generated by &lt;strong&gt;MagicTime&lt;/strong&gt;, &lt;a href=&#34;https://github.com/xuduo35/MakeLongVideo&#34;&gt;MakeLongVideo&lt;/a&gt;, &lt;a href=&#34;https://github.com/modelscope&#34;&gt;ModelScopeT2V&lt;/a&gt;, &lt;a href=&#34;https://github.com/AILab-CVC/VideoCrafter?tab=readme-ov-file&#34;&gt;VideoCrafter&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/cerspense/zeroscope_v2_576w&#34;&gt;ZeroScope&lt;/a&gt;, &lt;a href=&#34;https://github.com/Vchitect/LaVie&#34;&gt;LaVie&lt;/a&gt;, &lt;a href=&#34;https://github.com/Picsart-AI-Research/Text2Video-Zero&#34;&gt;T2V-Zero&lt;/a&gt;, &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt; and &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;Animatediff&lt;/a&gt; below.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     Method&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;cherry blossoms transitioning [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;dough balls baking process [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;an ice cube is melting [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;a simple modern house&#39;s construction [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MakeLongVideo&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_0_0.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_0_1.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_0_2.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_0_3.gif?raw=true&#34; alt=&#34;MakeLongVideo&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ModelScopeT2V&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_1_0.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_1_1.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_1_2.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_1_3.gif?raw=true&#34; alt=&#34;ModelScopeT2V&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;VideoCrafter&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_2_0.gif?raw=true&#34; alt=&#34;VideoCrafter&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_2_1.gif?raw=true&#34; alt=&#34;VideoCrafter&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_2_2.gif?raw=true&#34; alt=&#34;VideoCrafter&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_2_3.gif?raw=true&#34; alt=&#34;VideoCrafter&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ZeroScope&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_3_0.gif?raw=true&#34; alt=&#34;ZeroScope&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_3_1.gif?raw=true&#34; alt=&#34;ZeroScope&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_3_2.gif?raw=true&#34; alt=&#34;ZeroScope&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_3_3.gif?raw=true&#34; alt=&#34;ZeroScope&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LaVie&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_4_0.gif?raw=true&#34; alt=&#34;LaVie&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_4_1.gif?raw=true&#34; alt=&#34;LaVie&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_4_2.gif?raw=true&#34; alt=&#34;LaVie&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_4_3.gif?raw=true&#34; alt=&#34;LaVie&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T2V-Zero&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_5_0.gif?raw=true&#34; alt=&#34;T2V-Zero&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_5_1.gif?raw=true&#34; alt=&#34;T2V-Zero&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_5_2.gif?raw=true&#34; alt=&#34;T2V-Zero&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_5_3.gif?raw=true&#34; alt=&#34;T2V-Zero&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Latte&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_6_0.gif?raw=true&#34; alt=&#34;Latte&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_6_1.gif?raw=true&#34; alt=&#34;Latte&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_6_2.gif?raw=true&#34; alt=&#34;Latte&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_6_3.gif?raw=true&#34; alt=&#34;Latte&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Animatediff&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_7_0.gif?raw=true&#34; alt=&#34;Animatediff&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_7_1.gif?raw=true&#34; alt=&#34;Animatediff&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_7_2.gif?raw=true&#34; alt=&#34;Animatediff&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_7_3.gif?raw=true&#34; alt=&#34;Animatediff&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ours&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_8_0.gif?raw=true&#34; alt=&#34;Ours&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_8_1.gif?raw=true&#34; alt=&#34;Ours&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_8_2.gif?raw=true&#34; alt=&#34;Ours&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/A_8_3.gif?raw=true&#34; alt=&#34;Ours&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;We show more metamorphic videos generated by &lt;strong&gt;MagicTime&lt;/strong&gt; with the help of &lt;a href=&#34;https://civitai.com/models/4201/realistic-vision-v20&#34;&gt;Realistic&lt;/a&gt;, &lt;a href=&#34;https://civitai.com/models/30240/toonyou&#34;&gt;ToonYou&lt;/a&gt; and &lt;a href=&#34;https://civitai.com/models/66347/rcnz-cartoon-3d&#34;&gt;RcnzCartoon&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_0_0.gif?raw=true&#34; alt=&#34;Realistic&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_0_1.gif?raw=true&#34; alt=&#34;Realistic&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_0_2.gif?raw=true&#34; alt=&#34;Realistic&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] bean sprouts grow and mature from seeds&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;dough [...] swells and browns in the oven [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;the construction [...] in Minecraft [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_1_0.gif?raw=true&#34; alt=&#34;RcnzCartoon&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_1_1.gif?raw=true&#34; alt=&#34;RcnzCartoon&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_1_2.gif?raw=true&#34; alt=&#34;RcnzCartoon&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;a bud transforms into a yellow flower&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;time-lapse of a plant germinating [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] a modern house being constructed in Minecraft [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_2_0.gif?raw=true&#34; alt=&#34;ToonYou&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_2_1.gif?raw=true&#34; alt=&#34;ToonYou&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/B_2_2.gif?raw=true&#34; alt=&#34;ToonYou&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;an ice cube is melting&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;bean plant sprouts grow and mature from the soil&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;time-lapse of delicate pink plum blossoms [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Prompts are trimmed for display, see &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/main/__assets__/promtp_unet.txt&#34;&gt;here&lt;/a&gt; for full prompts.&lt;/p&gt; &#xA;&lt;h3&gt;Integrate into DiT-based Architecture&lt;/h3&gt; &#xA;&lt;p&gt;The mission of this project is to help reproduce Sora and provide high-quality video-text data and data annotation pipelines, to support &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;Open-Sora-Plan&lt;/a&gt; or other DiT-based T2V models. To this end, we take an initial step to integrate our MagicTime scheme into the DiT-based Framework. Specifically, our method supports the Open-Sora-Plan v1.0.0 for fine-tuning. We first scale up with additional metamorphic landscape time-lapse videos in the same annotation framework to get the ChronoMagic-Landscape dataset. Then, we fine-tune the Open-Sora-Plan v1.0.0 with the ChronoMagic-Landscape dataset to get the MagicTime-DiT model. The results are as follows (&lt;strong&gt;257×512×512 (10s)&lt;/strong&gt;):&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_0.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_1.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_2.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_3.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Time-lapse of a coastal landscape [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Display the serene beauty of twilight [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Sunrise Splendor: Capture the breathtaking moment [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Nightfall Elegance: Embrace the tranquil beauty [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt;&#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_4.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_5.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_6.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/ProjectPage/static/videos/D_0_7.gif?raw=true&#34; alt=&#34;OpenSora&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;The sun descending below the horizon [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;[...] daylight fades into the embrace of the night [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Time-lapse of the dynamic formations of clouds [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;   &lt;td colspan=&#34;1&#34;&gt;&#xA;    &lt;center&gt;&#xA;     &#34;Capture the dynamic formations of clouds [...]&#34;&#xA;    &lt;/center&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;Prompts are trimmed for display, see &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/main/__assets__/promtp_opensora.txt&#34;&gt;here&lt;/a&gt; for full prompts.&lt;/p&gt; &#xA;&lt;h2&gt;🤗 Demo&lt;/h2&gt; &#xA;&lt;h3&gt;Gradio Web UI&lt;/h3&gt; &#xA;&lt;p&gt;Highly recommend trying out our web demo by the following command, which incorporates all features currently supported by MagicTime. We also provide &lt;a href=&#34;https://huggingface.co/spaces/BestWishYsh/MagicTime?logs=build&#34;&gt;online demo&lt;/a&gt; in Hugging Face Spaces.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;CLI Inference&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Realistic&#xA;python inference_magictime.py --config sample_configs/RealisticVision.yaml --human&#xA;&#xA;# or you can directly run the .sh&#xA;sh inference_cli.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;warning: It is worth noting that even if we use the same seed and prompt but we change a machine, the results will be different.&lt;/p&gt; &#xA;&lt;h2&gt;⚙️ Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;We recommend the requirements as follows.&lt;/p&gt; &#xA;&lt;h3&gt;Environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone --depth=1 https://github.com/PKU-YuanGroup/MagicTime.git&#xA;cd MagicTime&#xA;conda create -n magictime python=3.10.13&#xA;conda activate magictime&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download Base Model and Dreambooth&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh prepare_weights/down_base_model.sh&#xA;sh prepare_weights/down_dreambooth.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Prepare MagicTime Module&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sh prepare_weights/down_magictime_module.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🗝️ Training &amp;amp; Inference&lt;/h2&gt; &#xA;&lt;p&gt;The training code is coming soon! For inference, some example are shown below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Realistic&#xA;python inference_magictime.py --config sample_configs/RealisticVision.yaml&#xA;# For ToonYou&#xA;python inference_magictime.py --config sample_configs/ToonYou.yaml&#xA;# For RcnzCartoon&#xA;python inference_magictime.py --config sample_configs/RcnzCartoon.yaml&#xA;# or you can directly run the .sh&#xA;sh inference.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Community Contributions&lt;/h2&gt; &#xA;&lt;p&gt;We found some plugins created by community developers. Thanks for their efforts:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ComfyUI Extension &lt;a href=&#34;https://github.com/kijai/ComfyUI-MagicTimeWrapper&#34;&gt;ComfyUI-MagicTimeWrapper&lt;/a&gt; (by &lt;a href=&#34;https://github.com/kijai&#34;&gt;@Jukka Seppänen&lt;/a&gt;). And you can click &lt;a href=&#34;https://www.bilibili.com/video/BV1wx421U7Gn/?spm_id_from=333.1007.top_right_bar_window_history.content.click&#34;&gt;here&lt;/a&gt; to view the installation tutorial.&lt;/li&gt; &#xA; &lt;li&gt;Replicate Demo &amp;amp; Cloud API &lt;a href=&#34;https://replicate.com/camenduru/magictime&#34;&gt;Replicate-MagicTime&lt;/a&gt; (by &lt;a href=&#34;https://twitter.com/camenduru&#34;&gt;@camenduru&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Jupyter Notebook &lt;a href=&#34;https://github.com/camenduru/MagicTime-jupyter&#34;&gt;Jupyter-MagicTime&lt;/a&gt; (by &lt;a href=&#34;https://modelslab.com/&#34;&gt;@ModelsLab&lt;/a&gt;).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you find related work, please let us know.&lt;/p&gt; &#xA;&lt;h2&gt;🐳 ChronoMagic Dataset&lt;/h2&gt; &#xA;&lt;p&gt;ChronoMagic with 2265 metamorphic time-lapse videos, each accompanied by a detailed caption. We released the subset of ChronoMagic used to train MagicTime. The dataset can be downloaded at &lt;a href=&#34;https://drive.google.com/drive/folders/1WsomdkmSp3ql3ImcNsmzFuSQ9Qukuyr8?usp=sharing&#34;&gt;Google Drive&lt;/a&gt;. Some samples can be found on our Project Page.&lt;/p&gt; &#xA;&lt;h2&gt;👍 Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/guoyww/AnimateDiff/tree/main&#34;&gt;Animatediff&lt;/a&gt; The codebase we built upon and it is a strong U-Net-based text-to-video generation model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan&#34;&gt;Open-Sora-Plan&lt;/a&gt; The codebase we built upon and it is a simple and scalable DiT-based text-to-video generation repo, to reproduce &lt;a href=&#34;https://openai.com/sora&#34;&gt;Sora&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🔒 License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The majority of this project is released under the Apache 2.0 license as found in the &lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/raw/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;The service is a research preview. Please contact us if you find any potential violations.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;✏️ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find our paper and code useful in your research, please consider giving a star &lt;span&gt;⭐&lt;/span&gt; and citation &lt;span&gt;📝&lt;/span&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-BibTeX&#34;&gt;@article{yuan2024magictime,&#xA;  title={MagicTime: Time-lapse Video Generation Models as Metamorphic Simulators},&#xA;  author={Yuan, Shenghai and Huang, Jinfa and Shi, Yujun and Xu, Yongqi and Zhu, Ruijie and Lin, Bin and Cheng, Xinhua and Yuan, Li and Luo, Jiebo},&#xA;  journal={arXiv preprint arXiv:2404.05014},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🤝 Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/PKU-YuanGroup/MagicTime/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=PKU-YuanGroup/MagicTime&amp;amp;anon=true&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>huggingface/parler-tts</title>
    <updated>2024-04-15T02:45:34Z</updated>
    <id>tag:github.com,2024-04-15:/huggingface/parler-tts</id>
    <link href="https://github.com/huggingface/parler-tts" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Inference and training library for high-quality TTS models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Parler-TTS&lt;/h1&gt; &#xA;&lt;p&gt;Parler-TTS is a lightweight text-to-speech (TTS) model that can generate high-quality, natural sounding speech in the style of a given speaker (gender, pitch, speaking style, etc). It is a reproduction of work from the paper &lt;a href=&#34;https://www.text-description-to-speech.com&#34;&gt;Natural language guidance of high-fidelity text-to-speech with synthetic annotations&lt;/a&gt; by Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively.&lt;/p&gt; &#xA;&lt;p&gt;Contrarily to other TTS models, Parler-TTS is a &lt;strong&gt;fully open-source&lt;/strong&gt; release. All of the datasets, pre-processing, training code and weights are released publicly under permissive license, enabling the community to build on our work and develop their own powerful TTS models.&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the inference and training code for Parler-TTS. It is designed to accompany the &lt;a href=&#34;https://github.com/huggingface/dataspeech&#34;&gt;Data-Speech&lt;/a&gt; repository for dataset annotation.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] We&#39;re proud to release &lt;a href=&#34;https://huggingface.co/parler-tts/parler_tts_mini_v0.1&#34;&gt;Parler-TTS Mini v0.1&lt;/a&gt;, our first 600M parameter model, trained on 10.5K hours of audio data. In the coming weeks, we&#39;ll be working on scaling up to 50k hours of data, in preparation for the v1 model.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;📖 Quick Index&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/spaces/parler-tts/parler_tts_mini&#34;&gt;Demo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/parler-tts&#34;&gt;Model weights and datasets&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Parler-TTS has light-weight dependencies and can be installed in one line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip install git+https://github.com/huggingface/parler-tts.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] You can directly try it out in an interactive demo &lt;a href=&#34;https://huggingface.co/spaces/parler-tts/parler_tts_mini&#34;&gt;here&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Using Parler-TTS is as simple as &#34;bonjour&#34;. Simply use the following inference snippet.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;from parler_tts import ParlerTTSForConditionalGeneration&#xA;from transformers import AutoTokenizer&#xA;import soundfile as sf&#xA;import torch&#xA;&#xA;device = &#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;&#xA;&#xA;model = ParlerTTSForConditionalGeneration.from_pretrained(&#34;parler-tts/parler_tts_mini_v0.1&#34;).to(device)&#xA;tokenizer = AutoTokenizer.from_pretrained(&#34;parler-tts/parler_tts_mini_v0.1&#34;)&#xA;&#xA;prompt = &#34;Hey, how are you doing today?&#34;&#xA;description = &#34;A female speaker with a slightly low-pitched voice delivers her words quite expressively, in a very confined sounding environment with clear audio quality. She speaks very fast.&#34;&#xA;&#xA;input_ids = tokenizer(description, return_tensors=&#34;pt&#34;).input_ids.to(device)&#xA;prompt_input_ids = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids.to(device)&#xA;&#xA;generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)&#xA;audio_arr = generation.cpu().numpy().squeeze()&#xA;sf.write(&#34;parler_tts_out.wav&#34;, audio_arr, model.config.sampling_rate)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/parler-tts/assets/52246514/251e2488-fe6e-42c1-81cd-814c5b7795b0&#34;&gt;https://github.com/huggingface/parler-tts/assets/52246514/251e2488-fe6e-42c1-81cd-814c5b7795b0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/&#34;&gt;training folder&lt;/a&gt; contains all the information to train or fine-tune your own Parler-TTS model. It consists of:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#1-architecture&#34;&gt;1. An introduction to the Parler-TTS architecture&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#2-getting-started&#34;&gt;2. The first steps to get started&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#3-training&#34;&gt;3. A training guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] &lt;strong&gt;TL;DR:&lt;/strong&gt; After having followed the &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/parler-tts/main/training/README.md#requirements&#34;&gt;installation steps&lt;/a&gt;, you can reproduce the Parler-TTS Mini v0.1 training recipe with the following command line:&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;accelerate launch ./training/run_parler_tts_training.py ./helpers/training_configs/starting_point_0.01.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This library builds on top of a number of open-source giants, to whom we&#39;d like to extend our warmest thanks for providing these tools!&lt;/p&gt; &#xA;&lt;p&gt;Special thanks to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dan Lyth and Simon King, from Stability AI and Edinburgh University respectively, for publishing such a promising and clear research paper: &lt;a href=&#34;https://arxiv.org/abs/2402.01912&#34;&gt;Natural language guidance of high-fidelity text-to-speech with synthetic annotations&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;the many libraries used, namely &lt;a href=&#34;https://huggingface.co/docs/datasets/v2.17.0/en/index&#34;&gt;🤗 datasets&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/docs/accelerate/en/index&#34;&gt;🤗 accelerate&lt;/a&gt;, &lt;a href=&#34;https://github.com/jitsi/jiwer&#34;&gt;jiwer&lt;/a&gt;, &lt;a href=&#34;https://wandb.ai/&#34;&gt;wandb&lt;/a&gt;, and &lt;a href=&#34;https://huggingface.co/docs/transformers/index&#34;&gt;🤗 transformers&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Descript for the &lt;a href=&#34;https://github.com/descriptinc/descript-audio-codec&#34;&gt;DAC codec model&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face 🤗 for providing compute resources and time to explore!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this repository useful, please consider citing this work and also the original Stability AI paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lacombe-etal-2024-parler-tts,&#xA;  author = {Yoach Lacombe and Vaibhav Srivastav and Sanchit Gandhi},&#xA;  title = {Parler-TTS},&#xA;  year = {2024},&#xA;  publisher = {GitHub},&#xA;  journal = {GitHub repository},&#xA;  howpublished = {\url{https://github.com/huggingface/parler-tts}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{lyth2024natural,&#xA;      title={Natural language guidance of high-fidelity text-to-speech with synthetic annotations},&#xA;      author={Dan Lyth and Simon King},&#xA;      year={2024},&#xA;      eprint={2402.01912},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.SD}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome, as the project offers many possibilities for improvement and exploration.&lt;/p&gt; &#xA;&lt;p&gt;Namely, we&#39;re looking at ways to improve both quality and speed:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Datasets: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Train on more data&lt;/li&gt; &#xA;   &lt;li&gt;Add more features such as accents&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Training: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add PEFT compatibility to do Lora fine-tuning.&lt;/li&gt; &#xA;   &lt;li&gt;Add possibility to train without description column.&lt;/li&gt; &#xA;   &lt;li&gt;Add notebook training.&lt;/li&gt; &#xA;   &lt;li&gt;Explore multilingual training.&lt;/li&gt; &#xA;   &lt;li&gt;Explore mono-speaker finetuning.&lt;/li&gt; &#xA;   &lt;li&gt;Explore more architectures.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Optimization: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Compilation and static cache&lt;/li&gt; &#xA;   &lt;li&gt;Support to FA2 and SDPA&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Evaluation: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Add more evaluation metrics&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>