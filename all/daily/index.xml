<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-01-17T01:27:26Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>KAIST-VICLab/FMA-Net</title>
    <updated>2024-01-17T01:27:26Z</updated>
    <id>tag:github.com,2024-01-17:/KAIST-VICLab/FMA-Net</id>
    <link href="https://github.com/KAIST-VICLab/FMA-Net" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;FMA-Net: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring&lt;/h2&gt; &#xA; &lt;div&gt; &#xA;  &lt;a href=&#34;https://www.viclab.kaist.ac.kr/&#34; target=&#34;_blank&#34;&gt;Geunhyuk Youk&lt;/a&gt;&#xA;  &lt;sup&gt;1&lt;/sup&gt;&amp;nbsp; &#xA;  &lt;a href=&#34;https://sites.google.com/view/ozbro/&#34; target=&#34;_blank&#34;&gt;Jihyong Oh&lt;/a&gt;&#xA;  &lt;sup&gt;‚Ä† 2&lt;/sup&gt;&amp;nbsp; &#xA;  &lt;a href=&#34;https://www.viclab.kaist.ac.kr/&#34; target=&#34;_blank&#34;&gt;Munchurl Kim&lt;/a&gt;&#xA;  &lt;sup&gt;‚Ä† 1&lt;/sup&gt; &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;sup&gt;‚Ä†&lt;/sup&gt;Co-corresponding authors &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;sup&gt;1&lt;/sup&gt;Korea Advanced Institute of Science and Technology, South Korea &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;sup&gt;2&lt;/sup&gt;Chung-Ang University, South Korea &#xA; &lt;/div&gt; &#xA; &lt;div&gt; &#xA;  &lt;h4 align=&#34;center&#34;&gt; &lt;a href=&#34;https://kaist-viclab.github.io/fmanet-site/&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/üê≥-Project%20Page-blue&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2401.03707&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/arXiv-2401.03707-b31b1b.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=kO7KavOH6vw&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/Demo%20Video-%23FF0000.svg?logo=YouTube&amp;amp;logoColor=white&#34;&gt; &lt;/a&gt; &lt;img alt=&#34;GitHub Repo stars&#34; src=&#34;https://img.shields.io/github/stars/KAIST-VICLab/FMA-Net&#34;&gt; &lt;/h4&gt; &#xA; &lt;/div&gt; &#xA; &lt;hr&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;h4&gt; This repository is the official PyTorch implementation of &#34;FMA-Net: Flow-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring&#34;. FMA-Net achieves state-of-the-art performance in joint video super-resolution and deblurring (VSRDB). &lt;/h4&gt; &#xA; &lt;/div&gt; &#xA; &lt;div style=&#34;width: 100%; text-align: center; margin:auto;&#34;&gt; &#xA;  &lt;img style=&#34;width:100%&#34; src=&#34;https://raw.githubusercontent.com/KAIST-VICLab/FMA-Net/main/assets/result.png&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;h4&gt; Please visit our &lt;a href=&#34;https://kaist-viclab.github.io/fmanet-site/&#34; target=&#34;_blank&#34;&gt;project page&lt;/a&gt; for more visual results. &lt;/h4&gt; &#xA; &lt;/div&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üé¨ Network Architecture&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/KAIST-VICLab/FMA-Net/main/assets/network.png&#34; alt=&#34;overall_structure&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt; Codes and pretrained models will be updated soon. &lt;/h4&gt;</summary>
  </entry>
  <entry>
    <title>MooreThreads/Moore-AnimateAnyone</title>
    <updated>2024-01-17T01:27:26Z</updated>
    <id>tag:github.com,2024-01-17:/MooreThreads/Moore-AnimateAnyone</id>
    <link href="https://github.com/MooreThreads/Moore-AnimateAnyone" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ü§ó Introduction&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt;Ôºöüî•üî•üî•We launch a HuggingFace Spaces demo of Moore-AnimateAnyone at &lt;a href=&#34;https://huggingface.co/spaces/xunsong/Moore-AnimateAnyone&#34;&gt;here&lt;/a&gt;!!&lt;/p&gt; &#xA;&lt;p&gt;This repository reproduces &lt;a href=&#34;https://github.com/HumanAIGC/AnimateAnyone&#34;&gt;AnimateAnyone&lt;/a&gt;. To align the results demonstrated by the original paper, we adopt various approaches and tricks, which may differ somewhat from the paper and another &lt;a href=&#34;https://github.com/guoqincode/Open-AnimateAnyone&#34;&gt;implementation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s worth noting that this is a very preliminary version, aiming for approximating the performance (roughly 80% under our test) showed in &lt;a href=&#34;https://github.com/HumanAIGC/AnimateAnyone&#34;&gt;AnimateAnyone&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We will continue to develop it, and also welcome feedbacks and ideas from the community. The enhanced version will also be launched on our &lt;a href=&#34;https://maliang.mthreads.com/&#34;&gt;MoBi MaLiang&lt;/a&gt; AIGC platform, running on our own full-featured GPU S4000 cloud computing platform.&lt;/p&gt; &#xA;&lt;h1&gt;üìù Release Plans&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Inference codes and pretrained weights&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training scripts&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; The training code involves private data and packages. We will organize this portion of the code as soon as possible and then release it.&lt;/p&gt; &#xA;&lt;h1&gt;üéûÔ∏è Examples&lt;/h1&gt; &#xA;&lt;p&gt;Here are some results we generated, with the resolution of 512x768.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/f0454f30-6726-4ad4-80a7-5b7a15619057&#34;&gt;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/f0454f30-6726-4ad4-80a7-5b7a15619057&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/337ff231-68a3-4760-a9f9-5113654acf48&#34;&gt;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/337ff231-68a3-4760-a9f9-5113654acf48&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/9c4d852e-0a99-4607-8d63-569a1f67a8d2&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/722c6535-2901-4e23-9de9-501b22306ebd&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/17b907cc-c97e-43cd-af18-b646393c8e8a&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td width=&#34;50%&#34; style=&#34;border: none&#34;&gt; &#xA;    &lt;video controls autoplay loop src=&#34;https://github.com/MooreThreads/Moore-AnimateAnyone/assets/138439222/86f2f6d2-df60-4333-b19b-4c5abcd5999d&#34; muted=&#34;false&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Limitation&lt;/strong&gt;: We observe following shortcomings in current version:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;The background may occur some artifacts, when the reference image has a clean background&lt;/li&gt; &#xA; &lt;li&gt;Suboptimal results may arise when there is a scale mismatch between the reference image and keypoints. We have yet to implement preprocessing techniques as mentioned in the &lt;a href=&#34;https://arxiv.org/pdf/2311.17117.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Some flickering and jittering may occur when the motion sequence is subtle or the scene is static.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;These issues will be addressed and improved in the near future. We appreciate your anticipation!&lt;/p&gt; &#xA;&lt;h1&gt;‚öíÔ∏è Installation&lt;/h1&gt; &#xA;&lt;h2&gt;Build Environtment&lt;/h2&gt; &#xA;&lt;p&gt;We Recommend a python version &lt;code&gt;&amp;gt;=3.10&lt;/code&gt; and cuda version &lt;code&gt;=11.7&lt;/code&gt;. Then build environment as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# [Optional] Create a virtual env&#xA;python -m venv .venv&#xA;source .venv/bin/activate&#xA;# Install with pip:&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Download weights&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatically downloading&lt;/strong&gt;: You can run the following command to download weights automatically:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/download_weights.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Weights will be placed under the &lt;code&gt;./pretrained_weights&lt;/code&gt; direcotry. The whole downloading process may take a long time.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Manually downloading&lt;/strong&gt;: You can also download weights manually, which has some steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Download our trained &lt;a href=&#34;https://huggingface.co/patrolli/AnimateAnyone/tree/main&#34;&gt;weights&lt;/a&gt;, which include four parts: &lt;code&gt;denoising_unet.pth&lt;/code&gt;, &lt;code&gt;reference_unet.pth&lt;/code&gt;, &lt;code&gt;pose_guider.pth&lt;/code&gt; and &lt;code&gt;motion_module.pth&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download pretrained weight of based models and other components:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;StableDiffusion V1.5&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lambdalabs/sd-image-variations-diffusers/tree/main/image_encoder&#34;&gt;image_encoder&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download dwpose weights (&lt;code&gt;dw-ll_ucoco_384.onnx&lt;/code&gt;, &lt;code&gt;yolox_l.onnx&lt;/code&gt;) following &lt;a href=&#34;https://github.com/IDEA-Research/DWPose?tab=readme-ov-file#-dwpose-for-controlnet&#34;&gt;this&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, these weights should be orgnized as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;./pretrained_weights/&#xA;|-- DWPose&#xA;|   |-- dw-ll_ucoco_384.onnx&#xA;|   `-- yolox_l.onnx&#xA;|-- image_encoder&#xA;|   |-- config.json&#xA;|   `-- pytorch_model.bin&#xA;|-- denoising_unet.pth&#xA;|-- motion_module.pth&#xA;|-- pose_guider.pth&#xA;|-- reference_unet.pth&#xA;|-- sd-vae-ft-mse&#xA;|   |-- config.json&#xA;|   |-- diffusion_pytorch_model.bin&#xA;|   `-- diffusion_pytorch_model.safetensors&#xA;`-- stable-diffusion-v1-5&#xA;    |-- feature_extractor&#xA;    |   `-- preprocessor_config.json&#xA;    |-- model_index.json&#xA;    |-- unet&#xA;    |   |-- config.json&#xA;    |   `-- diffusion_pytorch_model.bin&#xA;    `-- v1-inference.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: If you have installed some of the pretrained models, such as &lt;code&gt;StableDiffusion V1.5&lt;/code&gt;, you can specify their paths in the config file (e.g. &lt;code&gt;./config/prompts/animation.yaml&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;h1&gt;üöÄ Inference&lt;/h1&gt; &#xA;&lt;p&gt;Here is the cli command for running inference scripts:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python -m scripts.pose2vid --config ./configs/prompts/animation.yaml -W 512 -H 784 -L 64&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can refer the format of &lt;code&gt;animation.yaml&lt;/code&gt; to add your own reference images or pose videos. To convert the raw video into a pose video (keypoint sequence), you can run with the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python tools/vid2pose.py --video_path /path/to/your/video.mp4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;üé® Gradio Demo&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;HuggingFace Demo&lt;/strong&gt;: We launch a quick preview demo of Moore-AnimateAnyone at &lt;a href=&#34;https://huggingface.co/spaces/xunsong/Moore-AnimateAnyone&#34;&gt;HuggingFace Spaces&lt;/a&gt;!! We appreciate the assistance provided by the HuggingFace team in setting up this demo.&lt;/p&gt; &#xA;&lt;p&gt;To reduce waiting time, we limit the size (width, height, and length) and inference steps when generating videos.&lt;/p&gt; &#xA;&lt;p&gt;If you have your own GPU resource (&amp;gt;= 16GB vram), you can run a local gradio app via following commands:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;python app.py&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Community Contributions&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Installation for Windows users: &lt;a href=&#34;https://github.com/sdbds/Moore-AnimateAnyone-for-windows&#34;&gt;Moore-AnimateAnyone-for-windows&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;üñåÔ∏è Try on Mobi MaLiang&lt;/h1&gt; &#xA;&lt;p&gt;We will launched this model on our &lt;a href=&#34;https://maliang.mthreads.com/&#34;&gt;MoBi MaLiang&lt;/a&gt; AIGC platform, running on our own full-featured GPU S4000 cloud computing platform. Mobi MaLiang has now integrated various AIGC applications and functionalities (e.g. text-to-image, controllable generation...). You can experience it by &lt;a href=&#34;https://maliang.mthreads.com/&#34;&gt;clicking this link&lt;/a&gt; or scanning the QR code bellow via WeChat!&lt;/p&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/MooreThreads/Moore-AnimateAnyone/master/assets/mini_program_maliang.png&#34; width=&#34;100&#xA;  &#34;&gt; &lt;/p&gt; &#xA;&lt;h1&gt;‚öñÔ∏è Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;This project is intended for academic research, and we explicitly disclaim any responsibility for user-generated content. Users are solely liable for their actions while using the generative model. The project contributors have no legal affiliation with, nor accountability for, users&#39; behaviors. It is imperative to use the generative model responsibly, adhering to both ethical and legal standards.&lt;/p&gt; &#xA;&lt;h1&gt;üôèüèª Acknowledgements&lt;/h1&gt; &#xA;&lt;p&gt;We first thank the authors of &lt;a href=&#34;&#34;&gt;AnimateAnyone&lt;/a&gt;. Additionally, we would like to thank the contributors to the &lt;a href=&#34;https://github.com/magic-research/magic-animate&#34;&gt;majic-animate&lt;/a&gt;, &lt;a href=&#34;https://github.com/guoyww/AnimateDiff&#34;&gt;animatediff&lt;/a&gt; and &lt;a href=&#34;https://github.com/guoqincode/Open-AnimateAnyone&#34;&gt;Open-AnimateAnyone&lt;/a&gt; repositories, for their open research and exploration. Furthermore, our repo incorporates some codes from &lt;a href=&#34;https://github.com/IDEA-Research/DWPose&#34;&gt;dwpose&lt;/a&gt; and &lt;a href=&#34;https://github.com/s9roll7/animatediff-cli-prompt-travel/&#34;&gt;animatediff-cli-prompt-travel&lt;/a&gt;, and we extend our thanks to them as well.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>VikParuchuri/surya</title>
    <updated>2024-01-17T01:27:26Z</updated>
    <id>tag:github.com,2024-01-17:/VikParuchuri/surya</id>
    <link href="https://github.com/VikParuchuri/surya" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Accurate line-level text detection and recognition (OCR) in any language&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Surya&lt;/h1&gt; &#xA;&lt;p&gt;Surya is a multilingual document OCR toolkit. It can do:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Accurate line-level text detection&lt;/li&gt; &#xA; &lt;li&gt;Text recognition (coming soon)&lt;/li&gt; &#xA; &lt;li&gt;Table and chart detection (coming soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;It works on a range of documents and languages (see &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/#usage&#34;&gt;usage&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/#benchmarks&#34;&gt;benchmarks&lt;/a&gt; for more details).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/excerpt.png&#34; alt=&#34;New York Times Article Example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Surya is named after the &lt;a href=&#34;https://en.wikipedia.org/wiki/Surya&#34;&gt;Hindu sun god&lt;/a&gt;, who has universal vision.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg//KuZwXNGnfH&#34;&gt;Discord&lt;/a&gt; is where we discuss future development.&lt;/p&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Text Detection&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;New York Times&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/nyt.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Japanese&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/japanese.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Chinese&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/chinese.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Hindi&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/hindi.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Presentation&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/pres.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scientific Paper&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/paper.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scanned Document&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/scanned.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Scanned Form&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/funsd.png&#34;&gt;Image&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;You&#39;ll need python 3.9+ and PyTorch. You may need to install the CPU version of torch first if you&#39;re not using a Mac or a GPU machine. See &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;here&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;Install with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install surya-ocr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Model weights will automatically download the first time you run surya.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspect the settings in &lt;code&gt;surya/settings.py&lt;/code&gt;. You can override any settings with environment variables.&lt;/li&gt; &#xA; &lt;li&gt;Your torch device will be automatically detected, but you can override this. For example, &lt;code&gt;TORCH_DEVICE=cuda&lt;/code&gt;. Note that the &lt;code&gt;mps&lt;/code&gt; device has a bug (on the &lt;a href=&#34;https://github.com/pytorch/pytorch/issues/84936&#34;&gt;Apple side&lt;/a&gt;) that may prevent it from working properly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Text line detection&lt;/h2&gt; &#xA;&lt;p&gt;You can detect text lines in an image, pdf, or folder of images/pdfs with the following command. This will write out a json file with the detected bboxes, and optionally save images of the pages with the bboxes.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;surya_detect DATA_PATH --images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;DATA_PATH&lt;/code&gt; can be an image, pdf, or folder of images/pdfs&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--images&lt;/code&gt; will save images of the pages and detected text lines (optional)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; specifies the maximum number of pages to process if you don&#39;t want to process everything&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; specifies the directory to save results to instead of the default&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;results.json&lt;/code&gt; file will contain these keys for each page of the input document(s):&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;polygons&lt;/code&gt; - polygons for each detected text line (these are more accurate than the bboxes) in (x1, y1), (x2, y2), (x3, y3), (x4, y4) format. The points are in clockwise order from the top left.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;bboxes&lt;/code&gt; - axis-aligned rectangles for each detected text line in (x1, y1, x2, y2) format. (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vertical_lines&lt;/code&gt; - vertical lines detected in the document in (x1, y1, x2, y2) format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;horizontal_lines&lt;/code&gt; - horizontal lines detected in the document in (x1, y1, x2, y2) format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;page_number&lt;/code&gt; - the page number of the document&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Performance tips&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Setting the &lt;code&gt;DETECTOR_BATCH_SIZE&lt;/code&gt; env var properly will make a big difference when using a GPU. Each batch item will use &lt;code&gt;280MB&lt;/code&gt; of VRAM, so very high batch sizes are possible. The default is a batch size &lt;code&gt;32&lt;/code&gt;, which will use about 9GB of VRAM.&lt;/p&gt; &#xA;&lt;p&gt;Depending on your CPU core count, &lt;code&gt;DETECTOR_BATCH_SIZE&lt;/code&gt; might make a difference there too - the default CPU batch size is &lt;code&gt;2&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can adjust &lt;code&gt;DETECTOR_NMS_THRESHOLD&lt;/code&gt; and &lt;code&gt;DETECTOR_TEXT_THRESHOLD&lt;/code&gt; if you don&#39;t get good results. Try lowering them to detect more text, and vice versa.&lt;/p&gt; &#xA;&lt;h3&gt;From Python&lt;/h3&gt; &#xA;&lt;p&gt;You can also do text detection from code with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;from PIL import Image&#xA;from surya.detection import batch_inference&#xA;from surya.model.segformer import load_model, load_processor&#xA;&#xA;image = Image.open(IMAGE_PATH)&#xA;model, processor = load_model(), load_processor()&#xA;&#xA;# predictions is a list of dicts, one per image&#xA;predictions = batch_inference([image], model, processor)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text recognition&lt;/h2&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;Table and chart detection&lt;/h2&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt; &#xA;&lt;h1&gt;Manual install&lt;/h1&gt; &#xA;&lt;p&gt;If you want to develop surya, you can install it manually:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;git clone https://github.com/VikParuchuri/surya.git&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;cd surya&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry install&lt;/code&gt; # Installs main and dev dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Limitations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;This is specialized for document OCR. It will likely not work on photos or other images.&lt;/li&gt; &#xA; &lt;li&gt;It is for printed text, not handwriting.&lt;/li&gt; &#xA; &lt;li&gt;The model has trained itself to ignore advertisements.&lt;/li&gt; &#xA; &lt;li&gt;This has worked for every language I&#39;ve tried, but languages with very different character sets may not work well.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Benchmarks&lt;/h1&gt; &#xA;&lt;h2&gt;Text line detection&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/VikParuchuri/surya/master/static/images/benchmark_chart_small.png&#34; alt=&#34;Benchmark chart&#34;&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Time (s)&lt;/th&gt; &#xA;   &lt;th&gt;Time per page (s)&lt;/th&gt; &#xA;   &lt;th&gt;precision&lt;/th&gt; &#xA;   &lt;th&gt;recall&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;surya&lt;/td&gt; &#xA;   &lt;td&gt;52.6892&lt;/td&gt; &#xA;   &lt;td&gt;0.205817&lt;/td&gt; &#xA;   &lt;td&gt;0.844426&lt;/td&gt; &#xA;   &lt;td&gt;0.937818&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;tesseract&lt;/td&gt; &#xA;   &lt;td&gt;74.4546&lt;/td&gt; &#xA;   &lt;td&gt;0.290838&lt;/td&gt; &#xA;   &lt;td&gt;0.631498&lt;/td&gt; &#xA;   &lt;td&gt;0.997694&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Tesseract is CPU-based, and surya is CPU or GPU. I ran the benchmarks on a system with an A6000 GPU, and a 32 core CPU. This was the resource usage:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;tesseract - 32 CPU cores, or 8 workers using 4 cores each&lt;/li&gt; &#xA; &lt;li&gt;surya - 32 batch size, for 9GB VRAM usage&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Surya predicts line-level bboxes, while tesseract and others predict word-level or character-level. It&#39;s also hard to find 100% correct datasets with line-level annotations. Merging bboxes can be noisy, so I chose not to use IoU as the metric for evaluation.&lt;/p&gt; &#xA;&lt;p&gt;I instead used coverage, which calculates:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Precision - how well predicted bboxes cover ground truth bboxes&lt;/li&gt; &#xA; &lt;li&gt;Recall - how well ground truth bboxes cover predicted bboxes&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;First calculate coverage for each bbox, then add a small penalty for double coverage, since we want the detection to have non-overlapping bboxes. Anything with a coverage of 0.5 or higher is considered a match.&lt;/p&gt; &#xA;&lt;p&gt;Then we calculate precision and recall for the whole dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Running your own benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;You can benchmark the performance of surya on your machine.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Follow the manual install instructions above.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;poetry install --group dev&lt;/code&gt; # Installs dev dependencies&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text line detection&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will evaluate tesseract and surya for text line detection across a randomly sampled set of images from &lt;a href=&#34;https://huggingface.co/datasets/vikp/doclaynet_bench&#34;&gt;doclaynet&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python benchmark/detection.py --max 256&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--max&lt;/code&gt; controls how many images to process for the benchmark&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--debug&lt;/code&gt; will render images and detected bboxes&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--pdf_path&lt;/code&gt; will let you specify a pdf to benchmark instead of the default data&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--results_dir&lt;/code&gt; will let you specify a directory to save results to instead of the default one&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;p&gt;This was trained on 4x A6000s for about 3 days. It used a diverse set of images as training data. It was trained from scratch using a modified segformer architecture that reduces inference RAM requirements.&lt;/p&gt; &#xA;&lt;h1&gt;Commercial usage&lt;/h1&gt; &#xA;&lt;p&gt;&lt;strong&gt;Text detection&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The text detection model was trained from scratch, so it&#39;s okay for commercial usage. The weights are licensed cc-by-nc-sa-4.0, but I will waive that for any organization under $5M USD in gross revenue in the most recent 12-month period.&lt;/p&gt; &#xA;&lt;p&gt;If you want to remove the GPL license requirements for inference or use the weights commercially over the revenue limit, please contact me at &lt;a href=&#34;mailto:surya@vikas.sh&#34;&gt;surya@vikas.sh&lt;/a&gt; for dual licensing.&lt;/p&gt; &#xA;&lt;h1&gt;Thanks&lt;/h1&gt; &#xA;&lt;p&gt;This work would not have been possible without amazing open source AI work:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2105.15203.pdf&#34;&gt;Segformer&lt;/a&gt; from NVIDIA&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; from huggingface&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/clovaai/CRAFT-pytorch&#34;&gt;CRAFT&lt;/a&gt;, a great scene text detection model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Thank you to everyone who makes open source AI possible.&lt;/p&gt;</summary>
  </entry>
</feed>