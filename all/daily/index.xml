<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-02T01:20:51Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>moom825/xeno-rat</title>
    <updated>2024-03-02T01:20:51Z</updated>
    <id>tag:github.com,2024-03-02:/moom825/xeno-rat</id>
    <link href="https://github.com/moom825/xeno-rat" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Xeno-RAT is an open-source remote access tool (RAT) developed in C#, providing a comprehensive set of features for remote system management. Has features such as HVNC, live microphone, reverse proxy, and much much more!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Xeno Rat&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/moom825/xeno-rat/main/logo.png&#34; width=&#34;200&#34; alt=&#34;Xeno-RAT Logo&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Xeno Rat is a remote access tool (RAT) that is used to control a computer remotely. It is written in C# and is compatible with Windows 10, 11. It is meant to stable, completely open source, easy to use and has a lot of features.&lt;/p&gt;  &#xA;&lt;h2&gt;What Sets Xeno Rat Apart&lt;/h2&gt; &#xA;&lt;p&gt;Xeno Rat stands out from the crowd for several reasons:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;HVNC (Hidden Virtual Network Computing)&lt;/strong&gt;: Xeno Rat offers HVNC, which is typically a paid feature in other RATs, but here, it&#39;s freely available to enhance your remote access experience.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Live Microphone&lt;/strong&gt;: Enjoy real-time audio surveillance with Xeno Rat, which provides a live microphone feature.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Socks5 Reverse Proxy&lt;/strong&gt;: Xeno Rat includes a Socks5 reverse proxy, allowing you to bypass network restrictions and access remote systems with ease.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Regular Updates and Much More&lt;/strong&gt;: We are committed to keeping Xeno Rat up to date and continually improving its features and functionality to better meet your needs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Built Completely from Scratch&lt;/strong&gt;: Xeno Rat is developed entirely from scratch, ensuring a unique and tailored approach to remote access tools.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Xeno Rat Builder&lt;/h2&gt; &#xA;&lt;p&gt;Inside the Xeno-Rat Server, head over to the &#34;Builder&#34; tab, select your custom settings and click &#34;Build&#34;. Then select a name and a location to save the file. The file will be saved as a .exe file and will be ready to use.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Fun&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Chat&lt;/li&gt; &#xA; &lt;li&gt;Bluescreen&lt;/li&gt; &#xA; &lt;li&gt;Message Box&lt;/li&gt; &#xA; &lt;li&gt;Fun menu (monitor on/off, cd tray open/close, etc)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Surveillance&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;HVNC (Hidden Virtual Network Computing)&lt;/li&gt; &#xA; &lt;li&gt;WebCam&lt;/li&gt; &#xA; &lt;li&gt;Live Microphone&lt;/li&gt; &#xA; &lt;li&gt;Key Logger&lt;/li&gt; &#xA; &lt;li&gt;Offline Key Logger&lt;/li&gt; &#xA; &lt;li&gt;Screen Control&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;System&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Reverse Proxy&lt;/li&gt; &#xA; &lt;li&gt;Process Manager&lt;/li&gt; &#xA; &lt;li&gt;File Manager&lt;/li&gt; &#xA; &lt;li&gt;Registry Manager&lt;/li&gt; &#xA; &lt;li&gt;Shell&lt;/li&gt; &#xA; &lt;li&gt;InfoGrab (cookies, Passwords, etc)&lt;/li&gt; &#xA; &lt;li&gt;Startup&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Uac Bypass&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Cmstp&lt;/li&gt; &#xA; &lt;li&gt;Windir + Disk Cleanup&lt;/li&gt; &#xA; &lt;li&gt;Fodhelper&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Uac Options&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Request admin&lt;/li&gt; &#xA; &lt;li&gt;De-escalate to user&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Client&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Close&lt;/li&gt; &#xA; &lt;li&gt;Relaunch&lt;/li&gt; &#xA; &lt;li&gt;Uninstall&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Power&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Shutdown&lt;/li&gt; &#xA; &lt;li&gt;Restart&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Misc Features&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Logs&lt;/li&gt; &#xA; &lt;li&gt;Listen on multiple ports&lt;/li&gt; &#xA; &lt;li&gt;password secured&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Issues, Bugs and Contributions&lt;/h2&gt; &#xA;&lt;p&gt;If you find any issues or bugs, please report them &lt;a href=&#34;https://github.com/moom825/xeno-rat/issues&#34;&gt;here&lt;/a&gt;. If you would like to contribute to the project, please fork the repository and submit a pull request. All contributions are welcome. If you don&#39;t know how to build the malware, or use it, then please don&#39;t open an issue as it will be closed as completed immediately. If you like the project, please leave a star!&lt;/p&gt; &#xA;&lt;h2&gt;Legal Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This tool is for educational purposes only. I am not responsible for any damage done by this tool. Please always stay within legal and ethical boundaries.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the &lt;a href=&#34;https://raw.githubusercontent.com/moom825/xeno-rat/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;How do I use this tool?&lt;/h3&gt; &#xA;&lt;p&gt;To get started with Xeno-RAT, follow these simple steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Go to the Releases Tab&lt;/strong&gt;: Navigate to the &#34;Releases&#34; tab of this GitHub repository to find the latest release. You can find it &lt;a href=&#34;https://github.com/moom825/xeno-rat/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Download the Latest Release&lt;/strong&gt;: Select the latest release from the list of releases and download the zip file containing the application files.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Unzip the Downloaded File&lt;/strong&gt;: After downloading, unzip the downloaded zip file to extract the contents. You can use your operating system&#39;s built-in tools or a third-party archive utility.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the Executable&lt;/strong&gt;: Inside the extracted folder, you will find the executable file. Double-click on this file to run Xeno-RAT.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Contact the Developer:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Telegram:&lt;/strong&gt; &lt;a href=&#34;https://t.me/moom825&#34;&gt;moom825&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discord:&lt;/strong&gt; moom825&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Donation&lt;/h2&gt; &#xA;&lt;h3&gt;Buy me a coffee!&lt;/h3&gt; &#xA;&lt;p&gt;BTC: bc1qg4zy8w5swc66k9xg29c2x6ennn5cyv2ytlp0a6&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>HumanAIGC/EMO</title>
    <updated>2024-03-02T01:20:51Z</updated>
    <id>tag:github.com,2024-03-02:/HumanAIGC/EMO</id>
    <link href="https://github.com/HumanAIGC/EMO" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;EMO&lt;/h1&gt; &#xA;&lt;p&gt;Emote Portrait Alive: Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions&lt;/p&gt; &#xA;&lt;p&gt;Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo,&lt;/p&gt; &#xA;&lt;p&gt;Institute for Intelligent Computing, Alibaba Group&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://humanaigc.github.io/emote-portrait-alive/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Project-Page-Green&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/2402.17485&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Paper-Arxiv-red&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/VlJ71kzcn9Y&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/youtube.svg?sanitize=true&#34; alt=&#34;YouTube&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/HumanAIGC/EMO/main/content/intro.png&#34; alt=&#34;pipeline&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{tian2024emo,&#xA;      title={EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions}, &#xA;      author={Linrui Tian and Qi Wang and Bang Zhang and Liefeng Bo},&#xA;      year={2024},&#xA;      eprint={2402.17485},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>evo-design/evo</title>
    <updated>2024-03-02T01:20:51Z</updated>
    <id>tag:github.com,2024-03-02:/evo-design/evo</id>
    <link href="https://github.com/evo-design/evo" rel="alternate"></link>
    <summary type="html">&lt;p&gt;DNA foundation modeling from molecular to genome scale&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Evo: DNA foundation modeling from molecular to genome scale&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/evo-design/evo/main/evo.jpg&#34; alt=&#34;Evo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Evo is a biological foundation model capable of long-context modeling and design. Evo uses the &lt;a href=&#34;https://github.com/togethercomputer/stripedhyena&#34;&gt;StripedHyena architecture&lt;/a&gt; to enable modeling of sequences at a single-nucleotide, byte-level resolution with near-linear scaling of compute and memory relative to context length. Evo has 7 billion parameters and is trained on OpenGenome, a prokaryotic whole-genome dataset containing ~300 billion tokens.&lt;/p&gt; &#xA;&lt;p&gt;We describe Evo in the paper &lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2024.02.27.582234v1&#34;&gt;“Sequence modeling and design from molecular to genome scale with Evo”&lt;/a&gt; and in the &lt;a href=&#34;https://arcinstitute.org/news/blog/evo&#34;&gt;accompanying blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide the following model checkpoints:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Checkpoint Name&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;evo-1-8k-base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model pretrained with 8,192 context. We use this model as the base model for molecular-scale finetuning tasks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;evo-1-131k-base&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A model pretrained with 131,072 context using &lt;code&gt;evo-1-8k-base&lt;/code&gt; as the base model. We use this model to reason about and generate sequences at the genome scale.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#setup&#34;&gt;Setup&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#requirements&#34;&gt;Requirements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#usage&#34;&gt;Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#huggingface&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#together-api&#34;&gt;Together API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;p&gt;Evo is based on &lt;a href=&#34;https://github.com/togethercomputer/stripedhyena/tree/main&#34;&gt;StripedHyena&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Evo uses &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34;&gt;FlashAttention-2&lt;/a&gt;, which may not work on all GPU architectures. Please consult the &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention#installation-and-features&#34;&gt;FlashAttention GitHub repository&lt;/a&gt; for the current list of supported GPUs.&lt;/p&gt; &#xA;&lt;p&gt;Make sure to install the correct &lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch version&lt;/a&gt; on your system.&lt;/p&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;You can install Evo using &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install evo-model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or directly from the GitHub source&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/evo-design/evo.git&#xA;cd evo/&#xA;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend that you install the PyTorch library first, before installing all other dependencies (due to dependency issues of the &lt;code&gt;flash-attn&lt;/code&gt; library; see, e.g., this &lt;a href=&#34;https://github.com/Dao-AILab/flash-attention/issues/246&#34;&gt;issue&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;One of our &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/&#34;&gt;example scripts&lt;/a&gt;, demonstrating how to go from generating sequences with Evo to folding proteins (&lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/generation_to_folding.py&#34;&gt;scripts/generation_to_folding.py&lt;/a&gt;), further requires the installation of &lt;code&gt;prodigal&lt;/code&gt;. We have created an &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/environment.yml&#34;&gt;environment.yml&lt;/a&gt; file for this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda env create -f environment.yml&#xA;conda activate evo-design&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Below is an example of how to download Evo and use it locally through the Python API.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from evo import Evo&#xA;import torch&#xA;&#xA;device = &#39;cuda:0&#39;&#xA;&#xA;evo_model = Evo(&#39;evo-1-131k-base&#39;)&#xA;model, tokenizer = evo_model.model, evo_model.tokenizer&#xA;model.to(device)&#xA;model.eval()&#xA;&#xA;sequence = &#39;ACGT&#39;&#xA;input_ids = torch.tensor(&#xA;    tokenizer.tokenize(sequence),&#xA;    dtype=torch.int,&#xA;).to(device).unsqueeze(0)&#xA;logits, _ = model(input_ids) # (batch, length, vocab)&#xA;&#xA;print(&#39;Logits: &#39;, logits)&#xA;print(&#39;Shape (batch, length, vocab): &#39;, logits.shape)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;An example of batched inference can be found in &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/example_inference.py&#34;&gt;&lt;code&gt;scripts/example_inference.py&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We provide an &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/generate.py&#34;&gt;example script&lt;/a&gt; for how to prompt the model and sample a set of sequences given the prompt.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m scripts.generate \&#xA;    --model-name &#39;evo-1-131k-base&#39; \&#xA;    --prompt ACGT \&#xA;    --n-samples 10 \&#xA;    --n-tokens 100 \&#xA;    --temperature 1. \&#xA;    --top-k 4 \&#xA;    --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also provide an &lt;a href=&#34;https://raw.githubusercontent.com/evo-design/evo/main/scripts/generate.py&#34;&gt;example script&lt;/a&gt; for using the model to score the log-likelihoods of a set of sequences.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m scripts.score \&#xA;    --input-fasta examples/example_seqs.fasta \&#xA;    --output-tsv scores.tsv \&#xA;    --model-name &#39;evo-1-131k-base&#39; \&#xA;    --device cuda:0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;HuggingFace&lt;/h2&gt; &#xA;&lt;p&gt;Evo is integrated with &lt;a href=&#34;https://huggingface.co/togethercomputer/evo-1-131k-base&#34;&gt;HuggingFace&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoConfig, AutoModelForCausalLM&#xA;&#xA;model_name = &#39;togethercomputer/evo-1-8k-base&#39;&#xA;&#xA;model_config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)&#xA;model_config.use_cache = True&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    model_name,&#xA;    config=model_config,&#xA;    trust_remote_code=True,&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Together API&lt;/h2&gt; &#xA;&lt;p&gt;Evo will also be soon available via an API by &lt;a href=&#34;https://www.together.ai/&#34;&gt;TogetherAI&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import openai&#xA;import os&#xA;&#xA;# Fill in your API information here.&#xA;client = openai.OpenAI(&#xA;  api_key=TOGETHER_API_KEY,&#xA;  base_url=&#39;https://api.together.xyz&#39;,&#xA;)&#xA;&#xA;chat_completion = client.chat.completions.create(&#xA;  messages=[&#xA;    {&#xA;      &#34;role&#34;: &#34;system&#34;,&#xA;      &#34;content&#34;: &#34;&#34;&#xA;    },&#xA;    {&#xA;      &#34;role&#34;: &#34;user&#34;,&#xA;      &#34;content&#34;: &#34;ACGT&#34;, # Prompt the model with a sequence.&#xA;    }&#xA;  ],&#xA;  model=&#34;togethercomputer/evo-1-131k-base&#34;,&#xA;  max_tokens=128, # Sample some number of new tokens.&#xA;  logprobs=True&#xA;)&#xA;print(&#xA;    chat_completion.choices[0].logprobs.token_logprobs,&#xA;    chat_completion.choices[0].message.content&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;Please cite the following preprint when referencing Evo.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article {nguyen2024sequence,&#xA;&#x9;author = {Eric Nguyen and Michael Poli and Matthew G Durrant and Armin W Thomas and Brian Kang and Jeremy Sullivan and Madelena Y Ng and Ashley Lewis and Aman Patel and Aaron Lou and Stefano Ermon and Stephen A Baccus and Tina Hernandez-Boussard and Christopher Ré and Patrick D Hsu and Brian L Hie},&#xA;&#x9;title = {Sequence modeling and design from molecular to genome scale with Evo},&#xA;&#x9;year = {2024},&#xA;&#x9;doi = {10.1101/2024.02.27.582234},&#xA;&#x9;publisher = {Cold Spring Harbor Laboratory},&#xA;&#x9;URL = {https://www.biorxiv.org/content/early/2024/02/27/2024.02.27.582234},&#xA;&#x9;journal = {bioRxiv}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>