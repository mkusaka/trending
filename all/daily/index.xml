<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-08-01T01:30:10Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>kijai/ComfyUI-WanVideoWrapper</title>
    <updated>2025-08-01T01:30:10Z</updated>
    <id>tag:github.com,2025-08-01:/kijai/ComfyUI-WanVideoWrapper</id>
    <link href="https://github.com/kijai/ComfyUI-WanVideoWrapper" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI wrapper nodes for &lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;WanVideo&lt;/a&gt; and related models.&lt;/h1&gt; &#xA;&lt;h1&gt;WORK IN PROGRESS (perpetually)&lt;/h1&gt; &#xA;&lt;h1&gt;Why should I use custom nodes when WanVideo works natively?&lt;/h1&gt; &#xA;&lt;p&gt;Short answer: Unless it&#39;s a model/feature not available yet on native, you shouldn&#39;t.&lt;/p&gt; &#xA;&lt;p&gt;Long answer: Due to the complexity of ComfyUI core code, and my lack of coding experience, in many cases it&#39;s far easier and faster to implement new models and features to a standalone wrapper, so this is a way to test things relatively quickly. I consider this my personal sandbox (which is obviously open for everyone) to play with without having to worry about compability issues etc, but as such this code is always work in progress and prone to have issues. Also not all new models end up being worth the trouble to implement in core Comfy, though I&#39;ve also made some patcher nodes to allow using them in native workflows, such as the &lt;a href=&#34;https://huggingface.co/bytedance-research/ATI&#34;&gt;ATI&lt;/a&gt; node available in this wrapper. This is also the end goal, idea isn&#39;t to compete or even offer alternatives to everything available in native workflows. All that said (this is clearly not a sales pitch) I do appreciate everyone using these nodes to explore new releases and possibilities with WanVideo.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone this repo into &lt;code&gt;custom_nodes&lt;/code&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;Install dependencies: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; or if you use the portable install, run this in ComfyUI_windows_portable -folder:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;code&gt;python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\ComfyUI-WanVideoWrapper\requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Models&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Kijai/WanVideo_comfy/tree/main&#34;&gt;https://huggingface.co/Kijai/WanVideo_comfy/tree/main&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;fp8 scaled models (personal recommendation):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled&#34;&gt;https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Text encoders to &lt;code&gt;ComfyUI/models/text_encoders&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Clip vision to &lt;code&gt;ComfyUI/models/clip_vision&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Transformer (main video model) to &lt;code&gt;ComfyUI/models/diffusion_models&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Vae to &lt;code&gt;ComfyUI/models/vae&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also use the native ComfyUI text encoding and clip vision loader with the wrapper instead of the original models:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/user-attachments/assets/6a2fd9a5-8163-4c93-b362-92ef34dbd3a4&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;GGUF models can now be loaded in the main model loader as well.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Supported extra models:&lt;/p&gt; &#xA;&lt;p&gt;SkyReels: &lt;a href=&#34;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&#34;&gt;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;WanVideoFun: &lt;a href=&#34;https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17&#34;&gt;https://huggingface.co/collections/alibaba-pai/wan21-fun-v11-680f514c89fe7b4df9d44f17&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ReCamMaster: &lt;a href=&#34;https://github.com/KwaiVGI/ReCamMaster&#34;&gt;https://github.com/KwaiVGI/ReCamMaster&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;VACE: &lt;a href=&#34;https://github.com/ali-vilab/VACE&#34;&gt;https://github.com/ali-vilab/VACE&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Phantom: &lt;a href=&#34;https://huggingface.co/bytedance-research/Phantom&#34;&gt;https://huggingface.co/bytedance-research/Phantom&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;ATI: &lt;a href=&#34;https://huggingface.co/bytedance-research/ATI&#34;&gt;https://huggingface.co/bytedance-research/ATI&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Uni3C: &lt;a href=&#34;https://github.com/alibaba-damo-academy/Uni3C&#34;&gt;https://github.com/alibaba-damo-academy/Uni3C&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MiniMaxRemover: &lt;a href=&#34;https://huggingface.co/zibojia/minimax-remover&#34;&gt;https://huggingface.co/zibojia/minimax-remover&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MAGREF: &lt;a href=&#34;https://huggingface.co/MAGREF-Video/MAGREF&#34;&gt;https://huggingface.co/MAGREF-Video/MAGREF&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;FantasyTalking: &lt;a href=&#34;https://github.com/Fantasy-AMAP/fantasy-talking&#34;&gt;https://github.com/Fantasy-AMAP/fantasy-talking&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;MultiTalk: &lt;a href=&#34;https://github.com/MeiGen-AI/MultiTalk&#34;&gt;https://github.com/MeiGen-AI/MultiTalk&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;EchoShot: &lt;a href=&#34;https://github.com/D2I-ai/EchoShot&#34;&gt;https://github.com/D2I-ai/EchoShot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Examples:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/KwaiVGI/ReCamMaster&#34;&gt;ReCamMaster&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/c58a12c2-13ba-4af8-8041-e283dbef197e&#34;&gt;https://github.com/user-attachments/assets/c58a12c2-13ba-4af8-8041-e283dbef197e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;TeaCache (with the old temporary WIP naive version, I2V):&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note that with the new version the threshold values should be 10x higher&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Range of 0.25-0.30 seems good when using the coefficients, start step can be 0, with more aggressive threshold values it may make sense to start later to avoid any potential step skips early on, that generally ruin the motion.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/504a9a50-3337-43d2-97b8-8e1661f29f46&#34;&gt;https://github.com/user-attachments/assets/504a9a50-3337-43d2-97b8-8e1661f29f46&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Context window test:&lt;/p&gt; &#xA;&lt;p&gt;1025 frames using window size of 81 frames, with 16 overlap. With the 1.3B T2V model this used under 5GB VRAM and took 10 minutes to gen on a 5090:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/89b393af-cf1b-49ae-aa29-23e57f65911e&#34;&gt;https://github.com/user-attachments/assets/89b393af-cf1b-49ae-aa29-23e57f65911e&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;This very first test was 512x512x81&lt;/p&gt; &#xA;&lt;p&gt;~16GB used with 20/40 blocks offloaded&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/fa6d0a4f-4a4d-4de5-84a4-877cc37b715f&#34;&gt;https://github.com/user-attachments/assets/fa6d0a4f-4a4d-4de5-84a4-877cc37b715f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Vid2vid example:&lt;/p&gt; &#xA;&lt;p&gt;with 14B T2V model:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/ef228b8a-a13a-4327-8a1b-1eb343cf00d8&#34;&gt;https://github.com/user-attachments/assets/ef228b8a-a13a-4327-8a1b-1eb343cf00d8&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;with 1.3B T2V model&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/4f35ba84-da7a-4d5b-97ee-9641296f391e&#34;&gt;https://github.com/user-attachments/assets/4f35ba84-da7a-4d5b-97ee-9641296f391e&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>linkwarden/linkwarden</title>
    <updated>2025-08-01T01:30:10Z</updated>
    <id>tag:github.com,2025-08-01:/linkwarden/linkwarden</id>
    <link href="https://github.com/linkwarden/linkwarden" rel="alternate"></link>
    <summary type="html">&lt;p&gt;⚡️⚡️⚡️ Self-hosted collaborative bookmark manager to collect, read, annotate, and fully preserve what matters, all in one place.&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/linkwarden/linkwarden/main/assets/logo.png&#34; width=&#34;100px&#34;&gt; &#xA; &lt;h1&gt;Linkwarden&lt;/h1&gt; &#xA; &lt;h3&gt;Bookmarks, Evolved&lt;/h3&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://discord.com/invite/CtuYV47nuJ&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1117993124669702164?logo=discord&amp;amp;style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://twitter.com/LinkwardenHQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/linkwarden&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://news.ycombinator.com/item?id=36942308&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Hacker%20News-280-%23FF6600&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/linkwarden/linkwarden/releases&#34;&gt;&lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/v/release/linkwarden/linkwarden&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://crowdin.com/project/linkwarden&#34;&gt; &lt;img src=&#34;https://badges.crowdin.net/linkwarden/localized.svg?sanitize=true&#34; alt=&#34;Crowdin&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opencollective.com/linkwarden&#34;&gt;&lt;img src=&#34;https://img.shields.io/opencollective/all/linkwarden&#34; alt=&#34;Open Collective&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://demo.linkwarden.app&#34;&gt;« LAUNCH DEMO »&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://cloud.linkwarden.app&#34;&gt;Cloud&lt;/a&gt; · &lt;a href=&#34;https://linkwarden.app&#34;&gt;Website&lt;/a&gt; · &lt;a href=&#34;https://github.com/linkwarden/linkwarden#features&#34;&gt;Features&lt;/a&gt; · &lt;a href=&#34;https://docs.linkwarden.app&#34;&gt;Docs&lt;/a&gt;&lt;/p&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/linkwarden/linkwarden/main/assets/home.png&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Intro &amp;amp; motivation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Linkwarden is a self-hosted, open-source collaborative bookmark manager to collect, read, annotate, and fully preserve what matters, all in one place.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The objective is to organize useful webpages and articles you find across the web in one place, and since useful webpages can go away (see the inevitability of &lt;a href=&#34;https://en.wikipedia.org/wiki/Link_rot&#34;&gt;Link Rot&lt;/a&gt;), Linkwarden also saves a copy of each webpage as a Screenshot and PDF, ensuring accessibility even if the original content is no longer available.&lt;/p&gt; &#xA;&lt;p&gt;In addition to preservation, Linkwarden provides a user-friendly reading and annotation experience that blends the simplicity of a “read-it-later” tool with the reliability of a web archive. Whether you’re highlighting key ideas, jotting down thoughts, or revisiting content long after it’s disappeared from the web, Linkwarden keeps your knowledge accessible and organized.&lt;/p&gt; &#xA;&lt;p&gt;Linkwarden is also designed with collaboration in mind, enabling you to share links with the public and/or collaborate seamlessly with multiple users.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP]&lt;br&gt; Our official &lt;a href=&#34;https://linkwarden.app/#pricing&#34;&gt;Cloud&lt;/a&gt; offering provides the simplest way to begin using Linkwarden and it&#39;s the preferred choice for many due to its time-saving benefits. &lt;br&gt; Your subscription supports our hosting infrastructure and ongoing development. &lt;br&gt; Alternatively, if you prefer self-hosting Linkwarden, you can do so by following our &lt;a href=&#34;https://docs.linkwarden.app/self-hosting/installation&#34;&gt;Installation documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;📸 Auto capture a screenshot, PDF, and single html file of each webpage.&lt;/li&gt; &#xA; &lt;li&gt;📖 Reader view of the webpage, with the ability to highlight and annotate text.&lt;/li&gt; &#xA; &lt;li&gt;🏛️ Send your webpage to Wayback Machine (&lt;a href=&#34;https://archive.org&#34;&gt;archive.org&lt;/a&gt;) for a snapshot. (Optional)&lt;/li&gt; &#xA; &lt;li&gt;✨ Local AI Tagging to automatically tag your links based on their content (Optional).&lt;/li&gt; &#xA; &lt;li&gt;📂 Organize links by collection, sub-collection, name, description and multiple tags.&lt;/li&gt; &#xA; &lt;li&gt;👥 Collaborate on gathering links in a collection.&lt;/li&gt; &#xA; &lt;li&gt;🎛️ Customize the permissions of each member.&lt;/li&gt; &#xA; &lt;li&gt;🌐 Share your collected links and preserved formats with the world.&lt;/li&gt; &#xA; &lt;li&gt;📌 Pin your favorite links to dashboard.&lt;/li&gt; &#xA; &lt;li&gt;🔍 Full text search, filter and sort for easy retrieval.&lt;/li&gt; &#xA; &lt;li&gt;📱 Responsive design and supports most modern browsers.&lt;/li&gt; &#xA; &lt;li&gt;🌓 Dark/Light mode support.&lt;/li&gt; &#xA; &lt;li&gt;🧩 Browser extension. &lt;a href=&#34;https://github.com/linkwarden/browser-extension&#34;&gt;Star it here!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;🔄 Browser Synchronization (using &lt;a href=&#34;https://floccus.org&#34;&gt;Floccus&lt;/a&gt;!)&lt;/li&gt; &#xA; &lt;li&gt;⬇️ Import and export your bookmarks.&lt;/li&gt; &#xA; &lt;li&gt;🔐 SSO integration. (Enterprise and Self-hosted users only)&lt;/li&gt; &#xA; &lt;li&gt;📦 Installable Progressive Web App (PWA).&lt;/li&gt; &#xA; &lt;li&gt;🍎 iOS Shortcut to save Links to Linkwarden.&lt;/li&gt; &#xA; &lt;li&gt;🔑 API keys.&lt;/li&gt; &#xA; &lt;li&gt;✅ Bulk actions.&lt;/li&gt; &#xA; &lt;li&gt;👥 User administration.&lt;/li&gt; &#xA; &lt;li&gt;🌐 Support for Other Languages (i18n).&lt;/li&gt; &#xA; &lt;li&gt;📁 Image and PDF Uploads.&lt;/li&gt; &#xA; &lt;li&gt;🎨 Custom Icons for Links and Collections.&lt;/li&gt; &#xA; &lt;li&gt;🔔 RSS Feed Subscription.&lt;/li&gt; &#xA; &lt;li&gt;✨ And many more features. (Literally!)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Like what we&#39;re doing? Give us a Star ⭐&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/linkwarden/linkwarden/main/assets/star_repo.gif&#34; alt=&#34;Star Us&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;We&#39;re building our Community 🌐&lt;/h2&gt; &#xA;&lt;p&gt;Join and follow us in the following platforms to stay up to date about the most recent features and for support:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.com/invite/CtuYV47nuJ&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1117993124669702164?logo=discord&amp;amp;style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/LinkwardenHQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/follow/linkwarden&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fosstodon.org/@linkwarden&#34;&gt;&lt;img src=&#34;https://img.shields.io/mastodon/follow/110748840237143200?domain=https%3A%2F%2Ffosstodon.org&#34; alt=&#34;Mastodon&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Suggestions&lt;/h2&gt; &#xA;&lt;p&gt;We &lt;em&gt;usually&lt;/em&gt; go after the &lt;a href=&#34;https://github.com/linkwarden/linkwarden/issues?q=is%3Aissue%20is%3Aopen%20sort%3Areactions-%2B1-desc&#34;&gt;popular suggestions&lt;/a&gt;. Feel free to open a &lt;a href=&#34;https://github.com/linkwarden/linkwarden/issues/new?assignees=&amp;amp;labels=enhancement&amp;amp;projects=&amp;amp;template=feature_request.md&amp;amp;title=&#34;&gt;new issue&lt;/a&gt; to suggest one - others might be interested too! :)&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;Make sure to check out our &lt;a href=&#34;https://github.com/orgs/linkwarden/projects/1&#34;&gt;public roadmap&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Community Projects&lt;/h2&gt; &#xA;&lt;p&gt;Here are some community-maintained projects that are built around Linkwarden:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://apps.apple.com/ca/app/my-links-for-linkwarden/id6504573402&#34;&gt;My Links&lt;/a&gt; - iOS and MacOS Apps, maintained by &lt;a href=&#34;https://github.com/JGeek00&#34;&gt;JGeek00&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fossdroid.com/a/linkdroid-for-linkwarden.html&#34;&gt;LinkDroid&lt;/a&gt; - Android App with share sheet integration, &lt;a href=&#34;https://github.com/Dacid99/LinkDroid-for-Linkwarden&#34;&gt;source code&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Elbullazul/LinkGuardian&#34;&gt;LinkGuardian&lt;/a&gt; - An Android client for Linkwarden. Built with Kotlin and Jetpack compose.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/rtuszik/starwarden&#34;&gt;StarWarden&lt;/a&gt; - A browser extension to save your starred GitHub repositories to Linkwarden.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Development&lt;/h2&gt; &#xA;&lt;p&gt;If you want to contribute, Thanks! Start by choosing one of our &lt;a href=&#34;https://github.com/linkwarden/linkwarden/issues?q=is%3Aissue%20is%3Aopen%20sort%3Areactions-%2B1-desc&#34;&gt;popular suggestions&lt;/a&gt;, just please stay in touch with &lt;a href=&#34;https://github.com/daniel31x13&#34;&gt;@daniel31x13&lt;/a&gt; before starting.&lt;/p&gt; &#xA;&lt;h1&gt;Translations&lt;/h1&gt; &#xA;&lt;p&gt;If you want to help us translate Linkwarden to your language, please check out our &lt;a href=&#34;https://crowdin.com/project/linkwarden&#34;&gt;Crowdin page&lt;/a&gt; and start translating. We would love to have your help!&lt;/p&gt; &#xA;&lt;p&gt;To start translating a new language, please create an issue so we can set it up for you. New languages will be added once they reach at least 50% translation completion.&lt;/p&gt; &#xA;&lt;a href=&#34;https://crowdin.com/project/linkwarden&#34;&gt; &lt;img src=&#34;https://badges.crowdin.net/linkwarden/localized.svg?sanitize=true&#34; alt=&#34;Crowdin&#34;&gt;&lt;/a&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;p&gt;If you found a security vulnerability, please do &lt;strong&gt;not&lt;/strong&gt; create a public issue, instead send an email to &lt;a href=&#34;mailto:security@linkwarden.app&#34;&gt;security@linkwarden.app&lt;/a&gt; stating the vulnerability. Thanks!&lt;/p&gt; &#xA;&lt;h2&gt;Support &amp;lt;3&lt;/h2&gt; &#xA;&lt;p&gt;Other than using our official &lt;a href=&#34;https://linkwarden.app/#pricing&#34;&gt;Cloud&lt;/a&gt; offering, any &lt;a href=&#34;https://opencollective.com/linkwarden&#34;&gt;donations&lt;/a&gt; are highly appreciated as well!&lt;/p&gt; &#xA;&lt;p&gt;Here are the other ways to support/cheer this project:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Starring this repository.&lt;/li&gt; &#xA; &lt;li&gt;Joining us on &lt;a href=&#34;https://discord.com/invite/CtuYV47nuJ&#34;&gt;Discord&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Referring Linkwarden to a friend.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you did any of the above, Thanksss! Otherwise thanks.&lt;/p&gt; &#xA;&lt;h2&gt;Thanks to All the Contributors 💪&lt;/h2&gt; &#xA;&lt;p&gt;Huge thanks to these guys for spending their time helping Linkwarden grow. They rock! ⚡️&lt;/p&gt; &#xA;&lt;img src=&#34;https://contributors-img.web.app/image?repo=linkwarden/linkwarden&#34; alt=&#34;Contributors&#34;&gt;</summary>
  </entry>
  <entry>
    <title>SkyworkAI/SkyReels-V2</title>
    <updated>2025-08-01T01:30:10Z</updated>
    <id>tag:github.com,2025-08-01:/SkyworkAI/SkyReels-V2</id>
    <link href="https://github.com/SkyworkAI/SkyReels-V2" rel="alternate"></link>
    <summary type="html">&lt;p&gt;SkyReels-V2: Infinite-length Film Generative model&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/logo2.png&#34; alt=&#34;SkyReels Logo&#34; width=&#34;50%&#34;&gt; &lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;SkyReels V2: Infinite-Length Film Generative Model&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; 📑 &lt;a href=&#34;https://arxiv.org/pdf/2504.13074&#34;&gt;Technical Report&lt;/a&gt; · 👋 &lt;a href=&#34;https://www.skyreels.ai/home?utm_campaign=github_SkyReels_V2&#34; target=&#34;_blank&#34;&gt;Playground&lt;/a&gt; · 💬 &lt;a href=&#34;https://discord.gg/PwM6NYtccQ&#34; target=&#34;_blank&#34;&gt;Discord&lt;/a&gt; · 🤗 &lt;a href=&#34;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&#34; target=&#34;_blank&#34;&gt;Hugging Face&lt;/a&gt; · 🤖 &lt;a href=&#34;https://www.modelscope.cn/collections/SkyReels-V2-f665650130b144&#34; target=&#34;_blank&#34;&gt;ModelScope&lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;Welcome to the &lt;strong&gt;SkyReels V2&lt;/strong&gt; repository! Here, you&#39;ll find the model weights and inference code for our infinite-length film generative models. To the best of our knowledge, it represents the first open-source video generative model employing &lt;strong&gt;AutoRegressive Diffusion-Forcing architecture&lt;/strong&gt; that achieves the &lt;strong&gt;SOTA performance&lt;/strong&gt; among publicly available models.&lt;/p&gt; &#xA;&lt;h2&gt;🔥🔥🔥 News!!&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Jun 1, 2025: 🎉 We published the technical report, &lt;a href=&#34;https://arxiv.org/pdf/2506.00830&#34;&gt;SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;May 16, 2025: 🔥 We release the inference code for &lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#ve&#34;&gt;video extension&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#se&#34;&gt;start/end frame control&lt;/a&gt; in diffusion forcing model.&lt;/li&gt; &#xA; &lt;li&gt;Apr 24, 2025: 🔥 We release the 720P models, &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P&#34;&gt;SkyReels-V2-DF-14B-720P&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P&#34;&gt;SkyReels-V2-I2V-14B-720P&lt;/a&gt;. The former facilitates infinite-length autoregressive video generation, and the latter focuses on Image2Video synthesis.&lt;/li&gt; &#xA; &lt;li&gt;Apr 21, 2025: 👋 We release the inference code and model weights of &lt;a href=&#34;https://huggingface.co/collections/Skywork/skyreels-v2-6801b1b93df627d441d0d0d9&#34;&gt;SkyReels-V2&lt;/a&gt; Series Models and the video captioning model &lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt; .&lt;/li&gt; &#xA; &lt;li&gt;Apr 3, 2025: 🔥 We also release &lt;a href=&#34;https://github.com/SkyworkAI/SkyReels-A2&#34;&gt;SkyReels-A2&lt;/a&gt;. This is an open-sourced controllable video generation framework capable of assembling arbitrary visual elements.&lt;/li&gt; &#xA; &lt;li&gt;Feb 18, 2025: 🔥 we released &lt;a href=&#34;https://github.com/SkyworkAI/SkyReels-A1&#34;&gt;SkyReels-A1&lt;/a&gt;. This is an open-sourced and effective framework for portrait image animation.&lt;/li&gt; &#xA; &lt;li&gt;Feb 18, 2025: 🔥 We released &lt;a href=&#34;https://github.com/SkyworkAI/SkyReels-V1&#34;&gt;SkyReels-V1&lt;/a&gt;. This is the first and most advanced open-source human-centric video foundation model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🎥 Demos&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/f6f9f9a7-5d5f-433c-9d73-d8d593b7ad25&#34; width=&#34;100%&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/0eb13415-f4d9-4aaf-bcd3-3031851109b9&#34; width=&#34;100%&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt; &#xA;    &lt;video src=&#34;https://github.com/user-attachments/assets/dcd16603-5bf4-4786-8e4d-1ed23889d07a&#34; width=&#34;100%&#34;&gt;&lt;/video&gt; &lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; The demos above showcase 30-second videos generated using our SkyReels-V2 Diffusion Forcing model. &#xA;&lt;h2&gt;📑 TODO List&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://arxiv.org/pdf/2504.13074&#34;&gt;Technical Report&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Checkpoints of the 14B and 1.3B Models Series&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Single-GPU &amp;amp; Multi-GPU Inference Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt;: A Video Captioning Model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Prompt Enhancer&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Diffusers integration&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Checkpoints of the 5B Models Series&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Checkpoints of the Camera Director Models&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Checkpoints of the Step &amp;amp; Guidance Distill Model&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 Quickstart&lt;/h2&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# clone the repository.&#xA;git clone https://github.com/SkyworkAI/SkyReels-V2&#xA;cd SkyReels-V2&#xA;# Install dependencies. Test environment uses Python 3.10.12.&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Model Download&lt;/h4&gt; &#xA;&lt;p&gt;You can download our models from Hugging Face:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Model Variant&lt;/th&gt; &#xA;   &lt;th&gt;Recommended Height/Width/Frame&lt;/th&gt; &#xA;   &lt;th&gt;Link&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;Diffusion Forcing&lt;/td&gt; &#xA;   &lt;td&gt;1.3B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-DF-1.3B-540P&#34;&gt;Huggingface&lt;/a&gt; 🤖 &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-1.3B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-540P&#34;&gt;Huggingface&lt;/a&gt; 🤖 &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-DF-14B-720P&#34;&gt;Huggingface&lt;/a&gt; 🤖 &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-DF-14B-720P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;Text-to-Video&lt;/td&gt; &#xA;   &lt;td&gt;1.3B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-540P&#34;&gt;Huggingface&lt;/a&gt; 🤖 &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-T2V-14B-720P&#34;&gt;Huggingface&lt;/a&gt; 🤖 &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-T2V-14B-720P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;5&#34;&gt;Image-to-Video&lt;/td&gt; &#xA;   &lt;td&gt;1.3B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-I2V-1.3B-540P&#34;&gt;Huggingface&lt;/a&gt; 🤖 &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-1.3B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-540P&#34;&gt;Huggingface&lt;/a&gt; 🤖 &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-540P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;🤗 &lt;a href=&#34;https://huggingface.co/Skywork/SkyReels-V2-I2V-14B-720P&#34;&gt;Huggingface&lt;/a&gt; 🤖 &lt;a href=&#34;https://www.modelscope.cn/models/Skywork/SkyReels-V2-I2V-14B-720P&#34;&gt;ModelScope&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td rowspan=&#34;3&#34;&gt;Camera Director&lt;/td&gt; &#xA;   &lt;td&gt;5B-540P&lt;/td&gt; &#xA;   &lt;td&gt;544 * 960 * 97f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;5B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B-720P&lt;/td&gt; &#xA;   &lt;td&gt;720 * 1280 * 121f&lt;/td&gt; &#xA;   &lt;td&gt;Coming Soon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;After downloading, set the model path in your generation commands:&lt;/p&gt; &#xA;&lt;h4&gt;Single GPU Inference&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusion Forcing for Long Video Generation&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://arxiv.org/abs/2407.01392&#34;&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/a&gt; version model allows us to generate Infinite-Length videos. This model supports both &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt; and &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; tasks, and it can perform inference in both synchronous and asynchronous modes. Here we demonstrate 2 running scripts as examples for long video generation. If you want to adjust the inference parameters, e.g., the duration of video, inference mode, read the Note below first.&lt;/p&gt; &#xA;&lt;p&gt;synchronous generation for 10s video&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# synchronous inference&#xA;python3 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 0 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 257 \&#xA;  --overlap_history 17 \&#xA;  --prompt &#34;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&#34; \&#xA;  --addnoise_condition 20 \&#xA;  --offload \&#xA;  --teacache \&#xA;  --use_ret_steps \&#xA;  --teacache_thresh 0.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;asynchronous generation for 30s video&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# asynchronous inference&#xA;python3 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 5 \&#xA;  --causal_block_size 5 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 737 \&#xA;  --overlap_history 17 \&#xA;  --prompt &#34;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&#34; \&#xA;  --addnoise_condition 20 \&#xA;  --offload&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;If you want to run the &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; task, add &lt;code&gt;--image ${image_path}&lt;/code&gt; to your command and it is also better to use &lt;strong&gt;text-to-video (T2V)&lt;/strong&gt;-like prompt which includes some descriptions of the first-frame image.&lt;/li&gt; &#xA;  &lt;li&gt;For long video generation, you can just switch the &lt;code&gt;--num_frames&lt;/code&gt;, e.g., &lt;code&gt;--num_frames 257&lt;/code&gt; for 10s video, &lt;code&gt;--num_frames 377&lt;/code&gt; for 15s video, &lt;code&gt;--num_frames 737&lt;/code&gt; for 30s video, &lt;code&gt;--num_frames 1457&lt;/code&gt; for 60s video. The number is not strictly aligned with the logical frame number for specified time duration, but it is aligned with some training parameters, which means it may perform better. When you use asynchronous inference with causal_block_size &amp;gt; 1, the &lt;code&gt;--num_frames&lt;/code&gt; should be carefully set.&lt;/li&gt; &#xA;  &lt;li&gt;You can use &lt;code&gt;--ar_step 5&lt;/code&gt; to enable asynchronous inference. When asynchronous inference, &lt;code&gt;--causal_block_size 5&lt;/code&gt; is recommended while it is not supposed to be set for synchronous generation. REMEMBER that the frame latent number inputted into the model in every iteration, e.g., base frame latent number (e.g., (97-1)//4+1=25 for base_num_frames=97) and (e.g., (237-97-(97-17)x1+17-1)//4+1=20 for base_num_frames=97, num_frames=237, overlap_history=17) for the last iteration, MUST be divided by causal_block_size. If you find it too hard to calculate and set proper values, just use our recommended setting above :). Asynchronous inference will take more steps to diffuse the whole sequence which means it will be SLOWER than synchronous mode. In our experiments, asynchronous inference may improve the instruction following and visual consistent performance.&lt;/li&gt; &#xA;  &lt;li&gt;To reduce peak VRAM, just lower the &lt;code&gt;--base_num_frames&lt;/code&gt;, e.g., to 77 or 57, while keeping the same generative length &lt;code&gt;--num_frames&lt;/code&gt; you want to generate. This may slightly reduce video quality, and it should not be set too small.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;--addnoise_condition&lt;/code&gt; is used to help smooth the long video generation by adding some noise to the clean condition. Too large noise can cause the inconsistency as well. 20 is a recommended value, and you may try larger ones, but it is recommended to not exceed 50.&lt;/li&gt; &#xA;  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 51.2GB peak VRAM.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span id=&#34;ve&#34;&gt;Video Extention&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# video extention&#xA;python3 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 0 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 120 \&#xA;  --overlap_history 17 \&#xA;  --prompt ${prompt} \&#xA;  --addnoise_condition 20 \&#xA;  --offload \&#xA;  --use_ret_steps \&#xA;  --teacache \&#xA;  --teacache_thresh 0.3 \&#xA;  --video_path ${video_path}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;When performing video extension, you need to pass the &lt;code&gt;--video_path ${video_path}&lt;/code&gt; parameter to specify the video to be extended.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;span id=&#34;se&#34;&gt;Start/End Frame Control&lt;/span&gt;&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# start/end frame control&#xA;python3 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 0 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 97 \&#xA;  --overlap_history 17 \&#xA;  --prompt ${prompt} \&#xA;  --addnoise_condition 20 \&#xA;  --offload \&#xA;  --use_ret_steps \&#xA;  --teacache \&#xA;  --teacache_thresh 0.3 \&#xA;  --image ${image} \&#xA;  --end_image ${end_image}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;When controlling the start and end frames, you need to pass the &lt;code&gt;--image ${image}&lt;/code&gt; parameter to control the generation of the start frame and the &lt;code&gt;--end_image ${end_image}&lt;/code&gt; parameter to control the generation of the end frame.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# run Text-to-Video Generation&#xA;model_id=Skywork/SkyReels-V2-T2V-14B-540P&#xA;python3 generate_video.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --num_frames 97 \&#xA;  --guidance_scale 6.0 \&#xA;  --shift 8.0 \&#xA;  --fps 24 \&#xA;  --prompt &#34;A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface.&#34; \&#xA;  --offload \&#xA;  --teacache \&#xA;  --use_ret_steps \&#xA;  --teacache_thresh 0.3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; &#xA;  &lt;li&gt;Generating a 540P video using the 1.3B model requires approximately 14.7GB peak VRAM, while the same resolution video using the 14B model demands around 43.4GB peak VRAM.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Prompt Enhancer&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The prompt enhancer is implemented based on &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-32B-Instruct&#34;&gt;Qwen2.5-32B-Instruct&lt;/a&gt; and is utilized via the &lt;code&gt;--prompt_enhancer&lt;/code&gt; parameter. It works ideally for short prompts, while for long prompts, it might generate an excessively lengthy prompt that could lead to over-saturation in the generative video. Note the peak memory of GPU is 64G+ if you use &lt;code&gt;--prompt_enhancer&lt;/code&gt;. If you want to obtain the enhanced prompt separately, you can also run the prompt_enhancer script separately for testing. The steps are as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;cd skyreels_v2_infer/pipelines&#xA;python3 prompt_enhancer.py --prompt &#34;A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface.&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;code&gt;--prompt_enhancer&lt;/code&gt; is not allowed if using &lt;code&gt;--use_usp&lt;/code&gt;. We recommend running the skyreels_v2_infer/pipelines/prompt_enhancer.py script first to generate enhanced prompt before enabling the &lt;code&gt;--use_usp&lt;/code&gt; parameter.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Advanced Configuration Options&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below are the key parameters you can customize for video generation:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameter&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Recommended Value&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--prompt&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Text description for generating your video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--image&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Path to input image for image-to-video generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--resolution&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;540P or 720P&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Output video resolution (select based on model type)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--num_frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97 or 121&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Total frames to generate (&lt;strong&gt;97 for 540P models&lt;/strong&gt;, &lt;strong&gt;121 for 720P models&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--inference_steps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;50&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Number of denoising steps&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--fps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Frames per second in the output video&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--shift&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.0 or 5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Flow matching scheduler parameter (&lt;strong&gt;8.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--guidance_scale&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6.0 or 5.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Controls text adherence strength (&lt;strong&gt;6.0 for T2V&lt;/strong&gt;, &lt;strong&gt;5.0 for I2V&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--seed&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Fixed seed for reproducible results (omit for random generation)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--offload&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Offloads model components to CPU to reduce VRAM usage (recommended)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--use_usp&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Enables multi-GPU acceleration with xDiT USP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--outdir&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;./video_out&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Directory where generated videos will be saved&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--prompt_enhancer&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;True&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Expand the prompt into a more detailed description&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--teacache&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Enables teacache for faster inference&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--teacache_thresh&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0.2&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Higher speedup will cause to worse quality&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--use_ret_steps&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;False&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Retention Steps for teacache&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Diffusion Forcing Additional Parameters&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Parameter&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Recommended Value&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--ar_step&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Controls asynchronous inference (0 for synchronous mode)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--base_num_frames&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;97 or 121&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Base frame count (&lt;strong&gt;97 for 540P&lt;/strong&gt;, &lt;strong&gt;121 for 720P&lt;/strong&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--overlap_history&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Number of frames to overlap for smooth transitions in long videos&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--addnoise_condition&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;20&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Improves consistency in long video generation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--causal_block_size&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Recommended when using asynchronous inference (--ar_step &amp;gt; 0)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--video_path&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Path to input video for video extension&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;--end_image&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Path to input image for end frame control&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h4&gt;Multi-GPU inference using xDiT USP&lt;/h4&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/xdit-project/xDiT&#34;&gt;xDiT&lt;/a&gt; USP to accelerate inference. For example, to generate a video with 2 GPUs, you can use the following command:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusion Forcing&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;model_id=Skywork/SkyReels-V2-DF-14B-540P&#xA;# diffusion forcing synchronous inference&#xA;torchrun --nproc_per_node=2 generate_video_df.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --ar_step 0 \&#xA;  --base_num_frames 97 \&#xA;  --num_frames 257 \&#xA;  --overlap_history 17 \&#xA;  --prompt &#34;A graceful white swan with a curved neck and delicate feathers swimming in a serene lake at dawn, its reflection perfectly mirrored in the still water as mist rises from the surface, with the swan occasionally dipping its head into the water to feed.&#34; \&#xA;  --addnoise_condition 20 \&#xA;  --use_usp \&#xA;  --offload \&#xA;  --seed 42&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Text To Video &amp;amp; Image To Video&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# run Text-to-Video Generation&#xA;model_id=Skywork/SkyReels-V2-T2V-14B-540P&#xA;torchrun --nproc_per_node=2 generate_video.py \&#xA;  --model_id ${model_id} \&#xA;  --resolution 540P \&#xA;  --num_frames 97 \&#xA;  --guidance_scale 6.0 \&#xA;  --shift 8.0 \&#xA;  --fps 24 \&#xA;  --offload \&#xA;  --prompt &#34;A serene lake surrounded by towering mountains, with a few swans gracefully gliding across the water and sunlight dancing on the surface.&#34; \&#xA;  --use_usp \&#xA;  --seed 42&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;:&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;When using an &lt;strong&gt;image-to-video (I2V)&lt;/strong&gt; model, you must provide an input image using the &lt;code&gt;--image ${image_path}&lt;/code&gt; parameter. The &lt;code&gt;--guidance_scale 5.0&lt;/code&gt; and &lt;code&gt;--shift 3.0&lt;/code&gt; is recommended for I2V model.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#methodology-of-skyreels-v2&#34;&gt;Methodology of SkyReels-V2&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#key-contributions-of-skyreels-v2&#34;&gt;Key Contributions of SkyReels-V2&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#video-captioner&#34;&gt;Video Captioner&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#diffusion-forcing&#34;&gt;Diffusion Forcing&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#high-quality-supervised-fine-tuning-sft&#34;&gt;High-Quality Supervised Fine-Tuning(SFT)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#performance&#34;&gt;Performance&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Recent advances in video generation have been driven by diffusion models and autoregressive frameworks, yet critical challenges persist in harmonizing prompt adherence, visual quality, motion dynamics, and duration: compromises in motion dynamics to enhance temporal visual quality, constrained video duration (5-10 seconds) to prioritize resolution, and inadequate shot-aware generation stemming from general-purpose MLLMs&#39; inability to interpret cinematic grammar, such as shot composition, actor expressions, and camera motions. These intertwined limitations hinder realistic long-form synthesis and professional film-style generation.&lt;/p&gt; &#xA;&lt;p&gt;To address these limitations, we introduce SkyReels-V2, the world&#39;s first infinite-length film generative model using a Diffusion Forcing framework. Our approach synergizes Multi-modal Large Language Models (MLLM), Multi-stage Pretraining, Reinforcement Learning, and Diffusion Forcing techniques to achieve comprehensive optimization. Beyond its technical innovations, SkyReels-V2 enables multiple practical applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and multi-subject consistent video generation through our &lt;a href=&#34;https://github.com/SkyworkAI/SkyReels-A2&#34;&gt;Skyreels-A2&lt;/a&gt; system.&lt;/p&gt; &#xA;&lt;h2&gt;Methodology of SkyReels-V2&lt;/h2&gt; &#xA;&lt;p&gt;The SkyReels-V2 methodology consists of several interconnected components. It starts with a comprehensive data processing pipeline that prepares various quality training data. At its core is the Video Captioner architecture, which provides detailed annotations for video content. The system employs a multi-task pretraining strategy to build fundamental video generation capabilities. Post-training optimization includes Reinforcement Learning to enhance motion quality, Diffusion Forcing Training for generating extended videos, and High-quality Supervised Fine-Tuning (SFT) stages for visual refinement. The model runs on optimized computational infrastructure for efficient training and inference. SkyReels-V2 supports multiple applications, including Story Generation, Image-to-Video Synthesis, Camera Director functionality, and Elements-to-Video Generation.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/SkyworkAI/SkyReels-V2/main/assets/main_pipeline.jpg&#34; alt=&#34;mainpipeline&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Key Contributions of SkyReels-V2&lt;/h2&gt; &#xA;&lt;h4&gt;Video Captioner&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt; serves as our video captioning model for data annotation. This model is trained on the captioning result from the base model &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct&#34;&gt;Qwen2.5-VL-72B-Instruct&lt;/a&gt; and the sub-expert captioners on a balanced video data. The balanced video data is a carefully curated dataset of approximately 2 million videos to ensure conceptual balance and annotation quality. Built upon the &lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&#34;&gt;Qwen2.5-VL-7B-Instruct&lt;/a&gt; foundation model, &lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt; is fine-tuned to enhance performance in domain-specific video captioning tasks. To compare the performance with the SOTA models, we conducted a manual assessment of accuracy across different captioning fields using a test set of 1,000 samples. The proposed &lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt; achieves the highest average accuracy among the baseline models, and show a dramatic result in the shot related fields&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;model&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct&#34;&gt;Qwen2.5-VL-7B-Ins.&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct&#34;&gt;Qwen2.5-VL-72B-Ins.&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://huggingface.co/omni-research/Tarsier2-Recap-7b&#34;&gt;Tarsier2-Recap-7b&lt;/a&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;a href=&#34;https://huggingface.co/Skywork/SkyCaptioner-V1&#34;&gt;SkyCaptioner-V1&lt;/a&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Avg accuracy&lt;/td&gt; &#xA;   &lt;td&gt;51.4%&lt;/td&gt; &#xA;   &lt;td&gt;58.7%&lt;/td&gt; &#xA;   &lt;td&gt;49.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;76.3%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;shot type&lt;/td&gt; &#xA;   &lt;td&gt;76.8%&lt;/td&gt; &#xA;   &lt;td&gt;82.5%&lt;/td&gt; &#xA;   &lt;td&gt;60.2%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;93.7%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;shot angle&lt;/td&gt; &#xA;   &lt;td&gt;60.0%&lt;/td&gt; &#xA;   &lt;td&gt;73.7%&lt;/td&gt; &#xA;   &lt;td&gt;52.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;89.8%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;shot position&lt;/td&gt; &#xA;   &lt;td&gt;28.4%&lt;/td&gt; &#xA;   &lt;td&gt;32.7%&lt;/td&gt; &#xA;   &lt;td&gt;23.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;83.1%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;camera motion&lt;/td&gt; &#xA;   &lt;td&gt;62.0%&lt;/td&gt; &#xA;   &lt;td&gt;61.2%&lt;/td&gt; &#xA;   &lt;td&gt;45.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;85.3%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;expression&lt;/td&gt; &#xA;   &lt;td&gt;43.6%&lt;/td&gt; &#xA;   &lt;td&gt;51.5%&lt;/td&gt; &#xA;   &lt;td&gt;54.3%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;68.8%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td colspan=&#34;5&#34; style=&#34;text-align: center; border-bottom: 1px solid #ddd; padding: 8px;&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TYPES_type&lt;/td&gt; &#xA;   &lt;td&gt;43.5%&lt;/td&gt; &#xA;   &lt;td&gt;49.7%&lt;/td&gt; &#xA;   &lt;td&gt;47.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;82.5%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;TYPES_sub_type&lt;/td&gt; &#xA;   &lt;td&gt;38.9%&lt;/td&gt; &#xA;   &lt;td&gt;44.9%&lt;/td&gt; &#xA;   &lt;td&gt;45.9%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;75.4%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;appearance&lt;/td&gt; &#xA;   &lt;td&gt;40.9%&lt;/td&gt; &#xA;   &lt;td&gt;52.0%&lt;/td&gt; &#xA;   &lt;td&gt;45.6%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;59.3%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;action&lt;/td&gt; &#xA;   &lt;td&gt;32.4%&lt;/td&gt; &#xA;   &lt;td&gt;52.0%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;69.8%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;68.8%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;position&lt;/td&gt; &#xA;   &lt;td&gt;35.4%&lt;/td&gt; &#xA;   &lt;td&gt;48.6%&lt;/td&gt; &#xA;   &lt;td&gt;45.5%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;57.5%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;is_main_subject&lt;/td&gt; &#xA;   &lt;td&gt;58.5%&lt;/td&gt; &#xA;   &lt;td&gt;68.7%&lt;/td&gt; &#xA;   &lt;td&gt;69.7%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;80.9%&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;environment&lt;/td&gt; &#xA;   &lt;td&gt;70.4%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;72.7%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;61.4%&lt;/td&gt; &#xA;   &lt;td&gt;70.5%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lighting&lt;/td&gt; &#xA;   &lt;td&gt;77.1%&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;80.0%&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;21.2%&lt;/td&gt; &#xA;   &lt;td&gt;76.5%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Reinforcement Learning&lt;/h4&gt; &#xA;&lt;p&gt;Inspired by the previous success in LLM, we propose to enhance the performance of the generative model by Reinforcement Learning. Specifically, we focus on the motion quality because we find that the main drawback of our generative model is:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;the generative model does not handle well with large, deformable motions.&lt;/li&gt; &#xA; &lt;li&gt;the generated videos may violate the physical law.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To avoid the degradation in other metrics, such as text alignment and video quality, we ensure the preference data pairs have comparable text alignment and video quality, while only the motion quality varies. This requirement poses greater challenges in obtaining preference annotations due to the inherently higher costs of human annotation. To address this challenge, we propose a semi-automatic pipeline that strategically combines automatically generated motion pairs and human annotation results. This hybrid approach not only enhances the data scale but also improves alignment with human preferences through curated quality control. Leveraging this enhanced dataset, we first train a specialized reward model to capture the generic motion quality differences between paired samples. This learned reward function subsequently guides the sample selection process for Direct Preference Optimization (DPO), enhancing the motion quality of the generative model.&lt;/p&gt; &#xA;&lt;h4&gt;Diffusion Forcing&lt;/h4&gt; &#xA;&lt;p&gt;We introduce the Diffusion Forcing Transformer to unlock our model’s ability to generate long videos. Diffusion Forcing is a training and sampling strategy where each token is assigned an independent noise level. This allows tokens to be denoised according to arbitrary, per-token schedules. Conceptually, this approach functions as a form of partial masking: a token with zero noise is fully unmasked, while complete noise fully masks it. Diffusion Forcing trains the model to &#34;unmask&#34; any combination of variably noised tokens, using the cleaner tokens as conditional information to guide the recovery of noisy ones. Building on this, our Diffusion Forcing Transformer can extend video generation indefinitely based on the last frames of the previous segment. Note that the synchronous full sequence diffusion is a special case of Diffusion Forcing, where all tokens share the same noise level. This relationship allows us to fine-tune the Diffusion Forcing Transformer from a full-sequence diffusion model.&lt;/p&gt; &#xA;&lt;h4&gt;High-Quality Supervised Fine-Tuning (SFT)&lt;/h4&gt; &#xA;&lt;p&gt;We implement two sequential high-quality supervised fine-tuning (SFT) stages at 540p and 720p resolutions respectively, with the initial SFT phase conducted immediately after pretraining but prior to reinforcement learning (RL) stage.This first-stage SFT serves as a conceptual equilibrium trainer, building upon the foundation model’s pretraining outcomes that utilized only fps24 video data, while strategically removing FPS embedding components to streamline thearchitecture. Trained with the high-quality concept-balanced samples, this phase establishes optimized initialization parameters for subsequent training processes. Following this, we execute a secondary high-resolution SFT at 720p after completing the diffusion forcing stage, incorporating identical loss formulations and the higher-quality concept-balanced datasets by the manually filter. This final refinement phase focuses on resolution increase such that the overall video quality will be further enhanced.&lt;/p&gt; &#xA;&lt;h2&gt;Performance&lt;/h2&gt; &#xA;&lt;p&gt;To comprehensively evaluate our proposed method, we construct the SkyReels-Bench for human assessment and leveraged the open-source &lt;a href=&#34;https://github.com/Vchitect/VBench&#34;&gt;V-Bench&lt;/a&gt; for automated evaluation. This allows us to compare our model with the state-of-the-art (SOTA) baselines, including both open-source and proprietary models.&lt;/p&gt; &#xA;&lt;h4&gt;Human Evaluation&lt;/h4&gt; &#xA;&lt;p&gt;For human evaluation, we design SkyReels-Bench with 1,020 text prompts, systematically assessing three dimensions: Instruction Adherence, Motion Quality, Consistency and Visual Quality. This benchmark is designed to evaluate both text-to-video (T2V) and image-to-video (I2V) generation models, providing comprehensive assessment across different generation paradigms. To ensure fairness, all models were evaluated under default settings with consistent resolutions, and no post-generation filtering was applied.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Text To Video Models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Name&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;   &lt;th&gt;Instruction Adherence&lt;/th&gt; &#xA;   &lt;th&gt;Consistency&lt;/th&gt; &#xA;   &lt;th&gt;Visual Quality&lt;/th&gt; &#xA;   &lt;th&gt;Motion Quality&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://runwayml.com/research/introducing-gen-3-alpha&#34;&gt;Runway-Gen3 Alpha&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.53&lt;/td&gt; &#xA;   &lt;td&gt;2.19&lt;/td&gt; &#xA;   &lt;td&gt;2.57&lt;/td&gt; &#xA;   &lt;td&gt;3.23&lt;/td&gt; &#xA;   &lt;td&gt;2.11&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.82&lt;/td&gt; &#xA;   &lt;td&gt;2.64&lt;/td&gt; &#xA;   &lt;td&gt;2.81&lt;/td&gt; &#xA;   &lt;td&gt;3.20&lt;/td&gt; &#xA;   &lt;td&gt;2.61&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://klingai.com&#34;&gt;Kling-1.6 STD Mode&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.99&lt;/td&gt; &#xA;   &lt;td&gt;2.77&lt;/td&gt; &#xA;   &lt;td&gt;3.05&lt;/td&gt; &#xA;   &lt;td&gt;3.39&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;2.76&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hailuoai.video&#34;&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.0&lt;/td&gt; &#xA;   &lt;td&gt;2.8&lt;/td&gt; &#xA;   &lt;td&gt;3.08&lt;/td&gt; &#xA;   &lt;td&gt;3.29&lt;/td&gt; &#xA;   &lt;td&gt;2.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.12&lt;/td&gt; &#xA;   &lt;td&gt;2.91&lt;/td&gt; &#xA;   &lt;td&gt;3.31&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.54&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.71&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SkyReels-V2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.14&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.15&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;3.35&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.34&lt;/td&gt; &#xA;   &lt;td&gt;2.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;The evaluation demonstrates that our model achieves significant advancements in &lt;strong&gt;instruction adherence (3.15)&lt;/strong&gt; compared to baseline methods, while maintaining competitive performance in &lt;strong&gt;motion quality (2.74)&lt;/strong&gt; without sacrificing the &lt;strong&gt;consistency (3.35)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Image To Video Models&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Average&lt;/th&gt; &#xA;   &lt;th&gt;Instruction Adherence&lt;/th&gt; &#xA;   &lt;th&gt;Consistency&lt;/th&gt; &#xA;   &lt;th&gt;Visual Quality&lt;/th&gt; &#xA;   &lt;th&gt;Motion Quality&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.84&lt;/td&gt; &#xA;   &lt;td&gt;2.97&lt;/td&gt; &#xA;   &lt;td&gt;2.95&lt;/td&gt; &#xA;   &lt;td&gt;2.87&lt;/td&gt; &#xA;   &lt;td&gt;2.56&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.85&lt;/td&gt; &#xA;   &lt;td&gt;3.10&lt;/td&gt; &#xA;   &lt;td&gt;2.81&lt;/td&gt; &#xA;   &lt;td&gt;3.00&lt;/td&gt; &#xA;   &lt;td&gt;2.48&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://hailuoai.video&#34;&gt;Hailuo-01&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.05&lt;/td&gt; &#xA;   &lt;td&gt;3.31&lt;/td&gt; &#xA;   &lt;td&gt;2.58&lt;/td&gt; &#xA;   &lt;td&gt;3.55&lt;/td&gt; &#xA;   &lt;td&gt;2.74&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://klingai.com&#34;&gt;Kling-1.6 Pro Mode&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.4&lt;/td&gt; &#xA;   &lt;td&gt;3.56&lt;/td&gt; &#xA;   &lt;td&gt;3.03&lt;/td&gt; &#xA;   &lt;td&gt;3.58&lt;/td&gt; &#xA;   &lt;td&gt;3.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://runwayml.com/research/introducing-runway-gen-4&#34;&gt;Runway-Gen4&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3.39&lt;/td&gt; &#xA;   &lt;td&gt;3.75&lt;/td&gt; &#xA;   &lt;td&gt;3.2&lt;/td&gt; &#xA;   &lt;td&gt;3.4&lt;/td&gt; &#xA;   &lt;td&gt;3.37&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SkyReels-V2-DF&lt;/td&gt; &#xA;   &lt;td&gt;3.24&lt;/td&gt; &#xA;   &lt;td&gt;3.64&lt;/td&gt; &#xA;   &lt;td&gt;3.21&lt;/td&gt; &#xA;   &lt;td&gt;3.18&lt;/td&gt; &#xA;   &lt;td&gt;2.93&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SkyReels-V2-I2V&lt;/td&gt; &#xA;   &lt;td&gt;3.29&lt;/td&gt; &#xA;   &lt;td&gt;3.42&lt;/td&gt; &#xA;   &lt;td&gt;3.18&lt;/td&gt; &#xA;   &lt;td&gt;3.56&lt;/td&gt; &#xA;   &lt;td&gt;3.01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our results demonstrate that both &lt;strong&gt;SkyReels-V2-I2V (3.29)&lt;/strong&gt; and &lt;strong&gt;SkyReels-V2-DF (3.24)&lt;/strong&gt; achieve state-of-the-art performance among open-source models, significantly outperforming HunyuanVideo-13B (2.84) and Wan2.1-14B (2.85) across all quality dimensions. With an average score of 3.29, SkyReels-V2-I2V demonstrates comparable performance to proprietary models Kling-1.6 (3.4) and Runway-Gen4 (3.39).&lt;/p&gt; &#xA;&lt;h4&gt;VBench&lt;/h4&gt; &#xA;&lt;p&gt;To objectively compare SkyReels-V2 Model against other leading open-source Text-To-Video models, we conduct comprehensive evaluations using the public benchmark &lt;a href=&#34;https://github.com/Vchitect/VBench&#34;&gt;V-Bench&lt;/a&gt;. Our evaluation specifically leverages the benchmark’s longer version prompt. For fair comparison with baseline models, we strictly follow their recommended setting for inference.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Total Score&lt;/th&gt; &#xA;   &lt;th&gt;Quality Score&lt;/th&gt; &#xA;   &lt;th&gt;Semantic Score&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora&#34;&gt;OpenSora 2.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;81.5 %&lt;/td&gt; &#xA;   &lt;td&gt;82.1 %&lt;/td&gt; &#xA;   &lt;td&gt;78.2 %&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/THUDM/CogVideo&#34;&gt;CogVideoX1.5-5B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;80.3 %&lt;/td&gt; &#xA;   &lt;td&gt;80.9 %&lt;/td&gt; &#xA;   &lt;td&gt;77.9 %&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanVideo&#34;&gt;HunyuanVideo-13B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;82.7 %&lt;/td&gt; &#xA;   &lt;td&gt;84.4 %&lt;/td&gt; &#xA;   &lt;td&gt;76.2 %&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan2.1-14B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;83.7 %&lt;/td&gt; &#xA;   &lt;td&gt;84.2 %&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;81.4 %&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SkyReels-V2&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;83.9 %&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;strong&gt;84.7 %&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;80.8 %&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p&gt;The VBench results demonstrate that SkyReels-V2 outperforms all compared models including HunyuanVideo-13B and Wan2.1-14B, With the highest &lt;strong&gt;total score (83.9%)&lt;/strong&gt; and &lt;strong&gt;quality score (84.7%)&lt;/strong&gt;. In this evaluation, the semantic score is slightly lower than Wan2.1-14B, while we outperform Wan2.1-14B in human evaluations, with the primary gap attributed to V-Bench’s insufficient evaluation of shot-scenario semantic adherence.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We would like to thank the contributors of &lt;a href=&#34;https://github.com/Wan-Video/Wan2.1&#34;&gt;Wan 2.1&lt;/a&gt;, &lt;a href=&#34;https://github.com/xdit-project/xDiT&#34;&gt;XDit&lt;/a&gt; and &lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5/&#34;&gt;Qwen 2.5&lt;/a&gt; repositories, for their open research and contributions.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{chen2025skyreelsv2infinitelengthfilmgenerative,&#xA;      title={SkyReels-V2: Infinite-length Film Generative Model}, &#xA;      author={Guibin Chen and Dixuan Lin and Jiangping Yang and Chunze Lin and Junchen Zhu and Mingyuan Fan and Hao Zhang and Sheng Chen and Zheng Chen and Chengcheng Ma and Weiming Xiong and Wei Wang and Nuo Pang and Kang Kang and Zhiheng Xu and Yuzhe Jin and Yupeng Liang and Yubing Song and Peng Zhao and Boyuan Xu and Di Qiu and Debang Li and Zhengcong Fei and Yang Li and Yahui Zhou},&#xA;      year={2025},&#xA;      eprint={2504.13074},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CV},&#xA;      url={https://arxiv.org/abs/2504.13074}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>