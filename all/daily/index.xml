<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-06-13T01:29:14Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>xiangechen/chili3d</title>
    <updated>2025-06-13T01:29:14Z</updated>
    <id>tag:github.com,2025-06-13:/xiangechen/chili3d</id>
    <link href="https://github.com/xiangechen/chili3d" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A web-based 3D CAD application for online model design and editing&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Chili3D&lt;/h1&gt; &#xA;&lt;p&gt;A web-based 3D CAD application for online model design and editing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/xiangechen/chili3d/main/screenshots/screenshot.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://chili3d.com&#34;&gt;Chili3D&lt;/a&gt; is an &lt;a href=&#34;https://github.com/xiangechen/chili3d&#34;&gt;open-source&lt;/a&gt;, browser-based 3D CAD (Computer-Aided Design) application built with TypeScript. It achieves near-native performance by compiling OpenCascade (OCCT) to WebAssembly and integrating with Three.js, enabling powerful online modeling, editing, and rendering‚Äîall without the need for local installation.&lt;/p&gt; &#xA;&lt;p&gt;You can access Chili3D online at:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Official website: &lt;a href=&#34;https://chili3d.com&#34;&gt;chili3d.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Cloudflare deployment: &lt;a href=&#34;https://chili3d.pages.dev&#34;&gt;chili3d.pages.dev&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;h3&gt;Modeling Tools&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Basic Shapes&lt;/strong&gt;: Create boxes, cylinders, cones, spheres, pyramids, and more&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2D Sketching&lt;/strong&gt;: Draw lines, arcs, circles, ellipses, rectangles, polygons, and Bezier curves&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced Operations&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Boolean operations (union, difference, intersection)&lt;/li&gt; &#xA;   &lt;li&gt;Extrusion and revolution&lt;/li&gt; &#xA;   &lt;li&gt;Sweeping and lofting&lt;/li&gt; &#xA;   &lt;li&gt;Offset surfaces&lt;/li&gt; &#xA;   &lt;li&gt;Section creation&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Snapping and Tracking&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Object Snapping&lt;/strong&gt;: Precisely snap to geometric features (points, edges, faces)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Workplane Snapping&lt;/strong&gt;: Snap to the current workplane for accurate planar operations&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Axis Tracking&lt;/strong&gt;: Create objects along tracked axes for precise alignment&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Feature Point Detection&lt;/strong&gt;: Automatically detect and snap to key geometric features&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tracking Visualization&lt;/strong&gt;: Visual guides showing tracking lines and reference points&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Editing Tools&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Modification&lt;/strong&gt;: Chamfer, fillet, trim, break, split&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Transformation&lt;/strong&gt;: Move, rotate, mirror&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Advanced Editing&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Feature removal&lt;/li&gt; &#xA;   &lt;li&gt;Sub-shape manipulation&lt;/li&gt; &#xA;   &lt;li&gt;Explode compound objects&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Measurement Tools&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Measure angles and lengths&lt;/li&gt; &#xA; &lt;li&gt;Calculate the sum of length, area, and volume&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Document Management&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create, open, and save documents&lt;/li&gt; &#xA; &lt;li&gt;Full undo/redo stack with transaction history&lt;/li&gt; &#xA; &lt;li&gt;Import/export of industry-standard formats (STEP, IGES, BREP)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;User Interface&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Office-style interface with contextual command organization&lt;/li&gt; &#xA; &lt;li&gt;Hierarchical assembly management with flexible grouping capabilities&lt;/li&gt; &#xA; &lt;li&gt;Dynamic workplane support&lt;/li&gt; &#xA; &lt;li&gt;3D viewport with camera controls&lt;/li&gt; &#xA; &lt;li&gt;Camera position recall&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Localization&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-Language Interface&lt;/strong&gt;: Built-in internationalization (i18n) supporting seamless adaptation to global user bases&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Current Languages&lt;/strong&gt;: Chinese &amp;amp; English; contributions for additional languages are welcome&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Technology Stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: TypeScript, Three.js&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;3D Engine&lt;/strong&gt;: OpenCascade (via WebAssembly)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Build Tools&lt;/strong&gt;: Rspack&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Testing&lt;/strong&gt;: Jest&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Change Log&lt;/h2&gt; &#xA;&lt;p&gt;You can view the full change log &lt;a href=&#34;https://github.com/xiangechen/chili3d/releases&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For Chinese users, you can also browse the &lt;a href=&#34;https://space.bilibili.com/539380032/lists/3108412?type=season&#34;&gt;media&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Node.js&lt;/li&gt; &#xA; &lt;li&gt;npm&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Clone the repository&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/xiangechen/chili3d.git&#xA;cd chili3d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install dependencies&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Development&lt;/h3&gt; &#xA;&lt;p&gt;Start the development server:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev # Launches at http://localhost:8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;Build the application:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Building wasm&lt;/h3&gt; &#xA;&lt;p&gt;if you want to build wasm by yourself, you can use the following commands:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Set up WebAssembly dependencies(if you have not installed them yet)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run setup:wasm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Build the WebAssembly module:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run build:wasm&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Development Status&lt;/h2&gt; &#xA;&lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Early Development Notice&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Chili3D is currently in active alpha development. Key considerations:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Core APIs may undergo breaking changes&lt;/li&gt; &#xA; &lt;li&gt;Essential features are under implementation&lt;/li&gt; &#xA; &lt;li&gt;Documentation is being progressively developed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome contributions in the form of code, bug reports, or feedback. Please feel free to submit pull requests or open issues.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Discussions&lt;/strong&gt;: Join our &lt;a href=&#34;https://github.com/xiangechen/chili3d/discussions&#34;&gt;GitHub discussions&lt;/a&gt; for general chat or questions&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Issues&lt;/strong&gt;: Use &lt;a href=&#34;https://github.com/xiangechen/chili3d/issues&#34;&gt;GitHub issues&lt;/a&gt; to report public suggestions or bugs&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Email&lt;/strong&gt;: Contact us privately at &lt;a href=&#34;mailto:xiangetg@msn.cn&#34;&gt;xiangetg@msn.cn&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Distributed under the GNU Affero General Public License v3.0 (AGPL-3.0). For commercial licensing options, contact &lt;a href=&#34;mailto:xiangetg@msn.cn&#34;&gt;xiangetg@msn.cn&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Full license details: &lt;a href=&#34;https://raw.githubusercontent.com/xiangechen/chili3d/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>confident-ai/deepeval</title>
    <updated>2025-06-13T01:29:14Z</updated>
    <id>tag:github.com,2025-06-13:/confident-ai/deepeval</id>
    <link href="https://github.com/confident-ai/deepeval" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The LLM Evaluation Framework&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/confident-ai/deepeval/raw/main/docs/static/img/deepeval.png&#34; alt=&#34;DeepEval Logo&#34; width=&#34;100%&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;/p&gt;&#xA;&lt;h1 align=&#34;center&#34;&gt;The LLM Evaluation Framework&lt;/h1&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/3SEyvpgu2f&#34;&gt; &lt;img alt=&#34;discord-invite&#34; src=&#34;https://dcbadge.vercel.app/api/server/3SEyvpgu2f?style=flat&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; &lt;p&gt; &lt;a href=&#34;https://deepeval.com/docs/getting-started?utm_source=GitHub&#34;&gt;Documentation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/confident-ai/deepeval/main/#-metrics-and-features&#34;&gt;Metrics and Features&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/confident-ai/deepeval/main/#-quickstart&#34;&gt;Getting Started&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/confident-ai/deepeval/main/#-integrations&#34;&gt;Integrations&lt;/a&gt; | &lt;a href=&#34;https://confident-ai.com?utm_source=GitHub&#34;&gt;DeepEval Platform&lt;/a&gt; &lt;/p&gt;&lt;p&gt; &lt;/p&gt;&lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/confident-ai/deepeval/releases&#34;&gt; &lt;img alt=&#34;GitHub release&#34; src=&#34;https://img.shields.io/github/release/confident-ai/deepeval.svg?color=violet&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1PPxYEBa6eu__LquGoFFJZkhYgWVYE6kh?usp=sharing&#34;&gt; &lt;img alt=&#34;Try Quickstart in Colab&#34; src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/confident-ai/deepeval/raw/master/LICENSE.md&#34;&gt; &lt;img alt=&#34;License&#34; src=&#34;https://img.shields.io/github/license/confident-ai/deepeval.svg?color=yellow&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;!-- Keep these links. Translations will automatically update with the README. --&gt; &lt;a href=&#34;https://www.readme-i18n.com/confident-ai/deepeval?lang=de&#34;&gt;Deutsch&lt;/a&gt; | &lt;a href=&#34;https://www.readme-i18n.com/confident-ai/deepeval?lang=es&#34;&gt;Espa√±ol&lt;/a&gt; | &lt;a href=&#34;https://www.readme-i18n.com/confident-ai/deepeval?lang=fr&#34;&gt;fran√ßais&lt;/a&gt; | &lt;a href=&#34;https://www.readme-i18n.com/confident-ai/deepeval?lang=ja&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt; | &lt;a href=&#34;https://www.readme-i18n.com/confident-ai/deepeval?lang=ko&#34;&gt;ÌïúÍµ≠Ïñ¥&lt;/a&gt; | &lt;a href=&#34;https://www.readme-i18n.com/confident-ai/deepeval?lang=pt&#34;&gt;Portugu√™s&lt;/a&gt; | &lt;a href=&#34;https://www.readme-i18n.com/confident-ai/deepeval?lang=ru&#34;&gt;–†—É—Å—Å–∫–∏–π&lt;/a&gt; | &lt;a href=&#34;https://www.readme-i18n.com/confident-ai/deepeval?lang=zh&#34;&gt;‰∏≠Êñá&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;DeepEval&lt;/strong&gt; is a simple-to-use, open-source LLM evaluation framework, for evaluating and testing large-language model systems. It is similar to Pytest but specialized for unit testing LLM outputs. DeepEval incorporates the latest research to evaluate LLM outputs based on metrics such as G-Eval, hallucination, answer relevancy, RAGAS, etc., which uses LLMs and various other NLP models that runs &lt;strong&gt;locally on your machine&lt;/strong&gt; for evaluation.&lt;/p&gt; &#xA;&lt;p&gt;Whether your LLM applications are RAG pipelines, chatbots, AI agents, implemented via LangChain or LlamaIndex, DeepEval has you covered. With it, you can easily determine the optimal models, prompts, and architecture to improve your RAG pipeline, agentic workflows, prevent prompt drifting, or even transition from OpenAI to hosting your own Deepseek R1 with confidence.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT] Need a place for your DeepEval testing data to live üè°‚ù§Ô∏è? &lt;a href=&#34;https://confident-ai.com?utm_source=GitHub&#34;&gt;Sign up to the DeepEval platform&lt;/a&gt; to compare iterations of your LLM app, generate &amp;amp; share testing reports, and more.&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/confident-ai/deepeval/main/assets/demo.gif&#34; alt=&#34;Demo GIF&#34;&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Want to talk LLM evaluation, need help picking metrics, or just to say hi? &lt;a href=&#34;https://discord.com/invite/3SEyvpgu2f&#34;&gt;Come join our discord.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üî• Metrics and Features&lt;/h1&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;ü•≥ You can now share DeepEval&#39;s test results on the cloud directly on &lt;a href=&#34;https://confident-ai.com?utm_source=GitHub&#34;&gt;Confident AI&lt;/a&gt;&#39;s infrastructure&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Supports both end-to-end and component-level LLM evaluation.&lt;/li&gt; &#xA; &lt;li&gt;Large variety of ready-to-use LLM evaluation metrics (all with explanations) powered by &lt;strong&gt;ANY&lt;/strong&gt; LLM of your choice, statistical methods, or NLP models that runs &lt;strong&gt;locally on your machine&lt;/strong&gt;: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;G-Eval&lt;/li&gt; &#xA;   &lt;li&gt;DAG (&lt;a href=&#34;https://deepeval.com/docs/metrics-dag&#34;&gt;deep acyclic graph&lt;/a&gt;)&lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;RAG metrics:&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Answer Relevancy&lt;/li&gt; &#xA;     &lt;li&gt;Faithfulness&lt;/li&gt; &#xA;     &lt;li&gt;Contextual Recall&lt;/li&gt; &#xA;     &lt;li&gt;Contextual Precision&lt;/li&gt; &#xA;     &lt;li&gt;Contextual Relevancy&lt;/li&gt; &#xA;     &lt;li&gt;RAGAS&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Agentic metrics:&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Task Completion&lt;/li&gt; &#xA;     &lt;li&gt;Tool Correctness&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Others:&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Hallucination&lt;/li&gt; &#xA;     &lt;li&gt;Summarization&lt;/li&gt; &#xA;     &lt;li&gt;Bias&lt;/li&gt; &#xA;     &lt;li&gt;Toxicity&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;strong&gt;Conversational metrics:&lt;/strong&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Knowledge Retention&lt;/li&gt; &#xA;     &lt;li&gt;Conversation Completeness&lt;/li&gt; &#xA;     &lt;li&gt;Conversation Relevancy&lt;/li&gt; &#xA;     &lt;li&gt;Role Adherence&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;etc.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Build your own custom metrics that are automatically integrated with DeepEval&#39;s ecosystem.&lt;/li&gt; &#xA; &lt;li&gt;Generate synthetic datasets for evaluation.&lt;/li&gt; &#xA; &lt;li&gt;Integrates seamlessly with &lt;strong&gt;ANY&lt;/strong&gt; CI/CD environment.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deepeval.com/docs/red-teaming-introduction&#34;&gt;Red team your LLM application&lt;/a&gt; for 40+ safety vulnerabilities in a few lines of code, including: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Toxicity&lt;/li&gt; &#xA;   &lt;li&gt;Bias&lt;/li&gt; &#xA;   &lt;li&gt;SQL Injection&lt;/li&gt; &#xA;   &lt;li&gt;etc., using advanced 10+ attack enhancement strategies such as prompt injections.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Easily benchmark &lt;strong&gt;ANY&lt;/strong&gt; LLM on popular LLM benchmarks in &lt;a href=&#34;https://deepeval.com/docs/benchmarks-introduction?utm_source=GitHub&#34;&gt;under 10 lines of code.&lt;/a&gt;, which includes: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;MMLU&lt;/li&gt; &#xA;   &lt;li&gt;HellaSwag&lt;/li&gt; &#xA;   &lt;li&gt;DROP&lt;/li&gt; &#xA;   &lt;li&gt;BIG-Bench Hard&lt;/li&gt; &#xA;   &lt;li&gt;TruthfulQA&lt;/li&gt; &#xA;   &lt;li&gt;HumanEval&lt;/li&gt; &#xA;   &lt;li&gt;GSM8K&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://confident-ai.com?utm_source=GitHub&#34;&gt;100% integrated with Confident AI&lt;/a&gt; for the full evaluation lifecycle: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Curate/annotate evaluation datasets on the cloud&lt;/li&gt; &#xA;   &lt;li&gt;Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best&lt;/li&gt; &#xA;   &lt;li&gt;Fine-tune metrics for custom results&lt;/li&gt; &#xA;   &lt;li&gt;Debug evaluation results via LLM traces&lt;/li&gt; &#xA;   &lt;li&gt;Monitor &amp;amp; evaluate LLM responses in product to improve datasets with real-world data&lt;/li&gt; &#xA;   &lt;li&gt;Repeat until perfection&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] Confident AI is the DeepEval platform. Create an account &lt;a href=&#34;https://app.confident-ai.com?utm_source=GitHub&#34;&gt;here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üîå Integrations&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ü¶Ñ LlamaIndex, to &lt;a href=&#34;https://www.deepeval.com/integrations/frameworks/llamaindex?utm_source=GitHub&#34;&gt;&lt;strong&gt;unit test RAG applications in CI/CD&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ü§ó Hugging Face, to &lt;a href=&#34;https://www.deepeval.com/integrations/frameworks/huggingface?utm_source=GitHub&#34;&gt;&lt;strong&gt;enable real-time evaluations during LLM fine-tuning&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;üöÄ QuickStart&lt;/h1&gt; &#xA;&lt;p&gt;Let&#39;s pretend your LLM application is a RAG based customer support chatbot; here&#39;s how DeepEval can help test what you&#39;ve built.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -U deepeval&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Create an account (highly recommended)&lt;/h2&gt; &#xA;&lt;p&gt;Using the &lt;code&gt;deepeval&lt;/code&gt; platform will allow you to generate sharable testing reports on the cloud. It is free, takes no additional code to setup, and we highly recommend giving it a try.&lt;/p&gt; &#xA;&lt;p&gt;To login, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;deepeval login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Follow the instructions in the CLI to create an account, copy your API key, and paste it into the CLI. All test cases will automatically be logged (find more information on data privacy &lt;a href=&#34;https://deepeval.com/docs/data-privacy?utm_source=GitHub&#34;&gt;here&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;h2&gt;Writing your first test case&lt;/h2&gt; &#xA;&lt;p&gt;Create a test file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;touch test_chatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open &lt;code&gt;test_chatbot.py&lt;/code&gt; and write your first test case to run an &lt;strong&gt;end-to-end&lt;/strong&gt; evaluation using DeepEval, which treats your LLM app as a black-box:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pytest&#xA;from deepeval import assert_test&#xA;from deepeval.metrics import GEval&#xA;from deepeval.test_case import LLMTestCase, LLMTestCaseParams&#xA;&#xA;def test_case():&#xA;    correctness_metric = GEval(&#xA;        name=&#34;Correctness&#34;,&#xA;        criteria=&#34;Determine if the &#39;actual output&#39; is correct based on the &#39;expected output&#39;.&#34;,&#xA;        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],&#xA;        threshold=0.5&#xA;    )&#xA;    test_case = LLMTestCase(&#xA;        input=&#34;What if these shoes don&#39;t fit?&#34;,&#xA;        # Replace this with the actual output from your LLM application&#xA;        actual_output=&#34;You have 30 days to get a full refund at no extra cost.&#34;,&#xA;        expected_output=&#34;We offer a 30-day full refund at no extra costs.&#34;,&#xA;        retrieval_context=[&#34;All customers are eligible for a 30 day full refund at no extra costs.&#34;]&#xA;    )&#xA;    assert_test(test_case, [correctness_metric])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set your &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; as an environment variable (you can also evaluate using your own custom model, for more details visit &lt;a href=&#34;https://deepeval.com/docs/metrics-introduction#using-a-custom-llm?utm_source=GitHub&#34;&gt;this part of our docs&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=&#34;...&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And finally, run &lt;code&gt;test_chatbot.py&lt;/code&gt; in the CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;deepeval test run test_chatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Congratulations! Your test case should have passed ‚úÖ&lt;/strong&gt; Let&#39;s breakdown what happened.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The variable &lt;code&gt;input&lt;/code&gt; mimics a user input, and &lt;code&gt;actual_output&lt;/code&gt; is a placeholder for what your application&#39;s supposed to output based on this input.&lt;/li&gt; &#xA; &lt;li&gt;The variable &lt;code&gt;expected_output&lt;/code&gt; represents the ideal answer for a given &lt;code&gt;input&lt;/code&gt;, and &lt;a href=&#34;https://deepeval.com/docs/metrics-llm-evals&#34;&gt;&lt;code&gt;GEval&lt;/code&gt;&lt;/a&gt; is a research-backed metric provided by &lt;code&gt;deepeval&lt;/code&gt; for you to evaluate your LLM output&#39;s on any custom custom with human-like accuracy.&lt;/li&gt; &#xA; &lt;li&gt;In this example, the metric &lt;code&gt;criteria&lt;/code&gt; is correctness of the &lt;code&gt;actual_output&lt;/code&gt; based on the provided &lt;code&gt;expected_output&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;All metric scores range from 0 - 1, which the &lt;code&gt;threshold=0.5&lt;/code&gt; threshold ultimately determines if your test have passed or not.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://deepeval.com/docs/getting-started?utm_source=GitHub&#34;&gt;Read our documentation&lt;/a&gt; for more information on more options to run end-to-end evaluation, how to use additional metrics, create your own custom metrics, and tutorials on how to integrate with other tools like LangChain and LlamaIndex.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Evaluating Nested Components&lt;/h2&gt; &#xA;&lt;p&gt;If you wish to evaluate individual components within your LLM app, you need to run &lt;strong&gt;component-level&lt;/strong&gt; evals - a powerful way to evaluate any component within an LLM system.&lt;/p&gt; &#xA;&lt;p&gt;Simply trace &#34;components&#34; such as LLM calls, retrievers, tool calls, and agents within your LLM application using the &lt;code&gt;@observe&lt;/code&gt; decorator to apply metrics on a component-level. Tracing with &lt;code&gt;deepeval&lt;/code&gt; is non-instrusive (learn more &lt;a href=&#34;https://deepeval.com/docs/evaluation-llm-tracing#dont-be-worried-about-tracing&#34;&gt;here&lt;/a&gt;) and helps you avoid rewriting your codebase just for evals:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepeval.tracing import observe, update_current_span&#xA;from deepeval.test_case import LLMTestCase&#xA;from deepeval.dataset import Golden&#xA;from deepeval.metrics import GEval&#xA;from deepeval import evaluate&#xA;&#xA;correctness = GEval(name=&#34;Correctness&#34;, criteria=&#34;Determine if the &#39;actual output&#39; is correct based on the &#39;expected output&#39;.&#34;, evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT])&#xA;&#xA;@observe(metrics=[correctness])&#xA;def inner_component():&#xA;    # Component can be anything from an LLM call, retrieval, agent, tool use, etc.&#xA;    update_current_span(test_case=LLMTestCase(input=&#34;...&#34;, actual_output=&#34;...&#34;))&#xA;    return&#xA;&#xA;@observe&#xA;def llm_app(input: str):&#xA;    inner_component()&#xA;    return&#xA;&#xA;evaluate(observed_callback=llm_app, goldens=[Golden(input=&#34;Hi!&#34;)])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can learn everything about component-level evaluations &lt;a href=&#34;https://www.deepeval.com/docs/evaluation-component-level-llm-evals&#34;&gt;here.&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Evaluating Without Pytest Integration&lt;/h2&gt; &#xA;&lt;p&gt;Alternatively, you can evaluate without Pytest, which is more suited for a notebook environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepeval import evaluate&#xA;from deepeval.metrics import AnswerRelevancyMetric&#xA;from deepeval.test_case import LLMTestCase&#xA;&#xA;answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)&#xA;test_case = LLMTestCase(&#xA;    input=&#34;What if these shoes don&#39;t fit?&#34;,&#xA;    # Replace this with the actual output from your LLM application&#xA;    actual_output=&#34;We offer a 30-day full refund at no extra costs.&#34;,&#xA;    retrieval_context=[&#34;All customers are eligible for a 30 day full refund at no extra costs.&#34;]&#xA;)&#xA;evaluate([test_case], [answer_relevancy_metric])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Using Standalone Metrics&lt;/h2&gt; &#xA;&lt;p&gt;DeepEval is extremely modular, making it easy for anyone to use any of our metrics. Continuing from the previous example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepeval.metrics import AnswerRelevancyMetric&#xA;from deepeval.test_case import LLMTestCase&#xA;&#xA;answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.7)&#xA;test_case = LLMTestCase(&#xA;    input=&#34;What if these shoes don&#39;t fit?&#34;,&#xA;    # Replace this with the actual output from your LLM application&#xA;    actual_output=&#34;We offer a 30-day full refund at no extra costs.&#34;,&#xA;    retrieval_context=[&#34;All customers are eligible for a 30 day full refund at no extra costs.&#34;]&#xA;)&#xA;&#xA;answer_relevancy_metric.measure(test_case)&#xA;print(answer_relevancy_metric.score)&#xA;# All metrics also offer an explanation&#xA;print(answer_relevancy_metric.reason)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that some metrics are for RAG pipelines, while others are for fine-tuning. Make sure to use our docs to pick the right one for your use case.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluating a Dataset / Test Cases in Bulk&lt;/h2&gt; &#xA;&lt;p&gt;In DeepEval, a dataset is simply a collection of test cases. Here is how you can evaluate these in bulk:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import pytest&#xA;from deepeval import assert_test&#xA;from deepeval.metrics import HallucinationMetric, AnswerRelevancyMetric&#xA;from deepeval.test_case import LLMTestCase&#xA;from deepeval.dataset import EvaluationDataset&#xA;&#xA;first_test_case = LLMTestCase(input=&#34;...&#34;, actual_output=&#34;...&#34;, context=[&#34;...&#34;])&#xA;second_test_case = LLMTestCase(input=&#34;...&#34;, actual_output=&#34;...&#34;, context=[&#34;...&#34;])&#xA;&#xA;dataset = EvaluationDataset(test_cases=[first_test_case, second_test_case])&#xA;&#xA;@pytest.mark.parametrize(&#xA;    &#34;test_case&#34;,&#xA;    dataset,&#xA;)&#xA;def test_customer_chatbot(test_case: LLMTestCase):&#xA;    hallucination_metric = HallucinationMetric(threshold=0.3)&#xA;    answer_relevancy_metric = AnswerRelevancyMetric(threshold=0.5)&#xA;    assert_test(test_case, [hallucination_metric, answer_relevancy_metric])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Run this in the CLI, you can also add an optional -n flag to run tests in parallel&#xA;deepeval test run test_&amp;lt;filename&amp;gt;.py -n 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Alternatively, although we recommend using &lt;code&gt;deepeval test run&lt;/code&gt;, you can evaluate a dataset/test cases without using our Pytest integration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from deepeval import evaluate&#xA;...&#xA;&#xA;evaluate(dataset, [answer_relevancy_metric])&#xA;# or&#xA;dataset.evaluate([answer_relevancy_metric])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;LLM Evaluation With Confident AI&lt;/h1&gt; &#xA;&lt;p&gt;The correct LLM evaluation lifecycle is only achievable with &lt;a href=&#34;https://confident-ai.com?utm_source=Github&#34;&gt;the DeepEval platform&lt;/a&gt;. It allows you to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Curate/annotate evaluation datasets on the cloud&lt;/li&gt; &#xA; &lt;li&gt;Benchmark LLM app using dataset, and compare with previous iterations to experiment which models/prompts works best&lt;/li&gt; &#xA; &lt;li&gt;Fine-tune metrics for custom results&lt;/li&gt; &#xA; &lt;li&gt;Debug evaluation results via LLM traces&lt;/li&gt; &#xA; &lt;li&gt;Monitor &amp;amp; evaluate LLM responses in product to improve datasets with real-world data&lt;/li&gt; &#xA; &lt;li&gt;Repeat until perfection&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Everything on Confident AI, including how to use Confident is available &lt;a href=&#34;https://documentation.confident-ai.com?utm_source=GitHub&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;To begin, login from the CLI:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deepeval login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Follow the instructions to log in, create your account, and paste your API key into the CLI.&lt;/p&gt; &#xA;&lt;p&gt;Now, run your test file again:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;deepeval test run test_chatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You should see a link displayed in the CLI once the test has finished running. Paste it into your browser to view the results!&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/confident-ai/deepeval/main/assets/demo.gif&#34; alt=&#34;Demo GIF&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Please read &lt;a href=&#34;https://github.com/confident-ai/deepeval/raw/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for details on our code of conduct, and the process for submitting pull requests to us.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Roadmap&lt;/h1&gt; &#xA;&lt;p&gt;Features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integration with Confident AI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement G-Eval&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement RAG metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Implement Conversational metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Evaluation Dataset Creation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Red-Teaming&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; DAG custom metrics&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Guardrails&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Authors&lt;/h1&gt; &#xA;&lt;p&gt;Built by the founders of Confident AI. Contact &lt;a href=&#34;mailto:jeffreyip@confident-ai.com&#34;&gt;jeffreyip@confident-ai.com&lt;/a&gt; for all enquiries.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;DeepEval is licensed under Apache 2.0 - see the &lt;a href=&#34;https://github.com/confident-ai/deepeval/raw/main/LICENSE.md&#34;&gt;LICENSE.md&lt;/a&gt; file for details.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>FareedKhan-dev/all-rag-techniques</title>
    <updated>2025-06-13T01:29:14Z</updated>
    <id>tag:github.com,2025-06-13:/FareedKhan-dev/all-rag-techniques</id>
    <link href="https://github.com/FareedKhan-dev/all-rag-techniques" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Implementation of all RAG techniques in a simpler way&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;All RAG Techniques: A Simpler, Hands-On Approach ‚ú®&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.python.org/downloads/release/python-370/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.7+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.7+&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://cloud.nebius.ai/services/llm-embedding&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Nebius%20AI-API-brightgreen&#34; alt=&#34;Nebius AI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://openai.com/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/OpenAI-API-lightgrey&#34; alt=&#34;OpenAI&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://medium.com/@fareedkhandev/testing-every-rag-technique-to-find-the-best-094d166af27f&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Medium-Blog-black?logo=medium&#34; alt=&#34;Medium&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository takes a clear, hands-on approach to &lt;strong&gt;Retrieval-Augmented Generation (RAG)&lt;/strong&gt;, breaking down advanced techniques into straightforward, understandable implementations. Instead of relying on frameworks like &lt;code&gt;LangChain&lt;/code&gt; or &lt;code&gt;FAISS&lt;/code&gt;, everything here is built using familiar Python libraries &lt;code&gt;openai&lt;/code&gt;, &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;matplotlib&lt;/code&gt;, and a few others.&lt;/p&gt; &#xA;&lt;p&gt;The goal is simple: provide code that is readable, modifiable, and educational. By focusing on the fundamentals, this project helps demystify RAG and makes it easier to understand how it really works.&lt;/p&gt; &#xA;&lt;h2&gt;Update: üì¢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;(12-May-2025) Added a new notebook on how to handle big data using Knowledge Graphs.&lt;/li&gt; &#xA; &lt;li&gt;(27-April-2025) Added a new notebook which finds best RAG technique for a given query (Simple RAG + Reranker + Query Rewrite).&lt;/li&gt; &#xA; &lt;li&gt;(20-Mar-2025) Added a new notebook on RAG with Reinforcement Learning.&lt;/li&gt; &#xA; &lt;li&gt;(07-Mar-2025) Added 20 RAG techniques to the repository.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üöÄ What&#39;s Inside?&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains a collection of Jupyter Notebooks, each focusing on a specific RAG technique. Each notebook provides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A concise explanation of the technique.&lt;/li&gt; &#xA; &lt;li&gt;A step-by-step implementation from scratch.&lt;/li&gt; &#xA; &lt;li&gt;Clear code examples with inline comments.&lt;/li&gt; &#xA; &lt;li&gt;Evaluations and comparisons to demonstrate the technique&#39;s effectiveness.&lt;/li&gt; &#xA; &lt;li&gt;Visualization to visualize the results.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Here&#39;s a glimpse of the techniques covered:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Notebook&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/01_simple_rag.ipynb&#34;&gt;1. Simple RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;A basic RAG implementation. A great starting point!&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/02_semantic_chunking.ipynb&#34;&gt;2. Semantic Chunking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Splits text based on semantic similarity for more meaningful chunks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/03_chunk_size_selector.ipynb&#34;&gt;3. Chunk Size Selector&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Explores the impact of different chunk sizes on retrieval performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/04_context_enriched_rag.ipynb&#34;&gt;4. Context Enriched RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Retrieves neighboring chunks to provide more context.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/05_contextual_chunk_headers_rag.ipynb&#34;&gt;5. Contextual Chunk Headers&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Prepends descriptive headers to each chunk before embedding.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/06_doc_augmentation_rag.ipynb&#34;&gt;6. Document Augmentation RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Generates questions from text chunks to augment the retrieval process.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/07_query_transform.ipynb&#34;&gt;7. Query Transform&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Rewrites, expands, or decomposes queries to improve retrieval. Includes &lt;strong&gt;Step-back Prompting&lt;/strong&gt; and &lt;strong&gt;Sub-query Decomposition&lt;/strong&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/08_reranker.ipynb&#34;&gt;8. Reranker&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Re-ranks initially retrieved results using an LLM for better relevance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/09_rse.ipynb&#34;&gt;9. RSE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Relevant Segment Extraction: Identifies and reconstructs continuous segments of text, preserving context.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/10_contextual_compression.ipynb&#34;&gt;10. Contextual Compression&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implements contextual compression to filter and compress retrieved chunks, maximizing relevant information.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/11_feedback_loop_rag.ipynb&#34;&gt;11. Feedback Loop RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Incorporates user feedback to learn and improve RAG system over time.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/12_adaptive_rag.ipynb&#34;&gt;12. Adaptive RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Dynamically selects the best retrieval strategy based on query type.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/13_self_rag.ipynb&#34;&gt;13. Self RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Implements Self-RAG, dynamically decides when and how to retrieve, evaluates relevance, and assesses support and utility.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/14_proposition_chunking.ipynb&#34;&gt;14. Proposition Chunking&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Breaks down documents into atomic, factual statements for precise retrieval.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/15_multimodel_rag.ipynb&#34;&gt;15. Multimodel RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Combines text and images for retrieval, generating captions for images using LLaVA.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/16_fusion_rag.ipynb&#34;&gt;16. Fusion RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Combines vector search with keyword-based (BM25) retrieval for improved results.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/17_graph_rag.ipynb&#34;&gt;17. Graph RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Organizes knowledge as a graph, enabling traversal of related concepts.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/18_hierarchy_rag.ipynb&#34;&gt;18. Hierarchy RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Builds hierarchical indices (summaries + detailed chunks) for efficient retrieval.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/19_HyDE_rag.ipynb&#34;&gt;19. HyDE RAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Uses Hypothetical Document Embeddings to improve semantic matching.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/20_crag.ipynb&#34;&gt;20. CRAG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Corrective RAG: Dynamically evaluates retrieval quality and uses web search as a fallback.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/21_rag_with_rl.ipynb&#34;&gt;21. Rag with RL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Maximize the reward of the RAG model using Reinforcement Learning.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/best_rag_finder.ipynb&#34;&gt;Best RAG Finder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Finds the best RAG technique for a given query using Simple RAG + Reranker + Query Rewrite.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/FareedKhan-dev/all-rag-techniques/main/22_Big_data_with_KG.ipynb&#34;&gt;22. Big Data with Knowledge Graphs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Handles large datasets using Knowledge Graphs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üóÇÔ∏è Repository Structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;fareedkhan-dev-all-rag-techniques/&#xA;‚îú‚îÄ‚îÄ README.md                          &amp;lt;- You are here!&#xA;‚îú‚îÄ‚îÄ 01_simple_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 02_semantic_chunking.ipynb&#xA;‚îú‚îÄ‚îÄ 03_chunk_size_selector.ipynb&#xA;‚îú‚îÄ‚îÄ 04_context_enriched_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 05_contextual_chunk_headers_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 06_doc_augmentation_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 07_query_transform.ipynb&#xA;‚îú‚îÄ‚îÄ 08_reranker.ipynb&#xA;‚îú‚îÄ‚îÄ 09_rse.ipynb&#xA;‚îú‚îÄ‚îÄ 10_contextual_compression.ipynb&#xA;‚îú‚îÄ‚îÄ 11_feedback_loop_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 12_adaptive_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 13_self_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 14_proposition_chunking.ipynb&#xA;‚îú‚îÄ‚îÄ 15_multimodel_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 16_fusion_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 17_graph_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 18_hierarchy_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 19_HyDE_rag.ipynb&#xA;‚îú‚îÄ‚îÄ 20_crag.ipynb&#xA;‚îú‚îÄ‚îÄ 21_rag_with_rl.ipynb&#xA;‚îú‚îÄ‚îÄ 22_big_data_with_KG.ipynb&#xA;‚îú‚îÄ‚îÄ best_rag_finder.ipynb&#xA;‚îú‚îÄ‚îÄ requirements.txt                   &amp;lt;- Python dependencies&#xA;‚îî‚îÄ‚îÄ data/&#xA;    ‚îî‚îÄ‚îÄ val.json                       &amp;lt;- Sample validation data (queries and answers)&#xA;    ‚îî‚îÄ‚îÄ AI_Information.pdf             &amp;lt;- A sample PDF document for testing.&#xA;    ‚îî‚îÄ‚îÄ attention_is_all_you_need.pdf  &amp;lt;- A sample PDF document for testing (for Multi-Modal RAG).&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Getting Started&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone the repository:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/FareedKhan-dev/all-rag-techniques.git&#xA;cd all-rag-techniques&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Install dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set up your OpenAI API key:&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Obtain an API key from &lt;a href=&#34;https://studio.nebius.com/&#34;&gt;Nebius AI&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Set the API key as an environment variable:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#39;YOUR_NEBIUS_AI_API_KEY&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;setx OPENAI_API_KEY &#34;YOUR_NEBIUS_AI_API_KEY&#34;  # On Windows&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or, within your Python script/notebook:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;YOUR_NEBIUS_AI_API_KEY&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Run the notebooks:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Open any of the Jupyter Notebooks (&lt;code&gt;.ipynb&lt;/code&gt; files) using Jupyter Notebook or JupyterLab. Each notebook is self-contained and can be run independently. The notebooks are designed to be executed sequentially within each file.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The &lt;code&gt;data/AI_Information.pdf&lt;/code&gt; file provides a sample document for testing. You can replace it with your own PDF. The &lt;code&gt;data/val.json&lt;/code&gt; file contains sample queries and ideal answers for evaluation. The &#39;attention_is_all_you_need.pdf&#39; is for testing Multi-Modal RAG Notebook.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üí° Core Concepts&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Embeddings:&lt;/strong&gt; Numerical representations of text that capture semantic meaning. We use Nebius AI&#39;s embedding API and, in many notebooks, also the &lt;code&gt;BAAI/bge-en-icl&lt;/code&gt; embedding model.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Vector Store:&lt;/strong&gt; A simple database to store and search embeddings. We create our own &lt;code&gt;SimpleVectorStore&lt;/code&gt; class using NumPy for efficient similarity calculations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Cosine Similarity:&lt;/strong&gt; A measure of similarity between two vectors. Higher values indicate greater similarity.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Chunking:&lt;/strong&gt; Dividing text into smaller, manageable pieces. We explore various chunking strategies.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Retrieval:&lt;/strong&gt; The process of finding the most relevant text chunks for a given query.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Generation:&lt;/strong&gt; Using a Large Language Model (LLM) to create a response based on the retrieved context and the user&#39;s query. We use the &lt;code&gt;meta-llama/Llama-3.2-3B-Instruct&lt;/code&gt; model via Nebius AI&#39;s API.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Evaluation:&lt;/strong&gt; Assessing the quality of the RAG system&#39;s responses, often by comparing them to a reference answer or using an LLM to score relevance.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü§ù Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions are welcome!&lt;/p&gt;</summary>
  </entry>
</feed>