<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-10T01:28:34Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pytorch/torchtune</title>
    <updated>2025-01-10T01:28:34Z</updated>
    <id>tag:github.com,2025-01-10:/pytorch/torchtune</id>
    <link href="https://github.com/pytorch/torchtune" rel="alternate"></link>
    <summary type="html">&lt;p&gt;PyTorch native post-training library&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;torchtune&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml&#34;&gt;&lt;img src=&#34;https://github.com/pytorch/torchtune/actions/workflows/unit_test.yaml/badge.svg?branch=main&#34; alt=&#34;Unit Test&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://github.com/pytorch/torchtune/actions/workflows/recipe_test.yaml/badge.svg?sanitize=true&#34; alt=&#34;Recipe Integration Test&#34;&gt; &lt;a href=&#34;https://discord.gg/4Xsdn8Rr9Q&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/4Xsdn8Rr9Q?style=flat&#34; alt=&#34;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#introduction&#34;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#installation&#34;&gt;&lt;strong&gt;Installation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#get-started&#34;&gt;&lt;strong&gt;Get Started&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://pytorch.org/torchtune/main/index.html&#34;&gt;&lt;strong&gt;Documentation&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#community&#34;&gt;&lt;strong&gt;Community&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#license&#34;&gt;&lt;strong&gt;License&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#citing-torchtune&#34;&gt;&lt;strong&gt;Citing torchtune&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;üì£ Recent updates üì£&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;December 2024&lt;/em&gt;: torchtune now supports &lt;strong&gt;Llama 3.3 70B&lt;/strong&gt;! Try it out by following our installation instructions &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#Installation&#34;&gt;here&lt;/a&gt;, then run any of the configs &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_3&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;November 2024&lt;/em&gt;: torchtune has released &lt;a href=&#34;https://github.com/pytorch/torchtune/releases/tag/v0.4.0&#34;&gt;v0.4.0&lt;/a&gt; which includes stable support for exciting features like activation offloading and multimodal QLoRA&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;November 2024&lt;/em&gt;: torchtune has added &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/gemma2&#34;&gt;Gemma2&lt;/a&gt; to its models!&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;October 2024&lt;/em&gt;: torchtune added support for Qwen2.5 models - find the recipes &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/qwen2_5/&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;em&gt;September 2024&lt;/em&gt;: torchtune has support for &lt;strong&gt;Llama 3.2 11B Vision&lt;/strong&gt;, &lt;strong&gt;Llama 3.2 3B&lt;/strong&gt;, and &lt;strong&gt;Llama 3.2 1B&lt;/strong&gt; models! Try them out by following our installation instructions &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/#Installation&#34;&gt;here&lt;/a&gt;, then run any of the text configs &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_2&#34;&gt;here&lt;/a&gt; or vision configs &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_2_vision&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;torchtune is a PyTorch library for easily authoring, finetuning and experimenting with LLMs.&lt;/p&gt; &#xA;&lt;p&gt;torchtune provides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PyTorch implementations of popular LLMs from Llama, Gemma, Mistral, Phi, and Qwen model families&lt;/li&gt; &#xA; &lt;li&gt;Hackable training recipes for full finetuning, LoRA, QLoRA, DPO, PPO, QAT, knowledge distillation, and more&lt;/li&gt; &#xA; &lt;li&gt;Out-of-the-box memory efficiency, performance improvements, and scaling with the latest PyTorch APIs&lt;/li&gt; &#xA; &lt;li&gt;YAML configs for easily configuring training, evaluation, quantization or inference recipes&lt;/li&gt; &#xA; &lt;li&gt;Built-in support for many popular dataset formats and prompt templates&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Models&lt;/h3&gt; &#xA;&lt;p&gt;torchtune currently supports the following models.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Sizes&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3&#34;&gt;Llama3.3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;70B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/llama3_3/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_3/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2#-llama-3.2-vision-models-(11b/90b)-&#34;&gt;Llama3.2-Vision&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;11B, 90B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/llama3_2_vision/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_2_vision/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2&#34;&gt;Llama3.2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1B, 3B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/llama3_2/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_2/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1&#34;&gt;Llama3.1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B, 70B, 405B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/llama3_1/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_1/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama3&#34;&gt;Llama3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8B, 70B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/llama3/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama2/&#34;&gt;Llama2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 70B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/llama2/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/blog/code-llama-large-language-model-coding/&#34;&gt;Code-Llama2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B, 13B, 70B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/code_llama2/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/code_llama2/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai&#34;&gt;Mistral&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/mistral/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/mistral/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/collections/google/gemma-release-65d5efbccdbb8c4202ec078b&#34;&gt;Gemma&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B, 7B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/gemma/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/gemma/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/docs/transformers/main/en/model_doc/gemma2&#34;&gt;Gemma2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2B, 9B, 27B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/gemma2/_model_builders.py&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/gemma2/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3&#34;&gt;Microsoft Phi3&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Mini [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/phi3/&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/phi3/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2/&#34;&gt;Qwen2&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.5B, 1.5B, 7B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/qwen2/&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/qwen2/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5/&#34;&gt;Qwen2.5&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.5B, 1.5B, 3B, 7B, 14B, 32B, 72B [&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/qwen2_5/&#34;&gt;models&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/qwen2_5/&#34;&gt;configs&lt;/a&gt;]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;We&#39;re always adding new models, but feel free to &lt;a href=&#34;https://github.com/pytorch/torchtune/issues/new&#34;&gt;file an issue&lt;/a&gt; if there&#39;s a new one you would like to see in torchtune.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Finetuning recipes&lt;/h3&gt; &#xA;&lt;p&gt;torchtune provides the following finetuning recipes for training on one or more devices.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Finetuning Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Devices&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Recipe&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Example Config(s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Full Finetuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1-8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/full_finetune_single_device.py&#34;&gt;full_finetune_single_device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/full_finetune_distributed.py&#34;&gt;full_finetune_distributed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_1/8B_full_single_device.yaml&#34;&gt;Llama3.1 8B single-device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_1/70B_full.yaml&#34;&gt;Llama 3.1 70B distributed&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LoRA Finetuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1-8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_finetune_single_device.py&#34;&gt;lora_finetune_single_device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_finetune_distributed.py&#34;&gt;lora_finetune_distributed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/qwen2/0.5B_lora_single_device.yaml&#34;&gt;Qwen2 0.5B single-device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/gemma/7B_lora.yaml&#34;&gt;Gemma 7B distributed&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;QLoRA Finetuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1-8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_finetune_single_device.py&#34;&gt;lora_finetune_single_device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_finetune_distributed.py&#34;&gt;lora_finetune_distributed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/phi3/mini_qlora_single_device.yaml&#34;&gt;Phi3 Mini single-device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3_1/405B_qlora.yaml&#34;&gt;Llama 3.1 405B distributed&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;DoRA/QDoRA Finetuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1-8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_finetune_single_device.py&#34;&gt;lora_finetune_single_device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_finetune_distributed.py&#34;&gt;lora_finetune_distributed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/8B_qdora_single_device.yaml&#34;&gt;Llama3 8B QDoRA single-device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/8B_dora.yaml&#34;&gt;Llama3 8B DoRA distributed&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Quantization-Aware Training&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2-8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/qat_distributed.py&#34;&gt;qat_distributed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/8B_qat_full.yaml&#34;&gt;Llama3 8B QAT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Quantization-Aware Training and LoRA Finetuning&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2-8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/qat_lora_finetune_distributed.py&#34;&gt;qat_lora_finetune_distributed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama3/8B_qat_lora.yaml&#34;&gt;Llama3 8B QAT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Direct Preference Optimization&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1-8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_dpo_single_device.py&#34;&gt;lora_dpo_single_device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_dpo_distributed.py&#34;&gt;lora_dpo_distributed&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_lora_dpo_single_device.yaml&#34;&gt;Llama2 7B single-device&lt;/a&gt; &lt;br&gt; &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_lora_dpo.yaml&#34;&gt;Llama2 7B distributed&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Proximal Policy Optimization&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/ppo_full_finetune_single_device.py&#34;&gt;ppo_full_finetune_single_device&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/mistral/7B_full_ppo_low_memory.yaml&#34;&gt;Mistral 7B&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Knowledge Distillation&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/knowledge_distillation_single_device.py&#34;&gt;knowledge_distillation_single_device&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/qwen2/knowledge_distillation_single_device.yaml&#34;&gt;Qwen2 1.5B -&amp;gt; 0.5B&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The above configs are just examples to get you started. If you see a model above not listed here, we likely still support it. If you&#39;re unsure whether something is supported, please open an issue on the repo.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Memory and training speed&lt;/h3&gt; &#xA;&lt;p&gt;Below is an example of the memory requirements and training speed for different Llama 3.1 models.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] For ease of comparison, all the below numbers are provided for batch size 2 (without gradient accumulation), a dataset packed to sequence length 2048, and torch compile enabled.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;If you are interested in running on different hardware or with different models, check out our documentation on memory optimizations &lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html&#34;&gt;here&lt;/a&gt; to find the right setup for you.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Model&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Finetuning Method&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Runnable On&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Peak Memory per GPU&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tokens/sec *&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.1 8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Full finetune&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1x 4090&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;18.9 GiB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1650&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.1 8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Full finetune&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1x A6000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;37.4 GiB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2579&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.1 8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1x 4090&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;16.2 GiB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3083&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.1 8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1x A6000&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;30.3 GiB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4699&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.1 8B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1x 4090&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7.4 GiB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2413&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.1 70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Full finetune&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x A100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;13.9 GiB **&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;1568&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.1 70B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;LoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x A100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;27.6 GiB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;3497&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Llama 3.1 405B&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;QLoRA&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8x A100&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;44.8 GB&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;653&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*= Measured over one full training epoch&lt;/p&gt; &#xA;&lt;p&gt;**= Uses CPU offload with fused optimizer&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Optimization flags&lt;/h3&gt; &#xA;&lt;p&gt;torchtune exposes a number of levers for memory efficiency and performance. The table below demonstrates the effects of applying some of these techniques sequentially to the Llama 3.2 3B model. Each technique is added on top of the previous one, except for LoRA and QLoRA, which do not use &lt;code&gt;optimizer_in_bwd&lt;/code&gt; or &lt;code&gt;AdamW8bit&lt;/code&gt; optimizer.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Baseline:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Llama 3.2 3B&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Batch size:&lt;/strong&gt; 2&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Max seq len:&lt;/strong&gt; 4096&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Precision:&lt;/strong&gt; bf16&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; A100&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Recipe:&lt;/strong&gt; full_finetune_single_device&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Technique&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Peak Memory Active (GiB)&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;% Change Memory vs Previous&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Tokens Per Second&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;% Change Tokens/sec vs Previous&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;Baseline&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;25.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;2091&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pytorch.org/torchtune/main/basics/packing.html&#34;&gt;+ Packed Dataset&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;60.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;+135.16%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7075&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;+238.40%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html&#34;&gt;+ Compile&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;51.0&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-14.93%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8998&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;+27.18%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pytorch.org/torchtune/main/generated/torchtune.modules.loss.CEWithChunkedOutputLoss.html&#34;&gt;+ Chunked Cross Entropy&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;42.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-15.83%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;9174&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;+1.96%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#activation-checkpointing&#34;&gt;+ Activation Checkpointing&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;24.9&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-41.93%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7210&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-21.41%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#fusing-optimizer-step-into-backward-pass&#34;&gt;+ Fuse optimizer step into backward&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;23.1&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-7.29%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7309&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;+1.38%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#activation-offloading&#34;&gt;+ Activation Offloading&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;21.8&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-5.48%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;7301&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-0.11%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#lower-precision-optimizers&#34;&gt;+ 8-bit AdamW&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;17.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-19.63%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;6960&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-4.67%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#glossary-lora&#34;&gt;LoRA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8.5&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-51.61%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8210&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;+17.96%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/memory_optimizations.html#quantized-low-rank-adaptation-qlora&#34;&gt;QLoRA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;4.6&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-45.71%&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;8035&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;-2.13%&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;The final row in the table vs baseline + Packed Dataset uses &lt;strong&gt;81.9%&lt;/strong&gt; less memory with a &lt;strong&gt;284.3%&lt;/strong&gt; increase in tokens per second. It can be run via the command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;tune run lora_finetune_single_device --config llama3_2/3B_qlora_single_device \&#xA;dataset.packed=True \&#xA;compile=True \&#xA;loss=torchtune.modules.loss.CEWithChunkedOutputLoss \&#xA;enable_activation_checkpointing=True \&#xA;optimizer_in_bwd=False \&#xA;enable_activation_offloading=True \&#xA;optimizer=torch.optim.AdamW \&#xA;tokenizer.max_seq_len=4096 \&#xA;gradient_accumulation_steps=1 \&#xA;epochs=1 \&#xA;batch_size=2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;torchtune is tested with the latest stable PyTorch release as well as the preview nightly version. torchtune leverages torchvision for finetuning multimodal LLMs and torchao for the latest in quantization techniques; you should install these as well.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Install stable release&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install stable PyTorch, torchvision, torchao stable releases&#xA;pip install torch torchvision torchao&#xA;pip install torchtune&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Install nightly release&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Install PyTorch, torchvision, torchao nightlies&#xA;pip install --pre --upgrade torch torchvision torchao --index-url https://download.pytorch.org/whl/nightly/cu126 # full options are cpu/cu118/cu121/cu124/cu126&#xA;pip install --pre --upgrade torchtune --extra-index-url https://download.pytorch.org/whl/nightly/cpu&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also check out our &lt;a href=&#34;https://pytorch.org/torchtune/main/install.html&#34;&gt;install documentation&lt;/a&gt; for more information, including installing torchtune from source.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;To confirm that the package is installed correctly, you can run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;And should see the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;usage: tune [-h] {ls,cp,download,run,validate} ...&#xA;&#xA;Welcome to the torchtune CLI!&#xA;&#xA;options:&#xA;  -h, --help            show this help message and exit&#xA;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;To get started with torchtune, see our &lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/first_finetune_tutorial.html&#34;&gt;First Finetune Tutorial&lt;/a&gt;. Our &lt;a href=&#34;https://pytorch.org/torchtune/main/tutorials/e2e_flow.html&#34;&gt;End-to-End Workflow Tutorial&lt;/a&gt; will show you how to evaluate, quantize and run inference with a Llama model. The rest of this section will provide a quick overview of these steps with Llama3.1.&lt;/p&gt; &#xA;&lt;h3&gt;Downloading a model&lt;/h3&gt; &#xA;&lt;p&gt;Follow the instructions on the official &lt;a href=&#34;https://huggingface.co/meta-llama&#34;&gt;&lt;code&gt;meta-llama&lt;/code&gt;&lt;/a&gt; repository to ensure you have access to the official Llama model weights. Once you have confirmed access, you can run the following command to download the weights to your local machine. This will also download the tokenizer model and a responsible use guide.&lt;/p&gt; &#xA;&lt;p&gt;To download Llama3.1, you can run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune download meta-llama/Meta-Llama-3.1-8B-Instruct \&#xA;--output-dir /tmp/Meta-Llama-3.1-8B-Instruct \&#xA;--ignore-patterns &#34;original/consolidated.00.pth&#34; \&#xA;--hf-token &amp;lt;HF_TOKEN&amp;gt; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Tip] Set your environment variable &lt;code&gt;HF_TOKEN&lt;/code&gt; or pass in &lt;code&gt;--hf-token&lt;/code&gt; to the command in order to validate your access. You can find your token at &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;https://huggingface.co/settings/tokens&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Running finetuning recipes&lt;/h3&gt; &#xA;&lt;p&gt;You can finetune Llama3.1 8B with LoRA on a single GPU using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run lora_finetune_single_device --config llama3_1/8B_lora_single_device&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For distributed training, tune CLI integrates with &lt;a href=&#34;https://pytorch.org/docs/stable/elastic/run.html&#34;&gt;torchrun&lt;/a&gt;. To run a full finetune of Llama3.1 8B on two GPUs:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run --nproc_per_node 2 full_finetune_distributed --config llama3_1/8B_full&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!Tip] Make sure to place any torchrun commands &lt;strong&gt;before&lt;/strong&gt; the recipe specification. Any CLI args after this will override the config and not impact distributed training.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Modify Configs&lt;/h3&gt; &#xA;&lt;p&gt;There are two ways in which you can modify configs:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Config Overrides&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can directly overwrite config fields from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run lora_finetune_single_device \&#xA;--config llama2/7B_lora_single_device \&#xA;batch_size=8 \&#xA;enable_activation_checkpointing=True \&#xA;max_steps_per_epoch=128&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Update a Local Copy&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also copy the config to your local directory and modify the contents directly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune cp llama3_1/8B_full ./my_custom_config.yaml&#xA;Copied to ./my_custom_config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, you can run your custom recipe by directing the &lt;code&gt;tune run&lt;/code&gt; command to your local files:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;tune run full_finetune_distributed --config ./my_custom_config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;p&gt;Check out &lt;code&gt;tune --help&lt;/code&gt; for all possible CLI commands and options. For more information on using and updating configs, take a look at our &lt;a href=&#34;https://pytorch.org/torchtune/main/deep_dives/configs.html&#34;&gt;config deep-dive&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Custom Datasets&lt;/h3&gt; &#xA;&lt;p&gt;torchtune supports finetuning on a variety of different datasets, including &lt;a href=&#34;https://pytorch.org/torchtune/main/basics/instruct_datasets.html&#34;&gt;instruct-style&lt;/a&gt;, &lt;a href=&#34;https://pytorch.org/torchtune/main/basics/chat_datasets.html&#34;&gt;chat-style&lt;/a&gt;, &lt;a href=&#34;https://pytorch.org/torchtune/main/basics/preference_datasets.html&#34;&gt;preference datasets&lt;/a&gt;, and more. If you want to learn more about how to apply these components to finetune on your own custom dataset, please check out the provided links along with our &lt;a href=&#34;https://pytorch.org/torchtune/main/api_ref_datasets.html&#34;&gt;API docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;torchtune focuses on integrating with popular tools and libraries from the ecosystem. These are just a few examples, with more under development:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/hub/en/index&#34;&gt;Hugging Face Hub&lt;/a&gt; for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/_cli/download.py&#34;&gt;accessing model weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;EleutherAI&#39;s LM Eval Harness&lt;/a&gt; for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/eleuther_eval.py&#34;&gt;evaluating&lt;/a&gt; trained models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/datasets/en/index&#34;&gt;Hugging Face Datasets&lt;/a&gt; for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/datasets/_instruct.py&#34;&gt;access&lt;/a&gt; to training and evaluation datasets&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch/torchtitan/raw/main/docs/fsdp.md&#34;&gt;PyTorch FSDP2&lt;/a&gt; for distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch-labs/ao&#34;&gt;torchao&lt;/a&gt; for lower precision dtypes and &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/quantize.py&#34;&gt;post-training quantization&lt;/a&gt; techniques&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://wandb.ai/site&#34;&gt;Weights &amp;amp; Biases&lt;/a&gt; for &lt;a href=&#34;https://pytorch.org/torchtune/main/deep_dives/wandb_logging.html&#34;&gt;logging&lt;/a&gt; metrics and checkpoints, and tracking training progress&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.comet.com/site/&#34;&gt;Comet&lt;/a&gt; as another option for &lt;a href=&#34;https://pytorch.org/torchtune/main/deep_dives/comet_logging.html&#34;&gt;logging&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/executorch-overview&#34;&gt;ExecuTorch&lt;/a&gt; for &lt;a href=&#34;https://github.com/pytorch/executorch/tree/main/examples/models/llama2#optional-finetuning&#34;&gt;on-device inference&lt;/a&gt; using finetuned models&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/docs/bitsandbytes/main/en/index&#34;&gt;bitsandbytes&lt;/a&gt; for low memory optimizers for our &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/configs/llama2/7B_full_low_memory.yaml&#34;&gt;single-device recipes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; for continued finetuning or inference with torchtune models in the Hugging Face ecosystem&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h3&gt;Community Contributions&lt;/h3&gt; &#xA;&lt;p&gt;We really value our community and the contributions made by our wonderful users. We&#39;ll use this section to call out some of these contributions. If you&#39;d like to help out as well, please see the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/CONTRIBUTING.md&#34;&gt;CONTRIBUTING&lt;/a&gt; guide.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/salmanmohammadi&#34;&gt;@SalmanMohammadi&lt;/a&gt; for adding a comprehensive end-to-end recipe for &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/ppo_full_finetune_single_device.py&#34;&gt;Reinforcement Learning from Human Feedback (RLHF)&lt;/a&gt; finetuning with PPO to torchtune&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fyabc&#34;&gt;@fyabc&lt;/a&gt; for adding Qwen2 models, tokenizer, and recipe integration to torchtune&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/solitude-alive&#34;&gt;@solitude-alive&lt;/a&gt; for adding the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/gemma/&#34;&gt;Gemma 2B model&lt;/a&gt; to torchtune, including recipe changes, numeric validations of the models and recipe correctness&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yechenzhi&#34;&gt;@yechenzhi&lt;/a&gt; for adding &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/recipes/lora_dpo_single_device.py&#34;&gt;Direct Preference Optimization (DPO)&lt;/a&gt; to torchtune, including the recipe and config along with correctness checks&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Optimox&#34;&gt;@Optimox&lt;/a&gt; for adding all the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/torchtune/models/gemma2&#34;&gt;Gemma2 variants&lt;/a&gt; to torchtune!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;The Llama2 code in this repository is inspired by the original &lt;a href=&#34;https://github.com/meta-llama/llama/raw/main/llama/model.py&#34;&gt;Llama2 code&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;We want to give a huge shout-out to EleutherAI, Hugging Face and Weights &amp;amp; Biases for being wonderful collaborators and for working with us on some of these integrations within torchtune.&lt;/p&gt; &#xA;&lt;p&gt;We also want to acknowledge some awesome libraries and tools from the ecosystem:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast&#34;&gt;gpt-fast&lt;/a&gt; for performant LLM inference techniques which we&#39;ve adopted out-of-the-box&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/meta-llama/llama-recipes&#34;&gt;llama recipes&lt;/a&gt; for spring-boarding the llama2 community&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt; for bringing several memory and performance based techniques to the PyTorch ecosystem&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/winglian/&#34;&gt;@winglian&lt;/a&gt; and &lt;a href=&#34;https://github.com/OpenAccess-AI-Collective/axolotl&#34;&gt;axolotl&lt;/a&gt; for early feedback and brainstorming on torchtune&#39;s design and feature set.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Lightning-AI/litgpt&#34;&gt;lit-gpt&lt;/a&gt; for pushing the LLM finetuning community forward.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/trl&#34;&gt;HF TRL&lt;/a&gt; for making reward modeling more accessible to the PyTorch community.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&amp;nbsp;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;torchtune is released under the &lt;a href=&#34;https://raw.githubusercontent.com/pytorch/torchtune/main/LICENSE&#34;&gt;BSD 3 license&lt;/a&gt;. However you may have other legal obligations that govern your use of other content, such as the terms of service for third-party models.&lt;/p&gt; &#xA;&lt;h2&gt;Citing torchtune&lt;/h2&gt; &#xA;&lt;p&gt;If you find the torchtune library useful, please cite it in your work as below.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{torchtune,&#xA;  title = {torchtune: PyTorch&#39;s finetuning library},&#xA;  author = {torchtune maintainers and contributors},&#xA;  url = {https//github.com/pytorch/torchtune},&#xA;  license = {BSD-3-Clause},&#xA;  month = apr,&#xA;  year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>bnb-chain/bsc</title>
    <updated>2025-01-10T01:28:34Z</updated>
    <id>tag:github.com,2025-01-10:/bnb-chain/bsc</id>
    <link href="https://github.com/bnb-chain/bsc" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A BNB Smart Chain client based on the go-ethereum fork&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;BNB Smart Chain&lt;/h2&gt; &#xA;&lt;p&gt;The goal of BNB Smart Chain is to bring programmability and interoperability to BNB Beacon Chain. In order to embrace the existing popular community and advanced technology, it will bring huge benefits by staying compatible with all the existing smart contracts on Ethereum and Ethereum tooling. And to achieve that, the easiest solution is to develop based on go-ethereum fork, as we respect the great work of Ethereum very much.&lt;/p&gt; &#xA;&lt;p&gt;BNB Smart Chain starts its development based on go-ethereum fork. So you may see many toolings, binaries and also docs are based on Ethereum ones, such as the name ‚Äúgeth‚Äù.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pkg.go.dev/github.com/ethereum/go-ethereum?tab=doc&#34;&gt;&lt;img src=&#34;https://pkg.go.dev/badge/github.com/ethereum/go-ethereum&#34; alt=&#34;API Reference&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/z2VpC455eU&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/discord-join%20chat-blue.svg?sanitize=true&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;But from that baseline of EVM compatible, BNB Smart Chain introduces a system of 21 validators with Proof of Staked Authority (PoSA) consensus that can support short block time and lower fees. The most bonded validator candidates of staking will become validators and produce blocks. The double-sign detection and other slashing logic guarantee security, stability, and chain finality.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The BNB Smart Chain&lt;/strong&gt; will be:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;A self-sovereign blockchain&lt;/strong&gt;: Provides security and safety with elected validators.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;EVM-compatible&lt;/strong&gt;: Supports all the existing Ethereum tooling along with faster finality and cheaper transaction fees.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Distributed with on-chain governance&lt;/strong&gt;: Proof of Staked Authority brings in decentralization and community participants. As the native token, BNB will serve as both the gas of smart contract execution and tokens for staking.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;More details in &lt;a href=&#34;https://github.com/bnb-chain/whitepaper/raw/master/WHITEPAPER.md&#34;&gt;White Paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Key features&lt;/h2&gt; &#xA;&lt;h3&gt;Proof of Staked Authority&lt;/h3&gt; &#xA;&lt;p&gt;Although Proof-of-Work (PoW) has been approved as a practical mechanism to implement a decentralized network, it is not friendly to the environment and also requires a large size of participants to maintain the security.&lt;/p&gt; &#xA;&lt;p&gt;Proof-of-Authority(PoA) provides some defense to 51% attack, with improved efficiency and tolerance to certain levels of Byzantine players (malicious or hacked). Meanwhile, the PoA protocol is most criticized for being not as decentralized as PoW, as the validators, i.e. the nodes that take turns to produce blocks, have all the authorities and are prone to corruption and security attacks.&lt;/p&gt; &#xA;&lt;p&gt;Other blockchains, such as EOS and Cosmos both, introduce different types of Deputy Proof of Stake (DPoS) to allow the token holders to vote and elect the validator set. It increases the decentralization and favors community governance.&lt;/p&gt; &#xA;&lt;p&gt;To combine DPoS and PoA for consensus, BNB Smart Chain implement a novel consensus engine called Parlia that:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Blocks are produced by a limited set of validators.&lt;/li&gt; &#xA; &lt;li&gt;Validators take turns to produce blocks in a PoA manner, similar to Ethereum&#39;s Clique consensus engine.&lt;/li&gt; &#xA; &lt;li&gt;Validator set are elected in and out based on a staking based governance on BNB Smart Chain.&lt;/li&gt; &#xA; &lt;li&gt;Parlia consensus engine will interact with a set of &lt;a href=&#34;https://docs.bnbchain.org/bnb-smart-chain/staking/overview/#system-contracts&#34;&gt;system contracts&lt;/a&gt; to achieve liveness slash, revenue distributing and validator set renewing func.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Native Token&lt;/h2&gt; &#xA;&lt;p&gt;BNB will run on BNB Smart Chain in the same way as ETH runs on Ethereum so that it remains as &lt;code&gt;native token&lt;/code&gt; for BSC. This means, BNB will be used to:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;pay &lt;code&gt;gas&lt;/code&gt; to deploy or invoke Smart Contract on BSC&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Building the source&lt;/h2&gt; &#xA;&lt;p&gt;Many of the below are the same as or similar to go-ethereum.&lt;/p&gt; &#xA;&lt;p&gt;For prerequisites and detailed build instructions please read the &lt;a href=&#34;https://geth.ethereum.org/docs/getting-started/installing-geth&#34;&gt;Installation Instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Building &lt;code&gt;geth&lt;/code&gt; requires both a Go (version 1.22 or later) and a C compiler (GCC 5 or higher). You can install them using your favourite package manager. Once the dependencies are installed, run&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make geth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or, to build the full suite of utilities:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;make all&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you get such error when running the node with self built binary:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;Caught SIGILL in blst_cgo_init, consult &amp;lt;blst&amp;gt;/bindinds/go/README.md.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;please try to add the following environment variables and build again:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;export CGO_CFLAGS=&#34;-O -D__BLST_PORTABLE__&#34; &#xA;export CGO_CFLAGS_ALLOW=&#34;-O -D__BLST_PORTABLE__&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Executables&lt;/h2&gt; &#xA;&lt;p&gt;The bsc project comes with several wrappers/executables found in the &lt;code&gt;cmd&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;Command&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;strong&gt;&lt;code&gt;geth&lt;/code&gt;&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main BNB Smart Chain client binary. It is the entry point into the BSC network (main-, test- or private net), capable of running as a full node (default), archive node (retaining all historical state) or a light node (retrieving data live). It has the same and more RPC and other interface as go-ethereum and can be used by other processes as a gateway into the BSC network via JSON RPC endpoints exposed on top of HTTP, WebSocket and/or IPC transports. &lt;code&gt;geth --help&lt;/code&gt; and the &lt;a href=&#34;https://geth.ethereum.org/docs/interface/command-line-options&#34;&gt;CLI page&lt;/a&gt; for command line options.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;clef&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stand-alone signing tool, which can be used as a backend signer for &lt;code&gt;geth&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;devp2p&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Utilities to interact with nodes on the networking layer, without running a full blockchain.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;abigen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Source code generator to convert Ethereum contract definitions into easy to use, compile-time type-safe Go packages. It operates on plain &lt;a href=&#34;https://docs.soliditylang.org/en/develop/abi-spec.html&#34;&gt;Ethereum contract ABIs&lt;/a&gt; with expanded functionality if the contract bytecode is also available. However, it also accepts Solidity source files, making development much more streamlined. Please see our &lt;a href=&#34;https://geth.ethereum.org/docs/dapp/native-bindings&#34;&gt;Native DApps&lt;/a&gt; page for details.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;bootnode&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Stripped down version of our Ethereum client implementation that only takes part in the network node discovery protocol, but does not run any of the higher level application protocols. It can be used as a lightweight bootstrap node to aid in finding peers in private networks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;evm&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Developer utility version of the EVM (Ethereum Virtual Machine) that is capable of running bytecode snippets within a configurable environment and execution mode. Its purpose is to allow isolated, fine-grained debugging of EVM opcodes (e.g. &lt;code&gt;evm --code 60ff60ff --debug run&lt;/code&gt;).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;code&gt;rlpdump&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Developer utility tool to convert binary RLP (&lt;a href=&#34;https://ethereum.org/en/developers/docs/data-structures-and-encoding/rlp&#34;&gt;Recursive Length Prefix&lt;/a&gt;) dumps (data encoding used by the Ethereum protocol both network as well as consensus wise) to user-friendlier hierarchical representation (e.g. &lt;code&gt;rlpdump --hex CE0183FFFFFFC4C304050583616263&lt;/code&gt;).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Running &lt;code&gt;geth&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;p&gt;Going through all the possible command line flags is out of scope here (please consult our &lt;a href=&#34;https://geth.ethereum.org/docs/fundamentals/command-line-options&#34;&gt;CLI Wiki page&lt;/a&gt;), but we&#39;ve enumerated a few common parameter combos to get you up to speed quickly on how you can run your own &lt;code&gt;geth&lt;/code&gt; instance.&lt;/p&gt; &#xA;&lt;h3&gt;Hardware Requirements&lt;/h3&gt; &#xA;&lt;p&gt;The hardware must meet certain requirements to run a full node on mainnet:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VPS running recent versions of Mac OS X, Linux, or Windows.&lt;/li&gt; &#xA; &lt;li&gt;IMPORTANT 3 TB(Dec 2023) of free disk space, solid-state drive(SSD), gp3, 8k IOPS, 500 MB/S throughput, read latency &amp;lt;1ms. (if node is started with snap sync, it will need NVMe SSD)&lt;/li&gt; &#xA; &lt;li&gt;16 cores of CPU and 64 GB of memory (RAM)&lt;/li&gt; &#xA; &lt;li&gt;Suggest m5zn.6xlarge or r7iz.4xlarge instance type on AWS, c2-standard-16 on Google cloud.&lt;/li&gt; &#xA; &lt;li&gt;A broadband Internet connection with upload/download speeds of 5 MB/S&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The requirement for testnet:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;VPS running recent versions of Mac OS X, Linux, or Windows.&lt;/li&gt; &#xA; &lt;li&gt;500G of storage for testnet.&lt;/li&gt; &#xA; &lt;li&gt;4 cores of CPU and 16 gigabytes of memory (RAM).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Steps to Run a Fullnode&lt;/h3&gt; &#xA;&lt;h4&gt;1. Download the pre-build binaries&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Linux&#xA;wget $(curl -s https://api.github.com/repos/bnb-chain/bsc/releases/latest |grep browser_ |grep geth_linux |cut -d\&#34; -f4)&#xA;mv geth_linux geth&#xA;chmod -v u+x geth&#xA;&#xA;# MacOS&#xA;wget $(curl -s https://api.github.com/repos/bnb-chain/bsc/releases/latest |grep browser_ |grep geth_mac |cut -d\&#34; -f4)&#xA;mv geth_macos geth&#xA;chmod -v u+x geth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;2. Download the config files&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;//== mainnet&#xA;wget $(curl -s https://api.github.com/repos/bnb-chain/bsc/releases/latest |grep browser_ |grep mainnet |cut -d\&#34; -f4)&#xA;unzip mainnet.zip&#xA;&#xA;//== testnet&#xA;wget $(curl -s https://api.github.com/repos/bnb-chain/bsc/releases/latest |grep browser_ |grep testnet |cut -d\&#34; -f4)&#xA;unzip testnet.zip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;3. Download snapshot&lt;/h4&gt; &#xA;&lt;p&gt;Download latest chaindata snapshot from &lt;a href=&#34;https://github.com/bnb-chain/bsc-snapshots&#34;&gt;here&lt;/a&gt;. Follow the guide to structure your files.&lt;/p&gt; &#xA;&lt;h4&gt;4. Start a full node&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;## It will run with Path-Base Storage Scheme by default and enable inline state prune, keeping the latest 90000 blocks&#39; history state.&#xA;./geth --config ./config.toml --datadir ./node  --cache 8000 --rpc.allow-unprotected-txs --history.transactions 0&#xA;&#xA;## It is recommend to run fullnode with `--tries-verify-mode none` if you want high performance and care little about state consistency.&#xA;./geth --config ./config.toml --datadir ./node  --cache 8000 --rpc.allow-unprotected-txs --history.transactions 0 --tries-verify-mode none&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;5. Monitor node status&lt;/h4&gt; &#xA;&lt;p&gt;Monitor the log from &lt;strong&gt;./node/bsc.log&lt;/strong&gt; by default. When the node has started syncing, should be able to see the following output:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;t=2022-09-08T13:00:27+0000 lvl=info msg=&#34;Imported new chain segment&#34;             blocks=1    txs=177   mgas=17.317   elapsed=31.131ms    mgasps=556.259  number=21,153,429 hash=0x42e6b54ba7106387f0650defc62c9ace3160b427702dab7bd1c5abb83a32d8db dirty=&#34;0.00 B&#34;&#xA;t=2022-09-08T13:00:29+0000 lvl=info msg=&#34;Imported new chain segment&#34;             blocks=1    txs=251   mgas=39.638   elapsed=68.827ms    mgasps=575.900  number=21,153,430 hash=0xa3397b273b31b013e43487689782f20c03f47525b4cd4107c1715af45a88796e dirty=&#34;0.00 B&#34;&#xA;t=2022-09-08T13:00:33+0000 lvl=info msg=&#34;Imported new chain segment&#34;             blocks=1    txs=197   mgas=19.364   elapsed=34.663ms    mgasps=558.632  number=21,153,431 hash=0x0c7872b698f28cb5c36a8a3e1e315b1d31bda6109b15467a9735a12380e2ad14 dirty=&#34;0.00 B&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;6. Interact with fullnode&lt;/h4&gt; &#xA;&lt;p&gt;Start up &lt;code&gt;geth&lt;/code&gt;&#39;s built-in interactive &lt;a href=&#34;https://geth.ethereum.org/docs/interface/javascript-console&#34;&gt;JavaScript console&lt;/a&gt;, (via the trailing &lt;code&gt;console&lt;/code&gt; subcommand) through which you can interact using &lt;a href=&#34;https://web3js.readthedocs.io/en/&#34;&gt;&lt;code&gt;web3&lt;/code&gt; methods&lt;/a&gt; (note: the &lt;code&gt;web3&lt;/code&gt; version bundled within &lt;code&gt;geth&lt;/code&gt; is very old, and not up to date with official docs), as well as &lt;code&gt;geth&lt;/code&gt;&#39;s own &lt;a href=&#34;https://geth.ethereum.org/docs/rpc/server&#34;&gt;management APIs&lt;/a&gt;. This tool is optional and if you leave it out you can always attach to an already running &lt;code&gt;geth&lt;/code&gt; instance with &lt;code&gt;geth attach&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;7. More&lt;/h4&gt; &#xA;&lt;p&gt;More details about &lt;a href=&#34;https://docs.bnbchain.org/bnb-smart-chain/developers/node_operators/full_node/&#34;&gt;running a node&lt;/a&gt; and &lt;a href=&#34;https://docs.bnbchain.org/bnb-smart-chain/validator/create-val/&#34;&gt;becoming a validator&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Note: Although some internal protective measures prevent transactions from crossing over between the main network and test network, you should always use separate accounts for play and real money. Unless you manually move accounts, &lt;code&gt;geth&lt;/code&gt; will by default correctly separate the two networks and will not make any accounts available between them.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;As an alternative to passing the numerous flags to the &lt;code&gt;geth&lt;/code&gt; binary, you can also pass a configuration file via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ geth --config /path/to/your_config.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get an idea of how the file should look like you can use the &lt;code&gt;dumpconfig&lt;/code&gt; subcommand to export your existing configuration:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;$ geth --your-favourite-flags dumpconfig&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Programmatically interfacing &lt;code&gt;geth&lt;/code&gt; nodes&lt;/h3&gt; &#xA;&lt;p&gt;As a developer, sooner rather than later you&#39;ll want to start interacting with &lt;code&gt;geth&lt;/code&gt; and the BSC network via your own programs and not manually through the console. To aid this, &lt;code&gt;geth&lt;/code&gt; has built-in support for a JSON-RPC based APIs (&lt;a href=&#34;https://ethereum.github.io/execution-apis/api-documentation/&#34;&gt;standard APIs&lt;/a&gt; and &lt;a href=&#34;https://geth.ethereum.org/docs/interacting-with-geth/rpc&#34;&gt;&lt;code&gt;geth&lt;/code&gt; specific APIs&lt;/a&gt;). These can be exposed via HTTP, WebSockets and IPC (UNIX sockets on UNIX based platforms, and named pipes on Windows).&lt;/p&gt; &#xA;&lt;p&gt;The IPC interface is enabled by default and exposes all the APIs supported by &lt;code&gt;geth&lt;/code&gt;, whereas the HTTP and WS interfaces need to manually be enabled and only expose a subset of APIs due to security reasons. These can be turned on/off and configured as you&#39;d expect.&lt;/p&gt; &#xA;&lt;p&gt;HTTP based JSON-RPC API options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--http&lt;/code&gt; Enable the HTTP-RPC server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--http.addr&lt;/code&gt; HTTP-RPC server listening interface (default: &lt;code&gt;localhost&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--http.port&lt;/code&gt; HTTP-RPC server listening port (default: &lt;code&gt;8545&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--http.api&lt;/code&gt; API&#39;s offered over the HTTP-RPC interface (default: &lt;code&gt;eth,net,web3&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--http.corsdomain&lt;/code&gt; Comma separated list of domains from which to accept cross origin requests (browser enforced)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ws&lt;/code&gt; Enable the WS-RPC server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ws.addr&lt;/code&gt; WS-RPC server listening interface (default: &lt;code&gt;localhost&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ws.port&lt;/code&gt; WS-RPC server listening port (default: &lt;code&gt;8546&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ws.api&lt;/code&gt; API&#39;s offered over the WS-RPC interface (default: &lt;code&gt;eth,net,web3&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ws.origins&lt;/code&gt; Origins from which to accept WebSocket requests&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ipcdisable&lt;/code&gt; Disable the IPC-RPC server&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ipcapi&lt;/code&gt; API&#39;s offered over the IPC-RPC interface (default: &lt;code&gt;admin,debug,eth,miner,net,personal,txpool,web3&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--ipcpath&lt;/code&gt; Filename for IPC socket/pipe within the datadir (explicit paths escape it)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You&#39;ll need to use your own programming environments&#39; capabilities (libraries, tools, etc) to connect via HTTP, WS or IPC to a &lt;code&gt;geth&lt;/code&gt; node configured with the above flags and you&#39;ll need to speak &lt;a href=&#34;https://www.jsonrpc.org/specification&#34;&gt;JSON-RPC&lt;/a&gt; on all transports. You can reuse the same connection for multiple requests!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: Please understand the security implications of opening up an HTTP/WS based transport before doing so! Hackers on the internet are actively trying to subvert BSC nodes with exposed APIs! Further, all browser tabs can access locally running web servers, so malicious web pages could try to subvert locally available APIs!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Operating a private network&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bnb-chain/node-deploy/&#34;&gt;BSC-Deploy&lt;/a&gt;: deploy tool for setting up BNB Smart Chain.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Running a bootnode&lt;/h2&gt; &#xA;&lt;p&gt;Bootnodes are super-lightweight nodes that are not behind a NAT and are running just discovery protocol. When you start up a node it should log your enode, which is a public identifier that others can use to connect to your node.&lt;/p&gt; &#xA;&lt;p&gt;First the bootnode requires a key, which can be created with the following command, which will save a key to boot.key:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bootnode -genkey boot.key&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This key can then be used to generate a bootnode as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bootnode -nodekey boot.key -addr :30311 -network bsc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The choice of port passed to -addr is arbitrary. The bootnode command returns the following logs to the terminal, confirming that it is running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;enode://3063d1c9e1b824cfbb7c7b6abafa34faec6bb4e7e06941d218d760acdd7963b274278c5c3e63914bd6d1b58504c59ec5522c56f883baceb8538674b92da48a96@127.0.0.1:0?discport=30311&#xA;Note: you&#39;re using cmd/bootnode, a developer tool.&#xA;We recommend using a regular node as bootstrap node for production deployments.&#xA;INFO [08-21|11:11:30.687] New local node record                    seq=1,692,616,290,684 id=2c9af1742f8f85ce ip=&amp;lt;nil&amp;gt; udp=0 tcp=0&#xA;INFO [08-21|12:11:30.753] New local node record                    seq=1,692,616,290,685 id=2c9af1742f8f85ce ip=54.217.128.118 udp=30311 tcp=0&#xA;INFO [09-01|02:46:26.234] New local node record                    seq=1,692,616,290,686 id=2c9af1742f8f85ce ip=34.250.32.100  udp=30311 tcp=0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for considering helping out with the source code! We welcome contributions from anyone on the internet, and are grateful for even the smallest of fixes!&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute to bsc, please fork, fix, commit and send a pull request for the maintainers to review and merge into the main code base. If you wish to submit more complex changes though, please check up with the core devs first on &lt;a href=&#34;https://discord.gg/bnbchain&#34;&gt;our discord channel&lt;/a&gt; to ensure those changes are in line with the general philosophy of the project and/or get some early feedback which can make both your efforts much lighter as well as our review and merge procedures quick and simple.&lt;/p&gt; &#xA;&lt;p&gt;Please make sure your contributions adhere to our coding guidelines:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Code must adhere to the official Go &lt;a href=&#34;https://golang.org/doc/effective_go.html#formatting&#34;&gt;formatting&lt;/a&gt; guidelines (i.e. uses &lt;a href=&#34;https://golang.org/cmd/gofmt/&#34;&gt;gofmt&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Code must be documented adhering to the official Go &lt;a href=&#34;https://golang.org/doc/effective_go.html#commentary&#34;&gt;commentary&lt;/a&gt; guidelines.&lt;/li&gt; &#xA; &lt;li&gt;Pull requests need to be based on and opened against the &lt;code&gt;master&lt;/code&gt; branch.&lt;/li&gt; &#xA; &lt;li&gt;Commit messages should be prefixed with the package(s) they modify. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;E.g. &#34;eth, rpc: make trace configs optional&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://geth.ethereum.org/docs/developers/geth-developer/dev-guide&#34;&gt;Developers&#39; Guide&lt;/a&gt; for more details on configuring your environment, managing project dependencies, and testing procedures.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The bsc library (i.e. all code outside of the &lt;code&gt;cmd&lt;/code&gt; directory) is licensed under the &lt;a href=&#34;https://www.gnu.org/licenses/lgpl-3.0.en.html&#34;&gt;GNU Lesser General Public License v3.0&lt;/a&gt;, also included in our repository in the &lt;code&gt;COPYING.LESSER&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;The bsc binaries (i.e. all code inside of the &lt;code&gt;cmd&lt;/code&gt; directory) is licensed under the &lt;a href=&#34;https://www.gnu.org/licenses/gpl-3.0.en.html&#34;&gt;GNU General Public License v3.0&lt;/a&gt;, also included in our repository in the &lt;code&gt;COPYING&lt;/code&gt; file.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>apache/thrift</title>
    <updated>2025-01-10T01:28:34Z</updated>
    <id>tag:github.com,2025-01-10:/apache/thrift</id>
    <link href="https://github.com/apache/thrift" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Apache Thrift&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Apache Thrift&lt;/h1&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Thrift is a lightweight, language-independent software stack for point-to-point RPC implementation. Thrift provides clean abstractions and implementations for data transport, data serialization, and application level processing. The code generation system takes a simple definition language as input and generates code across programming languages that uses the abstracted stack to build interoperable RPC clients and servers.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/apache/thrift/master/doc/images/thrift-layers.png&#34; alt=&#34;Apache Thrift Layered Architecture&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Thrift makes it easy for programs written in different programming languages to share data and call remote procedures. With support for &lt;a href=&#34;https://raw.githubusercontent.com/apache/thrift/master/LANGUAGES.md&#34;&gt;28 programming languages&lt;/a&gt;, chances are Thrift supports the languages that you currently use.&lt;/p&gt; &#xA;&lt;p&gt;Thrift is specifically designed to support non-atomic version changes across client and server code. This allows you to upgrade your server while still being able to service older clients; or have newer clients issue requests to older servers. An excellent community-provided write-up about thrift and compatibility when versioning an API can be found in the &lt;a href=&#34;https://diwakergupta.github.io/thrift-missing-guide/#_versioning_compatibility&#34;&gt;Thrift Missing Guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For more details on Thrift&#39;s design and implementation, see the Thrift whitepaper included in this distribution, or at the README.md file in your particular subdirectory of interest.&lt;/p&gt; &#xA;&lt;h1&gt;Status&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Branch&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Travis&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Appveyor&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Coverity Scan&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;codecov.io&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Website&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/apache/thrift/tree/master&#34;&gt;&lt;code&gt;master&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://app.travis-ci.com/apache/thrift/branches&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/apache/thrift.svg?branch=master&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://ci.appveyor.com/project/ApacheSoftwareFoundation/thrift/history&#34;&gt;&lt;img src=&#34;https://ci.appveyor.com/api/projects/status/github/apache/thrift?branch=master&amp;amp;svg=true&#34; alt=&#34;Build status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://scan.coverity.com/projects/thrift&#34;&gt;&lt;img src=&#34;https://scan.coverity.com/projects/1345/badge.svg?sanitize=true&#34; alt=&#34;Coverity Scan Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://thrift.apache.org/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/official-website-brightgreen.svg?sanitize=true&#34; alt=&#34;Website&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://github.com/apache/thrift/tree/0.17.0&#34;&gt;&lt;code&gt;0.17.0&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;a href=&#34;https://app.travis-ci.com/apache/thrift/branches&#34;&gt;&lt;img src=&#34;https://api.travis-ci.com/apache/thrift.svg?branch=0.17.0&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Releases&lt;/h1&gt; &#xA;&lt;p&gt;Thrift does not maintain a specific release calendar at this time.&lt;/p&gt; &#xA;&lt;p&gt;We strive to release twice yearly. Download the &lt;a href=&#34;http://thrift.apache.org/download&#34;&gt;current release&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file except in compliance with the License. You may obtain a copy of the License at&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.&lt;/p&gt; &#xA;&lt;h1&gt;Project Hierarchy&lt;/h1&gt; &#xA;&lt;p&gt;thrift/&lt;/p&gt; &#xA;&lt;p&gt;compiler/&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Contains the Thrift compiler, implemented in C++.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;lib/&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Contains the Thrift software library implementation, subdivided by&#xA;language of implementation.&#xA;&#xA;cpp/&#xA;go/&#xA;java/&#xA;php/&#xA;py/&#xA;rb/&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;test/&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Contains sample Thrift files and test code across the target programming&#xA;languages.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;tutorial/&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Contains a basic tutorial that will teach you how to develop software&#xA;using Thrift.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Development&lt;/h1&gt; &#xA;&lt;p&gt;To build the same way Travis CI builds the project you should use docker. We have &lt;a href=&#34;https://raw.githubusercontent.com/apache/thrift/master/build/docker/README.md&#34;&gt;comprehensive building instructions for docker&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Requirements&lt;/h1&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;http://thrift.apache.org/docs/install&#34;&gt;http://thrift.apache.org/docs/install&lt;/a&gt; for a list of build requirements (may be stale). Alternatively, see the docker build environments for a list of prerequisites.&lt;/p&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;p&gt;More information about Thrift can be obtained on the Thrift webpage at:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt; http://thrift.apache.org&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Acknowledgments&lt;/h1&gt; &#xA;&lt;p&gt;Thrift was inspired by pillar, a lightweight RPC tool written by Adam D&#39;Angelo, and also by Google&#39;s protocol buffers.&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;p&gt;If you are building from the first time out of the source repository, you will need to generate the configure scripts. (This is not necessary if you downloaded a tarball.) From the top directory, do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./bootstrap.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Once the configure scripts are generated, thrift can be configured. From the top directory, do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You may need to specify the location of the boost files explicitly. If you installed boost in &lt;code&gt;/usr/local&lt;/code&gt;, you would run configure as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure --with-boost=/usr/local&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that by default the thrift C++ library is typically built with debugging symbols included. If you want to customize these options you should use the CXXFLAGS option in configure, as such:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure CXXFLAGS=&#39;-g -O2&#39;&#xA;./configure CFLAGS=&#39;-g -O2&#39;&#xA;./configure CPPFLAGS=&#39;-DDEBUG_MY_FEATURE&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To enable gcov required options -fprofile-arcs -ftest-coverage enable them:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;./configure  --enable-coverage&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run ./configure --help to see other configuration options&lt;/p&gt; &#xA;&lt;p&gt;Please be aware that the Python library will ignore the --prefix option and just install wherever Python&#39;s distutils puts it (usually along the lines of &lt;code&gt;/usr/lib/pythonX.Y/site-packages/&lt;/code&gt;). If you need to control where the Python modules are installed, set the PY_PREFIX variable. (DESTDIR is respected for Python and C++.)&lt;/p&gt; &#xA;&lt;p&gt;Make thrift:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;From the top directory, become superuser and do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Uninstall thrift:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make uninstall&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that some language packages must be installed manually using build tools better suited to those languages (at the time of this writing, this applies to Java, Ruby, PHP).&lt;/p&gt; &#xA;&lt;p&gt;Look for the README.md file in the lib/&#xA; &lt;language&gt;&#xA;  / folder for more details on the installation of each language library package.&#xA; &lt;/language&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Package Managers&lt;/h1&gt; &#xA;&lt;p&gt;Apache Thrift is available via a number of package managers, a list which is is steadily growing. A more detailed overview can be found &lt;a href=&#34;http://thrift.apache.org/lib/&#34;&gt;at the Apache Thrift web site under &#34;Libraries&#34;&lt;/a&gt; and/or in the respective READMEs for each language under /lib&lt;/p&gt; &#xA;&lt;h1&gt;Testing&lt;/h1&gt; &#xA;&lt;p&gt;There are a large number of client library tests that can all be run from the top-level directory.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make -k check&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will make all of the libraries (as necessary), and run through the unit tests defined in each of the client libraries. If a single language fails, the make check will continue on and provide a synopsis at the end.&lt;/p&gt; &#xA;&lt;p&gt;To run the cross-language test suite, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make cross&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will run a set of tests that use different language clients and servers.&lt;/p&gt;</summary>
  </entry>
</feed>