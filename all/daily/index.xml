<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-06T01:22:24Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>PKU-YuanGroup/Open-Sora-Plan</title>
    <updated>2024-03-06T01:22:24Z</updated>
    <id>tag:github.com,2024-03-06:/PKU-YuanGroup/Open-Sora-Plan</id>
    <link href="https://github.com/PKU-YuanGroup/Open-Sora-Plan" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This project aim to reproducing Sora (Open AI T2V model), but we only have limited resource. We deeply wish the all open source community can contribute to this project.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open-Sora Plan&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/&#34;&gt;[Project Page]&lt;/a&gt; &lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/blog_cn.html&#34;&gt;[中文主页]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Goal&lt;/h2&gt; &#xA;&lt;p&gt;This project aims to create a simple and scalable repo, to reproduce &lt;a href=&#34;https://openai.com/sora&#34;&gt;Sora&lt;/a&gt; (OpenAI, but we prefer to call it &#34;CloseAI&#34; ) and build knowledge about Video-VQVAE (VideoGPT) + DiT at scale. However, we have limited resources, we deeply wish all open-source community can contribute to this project. Pull request are welcome!!!&lt;/p&gt; &#xA;&lt;p&gt;本项目希望通过开源社区的力量复现Sora，由北大-兔展AIGC联合实验室共同发起，当前我们资源有限仅搭建了基础架构，无法进行完整训练，希望通过开源社区逐步增加模块并筹集资源进行训练，当前版本离目标差距巨大，仍需持续完善和快速迭代，欢迎Pull request！！！&lt;/p&gt; &#xA;&lt;p&gt;Project stages:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Primary&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Setup the codebase and train a un-conditional model on landscape dataset.&lt;/li&gt; &#xA; &lt;li&gt;Train models that boost resolution and duration.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Extensions&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Conduct text2video experiments on landscape dataset.&lt;/li&gt; &#xA; &lt;li&gt;Train the 1080p model on video2text dataset.&lt;/li&gt; &#xA; &lt;li&gt;Control model with more condition.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div style=&#34;display: flex; justify-content: center;&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/we_want_you.jpg&#34; width=&#34;200&#34;&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/framework.jpg&#34; width=&#34;600&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.05]&lt;/strong&gt; See our latest &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan?tab=readme-ov-file#todo&#34;&gt;todo&lt;/a&gt;, welcome to pull request.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.04]&lt;/strong&gt; We re-organize and modulize our codes and make it easy to &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan?tab=readme-ov-file#how-to-contribute-to-the-open-sora-plan-community&#34;&gt;contribute&lt;/a&gt; to the project, please see the &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan?tab=readme-ov-file#repo-structure&#34;&gt;Repo structure&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.03]&lt;/strong&gt; We open some &lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/discussions&#34;&gt;discussions&lt;/a&gt; and clarify several issues.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2024.03.01]&lt;/strong&gt; Training codes are available now! Learn more in our &lt;a href=&#34;https://pku-yuangroup.github.io/Open-Sora-Plan/&#34;&gt;project page&lt;/a&gt;. Please feel free to watch 👀 this repository for the latest updates.&lt;/p&gt; &#xA;&lt;h2&gt;Todo&lt;/h2&gt; &#xA;&lt;h4&gt;Setup the codebase and train a unconditional model on landscape dataset&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Setup repo-structure.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add Video-VQGAN model, which is borrowed from &lt;a href=&#34;https://github.com/wilson1yan/VideoGPT&#34;&gt;VideoGPT&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support variable aspect ratios, resolutions, durations training on &lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Support Dynamic mask input inspired &lt;a href=&#34;https://github.com/whlzy/FiT&#34;&gt;FiT&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add class-conditioning on embeddings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt; as main codebase.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add VAE model, which is borrowed from &lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Joint dynamic mask input with VAE.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Make the codebase ready for the cluster training. Add SLURM scripts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add sampling script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://github.com/willisma/SiT&#34;&gt;SiT&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Train models that boost resolution and duration&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add &lt;a href=&#34;https://arxiv.org/abs/2306.15595&#34;&gt;PI&lt;/a&gt; to support out-of-domain size.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add frame interpolation model.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Conduct text2video experiments on landscape dataset.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finish data loading, pre-processing utils.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add CLIP and T5 support.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add text2image training script.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add prompt captioner.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Train the 1080p model on video2text dataset&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Looking for a suitable dataset, welcome to discuss and recommend.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Finish data loading, pre-processing utils.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support memory friendly training. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add flash-attention2 from pytorch.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add xformers.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add accelerate to automatically manage training, e.g. mixed precision training.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add gradient checkpoint.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Train using the deepspeed engine.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Control model with more condition&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Load pretrained weight from &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-α&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incorporating &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Repo structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;├── README.md&#xA;├── docs&#xA;│   ├── Data.md                    -&amp;gt; Datasets description.&#xA;│   ├── Contribution_Guidelines.md -&amp;gt; Contribution guidelines description.&#xA;├── scripts                        -&amp;gt; All training scripts.&#xA;│   └── train.sh&#xA;├── sora&#xA;│   ├── dataset                    -&amp;gt; Dataset code to read videos&#xA;│   ├── models &#xA;│   │   ├── captioner               &#xA;│   │   ├── super_resolution        &#xA;│   ├── modules&#xA;│   │   ├── ae                     -&amp;gt; compress videos to latents&#xA;│   │   │   ├── vqvae&#xA;│   │   │   ├── vae&#xA;│   │   ├── diffusion              -&amp;gt; denoise latents&#xA;│   │   │   ├── dit&#xA;│   │   │   ├── unet&#xA;|   ├── utils.py                   &#xA;│   ├── train.py                   -&amp;gt; Training code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Requirements and Installation&lt;/h2&gt; &#xA;&lt;p&gt;The recommended requirements are as follows.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8&lt;/li&gt; &#xA; &lt;li&gt;Pytorch &amp;gt;= 1.13.1&lt;/li&gt; &#xA; &lt;li&gt;CUDA Version &amp;gt;= 11.7&lt;/li&gt; &#xA; &lt;li&gt;Install required packages:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/PKU-YuanGroup/Open-Sora-Plan&#xA;cd Open-Sora-Plan&#xA;conda create -n opensora python=3.8 -y&#xA;conda activate opensora&#xA;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117&#xA;pip install -r requirements.txt&#xA;cd VideoGPT&#xA;pip install -e .&#xA;cd ..&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;Refer to &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/docs/Data.md&#34;&gt;Data.md&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Video-VQVAE (VideoGPT)&lt;/h3&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd src/sora/modules/ae/vqvae/videogpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to origin &lt;a href=&#34;https://github.com/wilson1yan/VideoGPT?tab=readme-ov-file#training-vq-vae&#34;&gt;repo&lt;/a&gt;. Use the &lt;code&gt;scripts/train_vqvae.py&lt;/code&gt; script to train a Video-VQVAE. Execute &lt;code&gt;python scripts/train_vqvae.py -h&lt;/code&gt; for information on all available training settings. A subset of more relevant settings are listed below, along with default values.&lt;/p&gt; &#xA;&lt;h5&gt;VQ-VAE Specific Settings&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--embedding_dim&lt;/code&gt;: number of dimensions for codebooks embeddings&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_codes 2048&lt;/code&gt;: number of codes in the codebook&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_hiddens 240&lt;/code&gt;: number of hidden features in the residual blocks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--n_res_layers 4&lt;/code&gt;: number of residual blocks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--downsample 4 4 4&lt;/code&gt;: T H W downsampling stride of the encoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Training Settings&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--gpus 2&lt;/code&gt;: number of gpus for distributed training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sync_batchnorm&lt;/code&gt;: uses &lt;code&gt;SyncBatchNorm&lt;/code&gt; instead of &lt;code&gt;BatchNorm3d&lt;/code&gt; when using &amp;gt; 1 gpu&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--gradient_clip_val 1&lt;/code&gt;: gradient clipping threshold for training&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--batch_size 16&lt;/code&gt;: batch size per gpu&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--num_workers 8&lt;/code&gt;: number of workers for each DataLoader&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;Dataset Settings&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;--data_path &amp;lt;path&amp;gt;&lt;/code&gt;: path to an &lt;code&gt;hdf5&lt;/code&gt; file or a folder containing &lt;code&gt;train&lt;/code&gt; and &lt;code&gt;test&lt;/code&gt; folders with subdirectories of videos&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--resolution 128&lt;/code&gt;: spatial resolution to train on&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;--sequence_length 16&lt;/code&gt;: temporal resolution, or video clip length&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Reconstructing&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;python rec_video.py --video-path &#34;assets/origin_video_0.mp4&#34; --rec-path &#34;rec_video_0.mp4&#34; --num-frames 500 --sample-rate 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Python&#34;&gt;python rec_video.py --video-path &#34;assets/origin_video_1.mp4&#34; --rec-path &#34;rec_video_1.mp4&#34; --resolution 196 --num-frames 600 --sample-rate 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We present four reconstructed videos in this demonstration, arranged from left to right as follows:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;3s 596x336&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;10s 256x256&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;18s 196x196&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;24s 168x96&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_2.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_0.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_1.gif&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/rec_video_3.gif&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;VideoDiT (DiT)&lt;/h3&gt; &#xA;&lt;h4&gt;Training&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code&gt;sh scripts/train.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/assets/loss.jpg&#34; width=&#34;60%&#34;&gt; &lt;/p&gt; &#xA;&lt;h4&gt;Sampling&lt;/h4&gt; &#xA;&lt;p&gt;Coming soon.&lt;/p&gt; &#xA;&lt;h2&gt;How to Contribute to the Open-Sora Plan Community&lt;/h2&gt; &#xA;&lt;p&gt;We greatly appreciate your contributions to the Open-Sora Plan open-source community and helping us make it even better than it is now!&lt;/p&gt; &#xA;&lt;p&gt;For more details, please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/docs/Contribution_Guidelines.md&#34;&gt;Contribution Guidelines&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT/tree/main&#34;&gt;DiT&lt;/a&gt;: Scalable Diffusion Models with Transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wilson1yan/VideoGPT&#34;&gt;VideoGPT&lt;/a&gt;: Video Generation using VQ-VAE and Transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/whlzy/FiT&#34;&gt;FiT&lt;/a&gt;: Flexible Vision Transformer for Diffusion Model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.15595&#34;&gt;Positional Interpolation&lt;/a&gt;: Extending Context Window of Large Language Models via Positional Interpolation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The service is a research preview intended for non-commercial use only. See &lt;a href=&#34;https://raw.githubusercontent.com/PKU-YuanGroup/Open-Sora-Plan/main/LICENSE.txt&#34;&gt;LICENSE.txt&lt;/a&gt; for details.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/PKU-YuanGroup/Open-Sora-Plan/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=PKU-YuanGroup/Open-Sora-Plan&#34;&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>layerdiffusion/sd-forge-layerdiffuse</title>
    <updated>2024-03-06T01:22:24Z</updated>
    <id>tag:github.com,2024-03-06:/layerdiffusion/sd-forge-layerdiffuse</id>
    <link href="https://github.com/layerdiffusion/sd-forge-layerdiffuse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[WIP] Layer Diffusion for WebUI (via Forge)&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;sd-forge-layerdiffuse&lt;/h1&gt; &#xA;&lt;p&gt;Transparent Image Layer Diffusion using Latent Transparency&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/36598904-ae5f-4578-87d3-4b496e11dcc5&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a WIP extension for SD WebUI &lt;a href=&#34;https://github.com/lllyasviel/stable-diffusion-webui-forge&#34;&gt;(via Forge)&lt;/a&gt; to generate transparent images and layers.&lt;/p&gt; &#xA;&lt;p&gt;The image generating and basic layer functionality is working now, but &lt;strong&gt;the transparent img2img is not finished yet (will finish in about one week)&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;This code base is highly dynamic and may change a lot in the next month. If you are from professional content creation studio and need all previous results to be strictly reproduced, you may consider backup files during each update.&lt;/p&gt; &#xA;&lt;h1&gt;Before You Start&lt;/h1&gt; &#xA;&lt;p&gt;Because many people may be curious about how the latent preview looks like during a transparent diffusion process, I recorded a video so that you can see it before you download the models and extensions:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/e93b71d1-3560-48e2-a970-0b8efbfebb42&#34;&gt;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/e93b71d1-3560-48e2-a970-0b8efbfebb42&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can see that the native transparent diffusion can process transparent glass, semi-transparent glowing effects, etc, that are not possible with simple background removal methods. Native transparent diffusion also gives you detailed fur, hair, whiskers, and detailed structure like that skeleton.&lt;/p&gt; &#xA;&lt;h1&gt;Model Notes&lt;/h1&gt; &#xA;&lt;p&gt;Note that all currently released models are for SDXL. Models for SD1.5 may be provided later if demanded.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note that in this extension, all model downloads/selections are fully automatic. In fact most users can just skip this section.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Below models are released:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_transparent_attn.safetensors&lt;/code&gt; This is a rank-256 LoRA to turn a SDXL into a transparent image generator. It will change the latent distribution of the model to a &#34;transparent latent space&#34; that can be decoded by the special VAE pipeline.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_transparent_conv.safetensors&lt;/code&gt; This is an alternative model to turn your SDXL into a transparent image generator. This safetensors file includes an offset of all conv layers (and actually, all layers that are not q,k,v of any attention layers). These offsets can be merged to any XL model to change the latent distribution to transparent images. Because we excluded the offset training of any q,k,v layers, the prompt understanding of SDXL should be perfectly preserved. However, in practice, I find the &lt;code&gt;layer_xl_transparent_attn.safetensors&lt;/code&gt; will lead to better results. This &lt;code&gt;layer_xl_transparent_conv.safetensors&lt;/code&gt; is still included for some special use cases that needs special prompt understanding. Also, this model may introduce a strong style influence to the base model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_fg2ble.safetensors&lt;/code&gt; This is a safetensors file includes offsets to turn a SDXL into a layer generating model, that is conditioned on foregrounds, and generates blended compositions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_fgble2bg.safetensors&lt;/code&gt; This is a safetensors file includes offsets to turn a SDXL into a layer generating model, that is conditioned on foregrounds and blended compositions, and generates backgrounds.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_bg2ble.safetensors&lt;/code&gt; This is a safetensors file includes offsets to turn a SDXL into a layer generating model, that is conditioned on backgrounds, and generates blended compositions.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;layer_xl_bgble2fg.safetensors&lt;/code&gt; This is a safetensors file includes offsets to turn a SDXL into a layer generating model, that is conditioned on backgrounds and blended compositions, and generates foregrounds.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vae_transparent_encoder.safetensors&lt;/code&gt; This is an image encoder to extract a latent offset from pixel space. The offset can be added to latent images to help the diffusion of transparency. Note that in the paper we used a relatively heavy model with exactly same amount of parameters as the SD VAE. The released model is more light weighted, requires much less vram, and does not influence result quality in my tests.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;vae_transparent_decoder.safetensors&lt;/code&gt; This is an image decoder that takes SD VAE outputs and latent image as inputs, and outputs a real PNG image. The model architecture is also more lightweight than the paper version to reduce VRAM requirement. I have made sure that the reduced parameters does not influence result quality.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;(Update Mar 4) Below models will be released soon:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Joint foreground-background generating model. The model will be 3x slower and requires 3.5x more VRAM, but will generate foregrounds and backgrounds together in one single pass.&lt;/li&gt; &#xA; &lt;li&gt;One step foreground-conditioned background model. The model will be 2x slower and requires 2.5x more VRAM, but will generate cleaner backgrounds in one single pass (compared to the released two-step models).&lt;/li&gt; &#xA; &lt;li&gt;One step background-conditioned foreground model. The model will be 2x slower and requires 2.5x more VRAM, but will generate cleaner foregrounds in one single pass (compared to the released two-step models).&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Below models may be released soon (if necessary):&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A model that can generate foreground and background together (using attention sharing similar to AnimateDiff). I put this model on hold because of these reasons: (1) the other released models can already achieve all functionalities and this model does not bring more functionalities. (2) the inference speed of this model is 3x slower than others and requires 4x more VRAM than other released model, and I am working on reducing the VRAM of this model if necessary. (3) This model will involve more hyperparameters and if demanded, I will investigate the best practice for inference/training before release it. &lt;strong&gt;this model is confirmed to be released soon with joint layer generating and one-step bg/fg-condition, after we finish the final VRAM optimization&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;The current background-conditioned foreground model may be a bit too lightweight. I will probably release a heavier one with more parameters and different behaviors (see also the discussions later).&lt;/li&gt; &#xA; &lt;li&gt;Because the difference between diffusers training and k-diffusion inference, I can observe some mystical problems like sometimes DPM++ will give artifacts but Euler A will fix it. I am looking into it and may provide some revised model that works better with all A1111 samplers.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Sanity Check&lt;/h1&gt; &#xA;&lt;p&gt;We highly encourage you to go through the sanity check and get exactly same results (so that if any problem occurs, we will know if the problem is on our side).&lt;/p&gt; &#xA;&lt;p&gt;The two used models are:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://civitai.com/models/133005?modelVersionId=198530&#34;&gt;https://civitai.com/models/133005?modelVersionId=198530&lt;/a&gt; Juggernaut XL V6 (note that the used one is &lt;strong&gt;V6&lt;/strong&gt;, not v7 or v8 or V9)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://civitai.com/models/261336?modelVersionId=295158&#34;&gt;https://civitai.com/models/261336?modelVersionId=295158&lt;/a&gt; anima_pencil-XL 1.0.0 (note that the used one is &lt;strong&gt;1.0.0&lt;/strong&gt;, not 1.5.0)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We will first test transparent image generating. Set your extension to this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/5b85b383-89c0-403e-aa07-d6e43ff3b8ae&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;an apple, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 5, Seed: 12345, Size: 1024x1024, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;Make sure that you get this apple&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/376fa8bc-547e-4cd7-b658-7d60f2e37f1d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/16efc57b-4da8-4227-a257-f45f3dfeaddc&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/38ace070-6530-43c9-9ca1-c98aa5b7a0ed&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;woman, messy hair, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 5, Seed: 12345, Size: 1024x1024, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;Make sure that you get the woman with hair as messy as this&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/17c86ba5-eb29-45d4-b708-caf7e836b509&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/6f1ef595-255c-4162-bdf9-c8e4eb321f31&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;a cup made of glass, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 5, Seed: 12345, Size: 1024x1024, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;Make sure that you get this cup&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/a99177e6-72ed-447b-b2a5-6ca0fe1dc105&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/3b7df3f3-c6c1-401d-afa8-5a1c404165c9&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;glowing effect, book of magic, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 1024x1024, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: True, layerdiffusion_bg_image: False, layerdiffusion_blend_image: True, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;make sure that you get this glowing book&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/c093c862-17a3-4604-8e23-6c7f3a0eb4b3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/fa0b02b0-b530-48ed-a8ec-17bd9cccfc87&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;OK then lets move on to a bit longer prompt:&lt;/p&gt; &#xA;&lt;p&gt;(this prompt is from &lt;a href=&#34;https://civitai.com/images/3160575&#34;&gt;https://civitai.com/images/3160575&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;photograph close up portrait of Female boxer training, serious, stoic cinematic 4k epic detailed 4k epic detailed photograph shot on kodak detailed bokeh cinematic hbo dark moody&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: (worst quality, low quality, normal quality, lowres, low details, oversaturated, undersaturated, overexposed, underexposed, grayscale, bw, bad photo, bad photography, bad art:1.4), (watermark, signature, text font, username, error, logo, words, letters, digits, autograph, trademark, name:1.2), (blur, blurry, grainy), morbid, ugly, asymmetrical, mutated malformed, mutilated, poorly lit, bad shadow, draft, cropped, out of frame, cut off, censored, jpeg artifacts, out of focus, glitch, duplicate, (airbrushed, cartoon, anime, semi-realistic, cgi, render, blender, digital art, manga, amateur:1.3), (3D ,3D Game, 3D Game Scene, 3D Character:1.1), (bad hands, bad anatomy, bad body, bad face, bad teeth, bad arms, bad legs, deformities:1.3)&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/845c0e35-0096-484b-be2c-d443b4dc63cd&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/47ee7ba1-7f64-4e27-857f-c82c9d2bbb14&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Anime model test:&lt;/p&gt; &#xA;&lt;p&gt;girl in dress, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: nsfw, bad, ugly, text, watermark&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 7ed8da12d9, Model: animaPencilXL_v100, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/fcec8ea5-32de-44af-847a-d66dd62b95d1&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/53d84e56-4061-4d91-982f-8f1e927f68b7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(I am not very good at writing prompts in the AnimagineXL format, and perhaps you can get better results with better prompts)&lt;/p&gt; &#xA;&lt;h3&gt;Background Condition&lt;/h3&gt; &#xA;&lt;p&gt;First download this image:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/e7e2d80e-ffbe-4724-812a-5139a88027e3&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;then set the interface with&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/99a7e648-a83f-4ea5-bff6-66a1c624c0bd&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;then set the parameters with&lt;/p&gt; &#xA;&lt;p&gt;old man sitting, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: From Background to Blending, layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: False, layerdiffusion_bg_image: True, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/4dd5022a-d9fd-4436-83b8-775e2456bfc6&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then set the interface with (you first change the mode and then drag the image from result to interface)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/8277399c-fc9b-43fd-a9bb-1c7a8dcebb3f&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then change the sampler to Euler A or UniPC or some other sampler that is not dpm (This is probably because of some difference between diffusers training script and webui&#39;s k-diffusion. I am still looking into this and may revise my training script and model very soon so that this step will be removed.)&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/2c7124c5-e5d4-40cf-b106-e55c33e40003&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;FAQ:&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;OK. But how can I get a background image like this?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can use the Foreground Condition to get a background like this. We will describe it in the next section.&lt;/p&gt; &#xA;&lt;p&gt;Or you can use old inpainting tech to perform foreground removal on any image to get a background like this.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;Wait. Why you generate it with two steps? Can I generate it with one pass?&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Two steps allows for more flexible editing. We will release the one-step model soon if necessary, but that model is 2x larger and requires 4x larger VRAM, and we are still working on reducing the computation requirement of that model. (But in my tests, the current solution is better than that model in most cases.)&lt;/p&gt; &#xA;&lt;p&gt;Also you can see that the current model is about 680MB and in particular I think it is a bit too lightweight and will soon release a relatively heavier model for potential stronger structure understanding (but that is still under experiments).&lt;/p&gt; &#xA;&lt;h1&gt;Foreground Condition&lt;/h1&gt; &#xA;&lt;p&gt;First we generate a dog&lt;/p&gt; &#xA;&lt;p&gt;a dog sitting, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: Only Generate Transparent Image (Attention Injection), layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: True, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/dd515df4-cc58-47e0-8fe0-89e21e8320c4&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/e2785fd4-c168-4062-ae2f-010540ff0991&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;then change to &lt;code&gt;From Foreground to Blending&lt;/code&gt; and drag the transparent image to foreground input.&lt;/p&gt; &#xA;&lt;p&gt;Note that you drag the real transparent image, not the visualization with checkboard background. Make sure tou see this&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/b912e1e8-7511-4afc-aa61-4bb31d6949f7&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;then do this&lt;/p&gt; &#xA;&lt;p&gt;a dog sitting in room, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: DPM++ 2M SDE Karras, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: From Foreground to Blending, layerdiffusion_weight: 1, layerdiffusion_ending_step: 1, layerdiffusion_fg_image: True, layerdiffusion_bg_image: False, layerdiffusion_blend_image: False, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/0b2abb76-56b9-448d-8f2a-8572a18c759b&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Then change mode, drag your image, so that&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/48667cbf-e460-4037-b059-a30580841bcd&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;(Note that here I set stop at as 0.5 to get better results since I do not need the bg to be exactly same)&lt;/p&gt; &#xA;&lt;p&gt;Then change the sampler to Euler A or UniPC or some other sampler that is not dpm (This is probably because of some difference between diffusers training script and webui&#39;s k-diffusion. I am still looking into this and may revise my training script and model very soon so that this step will be removed.)&lt;/p&gt; &#xA;&lt;p&gt;then do this&lt;/p&gt; &#xA;&lt;p&gt;room, high quality&lt;/p&gt; &#xA;&lt;p&gt;Negative prompt: bad, ugly&lt;/p&gt; &#xA;&lt;p&gt;Steps: 20, Sampler: UniPC, CFG scale: 7, Seed: 12345, Size: 896x1152, Model hash: 1fe6c7ec54, Model: juggernautXL_version6Rundiffusion, layerdiffusion_enabled: True, layerdiffusion_method: From Foreground and Blending to Background, layerdiffusion_weight: 1, layerdiffusion_ending_step: 0.5, layerdiffusion_fg_image: True, layerdiffusion_bg_image: False, layerdiffusion_blend_image: True, layerdiffusion_resize_mode: Crop and Resize, Version: f0.0.17v1.8.0rc-latest-269-gef35383b&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/layerdiffusion/sd-forge-layerdiffusion/assets/161511761/5f5a5b6a-7dd2-4e16-9571-1458a9ef465d&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>nikic/PHP-Parser</title>
    <updated>2024-03-06T01:22:24Z</updated>
    <id>tag:github.com,2024-03-06:/nikic/PHP-Parser</id>
    <link href="https://github.com/nikic/PHP-Parser" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A PHP parser written in PHP&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;PHP Parser&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://coveralls.io/github/nikic/PHP-Parser?branch=master&#34;&gt;&lt;img src=&#34;https://coveralls.io/repos/github/nikic/PHP-Parser/badge.svg?branch=master&#34; alt=&#34;Coverage Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a PHP parser written in PHP. Its purpose is to simplify static code analysis and manipulation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nikic/PHP-Parser/tree/master/doc&#34;&gt;&lt;strong&gt;Documentation for version 5.x&lt;/strong&gt;&lt;/a&gt; (current; for running on PHP &amp;gt;= 7.4; for parsing PHP 7.0 to PHP 8.3, with limited support for parsing PHP 5.x).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/nikic/PHP-Parser/tree/4.x/doc&#34;&gt;Documentation for version 4.x&lt;/a&gt; (supported; for running on PHP &amp;gt;= 7.0; for parsing PHP 5.2 to PHP 8.3).&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;The main features provided by this library are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Parsing PHP 7, and PHP 8 code into an abstract syntax tree (AST). &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Invalid code can be parsed into a partial AST.&lt;/li&gt; &#xA;   &lt;li&gt;The AST contains accurate location information.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Dumping the AST in human-readable form.&lt;/li&gt; &#xA; &lt;li&gt;Converting an AST back to PHP code. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Formatting can be preserved for partially changed ASTs.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Infrastructure to traverse and modify ASTs.&lt;/li&gt; &#xA; &lt;li&gt;Resolution of namespaced names.&lt;/li&gt; &#xA; &lt;li&gt;Evaluation of constant expressions.&lt;/li&gt; &#xA; &lt;li&gt;Builders to simplify AST construction for code generation.&lt;/li&gt; &#xA; &lt;li&gt;Converting an AST into JSON and back.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Install the library using &lt;a href=&#34;https://getcomposer.org&#34;&gt;composer&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;php composer.phar require nikic/php-parser&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Parse some PHP code into an AST and dump the result in human-readable form:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;&amp;lt;?php&#xA;use PhpParser\Error;&#xA;use PhpParser\NodeDumper;&#xA;use PhpParser\ParserFactory;&#xA;&#xA;$code = &amp;lt;&amp;lt;&amp;lt;&#39;CODE&#39;&#xA;&amp;lt;?php&#xA;&#xA;function test($foo)&#xA;{&#xA;    var_dump($foo);&#xA;}&#xA;CODE;&#xA;&#xA;$parser = (new ParserFactory())-&amp;gt;createForNewestSupportedVersion();&#xA;try {&#xA;    $ast = $parser-&amp;gt;parse($code);&#xA;} catch (Error $error) {&#xA;    echo &#34;Parse error: {$error-&amp;gt;getMessage()}\n&#34;;&#xA;    return;&#xA;}&#xA;&#xA;$dumper = new NodeDumper;&#xA;echo $dumper-&amp;gt;dump($ast) . &#34;\n&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This dumps an AST looking something like this:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;array(&#xA;    0: Stmt_Function(&#xA;        attrGroups: array(&#xA;        )&#xA;        byRef: false&#xA;        name: Identifier(&#xA;            name: test&#xA;        )&#xA;        params: array(&#xA;            0: Param(&#xA;                attrGroups: array(&#xA;                )&#xA;                flags: 0&#xA;                type: null&#xA;                byRef: false&#xA;                variadic: false&#xA;                var: Expr_Variable(&#xA;                    name: foo&#xA;                )&#xA;                default: null&#xA;            )&#xA;        )&#xA;        returnType: null&#xA;        stmts: array(&#xA;            0: Stmt_Expression(&#xA;                expr: Expr_FuncCall(&#xA;                    name: Name(&#xA;                        name: var_dump&#xA;                    )&#xA;                    args: array(&#xA;                        0: Arg(&#xA;                            name: null&#xA;                            value: Expr_Variable(&#xA;                                name: foo&#xA;                            )&#xA;                            byRef: false&#xA;                            unpack: false&#xA;                        )&#xA;                    )&#xA;                )&#xA;            )&#xA;        )&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Let&#39;s traverse the AST and perform some kind of modification. For example, drop all function bodies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;use PhpParser\Node;&#xA;use PhpParser\Node\Stmt\Function_;&#xA;use PhpParser\NodeTraverser;&#xA;use PhpParser\NodeVisitorAbstract;&#xA;&#xA;$traverser = new NodeTraverser();&#xA;$traverser-&amp;gt;addVisitor(new class extends NodeVisitorAbstract {&#xA;    public function enterNode(Node $node) {&#xA;        if ($node instanceof Function_) {&#xA;            // Clean out the function body&#xA;            $node-&amp;gt;stmts = [];&#xA;        }&#xA;    }&#xA;});&#xA;&#xA;$ast = $traverser-&amp;gt;traverse($ast);&#xA;echo $dumper-&amp;gt;dump($ast) . &#34;\n&#34;;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This gives us an AST where the &lt;code&gt;Function_::$stmts&lt;/code&gt; are empty:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;array(&#xA;    0: Stmt_Function(&#xA;        attrGroups: array(&#xA;        )&#xA;        byRef: false&#xA;        name: Identifier(&#xA;            name: test&#xA;        )&#xA;        params: array(&#xA;            0: Param(&#xA;                attrGroups: array(&#xA;                )&#xA;                type: null&#xA;                byRef: false&#xA;                variadic: false&#xA;                var: Expr_Variable(&#xA;                    name: foo&#xA;                )&#xA;                default: null&#xA;            )&#xA;        )&#xA;        returnType: null&#xA;        stmts: array(&#xA;        )&#xA;    )&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, we can convert the new AST back to PHP code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;use PhpParser\PrettyPrinter;&#xA;&#xA;$prettyPrinter = new PrettyPrinter\Standard;&#xA;echo $prettyPrinter-&amp;gt;prettyPrintFile($ast);&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This gives us our original code, minus the &lt;code&gt;var_dump()&lt;/code&gt; call inside the function:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-php&#34;&gt;&amp;lt;?php&#xA;&#xA;function test($foo)&#xA;{&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a more comprehensive introduction, see the documentation.&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/0_Introduction.markdown&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/2_Usage_of_basic_components.markdown&#34;&gt;Usage of basic components&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Component documentation:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/Walking_the_AST.markdown&#34;&gt;Walking the AST&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Node visitors&lt;/li&gt; &#xA;   &lt;li&gt;Modifying the AST from a visitor&lt;/li&gt; &#xA;   &lt;li&gt;Short-circuiting traversals&lt;/li&gt; &#xA;   &lt;li&gt;Interleaved visitors&lt;/li&gt; &#xA;   &lt;li&gt;Simple node finding API&lt;/li&gt; &#xA;   &lt;li&gt;Parent and sibling references&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/Name_resolution.markdown&#34;&gt;Name resolution&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Name resolver options&lt;/li&gt; &#xA;   &lt;li&gt;Name resolution context&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/Pretty_printing.markdown&#34;&gt;Pretty printing&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Converting AST back to PHP code&lt;/li&gt; &#xA;   &lt;li&gt;Customizing formatting&lt;/li&gt; &#xA;   &lt;li&gt;Formatting-preserving code transformations&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/AST_builders.markdown&#34;&gt;AST builders&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fluent builders for AST nodes&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/Lexer.markdown&#34;&gt;Lexer&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Emulation&lt;/li&gt; &#xA;   &lt;li&gt;Tokens, positions and attributes&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/Error_handling.markdown&#34;&gt;Error handling&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Column information for errors&lt;/li&gt; &#xA;   &lt;li&gt;Error recovery (parsing of syntactically incorrect code)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/Constant_expression_evaluation.markdown&#34;&gt;Constant expression evaluation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Evaluating constant/property/etc initializers&lt;/li&gt; &#xA;   &lt;li&gt;Handling errors and unsupported expressions&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/JSON_representation.markdown&#34;&gt;JSON representation&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;JSON encoding and decoding of ASTs&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/Performance.markdown&#34;&gt;Performance&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Disabling Xdebug&lt;/li&gt; &#xA;   &lt;li&gt;Reusing objects&lt;/li&gt; &#xA;   &lt;li&gt;Garbage collection impact&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/nikic/PHP-Parser/master/doc/component/FAQ.markdown&#34;&gt;Frequently asked questions&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Parent and sibling references&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>