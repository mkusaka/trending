<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-08T01:29:18Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pineappleEA/pineapple-src</title>
    <updated>2023-05-08T01:29:18Z</updated>
    <id>tag:github.com,2023-05-08:/pineappleEA/pineapple-src</id>
    <link href="https://github.com/pineappleEA/pineapple-src" rel="alternate"></link>
    <summary type="html">&lt;p&gt;yuzu Early Access source code&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;yuzu emulator early access&lt;/h1&gt; &#xA;&lt;p&gt;This is the source code for early-access 3568.&lt;/p&gt; &#xA;&lt;h2&gt;Legal Notice&lt;/h2&gt; &#xA;&lt;p&gt;yuzu is a GPLv3 program, which allows fully free redistribution of its source code.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>shchmue/Lockpick_RCM</title>
    <updated>2023-05-08T01:29:18Z</updated>
    <id>tag:github.com,2023-05-08:/shchmue/Lockpick_RCM</id>
    <link href="https://github.com/shchmue/Lockpick_RCM" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Nintendo Switch encryption key derivation bare metal RCM payload&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Lockpick_RCM&lt;/h1&gt; &#xA;&lt;p&gt;Lockpick_RCM is a bare metal Nintendo Switch payload that derives encryption keys for use in Switch file handling software like hactool, hactoolnet/LibHac, ChoiDujour, etc. without booting Horizon OS.&lt;/p&gt; &#xA;&lt;p&gt;Due to changes imposed by firmware 7.0.0, Lockpick homebrew can no longer derive the latest keys. In the boot-time environment however, there is no such limitation.&lt;/p&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;It is highly recommended, but not required, to place Minerva on SD from the latest &lt;a href=&#34;https://github.com/CTCaer/hekate/releases&#34;&gt;Hekate&lt;/a&gt; for best performance, especially while dumping titlekeys - the file and path is &lt;code&gt;/bootloader/sys/libsys_minerva.bso&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Launch Lockpick_RCM.bin using your favorite payload injector or chainloader&lt;/li&gt; &#xA; &lt;li&gt;Upon completion, keys will be saved to &lt;code&gt;/switch/prod.keys&lt;/code&gt; and titlekeys to &lt;code&gt;/switch/title.keys&lt;/code&gt; on SD&lt;/li&gt; &#xA; &lt;li&gt;This release bundles the Falcon keygen from &lt;a href=&#34;https://github.com/Atmosphere-NX/Atmosphere&#34;&gt;AtmosphÃ¨re-NX&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Mariko-Specific Keys&lt;/h1&gt; &#xA;&lt;p&gt;Mariko consoles have several unique keys and protected keyslots. To get your SBK or the Mariko specific keys, you will need to use the &lt;code&gt;/switch/partialaes.keys&lt;/code&gt; file along with a brute forcing tool such as &lt;a href=&#34;https://files.sshnuke.net/PartialAesKeyCrack.zip&#34;&gt;https://files.sshnuke.net/PartialAesKeyCrack.zip&lt;/a&gt;. The contents of this file are the keyslot number followed by the result of that keyslot encrypting 16 null bytes. With the tool linked above, enter them in sequence for a given keyslot you want the contents of, for example: &lt;code&gt;PartialAesKeyCrack.exe &amp;lt;num1&amp;gt; &amp;lt;num2&amp;gt; &amp;lt;num3&amp;gt; &amp;lt;num4&amp;gt;&lt;/code&gt; with the &lt;code&gt;--numthreads=N&lt;/code&gt; where N is the number of threads you can dedicate to the brute force.&lt;/p&gt; &#xA;&lt;p&gt;The keyslots are as follows, with names recognized by &lt;code&gt;hactool&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;0-11 - &lt;code&gt;mariko_aes_class_key_xx&lt;/code&gt; (this is not used by the Switch but is set by the bootrom; hactoolnet recognizes it but it serves no purpose)&lt;/li&gt; &#xA; &lt;li&gt;12 - &lt;code&gt;mariko_kek&lt;/code&gt; (not unique - this is used for master key derivation)&lt;/li&gt; &#xA; &lt;li&gt;13 - &lt;code&gt;mariko_bek&lt;/code&gt; (not unique - this is used for BCT and package1 decryption)&lt;/li&gt; &#xA; &lt;li&gt;14 - &lt;code&gt;secure_boot_key&lt;/code&gt; (console unique - this isn&#39;t needed for further key derivation than what Lockpick_RCM does but might be nice to have for your records)&lt;/li&gt; &#xA; &lt;li&gt;15 - Secure storage key (console unique - this is not used on retail or dev consoles and not recognized by any tools)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;So if you want to brute force the &lt;code&gt;mariko_kek&lt;/code&gt;, open your &lt;code&gt;partialaes.keys&lt;/code&gt; and observe the numbers beneath keyslot 12. Here&#39;s an example with fake numbers:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;12&#xA;11111111111111111111111111111111 22222222222222222222222222222222 33333333333333333333333333333333 44444444444444444444444444444444&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then take those numbers and open a command prompt window at the location of the exe linked above and type: &lt;code&gt;PartialAesKeyCrack.exe 11111111111111111111111111111111 22222222222222222222222222222222 33333333333333333333333333333333 44444444444444444444444444444444&lt;/code&gt; and if you&#39;re on a powerful enough multicore system, add &lt;code&gt; --numthreads=[whatever number of threads]&lt;/code&gt;, ideally not your system&#39;s maximum if it&#39;s, for example, an older laptop with a low-end dual core CPU. On a Ryzen 3900x with 24 threads this generates a lot of heat but finishes in about 45 seconds.&lt;/p&gt; &#xA;&lt;p&gt;These keys never change so a brute force need only be conducted once.&lt;/p&gt; &#xA;&lt;p&gt;This works due to the security engine immediately flushing writes to keyslots which can be written one 32-bit chunk at a time. See: &lt;a href=&#34;https://switchbrew.org/wiki/Switch_System_Flaws#Hardware&#34;&gt;https://switchbrew.org/wiki/Switch_System_Flaws#Hardware&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Building&lt;/h1&gt; &#xA;&lt;p&gt;Install &lt;a href=&#34;https://devkitpro.org/&#34;&gt;devkitARM&lt;/a&gt; and run &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Massive Thanks to CTCaer!&lt;/h1&gt; &#xA;&lt;p&gt;This software is heavily based on &lt;a href=&#34;https://github.com/CTCaer/hekate&#34;&gt;Hekate&lt;/a&gt;. Beyond that, CTCaer was exceptionally helpful in the development of this project, lending loads of advice, expertise, and humor.&lt;/p&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;This project is under the GPLv2 license. The Save processing module is adapted from &lt;a href=&#34;https://github.com/SciresM/hactool&#34;&gt;hactool&lt;/a&gt; code under ISC.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>bigcode-project/starcoder</title>
    <updated>2023-05-08T01:29:18Z</updated>
    <id>tag:github.com,2023-05-08:/bigcode-project/starcoder</id>
    <link href="https://github.com/bigcode-project/starcoder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Home of StarCoder: fine-tuning &amp; inference!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ðŸ’« StarCoder&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/bigcode/starcoder&#34;&gt;Model&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-playground&#34;&gt;Playground&lt;/a&gt; | &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=HuggingFace.huggingface-vscode&#34;&gt;VSCode&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/chat/?model=bigcode/starcoder&#34;&gt;Chat&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;What is this about?&lt;/h1&gt; &#xA;&lt;p&gt;ðŸ’« StarCoder is a language model (LM) trained on source code and natural language text. Its training data incorporates more that 80 different programming languages as well as text extracted from github issues and commits and from notebooks. This repository showcases how we get an overview of this LM&#39;s capabilities.&lt;/p&gt; &#xA;&lt;h1&gt;Disclaimer&lt;/h1&gt; &#xA;&lt;p&gt;Before you can use the model go to &lt;code&gt;hf.co/bigcode/starcoder&lt;/code&gt; and accept the agreement.&lt;/p&gt; &#xA;&lt;h1&gt;Table of Contents&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#quickstart&#34;&gt;Quickstart&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#code-generation&#34;&gt;Code generation with StarCoder&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#text-generation-inference&#34;&gt;Text-generation-inference code&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#fine-tuning&#34;&gt;Fine-tuning&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#step-by-step-installation-with-conda&#34;&gt;Step by step installation with conda&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#datasets&#34;&gt;Datasets&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#stack-exchange-se&#34;&gt;Stack Exchange&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bigcode-project/starcoder/main/#merging-peft-adapter-layers&#34;&gt;Merging PEFT adapter layers&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Quickstart&lt;/h1&gt; &#xA;&lt;p&gt;StarCoder was trained on github code, thus it can be used to perform code generation. More precisely, the model can complete the implementation of a function or infer the following characters in a line of code. This can be done with the help of the ðŸ¤—&#39;s &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;transformers&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;First, we have to install all the libraries listed in &lt;code&gt;requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Code generation&lt;/h2&gt; &#xA;&lt;p&gt;The code generation pipeline is as follows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer&#xA;&#xA;checkpoint = &#34;bigcode/starcoder&#34;&#xA;device = &#34;cuda&#34; # for GPU usage or &#34;cpu&#34; for CPU usage&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(checkpoint)&#xA;model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)&#xA;&#xA;inputs = tokenizer.encode(&#34;def print_hello_world():&#34;, return_tensors=&#34;pt&#34;).to(device)&#xA;outputs = model.generate(inputs)&#xA;print(tokenizer.decode(outputs[0]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline&#xA;checkpoint = &#34;bigcode/starcoder&#34;&#xA;&#xA;model = AutoModelForCausalLM.from_pretrained(checkpoint)&#xA;tokenizer = AutoTokenizer.from_pretrained(checkpoint)&#xA;&#xA;pipe = pipeline(&#34;text-generation&#34;, model=model, tokenizer=tokenizer, device=0)&#xA;print( pipe(&#34;def hello():&#34;) )&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Text-generation-inference&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker run --gpus &#39;&#34;device:0&#34;&#39; -p 8080:80 -v $PWD/data:/data -e HUGGING_FACE_HUB_TOKEN=&amp;lt;YOUR BIGCODE ENABLED TOKEN&amp;gt; -e HF_HUB_ENABLE_HF_TRANSFER=0 -d  ghcr.io/huggingface/text-generation-inference:sha-880a76e --model-id bigcode/starcoder --max-total-tokens 8192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more details, see &lt;a href=&#34;https://github.com/huggingface/text-generation-inference&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Fine-tuning&lt;/h1&gt; &#xA;&lt;p&gt;Here, we showcase how we can fine-tune this LM on a specific downstream task.&lt;/p&gt; &#xA;&lt;h2&gt;Step by step installation with conda&lt;/h2&gt; &#xA;&lt;p&gt;Create a new conda environment and activate it&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -n env&#xA;conda activate env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the &lt;code&gt;pytorch&lt;/code&gt; version compatible with your version of cuda &lt;a href=&#34;https://pytorch.org/get-started/previous-versions/&#34;&gt;here&lt;/a&gt;, for example the following command works with cuda 11.6&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;peft&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c huggingface transformers &#xA;pip install git+https://github.com/huggingface/peft.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that you can install the latest stable version of transformers by using&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install git+https://github.com/huggingface/transformers&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install &lt;code&gt;datasets&lt;/code&gt;, &lt;code&gt;accelerate&lt;/code&gt; and &lt;code&gt;huggingface_hub&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install -c huggingface -c conda-forge datasets&#xA;conda install -c conda-forge accelerate&#xA;conda install -c conda-forge huggingface_hub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, install &lt;code&gt;bitsandbytes&lt;/code&gt; and &lt;code&gt;wandb&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install bitsandbytes&#xA;pip install wandb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To get the full list of arguments with descriptions you can run the following command on any script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python scripts/some_script.py --help&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Before you run any of the scripts make sure you are logged in and can push to the hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure you are logged in &lt;code&gt;wandb&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now that everything is done, you can clone the repository and get into the corresponding directory.&lt;/p&gt; &#xA;&lt;h2&gt;Datasets&lt;/h2&gt; &#xA;&lt;p&gt;ðŸ’« StarCoder can be fine-tuned to achieve multiple downstream tasks. Our interest here is to fine-tune StarCoder in order to make it follow instructions. &lt;a href=&#34;https://arxiv.org/pdf/2109.01652.pdf&#34;&gt;Instruction fine-tuning&lt;/a&gt; has gained a lot of attention recently as it proposes a simple framework that teaches language models to align their outputs with human needs. That procedure requires the availability of quality instruction datasets, which contain multiple &lt;code&gt;instruction - answer&lt;/code&gt; pairs. Unfortunately such datasets are not ubiquitous but thanks to Hugging Face ðŸ¤—&#39;s &lt;a href=&#34;https://github.com/huggingface/datasets&#34;&gt;datasets&lt;/a&gt; library we can have access to some good proxies. To fine-tune cheaply and efficiently, we use Hugging Face ðŸ¤—&#39;s &lt;a href=&#34;https://github.com/huggingface/peft&#34;&gt;PEFT&lt;/a&gt; as well as Tim Dettmers&#39; &lt;a href=&#34;https://github.com/TimDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Stack Exchange SE&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Stack_Exchange&#34;&gt;Stack Exchange&lt;/a&gt; is a well-known network of Q&amp;amp;A websites on topics in diverse fields. It is a place where a user can ask a question and obtain answers from other users. Those answers are scored and ranked based on their quality. &lt;a href=&#34;https://huggingface.co/datasets/ArmelR/stack-exchange-instruction&#34;&gt;Stack exchange instruction&lt;/a&gt; is a dataset that was obtained by scrapping the site in order to build a collection of Q&amp;amp;A pairs. A language model can then be fine-tuned on that dataset to make it elicit strong and diverse question-answering skills.&lt;/p&gt; &#xA;&lt;p&gt;To execute the fine-tuning script run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune/finetune.py \&#xA;  --model_path=&#34;bigcode/starcoder&#34;\&#xA;  --dataset_name=&#34;ArmelR/stack-exchange-instruction&#34;\&#xA;  --subset=&#34;data/finetune&#34;\&#xA;  --split=&#34;train&#34;\&#xA;  --size_valid_set 10000\&#xA;  --streaming\&#xA;  --seq_length 2048\&#xA;  --max_steps 1000\&#xA;  --batch_size 1\&#xA;  --input_column_name=&#34;question&#34;\&#xA;  --output_column_name=&#34;response&#34;\ &#xA;  --gradient_accumulation_steps 16\&#xA;  --learning_rate 1e-4\&#xA;  --lr_scheduler_type=&#34;cosine&#34;\&#xA;  --num_warmup_steps 100\&#xA;  --weight_decay 0.05\&#xA;  --output_dir=&#34;./checkpoints&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The size of the SE dataset is better manageable when using streaming. We also have to precise the split of the dataset that is used. For more details, check the &lt;a href=&#34;https://huggingface.co/datasets/ArmelR/stack-exchange-instruction&#34;&gt;dataset&#39;s page&lt;/a&gt; on ðŸ¤—. Similarly we can modify the command to account for the availability of GPUs&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m torch.distributed.launch \&#xA;  --nproc_per_node number_of_gpus finetune/finetune.py \&#xA;  --model_path=&#34;bigcode/starcoder&#34;\&#xA;  --dataset_name=&#34;ArmelR/stack-exchange-instruction&#34;\&#xA;  --subset=&#34;data/finetune&#34;\&#xA;  --split=&#34;train&#34;\&#xA;  --size_valid_set 10000\&#xA;  --streaming \&#xA;  --seq_length 2048\&#xA;  --max_steps 1000\&#xA;  --batch_size 1\&#xA;  --input_column_name=&#34;question&#34;\&#xA;  --output_column_name=&#34;response&#34;\ &#xA;  --gradient_accumulation_steps 16\&#xA;  --learning_rate 1e-4\&#xA;  --lr_scheduler_type=&#34;cosine&#34;\&#xA;  --num_warmup_steps 100\&#xA;  --weight_decay 0.05\&#xA;  --output_dir=&#34;./checkpoints&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Merging PEFT adapter layers&lt;/h2&gt; &#xA;&lt;p&gt;If you train a model with PEFT, you&#39;ll need to merge the adapter layers with the base model if you want to run inference / evaluation. To do so, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune/merge_peft_adapters.py --model_name_or_path model_to_merge --peft_model_path model_checkpoint&#xA;&#xA;# Push merged model to the Hub&#xA;python finetune/merge_peft_adapters.py --model_name_or_path model_to_merge --peft_model_path model_checkpoint --push_to_hub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python finetune/merge_peft_adapters.py --model_name_or_path bigcode/starcoder --peft_model_path checkpoints/checkpoint-1000 --push_to_hub&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>