<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-04T01:25:12Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>pytorch-labs/gpt-fast</title>
    <updated>2023-12-04T01:25:12Z</updated>
    <id>tag:github.com,2023-12-04:/pytorch-labs/gpt-fast</id>
    <link href="https://github.com/pytorch-labs/gpt-fast" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Simple and efficient pytorch-native transformer text generation in &lt;1000 LOC of python.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;gpt-fast&lt;/h1&gt; &#xA;&lt;p&gt;Simple and efficient pytorch-native transformer text generation.&lt;/p&gt; &#xA;&lt;p&gt;Featuring:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Very low latency&lt;/li&gt; &#xA; &lt;li&gt;&amp;lt;1000 lines of python&lt;/li&gt; &#xA; &lt;li&gt;No dependencies other than PyTorch and sentencepiece&lt;/li&gt; &#xA; &lt;li&gt;int8/int4 quantization&lt;/li&gt; &#xA; &lt;li&gt;Speculative decoding&lt;/li&gt; &#xA; &lt;li&gt;Tensor parallelism&lt;/li&gt; &#xA; &lt;li&gt;Supports Nvidia and AMD GPUs&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This is &lt;em&gt;NOT&lt;/em&gt; intended to be a &#34;framework&#34; or &#34;library&#34; - it is intended to show off what kind of performance you can get with native PyTorch :) Please copy-paste and fork as you desire.&lt;/p&gt; &#xA;&lt;p&gt;For an in-depth walkthrough of what&#39;s in this codebase, see this &lt;a href=&#34;https://pytorch.org/blog/accelerating-generative-ai-2/&#34;&gt;blog post&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;Download PyTorch nightly&lt;/a&gt; Install sentencepiece and huggingface_hub&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install sentencepiece huggingface_hub&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To download llama models, go to &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b&#34;&gt;https://huggingface.co/meta-llama/Llama-2-7b&lt;/a&gt; and go through steps to obtain access. Then login with &lt;code&gt;huggingface-cli login&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Downloading Weights&lt;/h2&gt; &#xA;&lt;p&gt;Models tested/supported&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;openlm-research/open_llama_7b&#xA;meta-llama/Llama-2-7b-chat-hf&#xA;meta-llama/Llama-2-13b-chat-hf&#xA;meta-llama/Llama-2-70b-chat-hf&#xA;codellama/CodeLlama-7b-Python-hf&#xA;codellama/CodeLlama-34b-Python-hf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, to convert Llama-2-7b-chat-hf&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export MODEL_REPO=meta-llama/Llama-2-7b-chat-hf&#xA;./scripts/prepare.sh $MODEL_REPO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;Benchmarks run on an A100-80GB, power limited to 330W.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Technique&lt;/th&gt; &#xA;   &lt;th&gt;Tokens/Second&lt;/th&gt; &#xA;   &lt;th&gt;Memory Bandwidth (GB/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;104.9&lt;/td&gt; &#xA;   &lt;td&gt;1397.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8-bit&lt;/td&gt; &#xA;   &lt;td&gt;155.58&lt;/td&gt; &#xA;   &lt;td&gt;1069.20&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4-bit (G=32)&lt;/td&gt; &#xA;   &lt;td&gt;196.80&lt;/td&gt; &#xA;   &lt;td&gt;862.69&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-70B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8-bit&lt;/td&gt; &#xA;   &lt;td&gt;19.13&lt;/td&gt; &#xA;   &lt;td&gt;1322.58&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4-bit (G=32)&lt;/td&gt; &#xA;   &lt;td&gt;25.25&lt;/td&gt; &#xA;   &lt;td&gt;1097.66&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Speculative Sampling&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/pytorch-labs/gpt-fast/main/scripts/speculate_70B_int4.sh&#34;&gt;Verifier: Llama-70B (int4), Draft: Llama-7B (int4)&lt;/a&gt;: 48.4 tok/s&lt;/p&gt; &#xA;&lt;h3&gt;Tensor Parallelism&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Number of GPUs&lt;/th&gt; &#xA;   &lt;th&gt;Tokens/Second&lt;/th&gt; &#xA;   &lt;th&gt;Memory Bandwidth (GB/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;104.9&lt;/td&gt; &#xA;   &lt;td&gt;1397.31&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;136.27&lt;/td&gt; &#xA;   &lt;td&gt;954.01&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;168.78&lt;/td&gt; &#xA;   &lt;td&gt;635.09&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;179.27&lt;/td&gt; &#xA;   &lt;td&gt;395.85&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-70B&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;OOM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2&lt;/td&gt; &#xA;   &lt;td&gt;20.53&lt;/td&gt; &#xA;   &lt;td&gt;1426.41&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;4&lt;/td&gt; &#xA;   &lt;td&gt;34.15&lt;/td&gt; &#xA;   &lt;td&gt;1204.62&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8&lt;/td&gt; &#xA;   &lt;td&gt;47.25&lt;/td&gt; &#xA;   &lt;td&gt;858.28&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;AMD&lt;/h3&gt; &#xA;&lt;p&gt;Benchmarks run on one GCD of a MI-250x.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Technique&lt;/th&gt; &#xA;   &lt;th&gt;Tokens/Second&lt;/th&gt; &#xA;   &lt;th&gt;Memory Bandwidth (GB/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama-2-7B&lt;/td&gt; &#xA;   &lt;td&gt;Base&lt;/td&gt; &#xA;   &lt;td&gt;76.33&lt;/td&gt; &#xA;   &lt;td&gt;1028.70&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;8-bit&lt;/td&gt; &#xA;   &lt;td&gt;101.86&lt;/td&gt; &#xA;   &lt;td&gt;700.06&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Generate Text&lt;/h2&gt; &#xA;&lt;p&gt;Model definition in &lt;code&gt;model.py&lt;/code&gt;, generation code in &lt;code&gt;generate.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model.pth --prompt &#34;Hello, my name is&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To squeeze out a little bit more performance, you can also compile the prefill with &lt;code&gt;--compile_prefill&lt;/code&gt;. This will increase compilation times though.&lt;/p&gt; &#xA;&lt;h2&gt;Quantization&lt;/h2&gt; &#xA;&lt;h3&gt;Int8 Weight-Only Quantization&lt;/h3&gt; &#xA;&lt;p&gt;To generate this version of the model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Spits out model at checkpoints/$MODEL_REPO/model_int8.pth&#xA;python quantize.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --mode int8&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run with int8, just pass the int8 checkpoint to generate.py.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model_int8.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Int4 Weight-Only Quantization&lt;/h3&gt; &#xA;&lt;p&gt;To generate int4 version of model&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Spits out model at checkpoints/$MODEL_REPO/model_int4.g32.pth&#xA;python quantize.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --mode int4 --groupsize 32&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To run with int4, just pass the int4 checkpoint to generate.py.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python generate.py --checkpoint_path checkpoints/$MODEL_REPO/model_int4.g32.pth --compile&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Speculative Sampling&lt;/h2&gt; &#xA;&lt;p&gt;To generate with speculative sampling (DRAFT_MODEL_REPO should point to a smaller model compared with MODEL_REPO).&lt;/p&gt; &#xA;&lt;p&gt;In this example, the &#34;smaller&#34; model is just the int8 quantized version of the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export DRAFT_MODEL_REPO=meta-llama/Llama-2-7b-chat-hf&#xA;python generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model.pth --draft_checkpoint_path checkpoints/$DRAFT_MODEL_REPO/model_int8.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Running on an A100 80GB, albeit power-limited to 330 watts. Empirically, seems like peak bandwidth is about 1700 GB/s.&lt;/p&gt; &#xA;&lt;h2&gt;Tensor Parallelism&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;torchrun --standalone --nproc_per_node=2 generate.py --compile --checkpoint_path checkpoints/$MODEL_REPO/model.pth&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Experimental&lt;/h2&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;We use the EleutherAI evaluation harness to evaluate our model accuracy. To evaluate the accuracy, make sure the evaluation harness is installed and pass your model checkpoint and desired tasks to eval.py.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python eval.py --checkpoint_path checkpoints/$MODEL_REPO/model.pth --compile --tasks hellaswag winogrande&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: Generative tasks are currently not supported for gpt-fast&lt;/p&gt; &#xA;&lt;p&gt;Installation Instructions for the evaluation harness: &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness/tree/master#install&#34;&gt;https://github.com/EleutherAI/lm-evaluation-harness/tree/master#install&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;GPTQ&lt;/h3&gt; &#xA;&lt;p&gt;We have a pure pytorch implementation of GPTQ that utilizes torch._dynamo.export to access the model structure. You can generate a GPTQ quantized version of int4 quantization by using the same command to quantize it but adding &#39;gptq&#39; to the quantization mode i.e.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Spits out model at checkpoints/$MODEL_REPO/model_int4-gptq.g32.pth&#xA;python quantize.py --mode int4-gptq --calibration_tasks wikitext --calibration_seq_length 2048&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can then eval or generate text with this model in the same way as above.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;gpt-fast&lt;/code&gt; is released under the &lt;a href=&#34;https://github.com/pytorch-labs/gpt-fast/main/LICENSE&#34;&gt;BSD 3&lt;/a&gt; license.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Lightning AI for supporting pytorch and work in flash attention, int8 quantization, and LoRA fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;GGML for driving forward fast, on device inference of LLMs&lt;/li&gt; &#xA; &lt;li&gt;Karpathy for spearheading simple, interpretable and fast LLM implementations&lt;/li&gt; &#xA; &lt;li&gt;MLC-LLM for pushing 4-bit quantization performance on heterogenous hardware&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>laravel/pulse</title>
    <updated>2023-12-04T01:25:12Z</updated>
    <id>tag:github.com,2023-12-04:/laravel/pulse</id>
    <link href="https://github.com/laravel/pulse" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Laravel Pulse is a real-time application performance monitoring tool and dashboard for your Laravel application.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/laravel/pulse/1.x/art/logo.svg?sanitize=true&#34; alt=&#34;Laravel Pulse Logo&#34;&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/laravel/pulse/actions&#34;&gt;&lt;img src=&#34;https://github.com/laravel/pulse/workflows/tests/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://packagist.org/packages/laravel/pulse&#34;&gt;&lt;img src=&#34;https://img.shields.io/packagist/dt/laravel/pulse&#34; alt=&#34;Total Downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://packagist.org/packages/laravel/pulse&#34;&gt;&lt;img src=&#34;https://img.shields.io/packagist/v/laravel/pulse&#34; alt=&#34;Latest Stable Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://packagist.org/packages/laravel/pulse&#34;&gt;&lt;img src=&#34;https://img.shields.io/packagist/l/laravel/pulse&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;Laravel Pulse is a real-time application performance monitoring tool and dashboard for your Laravel application.&lt;/p&gt; &#xA;&lt;h2&gt;Official Documentation&lt;/h2&gt; &#xA;&lt;p&gt;Documentation for Pulse can be found on the &lt;a href=&#34;https://laravel.com/docs/pulse&#34;&gt;Laravel website&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Thank you for considering contributing to Pulse! The contribution guide can be found in the &lt;a href=&#34;https://laravel.com/docs/contributions&#34;&gt;Laravel documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code of Conduct&lt;/h2&gt; &#xA;&lt;p&gt;In order to ensure that the Laravel community is welcoming to all, please review and abide by the &lt;a href=&#34;https://laravel.com/docs/contributions#code-of-conduct&#34;&gt;Code of Conduct&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Security Vulnerabilities&lt;/h2&gt; &#xA;&lt;p&gt;Please review &lt;a href=&#34;https://github.com/laravel/pulse/security/policy&#34;&gt;our security policy&lt;/a&gt; on how to report security vulnerabilities.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Laravel Pulse is open-sourced software licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/laravel/pulse/1.x/LICENSE.md&#34;&gt;MIT license&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cubiq/ComfyUI_IPAdapter_plus</title>
    <updated>2023-12-04T01:25:12Z</updated>
    <id>tag:github.com,2023-12-04:/cubiq/ComfyUI_IPAdapter_plus</id>
    <link href="https://github.com/cubiq/ComfyUI_IPAdapter_plus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ComfyUI IPAdapter plus&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt; reference implementation for &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter/&#34;&gt;IPAdapter&lt;/a&gt; models.&lt;/p&gt; &#xA;&lt;p&gt;IPAdapter implementation that follows the ComfyUI way of doing things. The code is memory efficient, fast, and shouldn&#39;t break with Comfy updates.&lt;/p&gt; &#xA;&lt;h2&gt;Important updates&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/29&lt;/strong&gt;: Added &lt;code&gt;unfold_batch&lt;/code&gt; option to send the reference images sequentially to a latent batch. Useful for animations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/26&lt;/strong&gt;: Added &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/#timestepping&#34;&gt;timestepping&lt;/a&gt;. You may need to delete the old nodes and recreate them. &lt;strong&gt;Important:&lt;/strong&gt; For this to work you need to update ComfyUI to the latest version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/24&lt;/strong&gt;: Support for multiple attention masks.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/23&lt;/strong&gt;: Small but important update: the new default location for the IPAdapter models is &lt;code&gt;ComfyUI/models/ipadapter&lt;/code&gt;. &lt;strong&gt;No panic&lt;/strong&gt;: the legacy &lt;code&gt;ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/models&lt;/code&gt; location still works and nothing will break.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/08&lt;/strong&gt;: Added &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/#attention-masking&#34;&gt;attention masking&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/07&lt;/strong&gt;: Added three ways to apply the weight. &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/#weight-types&#34;&gt;See below&lt;/a&gt; for more info. &lt;strong&gt;This might break things!&lt;/strong&gt; Please let me know if you are having issues. When loading an old workflow try to reload the page a couple of times or delete the &lt;code&gt;IPAdapter Apply&lt;/code&gt; node and insert a new one.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/11/02&lt;/strong&gt;: Added compatibility with the new models in safetensors format (available on &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;huggingface&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/10/12&lt;/strong&gt;: Added image weighting in the &lt;code&gt;IPAdapterEncoder&lt;/code&gt; node. This update is somewhat breaking; if you use &lt;code&gt;IPAdapterEncoder&lt;/code&gt; and &lt;code&gt;PrepImageForClipVision&lt;/code&gt; nodes you need to remove them from your workflow, refresh and recreate them. In the examples you&#39;ll find a &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_weighted.json&#34;&gt;workflow&lt;/a&gt; for weighted images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2023/9/29&lt;/strong&gt;: Added save/load of encoded images. Fix minor bugs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;(previous updates removed for better readability)&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;What is it?&lt;/h2&gt; &#xA;&lt;p&gt;The IPAdapter are very powerful models for image-to-image conditioning. Given a reference image you can do variations augmented by text prompt, controlnets and masks. Think of it as a 1-image lora.&lt;/p&gt; &#xA;&lt;h2&gt;Example workflow&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/ipadapter_workflow.png&#34; alt=&#34;IPAdapter Example workflow&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Video Introduction&lt;/h2&gt; &#xA;&lt;a href=&#34;https://youtu.be/7m9ZZFU3HWo&#34; target=&#34;_blank&#34;&gt; &lt;img src=&#34;https://img.youtube.com/vi/7m9ZZFU3HWo/hqdefault.jpg&#34; alt=&#34;Watch the video&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ðŸ¤“&lt;/span&gt; &lt;a href=&#34;https://youtu.be/7m9ZZFU3HWo&#34;&gt;Basic usage video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ðŸš€&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=mJQ62ly7jrg&#34;&gt;Advanced features video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ðŸ‘º&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=vqG1VXKteQg&#34;&gt;Attention Masking video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;ðŸŽ¥&lt;/span&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=ddYbhv3WgWw&#34;&gt;Animation Features video&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download or git clone this repository inside &lt;code&gt;ComfyUI/custom_nodes/&lt;/code&gt; directory.&lt;/p&gt; &#xA;&lt;p&gt;The pre-trained models are available on &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;huggingface&lt;/a&gt;, download and place them in the &lt;code&gt;ComfyUI/models/ipadapter&lt;/code&gt; directory (create it if not present). You can also use any custom location setting an &lt;code&gt;ipadapter&lt;/code&gt; entry in the &lt;code&gt;extra_model_paths.yaml&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;Note: the legacy &lt;code&gt;ComfyUI/custom_nodes/ComfyUI_IPAdapter_plus/models&lt;/code&gt; is still supported and it will be ignored only if the global directory is present.&lt;/p&gt; &#xA;&lt;p&gt;For SD1.5 you need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15.bin&#34;&gt;ip-adapter_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/blob/main/models/ip-adapter_sd15_light.safetensors&#34;&gt;ip-adapter_sd15_light.bin&lt;/a&gt;, use this when text prompt is more important than reference images&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus_sd15.bin&#34;&gt;ip-adapter-plus_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-plus-face_sd15.bin&#34;&gt;ip-adapter-plus-face_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter-full-face_sd15.bin&#34;&gt;ip-adapter-full-face_sd15.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/ip-adapter_sd15_vit-G.bin&#34;&gt;ip-adapter_sd15_vit-G.bin&lt;/a&gt;, this model requires the vit-bigG image encoder (the SDXL one below)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For SDXL you need:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl.bin&#34;&gt;ip-adapter_sdxl.bin&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter_sdxl_vit-h.bin&#34;&gt;ip-adapter_sdxl_vit-h.bin&lt;/a&gt; &lt;strong&gt;This model requires the use of the SD1.5 encoder despite being for SDXL checkpoints&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus_sdxl_vit-h.bin&#34;&gt;ip-adapter-plus_sdxl_vit-h.bin&lt;/a&gt; Same as above, use the SD1.5 encoder&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/ip-adapter-plus-face_sdxl_vit-h.bin&#34;&gt;ip-adapter-plus-face_sdxl_vit-h.bin&lt;/a&gt; As always, use the SD1.5 encoder&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please note that now the models are also available in safetensors format, you can find them on &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;huggingface&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Additionally you need the image encoders to be placed in the &lt;code&gt;ComfyUI/models/clip_vision/&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/models/image_encoder/model.safetensors&#34;&gt;SD 1.5 model&lt;/a&gt; (use this also for all models ending with &lt;strong&gt;_vit-h&lt;/strong&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/h94/IP-Adapter/resolve/main/sdxl_models/image_encoder/model.safetensors&#34;&gt;SDXL model&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can rename them to something easier to remember or put them into a sub-directory.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the image encoders are actually &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K&#34;&gt;ViT-H&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k&#34;&gt;ViT-bigG&lt;/a&gt; (used only for one SDXL model). You probably already have them.&lt;/p&gt; &#xA;&lt;h2&gt;How to&lt;/h2&gt; &#xA;&lt;p&gt;There&#39;s a basic workflow included in this repo and a few examples in the &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/&#34;&gt;examples&lt;/a&gt; directory. Usually it&#39;s a good idea to lower the &lt;code&gt;weight&lt;/code&gt; to at least &lt;code&gt;0.8&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;noise&lt;/code&gt; paramenter is an experimental exploitation of the IPAdapter models. You can set it as low as &lt;code&gt;0.01&lt;/code&gt; for an arguably better result.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;strong&gt;More info about the noise option&lt;/strong&gt;&lt;/summary&gt; &#xA; &lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/noise_example.jpg&#34; width=&#34;100%&#34; alt=&#34;canny controlnet&#34;&gt; &#xA; &lt;p&gt;Basically the IPAdapter sends two pictures for the conditioning, one is the reference the other --that you don&#39;t see-- is an empty image that could be considered like a negative conditioning.&lt;/p&gt; &#xA; &lt;p&gt;What I&#39;m doing is to send a very noisy image instead of an empty one. The &lt;code&gt;noise&lt;/code&gt; parameter determines the amount of noise that is added. A value of &lt;code&gt;0.01&lt;/code&gt; adds a lot of noise (more noise == less impact becaue the model doesn&#39;t get it); a value of &lt;code&gt;1.0&lt;/code&gt; removes most of noise so the generated image gets conditioned more.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;Preparing the reference image&lt;/h3&gt; &#xA;&lt;p&gt;The reference image needs to be encoded by the CLIP vision model. The encoder resizes the image to 224Ã—224 &lt;strong&gt;and crops it to the center!&lt;/strong&gt;. It&#39;s not an IPAdapter thing, it&#39;s how the clip vision works. This means that if you use a portrait or landscape image and the main attention (eg: the face of a character) is not in the middle you&#39;ll likely get undesired results. Use square pictures as reference for more predictable results.&lt;/p&gt; &#xA;&lt;p&gt;I&#39;ve added a &lt;code&gt;PrepImageForClipVision&lt;/code&gt; node that does all the required operations for you. You just have to select the crop position (top/left/center/etc...) and a sharpening amount if you want.&lt;/p&gt; &#xA;&lt;p&gt;In the image below you can see the difference between prepped and not prepped images.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/prep_images.jpg&#34; width=&#34;100%&#34; alt=&#34;prepped images&#34;&gt; &#xA;&lt;h3&gt;KSampler configuration suggestions&lt;/h3&gt; &#xA;&lt;p&gt;The IPAdapter generally requires a few more &lt;code&gt;steps&lt;/code&gt; than usual, if the result is underwhelming try to add 10+ steps. The model tends to burn the images a little. If needed lower the CFG scale.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;noise&lt;/code&gt; option generally grants better results, experiment with it.&lt;/p&gt; &#xA;&lt;h3&gt;IPAdapter + ControlNet&lt;/h3&gt; &#xA;&lt;p&gt;The model is very effective when paired with a ControlNet. In the example below I experimented with Canny. &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_Canny.json&#34;&gt;The workflow&lt;/a&gt; is in the examples directory.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/canny_controlnet.jpg&#34; width=&#34;100%&#34; alt=&#34;canny controlnet&#34;&gt; &#xA;&lt;h3&gt;IPAdapter Face&lt;/h3&gt; &#xA;&lt;p&gt;IPAdapter offers an interesting model for a kind of &#34;face swap&#34; effect. &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_face.json&#34;&gt;The workflow is provided&lt;/a&gt;. Set a close up face as reference image and then input your text prompt as always. The generated character should have the face of the reference. It also works with img2img given a high denoise.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/face_swap.jpg&#34; width=&#34;50%&#34; alt=&#34;face swap&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; there&#39;s a new &lt;code&gt;full-face&lt;/code&gt; model available that&#39;s arguably better.&lt;/p&gt; &#xA;&lt;h3&gt;Masking (Inpainting)&lt;/h3&gt; &#xA;&lt;p&gt;The most effective way to apply the IPAdapter to a region is by an &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_inpaint.json&#34;&gt;inpainting workflow&lt;/a&gt;. Remeber to use a specific checkpoint for inpainting otherwise it won&#39;t work. Even if you are inpainting a face I find that the &lt;em&gt;IPAdapter-Plus&lt;/em&gt; (not the &lt;em&gt;face&lt;/em&gt; one), works best.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/inpainting.jpg&#34; width=&#34;100%&#34; alt=&#34;inpainting&#34;&gt; &#xA;&lt;h3&gt;Image Batches&lt;/h3&gt; &#xA;&lt;p&gt;It is possible to pass multiple images for the conditioning with the &lt;code&gt;Batch Images&lt;/code&gt; node. An &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_batch_images.json&#34;&gt;example workflow&lt;/a&gt; is provided; in the picture below you can see the result of one and two images conditioning.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/batch_images.jpg&#34; width=&#34;100%&#34; alt=&#34;batcg images&#34;&gt; &#xA;&lt;p&gt;It seems to be effective with 2-3 images, beyond that it tends to &lt;em&gt;blur&lt;/em&gt; the information too much.&lt;/p&gt; &#xA;&lt;h3&gt;Image Weighting&lt;/h3&gt; &#xA;&lt;p&gt;When sending multiple images you can increase/decrease the weight of each image by using the &lt;code&gt;IPAdapterEncoder&lt;/code&gt; node. The workflow (&lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_weighted.json&#34;&gt;included in the examples&lt;/a&gt;) looks like this:&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/image_weighting.jpg&#34; width=&#34;100%&#34; alt=&#34;image weighting&#34;&gt; &#xA;&lt;p&gt;The node accepts 4 images, but remember that you can send batches of images to each slot.&lt;/p&gt; &#xA;&lt;h3&gt;Weight types&lt;/h3&gt; &#xA;&lt;p&gt;You can choose how the IPAdapter weight is applied to the image embeds. Options are:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;original&lt;/strong&gt;: The weight is applied to the aggregated tensors. The weight works predictably for values greater and lower than 1.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;linear&lt;/strong&gt;: The weight is applied to the individual tensors before aggretating them. Compared to &lt;code&gt;original&lt;/code&gt; the influence is weaker when weight is &amp;lt;1 and stronger when &amp;gt;1. &lt;strong&gt;Note:&lt;/strong&gt; at weight &lt;code&gt;1&lt;/code&gt; the two methods are equivalent.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;channel penalty&lt;/strong&gt;: This method is a modified version of Lvmin Zhang&#39;s (Fooocus). Results are sometimes sharper. It works very well also when weight is &amp;gt;1. Still experimental, may change in the future.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The image below shows the difference (zoom in).&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/weight_types.jpg&#34; width=&#34;100%&#34; alt=&#34;weight types&#34;&gt; &#xA;&lt;p&gt;In the examples directory you can find &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_weight_types.json&#34;&gt;a workflow&lt;/a&gt; that lets you easily compare the three methods.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I&#39;m not still sure whether all methods will stay. &lt;code&gt;Linear&lt;/code&gt; seems the most sensible but I wanted to keep the &lt;code&gt;original&lt;/code&gt; for backward compatibility. &lt;code&gt;channel penalty&lt;/code&gt; has a weird non-commercial clause but it&#39;s still part of a GNU GPLv3 software (ie: there&#39;s a licensing clash) so I&#39;m trying to understand how to deal with that.&lt;/p&gt; &#xA;&lt;h3&gt;Attention masking&lt;/h3&gt; &#xA;&lt;p&gt;It&#39;s possible to add a mask to define the area where the IPAdapter will be applied to. Everything outside the mask will ignore the reference images and will only listen to the text prompt.&lt;/p&gt; &#xA;&lt;p&gt;It is suggested to use a mask of the same size of the final generated image.&lt;/p&gt; &#xA;&lt;p&gt;In the picture below I use two reference images masked one on the left and the other on the right. The image is generated only with IPAdapter and one ksampler (without in/outpainting or area conditioning).&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/masking.jpg&#34; width=&#34;512&#34; alt=&#34;masking&#34;&gt; &#xA;&lt;p&gt;It is also possible to send a batch of masks that will be applied to a batch of latents, one per frame. The size should be the same but if needed some normalization will be performed to avoid errors. This feature also supports (experimentally) AnimateDiff including context sliding.&lt;/p&gt; &#xA;&lt;p&gt;In the examples directory you&#39;ll find a couple of masking workflows: &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_mask.json&#34;&gt;simple&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/IPAdapter_2_masks.json&#34;&gt;two masks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Timestepping&lt;/h3&gt; &#xA;&lt;p&gt;In the &lt;code&gt;Apply IPAdapter&lt;/code&gt; node you can set a start and an end point. The IPAdapter will be applied exclusively in that timeframe of the generation. This is a very powerful tool to modulate the intesity of IPAdapter models.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/cubiq/ComfyUI_IPAdapter_plus/main/examples/timestepping.jpg&#34; width=&#34;100%&#34; alt=&#34;timestepping&#34;&gt; &#xA;&lt;h2&gt;Troubleshooting&lt;/h2&gt; &#xA;&lt;p&gt;Please check the &lt;a href=&#34;https://github.com/cubiq/ComfyUI_IPAdapter_plus/issues/108&#34;&gt;troubleshooting&lt;/a&gt; before posting a new issue.&lt;/p&gt; &#xA;&lt;h2&gt;Diffusers version&lt;/h2&gt; &#xA;&lt;p&gt;If you are interested I&#39;ve also implemented the same features for &lt;a href=&#34;https://github.com/cubiq/Diffusers_IPAdapter&#34;&gt;Huggingface Diffusers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter/&#34;&gt;IPAdapter&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/laksjdjf/IPAdapter-ComfyUI/&#34;&gt;laksjdjf&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lllyasviel/Fooocus/raw/main/fooocus_extras/ip_adapter.py&#34;&gt;fooocus&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;IPAdapter in the wild&lt;/h2&gt; &#xA;&lt;p&gt;Let me know if you spot the IPAdapter in the wild or tag @latentvision in the video description!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;For German speakers you can find interesting YouTube tutorials on &lt;a href=&#34;https://www.youtube.com/watch?v=rAWn_0YOBU0&#34;&gt;A Latent Place&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;In Chinese &lt;a href=&#34;https://www.youtube.com/watch?v=xl8f3oxZgY8&#34;&gt;Introversify&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=xzGdynQDzsM&#34;&gt;Scott Detweiler&lt;/a&gt; covered this extension.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>