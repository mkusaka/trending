<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-31T01:30:41Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ClassmateLin/dm-ticket</title>
    <updated>2023-05-31T01:30:41Z</updated>
    <id>tag:github.com,2023-05-31:/ClassmateLin/dm-ticket</id>
    <link href="https://github.com/ClassmateLin/dm-ticket" rel="alternate"></link>
    <summary type="html">&lt;p&gt;大麦网自动购票, 支持docker一键部署。https://t.me/+2EELgNTYiMYxMTFl&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;dm-ticket&lt;/h1&gt; &#xA;&lt;h2&gt;简介&lt;/h2&gt; &#xA;&lt;p&gt;大麦网自动购票, 支持docker一键部署。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;友情提示:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;请先将README看完, 会避免一些问题。&lt;/li&gt; &#xA; &lt;li&gt;如有问题，大部分可以在issue中找到答案。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;特别声明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本项目内所有资源文件，禁止任何公众号、自媒体进行任何形式的转载、发布。&lt;/li&gt; &#xA; &lt;li&gt;编写本项目主要目的为学习和研究Rust，无法保证项目内容的合法性、准确性、完整性和有效性。&lt;/li&gt; &#xA; &lt;li&gt;本项目涉及的数据由使用的个人或组织自行填写，本项目不对数据内容负责，包括但不限于数据的真实性、准确性、合法性。使用本项目所造成的一切后果，与本项目的所有贡献者无关，由使用的个人或组织完全承担。&lt;/li&gt; &#xA; &lt;li&gt;本项目中涉及的第三方硬件、软件等，与本项目没有任何直接或间接的关系。本项目仅对部署和使用过程进行客观描述，不代表支持使用任何第三方硬件、软件。使用任何第三方硬件、软件，所造成的一切后果由使用的个人或组织承担，与本项目无关。&lt;/li&gt; &#xA; &lt;li&gt;本项目中所有内容只供学习和研究使用，不得将本项目中任何内容用于违反国家/地区/组织等的法律法规或相关规定的其他用途。&lt;/li&gt; &#xA; &lt;li&gt;所有基于本项目源代码，进行的任何修改，为其他个人或组织的自发行为，与本项目没有任何直接或间接的关系，所造成的一切后果亦与本项目无关。&lt;/li&gt; &#xA; &lt;li&gt;所有直接或间接使用本项目的个人和组织，应24小时内完成学习和研究，并及时删除本项目中的所有内容。如对本项目的功能有需求，应自行开发相关功能。&lt;/li&gt; &#xA; &lt;li&gt;本项目保留随时对免责声明进行补充或更改的权利，直接或间接使用本项目内容的个人或组织，视为接受本项目的特别声明。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用说明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;下载docker-compose配置文件: &lt;code&gt;wget https://github.com/ClassmateLin/dm-ticket/releases/download/v0.1.0/dm-ticket.zip&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;解压zip: &lt;code&gt;unzip dm-ticket.zip &amp;amp;&amp;amp; cd dm-ticket&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;运行容器: &lt;code&gt;docker-compose up -d&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;修改配置: &lt;code&gt;vim config/config.yaml&lt;/code&gt;, 配置项在config/config.yaml中有详细注释。&lt;/li&gt; &#xA; &lt;li&gt;运行脚本: &lt;code&gt;docker exec -it dm-ticket dm-ticket&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;sample 1: &lt;img src=&#34;https://raw.githubusercontent.com/ClassmateLin/dm-ticket/main/images/run.png&#34; alt=&#34;run.png&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ClassmateLin/dm-ticket/main/images/run_res.jpeg&#34; alt=&#34;run_res.png&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;sample 2: &lt;img src=&#34;https://raw.githubusercontent.com/ClassmateLin/dm-ticket/main/images/run2.png&#34; alt=&#34;run2.png&#34;&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;命令列表&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;自动购票: &lt;code&gt;docker exec -it dm-ticket dm-ticket&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;扫码登录: &lt;code&gt;docker exec -it dm-ticket dm-login&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;常见问题&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;如遇到&lt;code&gt;Connection refused (os error 111)&lt;/code&gt;错误, 说明token-server还没启动完成, 等待片刻即可。 &lt;img src=&#34;https://raw.githubusercontent.com/ClassmateLin/dm-ticket/main/images/connection_errors.png&#34; alt=&#34;Connection refused (os error 111)&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;生成订单失败, [&#34;RGV587_ERROR::SM::哎哟喂,被挤爆啦,请稍后重试!&#34;], 请检查是否复制了完整的cookie, ip有问题(一般是使用了大厂服务器/代理, 实在需要可以使用这种&lt;a href=&#34;https://www.fwvps.com/?aff=6bb13&#34;&gt;动态VPS&lt;/a&gt;)。&lt;/li&gt; &#xA; &lt;li&gt;[&#34;B-00203-200-100::网络开小差了，再试一次吧~&#34;], 请检查是否复制了完整的cookie。&lt;/li&gt; &#xA; &lt;li&gt;docker/docker-compose安装使用问题，请善用搜索引擎, 自行搜索解决方案。&lt;/li&gt; &#xA; &lt;li&gt;是否支持多账号, v0.1.0版本是支持多账号的。后续可能取消。要实现多账号支持, 开启多个docker容器也可以支持。&lt;/li&gt; &#xA; &lt;li&gt;频繁尝试运行程序出现, [&#34;RGV587_ERROR::SM::哎哟喂,被挤爆啦,请稍后重试!&#34;]。请重新登陆。&lt;/li&gt; &#xA; &lt;li&gt;仅支持&lt;a href=&#34;https://m.daima.cn&#34;&gt;H5端&lt;/a&gt;可以购买的票。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;其他说明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;如何获取cookie?&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;登录&lt;a href=&#34;https://m.damai.cn/&#34;&gt;大麦网&lt;/a&gt;, F12打开控制台查看网络请求, 复制请求中的cookie。 &lt;img src=&#34;https://raw.githubusercontent.com/ClassmateLin/dm-ticket/main/images/cookie.png&#34; alt=&#34;img.png&#34;&gt;&lt;/li&gt; &#xA;   &lt;li&gt;使用扫码登录: &lt;code&gt;docker exec -it dm-ticket dm-login&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;如何获取演唱会id？&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;进入门票详情, 复制URL中的itemId。 &lt;img src=&#34;https://raw.githubusercontent.com/ClassmateLin/dm-ticket/main/images/ticket.png&#34; alt=&#34;ticket_id&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;如何获取场次？&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;点击购买按钮, 弹出的场次。第一个就是1, 以此类推。 &lt;img src=&#34;https://raw.githubusercontent.com/ClassmateLin/dm-ticket/main/images/session_id.png&#34; alt=&#34;img.png&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;如何获取票档?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;选择场次之后, 弹出票档信息, 从左到右, 从上到下, 从1开始递增。如图: &lt;img src=&#34;https://raw.githubusercontent.com/ClassmateLin/dm-ticket/main/images/grade.png&#34; alt=&#34;img.png&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;实名信息怎么选择?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;按实名信息顺序, 自动选择。 如购买2张票, 默认选择前两位实名人。&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 扫码登录&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 捡漏功能&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 其他...&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;赞赏&lt;/h2&gt; &#xA;&lt;p&gt;如果我的项目对你有帮助, 可以通过以下方式支持我:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;点个star。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;又或者:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/ClassmateLin/dm-ticket/main/images/pay.jpeg&#34; width=&#34;256px;&#34;&gt;</summary>
  </entry>
  <entry>
    <title>sjvasquez/handwriting-synthesis</title>
    <updated>2023-05-31T01:30:41Z</updated>
    <id>tag:github.com,2023-05-31:/sjvasquez/handwriting-synthesis</id>
    <link href="https://github.com/sjvasquez/handwriting-synthesis" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Handwriting Synthesis with RNNs ✏️&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/banner.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Handwriting Synthesis&lt;/h1&gt; &#xA;&lt;p&gt;Implementation of the handwriting synthesis experiments in the paper &lt;a href=&#34;https://arxiv.org/abs/1308.0850&#34;&gt;Generating Sequences with Recurrent Neural Networks&lt;/a&gt; by Alex Graves. The implementation closely follows the original paper, with a few slight deviations, and the generated samples are of similar quality to those presented in the paper.&lt;/p&gt; &#xA;&lt;p&gt;Web demo is available &lt;a href=&#34;https://seanvasquez.com/handwriting-generation/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;lines = [&#xA;    &#34;Now this is a story all about how&#34;,&#xA;    &#34;My life got flipped turned upside down&#34;,&#xA;    &#34;And I&#39;d like to take a minute, just sit right there&#34;,&#xA;    &#34;I&#39;ll tell you how I became the prince of a town called Bel-Air&#34;,&#xA;]&#xA;biases = [.75 for i in lines]&#xA;styles = [9 for i in lines]&#xA;stroke_colors = [&#39;red&#39;, &#39;green&#39;, &#39;black&#39;, &#39;blue&#39;]&#xA;stroke_widths = [1, 2, 1, 2]&#xA;&#xA;hand = Hand()&#xA;hand.write(&#xA;    filename=&#39;img/usage_demo.svg&#39;,&#xA;    lines=lines,&#xA;    biases=biases,&#xA;    styles=styles,&#xA;    stroke_colors=stroke_colors,&#xA;    stroke_widths=stroke_widths&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/usage_demo.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Currently, the &lt;code&gt;Hand&lt;/code&gt; class must be imported from &lt;code&gt;demo.py&lt;/code&gt;. If someone would like to package this project to make it more usable, please &lt;a href=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/#contribute&#34;&gt;contribute&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;A pretrained model is included, but if you&#39;d like to train your own, read &lt;a href=&#34;https://github.com/sjvasquez/handwriting-synthesis/tree/master/data/raw&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Demonstrations&lt;/h2&gt; &#xA;&lt;p&gt;Below are a few hundred samples from the model, including some samples demonstrating the effect of priming and biasing the model. Loosely speaking, biasing controls the neatness of the samples and priming controls the style of the samples. The code for these demonstrations can be found in &lt;code&gt;demo.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Demo #1:&lt;/h3&gt; &#xA;&lt;p&gt;The following samples were generated with a fixed style and fixed bias.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Smash Mouth – All Star (&lt;a href=&#34;https://www.azlyrics.com/lyrics/smashmouth/allstar.html&#34;&gt;lyrics&lt;/a&gt;)&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/all_star.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo #2&lt;/h3&gt; &#xA;&lt;p&gt;The following samples were generated with varying style and fixed bias. Each verse is generated in a different style.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Vanessa Carlton – A Thousand Miles (&lt;a href=&#34;https://www.azlyrics.com/lyrics/vanessacarlton/athousandmiles.html&#34;&gt;lyrics&lt;/a&gt;)&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/downtown.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Demo #3&lt;/h3&gt; &#xA;&lt;p&gt;The following samples were generated with a fixed style and varying bias. Each verse has a lower bias than the previous, with the last verse being unbiased.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Leonard Cohen – Hallelujah (&lt;a href=&#34;https://www.youtube.com/watch?v=dQw4w9WgXcQ&#34;&gt;lyrics&lt;/a&gt;)&lt;/strong&gt; &lt;img src=&#34;https://raw.githubusercontent.com/sjvasquez/handwriting-synthesis/master/img/give_up.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;This project was intended to serve as a reference implementation for a research paper, but since the results are of decent quality, it may be worthwile to make the project more broadly usable. I plan to continue focusing on the machine learning side of things. That said, I&#39;d welcome contributors who can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Package this, and otherwise make it look more like a usable software project and less like research code.&lt;/li&gt; &#xA; &lt;li&gt;Add support for more sophisticated drawing, animations, or anything else in this direction. Currently, the project only creates some simple svg files.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>ShishirPatil/gorilla</title>
    <updated>2023-05-31T01:30:41Z</updated>
    <id>tag:github.com,2023-05-31:/ShishirPatil/gorilla</id>
    <link href="https://github.com/ShishirPatil/gorilla" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Gorilla: An API store for LLMs&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Gorilla: Large Language Model Connected with Massive APIs&lt;/h1&gt; &#xA;&lt;p&gt;By Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez (&lt;a href=&#34;https://shishirpatil.github.io/gorilla/&#34;&gt;Project Website&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/ShishirPatil/gorilla/raw/gh-pages/assets/img/logo.png&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;🗞&lt;/span&gt; Checkout our paper!&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2305.15334&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-2305.15334-%3CCOLOR%3E.svg?style=flat-square&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;👋&lt;/span&gt; Join our Discord!&lt;/strong&gt; &lt;a href=&#34;https://discord.gg/3apqwwME&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1111172801899012102?label=Discord&amp;amp;logo=discord&amp;amp;logoColor=green&amp;amp;style=flat-square&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;span&gt;🚀&lt;/span&gt; Try Gorilla in 60s&lt;/strong&gt; &lt;a href=&#34;https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Gorilla&lt;/code&gt; enables LLMs to use tools by invoking APIs. Given a natural language query, Gorilla comes up with the semantically- and syntactically- correct API to invoke. With Gorilla, we are the first to demonstrate how to use LLMs to invoke 1,600+ (and growing) API calls accurately while reducing hallucination. We also release APIBench, the largest collection of APIs, curated and easy to be trained on! Join us, as we try to expand the largest API store and teach LLMs how to write them! Hop on our Discord, or open a PR, or email us if you would like to have your API incorporated as well.&lt;/p&gt; &#xA;&lt;h2&gt;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;span&gt;🚀&lt;/span&gt; [05/30] Provided the &lt;a href=&#34;https://raw.githubusercontent.com/ShishirPatil/gorilla/main/inference/README.md&#34;&gt;CLI interface&lt;/a&gt; to chat with Gorilla!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🚀&lt;/span&gt; [05/28] Released Torch Hub and TensorFlow Hub Models!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🚀&lt;/span&gt; [05/27] Released the first Gorilla model! &lt;a href=&#34;https://colab.research.google.com/drive/1DEBPsccVLF_aUnmD0FwPeHFrtdC0QIUP?usp=sharing&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/gorilla-llm/gorilla-7b-hf-delta-v0&#34;&gt;&lt;span&gt;🤗&lt;/span&gt;&lt;/a&gt;!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🔥&lt;/span&gt; [05/27] We released the APIZoo contribution guide for community API contributions!&lt;/li&gt; &#xA; &lt;li&gt;&lt;span&gt;🔥&lt;/span&gt; [05/25] We release the APIBench dataset and the evaluation code of Gorilla!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Get Started&lt;/h2&gt; &#xA;&lt;p&gt;Inference: Run Gorilla locally &lt;a href=&#34;https://raw.githubusercontent.com/ShishirPatil/gorilla/main/inference/README.md&#34;&gt;&lt;code&gt;inference/README.md&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Evaluation: We have included prompts and responces for the APIBench with and without retrievers along with the Abstract Syntax Tree (AST) matching evaluation script at &lt;a href=&#34;https://github.com/ShishirPatil/gorilla/tree/main/eval&#34;&gt;evaluation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Repository Organization&lt;/h2&gt; &#xA;&lt;p&gt;Our repository organization is shown below.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The &lt;code&gt;data&lt;/code&gt; folder contains all the evaluation APIs &lt;code&gt;(APIBench)&lt;/code&gt; and the community contributed APIs.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;eval&lt;/code&gt; folder contains all our evaluation code as well as the Gorilla outputs.&lt;/li&gt; &#xA; &lt;li&gt;The &lt;code&gt;inference&lt;/code&gt; folder contains all the inference code for running Gorilla locally.&lt;/li&gt; &#xA; &lt;li&gt;&lt;span style=&#34;color:hr&#34;&gt;[Coming Soon!]&lt;/span&gt; The &lt;code&gt;train&lt;/code&gt; folder contains all the training code associated with Gorilla finetuning.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For our dataset collections, all the 1640 API documentation is in &lt;code&gt;data/api&lt;/code&gt;. We also include the &lt;code&gt;APIBench&lt;/code&gt; dataset created by self-instruct in &lt;code&gt;data/apibench&lt;/code&gt;. For evaluation, we convert this into a LLM-friendly chat format, and the questions are in &lt;code&gt;eval/eval-data/questions&lt;/code&gt;, and the corresponding responces are in &lt;code&gt;eval/eval-data/responses&lt;/code&gt;. We have also included the evaluation scripts are in &lt;code&gt;eval/eval-scripts&lt;/code&gt;. This would be entirely sufficient to train Gorilla yourself, and reproduce our results. Please see &lt;a href=&#34;https://github.com/ShishirPatil/gorilla/tree/main/eval&#34;&gt;evaluation&lt;/a&gt; for the details on how to use our evaluation pipeline.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, we have released all the model weights. &lt;code&gt;gorilla-7b-hf-v0&lt;/code&gt; lets you invoke over 925 Hugging Face APIs. Similarly, &lt;code&gt;gorilla-7b-tf-v0&lt;/code&gt; and &lt;code&gt;gorilla-7b-th-v0&lt;/code&gt; have 626 (exhaustive) Tensorflow v2, and 94 (exhaustive) Torch Hub APIs. We will release a model with all three combined with generic chat capability and community contributed APIs as soon as we can scale our serving infrastructure. You can run Gorilla locally from instructions in the &lt;code&gt;inference/&lt;/code&gt; sub-directory, or we also provide a hosted Gorilla chat completion API (see Colab)! If you have any suggestions, or if you run into any issues please feel free to reach out to us either through Discord or email or raise a Github issue.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;gorilla&#xA;├── data&#xA;│   ├── api (TF/HF/TH APIs used in generating apibench)&#xA;│   │   ├── {api_name}_api.jsonl&#xA;│   ├── apibench (Evaluating LLM models) v-1.0&#xA;│   │   ├── {api_name}_train.jsonl, {api_name}_eval.jsonl&#xA;|   |── apizoo (Contributed by the community - evolving)&#xA;│   |   ├── username1.json&#xA;│   │   ├── username2.json&#xA;│   │   ├── ...&#xA;├── eval&#xA;│   ├── README.md&#xA;│   ├── get_llm_responses.py&#xA;│   ├── eval-scripts&#xA;│   │   ├── ast_eval_{api_name}.py&#xA;│   ├── eval-data&#xA;│   │   ├── questions&#xA;│   │   │   ├── API name&#xA;│   │   │   │   ├── questions_{api_name}_{eval_metric}.jsonl&#xA;│   │   ├── responses&#xA;│   │   │   ├── API name&#xA;│   │   │   │   ├── responses_{api_name}_Gorilla_FT_{eval_metric}.jsonl&#xA;│   │   │   │   ├── responses_{api_name}_Gorilla_RT_{eval_metric}.jsonl&#xA;├── inference&#xA;│   ├── README.md&#xA;│   ├── serve&#xA;│   │   ├── gorilla_cli.py&#xA;│   │   ├── conv_template.py&#xA;├── train (Coming Soon!)&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing Your API&lt;/h2&gt; &#xA;&lt;p&gt;We aim to build an open-source, one-stop-shop for all APIs, LLMs can interact with! Any suggestions and contributions are welcome! Please see the details on &lt;a href=&#34;https://github.com/ShishirPatil/gorilla/tree/main/data/README.md&#34;&gt;how to contribute&lt;/a&gt;. THIS WILL ALWAYS REMAIN OPEN SOURCE.&lt;/p&gt; &#xA;&lt;h2&gt;FAQ(s)&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;I would like to use Gorilla commercially. Is there going to be a Apache 2.0 licensed version?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Yes! We are actively working on it. We will release a Gorilla model with Apache 2.0 license by Jun 5. Please stay tuned, and let us know if you are interested.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Can we use Gorilla with Langchain, Toolformer, AutoGPT etc?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Absolutely! You&#39;ve highlighted a great aspect of our tools. Gorilla is an end-to-end model, specifically tailored to serve correct API calls without requiring any additional coding. It&#39;s designed to work as part of a wider ecosystem and can be flexibly integrated with other tools.&lt;/p&gt; &#xA;&lt;p&gt;Langchain, is a versatile developer tool. Its &#34;agents&#34; can efficiently swap in any LLM, Gorilla included, making it a highly adaptable solution for various needs.&lt;/p&gt; &#xA;&lt;p&gt;AutoGPT, on the other hand, concentrates on the art of prompting GPT series models. It&#39;s worth noting that Gorilla, as a fully fine-tuned model, consistently shows remarkable accuracy, and lowers hallucination, outperforming GPT-4 in making specific API calls.&lt;/p&gt; &#xA;&lt;p&gt;Now, when it comes to ToolFormer, Toolformer zeroes in on a select set of tools, providing specialized functionalities. Gorilla, in contrast, has the capacity to manage thousands of API calls, offering a broader coverage over a more extensive range of tools.&lt;/p&gt; &#xA;&lt;p&gt;The beauty of these tools truly shines when they collaborate, complementing each other&#39;s strengths and capabilities to create an even more powerful and comprehensive solution. This is where your contribution can make a difference. We enthusiastically welcome any inputs to further refine and enhance these tools.&lt;/p&gt; &#xA;&lt;h2&gt;Project Roadmap&lt;/h2&gt; &#xA;&lt;p&gt;In the immediate future, we plan to release the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Dataset and Eval Code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Opening up the APIZoo for contributions from community&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Hosted Gorilla LLM chat for HF model APIs [May 27, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release weights for HF model APIs [May 27, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Run Gorilla LLM locally [May 28, 2023]&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release weights for all APIs from APIBench [May 28, 2023]&lt;/li&gt; &#xA; &lt;li&gt;[] Release a commercially usable, Apache 2.0 licensed Gorilla model [Jun 5, 2023]&lt;/li&gt; &#xA; &lt;li&gt;[] Train a model with first batch of community contributed APIs from APIZoo [Jun 5, 2023]&lt;/li&gt; &#xA; &lt;li&gt;[] Release training code [Jun 5, 2023]&lt;/li&gt; &#xA; &lt;li&gt;[] Train SOTA Gorilla LLM with expanded APIBench and APIZoo &lt;span&gt;🚀&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Propose a new task you would like to work on &lt;span&gt;🤩&lt;/span&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you use Gorilla or APIBench, please cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;@article{patil2023gorilla,&#xA;  title={Gorilla: Large Language Model Connected with Massive APIs},&#xA;  author={Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},&#xA;  year={2023},&#xA;  journal={arXiv preprint arXiv:2305.15334},&#xA;} &#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>