<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-12T01:30:23Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>CStanKonrad/long_llama</title>
    <updated>2023-07-12T01:30:23Z</updated>
    <id>tag:github.com,2023-07-12:/CStanKonrad/long_llama</id>
    <link href="https://github.com/CStanKonrad/long_llama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LongLLaMA is a large language model capable of handling long contexts. It is based on OpenLLaMA and fine-tuned with the Focused Transformer (FoT) method.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/assets/longllama.png&#34; alt=&#34;LongLLaMA&#34; style=&#34;width: 50%;  display: block; margin: auto;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;LongLLaMA: Focused Transformer Training for Context Scaling&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/CStanKonrad/long_llama/blob/main/long_llama_colab.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/#TLDR&#34;&gt;TLDR&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/#Overview&#34;&gt;Overview&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/#Usage&#34;&gt;Usage&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/#LongLLaMA-performance&#34;&gt;LongLLaMA performance&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/#Authors&#34;&gt;Authors&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/#Citation&#34;&gt;Citation&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/#License&#34;&gt;License&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/#Acknowledgments&#34;&gt;Acknowledgments&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;TLDR&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains the research preview of &lt;strong&gt;LongLLaMA, a large language model capable of handling long contexts of 256k tokens or even more&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;p&gt;LongLLaMA is built upon the foundation of &lt;a href=&#34;https://github.com/openlm-research/open_llama&#34;&gt;OpenLLaMA&lt;/a&gt; and fine-tuned using the &lt;a href=&#34;https://arxiv.org/abs/2307.03170&#34;&gt;Focused Transformer (FoT)&lt;/a&gt; method. We release a smaller 3B variant of the LongLLaMA model on a permissive license (Apache 2.0) and inference code supporting longer contexts on &lt;a href=&#34;https://huggingface.co/syzymon/long_llama_3b&#34;&gt;Hugging Face&lt;/a&gt;. Our model weights can serve as the drop-in replacement of LLaMA in existing implementations (for short context up to 2048 tokens). Additionally, we provide evaluation results and comparisons against the original OpenLLaMA models. Stay tuned for further updates.&lt;/p&gt; &#xA;&lt;h2&gt;Overview&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2307.03170&#34;&gt;Focused Transformer: Contrastive Training for Context Scaling&lt;/a&gt; (FoT) presents a simple method for endowing language models with the ability to handle context consisting possibly of millions of tokens while training on significantly shorter input. FoT permits a subset of attention layers to access a memory cache of (key, value) pairs to extend the context length. The distinctive aspect of FoT is its training procedure, drawing from contrastive learning. Specifically, we deliberately expose the memory attention layers to both relevant and irrelevant keys (like negative samples from unrelated documents). This strategy incentivizes the model to differentiate keys connected with semantically diverse values, thereby enhancing their structure. This, in turn, makes it possible to extrapolate the effective context length much beyond what is seen in training.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;LongLLaMA&lt;/strong&gt; is an &lt;a href=&#34;https://github.com/openlm-research/open_llama&#34;&gt;OpenLLaMA&lt;/a&gt; model finetuned with the FoT method, with three layers used for context extension. &lt;strong&gt;Crucially, LongLLaMA is able to extrapolate much beyond the context length seen in training: $8k$. E.g., in the passkey retrieval task, it can handle inputs of length $256k$&lt;/strong&gt;.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;&lt;/th&gt; &#xA;    &lt;th&gt;&lt;a href=&#34;https://huggingface.co/syzymon/long_llama_3b&#34;&gt;LongLLaMA-3B&lt;/a&gt;&lt;/th&gt; &#xA;    &lt;th&gt;LongLLaMA-7B&lt;br&gt;&lt;em&gt;(coming soon)&lt;/em&gt;&lt;/th&gt; &#xA;    &lt;th&gt;LongLLaMA-13B&lt;br&gt;&lt;em&gt;(coming soon)&lt;/em&gt;&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Source model&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openlm-research/open_llama_3b_easylm&#34;&gt;OpenLLaMA-3B&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Source model tokens&lt;/td&gt; &#xA;    &lt;td&gt;1T&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Fine-tuning tokens&lt;/td&gt; &#xA;    &lt;td&gt;10B&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Memory layers&lt;/td&gt; &#xA;    &lt;td&gt;6, 12, 18&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;    &lt;td&gt;-&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;See also: &lt;a href=&#34;https://colab.research.google.com/github/CStanKonrad/long_llama/blob/main/long_llama_colab.ipynb&#34;&gt;Colab with an example usage of LongLLaMA&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install --upgrade pip&#xA;pip install transformers==4.30  sentencepiece accelerate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Loading model&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import LlamaTokenizer, AutoModelForCausalLM&#xA;&#xA;tokenizer = LlamaTokenizer.from_pretrained(&#34;syzymon/long_llama_3b&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(&#34;syzymon/long_llama_3b&#34;, &#xA;                                            torch_dtype=torch.float32, &#xA;                                            trust_remote_code=True)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Input handling and generation&lt;/h3&gt; &#xA;&lt;p&gt;LongLLaMA uses the Hugging Face interface, the long input given to the model will be split into context windows and loaded into the memory cache.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;prompt = &#34;My name is Julien and I like to&#34;&#xA;input_ids = tokenizer(prompt, return_tensors=&#34;pt&#34;).input_ids&#xA;outputs = model(input_ids=input_ids)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;During the model call, one can provide the parameter &lt;code&gt;last_context_length&lt;/code&gt; (default $1024$), which specifies the number of tokens left in the last context window. Tuning this parameter can improve generation as the first layers do not have access to memory. See details in &lt;a href=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/#How-LongLLaMA-handles-long-inputs&#34;&gt;How LongLLaMA handles long inputs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;generation_output = model.generate(&#xA;    input_ids=input_ids,&#xA;    max_new_tokens=256,&#xA;    num_beams=1,&#xA;    last_context_length=1792,&#xA;    do_sample=True,&#xA;    temperature=1.0,&#xA;)&#xA;print(tokenizer.decode(generation_output[0]))&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Additional configuration&lt;/h3&gt; &#xA;&lt;p&gt;LongLLaMA has several other parameters:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;mem_layers&lt;/code&gt; specifies layers endowed with memory (should be either an empty list or a list of all memory layers specified in the description of the checkpoint).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mem_dtype&lt;/code&gt; allows changing the type of memory cache&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;mem_attention_grouping&lt;/code&gt; can trade off speed for reduced memory usage. When equal to &lt;code&gt;(4, 2048)&lt;/code&gt;, the memory layers will process at most $4*2048$ queries at once ($4$ heads and $2048$ queries for each head).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;from transformers import LlamaTokenizer, AutoModelForCausalLM&#xA;&#xA;tokenizer = LlamaTokenizer.from_pretrained(&#34;syzymon/long_llama_3b&#34;)&#xA;model = AutoModelForCausalLM.from_pretrained(&#xA;    &#34;syzymon/long_llama_3b&#34;, torch_dtype=torch.float32, &#xA;    mem_layers=[], &#xA;    mem_dtype=&#39;bfloat16&#39;,&#xA;    trust_remote_code=True,&#xA;    mem_attention_grouping=(4, 2048),&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Drop-in use with LLaMA code&lt;/h3&gt; &#xA;&lt;p&gt;LongLLaMA checkpoints can also be used as a drop-in replacement for LLaMA checkpoints in &lt;a href=&#34;https://huggingface.co/docs/transformers/main/model_doc/llama&#34;&gt;Hugging Face implementation of LLaMA&lt;/a&gt;, but in this case, they will be limited to the original context length of $2048$.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import LlamaTokenizer, LlamaForCausalLM&#xA;import torch&#xA;&#xA;tokenizer = LlamaTokenizer.from_pretrained(&#34;syzymon/long_llama_3b&#34;)&#xA;model = LlamaForCausalLM.from_pretrained(&#34;syzymon/long_llama_3b&#34;, torch_dtype=torch.float32)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;How LongLLaMA handles long inputs&lt;/h3&gt; &#xA;&lt;p&gt;Inputs over $2048$ tokens are automatically split into windows $w_1, \ldots, w_m$. The first $m-2$ windows contain $2048$ tokens each, $w_{m-1}$ has no more than $2048$ tokens, and $w_m$ contains the number of tokens specified by &lt;code&gt;last_context_length&lt;/code&gt;. The model processes the windows one by one extending the memory cache after each. If &lt;code&gt;use_cache&lt;/code&gt; is &lt;code&gt;True&lt;/code&gt;, the last window will not be loaded to the memory cache but to the local (generation) cache.&lt;/p&gt; &#xA;&lt;p&gt;The memory cache stores $(key, value)$ pairs for each head of the specified memory layers &lt;code&gt;mem_layers&lt;/code&gt;. In addition to this, it stores attention masks.&lt;/p&gt; &#xA;&lt;p&gt;If &lt;code&gt;use_cache=True&lt;/code&gt; (which is the case in generation), LongLLaMA will use two caches: the memory cache for the specified layers and the local (generation) cache for all layers. When the local cache exceeds $2048$ elements, its content is moved to the memory cache for the memory layers.&lt;/p&gt; &#xA;&lt;p&gt;For simplicity, context extension is realized with a memory cache and full attention in this repo. Replacing this simple mechanism with a KNN search over an external database is possible with systems like &lt;a href=&#34;https://github.com/facebookresearch/faiss&#34;&gt;Faiss&lt;/a&gt;. This potentially would enable further context length scaling. We leave this as a future work.&lt;/p&gt; &#xA;&lt;h2&gt;LongLLaMA performance&lt;/h2&gt; &#xA;&lt;p&gt;We present some illustrative examples of LongLLaMA results and refer to our paper &lt;a href=&#34;https://arxiv.org/abs/2307.03170&#34;&gt;Focused Transformer: Contrastive Training for Context Scaling&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;We manage to achieve good performance on the passkey retrieval task from &lt;a href=&#34;https://arxiv.org/abs/2305.16300&#34;&gt;Landmark Attention: Random-Access Infinite Context Length for Transformers&lt;/a&gt;. The code for generating the prompt and running the model is located in &lt;code&gt;examples/passkey.py&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/CStanKonrad/long_llama/main/assets/plot_passkey.png&#34; alt=&#34;LongLLaMA&#34; style=&#34;width: 70%; min-width: 300px; display: block; margin: auto;&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Our LongLLaMA 3B model also shows improvements when using long context on two downstream tasks, TREC question classification and WebQS question answering.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Context/Dataset&lt;/th&gt; &#xA;    &lt;th&gt;TREC&lt;/th&gt; &#xA;    &lt;th&gt;WebQS&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;$2K$&lt;/td&gt; &#xA;    &lt;td&gt;67.0&lt;/td&gt; &#xA;    &lt;td&gt;21.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;$4K$&lt;/td&gt; &#xA;    &lt;td&gt;71.6&lt;/td&gt; &#xA;    &lt;td&gt;21.4&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;$6K$&lt;/td&gt; &#xA;    &lt;td&gt;72.9&lt;/td&gt; &#xA;    &lt;td&gt;22.2&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;$8K$&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;73.3&lt;/strong&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;strong&gt;22.4&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;LongLLaMA retains performance on tasks that do not require long context. We provide a comparison with OpenLLaMA on &lt;a href=&#34;https://github.com/EleutherAI/lm-evaluation-harness&#34;&gt;lm-evaluation-harness&lt;/a&gt; in the zero-shot setting.&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Task/Metric&lt;/th&gt; &#xA;    &lt;th&gt;OpenLLaMA-3B&lt;/th&gt; &#xA;    &lt;th&gt;LongLLaMA-3B&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;anli_r1/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.33&lt;/td&gt; &#xA;    &lt;td&gt;0.32&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;anli_r2/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.32&lt;/td&gt; &#xA;    &lt;td&gt;0.33&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;anli_r3/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.35&lt;/td&gt; &#xA;    &lt;td&gt;0.35&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;arc_challenge/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.34&lt;/td&gt; &#xA;    &lt;td&gt;0.34&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;arc_challenge/acc_norm&lt;/td&gt; &#xA;    &lt;td&gt;0.37&lt;/td&gt; &#xA;    &lt;td&gt;0.37&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;arc_easy/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.69&lt;/td&gt; &#xA;    &lt;td&gt;0.68&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;arc_easy/acc_norm&lt;/td&gt; &#xA;    &lt;td&gt;0.65&lt;/td&gt; &#xA;    &lt;td&gt;0.63&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;boolq/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.68&lt;/td&gt; &#xA;    &lt;td&gt;0.68&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;hellaswag/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.49&lt;/td&gt; &#xA;    &lt;td&gt;0.48&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;hellaswag/acc_norm&lt;/td&gt; &#xA;    &lt;td&gt;0.67&lt;/td&gt; &#xA;    &lt;td&gt;0.65&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;openbookqa/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.27&lt;/td&gt; &#xA;    &lt;td&gt;0.28&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;openbookqa/acc_norm&lt;/td&gt; &#xA;    &lt;td&gt;0.40&lt;/td&gt; &#xA;    &lt;td&gt;0.38&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;piqa/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.75&lt;/td&gt; &#xA;    &lt;td&gt;0.73&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;piqa/acc_norm&lt;/td&gt; &#xA;    &lt;td&gt;0.76&lt;/td&gt; &#xA;    &lt;td&gt;0.75&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;record/em&lt;/td&gt; &#xA;    &lt;td&gt;0.88&lt;/td&gt; &#xA;    &lt;td&gt;0.87&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;record/f1&lt;/td&gt; &#xA;    &lt;td&gt;0.89&lt;/td&gt; &#xA;    &lt;td&gt;0.87&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;rte/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.58&lt;/td&gt; &#xA;    &lt;td&gt;0.60&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;truthfulqa_mc/mc1&lt;/td&gt; &#xA;    &lt;td&gt;0.22&lt;/td&gt; &#xA;    &lt;td&gt;0.24&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;truthfulqa_mc/mc2&lt;/td&gt; &#xA;    &lt;td&gt;0.35&lt;/td&gt; &#xA;    &lt;td&gt;0.38&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;wic/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.48&lt;/td&gt; &#xA;    &lt;td&gt;0.50&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;winogrande/acc&lt;/td&gt; &#xA;    &lt;td&gt;0.62&lt;/td&gt; &#xA;    &lt;td&gt;0.60&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Avg score&lt;/td&gt; &#xA;    &lt;td&gt;0.53&lt;/td&gt; &#xA;    &lt;td&gt;0.53&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=1V8AeXYAAAAJ&amp;amp;hl=en&#34;&gt;Szymon Tworkowski&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=CM6PCBYAAAAJ&#34;&gt;Konrad Staniszewski&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=eh6iEbQAAAAJ&amp;amp;hl=en&amp;amp;oi=ao&#34;&gt;MikoÅ‚aj Pacek&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=YdHW1ycAAAAJ&amp;amp;hl=en&#34;&gt;Henryk Michalewski&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scholar.google.com/citations?user=bOQGfFIAAAAJ&amp;amp;hl=en&#34;&gt;Yuhuai Wu&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://scholar.google.pl/citations?user=Se68XecAAAAJ&amp;amp;hl=pl&amp;amp;oi=ao&#34;&gt;Piotr MiÅ‚oÅ›&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;To cite this work please use&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@misc{tworkowski2023focused,&#xA;      title={Focused Transformer: Contrastive Training for Context Scaling}, &#xA;      author={Szymon Tworkowski and Konrad Staniszewski and MikoÅ‚aj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr MiÅ‚oÅ›},&#xA;      year={2023},&#xA;      eprint={2307.03170},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.CL}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;The code and checkpoints are licensed under &lt;a href=&#34;http://www.apache.org/licenses/LICENSE-2.0&#34;&gt;Apache License, Version 2.0&lt;/a&gt;. Some of the examples use external code (see headers of files for copyright notices and licenses).&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;We gratefully acknowledge the TPU Research Cloud program, which was instrumental to our research by providing significant computational resources. We are also grateful to Xinyang Geng and Hao Liu for releasing &lt;a href=&#34;https://github.com/openlm-research/open_llama&#34;&gt;OpenLLaMA&lt;/a&gt; checkpoints and the &lt;a href=&#34;https://github.com/young-geng/EasyLM&#34;&gt;EasyLM&lt;/a&gt; library.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dttung2905/kafka-in-production</title>
    <updated>2023-07-12T01:30:23Z</updated>
    <id>tag:github.com,2023-07-12:/dttung2905/kafka-in-production</id>
    <link href="https://github.com/dttung2905/kafka-in-production" rel="alternate"></link>
    <summary type="html">&lt;p&gt;ğŸ“š Tech blogs &amp; talks by companies that run Kafka in production&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;kafka-in-production&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;http://hits.dwyl.com/dttung2905/kafka-in-production.svg?sanitize=true&#34; alt=&#34;HitCount&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/license/dttung2905/kafka-in-production&#34; alt=&#34;license&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/stars/dttung2905/kafka-in-production&#34; alt=&#34;stars&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Curious to know how big companies are operating their kafka fleet in production? This might be the repo for you:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;What&lt;/strong&gt; are the issues encountered when running kafka in production? ğŸ“&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;How&lt;/strong&gt; other organisations attempt to solve the issues? ğŸ› ï¸&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Why&lt;/strong&gt; certain approaches are adopted over others? &lt;span&gt;âš–&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;What&lt;/strong&gt; can we learn for our own use case?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#adobe&#34;&gt;Adobe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#agoda&#34;&gt;Agoda&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#airbnb&#34;&gt;Airbnb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#apple&#34;&gt;Apple&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#appsflyer&#34;&gt;AppsFlyer&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#bloomberg&#34;&gt;Bloomberg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#bolt&#34;&gt;Bolt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#bookingcom&#34;&gt;Booking.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#brex&#34;&gt;Brex&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#cloudflare&#34;&gt;Cloudflare&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#coinbase&#34;&gt;Coinbase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#criteo&#34;&gt;Criteo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#datadog&#34;&gt;Datadog&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#deliveroo&#34;&gt;Deliveroo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#goto&#34;&gt;GoTo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#grab&#34;&gt;Grab&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#honeycomb&#34;&gt;Honeycomb&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#linkedin&#34;&gt;LinkedIn&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#lyft&#34;&gt;Lyft&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#morgan-stanley&#34;&gt;Morgan Stanley&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#netflix&#34;&gt;Netflix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#pinterest&#34;&gt;Pinterest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#riskified&#34;&gt;Riskified&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#robinhood&#34;&gt;Robinhood&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#slack&#34;&gt;Slack&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#stripe&#34;&gt;Stripe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#uber&#34;&gt;Uber&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#wise&#34;&gt;Wise&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#wix&#34;&gt;Wix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#yelp&#34;&gt;Yelp&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#zalando&#34;&gt;Zalando&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/dttung2905/kafka-in-production/main/#zopa-bank&#34;&gt;Zopa Bank&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Adobe&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.developer.adobe.com/how-adobe-experience-platform-pipeline-became-the-cornerstone-of-in-flight-processing-for-adobe-51c0e0a91521&#34;&gt;How Adobe Experience Platform Pipeline Became the Cornerstone of In-Flight Processing for Adobe&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.developer.adobe.com/moving-beyond-newtonian-reductionism-in-the-management-of-large-scale-distributed-systems-part-2-35c3f91f96e3&#34;&gt;Moving Beyond Newtonian Reductionism in the Management of Large-Scale Distributed Systems, Part 2&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.developer.adobe.com/adobe-experience-platforms-streaming-sources-and-destinations-overview-and-architecture-ba0b4d3e7ded&#34;&gt;Adobe Experience Platformâ€™s Streaming Sources and Destinations Overview and Architecture&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.developer.adobe.com/wins-from-effective-kafka-monitoring-at-adobe-stability-performance-and-cost-savings-a3ecb701ee5b&#34;&gt;Wins from Effective Kafka Monitoring at Adobe: Stability, Performance, and Cost Savings&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.developer.adobe.com/creating-the-adobe-experience-platform-pipeline-with-kafka-4f1057a11ef&#34;&gt;Creating Adobe Experience Platform Pipeline with Kafka&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Agoda&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/agoda-engineering/how-agoda-manages-1-5-trillion-events-per-day-on-kafka-f0a27fc32ecb&#34;&gt;How Agoda manages 1.5 Trillion Events per day on Kafka&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/agoda-engineering/adding-time-lag-to-monitor-kafka-consumer-2c626fa61cfc&#34;&gt;Adding Time Lag to Monitor Kafka Consumer&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/agoda-engineering/ingesting-petabytes-of-data-per-week-into-hadoop-from-kafka-457718cc308c&#34;&gt;How our data scientists&#39; petabytes of data is ingested into Hadoop (from Kafka)&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Airbnb&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/airbnb-engineering/migrating-kafka-transparently-between-zookeeper-clusters-e68a75062f65&#34;&gt;Migrating Kafka transparently between Zookeeper clusters&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Apple&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/events/kafka-summit-london-2023/balance-kafka-cluster-with-zero-data-movement/&#34;&gt;Balance Kafka Cluster with Zero Data Movement&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-ny19/experiences-operating-apache-kafka-at-scale/&#34;&gt;Experiences Operating Apache KafkaÂ® at Scale&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/blog/rounding-up-kafka-summit-london-2018/&#34;&gt;Kafka as a Service A Tale of Security and Multi Tenancy&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;AppsFlyer&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/appsflyerengineering/four-crucial-steps-to-take-before-changing-kafka-partition-key-at-scale-3c2e553c73b2&#34;&gt;Four Crucial Steps to Take Before Changing Kafka Partition Key at Scale&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/resources/kafka-summit-2020/kafka-lag-monitoring-for-human-beings/&#34;&gt;Kafka Lag Monitoring For Human Beings&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/blog/kafka-lag-monitoring-and-metrics-at-appsflyer/&#34;&gt;Apache Kafka Lag Monitoring at AppsFlyer&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=tjjeaCtsw_M&#34;&gt;Managing your Kafka in an explosive growth environment&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Bloomberg&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/resources/kafka-summit-2020/fully-managed-multi-tenant-kafka-clusters-tips-tricks-and-tools/&#34;&gt;Fully-Managed, Multi-Tenant Kafka Clusters: Tips, Tricks, and Tools&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Bolt&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=ymx55BA8eQU&amp;amp;ab_channel=Confluent&#34;&gt;Using Apache Kafka and ksqlDB for Data Replication at Bolt&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/blog/how-bolt-adopted-cdc-with-confluent-for-real-time-data-and-analytics/&#34;&gt;How Bolt Has Adopted Change Data Capture with Confluent Platform&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/bolt-labs/streaming-vitess-at-bolt-f8ea93211c3f&#34;&gt;Kewei Shang&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Booking.com&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-sf18/data-streaming-ecosystem-management/&#34;&gt;Data Streaming Ecosystem Management at Booking.com&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Brex&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/brexeng/transactional-events-publishing-at-brex-66a5984f0726&#34;&gt;Transactional Events Publishing At Brex&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cloudflare&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.cloudflare.com/intelligent-automatic-restarts-for-unhealthy-kafka-consumers/&#34;&gt;Intelligent, automatic restarts for unhealthy Kafka consumers&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.cloudflare.com/using-apache-kafka-to-process-1-trillion-messages/&#34;&gt;Using Apache Kafka to process 1 trillion inter-service messages&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Coinbase&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coinbase.com/blog/kafka-infrastructure-renovation&#34;&gt;Kafka infrastructure renovation at Coinbase&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.coinbase.com/blog/how-we-scaled-data-streaming-at-coinbase-using-aws-msk&#34;&gt;How we scaled data streaming at Coinbase using AWS MSK&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Criteo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/criteo-engineering/managing-kafka-and-data-streams-at-criteo-566ffbfda6ba&#34;&gt;Managing Kafka and Data Streams at Criteo&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/criteo-engineering/upgrading-kafka-on-a-large-infra-3ee99f56e970&#34;&gt;Upgrading Kafka on a large infra, or: when moving at scale requires careful planning&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/RicardoPaiva17/how-criteo-is-managing-one-of-the-largest-kafka-infrastructure-in-europe&#34;&gt;How Criteo is managing one of the largest Kafka Infrastructure in Europe&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Datadog&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-lon19/running-production-kafka-clusters-kubernetes/&#34;&gt;Running Production Kafka Clusters in Kubernetes&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Deliveroo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://deliveroo.engineering/2019/02/05/improving-stream-data-quality-with-protobuf-schema-validation.html&#34;&gt;Improving Stream Data Quality With Protobuf Schema Validation&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;GoTo&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.gojek.io/sink-kafka-messages-to-clickhouse-using-clickhouse-kafka-ingestor/&#34;&gt;Sink Kafka Messages to ClickHouse Using &#39;ClickHouse Kafka Ingestor&#39;&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.gojek.io/when-kafka-went-offshore/&#34;&gt;When Kafka Went Offshore&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.gojek.io/enhancing-ziggurat-the-backbone-of-gojeks-kafka-ecosystem/&#34;&gt;Enhancing Zigguratâ€Š-â€ŠThe Backbone Of Gojek&#39;s Kafka Ecosystem&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.gojek.io/handling-dead-letters-in-a-streaming-system/&#34;&gt;Handling Dead Letters in a Streaming System&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.gojek.io/how-kafka-solved-a-culture-problem-at-gojek/&#34;&gt;How Kafka Solved a Culture Problem at Gojek&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.gojek.io/fronting-an-armoured-car-for-kafka-ingestion/&#34;&gt;Fronting : An Armoured Car for Kafka Ingestion&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://blog.gojek.io/sakaar-taking-kafka-data-to-cloud-storage-at-go-jek/&#34;&gt;Sakaar: Taking Kafka data to cloud storage at GO-JEK&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Grab&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.grab.com/zero-trust-with-kafka&#34;&gt;Zero trust with Kafka&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.grab.com/kafka-connect&#34;&gt;How Kafka Connect helps move data seamlessly&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.grab.com/exposing-kafka-cluster&#34;&gt;Exposing a Kafka Cluster via a VPC Endpoint Service&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/events/kafka-summit-apac-2021/detect-fraud-successfully-with-grabdefence/&#34;&gt;Detect Fraud Successfully with GrabDefence!&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.grab.com/optimally-scaling-kafka-consumer-applications&#34;&gt;Optimally Scaling Kafka Consumer Applications&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Honeycomb&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.usenix.org/conference/srecon23americas/presentation/fong-jones&#34;&gt;Scaling Telemetry Systems with Streaming&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.honeycomb.io/blog/kafka-migration-lessons-learned&#34;&gt;Lessons Learned From the Migration to Confluent Kafka&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.honeycomb.io/blog/scaling-kafka-observability-pipelines&#34;&gt;Scaling Kafka at Honeycomb&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.honeycomb.io/blog/bitten-by-a-kafka-bug-postmortem&#34;&gt;Bitten by a Kafka Bug - Postmortem&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;LinkedIn&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog/2022/load-balanced-brooklin-mirror-maker--replicating-large-scale-kaf&#34;&gt;Load-balanced Brooklin Mirror Maker: Replicating large-scale Kafka clusters at LinkedIn&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog/2022/topicgc_how-linkedin-cleans-up-unused-metadata-for-its-kafka-clu&#34;&gt;TopicGC: How LinkedIn cleans up unused metadata for its Kafka clusters&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog/2019/apache-kafka-trillion-messages&#34;&gt;How LinkedIn customizes Apache Kafka for 7 trillion messages per day&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-london18/urp-excuse-you-the-three-metrics-you-have-to-know/&#34;&gt;URP? Excuse You! The Three Metrics You Have to Know&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog/2017/04/test-strategy-for-samza-kafka-services&#34;&gt;Test Strategy for Samza/Kafka Services&lt;/a&gt; - &lt;code&gt;2017&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog/2016/04/kafka-ecosystem-at-linkedin&#34;&gt;Kafka Ecosystem at LinkedIn&lt;/a&gt; - &lt;code&gt;2016&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/blog/2016/05/kafkaesque-days-at-linkedin--part-1&#34;&gt;Kafkaesque Days at LinkedIn â€“ Part 1&lt;/a&gt; - &lt;code&gt;2016&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.linkedin.com/apache-kafka/how-we_re-improving-and-advancing-kafka-linkedin&#34;&gt;How Weâ€™re Improving and Advancing Kafka at LinkedIn&lt;/a&gt; - &lt;code&gt;2015&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Lyft&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://eng.lyft.com/building-an-adaptive-multi-tenant-stream-bus-with-kafka-and-golang-5f1410bf2b40&#34;&gt;Building an Adaptive, Multi-Tenant Stream Bus with Kafka and Golang&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/resources/kafka-summit-2020/can-kafka-handle-a-lyft-ride/&#34;&gt;Can Kafka Handle a Lyft Ride?&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://eng.lyft.com/operating-apache-kafka-clusters-24-7-without-a-global-ops-team-417813a5ce70&#34;&gt;Operating Apache Kafka Clusters 24/7 Without A Global Ops Team&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-ny19/bulletproof-kafka-with-fault-tree-analysis/&#34;&gt;Bulletproof Apache KafkaÂ® with Fault Tree Analysis&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-san-francisco-2019/production-ready-kafka-on-kubernetes/&#34;&gt;Production Ready Kafka on Kubernetes&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Morgan Stanley&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/events/kafka-summit-london-2023/consistent-high-throughput-real-time-calculation-engines-using-kafka-streams/&#34;&gt;Consistent, High-throughput, Real-time Calculation Engines Using Kafka Streams&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Netflix&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/blog/how-kafka-is-used-by-netflix/&#34;&gt;Featuring Apache Kafka in the Netflix Studio and Finance World&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://netflixtechblog.medium.com/inca-message-tracing-and-loss-detection-for-streaming-data-netflix-de4836fc38c9&#34;&gt;Inca â€” Message Tracing and Loss Detection For Streaming Data @Netflix&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://netflixtechblog.com/evolution-of-the-netflix-data-pipeline-da246ca36905&#34;&gt;Evolution of the Netflix Data Pipeline&lt;/a&gt; - &lt;code&gt;2016&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://netflixtechblog.com/kafka-inside-keystone-pipeline-dd5aeabaf6bb&#34;&gt;Kafka Inside Keystone Pipeline&lt;/a&gt; - &lt;code&gt;2016&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Pinterest&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/blog/running-kafka-at-scale-at-pinterest/&#34;&gt;Lessons Learned from Running Apache Kafka at Scale at Pinterest&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/pinterest-engineering/how-pinterest-runs-kafka-at-scale-ff9c6f735be&#34;&gt;How Pinterest runs Kafka at scale&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/pinterest-engineering/open-sourcing-doctorkafka-kafka-cluster-healing-and-workload-balancing-e51ad25b6b17&#34;&gt;Open sourcing DoctorKafka: Kafka cluster healing and workload balancing&lt;/a&gt; - &lt;code&gt;2017&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Riskified&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/riskified-technology/how-riskified-manages-schemas-and-handles-standardization-fda9eb236e28&#34;&gt;How to Manage Schemas and Handle Standardization&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/riskified-technology/how-to-roll-your-kafka-cluster-with-zero-downtime-and-no-data-loss-770fd0a35971&#34;&gt;How to Roll Your Kafka Cluster With Zero Downtime and No Data Loss&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/riskified-technology/know-your-limits-cluster-benchmarks-ecc6c3c77574&#34;&gt;Know Your Limits: Cluster Benchmarks&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/en-gb/events/kafka-summit-london-2022/lets-make-your-cfo-happy-a-practical-guide-for-kafka-cost-reduction/&#34;&gt;Letâ€™s Make Your CFO Happy; A Practical Guide for Kafka Cost Reduction&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/riskified-technology/from-aws-cloudformation-to-terraform-migrating-apache-kafka-32bdabdbaa59&#34;&gt;From AWS CloudFormation to Terraform: Migrating Apache Kafka&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Robinhood&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-san-francisco-2019/tackling-kafka-with-a-small-team/&#34;&gt;Tackling Kafka, with a Small Team&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Slack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://slack.engineering/building-self-driving-kafka-clusters-using-open-source-components/&#34;&gt;Building Self-driving Kafka clusters using open source components&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Stripe&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/events/kafka-summit-london-2022/6-nines-how-stripe-keeps-kafka-highly-available-across-the-globe/&#34;&gt;6 Nines: How Stripe keeps Kafka highly-available across the globe&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Uber&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uber.com/en-SG/blog/securing-kafka-infrastructure-at-uber/&#34;&gt;Securing KafkaÂ® Infrastructure at Uber&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uber.com/en-SG/blog/real-time-exactly-once-ad-event-processing/&#34;&gt;Real-Time Exactly-Once Ad Event Processing with Apache Flink, Kafka, and Pinot&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uber.com/en-SG/blog/introducing-ugroup-ubers-consumer-management-framework/&#34;&gt;Introducing uGroup: Uberâ€™s Consumer Management Framework&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uber.com/en-SG/blog/kafka/&#34;&gt;Disaster Recovery for Multi-Region Kafka at Uber&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/kafka-summit-san-francisco-2019/kafka-cluster-federation-at-uber/&#34;&gt;Kafka Cluster Federation at Uber&lt;/a&gt; - &lt;code&gt;2019&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uber.com/en-SG/blog/reliable-reprocessing/&#34;&gt;Building Reliable Reprocessing and Dead Letter Queues with Apache Kafka&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uber.com/en-SG/blog/chaperone-audit-kafka-messages/&#34;&gt;Introducing Chaperone: How Uber Engineering Audits Apache Kafka End-to-End&lt;/a&gt; - &lt;code&gt;2016&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.uber.com/blog/ureplicator-apache-kafka-replicator/&#34;&gt;uReplicator: Uber Engineeringâ€™s Robust Apache Kafka Replicator&lt;/a&gt; - &lt;code&gt;2016&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Wise&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/events/kafka-summit-london-2023/streaming-infrastructure-at-wise/&#34;&gt;Streaming Infrastructure at Wise&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/wise-engineering/rack-awareness-in-kafka-streams-448d7e5225a3&#34;&gt;Rack awareness in Kafka Streams&lt;/a&gt; - &lt;code&gt;2022&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/wise-engineering/teamwork-implementing-a-kafka-retry-strategy-at-wise-82e0887e243b&#34;&gt;Teamwork: Implementing a Kafka retry strategy at Wise&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/wise-engineering/running-kafka-in-kubernetes-part-1-why-we-migrated-our-kafka-clusters-to-kubernetes-722101a2e751&#34;&gt;Running Kafka in Kubernetes, Part 1: Why we migrated our Kafka clusters to Kubernetes.&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/wise-engineering/running-kafka-in-kubernetes-part-2-how-we-migrated-our-kafka-clusters-to-kubernetes-69174cea1559&#34;&gt;Running Kafka in Kubernetes, Part 2: How we migrated our Kafka clusters to Kubernetes.&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=4pfY0uFW7yk&amp;amp;ab_channel=CNCF%5BCloudNativeComputingFoundation%5D&#34;&gt;Securing Kafka with SPIFFE at TransferWise - Jonathan Oddy, Levani Kokhreidze&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://medium.com/wise-engineering/achieving-high-availability-with-stateful-kafka-streams-applications-cba429ca7238&#34;&gt;Achieving high availability with stateful Kafka Streams applications&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Wix&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wix.engineering/post/4-steps-for-kafka-rebalance-notes-from-the-field&#34;&gt;4 Steps for Kafka Rebalance - Notes From the Field&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wix.engineering/post/wix-s-journey-into-data-streams&#34;&gt;Wixâ€™s Journey Into Data Streams&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.wix.engineering/post/building-a-high-level-sdk-for-kafka-greyhound-unleashed&#34;&gt;Building a High-level SDK for Kafka: Greyhound Unleashed&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Yelp&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineeringblog.yelp.com/2021/12/kafka-on-paasta-part-one.html&#34;&gt;Kafka on PaaSTA: Running Kafka on Kubernetes at Yelp (Part 1 - Architecture)&lt;/a&gt; - &lt;code&gt;2021&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineeringblog.yelp.com/2020/01/streams-and-monk-how-yelp-approaches-kafka-in-2020.html&#34;&gt;Streams and Monk â€“ How Yelp is Approaching Kafka in 2020&lt;/a&gt; - &lt;code&gt;2020&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/es-es/kafka-summit-nyc17/billions-messages-day-yelps-real-time-data-pipeline/&#34;&gt;Billions of Messages a Day â€“ Yelpâ€™s Real-time Data Pipeline&lt;/a&gt; - &lt;code&gt;2017&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Zalando&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.zalando.com/posts/2018/01/rock-solid-kafka.html&#34;&gt;Rock Solid Kafka and ZooKeeper Ops on AWS&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.zalando.com/posts/2018/05/many-to-many-using-kafka.html&#34;&gt;Many-to-Many Relationships Using Kafka&lt;/a&gt; - &lt;code&gt;2018&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.zalando.com/posts/2017/10/event-first-development---moving-towards-kafka-pipeline-applications.html&#34;&gt;Event First Development - Moving Towards Kafka Pipeline Applications&lt;/a&gt; - &lt;code&gt;2017&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.zalando.com/posts/2017/10/reattaching-kafka-ebs-in-aws.html&#34;&gt;Reattaching Kafka EBS in AWS&lt;/a&gt; - &lt;code&gt;2017&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.zalando.com/posts/2017/11/real-time-ranking-kafka.html&#34;&gt;Real-time Ranking with Apache Kafkaâ€™s Streams API&lt;/a&gt; - &lt;code&gt;2017&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.zalando.com/posts/2017/11/running-kafka-streams-applications-aws.html&#34;&gt;Running Kafka Streams applications in AWS&lt;/a&gt; - &lt;code&gt;2017&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.zalando.com/posts/2017/12/recipe-for-kafka-lag-monitoring.html&#34;&gt;A Recipe for Kafka Lag Monitoring&lt;/a&gt; - &lt;code&gt;2017&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://engineering.zalando.com/posts/2017/12/backing-up-kafka-zookeeper.html&#34;&gt;Surviving Data Loss&lt;/a&gt; - &lt;code&gt;2017&lt;/code&gt; - &lt;span&gt;ğŸ“š&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Zopa Bank&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.confluent.io/events/kafka-summit-london-2023/highly-available-kafka-consumers-and-kafka-streams-on-kubernetes/&#34;&gt;Highly Available Kafka Consumers and Kafka Streams on Kubernetes&lt;/a&gt; - &lt;code&gt;2023&lt;/code&gt; - &lt;span&gt;ğŸ™&lt;/span&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>CoretechR/OMOTE</title>
    <updated>2023-07-12T01:30:23Z</updated>
    <id>tag:github.com,2023-07-12:/CoretechR/OMOTE</id>
    <link href="https://github.com/CoretechR/OMOTE" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open Source Remote Using ESP32 and LVGL&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;OMOTE - Open Universal Remote&lt;/h1&gt; &#xA;&lt;p&gt;OMOTE is an ESP32 based open source universal remote. Its capacitive 2.8â€ touchscreen provides an intuitive and snappy user interface for switching devices and settings. No hub or docking station is required as the remote features infrared, Wi-Fi and Bluetooth connectivity. With its well optimized power consumption, OMOTE can run for months on a charge. And since the design files are open source, you can fully customize them to your devices and needs.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CoretechR/OMOTE/main/P1030424_small.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/CoretechR/OMOTE/main/Menu.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
</feed>