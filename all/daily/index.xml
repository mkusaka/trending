<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-01-12T01:28:57Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Nutlope/llamacoder</title>
    <updated>2025-01-12T01:28:57Z</updated>
    <id>tag:github.com,2025-01-12:/Nutlope/llamacoder</id>
    <link href="https://github.com/Nutlope/llamacoder" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open source Claude Artifacts ‚Äì built with Llama 3.1 405B&lt;/p&gt;&lt;hr&gt;&lt;a href=&#34;https://www.llamacoder.io&#34;&gt; &lt;img alt=&#34;Llama Coder&#34; src=&#34;https://raw.githubusercontent.com/Nutlope/llamacoder/main/public/og-image.png&#34;&gt; &lt;h1 align=&#34;center&#34;&gt;Llama Coder&lt;/h1&gt; &lt;/a&gt; &#xA;&lt;p align=&#34;center&#34;&gt; An open source Claude Artifacts ‚Äì generate small apps with one prompt. Powered by Llama 3 on Together.ai. &lt;/p&gt; &#xA;&lt;h2&gt;Tech stack&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://ai.meta.com/blog/meta-llama-3-1/&#34;&gt;Llama 3.1 405B&lt;/a&gt; from Meta for the LLM&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://dub.sh/together-ai/?utm_source=example-app&amp;amp;utm_medium=llamacoder&amp;amp;utm_campaign=llamacoder-app-signup&#34;&gt;Together AI&lt;/a&gt; for LLM inference&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://sandpack.codesandbox.io/&#34;&gt;Sandpack&lt;/a&gt; for the code sandbox&lt;/li&gt; &#xA; &lt;li&gt;Next.js app router with Tailwind&lt;/li&gt; &#xA; &lt;li&gt;Helicone for observability&lt;/li&gt; &#xA; &lt;li&gt;Plausible for website analytics&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Cloning &amp;amp; running&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repo: &lt;code&gt;git clone https://github.com/Nutlope/llamacoder&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file and add your &lt;a href=&#34;https://dub.sh/together-ai/?utm_source=example-app&amp;amp;utm_medium=llamacoder&amp;amp;utm_campaign=llamacoder-app-signup&#34;&gt;Together AI API key&lt;/a&gt;: &lt;code&gt;TOGETHER_API_KEY=&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run &lt;code&gt;npm install&lt;/code&gt; and &lt;code&gt;npm run dev&lt;/code&gt; to install dependencies and run locally&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;For contributing to the repo, please see the &lt;a href=&#34;https://raw.githubusercontent.com/Nutlope/llamacoder/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>fixie-ai/ultravox</title>
    <updated>2025-01-12T01:28:57Z</updated>
    <id>tag:github.com,2025-01-12:/fixie-ai/ultravox</id>
    <link href="https://github.com/fixie-ai/ultravox" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A fast multimodal LLM for real-time voice&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;img alt=&#34;Ultravox&#34; src=&#34;https://zfmrfvimiaqahezndsse.supabase.co/storage/v1/object/public/images/custom/Introducing%20Ultravox%20Wide.jpg&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; A fast multimodal LLM for real-time voice &lt;/h3&gt; &#xA;&lt;p&gt;&lt;em&gt;Latest News&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024/11 ‚Äî &lt;a href=&#34;https://github.com/fixie-ai/ultravox/releases/tag/v0.4.1&#34;&gt;Ultravox 0.4.1&lt;/a&gt; available&lt;/li&gt; &#xA; &lt;li&gt;2024/08 ‚Äî &lt;a href=&#34;https://github.com/fixie-ai/ultravox/releases/tag/v0.4&#34;&gt;Ultravox 0.4&lt;/a&gt; available&lt;/li&gt; &#xA; &lt;li&gt;2024/08 ‚Äî &lt;a href=&#34;https://github.com/fixie-ai/ultravox/releases/tag/v0.3&#34;&gt;Ultravox 0.3&lt;/a&gt; available&lt;/li&gt; &#xA; &lt;li&gt;2024/08 ‚Äî Preview of Ultravox APIs available, more information &lt;a href=&#34;https://fixie-ai.github.io/ultradox/&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;About&lt;/h1&gt; &#xA;&lt;p&gt;Ultravox is a new kind of multimodal LLM that can understand text as well as human speech, without the need for a separate Audio Speech Recognition (ASR) stage. Building on research like &lt;a href=&#34;https://arxiv.org/abs/2209.03143&#34;&gt;AudioLM&lt;/a&gt;, &lt;a href=&#34;https://ai.meta.com/blog/seamless-m4t/&#34;&gt;SeamlessM4T&lt;/a&gt;, &lt;a href=&#34;https://tincans.ai/slm&#34;&gt;Gazelle&lt;/a&gt;, &lt;a href=&#34;https://github.com/0nutation/SpeechGPT/tree/main/speechgpt&#34;&gt;SpeechGPT&lt;/a&gt;, and others, Ultravox is able to extend any open-weight LLM with a multimodal projector that converts audio directly into the high-dimensional space used by LLM. We&#39;ve trained versions on Llama 3, Mistral, and Gemma. This direct coupling allows Ultravox to respond much more quickly than systems that combine separate ASR and LLM components. In the future this will also allow Ultravox to natively understand the paralinguistic cues of timing and emotion that are omnipresent in human speech.&lt;/p&gt; &#xA;&lt;p&gt;The current version of Ultravox (v0.4), when invoked with audio content, has a time-to-first-token (TTFT) of approximately 150ms, and a tokens-per-second rate of ~60 using a Llama 3.1 8B backbone. While quite fast, we believe there is considerable room for improvement in these numbers.&lt;/p&gt; &#xA;&lt;p&gt;Ultravox currently takes in audio and emits streaming text. As we evolve the model, we&#39;ll train it to be able to emit a stream of speech tokens that can then be converted directly into raw audio by an appropriate unit vocoder.&lt;/p&gt; &#xA;&lt;h3&gt;Demo&lt;/h3&gt; &#xA;&lt;p&gt;See Ultravox in action on our &lt;a href=&#34;https://demo.ultravox.ai&#34;&gt;demo page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;You can run the Gradio demo locally with &lt;code&gt;just gradio&lt;/code&gt;. You can run the demo in &#34;voice mode&#34; which allows natural audio conversations with ultravox by running &lt;code&gt;just gradio --voice_mode=True&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Discord&lt;/h3&gt; &#xA;&lt;p&gt;Join us on our Discord server &lt;a href=&#34;https://discord.gg/Qw6KHxv8YB&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Jobs&lt;/h3&gt; &#xA;&lt;p&gt;If you&#39;re interested in working on Ultravox fulltime, we&#39;re hiring! Check out our jobs page &lt;a href=&#34;https://careers.fixie.ai&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Inference Server&lt;/h3&gt; &#xA;&lt;p&gt;You can try out Ultravox using your own audio content (as a WAV file) by spinning up an Ultravox instance on our partner, BaseTen: &lt;a href=&#34;https://www.baseten.co/library/ultravox/&#34;&gt;https://www.baseten.co/library/ultravox/&lt;/a&gt;. They offer free credits to get started.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re interested in running Ultravox in a real-time capacity, we offer a set of managed APIs as well. You can learn more about getting access to those &lt;a href=&#34;https://docs.ultravox.ai&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Model&lt;/h3&gt; &#xA;&lt;p&gt;You can download the latest weights from the &lt;a href=&#34;https://huggingface.co/fixie-ai/&#34;&gt;Ultravox Hugging Face page&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Architecture&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://docs.google.com/presentation/d/1ey81xuuMzrJaBwztb_Rq24Cit37GQokD2aAes_KkGVI/edit&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/docs/assets/Ultravox%20Model%20Architecture.svg?sanitize=true&#34; alt=&#34;architecture diagram&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;Read on if you&#39;re interested in training your own version of Ultravox.&lt;/p&gt; &#xA;&lt;h2&gt;Environment Setup (Mac)&lt;/h2&gt; &#xA;&lt;p&gt;Install the basic tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://brew.sh&#34;&gt;&lt;code&gt;Homebrew&lt;/code&gt;&lt;/a&gt; is a package manager for MacOS that also mostly works for Linux. If you&#39;re running Debian or Ubuntu Linux, you can alternatively get by with apt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://just.systems/man/en/&#34;&gt;&lt;code&gt;Just&lt;/code&gt;&lt;/a&gt; simplifies our shell workflows. It frequently functions as our interface to all the other tools.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&#34;&#xA;brew update&#xA;brew install just&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a Python virtual environment and install the necessary packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;just install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We&#39;re using Poetry to manage the Python virtual environment.&lt;/p&gt; &#xA;&lt;h3&gt;Mosaic Environment Setup (Fixie Internal)&lt;/h3&gt; &#xA;&lt;p&gt;If you want to use &lt;a href=&#34;https://docs.mosaicml.com/projects/mcli/en/latest/quick_start/getting_started.html&#34;&gt;Mosaic&lt;/a&gt; for training, you need to setup a few things to run on the Mosaic Platform.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Install &amp;amp; login to the Mosaic CLI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install --upgrade mosaicml-cli&#xA;&#xA;mcli init&#xA;&#xA;mcli set api-key &amp;lt;new-value&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;set API keys for tools we use:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Huggging Face token for accessing walled data and models&#xA;mcli create secret env HF_TOKEN=hf_&amp;lt;your_token&amp;gt;&#xA;&#xA;# WandB token for logging experiments&#xA;mcli create secret env WANDB_PROJECT=ultravox&#xA;mcli create secret env WANDB_API_KEY=&amp;lt;your_wandb_key&amp;gt;&#xA;&#xA;# GCP credentials for accessing data (e.g. BoolQ)&#xA;# Get service_account.json file from Justin/Farzad and put it in the root dir, then&#xA;mcli create secret gcp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;Currently, we keep both the LLM and the audio encoder frozen and only train the adapter/projector. Training Ultraox v0.4 took 2-3 hours on 8xH100 GPUs for 14K training steps.&lt;/p&gt; &#xA;&lt;h3&gt;Use-Cases for Training Ultravox&lt;/h3&gt; &#xA;&lt;p&gt;Why would you want to (re-) train Ultravox? Here are a few scenarios:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;You want to use a different LLM or audio encoder backbone.&lt;/p&gt; &lt;p&gt;a. In this case you need to re-train the adapter. You can use &lt;code&gt;release_config.yaml&lt;/code&gt;, which contains our config for our latest release, and you should be able to simply change the base LLM or encoder by specifying &lt;code&gt;--text-model &amp;lt;hf-model-id-for-llm&amp;gt;&lt;/code&gt; and/or &lt;code&gt;--audio-model &amp;lt;hf-model-id-for-encoder&amp;gt;&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You want to improve the knowledge of the model&lt;/p&gt; &lt;p&gt;a. We suggest to either use RAG on the fly (no training needed), or fine-tune the LLM backbone instead. Fine-tuning the LLM backbone does not require re-training Ultravox (i.e., the existing adapter will work).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You want to use your own audio data, for example to add support for a new language.&lt;/p&gt; &lt;p&gt;a. First step, prepare your dataset: at bare minimum, the samples should have an &lt;code&gt;audio&lt;/code&gt; and a text &lt;code&gt;continuation&lt;/code&gt; field.&lt;/p&gt; &lt;p&gt;b. Take a look at &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/ultravox/tools/ds_tool/ds_tool.py&#34;&gt;&lt;code&gt;ds_tool.py&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/ultravox/tools/ds_tool/continuation.jinja&#34;&gt;&lt;code&gt;continuation.jinja&lt;/code&gt;&lt;/a&gt; as well as &lt;a href=&#34;https://huggingface.co/datasets/fixie-ai/common_voice_17_0/viewer/fr&#34;&gt;our variant of Common Voice&lt;/a&gt; that was created using &lt;code&gt;ds_tool&lt;/code&gt; to add the &lt;code&gt;continuation&lt;/code&gt; field.&lt;/p&gt; &lt;p&gt;c. Add your dataset to the dataset mix in &lt;code&gt;release_config.yaml&lt;/code&gt; and train.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;There&#39;s no one-size fits all. If you need help you can find us on our Discord server &lt;a href=&#34;https://discord.gg/Qw6KHxv8YB&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;How to Train&lt;/h3&gt; &#xA;&lt;p&gt;We do most of our training on the &lt;a href=&#34;https://docs.mosaicml.com&#34;&gt;MosaicML platform&lt;/a&gt; and therefore most of our tooling and docs are Mosaic-related. However, you can do the same training on your own GPU without much difficulty. Here we assume you have the environment set up (run &lt;code&gt;just install&lt;/code&gt;). You can also take a look at &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/setup.sh&#34;&gt;&lt;code&gt;setup.sh&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To kick off a training run you can do:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run python -m ultravox.training.train --config_path ultravox/training/configs/release_config.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For DDP training make sure to add &lt;code&gt;torchrun&lt;/code&gt;. We also recommend prefetching weights in advance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;TRAIN_ARGS=&#34;--config_path ultravox/training/configs/release_config.yaml&#34;&#xA;poetry run python -m ultravox.training.helpers.prefetch_weights $TRAIN_ARGS&#xA;poetry run torchrun --nproc_per_node=8 -m ultravox.training.train $TRAIN_ARGS&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For a debug run, you can use smaller models, datasets, or batch size. Here&#39;s a config that uses TinyLlama as the LLM backbone:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;poetry run python -m ultravox.training.train --config_path ultravox/training/configs/asr_tinyllama_100s.yaml --batch_size 1 --report_logs_to tensorboard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We use &lt;a href=&#34;https://github.com/lebrice/simpleparsing/&#34;&gt;SimpleParsing&lt;/a&gt; for configs. Configs are composable (i.e. you can specify zero or many configs) and &lt;code&gt;meta_config.yaml&lt;/code&gt; is always used as the default. See &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/ultravox/training/config_base.py&#34;&gt;&lt;code&gt;configs_base.py&lt;/code&gt;&lt;/a&gt; to find the parameters you modify, such as the &lt;code&gt;--text-model&lt;/code&gt;, &lt;code&gt;--device&lt;/code&gt;, &lt;code&gt;--exp-name&lt;/code&gt;, etc.&lt;/p&gt; &#xA;&lt;h3&gt;MosaicML Training (Fixie Internal)&lt;/h3&gt; &#xA;&lt;p&gt;Before running any training jobs, you need to setup your SSH key in the Mosaic Platform: &lt;a href=&#34;https://docs.mosaicml.com/projects/mcli/en/latest/resources/secrets/ssh.html#page-secrets-ssh&#34;&gt;https://docs.mosaicml.com/projects/mcli/en/latest/resources/secrets/ssh.html#page-secrets-ssh&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;## Create a new SSH key and add it to the Mosaic Platform&#xA;# ssh-keygen -f ~/.ssh/mclid_id_rsa&#xA;## add the **public** key to Github&#xA;# mcli create secret ssh ~/.ssh/mclid_id_rsa&#xA;&#xA;mcli run -f mcloud.yaml --follow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Other useful commands:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mcli get clusters&#xA;&#xA;mcli util r7z2&#xA;mcli get runs&#xA;mcli get runs --cluster r7z2&#xA;&#xA;mcli run -f mcloud.yaml --follow&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For interactive runs you can use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;just mcloud --image mosaicml/composer:latest --max-duration 1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;IMPORTANT: Make sure to monitor your jobs and stop the machine when you&#39;re done with any job, specially interactive ones!&lt;/p&gt; &#xA;&lt;h3&gt;Running evaluations&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Use &lt;code&gt;infer_tool.py --json &amp;gt; file&lt;/code&gt; to create a jsonl output from a given model/dataset combo, where each line contains two values: &lt;strong&gt;question&lt;/strong&gt; and &lt;strong&gt;answer&lt;/strong&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;code&gt;eval_tool.py -f file&lt;/code&gt; to evaluate the jsonl file, which will produce an average score for the model on the dataset.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Misc&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/fixie-ai/ultravox/main/Justfile&#34;&gt;Justfile&lt;/a&gt; is a good resource for finding popular commands. Here are a few:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;just update    # update dependencies&#xA;just format    # run formatting (black, isort, autoflake)&#xA;just test      # run tests&#xA;just python    # activate venv and run python&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/vcpkg</title>
    <updated>2025-01-12T01:28:57Z</updated>
    <id>tag:github.com,2025-01-12:/microsoft/vcpkg</id>
    <link href="https://github.com/microsoft/vcpkg" rel="alternate"></link>
    <summary type="html">&lt;p&gt;C++ Library Manager for Windows, Linux, and MacOS&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/locale/?target=https%3A%2F%2Flearn.microsoft.com%2Fvcpkg%2F&#34;&gt;üåê Read in a different language&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;vcpkg overview&lt;/h1&gt; &#xA;&lt;p&gt;vcpkg is a free and open-source C/C++ package manager maintained by Microsoft and the C++ community.&lt;/p&gt; &#xA;&lt;p&gt;Initially launched in 2016 as a tool for assisting developers in migrating their projects to newer versions of Visual Studio, vcpkg has evolved into a cross-platform tool used by developers on Windows, macOS, and Linux. vcpkg has a large collection of open-source libraries and enterprise-ready features designed to facilitate your development process with support for any build and project systems. vcpkg is a C++ tool at heart and is written in C++ with scripts in CMake. It is designed from the ground up to address the unique pain points C/C++ developers experience.&lt;/p&gt; &#xA;&lt;p&gt;This tool and ecosystem are constantly evolving, and we always appreciate contributions! Learn how to start contributing with our &lt;a href=&#34;https://learn.microsoft.com/vcpkg/get_started/get-started-adding-to-registry&#34;&gt;packaging tutorial&lt;/a&gt; and &lt;a href=&#34;https://learn.microsoft.com/vcpkg/contributing/maintainer-guide&#34;&gt;maintainer guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Get started&lt;/h1&gt; &#xA;&lt;p&gt;First, follow one of our quick start guides.&lt;/p&gt; &#xA;&lt;p&gt;Whether you&#39;re using CMake, MSBuild, or any other build system, vcpkg has you covered:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/get_started/get-started&#34;&gt;vcpkg with CMake&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/get_started/get-started-msbuild&#34;&gt;vcpkg with MSBuild&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/users/buildsystems/manual-integration&#34;&gt;vcpkg with other build systems&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can also use any editor:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/get_started/get-started-vs&#34;&gt;vcpkg with Visual Studio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/get_started/get-started-vscode&#34;&gt;vcpkg with Visual Studio Code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.jetbrains.com/help/clion/package-management.html&#34;&gt;vcpkg with CLion&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://doc.qt.io/qtcreator/creator-vcpkg.html&#34;&gt;vcpkg with Qt Creator&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If a library you need is not present in the vcpkg registry, &lt;a href=&#34;https://github.com/microsoft/vcpkg/issues/new/choose&#34;&gt;open an issue on the GitHub repository&lt;/a&gt; or &lt;a href=&#34;https://learn.microsoft.com/vcpkg/get_started/get-started-adding-to-registry&#34;&gt;contribute the package yourself&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After you&#39;ve gotten vcpkg installed and working, you may wish to &lt;a href=&#34;https://learn.microsoft.com/vcpkg/commands/integrate#vcpkg-autocompletion&#34;&gt;add tab completion to your terminal&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Use vcpkg&lt;/h1&gt; &#xA;&lt;p&gt;Create a &lt;a href=&#34;https://learn.microsoft.com/vcpkg/consume/manifest-mode&#34;&gt;manifest for your project&#39;s dependencies&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Console&#34;&gt;vcpkg new --application&#xA;vcpkg add port fmt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or &lt;a href=&#34;https://learn.microsoft.com/vcpkg/consume/classic-mode&#34;&gt;install packages through the command line&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-Console&#34;&gt;vcpkg install fmt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then use one of our available integrations for &lt;a href=&#34;https://learn.microsoft.com/vcpkg/concepts/build-system-integration#cmake-integration&#34;&gt;CMake&lt;/a&gt;, &lt;a href=&#34;https://learn.microsoft.com/vcpkg/concepts/build-system-integration#msbuild-integration&#34;&gt;MSBuild&lt;/a&gt; or &lt;a href=&#34;https://learn.microsoft.com/vcpkg/concepts/build-system-integration#manual-integration&#34;&gt;other build systems&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;For a short description of all available commands, run &lt;code&gt;vcpkg help&lt;/code&gt;. Run &lt;code&gt;vcpkg help [topic]&lt;/code&gt; for details on a specific topic.&lt;/p&gt; &#xA;&lt;h1&gt;Key features&lt;/h1&gt; &#xA;&lt;p&gt;vcpkg offers powerful features for your package management needs:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/concepts/build-system-integration&#34;&gt;easily integrate with your build system&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/users/versioning&#34;&gt;control the versions of your dependencies&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/concepts/registries&#34;&gt;package and publish your own packages&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/users/binarycaching&#34;&gt;reuse your binary artifacts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/vcpkg/concepts/asset-caching&#34;&gt;enable offline scenarios with asset caching&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Contribute&lt;/h1&gt; &#xA;&lt;p&gt;vcpkg is an open source project, and is thus built with your contributions. Here are some ways you can contribute:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/vcpkg/issues/new/choose&#34;&gt;Submit issues&lt;/a&gt; in vcpkg or existing packages&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/vcpkg/pulls&#34;&gt;Submit fixes and new packages&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Please refer to our &lt;a href=&#34;https://learn.microsoft.com/vcpkg/contributing/maintainer-guide&#34;&gt;mantainer guide&lt;/a&gt; and &lt;a href=&#34;https://learn.microsoft.com/vcpkg/get_started/get-started-packaging&#34;&gt;packaging tutorial&lt;/a&gt; for more details.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or email &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h1&gt;Resources&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ports: &lt;a href=&#34;https://github.com/microsoft/vcpkg&#34;&gt;Microsoft/vcpkg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Source code: &lt;a href=&#34;https://github.com/microsoft/vcpkg-tool&#34;&gt;Microsoft/vcpkg-tool&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Docs: &lt;a href=&#34;https://learn.microsoft.com/vcpkg&#34;&gt;Microsoft Learn | vcpkg&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Website: &lt;a href=&#34;https://vcpkg.io&#34;&gt;vcpkg.io&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Email: &lt;a href=&#34;mailto:vcpkg@microsoft.com&#34;&gt;vcpkg@microsoft.com&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Discord: &lt;a href=&#34;https://www.includecpp.org&#34;&gt;#include &amp;lt;C++&amp;gt;&#39;s Discord server&lt;/a&gt;, in the #üåèvcpkg channel&lt;/li&gt; &#xA; &lt;li&gt;Slack: &lt;a href=&#34;https://cppalliance.org/slack/&#34;&gt;C++ Alliance&#39;s Slack server&lt;/a&gt;, in the #vcpkg channel&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;License&lt;/h1&gt; &#xA;&lt;p&gt;The code in this repository is licensed under the MIT License. The libraries provided by ports are licensed under the terms of their original authors. Where available, vcpkg places the associated license(s) in the location &lt;a href=&#34;https://learn.microsoft.com/vcpkg/contributing/maintainer-guide#install-copyright-file&#34;&gt;&lt;code&gt;installed/&amp;lt;triplet&amp;gt;/share/&amp;lt;port&amp;gt;/copyright&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h1&gt;Security&lt;/h1&gt; &#xA;&lt;p&gt;Most ports in vcpkg build the libraries in question using the original build system preferred by the original developers of those libraries, and download source code and build tools from their official distribution locations. For use behind a firewall, the specific access needed will depend on which ports are being installed. If you must install it in an &#34;air gapped&#34; environment, consider instaling once in a non-&#34;air gapped&#34; environment, populating an &lt;a href=&#34;https://learn.microsoft.com/vcpkg/users/assetcaching&#34;&gt;asset cache&lt;/a&gt; shared with the otherwise &#34;air gapped&#34; environment.&lt;/p&gt; &#xA;&lt;h1&gt;Telemetry&lt;/h1&gt; &#xA;&lt;p&gt;vcpkg collects usage data in order to help us improve your experience. The data collected by Microsoft is anonymous. You can opt-out of telemetry by:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;running the bootstrap-vcpkg script with &lt;code&gt;-disableMetrics&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;passing &lt;code&gt;--disable-metrics&lt;/code&gt; to vcpkg on the command line&lt;/li&gt; &#xA; &lt;li&gt;setting the &lt;code&gt;VCPKG_DISABLE_METRICS&lt;/code&gt; environment variable&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Read more about vcpkg telemetry at &lt;a href=&#34;https://learn.microsoft.com/vcpkg/about/privacy&#34;&gt;https://learn.microsoft.com/vcpkg/about/privacy&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>