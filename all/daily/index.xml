<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-04-30T01:28:55Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>carykh/jes</title>
    <updated>2025-04-30T01:28:55Z</updated>
    <id>tag:github.com,2025-04-30:/carykh/jes</id>
    <link href="https://github.com/carykh/jes" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Jelly Evolution Simulator&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Jelly Evolution Simulator&lt;/h1&gt; &#xA;&lt;p&gt;To run this program, run this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cmd python jes.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;NOTE: This project, like all my projects, are not meant to be consumer products with perfect QA. Rather, it&#39;s just me, as one person, coding a casual experiment to the point that it works well enough on my computer to make a video from it! No more, no less. (I used to not put my code online, just like when you create a Minecraft world with your friends, you don&#39;t have to share the world with everyone. I just started posting code here because I wanted to make it easier for eager devs to make mods.) Long story short, I won&#39;t be doing bug-fixing or tech support on this project.&lt;/p&gt; &#xA;&lt;h1&gt;Key-controls&lt;/h1&gt; &#xA;&lt;p&gt;ESC: Close the program&lt;/p&gt; &#xA;&lt;p&gt;X: Toggle whether or not X&#39;s show up over killed jellies&lt;/p&gt; &#xA;&lt;p&gt;S: Store the species you&#39;re highlighting in memory. (Press S a 2nd time to unstore.) Why do this? Well, say you notice there&#39;s a creature who got #1 in a certain generation, but you can&#39;t find any trace of it elsewhere. Now, you can highlight the creature, press S, and their species bubble will show up in the upper-left. Then, roll your mouse over the species bubble, and there&#39;s all the species info!&lt;/p&gt; &#xA;&lt;p&gt;C: Change the color of the species you&#39;re highlighting. Do this when 2 species are annoyingly close in color, and you want a better way to tell them apart.&lt;/p&gt; &#xA;&lt;p&gt;Q: Open/close the creature mosaic (can also be done by clicking &#34;Show creatures&#34; button)&lt;/p&gt; &#xA;&lt;p&gt;LEFT/RIGHT: Scroll through forward/backward through the timeline (can also be done by scrolling the scroll bar)&lt;/p&gt; &#xA;&lt;h1&gt;Updates (2025-01-11)&lt;/h1&gt; &#xA;&lt;p&gt;-Mutation-finding-bug fixed (I think)&lt;/p&gt; &#xA;&lt;p&gt;There used to be a bug where, late in the simulation, big mutations would take forever to find. That has been resolved. (It was the 0.5+ rigidity-forcing)&lt;/p&gt; &#xA;&lt;p&gt;-Added all those key controls&lt;/p&gt; &#xA;&lt;p&gt;-Allowed the user to change the number of creatures in the simulation (first user input)&lt;/p&gt; &#xA;&lt;p&gt;-Fixed but where, when you click &#34;Watch sample&#34;, it ONLY shows you a sampling of 8 creatures from the recent-est generation. Now, it always shows you a sampling of the generation your scroll bar is currently at.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>simular-ai/Agent-S</title>
    <updated>2025-04-30T01:28:55Z</updated>
    <id>tag:github.com,2025-04-30:/simular-ai/Agent-S</id>
    <link href="https://github.com/simular-ai/Agent-S" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Agent S: an open agentic framework that uses computers like a human&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s.png&#34; alt=&#34;Logo&#34; style=&#34;vertical-align:middle&#34; width=&#34;60&#34;&gt; Agent S2: &lt;small&gt;A Compositional Generalist-Specialist Framework for Computer Use Agents&lt;/small&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&amp;nbsp; üåê &lt;a href=&#34;https://www.simular.ai/articles/agent-s2-technical-review&#34;&gt;[S2 blog]&lt;/a&gt;&amp;nbsp; üìÑ &lt;a href=&#34;https://arxiv.org/abs/2504.00906&#34;&gt;[S2 Paper]&lt;/a&gt;&amp;nbsp; üé• &lt;a href=&#34;https://www.youtube.com/watch?v=wUGVQl7c0eg&#34;&gt;[S2 Video]&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&amp;nbsp; üåê &lt;a href=&#34;https://www.simular.ai/agent-s&#34;&gt;[S1 blog]&lt;/a&gt;&amp;nbsp; üìÑ &lt;a href=&#34;https://arxiv.org/abs/2410.08164&#34;&gt;[S1 Paper (ICLR 2025)]&lt;/a&gt;&amp;nbsp; üé• &lt;a href=&#34;https://www.youtube.com/watch?v=OBDE3Knte0g&#34;&gt;[S1 Video]&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&amp;nbsp; &lt;a href=&#34;https://trendshift.io/repositories/13151&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/13151&#34; alt=&#34;simular-ai%2FAgent-S | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://discord.gg/E2XfsK9fPV&#34;&gt; &lt;img src=&#34;https://dcbadge.limes.pink/api/server/https://discord.gg/E2XfsK9fPV?style=flat&#34; alt=&#34;Discord&#34;&gt; &lt;/a&gt; &amp;nbsp;&amp;nbsp; &lt;a href=&#34;https://pepy.tech/projects/gui-agents&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/badge/gui-agents&#34; alt=&#34;PyPI Downloads&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;h2&gt;ü•≥ Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2025/04/01&lt;/strong&gt;: Released &lt;a href=&#34;https://arxiv.org/abs/2504.00906&#34;&gt;Agent S2 paper&lt;/a&gt; with new SOTA results on OSWorld, WindowsAgentArena, and AndroidWorld!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2025/03/12&lt;/strong&gt;: Released Agent S2 along with v0.2.0 of &lt;a href=&#34;https://github.com/simular-ai/Agent-S&#34;&gt;gui-agents&lt;/a&gt;, the new state-of-the-art for computer use agents (CUA), outperforming OpenAI&#39;s CUA/Operator and Anthropic&#39;s Claude 3.7 Sonnet Computer-Use!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2025/01/22&lt;/strong&gt;: The &lt;a href=&#34;https://arxiv.org/abs/2410.08164&#34;&gt;Agent S paper&lt;/a&gt; is accepted to ICLR 2025!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2025/01/21&lt;/strong&gt;: Released v0.1.2 of &lt;a href=&#34;https://github.com/simular-ai/Agent-S&#34;&gt;gui-agents&lt;/a&gt; library, with support for Linux and Windows!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2024/12/05&lt;/strong&gt;: Released v0.1.0 of &lt;a href=&#34;https://github.com/simular-ai/Agent-S&#34;&gt;gui-agents&lt;/a&gt; library, allowing you to use Agent-S for Mac, OSWorld, and WindowsAgentArena with ease!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;strong&gt;2024/10/10&lt;/strong&gt;: Released the &lt;a href=&#34;https://arxiv.org/abs/2410.08164&#34;&gt;Agent S paper&lt;/a&gt; and codebase!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-introduction&#34;&gt;üí° Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-current-results&#34;&gt;üéØ Current Results&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#%EF%B8%8F-installation--setup&#34;&gt;üõ†Ô∏è Installation &amp;amp; Setup&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-usage&#34;&gt;üöÄ Usage&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-acknowledgements&#34;&gt;ü§ù Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/#-citation&#34;&gt;üí¨ Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;üí° Introduction&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s2_teaser.png&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;Welcome to &lt;strong&gt;Agent S&lt;/strong&gt;, an open-source framework designed to enable autonomous interaction with computers through Agent-Computer Interface. Our mission is to build intelligent GUI agents that can learn from past experiences and perform complex tasks autonomously on your computer.&lt;/p&gt; &#xA;&lt;p&gt;Whether you&#39;re interested in AI, automation, or contributing to cutting-edge agent-based systems, we&#39;re excited to have you here!&lt;/p&gt; &#xA;&lt;h2&gt;üéØ Current Results&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/images/agent_s2_osworld_result.png&#34; width=&#34;600&#34;&gt; &lt;br&gt; Results of Agent S2&#39;s Successful Rate (%) on the OSWorld full test set using Screenshot input only. &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table border=&#34;0&#34; cellspacing=&#34;0&#34; cellpadding=&#34;5&#34;&gt; &#xA;  &lt;tbody&gt;&#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Benchmark&lt;/th&gt; &#xA;    &lt;th&gt;Agent S2&lt;/th&gt; &#xA;    &lt;th&gt;Previous SOTA&lt;/th&gt; &#xA;    &lt;th&gt;Œî improve&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OSWorld (15 step)&lt;/td&gt; &#xA;    &lt;td&gt;27.0%&lt;/td&gt; &#xA;    &lt;td&gt;22.7% (UI-TARS)&lt;/td&gt; &#xA;    &lt;td&gt;+4.3%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OSWorld (50 step)&lt;/td&gt; &#xA;    &lt;td&gt;34.5%&lt;/td&gt; &#xA;    &lt;td&gt;32.6% (OpenAI CUA)&lt;/td&gt; &#xA;    &lt;td&gt;+1.9%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;WindowsAgentArena&lt;/td&gt; &#xA;    &lt;td&gt;29.8%&lt;/td&gt; &#xA;    &lt;td&gt;19.5% (NAVI)&lt;/td&gt; &#xA;    &lt;td&gt;+10.3%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;AndroidWorld&lt;/td&gt; &#xA;    &lt;td&gt;54.3%&lt;/td&gt; &#xA;    &lt;td&gt;46.8% (UI-TARS)&lt;/td&gt; &#xA;    &lt;td&gt;+7.5%&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt;&#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Installation &amp;amp; Setup&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùó&lt;strong&gt;Warning&lt;/strong&gt;‚ùó: If you are on a Linux machine, creating a &lt;code&gt;conda&lt;/code&gt; environment will interfere with &lt;code&gt;pyatspi&lt;/code&gt;. As of now, there&#39;s no clean solution for this issue. Proceed through the installation without using &lt;code&gt;conda&lt;/code&gt; or any virtual environment.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ö†Ô∏è&lt;strong&gt;Disclaimer&lt;/strong&gt;‚ö†Ô∏è: To leverage the full potential of Agent S2, we utilize &lt;a href=&#34;https://github.com/bytedance/UI-TARS&#34;&gt;UI-TARS&lt;/a&gt; as a grounding model (7B-DPO or 72B-DPO for better performance). They can be hosted locally, or on Hugging Face Inference Endpoints. Our code supports Hugging Face Inference Endpoints. Check out &lt;a href=&#34;https://huggingface.co/learn/cookbook/en/enterprise_dedicated_endpoints&#34;&gt;Hugging Face Inference Endpoints&lt;/a&gt; for more information on how to set up and query this endpoint. However, running Agent S2 does not require this model, and you can use alternative API based models for visual grounding, such as Claude.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Install the package:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gui-agents&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Set your LLM API Keys and other environment variables. You can do this by adding the following line to your .bashrc (Linux), or .zshrc (MacOS) file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export OPENAI_API_KEY=&amp;lt;YOUR_API_KEY&amp;gt;&#xA;export ANTHROPIC_API_KEY=&amp;lt;YOUR_ANTHROPIC_API_KEY&amp;gt;&#xA;export HF_TOKEN=&amp;lt;YOUR_HF_TOKEN&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can set the environment variable in your Python script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import os&#xA;os.environ[&#34;OPENAI_API_KEY&#34;] = &#34;&amp;lt;YOUR_API_KEY&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also support Azure OpenAI, Anthropic, Gemini, Open Router, and vLLM inference. For more information refer to &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&#34;&gt;models.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Setup Retrieval from Web using Perplexica&lt;/h3&gt; &#xA;&lt;p&gt;Agent S works best with web-knowledge retrieval. To enable this feature, you need to setup Perplexica:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure Docker Desktop is installed and running on your system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Navigate to the directory containing the project files.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt; cd Perplexica&#xA; git submodule update --init&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Rename the &lt;code&gt;sample.config.toml&lt;/code&gt; file to &lt;code&gt;config.toml&lt;/code&gt;. For Docker setups, you need only fill in the following fields:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;OPENAI&lt;/code&gt;: Your OpenAI API key. &lt;strong&gt;You only need to fill this if you wish to use OpenAI&#39;s models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;OLLAMA&lt;/code&gt;: Your Ollama API URL. You should enter it as &lt;code&gt;http://host.docker.internal:PORT_NUMBER&lt;/code&gt;. If you installed Ollama on port 11434, use &lt;code&gt;http://host.docker.internal:11434&lt;/code&gt;. For other ports, adjust accordingly. &lt;strong&gt;You need to fill this if you wish to use Ollama&#39;s models instead of OpenAI&#39;s&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;GROQ&lt;/code&gt;: Your Groq API key. &lt;strong&gt;You only need to fill this if you wish to use Groq&#39;s hosted models&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;ANTHROPIC&lt;/code&gt;: Your Anthropic API key. &lt;strong&gt;You only need to fill this if you wish to use Anthropic models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can change these after starting Perplexica from the settings dialog.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;code&gt;SIMILARITY_MEASURE&lt;/code&gt;: The similarity measure to use (This is filled by default; you can leave it as is if you are unsure about it.)&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Ensure you are in the directory containing the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and execute:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Next, export your Perplexica URL. This URL is used to interact with the Perplexica API backend. The port is given by the &lt;code&gt;config.toml&lt;/code&gt; in your Perplexica directory.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export PERPLEXICA_URL=http://localhost:{port}/api/search&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Our implementation of Agent S incorporates the Perplexica API to integrate a search engine capability, which allows for a more convenient and responsive user experience. If you want to tailor the API to your settings and specific requirements, you may modify the URL and the message of request parameters in &lt;code&gt;agent_s/query_perplexica.py&lt;/code&gt;. For a comprehensive guide on configuring the Perplexica API, please refer to &lt;a href=&#34;https://github.com/ItzCrazyKns/Perplexica/raw/master/docs/API/SEARCH.md&#34;&gt;Perplexica Search API Documentation&lt;/a&gt;. For a more detailed setup and usage guide, please refer to the &lt;a href=&#34;https://github.com/ItzCrazyKns/Perplexica.git&#34;&gt;Perplexica Repository&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;‚ùó&lt;strong&gt;Warning&lt;/strong&gt;‚ùó: The agent will directly run python code to control your computer. Please use with care.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;üöÄ Usage&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Our best configuration uses Claude 3.7 with extended thinking and UI-TARS-72B-DPO. If you are unable to run UI-TARS-72B-DPO due to resource constraints, UI-TARS-7B-DPO can be used as a lighter alternative with minimal performance degradation.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;CLI&lt;/h3&gt; &#xA;&lt;p&gt;Run Agent S2 with a specific model (default is &lt;code&gt;gpt-4o&lt;/code&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;agent_s2 \&#xA;  --provider &#34;anthropic&#34; \&#xA;  --model &#34;claude-3-7-sonnet-20250219&#34; \&#xA;  --grounding_model_provider &#34;anthropic&#34; \&#xA;  --grounding_model &#34;claude-3-7-sonnet-20250219&#34; \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or use a custom endpoint:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;agent_s2 \&#xA;  --provider &#34;anthropic&#34; \&#xA;  --model &#34;claude-3-7-sonnet-20250219&#34; \&#xA;  --endpoint_provider &#34;huggingface&#34; \&#xA;  --endpoint_url &#34;&amp;lt;endpoint_url&amp;gt;/v1/&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Main Model Settings&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;--provider&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;--model&lt;/code&gt;&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: Specifies the main generation model&lt;/li&gt; &#xA;   &lt;li&gt;Supports: all model providers in &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&#34;&gt;models.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Default: &lt;code&gt;--provider &#34;anthropic&#34; --model &#34;claude-3-7-sonnet-20250219&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Grounding Configuration Options&lt;/h4&gt; &#xA;&lt;p&gt;You can use either Configuration 1 or Configuration 2:&lt;/p&gt; &#xA;&lt;h5&gt;&lt;strong&gt;(Default) Configuration 1: API-Based Models&lt;/strong&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;&lt;code&gt;--grounding_model_provider&lt;/code&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;code&gt;--grounding_model&lt;/code&gt;&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: Specifies the model for visual grounding (coordinate prediction)&lt;/li&gt; &#xA;   &lt;li&gt;Supports: all model providers in &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&#34;&gt;models.md&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Default: &lt;code&gt;--grounding_model_provider &#34;anthropic&#34; --grounding_model &#34;claude-3-7-sonnet-20250219&#34;&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;‚ùó&lt;strong&gt;Important&lt;/strong&gt;‚ùó &lt;strong&gt;&lt;code&gt;--grounding_model_resize_width&lt;/code&gt;&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: Some API providers automatically rescale images. Therefore, the generated (x, y) will be relative to the rescaled image dimensions, instead of the original image dimensions.&lt;/li&gt; &#xA;   &lt;li&gt;Supports: &lt;a href=&#34;https://docs.anthropic.com/en/docs/build-with-claude/vision#&#34;&gt;Anthropic rescaling&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Tips: If your grounding is inaccurate even for very simple queries, double check your rescaling width is correct for your machine&#39;s resolution.&lt;/li&gt; &#xA;   &lt;li&gt;Default: &lt;code&gt;--grounding_model_resize_width 1366&lt;/code&gt; (Anthropic)&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h5&gt;&lt;strong&gt;Configuration 2: Custom Endpoint&lt;/strong&gt;&lt;/h5&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;--endpoint_provider&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: Specifies the endpoint provider&lt;/li&gt; &#xA;   &lt;li&gt;Supports: HuggingFace TGI, vLLM, Open Router&lt;/li&gt; &#xA;   &lt;li&gt;Default: None&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;&lt;code&gt;--endpoint_url&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Purpose: The URL for your custom endpoint&lt;/li&gt; &#xA;   &lt;li&gt;Default: None&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: Configuration 2 takes precedence over Configuration 1.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;This will show a user query prompt where you can enter your query and interact with Agent S2. You can use any model from the list of supported models in &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/models.md&#34;&gt;models.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;&lt;code&gt;gui_agents&lt;/code&gt; SDK&lt;/h3&gt; &#xA;&lt;p&gt;First, we import the necessary modules. &lt;code&gt;AgentS2&lt;/code&gt; is the main agent class for Agent S2. &lt;code&gt;OSWorldACI&lt;/code&gt; is our grounding agent that translates agent actions into executable python code.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;import pyautogui&#xA;import io&#xA;from gui_agents.s2.agents.agent_s import AgentS2&#xA;from gui_agents.s2.agents.grounding import OSWorldACI&#xA;&#xA;# Load in your API keys.&#xA;from dotenv import load_dotenv&#xA;load_dotenv()&#xA;&#xA;current_platform = &#34;linux&#34;  # &#34;darwin&#34;, &#34;windows&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, we define our engine parameters. &lt;code&gt;engine_params&lt;/code&gt; is used for the main agent, and &lt;code&gt;engine_params_for_grounding&lt;/code&gt; is for grounding. For &lt;code&gt;engine_params_for_grounding&lt;/code&gt;, we support the Claude, GPT series, and Hugging Face Inference Endpoints.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;engine_type_for_grounding = &#34;huggingface&#34;&#xA;&#xA;engine_params = {&#xA;    &#34;engine_type&#34;: &#34;openai&#34;,&#xA;    &#34;model&#34;: &#34;gpt-4o&#34;,&#xA;}&#xA;&#xA;if engine_type_for_grounding == &#34;huggingface&#34;:&#xA;  engine_params_for_grounding = {&#xA;      &#34;engine_type&#34;: &#34;huggingface&#34;,&#xA;      &#34;endpoint_url&#34;: &#34;&amp;lt;endpoint_url&amp;gt;/v1/&#34;,&#xA;  }&#xA;elif engine_type_for_grounding == &#34;claude&#34;:&#xA;  engine_params_for_grounding = {&#xA;      &#34;engine_type&#34;: &#34;claude&#34;,&#xA;      &#34;model&#34;: &#34;claude-3-7-sonnet-20250219&#34;,&#xA;  }&#xA;elif engine_type_for_grounding == &#34;gpt&#34;:&#xA;  engine_params_for_grounding = {&#xA;    &#34;engine_type&#34;: &#34;gpt&#34;,&#xA;    &#34;model&#34;: &#34;gpt-4o&#34;,&#xA;  }&#xA;else:&#xA;  raise ValueError(&#34;Invalid engine type for grounding&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, we define our grounding agent and Agent S2.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;grounding_agent = OSWorldACI(&#xA;    platform=current_platform,&#xA;    engine_params_for_generation=engine_params,&#xA;    engine_params_for_grounding=engine_params_for_grounding&#xA;)&#xA;&#xA;agent = AgentS2(&#xA;  engine_params,&#xA;  grounding_agent,&#xA;  platform=current_platform,&#xA;  action_space=&#34;pyautogui&#34;,&#xA;  observation_type=&#34;mixed&#34;,&#xA;  search_engine=&#34;Perplexica&#34;  # Assuming you have set up Perplexica.&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Finally, let&#39;s query the agent!&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Get screenshot.&#xA;screenshot = pyautogui.screenshot()&#xA;buffered = io.BytesIO() &#xA;screenshot.save(buffered, format=&#34;PNG&#34;)&#xA;screenshot_bytes = buffered.getvalue()&#xA;&#xA;obs = {&#xA;  &#34;screenshot&#34;: screenshot_bytes,&#xA;}&#xA;&#xA;instruction = &#34;Close VS Code&#34;&#xA;info, action = agent.predict(instruction=instruction, observation=obs)&#xA;&#xA;exec(action[0])&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Refer to &lt;code&gt;gui_agents/s2/cli_app.py&lt;/code&gt; for more details on how the inference loop works.&lt;/p&gt; &#xA;&lt;h4&gt;Downloading the Knowledge Base&lt;/h4&gt; &#xA;&lt;p&gt;Agent S2 uses a knowledge base that continually updates with new knowledge during inference. The knowledge base is initially downloaded when initializing &lt;code&gt;AgentS2&lt;/code&gt;. The knowledge base is stored as assets under our &lt;a href=&#34;https://github.com/simular-ai/Agent-S/releases&#34;&gt;GitHub Releases&lt;/a&gt;. The &lt;code&gt;AgentS2&lt;/code&gt; initialization will only download the knowledge base for your specified platform and agent version (e.g s1, s2). If you&#39;d like to download the knowledge base programmatically, you can use the following code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;download_kb_data(&#xA;    version=&#34;s2&#34;,&#xA;    release_tag=&#34;v0.2.2&#34;,&#xA;    download_dir=&#34;kb_data&#34;,&#xA;    platform=&#34;linux&#34;  # &#34;darwin&#34;, &#34;windows&#34;&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will download Agent S2&#39;s knowledge base for Linux from release tag &lt;code&gt;v0.2.2&lt;/code&gt; to the &lt;code&gt;kb_data&lt;/code&gt; directory. Refer to our &lt;a href=&#34;https://github.com/simular-ai/Agent-S/releases&#34;&gt;GitHub Releases&lt;/a&gt; or release tags that include the knowledge bases.&lt;/p&gt; &#xA;&lt;h3&gt;OSWorld&lt;/h3&gt; &#xA;&lt;p&gt;To deploy Agent S2 in OSWorld, follow the &lt;a href=&#34;https://raw.githubusercontent.com/simular-ai/Agent-S/main/OSWorld.md&#34;&gt;OSWorld Deployment instructions&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;We extend our sincere thanks to Tianbao Xie for developing OSWorld and discussing computer use challenges. We also appreciate the engaging discussions with Yujia Qin and Shihao Liang regarding UI-TARS.&lt;/p&gt; &#xA;&lt;h2&gt;üí¨ Citations&lt;/h2&gt; &#xA;&lt;p&gt;If you find this codebase useful, please cite&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{Agent-S2,&#xA;      title={Agent S2: A Compositional Generalist-Specialist Framework for Computer Use Agents}, &#xA;      author={Saaket Agashe and Kyle Wong and Vincent Tu and Jiachen Yang and Ang Li and Xin Eric Wang},&#xA;      year={2025},&#xA;      eprint={2504.00906},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI},&#xA;      url={https://arxiv.org/abs/2504.00906}, &#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;@inproceedings{Agent-S,&#xA;    title={{Agent S: An Open Agentic Framework that Uses Computers Like a Human}},&#xA;    author={Saaket Agashe and Jiuzhou Han and Shuyu Gan and Jiachen Yang and Ang Li and Xin Eric Wang},&#xA;    booktitle={International Conference on Learning Representations (ICLR)},&#xA;    year={2025},&#xA;    url={https://arxiv.org/abs/2410.08164}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>