<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-18T01:26:38Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>openai/weak-to-strong</title>
    <updated>2023-12-18T01:26:38Z</updated>
    <id>tag:github.com,2023-12-18:/openai/weak-to-strong</id>
    <link href="https://github.com/openai/weak-to-strong" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;strong&gt;STATUS&lt;/strong&gt;: This codebase is not well tested and does not use the exact same settings we used in the paper, but in our experience gives qualitatively similar results when using large model size gaps and multiple seeds. Expected results can be found for two datasets below. We may update the code significantly in the coming week.&lt;/p&gt; &#xA;&lt;h1&gt;Weak-to-strong generalization&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/weak-to-strong-setup.png&#34; alt=&#34;Our setup and how it relates to superhuman AI alignment&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This project contains code for implementing our &lt;a href=&#34;https://cdn.openai.com/papers/weak-to-strong-generalization.pdf&#34;&gt;paper on weak-to-strong generalization&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The primary codebase contains a re-implementation of our weak-to-strong learning setup for binary classification tasks. The codebase contains code for fine-tuning pretrained language models, and also training against the labels from another language model. We support various losses described in the paper as well, such as the confidence auxiliary loss.&lt;/p&gt; &#xA;&lt;p&gt;The &lt;code&gt;vision&lt;/code&gt; directory contains stand-alone code for weak-to-strong in the vision models setting (AlexNet -&amp;gt; DINO on ImageNet).&lt;/p&gt; &#xA;&lt;h3&gt;Getting Started&lt;/h3&gt; &#xA;&lt;p&gt;These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.&lt;/p&gt; &#xA;&lt;h4&gt;Installation&lt;/h4&gt; &#xA;&lt;p&gt;You need to have Python installed on your machine. The project uses &lt;code&gt;pyproject.toml&lt;/code&gt; to manage dependencies. To install the dependencies, you can use a package manager like &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Running the Script&lt;/h4&gt; &#xA;&lt;p&gt;The main script of the project is train_weak_to_strong.py. It can be run from the command line using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_weak_to_strong.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script accepts several command-line arguments to customize the training process. Here are some examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python train_weak_to_strong.py --batch_size 32 --max_ctx 512 --ds_name &#34;sciq&#34; --loss &#34;logconf&#34; --n_docs 1000 --n_test_docs 100 --weak_model_size &#34;gpt2-medium&#34; --strong_model_size &#34;gpt2-large&#34; --seed 42&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Expected results&lt;/h4&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/notebooks/amazon_polarity_None.png&#34; width=&#34;350&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/notebooks/sciq_None.png&#34; width=&#34;350&#34;&gt; &#xA;&lt;br&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/openai/weak-to-strong/main/notebooks/Anthropic-hh-rlhf_None.png&#34; width=&#34;350&#34;&gt; &#xA;&lt;h3&gt;Authors&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adrien Ecoffet&lt;/li&gt; &#xA; &lt;li&gt;Manas Joglekar&lt;/li&gt; &#xA; &lt;li&gt;Jeffrey Wu&lt;/li&gt; &#xA; &lt;li&gt;Jan Hendrik Kirchner&lt;/li&gt; &#xA; &lt;li&gt;Pavel Izmailov (vision)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;License&lt;/h3&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License - see the LICENSE.md file for details.&lt;/p&gt; &#xA;&lt;h3&gt;Acknowledgments&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Hugging Face for their open-source transformer models&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>robotics-survey/Awesome-Robotics-Foundation-Models</title>
    <updated>2023-12-18T01:26:38Z</updated>
    <id>tag:github.com,2023-12-18:/robotics-survey/Awesome-Robotics-Foundation-Models</id>
    <link href="https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Awesome-Robotics-Foundation-Models&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://awesome.re&#34;&gt;&lt;img src=&#34;https://awesome.re/badge.svg?sanitize=true&#34; alt=&#34;Awesome&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/robotics-survey/Awesome-Robotics-Foundation-Models/raw/main/survey_tree.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the partner repository for the survey paper &#34;Foundation Models in Robotics: Applications, Challenges, and the Future&#34;. The authors hope this repository can act as a quick reference for roboticists who wish to read the relevant papers and implement the associated methods. The organization of this readme follows Figure 1 in the paper (shown above) and is thus divided into foundation models that have been applied to robotics and those that are relevant to robotics in some way.&lt;/p&gt; &#xA;&lt;p&gt;We welcome contributions to this repository to add more resources. Please submit a pull request if you want to contribute!&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#survey&#34;&gt;Survey&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#robotics&#34;&gt;Robotics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#robot-policy-learning-for-decision-making-and-controls&#34;&gt;Robot Policy Learning for Decision Making and Controls&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#language-image-goal-conditioned-value-learning&#34;&gt;Language-Image Goal-Conditioned Value Learning&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#robot-task-planning-using-large-language-models&#34;&gt;Robot Task Planning Using Large Language Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#robot-transformers&#34;&gt;Robot Transformers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#in-context-learning-for-decision-making&#34;&gt;In-context Learning for Decision-Making&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#open-vocabulary-robot-navigation-and-manipulation&#34;&gt;Open-Vocabulary Robot Navigation and Manipulation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#relevant-to-robotics&#34;&gt;Relevant to Robotics&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#open-vocabulary-object-detection-and-3D-classification&#34;&gt;Open-Vocabulary Object Detection and 3D Classification&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#open-vocabulary-semantic-segmentation&#34;&gt;Open-Vocabulary Semantic Segmentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#open-vocabulary-3D-scene-representations&#34;&gt;Open-Vocabulary 3D Scene Representations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#open-vocabulary-object-representations&#34;&gt;Open-Vocabulary Object Representations&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#affordance-information&#34;&gt;Affordance Information&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#predictive-models&#34;&gt;Predictive Models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#generalist-AI&#34;&gt;Generalist AI&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/robotics-survey/Awesome-Robotics-Foundation-Models/main/#simulators&#34;&gt;Simulators&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Survey&lt;/h2&gt; &#xA;&lt;p&gt;This repository is largely based on the following paper:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;&#34;&gt;Foundation Models in Robotics: Applications, Challenges, and the Future&lt;/a&gt;&lt;/strong&gt; &lt;br&gt; Roya Firoozi Jiankai Sun, Johnathan Tucker, Anirudha Majumdar, Yuke Zhu, Shuran Song, Ashish Kapoor, Weiyu Liu, Stephen Tian, Karol Hausman, Brian Ichter, Danny Driess, Jiajun Wu, Cewu Lu, Mac Schwager &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you find this repository helpful, please consider citing:&lt;/p&gt; &#xA;&lt;h2&gt;Robotics&lt;/h2&gt; &#xA;&lt;h3&gt;Robot Policy Learning for Decision-Making and Controls&lt;/h3&gt; &#xA;&lt;h4&gt;Language-Conditioned Imitation Learning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CLIPort: What and Where Pathways for Robotic Manipulation &lt;a href=&#34;https://arxiv.org/abs/2109.12098&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://cliport.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/cliport/cliport&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation &lt;a href=&#34;https://arxiv.org/abs/2209.05451&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://peract.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/peract/peract&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Play-LMP: Learning Latent Plans from Play &lt;a href=&#34;https://learning-from-play.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Multi-Context Imitation: Language-Conditioned Imitation Learning over Unstructured Data &lt;a href=&#34;https://language-play.github.io&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Language-Assisted Reinforcement Learning&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Towards A Unified Agent with Foundation Models &lt;a href=&#34;https://arxiv.org/abs/2307.09668&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Reward Design with Language Models &lt;a href=&#34;https://arxiv.org/abs/2303.00001&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning to generate better than your llm &lt;a href=&#34;https://arxiv.org/pdf/2306.11816.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/Cornell-RL/tril&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Guiding Pretraining in Reinforcement Learning with Large Language Models &lt;a href=&#34;https://arxiv.org/abs/2302.06692&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/yuqingd/ellm&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Language-Image Goal-Conditioned Value Learning&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;SayCan: Do As I Can, Not As I Say: Grounding Language in Robotic Affordances &lt;a href=&#34;https://arxiv.org/abs/2204.01691&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://say-can.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/saycan&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Zero-Shot Reward Specification via Grounded Natural Language &lt;a href=&#34;https://proceedings.mlr.press/v162/mahmoudieh22a/mahmoudieh22a.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models &lt;a href=&#34;https://voxposer.github.io&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training &lt;a href=&#34;https://arxiv.org/abs/2210.00030&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://sites.google.com/view/vip-rl&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LIV: Language-Image Representations and Rewards for Robotic Control &lt;a href=&#34;https://arxiv.org/abs/2306.00958&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://penn-pal-lab.github.io/LIV/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LOReL: Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation &lt;a href=&#34;https://arxiv.org/abs/2109.01115&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://sites.google.com/view/robotlorel&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Text2Motion: From Natural Language Instructions to Feasible Plans &lt;a href=&#34;https://arxiv.org/abs/2303.12153&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://sites.google.com/stanford.edu/text2motion&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Robot Task Planning Using Large Language Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents &lt;a href=&#34;https://arxiv.org/abs/2201.07207&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://wenlong.page/language-planner/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open-vocabulary Queryable Scene Representations for Real World Planning (NLMap) &lt;a href=&#34;https://arxiv.org/pdf/2209.09874.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://nlmap-saycan.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models &lt;a href=&#34;https://arxiv.org/pdf/2305.07766.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://yongchao98.github.io/MIT-realm-NL2TL/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/yongchao98/NL2TL&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers&lt;a href=&#34;https://arxiv.org/abs/2306.06531&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://yongchao98.github.io/MIT-REALM-AutoTAMP/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LATTE: LAnguage Trajectory TransformEr &lt;a href=&#34;https://arxiv.org/abs/2208.02918&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/arthurfenderbucker/LaTTe-Language-Trajectory-TransformEr&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Planning with Large Language Models via Corrective Re-prompting &lt;a href=&#34;https://arxiv.org/abs/2211.09935&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Describe, explain, plan and select: interactive planning with LLMs enables open-world multi-task agents &lt;a href=&#34;https://arxiv.org/pdf/2302.01560.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/CraftJarvis/MC-Planner&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models &lt;a href=&#34;https://arxiv.org/pdf/2311.05997.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://craftjarvis.github.io/JARVIS-1/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/CraftJarvis/JARVIS-1&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An Embodied Generalist Agent in 3D World &lt;a href=&#34;https://arxiv.org/pdf/2311.12871.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://embodied-generalist.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/embodied-generalist/embodied-generalist&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LLM-Based Code Generation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;ProgPrompt: Generating Situated Robot Task Plans using Large Language Models &lt;a href=&#34;https://arxiv.org/abs/2209.11302&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://progprompt.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Code as Policies: Language Model Programs for Embodied Control &lt;a href=&#34;https://arxiv.org/abs/2209.07753&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://code-as-policies.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ChatGPT for Robotics: Design Principles and Model Abilities &lt;a href=&#34;https://arxiv.org/abs/2306.17582&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/chatgpt-for-robotics/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/microsoft/PromptCraft-Robotics&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Voyager: An Open-Ended Embodied Agent with Large Language Models &lt;a href=&#34;https://arxiv.org/abs/2305.16291&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://voyager.minedojo.org/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Visual Programming: Compositional visual reasoning without training &lt;a href=&#34;https://arxiv.org/abs/2211.11559&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://prior.allenai.org/projects/visprog&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/allenai/visprog&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Robot Transformers&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MotionGPT: Finetuned LLMs are General-Purpose Motion Generators &lt;a href=&#34;https://arxiv.org/abs/2306.10900&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://qiqiapink.github.io/MotionGPT/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;RT-1: Robotics Transformer for Real-World Control at Scale &lt;a href=&#34;https://robotics-transformer.github.io/assets/rt1.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://robotics-transformer.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/google-research/robotics_transformer&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Masked Visual Pre-training for Motor Control &lt;a href=&#34;https://arxiv.org/abs/2203.06173&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://tetexiao.com/projects/mvp&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/ir413/mvp&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Real-world robot learning with masked visual pre-training &lt;a href=&#34;https://arxiv.org/abs/2210.03109&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://tetexiao.com/projects/real-mvp&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Robot Learning with Sensorimotor Pre-training &lt;a href=&#34;https://arxiv.org/abs/2306.10007&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://robotic-pretrained-transformer.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Rt-2: Vision-language-action models transfer web knowledge to robotic control &lt;a href=&#34;https://arxiv.org/abs/2307.15818&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://robotics-transformer2.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-Training &lt;a href=&#34;https://arxiv.org/abs/2209.11133&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GROOT: Learning to Follow Instructions by Watching Gameplay Videos &lt;a href=&#34;https://arxiv.org/pdf/2310.08235.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://craftjarvis.github.io/GROOT/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/CraftJarvis/GROOT&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;In-context Learning for Decision-Making&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A Survey on In-context Learning &lt;a href=&#34;https://arxiv.org/abs/2301.00234&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Large Language Models as General Pattern Machines &lt;a href=&#34;https://arxiv.org/abs/2307.04721&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Chain-of-Thought Predictive Control &lt;a href=&#34;https://arxiv.org/abs/2304.00776&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ReAct: Synergizing Reasoning and Acting in Language Models &lt;a href=&#34;https://arxiv.org/abs/2210.03629&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Open-Vocabulary Robot Navigation and Manipulation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;CoWs on PASTURE: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation &lt;a href=&#34;https://arxiv.org/pdf/2203.10421.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://cow.cs.columbia.edu/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open-vocabulary Queryable Scene Representations for Real World Planning (NLMap) &lt;a href=&#34;https://arxiv.org/pdf/2209.09874.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://nlmap-saycan.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LSC: Language-guided Skill Coordination for Open-Vocabulary Mobile Pick-and-Place &lt;a href=&#34;&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://languageguidedskillcoordination.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;L3MVN: Leveraging Large Language Models for Visual Target Navigation &lt;a href=&#34;https://arxiv.org/abs/2304.05501&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open-World Object Manipulation using Pre-trained Vision-Language Models &lt;a href=&#34;https://robot-moo.github.io/assets/moo.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://robot-moo.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VIMA: General Robot Manipulation with Multimodal Prompts &lt;a href=&#34;https://arxiv.org/abs/2210.03094&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://vimalabs.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/vimalabs/VIMA&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Diffusion-based Generation, Optimization, and Planning in 3D Scenes &lt;a href=&#34;https://arxiv.org/pdf/2301.06015.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://scenediffuser.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/scenediffuser/Scene-Diffuser&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery &lt;a href=&#34;http://arxiv.org/abs/2311.02058&#34;&gt;[Paper]&lt;/a&gt; &lt;a href=&#34;https://ut-austin-rpl.github.io/Lotus/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Relevant to Robotics (Perception)&lt;/h2&gt; &#xA;&lt;h3&gt;Open-Vocabulary Object Detection and 3D Classification&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Simple Open-Vocabulary Object Detection with Vision Transformers &lt;a href=&#34;https://arxiv.org/pdf/2205.06230.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Grounded Language-Image Pre-training &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/microsoft/GLIP&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection &lt;a href=&#34;https://arxiv.org/abs/2303.05499&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;PointCLIP: Point Cloud Understanding by CLIP &lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_PointCLIP_Point_Cloud_Understanding_by_CLIP_CVPR_2022_paper.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/ZrrSkywalker/PointCLIP&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Point-bert: Pre-training 3d point cloud transformers with masked point modeling &lt;a href=&#34;https://arxiv.org/abs/2111.14819&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/lulutang0608/Point-BERT&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding &lt;a href=&#34;https://arxiv.org/abs/2212.05171&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://tycho-xue.github.io/ULIP/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/salesforce/ULIP&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ulip-2: Towards scalable multimodal pre-training for 3d understanding &lt;a href=&#34;https://arxiv.org/pdf/2305.08275.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/salesforce/ULIP&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment &lt;a href=&#34;https://arxiv.org/pdf/2308.04352.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://3d-vista.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/3d-vista/3D-VisTA&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Open-Vocabulary Semantic Segmentation&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Language-driven Semantic Segmentation &lt;a href=&#34;https://arxiv.org/abs/2201.03546&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/isl-org/lang-seg&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Emerging Properties in Self-Supervised Vision Transformers &lt;a href=&#34;https://arxiv.org/abs/2104.14294&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/facebookresearch/dino&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Segment Anything &lt;a href=&#34;https://arxiv.org/abs/2304.02643&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://segment-anything.com/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fast segment anything &lt;a href=&#34;https://arxiv.org/abs/2306.12156&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/CASIA-IVA-Lab/FastSAM&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Faster Segment Anything: Towards Lightweight SAM for Mobile Applications &lt;a href=&#34;https://arxiv.org/abs/2306.14289&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/ChaoningZhang/MobileSAM&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Track anything: Segment anything meets videos &lt;a href=&#34;https://arxiv.org/abs/2304.11968&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/gaomingqi/Track-Anything&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Open-Vocabulary 3D Scene Representations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Open-vocabulary Queryable Scene Representations for Real World Planning (NLMap) &lt;a href=&#34;https://arxiv.org/pdf/2209.09874.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://nlmap-saycan.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Clip-NeRF: Text-and-image driven manipulation of neural radiance fields &lt;a href=&#34;https://arxiv.org/abs/2112.05139&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://cassiepython.github.io/clipnerf/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LERF: Language Embedded Radiance Fields &lt;a href=&#34;https://arxiv.org/abs/2303.09553&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://www.lerf.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/kerrj/lerf&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Decomposing nerf for editing via feature field distillation &lt;a href=&#34;https://arxiv.org/abs/2205.15585&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://pfnet-research.github.io/distilled-feature-fields&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Object Representations&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation &lt;a href=&#34;https://arxiv.org/abs/2112.05124&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://yilundu.github.io/ndf/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation &lt;a href=&#34;https://arxiv.org/abs/2308.07931&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://f3rm.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;You Only Look at One: Category-Level Object Representations for Pose Estimation From a Single Example &lt;a href=&#34;https://arxiv.org/abs/2305.12626&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Zero-Shot Category-Level Object Pose Estimation &lt;a href=&#34;https://arxiv.org/abs/2204.03635&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/applied-ai-lab/zero-shot-pose&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VIOLA: Imitation Learning for Vision-Based Manipulation with Object Proposal Priors &lt;a href=&#34;https://arxiv.org/abs/2210.11339&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://ut-austin-rpl.github.io/VIOLA/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/UT-Austin-RPL/VIOLA&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning Generalizable Manipulation Policies with Object-Centric 3D Representations &lt;a href=&#34;http://arxiv.org/abs/2310.14386&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://ut-austin-rpl.github.io/GROOT/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/UT-Austin-RPL/GROOT&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Affordance Information&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Affordance Diffusion: Synthesizing Hand-Object Interactions &lt;a href=&#34;https://arxiv.org/abs/2303.12538&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://judyye.github.io/affordiffusion-www/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Affordances from Human Videos as a Versatile Representation for Robotics &lt;a href=&#34;https://arxiv.org/abs/2304.08488&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://robo-affordances.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Predictive Models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Adversarial Inverse Reinforcement Learning With Self-Attention Dynamics Model &lt;a href=&#34;https://ieeexplore.ieee.org/document/9361118&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Connected Autonomous Vehicle Motion Planning with Video Predictions from Smart, Self-Supervised Infrastructure &lt;a href=&#34;https://arxiv.org/pdf/2309.07504.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Self-Supervised Traffic Advisors: Distributed, Multi-view Traffic Prediction for Smart Cities &lt;a href=&#34;https://arxiv.org/abs/2204.06171&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Planning with diffusion for flexible behavior synthesis &lt;a href=&#34;https://arxiv.org/abs/2205.09991&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Phenaki: Variable-length video generation from open domain textual description &lt;a href=&#34;https://arxiv.org/abs/2210.02399&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Robonet: Large-scale multi-robot learning &lt;a href=&#34;https://arxiv.org/abs/1910.11215&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GAIA-1: A Generative World Model for Autonomous Driving &lt;a href=&#34;https://arxiv.org/abs/2309.17080&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Learning universal policies via text-guided video generation &lt;a href=&#34;https://arxiv.org/abs/2302.00111&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Video language planning &lt;a href=&#34;https://arxiv.org/abs/2302.00111&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Relevant to Robotics (Embodied AI)&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inner Monologue: Embodied Reasoning through Planning with Language Models &lt;a href=&#34;https://arxiv.org/abs/2207.05608&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://innermonologue.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Statler: State-Maintaining Language Models for Embodied Reasoning &lt;a href=&#34;https://arxiv.org/abs/2306.17840&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://statler-lm.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought &lt;a href=&#34;https://arxiv.org/pdf/2305.15021.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://embodiedgpt.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge &lt;a href=&#34;https://openreview.net/forum?id=rc8o_j8I8PX&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/MineDojo/MineDojo&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos &lt;a href=&#34;https://arxiv.org/abs/2206.11795&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Open-World Multi-Task Control Through Goal-Aware Representation Learning and Adaptive Horizon Prediction &lt;a href=&#34;https://arxiv.org/pdf/2301.10034.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/CraftJarvis/MC-Controller&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Describe, explain, plan and select: interactive planning with LLMs enables open-world multi-task agents &lt;a href=&#34;https://arxiv.org/pdf/2302.01560.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/CraftJarvis/MC-Planner&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Voyager: An Open-Ended Embodied Agent with Large Language Models &lt;a href=&#34;https://arxiv.org/abs/2305.16291&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://voyager.minedojo.org/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/MineDojo/Voyager&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory &lt;a href=&#34;https://arxiv.org/abs/2305.17144&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://github.com/OpenGVLab/GITM&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents &lt;a href=&#34;https://arxiv.org/pdf/2201.07207.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://wenlong.page/language-planner/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/huangwl18/language-planner&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;GROOT: Learning to Follow Instructions by Watching Gameplay Videos &lt;a href=&#34;https://arxiv.org/pdf/2310.08235.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://craftjarvis.github.io/GROOT/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/CraftJarvis/GROOT&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;JARVIS-1: Open-World Multi-task Agents with Memory-Augmented Multimodal Language Models &lt;a href=&#34;https://arxiv.org/pdf/2311.05997.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://craftjarvis.github.io/JARVIS-1/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/CraftJarvis/JARVIS-1&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;SQA3D: Situated Question Answering in 3D Scenes &lt;a href=&#34;https://arxiv.org/pdf/2210.07474.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://sqa3d.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/SilongYong/SQA3D&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Generalist AI&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Generative Agents: Interactive Simulacra of Human Behavior &lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Towards Generalist Robots: A Promising Paradigm via Generative Simulation &lt;a href=&#34;https://arxiv.org/abs/2305.10455&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A generalist agent &lt;a href=&#34;https://arxiv.org/abs/2205.06175&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;An Embodied Generalist Agent in 3D World &lt;a href=&#34;https://arxiv.org/pdf/2311.12871.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://embodied-generalist.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/embodied-generalist/embodied-generalist&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Simulators&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Gibson Env: real-world perception for embodied agents &lt;a href=&#34;https://arxiv.org/abs/1808.10654&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks &lt;a href=&#34;https://arxiv.org/abs/2108.03272&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://svl.stanford.edu/igibson/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;BEHAVIOR-1k: A benchmark for embodied AI with 1,000 everyday activities and realistic simulation &lt;a href=&#34;https://openreview.net/forum?id=_8DoIe8G3t&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://behavior.stanford.edu/behavior-1k&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Habitat: A Platform for Embodied AI Research &lt;a href=&#34;https://arxiv.org/abs/1904.01201&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://aihabitat.org/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Habitat 2.0: Training home assistants to rearrange their habitat &lt;a href=&#34;https://arxiv.org/abs/2106.14405&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Robothor: An open simulation-to-real embodied ai platform &lt;a href=&#34;https://arxiv.org/abs/2004.06799&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://ai2thor.allenai.org/robothor/&#34;&gt;[Project]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;VirtualHome: Simulating Household Activities via Programs &lt;a href=&#34;https://arxiv.org/abs/1806.07011&#34;&gt;[Paper]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes &lt;a href=&#34;https://arxiv.org/pdf/2304.04321.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://arnold-benchmark.github.io/&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/arnold-benchmark/arnold&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;LIBERO: Benchmarking Knowledge Transfer in Lifelong Robot Learning &lt;a href=&#34;https://arxiv.org/pdf/2306.03310.pdf&#34;&gt;[Paper]&lt;/a&gt;&lt;a href=&#34;https://lifelong-robot-learning.github.io/LIBERO/html/getting_started/overview.html&#34;&gt;[Project]&lt;/a&gt;&lt;a href=&#34;https://github.com/Lifelong-Robot-Learning/LIBERO&#34;&gt;[Code]&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>bytebase/bytebase</title>
    <updated>2023-12-18T01:26:38Z</updated>
    <id>tag:github.com,2023-12-18:/bytebase/bytebase</id>
    <link href="https://github.com/bytebase/bytebase" rel="alternate"></link>
    <summary type="html">&lt;p&gt;World&#39;s most advanced database DevOps and CI/CD for Developer, DBA and Platform Engineering teams. The GitLab for database DevOps&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt; &lt;a target=&#34;_blank&#34; href=&#34;https://bytebase.com?source=github&#34;&gt; &lt;img align=&#34;center&#34; alt=&#34;Bytebase&#34; src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/banner.webp&#34; style=&#34;width:100%;&#34;&gt; &lt;/a&gt; &lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://bytebase.com/docs/get-started/install/overview&#34; target=&#34;_blank&#34;&gt;&lt;b&gt;⚙️ Install&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://bytebase.com/docs&#34;&gt;&lt;b&gt;📚 Docs&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://discord.gg/huyw7gRsyA&#34;&gt;&lt;b&gt;💬 Discord&lt;/b&gt;&lt;/a&gt; • &lt;a href=&#34;https://www.bytebase.com/request-demo/&#34;&gt;&lt;b&gt;🙋‍♀️ Book Demo&lt;/b&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://goreportcard.com/report/github.com/bytebase/bytebase&#34;&gt; &lt;img alt=&#34;go report&#34; src=&#34;https://goreportcard.com/badge/github.com/bytebase/bytebase&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://artifacthub.io/packages/search?repo=bytebase&#34;&gt; &lt;img alt=&#34;Artifact Hub&#34; src=&#34;https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/bytebase&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://github.com/bytebase/bytebase&#34; target=&#34;_blank&#34;&gt; &lt;img alt=&#34;Github Stars&#34; src=&#34;https://img.shields.io/github/stars/bytebase/bytebase?logo=github&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt; Different &lt;/b&gt; database development tasks &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt; Multiple &lt;/b&gt; database systems &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt; Unified &lt;/b&gt; process &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;b&gt; Single &lt;/b&gt; tool &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/old-to-new-world.webp&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/fish.webp&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/change-query-secure-govern.webp&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt;🪜&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Change&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Want to formalize the database change process but don&#39;t know how? &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;Standard Operating Procedure (SOP) &lt;/b&gt;&lt;br&gt;Standardize the database schema and data change process across different database systems, small or &lt;a href=&#34;https://www.bytebase.com/docs/change-database/online-schema-migration-for-mysql&#34;&gt;large tables&lt;/a&gt; and &lt;a href=&#34;https://www.bytebase.com/docs/change-database/batch-change/#change-databases-from-multiple-tenants&#34;&gt;different tenants&lt;/a&gt;.&lt;br&gt;&lt;br&gt;&lt;b&gt;SQL Review&lt;/b&gt;&lt;br&gt;&lt;a href=&#34;https://www.bytebase.com/docs/sql-review/review-rules&#34;&gt;100+ lint rules&lt;/a&gt; to detect SQL anti-patterns and enforce consistent SQL style in the organization.&lt;br&gt;&lt;br&gt;&lt;b&gt;GitOps&lt;/b&gt;&lt;br&gt;&lt;a href=&#34;https://www.bytebase.com/docs/vcs-integration/overview&#34;&gt;Point-and-click GitHub and GitLab integration&lt;/a&gt; to enable GitOps workflow for changing database.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/issue-detail.webp&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt;🔮&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Query&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Want to control the data access but don&#39;t know how? &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;All-in-one SQL Editor&lt;/b&gt;&lt;br&gt;Web-based IDE specifically for performing SQL specific tasks.&lt;br&gt;&lt;br&gt;&lt;b&gt;Data Masking&lt;/b&gt;&lt;br&gt;State-of-the-art &lt;a href=&#34;https://www.bytebase.com/docs/sql-editor/mask-data&#34;&gt;column level masking&lt;/a&gt; engine to cover complex situations like subquery, CTE.&lt;br&gt;&lt;br&gt;&lt;b&gt;Data Access Control&lt;/b&gt;&lt;br&gt;Organization level policy to centralize the &lt;a href=&#34;https://www.bytebase.com/docs/security/data-access-control&#34;&gt;data access control&lt;/a&gt;.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/sql-editor.webp&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt;🔒&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Secure&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Want to avoid data leakage, change outage and detect malicious behavior but don&#39;t know how? &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;Centralize Change, Query and Admin Tasks&lt;/b&gt;&lt;br&gt;A single place to perform different tasks on different databases, thus enforce policy and monitor activity accordingly. &lt;br&gt;&lt;br&gt;&lt;b&gt;RBAC&lt;/b&gt;&lt;br&gt;&lt;a href=&#34;https://www.bytebase.com/docs/concepts/roles-and-permissions&#34;&gt;Two-level RBAC model&lt;/a&gt; mapping to the organization wide privileges and application team privileges respectively.&lt;br&gt;&lt;br&gt;&lt;b&gt;Anomaly Center and Audit Logging&lt;/b&gt;&lt;br&gt; Capture all database &lt;a href=&#34;https://www.bytebase.com/docs/administration/anomaly-center&#34;&gt;anomalies&lt;/a&gt;, user actions and system events and present them in a holistic view.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/anomaly-center.webp&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;center&#34;&gt;👩‍💼&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Govern&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; Want to enforce organization policy but don&#39;t know how? &lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;b&gt;Manage Database Resources&lt;/b&gt;&lt;br&gt; A single place to manage environments, database instances, database users for application development, with optional &lt;a href=&#34;https://registry.terraform.io/providers/bytebase/bytebase/latest/docs&#34;&gt;Terraform integration&lt;/a&gt;. &lt;br&gt;&lt;br&gt;&lt;b&gt;Policy Enforcement&lt;/b&gt;&lt;br&gt;Enforce organization wide SQL Review policy, backup policy and data access policy.&lt;br&gt;&lt;br&gt;&lt;b&gt;SQL Editor Admin mode&lt;/b&gt;&lt;br&gt;&lt;a href=&#34;https://www.bytebase.com/docs/sql-editor/admin-mode&#34;&gt;CLI like experience&lt;/a&gt; without setting up bastion.&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/sql-review-policy.webp&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🖖 Intro&lt;/h1&gt; &#xA;&lt;p&gt;Bytebase is a Database CI/CD solution for the Developers and DBAs. It&#39;s the &lt;strong&gt;only database CI/CD project&lt;/strong&gt; included by the &lt;a href=&#34;https://landscape.cncf.io/?selected=bytebase&#34;&gt;CNCF Landscape&lt;/a&gt; and &lt;a href=&#34;https://platformengineering.org/tools/bytebase&#34;&gt;Platform Engineering&lt;/a&gt;. The Bytebase family consists of these tools:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bytebase.com/?source=github&#34;&gt;Bytebase Console&lt;/a&gt;: A web-based GUI for developers and DBAs to manage the database development lifecycle.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/docs/cli/overview&#34;&gt;Bytebase CLI (bb)&lt;/a&gt;: The CLI to help developers integrate database changes into the existing CI/CD workflow.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/marketplace/bytebase&#34;&gt;Bytebase GitHub App&lt;/a&gt; and &lt;a href=&#34;https://github.com/marketplace/actions/sql-review&#34;&gt;SQL Review GitHub Action&lt;/a&gt;: The GitHub App and GitHub Action to detect SQL anti-patterns and enforce a consistent SQL style guide during Pull Request.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://registry.terraform.io/providers/bytebase/bytebase/latest/docs&#34;&gt;Terraform Bytebase Provider&lt;/a&gt;: The Terraform provider enables team to manage Bytebase resources via Terraform. A typical setup involves teams using Terraform to provision database instances from Cloud vendors, followed by using Bytebase provider to prepare those instances ready for application use.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Topic&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🔧&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/#-installation&#34;&gt;Installation&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🎮&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/#-demo&#34;&gt;Demo&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;👩‍🏫&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/#-tutorials&#34;&gt;Tutorials&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;💎&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/#-design-principles&#34;&gt;Design Principles&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🧩&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/#-data-model&#34;&gt;Data Model&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🎭&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/#-roles&#34;&gt;Roles&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🕊&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/#-developing-and-contributing&#34;&gt;Developing and Contributing&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🤺&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;b&gt;&lt;a href=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/#-bytebase-vs-alternatives&#34;&gt;Bytebase vs Alternatives&lt;/a&gt;&lt;/b&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🔧 Installation&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;One liner installation script from latest release &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/bin/bash -c &#34;$(curl -fsSL https://raw.githubusercontent.com/bytebase/install/main/install.sh)&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/docs/get-started/install/deploy-with-docker&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/docs/get-started/install/deploy-to-kubernetes&#34;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/docs/get-started/install/build-from-source-code&#34;&gt;Build from source&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🎮 Demo&lt;/h1&gt; &#xA;&lt;p&gt;Live demo at &lt;a href=&#34;https://demo.bytebase.com&#34;&gt;https://demo.bytebase.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can also &lt;a href=&#34;https://cal.com/adela-bytebase/30min&#34;&gt;book a 30min product walkthrough&lt;/a&gt; with one of our product experts.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;👩‍🏫 Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;Product tutorials are available at &lt;a href=&#34;https://www.bytebase.com/tutorial&#34;&gt;https://www.bytebase.com/tutorial&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/docs/how-to/integrations/supabase&#34;&gt;Manage Supabase PostgreSQL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/docs/how-to/integrations/render&#34;&gt;Manage render PostgreSQL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/docs/how-to/integrations/neon&#34;&gt;Manage Neon database&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/docs/get-started/install/deploy-to-sealos&#34;&gt;Deploy to sealos&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/docs/get-started/install/deploy-to-rainbond&#34;&gt;Deploy to Rainbond&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;💎 Design Principles&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🪶&lt;/td&gt; &#xA;   &lt;td&gt;Dependency Free&lt;/td&gt; &#xA;   &lt;td&gt;Start with a single command &lt;code&gt;./bytebase&lt;/code&gt; without any external dependency. External PostgreSQL data store and others are optional.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;🔗&lt;/td&gt; &#xA;   &lt;td&gt;Integration First&lt;/td&gt; &#xA;   &lt;td&gt;Solely focus on database management and leave the rest to others. We have native VCS integration with &lt;a href=&#34;https://www.bytebase.com/docs/vcs-integration/overview&#34;&gt;GitHub/GitLab&lt;/a&gt;, &lt;a href=&#34;https://registry.terraform.io/providers/bytebase/bytebase/latest/docs&#34;&gt;Terraform Provider&lt;/a&gt;, &lt;a href=&#34;https://www.bytebase.com/docs/change-database/webhook&#34;&gt;webhook&lt;/a&gt;, and etc.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;💂‍♀️&lt;/td&gt; &#xA;   &lt;td&gt;Engineering Disciplined&lt;/td&gt; &#xA;   &lt;td&gt;Disciplined &lt;a href=&#34;https://www.bytebase.com/changelog&#34;&gt;bi-weekly release&lt;/a&gt; and &lt;a href=&#34;https://github.com/bytebase/bytebase/raw/main/docs/life-of-a-feature.md&#34;&gt;engineering practice&lt;/a&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🧩 Data Model&lt;/h1&gt; &#xA;&lt;p&gt;More details in &lt;a href=&#34;https://www.bytebase.com/docs/concepts/data-model&#34;&gt;Data Model Doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; alt=&#34;Data Model&#34; src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/data-model-v2.webp&#34; style=&#34;width:100%;&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🎭 Roles&lt;/h1&gt; &#xA;&lt;p&gt;More details in &lt;a href=&#34;https://www.bytebase.com/docs/concepts/roles-and-permissions&#34;&gt;Roles and Permissions Doc&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Bytebase employs RBAC (role based access control) and provides two role sets at the workspace and project level:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Workspace roles: &lt;code&gt;Admin&lt;/code&gt;, &lt;code&gt;DBA&lt;/code&gt;, &lt;code&gt;Member&lt;/code&gt;. The workspace role maps to the role in an organization.&lt;/li&gt; &#xA; &lt;li&gt;Project roles: &lt;code&gt;Owner&lt;/code&gt;, &lt;code&gt;Developer&lt;/code&gt;, &lt;code&gt;Releaser&lt;/code&gt;, &lt;code&gt;Querier&lt;/code&gt;, &lt;code&gt;Exporter&lt;/code&gt;, &lt;code&gt;Viewer&lt;/code&gt;. The project level role maps to the role in a specific team or project.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Every user is assigned a workspace role, and if a particular user is involved in a particular project, then she will also be assigned a project role accordingly.&lt;/p&gt; &#xA;&lt;p&gt;Below diagram describes a typical mapping between an engineering org and the corresponding roles in the Bytebase workspace&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; alt=&#34;Role Mapping&#34; src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/org-role-mapping.webp&#34; style=&#34;width:100%;&#34;&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🕊 Developing and Contributing&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; alt=&#34;Tech Stack&#34; src=&#34;https://raw.githubusercontent.com/bytebase/bytebase/main/docs/assets/techstack.webp&#34; style=&#34;width:100%;&#34;&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Bytebase is built with a curated tech stack. It is optimized for &lt;strong&gt;developer experience&lt;/strong&gt; and is very easy to start working on the code:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;It has no external dependency.&lt;/li&gt; &#xA;   &lt;li&gt;It requires zero config.&lt;/li&gt; &#xA;   &lt;li&gt;1 command to start backend and 1 command to start frontend, both with live reload support.&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Interactive code walkthrough&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://sourcegraph.com/github.com/bytebase/bytebase/-/blob/docs/design/life-of-a-schema-change.snb.md&#34;&gt;Life of a schema change&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://sourcegraph.com/github.com/bytebase/bytebase/-/blob/docs/design/sql-review-source-code-tour.snb.md&#34;&gt;SQL Review&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Follow &lt;a href=&#34;https://github.com/bytebase/bytebase/raw/main/docs/life-of-a-feature.md&#34;&gt;Life of a Feature&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dev Environment Setup&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://golang.org/doc/install&#34;&gt;Go&lt;/a&gt; (1.21.5 or later)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pnpm.io/installation&#34;&gt;pnpm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/bytebase/air&#34;&gt;Air&lt;/a&gt; (&lt;strong&gt;our forked repo @87187cc with the proper signal handling&lt;/strong&gt;). This is for backend live reload. &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;go install github.com/bytebase/air@87187cc&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Steps&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Pull source.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/bytebase/bytebase&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Create an external Postgres database on localhost.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;CREATE USER bbdev SUPERUSER;&#xA;CREATE DATABASE bbdev;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start backend using air (with live reload).&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;PG_URL=postgresql://bbdev@localhost/bbdev air -c scripts/.air.toml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Change the open file limit if you encounter &#34;error: too many open files&#34;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ulimit -n 10240&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you need additional runtime parameters such as --backup-bucket, please add them like this:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;air -c scripts/.air.toml -- --backup-region us-east-1 --backup-bucket s3:\\/\\/example-bucket --backup-credential ~/.aws/credentials&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Start frontend (with live reload).&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd frontend &amp;amp;&amp;amp; pnpm i &amp;amp;&amp;amp; pnpm dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Bytebase should now be running at &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; and change either frontend or backend code would trigger live reload.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Tips&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://en.inspector.fe-dev.cn/guide/start.html#method1-recommend&#34;&gt;Code Inspector&lt;/a&gt; to locate frontend code from UI. Hold &lt;code&gt;Option + Shift&lt;/code&gt; on Mac or &lt;code&gt;Alt + Shift&lt;/code&gt; on Windows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Bytebase vs Flyway, Liquibase&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/blog/bytebase-vs-liquibase/&#34;&gt;Bytebase vs Liquibase&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bytebase.com/blog/bytebase-vs-flyway/&#34;&gt;Bytebase vs Flyway&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Either Flyway or Liquibase is a library and CLI focusing on schema change. While Bytebase is an one-stop solution covering the entire database development lifecycle for Developers and DBAs to collaborate.&lt;/p&gt; &#xA;&lt;p&gt;Another key difference is Bytebase &lt;strong&gt;doesn&#39;t&lt;/strong&gt; support Oracle and SQL Server. This is a conscious decision we make so that we can focus on supporting other databases without good tooling support. In particular, many of our users tell us Bytebase is by far the best (and sometimes the only) database tool that can support their PostgreSQL and ClickHouse use cases.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#bytebase/bytebase&amp;amp;liquibase/liquibase&amp;amp;flyway/flyway&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=bytebase/bytebase,liquibase/liquibase,flyway/flyway&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Bytebase vs Yearning, Archery&lt;/h1&gt; &#xA;&lt;p&gt;Either Yearning or Archery provides a DBA operation portal. While Bytebase provides a collaboration workspace for DBAs and Developers, and brings DevOps practice to the Database Change Management (DCM). Bytebase has the similar &lt;code&gt;Project&lt;/code&gt; concept seen in GitLab/GitHub and provides native GitOps integration with GitLab/GitHub.&lt;/p&gt; &#xA;&lt;p&gt;Another key difference is Yearning, Archery are open source projects maintained by the individuals part-time. While Bytebase is open-sourced, it adopts an open-core model and is a commercialized product, supported by a &lt;a href=&#34;https://www.bytebase.com/about#team&#34;&gt;fully staffed team&lt;/a&gt; &lt;a href=&#34;https://www.bytebase.com/changelog&#34;&gt;releasing new version every 2 weeks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#bytebase/bytebase&amp;amp;cookieY/Yearning&amp;amp;hhyo/Archery&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=bytebase/bytebase,cookieY/Yearning,hhyo/Archery&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;👨‍👩‍👧‍👦 Community&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/huyw7gRsyA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%20-Hang%20out%20on%20Discord-5865F2?style=for-the-badge&amp;amp;logo=discord&amp;amp;labelColor=EEEEEE&#34; alt=&#34;Hang out on Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/Bytebase&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Follow%20us%20on%20Twitter-1DA1F2?style=for-the-badge&amp;amp;logo=twitter&amp;amp;labelColor=EEEEEE&#34; alt=&#34;Follow us on Twitter&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🤔 Frequently Asked Questions (FAQs)&lt;/h1&gt; &#xA;&lt;p&gt;Check out our &lt;a href=&#34;https://www.bytebase.com/docs/faq&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;🙋 Contact Us&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Interested in joining us? Check out our &lt;a href=&#34;https://bytebase.com/jobs?source=github&#34;&gt;jobs page&lt;/a&gt; for openings.&lt;/li&gt; &#xA; &lt;li&gt;Want to solve your schema change and database management headache? Book a &lt;a href=&#34;https://cal.com/adela-bytebase/30min&#34;&gt;30min demo&lt;/a&gt; with one of our product experts.&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>