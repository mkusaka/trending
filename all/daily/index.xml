<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-02-20T01:21:45Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>karpathy/minbpe</title>
    <updated>2024-02-20T01:21:45Z</updated>
    <id>tag:github.com,2024-02-20:/karpathy/minbpe</id>
    <link href="https://github.com/karpathy/minbpe" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Minimal, clean, code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;minbpe&lt;/h1&gt; &#xA;&lt;p&gt;Minimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization. The BPE algorithm is &#34;byte-level&#34; because it runs on UTF-8 encoded strings.&lt;/p&gt; &#xA;&lt;p&gt;This algorithm was popularized for LLMs by the &lt;a href=&#34;https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;GPT-2 paper&lt;/a&gt; and the associated GPT-2 &lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;code release&lt;/a&gt; from OpenAI. &lt;a href=&#34;https://arxiv.org/abs/1508.07909&#34;&gt;Sennrich et al. 2015&lt;/a&gt; is cited as the original reference for the use of BPE in NLP applications. Today, all modern LLMs (e.g. GPT, Llama, Mistral) use this algorithm to train their tokenizers.&lt;/p&gt; &#xA;&lt;p&gt;There are two Tokenizers in this repository, both of which can perform the 3 primary functions of a Tokenizer: 1) train the tokenizer vocabulary and merges on a given text, 2) encode from text to tokens, 3) decode from tokens to text. The files of the repo are as follows:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/minbpe/base.py&#34;&gt;minbpe/base.py&lt;/a&gt;: Implements the &lt;code&gt;Tokenizer&lt;/code&gt; class, which is the base class. It contains the &lt;code&gt;train&lt;/code&gt;, &lt;code&gt;encode&lt;/code&gt;, and &lt;code&gt;decode&lt;/code&gt; stubs, save/load functionality, and there are also a few common utility functions. This class is not meant to be used directly, but rather to be inherited from.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/minbpe/basic.py&#34;&gt;minbpe/basic.py&lt;/a&gt;: Implements the &lt;code&gt;BasicTokenizer&lt;/code&gt;, the simplest implementation of the BPE algorithm that runs directly on text.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/minbpe/regex.py&#34;&gt;minbpe/regex.py&lt;/a&gt;: Implements the &lt;code&gt;RegexTokenizer&lt;/code&gt; that further splits the input text by a regex pattern, which is a preprocessing stage that splits up the input text by categories (think: letters, numbers, punctuation) before tokenization. This ensures that no merges will happen across category boundaries. This was introduced in the GPT-2 paper and continues to be in use as of GPT-4. This class also handles special tokens, if any.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/minbpe/gpt4.py&#34;&gt;minbpe/gpt4.py&lt;/a&gt;: Implements the &lt;code&gt;GPT4Tokenizer&lt;/code&gt;. This class is a light wrapper around the &lt;code&gt;RegexTokenizer&lt;/code&gt; (2, above) that exactly reproduces the tokenization of GPT-4 in the &lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;tiktoken&lt;/a&gt; library. The wrapping handles some details around recovering the exact merges in the tokenizer, and the handling of some unfortunate (and likely historical?) 1-byte token permutations.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Finally, the script &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/train.py&#34;&gt;train.py&lt;/a&gt; trains the two major tokenizers on the input text &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/tests/taylorswift.txt&#34;&gt;tests/taylorswift.txt&lt;/a&gt; (this is the Wikipedia entry for her kek) and saves the vocab to disk for visualization. This script runs in about 25 seconds on my (M1) MacBook.&lt;/p&gt; &#xA;&lt;p&gt;All of the files above are very short and thoroughly commented, and also contain a usage example on the bottom of the file.&lt;/p&gt; &#xA;&lt;h2&gt;quick start&lt;/h2&gt; &#xA;&lt;p&gt;As the simplest example, we can reproduce the &lt;a href=&#34;https://en.wikipedia.org/wiki/Byte_pair_encoding&#34;&gt;Wikipedia article on BPE&lt;/a&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from minbpe import BasicTokenizer&#xA;tokenizer = BasicTokenizer()&#xA;text = &#34;aaabdaaabac&#34;&#xA;tokenizer.train(text, 256 + 3) # 256 are the byte tokens, then do 3 merges&#xA;print(tokenizer.encode(text))&#xA;# [258, 100, 258, 97, 99]&#xA;print(tokenizer.decode([258, 100, 258, 97, 99]))&#xA;# aaabdaaabac&#xA;tokenizer.save(&#34;toy&#34;)&#xA;# writes two files: toy.model (for loading) and toy.vocab (for viewing)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;According to Wikipedia, running bpe on the input string: &#34;aaabdaaabac&#34; for 3 merges results in the string: &#34;XdXac&#34; where X=ZY, Y=ab, and Z=aa. The tricky thing to note is that minbpe always allocates the 256 individual bytes as tokens, and then merges bytes as needed from there. So for us a=97, b=98, c=99, d=100 (their &lt;a href=&#34;https://www.asciitable.com&#34;&gt;ASCII&lt;/a&gt; values). Then when (a,a) is merged to Z, Z will become 256. Likewise Y will become 257 and X 258. So we start with the 256 bytes, and do 3 merges to get to the result above, with the expected output of [258, 100, 258, 97, 99].&lt;/p&gt; &#xA;&lt;h2&gt;inference: GPT-4 comparison&lt;/h2&gt; &#xA;&lt;p&gt;We can verify that the &lt;code&gt;RegexTokenizer&lt;/code&gt; has feature parity with the GPT-4 tokenizer from &lt;a href=&#34;https://github.com/openai/tiktoken&#34;&gt;tiktoken&lt;/a&gt; as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &#34;hello123!!!? (안녕하세요!) 😉&#34;&#xA;&#xA;# tiktoken&#xA;import tiktoken&#xA;enc = tiktoken.get_encoding(&#34;cl100k_base&#34;)&#xA;print(enc.encode(text))&#xA;# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]&#xA;&#xA;# ours&#xA;from minbpe import GPT4Tokenizer&#xA;tokenizer = GPT4Tokenizer()&#xA;print(tokenizer.encode(text))&#xA;# [15339, 4513, 12340, 30, 320, 31495, 230, 75265, 243, 92245, 16715, 57037]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(you&#39;ll have to &lt;code&gt;pip install tiktoken&lt;/code&gt; to run). Under the hood, the &lt;code&gt;GPT4Tokenizer&lt;/code&gt; is just a light wrapper around &lt;code&gt;RegexTokenizer&lt;/code&gt;, passing in the merges and the special tokens of GPT-4. We can also ensure the special tokens are handled correctly:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;text = &#34;&amp;lt;|endoftext|&amp;gt;hello world&#34;&#xA;&#xA;# tiktoken&#xA;import tiktoken&#xA;enc = tiktoken.get_encoding(&#34;cl100k_base&#34;)&#xA;print(enc.encode(text, allowed_special=&#34;all&#34;))&#xA;# [100257, 15339, 1917]&#xA;&#xA;# ours&#xA;from minbpe import GPT4Tokenizer&#xA;tokenizer = GPT4Tokenizer()&#xA;print(tokenizer.encode(text, allowed_special=&#34;all&#34;))&#xA;# [100257, 15339, 1917]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note that just like tiktoken, we have to explicitly declare our intent to use and parse special tokens in the call to encode. Otherwise this can become a major footgun, unintentionally tokenizing attacker-controlled data (e.g. user prompts) with special tokens. The &lt;code&gt;allowed_special&lt;/code&gt; parameter can be set to &#34;all&#34;, &#34;none&#34;, or a list of special tokens to allow.&lt;/p&gt; &#xA;&lt;h2&gt;training&lt;/h2&gt; &#xA;&lt;p&gt;Unlike tiktoken, this code allows you to train your own tokenizer. In principle and to my knowledge, if you train the &lt;code&gt;RegexTokenizer&lt;/code&gt; on a large dataset with a vocabulary size of 100K, you would reproduce the GPT-4 tokenizer.&lt;/p&gt; &#xA;&lt;p&gt;There are two paths you can follow. First, you can decide that you don&#39;t want the complexity of splitting and preprocessing text with regex patterns, and you also don&#39;t care for special tokens. In that case, reach for the &lt;code&gt;BasicTokenizer&lt;/code&gt;. You can train it, and then encode and decode for example as follows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from minbpe import BasicTokenizer&#xA;tokenizer = BasicTokenizer()&#xA;tokenizer.train(very_long_training_string, vocab_size=4096)&#xA;tokenizer.encode(&#34;hello world&#34;) # string -&amp;gt; tokens&#xA;tokenizer.decode([1000, 2000, 3000]) # tokens -&amp;gt; string&#xA;tokenizer.save(&#34;mymodel&#34;) # writes mymodel.model and mymodel.vocab&#xA;tokenizer.load(&#34;mymodel.model&#34;) # loads the model back, the vocab is just for vis&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you instead want to follow along with OpenAI did for their text tokenizer, it&#39;s a good idea to adopt their approach of using regex pattern to split the text by categories. The GPT-4 pattern is a default with the &lt;code&gt;RegexTokenizer&lt;/code&gt;, so you&#39;d simple do something like:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from minbpe import RegexTokenizer&#xA;tokenizer = RegexTokenizer()&#xA;tokenizer.train(very_long_training_string, vocab_size=32768)&#xA;tokenizer.encode(&#34;hello world&#34;) # string -&amp;gt; tokens&#xA;tokenizer.decode([1000, 2000, 3000]) # tokens -&amp;gt; string&#xA;tokenizer.save(&#34;tok32k&#34;) # writes tok32k.model and tok32k.vocab&#xA;tokenizer.load(&#34;tok32k.model&#34;) # loads the model back from disk&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Where, of course, you&#39;d want to change around the vocabulary size depending on the size of your dataset.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Special tokens&lt;/strong&gt;. Finally, you might wish to add special tokens to your tokenizer. Register these using the &lt;code&gt;register_special_tokens&lt;/code&gt; function. For example if you train with vocab_size of 32768, then the first 256 tokens are raw byte tokens, the next 32768-256 are merge tokens, and after those you can add the special tokens. The last &#34;real&#34; merge token will have id of 32767 (vocab_size - 1), so your first special token should come right after that, with an id of exactly 32768. So:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from minbpe import RegexTokenizer&#xA;tokenizer = RegexTokenizer()&#xA;tokenizer.train(very_long_training_string, vocab_size=32768)&#xA;tokenizer.register_special_tokens({&#34;&amp;lt;|endoftext|&amp;gt;&#34;: 32768})&#xA;tokenizer.encode(&#34;&amp;lt;|endoftext|&amp;gt;hello world&#34;, allowed_special=&#34;all&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can of course add more tokens after that as well, as you like. Finally, I&#39;d like to stress that I tried hard to keep the code itself clean, readable and hackable. You should not have feel scared to read the code and understand how it works. The tests are also a nice place to look for more usage examples. That reminds me:&lt;/p&gt; &#xA;&lt;h2&gt;tests&lt;/h2&gt; &#xA;&lt;p&gt;We use the pytest library for tests. All of them are located in the &lt;code&gt;tests/&lt;/code&gt; directory. First &lt;code&gt;pip install pytest&lt;/code&gt; if you haven&#39;t already, then:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pytest -v .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;to run the tests. (-v is verbose, slightly prettier).&lt;/p&gt; &#xA;&lt;h2&gt;exercise&lt;/h2&gt; &#xA;&lt;p&gt;For those trying to study BPE, here is the advised progression exercise for how you can build your own minbpe step by step. See &lt;a href=&#34;https://raw.githubusercontent.com/karpathy/minbpe/master/exercise.md&#34;&gt;exercise.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;todos&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;write a more optimized Python version that could run over large files and big vocabs&lt;/li&gt; &#xA; &lt;li&gt;write an even more optimized C or Rust version (think through)&lt;/li&gt; &#xA; &lt;li&gt;rename GPT4Tokenizer to GPTTokenizer and support GPT-2/GPT-3/GPT-3.5 as well?&lt;/li&gt; &#xA; &lt;li&gt;write a LlamaTokenizer similar to GPT4Tokenizer (i.e. attempt sentencepiece equivalent)&lt;/li&gt; &#xA; &lt;li&gt;video coming soon ;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;MIT&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>binary-husky/gpt_academic</title>
    <updated>2024-02-20T01:21:45Z</updated>
    <id>tag:github.com,2024-02-20:/binary-husky/gpt_academic</id>
    <link href="https://github.com/binary-husky/gpt_academic" rel="alternate"></link>
    <summary type="html">&lt;p&gt;为GPT/GLM等LLM大语言模型提供实用化交互接口，特别优化论文阅读/润色/写作体验，模块化设计，支持自定义快捷按钮&amp;函数插件，支持Python和C++等项目剖析&amp;自译解功能，PDF/LaTex论文翻译&amp;总结功能，支持并行问询多种LLM模型，支持chatglm3等本地模型。接入通义千问, deepseekcoder, 讯飞星火, 文心一言, llama2, rwkv, claude2, moss等。&lt;/p&gt;&lt;hr&gt;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;br&gt; 2024.1.18: 更新3.70版本，支持Mermaid绘图库（让大模型绘制脑图）&lt;br&gt; 2024.1.17: 恭迎GLM4，全力支持Qwen、GLM、DeepseekCoder等国内中文大语言基座模型！&lt;br&gt; 2024.1.17: 某些依赖包尚不兼容python 3.12，推荐python 3.11。&lt;br&gt; 2024.1.17: 安装依赖时，请选择&lt;code&gt;requirements.txt&lt;/code&gt;中&lt;strong&gt;指定的版本&lt;/strong&gt;。 安装命令：&lt;code&gt;pip install -r requirements.txt&lt;/code&gt;。本项目完全开源免费，您可通过订阅&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/online&#34;&gt;在线服务&lt;/a&gt;的方式鼓励本项目的发展。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1 aligh=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/logo.png&#34; width=&#34;40&#34;&gt; GPT 学术优化 (GPT Academic) &lt;/h1&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/binary-husky/gpt_academic&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/github-12100E.svg?style=flat-square&#34; alt=&#34;Github&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/binary-husky/gpt_academic?label=License&amp;amp;style=flat-square&amp;amp;color=orange&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/release/binary-husky/gpt_academic?label=Release&amp;amp;style=flat-square&amp;amp;color=blue&#34; alt=&#34;Releases&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binary-husky/gpt_academic#installation&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/dynamic/json?color=blue&amp;amp;url=https://raw.githubusercontent.com/binary-husky/gpt_academic/master/version&amp;amp;query=$.version&amp;amp;label=Installation&amp;amp;style=flat-square&#34; alt=&#34;Installation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/wiki-%E9%A1%B9%E7%9B%AE%E6%96%87%E6%A1%A3-black?style=flat-square&#34; alt=&#34;Wiki&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/PRs-welcome-pink?style=flat-square&#34; alt=&#34;PR&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;如果喜欢这个项目，请给它一个Star；如果您发明了好用的快捷键或插件，欢迎发pull requests！&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you like this project, please give it a Star. Read this in &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/README.English.md&#34;&gt;English&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/README.Japanese.md&#34;&gt;日本語&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/README.Korean.md&#34;&gt;한국어&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/README.Russian.md&#34;&gt;Русский&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/README.French.md&#34;&gt;Français&lt;/a&gt;. All translations have been provided by the project itself. To translate this project to arbitrary language with GPT, read and run &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/multi_language.py&#34;&gt;&lt;code&gt;multi_language.py&lt;/code&gt;&lt;/a&gt; (experimental). &lt;br&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!NOTE] 1.本项目中每个文件的功能都在&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/GPT%E2%80%90Academic%E9%A1%B9%E7%9B%AE%E8%87%AA%E8%AF%91%E8%A7%A3%E6%8A%A5%E5%91%8A&#34;&gt;自译解报告&lt;/a&gt;&lt;code&gt;self_analysis.md&lt;/code&gt;详细说明。随着版本的迭代，您也可以随时自行点击相关函数插件，调用GPT重新生成项目的自我解析报告。常见问题请查阅wiki。 &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/#installation&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=&amp;amp;message=%E5%B8%B8%E8%A7%84%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95&amp;amp;color=gray&#34; alt=&#34;常规安装方法&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=&amp;amp;message=%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E8%84%9A%E6%9C%AC&amp;amp;color=gray&#34; alt=&#34;一键安装脚本&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=&amp;amp;message=%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E&amp;amp;color=gray&#34; alt=&#34;配置说明&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/%5Bhttps://github.com/binary-husky/gpt_academic/wiki/%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E%5D(https://github.com/binary-husky/gpt_academic/wiki)&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=&amp;amp;message=wiki&amp;amp;color=gray&#34; alt=&#34;wiki&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA; &lt;p&gt;2.本项目兼容并鼓励尝试国内中文大语言基座模型如通义千问，智谱GLM等。支持多个api-key共存，可在配置文件中填写如&lt;code&gt;API_KEY=&#34;openai-key1,openai-key2,azure-key3,api2d-key4&#34;&lt;/code&gt;。需要临时更换&lt;code&gt;API_KEY&lt;/code&gt;时，在输入区输入临时的&lt;code&gt;API_KEY&lt;/code&gt;然后回车键提交即可生效。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;功能（⭐= 近期新增功能）&lt;/th&gt; &#xA;    &lt;th&gt;描述&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;⭐&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E5%A6%82%E4%BD%95%E5%88%87%E6%8D%A2%E6%A8%A1%E5%9E%8B&#34;&gt;接入新模型&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;百度&lt;a href=&#34;https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Nlks5zkzu&#34;&gt;千帆&lt;/a&gt;与文心一言, 通义千问&lt;a href=&#34;https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary&#34;&gt;Qwen&lt;/a&gt;，上海AI-Lab&lt;a href=&#34;https://github.com/InternLM/InternLM&#34;&gt;书生&lt;/a&gt;，讯飞&lt;a href=&#34;https://xinghuo.xfyun.cn/&#34;&gt;星火&lt;/a&gt;，&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-7b-chat-hf&#34;&gt;LLaMa2&lt;/a&gt;，&lt;a href=&#34;https://open.bigmodel.cn/&#34;&gt;智谱GLM4&lt;/a&gt;，DALLE3, &lt;a href=&#34;https://coder.deepseek.com/&#34;&gt;DeepseekCoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;⭐支持mermaid图像渲染&lt;/td&gt; &#xA;    &lt;td&gt;支持让GPT生成&lt;a href=&#34;https://www.bilibili.com/video/BV18c41147H9/&#34;&gt;流程图&lt;/a&gt;、状态转移图、甘特图、饼状图、GitGraph等等（3.7版本）&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;⭐Arxiv论文精细翻译 (&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/pkgs/container/gpt_academic_with_latex&#34;&gt;Docker&lt;/a&gt;)&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 一键&lt;a href=&#34;https://www.bilibili.com/video/BV1dz4y1v77A/&#34;&gt;以超高质量翻译arxiv论文&lt;/a&gt;，目前最好的论文翻译工具&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;⭐&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/raw/master/docs/use_audio.md&#34;&gt;实时语音对话输入&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 异步&lt;a href=&#34;https://www.bilibili.com/video/BV1AV4y187Uy/&#34;&gt;监听音频&lt;/a&gt;，自动断句，自动寻找回答时机&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;⭐AutoGen多智能体插件&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 借助微软AutoGen，探索多Agent的智能涌现可能！&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;⭐虚空终端插件&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 能够使用自然语言直接调度本项目其他插件&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;润色、翻译、代码解释&lt;/td&gt; &#xA;    &lt;td&gt;一键润色、翻译、查找论文语法错误、解释代码&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV14s4y1E7jN&#34;&gt;自定义快捷键&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;支持自定义快捷键&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;模块化设计&lt;/td&gt; &#xA;    &lt;td&gt;支持自定义强大的&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/tree/master/crazy_functions&#34;&gt;插件&lt;/a&gt;，插件支持&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E5%87%BD%E6%95%B0%E6%8F%92%E4%BB%B6%E6%8C%87%E5%8D%97&#34;&gt;热更新&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1cj411A7VW&#34;&gt;程序剖析&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 一键剖析Python/C/C++/Java/Lua/...项目树 或 &lt;a href=&#34;https://www.bilibili.com/video/BV1cj411A7VW&#34;&gt;自我剖析&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;读论文、&lt;a href=&#34;https://www.bilibili.com/video/BV1KT411x7Wn&#34;&gt;翻译&lt;/a&gt;论文&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 一键解读latex/pdf论文全文并生成摘要&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Latex全文&lt;a href=&#34;https://www.bilibili.com/video/BV1nk4y1Y7Js/&#34;&gt;翻译&lt;/a&gt;、&lt;a href=&#34;https://www.bilibili.com/video/BV1FT411H7c5/&#34;&gt;润色&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 一键翻译或润色latex论文&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;批量注释生成&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 一键批量生成函数注释&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Markdown&lt;a href=&#34;https://www.bilibili.com/video/BV1yo4y157jV/&#34;&gt;中英互译&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 看到上面5种语言的&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/raw/master/docs/README_EN.md&#34;&gt;README&lt;/a&gt;了吗？就是出自他的手笔&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1KT411x7Wn&#34;&gt;PDF论文全文翻译功能&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[插件] PDF论文提取题目&amp;amp;摘要+翻译全文（多线程）&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1LM4y1279X&#34;&gt;Arxiv小助手&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 输入arxiv文章url即可一键翻译摘要+下载PDF&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Latex论文一键校对&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 仿Grammarly对Latex文章进行语法、拼写纠错+输出对照PDF&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV19L411U7ia&#34;&gt;谷歌学术统合小助手&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 给定任意谷歌学术搜索页面URL，让gpt帮你&lt;a href=&#34;https://www.bilibili.com/video/BV1GP411U7Az/&#34;&gt;写relatedworks&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;互联网信息聚合+GPT&lt;/td&gt; &#xA;    &lt;td&gt;[插件] 一键&lt;a href=&#34;https://www.bilibili.com/video/BV1om4y127ck&#34;&gt;让GPT从互联网获取信息&lt;/a&gt;回答问题，让信息永不过时&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;公式/图片/表格显示&lt;/td&gt; &#xA;    &lt;td&gt;可以同时显示公式的&lt;a href=&#34;https://user-images.githubusercontent.com/96192199/230598842-1d7fcddd-815d-40ee-af60-baf488a199df.png&#34;&gt;tex形式和渲染形式&lt;/a&gt;，支持公式、代码高亮&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;启动暗色&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/issues/173&#34;&gt;主题&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;在浏览器url后面添加&lt;code&gt;/?__theme=dark&lt;/code&gt;可以切换dark主题&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1wT411p7yf&#34;&gt;多LLM模型&lt;/a&gt;支持&lt;/td&gt; &#xA;    &lt;td&gt;同时被GPT3.5、GPT4、&lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34;&gt;清华ChatGLM2&lt;/a&gt;、&lt;a href=&#34;https://github.com/OpenLMLab/MOSS&#34;&gt;复旦MOSS&lt;/a&gt;伺候的感觉一定会很不错吧？&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;更多LLM模型接入，支持&lt;a href=&#34;https://huggingface.co/spaces/qingxu98/gpt-academic&#34;&gt;huggingface部署&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;加入Newbing接口(新必应)，引入清华&lt;a href=&#34;https://github.com/Jittor/JittorLLMs&#34;&gt;Jittorllms&lt;/a&gt;支持&lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;LLaMA&lt;/a&gt;和&lt;a href=&#34;https://openi.org.cn/pangu/&#34;&gt;盘古α&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;⭐&lt;a href=&#34;https://github.com/binary-husky/void-terminal&#34;&gt;void-terminal&lt;/a&gt; pip包&lt;/td&gt; &#xA;    &lt;td&gt;脱离GUI，在Python中直接调用本项目的所有函数插件（开发中）&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;更多新功能展示 (图像生成等) ……&lt;/td&gt; &#xA;    &lt;td&gt;见本文档结尾处 ……&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;新界面（修改&lt;code&gt;config.py&lt;/code&gt;中的LAYOUT选项即可实现“左右布局”和“上下布局”的切换）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/279702205-d81137c3-affd-4cd1-bb5e-b15610389762.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;所有按钮都通过读取functional.py动态生成，可随意加自定义功能，解放剪贴板&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/231975334-b4788e91-4887-412f-8b43-2b9c5f41d248.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;润色/纠错&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/231980294-f374bdcb-3309-4560-b424-38ef39f04ebd.gif&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;如果输出包含公式，会以tex形式和渲染形式同时显示，方便复制和阅读&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/230598842-1d7fcddd-815d-40ee-af60-baf488a199df.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;懒得看项目代码？直接把整个工程炫ChatGPT嘴里&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226935232-6b6a73ce-8900-4aee-93f9-733c7e6fef53.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;多种大语言模型混合调用（ChatGLM + OpenAI-GPT3.5 + GPT4）&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/232537274-deca0563-7aa6-4b5d-94a2-b7c453c47794.png&#34; width=&#34;700&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Installation&lt;/h1&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;flowchart TD&#xA;    A{&#34;安装方法&#34;} --&amp;gt; W1(&#34;I. 🔑直接运行 (Windows, Linux or MacOS)&#34;)&#xA;    W1 --&amp;gt; W11[&#34;1. Python pip包管理依赖&#34;]&#xA;    W1 --&amp;gt; W12[&#34;2. Anaconda包管理依赖（推荐⭐）&#34;]&#xA;&#xA;    A --&amp;gt; W2[&#34;II. 🐳使用Docker (Windows, Linux or MacOS)&#34;]&#xA;&#xA;    W2 --&amp;gt; k1[&#34;1. 部署项目全部能力的大镜像（推荐⭐）&#34;]&#xA;    W2 --&amp;gt; k2[&#34;2. 仅在线模型（GPT, GLM4等）镜像&#34;]&#xA;    W2 --&amp;gt; k3[&#34;3. 在线模型 + Latex的大镜像&#34;]&#xA;&#xA;    A --&amp;gt; W4[&#34;IV. 🚀其他部署方法&#34;]&#xA;    W4 --&amp;gt; C1[&#34;1. Windows/MacOS 一键安装运行脚本（推荐⭐）&#34;]&#xA;    W4 --&amp;gt; C2[&#34;2. Huggingface, Sealos远程部署&#34;]&#xA;    W4 --&amp;gt; C4[&#34;3. ... 其他 ...&#34;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;安装方法I：直接运行 (Windows, Linux or MacOS)&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;下载项目&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone --depth=1 https://github.com/binary-husky/gpt_academic.git&#xA;cd gpt_academic&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;配置API_KEY等变量&lt;/p&gt; &lt;p&gt;在&lt;code&gt;config.py&lt;/code&gt;中，配置API KEY等变量。&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/issues/1&#34;&gt;特殊网络环境设置方法&lt;/a&gt;、&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E&#34;&gt;Wiki-项目配置说明&lt;/a&gt;。&lt;/p&gt; &lt;p&gt;「 程序会优先检查是否存在名为&lt;code&gt;config_private.py&lt;/code&gt;的私密配置文件，并用其中的配置覆盖&lt;code&gt;config.py&lt;/code&gt;的同名配置。如您能理解以上读取逻辑，我们强烈建议您在&lt;code&gt;config.py&lt;/code&gt;同路径下创建一个名为&lt;code&gt;config_private.py&lt;/code&gt;的新配置文件，并使用&lt;code&gt;config_private.py&lt;/code&gt;配置项目，从而确保自动更新时不会丢失配置 」。&lt;/p&gt; &lt;p&gt;「 支持通过&lt;code&gt;环境变量&lt;/code&gt;配置项目，环境变量的书写格式参考&lt;code&gt;docker-compose.yml&lt;/code&gt;文件或者我们的&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E&#34;&gt;Wiki页面&lt;/a&gt;。配置读取优先级: &lt;code&gt;环境变量&lt;/code&gt; &amp;gt; &lt;code&gt;config_private.py&lt;/code&gt; &amp;gt; &lt;code&gt;config.py&lt;/code&gt; 」。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;安装依赖&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# （选择I: 如熟悉python, python推荐版本 3.9 ~ 3.11）备注：使用官方pip源或者阿里pip源, 临时换源方法：python -m pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/&#xA;python -m pip install -r requirements.txt&#xA;&#xA;# （选择II: 使用Anaconda）步骤也是类似的 (https://www.bilibili.com/video/BV1rc411W7Dr)：&#xA;conda create -n gptac_venv python=3.11    # 创建anaconda环境&#xA;conda activate gptac_venv                 # 激活anaconda环境&#xA;python -m pip install -r requirements.txt # 这个步骤和pip安装一样的步骤&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;details&gt;&#xA; &lt;summary&gt;如果需要支持清华ChatGLM2/复旦MOSS/RWKV作为后端，请点击展开此处&lt;/summary&gt; &#xA; &lt;p&gt; &lt;/p&gt;&#xA; &lt;p&gt;【可选步骤】如果需要支持清华ChatGLM3/复旦MOSS作为后端，需要额外安装更多依赖（前提条件：熟悉Python + 用过Pytorch + 电脑配置够强）：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 【可选步骤I】支持清华ChatGLM3。清华ChatGLM备注：如果遇到&#34;Call ChatGLM fail 不能正常加载ChatGLM的参数&#34; 错误，参考如下： 1：以上默认安装的为torch+cpu版，使用cuda需要卸载torch重新安装torch+cuda； 2：如因本机配置不够无法加载模型，可以修改request_llm/bridge_chatglm.py中的模型精度, 将 AutoTokenizer.from_pretrained(&#34;THUDM/chatglm-6b&#34;, trust_remote_code=True) 都修改为 AutoTokenizer.from_pretrained(&#34;THUDM/chatglm-6b-int4&#34;, trust_remote_code=True)&#xA;python -m pip install -r request_llms/requirements_chatglm.txt&#xA;&#xA;# 【可选步骤II】支持复旦MOSS&#xA;python -m pip install -r request_llms/requirements_moss.txt&#xA;git clone --depth=1 https://github.com/OpenLMLab/MOSS.git request_llms/moss  # 注意执行此行代码时，必须处于项目根路径&#xA;&#xA;# 【可选步骤III】支持RWKV Runner&#xA;参考wiki：https://github.com/binary-husky/gpt_academic/wiki/%E9%80%82%E9%85%8DRWKV-Runner&#xA;&#xA;# 【可选步骤IV】确保config.py配置文件的AVAIL_LLM_MODELS包含了期望的模型，目前支持的全部模型如下(jittorllms系列目前仅支持docker方案)：&#xA;AVAIL_LLM_MODELS = [&#34;gpt-3.5-turbo&#34;, &#34;api2d-gpt-3.5-turbo&#34;, &#34;gpt-4&#34;, &#34;api2d-gpt-4&#34;, &#34;chatglm&#34;, &#34;moss&#34;] # + [&#34;jittorllms_rwkv&#34;, &#34;jittorllms_pangualpha&#34;, &#34;jittorllms_llama&#34;]&#xA;&#xA;# 【可选步骤V】支持本地模型INT8,INT4量化（这里所指的模型本身不是量化版本，目前deepseek-coder支持，后面测试后会加入更多模型量化选择）&#xA;pip install bitsandbyte&#xA;# windows用户安装bitsandbytes需要使用下面bitsandbytes-windows-webui&#xA;python -m pip install bitsandbytes --prefer-binary --extra-index-url=https://jllllll.github.io/bitsandbytes-windows-webui&#xA;pip install -U git+https://github.com/huggingface/transformers.git&#xA;pip install -U git+https://github.com/huggingface/accelerate.git&#xA;pip install peft&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;运行 &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python main.py&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;安装方法II：使用Docker&lt;/h3&gt; &#xA;&lt;ol start=&#34;0&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;部署项目的全部能力（这个是包含cuda和latex的大型镜像。但如果您网速慢、硬盘小，则不推荐该方法部署完整项目） &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-all-capacity.yml&#34;&gt;&lt;img src=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-all-capacity.yml/badge.svg?branch=master&#34; alt=&#34;fullcapacity&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 修改docker-compose.yml，保留方案0并删除其他方案。然后运行：&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;仅ChatGPT + GLM4 + 文心一言+spark等在线模型（推荐大多数人选择） &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-without-local-llms.yml&#34;&gt;&lt;img src=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-without-local-llms.yml/badge.svg?branch=master&#34; alt=&#34;basic&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-latex.yml&#34;&gt;&lt;img src=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-latex.yml/badge.svg?branch=master&#34; alt=&#34;basiclatex&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-audio-assistant.yml&#34;&gt;&lt;img src=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-audio-assistant.yml/badge.svg?branch=master&#34; alt=&#34;basicaudio&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 修改docker-compose.yml，保留方案1并删除其他方案。然后运行：&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;P.S. 如果需要依赖Latex的插件功能，请见Wiki。另外，您也可以直接使用方案4或者方案0获取Latex功能。&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt; &lt;p&gt;ChatGPT + GLM3 + MOSS + LLAMA2 + 通义千问（需要熟悉&lt;a href=&#34;https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#installing-on-ubuntu-and-debian&#34;&gt;Nvidia Docker&lt;/a&gt;运行时） &lt;a href=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-chatglm.yml&#34;&gt;&lt;img src=&#34;https://github.com/binary-husky/gpt_academic/actions/workflows/build-with-chatglm.yml/badge.svg?branch=master&#34; alt=&#34;chatglm&#34;&gt;&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;# 修改docker-compose.yml，保留方案2并删除其他方案。然后运行：&#xA;docker-compose up&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;安装方法III：其他部署方法&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Windows一键运行脚本&lt;/strong&gt;。 完全不熟悉python环境的Windows用户可以下载&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/releases&#34;&gt;Release&lt;/a&gt;中发布的一键运行脚本安装无本地模型的版本。脚本贡献来源：&lt;a href=&#34;https://github.com/oobabooga/one-click-installers&#34;&gt;oobabooga&lt;/a&gt;。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;使用第三方API、Azure等、文心一言、星火等，见&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E9%A1%B9%E7%9B%AE%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E&#34;&gt;Wiki页面&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;云服务器远程部署避坑指南。 请访问&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9C%E7%A8%8B%E9%83%A8%E7%BD%B2%E6%8C%87%E5%8D%97&#34;&gt;云服务器远程部署wiki&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;在其他平台部署&amp;amp;二级网址部署&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;使用Sealos&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/issues/993&#34;&gt;一键部署&lt;/a&gt;。&lt;/li&gt; &#xA;   &lt;li&gt;使用WSL2（Windows Subsystem for Linux 子系统）。请访问&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E4%BD%BF%E7%94%A8WSL2%EF%BC%88Windows-Subsystem-for-Linux-%E5%AD%90%E7%B3%BB%E7%BB%9F%EF%BC%89%E9%83%A8%E7%BD%B2&#34;&gt;部署wiki-2&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;如何在二级网址（如&lt;code&gt;http://localhost/subpath&lt;/code&gt;）下运行。请访问&lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/docs/WithFastapi.md&#34;&gt;FastAPI运行说明&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Advanced Usage&lt;/h1&gt; &#xA;&lt;h3&gt;I：自定义新的便捷按钮（学术快捷键）&lt;/h3&gt; &#xA;&lt;p&gt;任意文本编辑器打开&lt;code&gt;core_functional.py&lt;/code&gt;，添加如下条目，然后重启程序。（如果按钮已存在，那么可以直接修改（前缀、后缀都已支持热修改），无需重启程序即可生效。） 例如&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&#34;超级英译中&#34;: {&#xA;    # 前缀，会被加在你的输入之前。例如，用来描述你的要求，例如翻译、解释代码、润色等等&#xA;    &#34;Prefix&#34;: &#34;请翻译把下面一段内容成中文，然后用一个markdown表格逐一解释文中出现的专有名词：\n\n&#34;,&#xA;&#xA;    # 后缀，会被加在你的输入之后。例如，配合前缀可以把你的输入内容用引号圈起来。&#xA;    &#34;Suffix&#34;: &#34;&#34;,&#xA;},&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226899272-477c2134-ed71-4326-810c-29891fe4a508.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;II：自定义函数插件&lt;/h3&gt; &#xA;&lt;p&gt;编写强大的函数插件来执行任何你想得到的和想不到的任务。 本项目的插件编写、调试难度很低，只要您具备一定的python基础知识，就可以仿照我们提供的模板实现自己的插件功能。 详情请参考&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/%E5%87%BD%E6%95%B0%E6%8F%92%E4%BB%B6%E6%8C%87%E5%8D%97&#34;&gt;函数插件指南&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Updates&lt;/h1&gt; &#xA;&lt;h3&gt;I：动态&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;对话保存功能。在函数插件区调用 &lt;code&gt;保存当前的对话&lt;/code&gt; 即可将当前对话保存为可读+可复原的html文件， 另外在函数插件区（下拉菜单）调用 &lt;code&gt;载入对话历史存档&lt;/code&gt; ，即可还原之前的会话。 Tip：不指定文件直接点击 &lt;code&gt;载入对话历史存档&lt;/code&gt; 可以查看历史html存档缓存。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/235222390-24a9acc0-680f-49f5-bc81-2f3161f1e049.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;⭐Latex/Arxiv论文翻译功能⭐&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/002a1a75-ace0-4e6a-94e2-ec1406a746f1&#34; height=&#34;250&#34;&gt; ===&amp;gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/9fdcc391-f823-464f-9322-f8719677043b&#34; height=&#34;250&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;虚空终端（从自然语言输入中，理解用户意图+自动调用其他插件）&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;步骤一：输入 “ 请调用插件翻译PDF论文，地址为&lt;a href=&#34;https://openreview.net/pdf?id=rJl0r3R9KX&#34;&gt;https://openreview.net/pdf?id=rJl0r3R9KX&lt;/a&gt; ”&lt;/li&gt; &#xA; &lt;li&gt;步骤二：点击“虚空终端”&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/66f1b044-e9ff-4eed-9126-5d4f3668f1ed&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;模块化功能设计，简单的接口却能支持强大的功能&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/229288270-093643c1-0018-487a-81e6-1d7809b6e90f.png&#34; height=&#34;400&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/227504931-19955f78-45cd-4d1c-adac-e71e50957915.png&#34; height=&#34;400&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;译解其他开源项目&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226935232-6b6a73ce-8900-4aee-93f9-733c7e6fef53.png&#34; height=&#34;250&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/226969067-968a27c1-1b9c-486b-8b81-ab2de8d3f88a.png&#34; height=&#34;250&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;装饰&lt;a href=&#34;https://github.com/fghrsh/live2d_demo&#34;&gt;live2d&lt;/a&gt;的小功能（默认关闭，需要修改&lt;code&gt;config.py&lt;/code&gt;）&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://user-images.githubusercontent.com/96192199/236432361-67739153-73e8-43fe-8111-b61296edabd9.png&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;OpenAI图像生成&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/bc7ab234-ad90-48a0-8d62-f703d9e74665&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;基于mermaid的流图、脑图绘制&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/c518b82f-bd53-46e2-baf5-ad1b081c1da4&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;9&#34;&gt; &#xA; &lt;li&gt;Latex全文校对纠错&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/651ccd98-02c9-4464-91e1-77a6b7d1b033&#34; height=&#34;200&#34;&gt; ===&amp;gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/476f66d9-7716-4537-b5c1-735372c25adb&#34; height=&#34;200&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;ol start=&#34;10&#34;&gt; &#xA; &lt;li&gt;语言、主题切换&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://github.com/binary-husky/gpt_academic/assets/96192199/b6799499-b6fb-4f0c-9c8e-1b441872f4e8&#34; width=&#34;500&#34;&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;II：版本:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;version 3.80(TODO): 优化AutoGen插件主题并设计一系列衍生插件&lt;/li&gt; &#xA; &lt;li&gt;version 3.70: 引入Mermaid绘图，实现GPT画脑图等功能&lt;/li&gt; &#xA; &lt;li&gt;version 3.60: 引入AutoGen作为新一代插件的基石&lt;/li&gt; &#xA; &lt;li&gt;version 3.57: 支持GLM3，星火v3，文心一言v4，修复本地模型的并发BUG&lt;/li&gt; &#xA; &lt;li&gt;version 3.56: 支持动态追加基础功能按钮，新汇报PDF汇总页面&lt;/li&gt; &#xA; &lt;li&gt;version 3.55: 重构前端界面，引入悬浮窗口与菜单栏&lt;/li&gt; &#xA; &lt;li&gt;version 3.54: 新增动态代码解释器（Code Interpreter）（待完善）&lt;/li&gt; &#xA; &lt;li&gt;version 3.53: 支持动态选择不同界面主题，提高稳定性&amp;amp;解决多用户冲突问题&lt;/li&gt; &#xA; &lt;li&gt;version 3.50: 使用自然语言调用本项目的所有函数插件（虚空终端），支持插件分类，改进UI，设计新主题&lt;/li&gt; &#xA; &lt;li&gt;version 3.49: 支持百度千帆平台和文心一言&lt;/li&gt; &#xA; &lt;li&gt;version 3.48: 支持阿里达摩院通义千问，上海AI-Lab书生，讯飞星火&lt;/li&gt; &#xA; &lt;li&gt;version 3.46: 支持完全脱手操作的实时语音对话&lt;/li&gt; &#xA; &lt;li&gt;version 3.45: 支持自定义ChatGLM2微调模型&lt;/li&gt; &#xA; &lt;li&gt;version 3.44: 正式支持Azure，优化界面易用性&lt;/li&gt; &#xA; &lt;li&gt;version 3.4: +arxiv论文翻译、latex论文批改功能&lt;/li&gt; &#xA; &lt;li&gt;version 3.3: +互联网信息综合功能&lt;/li&gt; &#xA; &lt;li&gt;version 3.2: 函数插件支持更多参数接口 (保存对话功能, 解读任意语言代码+同时询问任意的LLM组合)&lt;/li&gt; &#xA; &lt;li&gt;version 3.1: 支持同时问询多个gpt模型！支持api2d，支持多个apikey负载均衡&lt;/li&gt; &#xA; &lt;li&gt;version 3.0: 对chatglm和其他小型llm的支持&lt;/li&gt; &#xA; &lt;li&gt;version 2.6: 重构了插件结构，提高了交互性，加入更多插件&lt;/li&gt; &#xA; &lt;li&gt;version 2.5: 自更新，解决总结大工程源代码时文本过长、token溢出的问题&lt;/li&gt; &#xA; &lt;li&gt;version 2.4: 新增PDF全文翻译功能; 新增输入区切换位置的功能&lt;/li&gt; &#xA; &lt;li&gt;version 2.3: 增强多线程交互性&lt;/li&gt; &#xA; &lt;li&gt;version 2.2: 函数插件支持热重载&lt;/li&gt; &#xA; &lt;li&gt;version 2.1: 可折叠式布局&lt;/li&gt; &#xA; &lt;li&gt;version 2.0: 引入模块化函数插件&lt;/li&gt; &#xA; &lt;li&gt;version 1.0: 基础功能&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;GPT Academic开发者QQ群：&lt;code&gt;610599535&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;已知问题 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;某些浏览器翻译插件干扰此软件前端的运行&lt;/li&gt; &#xA;   &lt;li&gt;官方Gradio目前有很多兼容性问题，请&lt;strong&gt;务必使用&lt;code&gt;requirement.txt&lt;/code&gt;安装Gradio&lt;/strong&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;timeline LR&#xA;    title GPT-Academic项目发展历程&#xA;    section 2.x&#xA;        1.0~2.2: 基础功能: 引入模块化函数插件: 可折叠式布局: 函数插件支持热重载&#xA;        2.3~2.5: 增强多线程交互性: 新增PDF全文翻译功能: 新增输入区切换位置的功能: 自更新&#xA;        2.6: 重构了插件结构: 提高了交互性: 加入更多插件&#xA;    section 3.x&#xA;        3.0~3.1: 对chatglm支持: 对其他小型llm支持: 支持同时问询多个gpt模型: 支持多个apikey负载均衡&#xA;        3.2~3.3: 函数插件支持更多参数接口: 保存对话功能: 解读任意语言代码: 同时询问任意的LLM组合: 互联网信息综合功能&#xA;        3.4: 加入arxiv论文翻译: 加入latex论文批改功能&#xA;        3.44: 正式支持Azure: 优化界面易用性&#xA;        3.46: 自定义ChatGLM2微调模型: 实时语音对话&#xA;        3.49: 支持阿里达摩院通义千问: 上海AI-Lab书生: 讯飞星火: 支持百度千帆平台 &amp;amp; 文心一言&#xA;        3.50: 虚空终端: 支持插件分类: 改进UI: 设计新主题&#xA;        3.53: 动态选择不同界面主题: 提高稳定性: 解决多用户冲突问题&#xA;        3.55: 动态代码解释器: 重构前端界面: 引入悬浮窗口与菜单栏&#xA;        3.56: 动态追加基础功能按钮: 新汇报PDF汇总页面&#xA;        3.57: GLM3, 星火v3: 支持文心一言v4: 修复本地模型的并发BUG&#xA;        3.60: 引入AutoGen&#xA;        3.70: 引入Mermaid绘图: 实现GPT画脑图等功能&#xA;        3.80(TODO): 优化AutoGen插件主题: 设计衍生插件&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;III：主题&lt;/h3&gt; &#xA;&lt;p&gt;可以通过修改&lt;code&gt;THEME&lt;/code&gt;选项（config.py）变更主题&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;Chuanhu-Small-and-Beautiful&lt;/code&gt; &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/&#34;&gt;网址&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;IV：本项目的开发分支&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;master&lt;/code&gt; 分支: 主分支，稳定版&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;frontier&lt;/code&gt; 分支: 开发分支，测试版&lt;/li&gt; &#xA; &lt;li&gt;如何&lt;a href=&#34;https://raw.githubusercontent.com/binary-husky/gpt_academic/master/request_llms/README.md&#34;&gt;接入其他大模型&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;访问GPT-Academic的&lt;a href=&#34;https://github.com/binary-husky/gpt_academic/wiki/online&#34;&gt;在线服务并支持我们&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;V：参考与学习&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;代码中参考了很多其他优秀项目中的设计，顺序不分先后：&#xA;&#xA;# 清华ChatGLM2-6B:&#xA;https://github.com/THUDM/ChatGLM2-6B&#xA;&#xA;# 清华JittorLLMs:&#xA;https://github.com/Jittor/JittorLLMs&#xA;&#xA;# ChatPaper:&#xA;https://github.com/kaixindelele/ChatPaper&#xA;&#xA;# Edge-GPT:&#xA;https://github.com/acheong08/EdgeGPT&#xA;&#xA;# ChuanhuChatGPT:&#xA;https://github.com/GaiZhenbiao/ChuanhuChatGPT&#xA;&#xA;# Oobabooga one-click installer:&#xA;https://github.com/oobabooga/one-click-installers&#xA;&#xA;# More：&#xA;https://github.com/gradio-app/gradio&#xA;https://github.com/fghrsh/live2d_demo&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>oddfar/campus-imaotai</title>
    <updated>2024-02-20T01:21:45Z</updated>
    <id>tag:github.com,2024-02-20:/oddfar/campus-imaotai</id>
    <link href="https://github.com/oddfar/campus-imaotai" rel="alternate"></link>
    <summary type="html">&lt;p&gt;i茅台app自动预约，每日自动预约，支持docker一键部署&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt;&lt;a href=&#34;https://oddfar.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&lt;img width=&#34;180&#34; src=&#34;https://note.oddfar.com/img/web.png&#34; alt=&#34;logo&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/oddfar/campus-imaotai/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/oddfar/campus-imaotai.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/oddfar/campus-imaotai/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/oddfar/campus-imaotai.svg?sanitize=true&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; i茅台app自动预约，每日自动预约，支持docker一键部署&lt;/p&gt; &#xA;&lt;h2 align=&#34;center&#34;&gt;Campus-imaotai&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oddfar/notes&#34;&gt;笔记仓库&lt;/a&gt; | &lt;a href=&#34;https://oddfar.com&#34;&gt;我的博客&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;项目介绍&lt;/h2&gt; &#xA;&lt;p&gt;i茅台app，每日自动预约茅台&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 平台注册账号&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 添加多个用户&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 自动预约&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 类型选择（本市出货量最大的门店，或位置附近门店）&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 自动旅行&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 首次旅行分享&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 获取申购耐力值&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 自定义时间/随机时间预约或旅行&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 申购结果消息推送&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;此项目使用 &lt;strong&gt;Campus&lt;/strong&gt; 进行编写：&lt;a href=&#34;https://github.com/oddfar/campus&#34;&gt;https://github.com/oddfar/campus&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;文档&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;文档：&lt;a href=&#34;https://oddfar.github.io/campus-doc/campus-imaotai&#34;&gt;https://oddfar.github.io/campus-doc/campus-imaotai&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;视频：&lt;a href=&#34;https://www.bilibili.com/video/BV1dj411H7oT&#34;&gt;https://www.bilibili.com/video/BV1dj411H7oT&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;演示图&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;i茅台预约&lt;/th&gt; &#xA;   &lt;th&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://gcore.jsdelivr.net/gh/oddfar/campus-imaotai/.github/image-20230707144241399.png&#34; alt=&#34;image-20230707144241399&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://gcore.jsdelivr.net/gh/oddfar/campus-imaotai/.github/image-20230707144404638.png&#34; alt=&#34;image-20230707144404638&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://gcore.jsdelivr.net/gh/oddfar/campus-imaotai/.github/image-20230707144703842.png&#34; alt=&#34;image-20230707144703842&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://gcore.jsdelivr.net/gh/oddfar/campus-imaotai/.github/image-20230707145525709.png&#34; alt=&#34;image-20230707145525709&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;贡献代码&lt;/h2&gt; &#xA;&lt;p&gt;若您有好的想法，发现一些 &lt;strong&gt;BUG&lt;/strong&gt; 并修复了，欢迎提交 &lt;strong&gt;Pull Request&lt;/strong&gt; 参与开源贡献&lt;/p&gt; &#xA;&lt;p&gt;发起 pull request 请求，提交到 master 分支，等待作者合并&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;感谢为这个项目贡献代码的朋友&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/oddfar/campus-imaotai/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=oddfar/campus-imaotai&#34;&gt; &lt;/a&gt; &#xA;&lt;h2&gt;star 趋势图&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://starchart.cc/oddfar/campus-imaotai.svg?sanitize=true&#34; alt=&#34;Stargazers over time&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;友情链接&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;本项目其他版&lt;/p&gt; &lt;p&gt;C#：&lt;a href=&#34;https://github.com/lisongkun/hygge-imaotai&#34;&gt;https://github.com/lisongkun/hygge-imaotai&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;葫芦娃项目&lt;/p&gt; &lt;p&gt;yize8888-maotai：&lt;a href=&#34;https://github.com/yize8888/maotai&#34;&gt;https://github.com/yize8888/maotai&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;声明&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;本项目涉及的数据由使用的个人或组织自行填写，本项目不对数据内容负责，包括但不限于数据的真实性、准确性、合法性。使用本项目所造成的一切后果，与本项目的所有贡献者无关，由使用的个人或组织完全承担。&lt;/li&gt; &#xA; &lt;li&gt;本项目中涉及的第三方硬件、软件等，与本项目没有任何直接或间接的关系。本项目仅对部署和使用过程进行客观描述，不代表支持使用任何第三方硬件、软件。使用任何第三方硬件、软件，所造成的一切后果由使用的个人或组织承担，与本项目无关。&lt;/li&gt; &#xA; &lt;li&gt;本项目中所有内容只供学习和研究使用，不得将本项目中任何内容用于违反国家/地区/组织等的法律法规或相关规定的其他用途。&lt;/li&gt; &#xA; &lt;li&gt;所有基于本项目源代码，进行的任何修改，为其他个人或组织的自发行为，与本项目没有任何直接或间接的关系，所造成的一切后果亦与本项目无关。&lt;/li&gt; &#xA; &lt;li&gt;所有直接或间接使用本项目的个人和组织，应24小时内完成学习和研究，并及时删除本项目中的所有内容。如对本项目的功能有需求，应自行开发相关功能。&lt;/li&gt; &#xA; &lt;li&gt;本项目保留随时对免责声明进行补充或更改的权利，直接或间接使用本项目内容的个人或组织，视为接受本项目的特别声明。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;鸣谢&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://zh.wikipedia.org/zh-hans/IntelliJ_IDEA&#34;&gt;IntelliJ IDEA&lt;/a&gt; 是一个在各个方面都最大程度地提高开发人员的生产力的 IDE，适用于 JVM 平台语言。&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;特别感谢 &lt;a href=&#34;https://www.jetbrains.com/?from=campus&#34;&gt;JetBrains&lt;/a&gt; 为开源项目提供免费的 &lt;a href=&#34;https://www.jetbrains.com/idea/?from=campus&#34;&gt;IntelliJ IDEA&lt;/a&gt; 等 IDE 的授权&lt;br&gt; &lt;a href=&#34;https://www.jetbrains.com/?from=campus&#34;&gt;&lt;img src=&#34;https://gcore.jsdelivr.net/gh/oddfar/campus-imaotai/.github/jetbrains-variant.png&#34; width=&#34;200&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>