<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-01T01:28:48Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>JackAILab/ConsistentID</title>
    <updated>2024-05-01T01:28:48Z</updated>
    <id>tag:github.com,2024-05-01:/JackAILab/ConsistentID</id>
    <link href="https://github.com/JackAILab/ConsistentID" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Customized ID Consistent for human&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/JackAILab/ConsistentID/assets/135965025/c0594480-d73d-4268-95ca-5494ca2a61e4&#34; height=&#34;100&#34;&gt; &lt;/p&gt; &#xA;&lt;!-- ## &lt;div align=&#34;center&#34;&gt;&lt;b&gt;ConsistentID&lt;/b&gt;&lt;/div&gt; --&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt;ConsistentID : Portrait Generation with Multimodal Fine-Grained Identity Preserving &lt;a href=&#34;&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/resolve/main/paper-page-md-dark.svg?sanitize=true&#34; alt=&#34;Paper page&#34;&gt;&lt;/a&gt;&lt;/h2&gt; &#xA; &lt;p&gt;[üìÑ&lt;a href=&#34;https://arxiv.org/abs/2404.16771&#34;&gt;Paper&lt;/a&gt;] ‚ÄÉ [üö©&lt;a href=&#34;https://ssugarwh.github.io/consistentid.github.io/&#34;&gt;Project Page&lt;/a&gt;] ‚ÄÉ [üñº&lt;a href=&#34;http://consistentid.natapp1.cc/&#34;&gt;Gradio Demo&lt;/a&gt;] &lt;br&gt;&lt;/p&gt; &#xA; &lt;p&gt;[ü§ó&lt;a href=&#34;https://huggingface.co/spaces/JackAILab/ConsistentID&#34;&gt;Faster Demo&lt;/a&gt;] ‚ÄÉ &lt;br&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3&gt;üå† &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Portrait generation with extremely high &lt;strong&gt;ID fidelity&lt;/strong&gt;, without sacrificing diversity, text controllability.&lt;/li&gt; &#xA; &lt;li&gt;Introducing &lt;strong&gt;FaceParsing&lt;/strong&gt; and &lt;strong&gt;FaceID&lt;/strong&gt; information into the Diffusion model.&lt;/li&gt; &#xA; &lt;li&gt;Rapid customization &lt;strong&gt;within seconds&lt;/strong&gt;, with no additional LoRA training.&lt;/li&gt; &#xA; &lt;li&gt;Can serve as an &lt;strong&gt;Adapter&lt;/strong&gt; to collaborate with other Base Models alongside LoRA modules in community.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;üî• &lt;strong&gt;Examples&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://github.com/JackAILab/ConsistentID/assets/135965025/f949a03d-bed2-4839-a995-7b451d8c981b&#34; height=&#34;450&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üö© To-Do List&lt;/h2&gt; &#xA;&lt;p&gt;Your star will help facilitate the process.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release training, evaluation code, and demo!&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Retrain with more data and the SDXL base model to enhance aesthetics and generalization.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release a multi-ID input version to guide the improvement of ID diversity.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimize training and inference structures to further improve text following and ID decoupling capabilities.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üè∑Ô∏è Abstract&lt;/h2&gt; &#xA;&lt;p&gt;This is a work in the field of AIGC that introduces FaceParsing information and FaceID information into the Diffusion model. Previous work mainly focused on overall ID preservation, even though fine-grained ID preservation models such as InstantID have recently been proposed, the injection of facial ID features will be fixed. In order to achieve more flexible consistency maintenance of fine-grained IDs for facial features, a batch of 50000 multimodal fine-grained ID datasets was reconstructed for training the proposed FacialEncoder model, which can support common functions such as personalized photos, gender/age changes, and identity confusion.&lt;/p&gt; &#xA;&lt;p&gt;At the same time, we have defined a unified measurement benchmark FGIS for Fine-Grained Identity Preservice, covering several common facial personalized character scenes and characters, and constructed a fine-grained ID preservation model baseline.&lt;/p&gt; &#xA;&lt;p&gt;Finally, a large number of experiments were conducted in this article, and ConsistentID achieved the effect of SOTA in facial personalization task processing. It was verified that ConsistentID can improve ID consistency and even modify facial features by selecting finer-grained prompts, which opens up a direction for future research on Fine-Grained facial personalization.&lt;/p&gt; &#xA;&lt;h2&gt;üîß Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Python &amp;gt;= 3.8 (Recommend to use &lt;a href=&#34;https://www.anaconda.com/download/#linux&#34;&gt;Anaconda&lt;/a&gt; or &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch &amp;gt;= 2.0.0&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;cuda==11.8&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create --name ConsistentID python=3.8.10&#xA;conda activate ConsistentID&#xA;pip install -U pip&#xA;&#xA;# Install requirements&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üì¶Ô∏è Data Preparation&lt;/h2&gt; &#xA;&lt;p&gt;Prepare Data in the following format&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;‚îú‚îÄ‚îÄ data&#xA;|   ‚îú‚îÄ‚îÄ JSON_all.json &#xA;|   ‚îú‚îÄ‚îÄ resize_IMG # Imgaes &#xA;|   ‚îú‚îÄ‚îÄ all_faceID  # FaceID&#xA;|   ‚îî‚îÄ‚îÄ parsing_mask_IMG # Parsing Mask &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The .json file should be like&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[&#xA;    {&#xA;        &#34;resize_IMG&#34;: &#34;Path to resized image...&#34;,&#xA;        &#34;parsing_color_IMG&#34;: &#34;...&#34;,&#xA;        &#34;parsing_mask_IMG&#34;: &#34;...&#34;,&#xA;        &#34;vqa_llva&#34;: &#34;...&#34;,&#xA;        &#34;id_embed_file_resize&#34;: &#34;...&#34;,&#xA;        &#34;vqa_llva_more_face_detail&#34;: &#34;...&#34;&#xA;    },&#xA;    ...&#xA;]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üöÄ Train&lt;/h2&gt; &#xA;&lt;p&gt;Ensure that the workspace is the root directory of the project.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-setup&#34;&gt;bash train_bash.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üß™ Infer&lt;/h2&gt; &#xA;&lt;p&gt;Ensure that the workspace is the root directory of the project.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-setup&#34;&gt;python infer.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;‚è¨ Model weights&lt;/h2&gt; &#xA;&lt;p&gt;We are hosting the model weights on &lt;strong&gt;huggingface&lt;/strong&gt; to achieve a faster and more stable demo experience, so stay tuned ~&lt;/p&gt; &#xA;&lt;p&gt;The pre-trained model parameters of the model can now be downloaded on &lt;a href=&#34;https://drive.google.com/file/d/1jCHICryESmNkzGi8J_FlY3PjJz9gqoSI/view?usp=drive_link&#34;&gt;Google Drive&lt;/a&gt; or &lt;a href=&#34;https://pan.baidu.com/s/1NAVmH8S7Ls5rZc-snDk1Ng?pwd=nsh6&#34;&gt;Baidu Netdisk&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Inspired from many excellent demos and repos, including &lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;IPAdapter&lt;/a&gt;, &lt;a href=&#34;https://github.com/mit-han-lab/fastcomposer&#34;&gt;FastComposer&lt;/a&gt;, &lt;a href=&#34;https://github.com/TencentARC/PhotoMaker&#34;&gt;PhotoMaker&lt;/a&gt;. Thanks for their great works!&lt;/li&gt; &#xA; &lt;li&gt;Thanks to the open source contributions of the following work: &lt;a href=&#34;https://github.com/zllrunning/face-parsing.PyTorch&#34;&gt;face-parsing.PyTorch&lt;/a&gt;, &lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;, &lt;a href=&#34;https://github.com/deepinsight/insightface&#34;&gt;insightface&lt;/a&gt;, &lt;a href=&#34;https://github.com/NVlabs/ffhq-dataset&#34;&gt;FFHQ&lt;/a&gt;, &lt;a href=&#34;https://github.com/switchablenorms/CelebAMask-HQ&#34;&gt;CelebA&lt;/a&gt;, &lt;a href=&#34;https://github.com/SelfishGene/SFHQ-dataset&#34;&gt;SFHQ&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;ü§ó Thanks to the huggingface gradio team &lt;a href=&#34;https://github.com/huggingface&#34;&gt;ZeroGPUs&lt;/a&gt; for their free GPU support!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project strives to impact the domain of AI-driven image generation positively. Users are granted the freedom to create images using this tool, but they are expected to comply with local laws and utilize it responsibly. The developers do not assume any responsibility for potential misuse by users.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you found this code helpful, please consider citing:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{huang2024consistentid,&#xA;  title={ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving},&#xA;  author={Huang, Jiehui and Dong, Xiao and Song, Wenhui and Li, Hanhui and Zhou, Jun and Cheng, Yuhao and Liao, Shutao and Chen, Long and Yan, Yiqiang and Liao, Shengcai and others},&#xA;  journal={arXiv preprint arXiv:2404.16771},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>rejunity/z80-open-silicon</title>
    <updated>2024-05-01T01:28:48Z</updated>
    <id>tag:github.com,2024-05-01:/rejunity/z80-open-silicon</id>
    <link href="https://github.com/rejunity/z80-open-silicon" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Z80 open-source silicon. Goal is to become a silicon proven, pin compatible, open-source replacement for classic Z80.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/rejunity/workflows/gds/badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rejunity/workflows/docs/badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/rejunity/workflows/test/badge.svg?sanitize=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Announcement&lt;/h1&gt; &#xA;&lt;p&gt;On April 15 of 2024 Zilog has &lt;a href=&#34;https://www.mouser.com/PCN/Littelfuse_PCN_Z84C00.pdf&#34;&gt;announced End-of-Life&lt;/a&gt; for Z80, one of the most famous 8-bit CPUs of all time.&lt;/p&gt; &#xA;&lt;p&gt;It is a time for open-source and hardware preservation community to step in with a Free and Open Source Silicon (FOSS) replacement for Zilog Z80.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;GOAL: To develop a drop-in Z80 replacement in 8-bit home computers such as &lt;a href=&#34;https://www.spectrumforeveryone.co.uk/technical/zx-spectrum-pcb-schematics-layout/&#34;&gt;ZX Spectrum&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;The &lt;strong&gt;first&lt;/strong&gt; fabrication of &lt;strong&gt;FOSS Z80&lt;/strong&gt; is scheduled for &lt;strong&gt;June of 2024&lt;/strong&gt;!&lt;/p&gt; &#xA;&lt;h1&gt;Zilog Z80 modern free and open source silicon clone&lt;/h1&gt; &#xA;&lt;p&gt;On the path to become a silicon proven, pin compatible, open-source replacement for classic Zilog Z80.&lt;/p&gt; &#xA;&lt;p&gt;FOSS Z80 leverages &lt;a href=&#34;https://openroad.readthedocs.io/en&#34;&gt;OpenROAD&lt;/a&gt; flow and FOSS &lt;a href=&#34;https://skywater-pdk.readthedocs.io/en/main/&#34;&gt;130 nm Skywater PDK&lt;/a&gt; to synthesize production ready silicon. &lt;a href=&#34;https://tinytapeout.com&#34;&gt;Tiny Tapeout&lt;/a&gt; infrastructure is used to test and pool design with many others to reduce the cost of physical chip fabrication at &lt;a href=&#34;https://en.wikipedia.org/wiki/SkyWater_Technology&#34;&gt;Skywater Foundries&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;The first iteration of FOSS Z80 silicon&lt;/h2&gt; &#xA;&lt;p&gt;The first iteration is developed with &lt;a href=&#34;https://tinytapeout.com&#34;&gt;Tiny Tapeout 07&lt;/a&gt; using 130 nm process and fits on a 0.064 mm&lt;sup&gt;2&lt;/sup&gt; die area. The first fabrication is scheduled for June of 2024 as a part of &lt;a href=&#34;https://platform.efabless.com/projects/shuttle/23&#34;&gt;CI 2406 Shuttle&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Check status here: &lt;a href=&#34;https://app.tinytapeout.com/projects/668&#34;&gt;https://app.tinytapeout.com/projects/668&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img width=&#34;30%&#34; src=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/docs/tt07_z80.png&#34;&gt; &lt;/p&gt; &#xA;&lt;p&gt;The implementation is based around Guy Hutchison&#39;s &lt;a href=&#34;https://github.com/hutch31/tv80&#34;&gt;TV80&lt;/a&gt; Verilog core.&lt;/p&gt; &#xA;&lt;p&gt;Below is the image of &lt;a href=&#34;https://en.wikipedia.org/wiki/GDSII&#34;&gt;GDSII&lt;/a&gt; integrated circuit layout for FOSS Z80. It is the result of automatic place-and-route flow in &lt;a href=&#34;https://openroad.readthedocs.io/en&#34;&gt;OpenROAD&lt;/a&gt; using &lt;a href=&#34;https://skywater-pdk.readthedocs.io/en/main/&#34;&gt;130 nm&lt;/a&gt; &#34;gates&#34; logic elements.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;img width=&#34;50%&#34; src=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/docs/2x2_tiles.png&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Plan&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Submit with &lt;a href=&#34;https://app.tinytapeout.com/projects/668&#34;&gt;Tiny Tapeout 07&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Write basic documentation for Tiny Tapeout 07: &lt;a href=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/docs/info.md&#34;&gt;docs/info.md&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add thorough tests for all Z80 instructions including the &#39;illegal&#39; ones &lt;a href=&#34;https://mdfs.net/Software/Z80/Exerciser/&#34;&gt;ZEXALL&lt;/a&gt; to a testbench&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add thorough timing test of the input/output signals&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Integrate the netlist based Z80 core into the testbench for ultimate validation&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Compare different implementations: Verilog core &lt;a href=&#34;https://github.com/gdevic/A-Z80&#34;&gt;A-Z80&lt;/a&gt;, Netlist based &lt;a href=&#34;https://github.com/gdevic/Z80Explorer&#34;&gt;Z80Explorer&lt;/a&gt;, etc&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Tapeout with ChipIgnite in QFN64 package, create a PCB adapter from QFN64 to DIP40&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Tapeout with DIP40 package&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create gate-level layouts that would resemble the original Z80 layout, see the original &lt;a href=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/#Z80-Die-shots&#34;&gt;chip dies&lt;/a&gt; below. Zilog designed Z80 by manually placing each transistor by hand.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Quick start&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;You can find the top module in &lt;a href=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/src/tt_um_rejunity_z80.v&#34;&gt;src/tt_um_rejunity_z80.v&lt;/a&gt;. It instantiates Z80 and adheres to &lt;a href=&#34;https://tinytapeout.com/specs/gpio/&#34;&gt;TinyTapeout constraints&lt;/a&gt; including multiplexing the output pins onto the 8 pins of TinyTapeout chip.&lt;/li&gt; &#xA; &lt;li&gt;The core Verilog Z80 implementation is in &lt;a href=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/src/tv80&#34;&gt;src/tv80&lt;/a&gt; folder.&lt;/li&gt; &#xA; &lt;li&gt;The configuration for &lt;a href=&#34;https://theopenroadproject.org&#34;&gt;OpenROAD&lt;/a&gt; synthesis and place-and-route flow is in the &lt;a href=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/src/config.tcl&#34;&gt;src/config.tcl&lt;/a&gt; file.&lt;/li&gt; &#xA; &lt;li&gt;Finally, the testbench is implemented in &lt;a href=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/src/test/test.py&#34;&gt;src/test/test.py&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Generated layout artifacts are in &lt;a href=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/gds&#34;&gt;gds&lt;/a&gt; folder. You can use &lt;a href=&#34;https://www.klayout.de&#34;&gt;KLayout&lt;/a&gt; viewer to inspect them:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/gds/tinytapeout_07_skywater130A/tt_um_rejunity_z80.gds&#34;&gt;GDSII file of Z80 core&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/rejunity/z80-open-silicon/main/gds/tinytapeout_07_skywater130A/caravel_24066810.oas&#34;&gt;OASIS file of the Tiny Tapeout 07 chip with the Z80 core&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run it locally&lt;/h2&gt; &#xA;&lt;p&gt;Follow the instructions from Tiny Tapeout&#39;s &lt;a href=&#34;https://tinytapeout.com/hdl/testing/&#34;&gt;Testing Your Design Guide&lt;/a&gt; and install required packages.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    sudo apt install iverilog verilator&#xA;    pip3 install cocotb pytest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, run the testbench.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;    cd src&#xA;    make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you are succesfull, you should see the tests passing:&lt;/p&gt; &#xA;&lt;img width=&#34;580&#34; alt=&#34;image&#34; src=&#34;https://github.com/rejunity/z80-open-silicon/assets/1733077/e90ee88a-b693-4b2a-a184-d827084d5905&#34;&gt; &#xA;&lt;img width=&#34;609&#34; alt=&#34;image&#34; src=&#34;https://github.com/rejunity/z80-open-silicon/assets/1733077/099c6126-7e7e-468c-b775-070823e9a06c&#34;&gt; &#xA;&lt;h1&gt;Z80&lt;/h1&gt; &#xA;&lt;h2&gt;Pinout&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;               ,-------.___.-------.&#xA;    &amp;lt;--    A11 |1                40| A10    --&amp;gt;&#xA;    &amp;lt;--    A12 |2                39| A9     --&amp;gt;&#xA;    &amp;lt;--    A13 |3     Z80 CPU    38| A8     --&amp;gt;&#xA;    &amp;lt;--    A14 |4                37| A7     --&amp;gt;&#xA;    &amp;lt;--    A15 |5                36| A6     --&amp;gt;&#xA;    --&amp;gt;    CLK |6                35| A5     --&amp;gt;&#xA;    &amp;lt;-&amp;gt;     D4 |7                34| A4     --&amp;gt;&#xA;    &amp;lt;-&amp;gt;     D3 |8                33| A3     --&amp;gt;&#xA;    &amp;lt;-&amp;gt;     D5 |9                32| A2     --&amp;gt;&#xA;    &amp;lt;-&amp;gt;     D6 |10               31| A1     --&amp;gt;&#xA;           VCC |11               30| A0     --&amp;gt;&#xA;    &amp;lt;-&amp;gt;     D2 |12               29| GND&#xA;    &amp;lt;-&amp;gt;     D7 |13               28| /RFSH  --&amp;gt;&#xA;    &amp;lt;-&amp;gt;     D0 |14               27| /M1    --&amp;gt;&#xA;    &amp;lt;-&amp;gt;     D1 |15               26| /RESET &amp;lt;--&#xA;    --&amp;gt;   /INT |16               25| /BUSRQ &amp;lt;--&#xA;    --&amp;gt;   /NMI |17               24| /WAIT  &amp;lt;--&#xA;    &amp;lt;--  /HALT |18               23| /BUSAK --&amp;gt;&#xA;    &amp;lt;--  /MREQ |19               22| /WR    --&amp;gt;&#xA;    &amp;lt;--  /IORQ |20               21| /RD    --&amp;gt;&#xA;               `-------------------&#39;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baltazarstudios.com/webshare/A-Z80/Z80_CPU_Users_Manual_2004.pdf&#34;&gt;Z80 Users Manual&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baltazarstudios.com/webshare/A-Z80/z80-mostek.pdf&#34;&gt;Z80 Users Manual from Mostek&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://cini.classiccmp.org//pdf/Zilog/Zilog%20Data%20Book.PDF&#34;&gt;Zilog Data Book&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.z80.info&#34;&gt;All the information about Z80&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baltazarstudios.com/webshare/A-Z80/z80-documented-v0.91.pdf&#34;&gt;Undocumented instructions&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baltazarstudios.com/webshare/A-Z80/Z80-Opcode-Tables.pdf&#34;&gt;Opcode table&lt;/a&gt; and &lt;a href=&#34;https://baltazarstudios.com/webshare/A-Z80/Z80-Instruction-List-with-T-states.pdf&#34;&gt;timing&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Oral History of the Development of the Z80&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;http://archive.computerhistory.org/resources/text/Oral_History/Zilog_Z80/102658073.05.01.pdf&#34;&gt;Oral History Panel on the Founding of the Company and the Development of the Z80 Microprocessor&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://baltazarstudios.com/webshare/A-Z80/Library/Demystifying%20Microprocessor%20Design%20-%20M.%20Shima.pdf&#34;&gt;M. Shima on Demystifying Microprocessor Design&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Z80 Patents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;(expired)&lt;/strong&gt; Patent &lt;a href=&#34;https://patents.google.com/patent/US4605980&#34;&gt;US4605980&lt;/a&gt; -- input voltage spike protection&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(expired)&lt;/strong&gt; Patent &lt;a href=&#34;https://patents.google.com/patent/US4332008A&#34;&gt;US4332008A&lt;/a&gt; -- ???&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;(expired)&lt;/strong&gt; Patent &lt;a href=&#34;https://patents.google.com/patent/US4486827A&#34;&gt;US4486827A&lt;/a&gt; -- reset circuitry&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Z80 Die shots&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://downloads.reactivemicro.com/Electronics/Reverse%20Engineering/6502%20-%20Guideline%20to%20Reverse%20Engineering%20v1.0.pdf&#34;&gt;How to &#34;read&#34; die shots&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;nMOS variant &lt;a href=&#34;https://siliconpr0n.org/map/zilog/z8400aps-z80acpu/bercovici_mz/&#34;&gt;Z8400 with &#39;Zilog 75&#39;&lt;/a&gt; marking and &lt;a href=&#34;https://siliconpr0n.org/map/zilog/z0840008/marmontel_mz_ms20x/&#34;&gt;Zilog Z8400 with &#39;DC&#39;&lt;/a&gt; letter marking&lt;/li&gt; &#xA; &lt;li&gt;CMOS variants &lt;a href=&#34;http://visual6502.org/images/pages/Zilog_Z84C00_die_shots.html&#34;&gt;Zilog Z84C00&lt;/a&gt; and its &lt;a href=&#34;https://siliconpr0n.org/map/zilog/z84c0008fec/marmontel_mz_ms20x/&#34;&gt;8MHz version&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Nintendo Z80 variant from Super Game Boy &lt;a href=&#34;https://siliconpr0n.org/map/nintendo/sgb-cpu-01/mcmaster_mz_mit20x/&#34;&gt;SGB-CPU 01&lt;/a&gt; produced in 1994&lt;/li&gt; &#xA; &lt;li&gt;Sean Riddle&#39;s image of the official second-source Mostek MK3880 &lt;a href=&#34;https://happytrees.org/dieshots/Mostek_-_MK3880_(top_metal_removed)#/media/File:Mostek_MK3880_top_metal_removed.jpg&#34;&gt;metal layer removed&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pauli Rautakorpi&#39;s images of Z80 clones: &lt;a href=&#34;https://commons.wikimedia.org/wiki/User:Birdman86#/media/File:NS_NSC800_die.jpg&#34;&gt;National Semiconductor NSC800&lt;/a&gt;, &lt;a href=&#34;https://commons.wikimedia.org/wiki/User:Birdman86#/media/File:Mostek_MK3880_die.jpg&#34;&gt;Mostek MK3880&lt;/a&gt;, &lt;a href=&#34;https://commons.wikimedia.org/wiki/User:Birdman86#/media/File:MME_80A-CPU_die.JPG&#34;&gt;MME9201 with &#39;U880/5&#39;&lt;/a&gt; markings&lt;/li&gt; &#xA; &lt;li&gt;Zeptobar‚Äôs images of &lt;a href=&#34;https://zeptobars.com/en/read/Zilog-Z80-Z0840004PSC&#34;&gt;Zilog Z0840004PSC&lt;/a&gt; from 1990, &lt;a href=&#34;https://happytrees.org/dieshots/Soviet_-_KR1858VM3#/media/File:KR1858VM3-HD.jpg&#34;&gt;Soviet CMOS KR1858VM3&lt;/a&gt; with an uncommon layout, &lt;a href=&#34;https://zeptobars.com/en/read/Zilog-Z80-Z80A&#34;&gt;MME Z80A&lt;/a&gt; a clone on a large 5um process, &lt;a href=&#34;https://zeptobars.com/en/read/KR1858VM1-Z80-MME-Angstrem&#34;&gt;Soviet KR1858VM1&lt;/a&gt; a clone of U880/6 which in turn was an unlicensed clone of Z80, &lt;a href=&#34;https://zeptobars.com/en/read/t34vm1-z80-angstrem-mme&#34;&gt;Soviet T34VM1&lt;/a&gt; based on U880/5&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;http://visual6502.org/images/Z84C00/Z84C00_die_shot_20x_1b_1600w.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Z80 Reverse Engineering&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baltazarstudios.com/z80-instruction-register-deciphered/&#34;&gt;Z80 Instruction Register deciphered&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baltazarstudios.com/anatomy-z80-gate/&#34;&gt;Z80 Tri-stated Data &amp;amp; Address bus gates&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baltazarstudios.com/zilog-z80-undocumented-behavior/&#34;&gt;Z80 (un)documented behavior&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://static.righto.com/files/z80-pla-table.html&#34;&gt;The instruction decode PLA in the Z80 microprocessor&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.righto.com/2014/09/why-z-80s-data-pins-are-scrambled.html&#34;&gt;Why the Z-80&#39;s data pins are scrambled&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.righto.com/2014/10/how-z80s-registers-are-implemented-down.html&#34;&gt;How the Z80&#39;s registers are implemented&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.righto.com/2013/11/the-z-80s-16-bit-incrementdecrement.html&#34;&gt;The Z-80&#39;s 16-bit increment/decrement circuit reverse engineered&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.righto.com/2013/09/the-z-80-has-4-bit-alu-heres-how-it.html&#34;&gt;The Z-80 has a 4-bit ALU&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://www.righto.com/2013/09/understanding-z-80-processor-one-gate.html&#34;&gt;XOR, the silicon for two interesting gates explained&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://baltazarstudios.com/webshare/A-Z80/memptr_eng.txt&#34;&gt;WZ aka MEMPTR, esoteric register of the Z80&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Existing Z80 implementations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;TV80 in Verilog &lt;a href=&#34;https://github.com/hutch31/tv80&#34;&gt;https://github.com/hutch31/tv80&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;TV80 in Verilog &lt;a href=&#34;https://github.com/Obijuan/Z80-FPGA&#34;&gt;https://github.com/Obijuan/Z80-FPGA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A-Z80 in Verilog &lt;a href=&#34;https://github.com/gdevic/A-Z80&#34;&gt;https://github.com/gdevic/A-Z80&lt;/a&gt; its &lt;a href=&#34;https://baltazarstudios.com/z80-ground/&#34;&gt;overview&lt;/a&gt; and &lt;a href=&#34;https://baltazarstudios.com/z80-cpu/&#34;&gt;details&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Z80 net-list level emulator &lt;a href=&#34;https://github.com/gdevic/Z80Explorer&#34;&gt;https://github.com/gdevic/Z80Explorer&lt;/a&gt; and its &lt;a href=&#34;https://baltazarstudios.com/z80explorer/&#34;&gt;overview&lt;/a&gt; and &lt;a href=&#34;https://gdevic.github.io/Z80Explorer/&#34;&gt;Users Guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;What is Tiny Tapeout?&lt;/h1&gt; &#xA;&lt;p&gt;Tiny Tapeout is an educational project that aims to make it easier and cheaper than ever to get your digital designs manufactured on a real chip.&lt;/p&gt; &#xA;&lt;p&gt;To learn more and get started, visit &lt;a href=&#34;https://tinytapeout.com&#34;&gt;https://tinytapeout.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinytapeout.com/faq/&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinytapeout.com/digital_design/&#34;&gt;Digital design lessons&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinytapeout.com/siliwiz/&#34;&gt;Learn how semiconductors work&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://tinytapeout.com/discord&#34;&gt;Join the community&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.google.com/document/d/1aUUZ1jthRpg4QURIIyzlOaPWlmQzr-jBn3wZipVUPt4&#34;&gt;Build your design locally&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>xlang-ai/OSWorld</title>
    <updated>2024-05-01T01:28:48Z</updated>
    <id>tag:github.com,2024-05-01:/xlang-ai/OSWorld</id>
    <link href="https://github.com/xlang-ai/OSWorld" rel="alternate"></link>
    <summary type="html">&lt;p&gt;OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://huggingface.co/datasets/xlangai/assets/resolve/main/github_banner_v2.png&#34; alt=&#34;Banner&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://os-world.github.io/&#34;&gt;Website&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://arxiv.org/abs/2404.07972&#34;&gt;Paper&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/xlang-ai/OSWorld/tree/main/evaluation_examples&#34;&gt;Data&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://os-world.github.io/explorer.html&#34;&gt;Data Viewer&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://discord.gg/4Gnw7eTEZR&#34;&gt;Discord&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://img.shields.io/badge/PRs-Welcome-red&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/PRs-Welcome-red&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://img.shields.io/github/last-commit/xlang-ai/OSWorld?color=green&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/last-commit/xlang-ai/OSWorld?color=green&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/desktop-env&#34;&gt; &lt;img src=&#34;https://badge.fury.io/py/desktop-env.svg?sanitize=true&#34;&gt; &lt;/a&gt; &lt;a href=&#34;https://pepy.tech/project/desktop-env&#34;&gt; &lt;img src=&#34;https://static.pepy.tech/badge/desktop-env&#34;&gt; &lt;/a&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;h2&gt;üì¢ Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;2024-04-11: We released our &lt;a href=&#34;https://arxiv.org/abs/2404.07972&#34;&gt;paper&lt;/a&gt;, &lt;a href=&#34;https://github.com/xlang-ai/OSWorld&#34;&gt;environment and benchmark&lt;/a&gt;, and &lt;a href=&#34;https://os-world.github.io/&#34;&gt;project page&lt;/a&gt;. Check it out!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üíæ Installation&lt;/h2&gt; &#xA;&lt;h3&gt;On Your Desktop or Server (Non-Virtualized Platform)&lt;/h3&gt; &#xA;&lt;p&gt;Suppose you are operating on a system that has not been virtualized, meaning you are not utilizing a virtualized environment like AWS, Azure, or k8s. If this is the case, proceed with the instructions below. However, if you are on a virtualized platform, please refer to the &lt;a href=&#34;https://github.com/xlang-ai/OSWorld?tab=readme-ov-file#virtualized-platform&#34;&gt;virtualized platform&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First, clone this repository and &lt;code&gt;cd&lt;/code&gt; into it. Then, install the dependencies listed in &lt;code&gt;requirements.txt&lt;/code&gt;. It is recommended that you use the latest version of Conda to manage the environment, but you can also choose to manually install the dependencies. Please ensure that the version of Python is &amp;gt;= 3.9.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the OSWorld repository&#xA;git clone https://github.com/xlang-ai/OSWorld&#xA;&#xA;# Change directory into the cloned repository&#xA;cd OSWorld&#xA;&#xA;# Optional: Create a Conda environment for OSWorld&#xA;# conda create -n osworld python=3.9&#xA;# conda activate osworld&#xA;&#xA;# Install required dependencies&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, you can install the environment without any benchmark tasks:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install desktop-env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Install &lt;a href=&#34;https://www.vmware.com/products/workstation-pro/workstation-pro-evaluation.html&#34;&gt;VMware Workstation Pro&lt;/a&gt; (for systems with Apple Chips, you should install &lt;a href=&#34;https://www.vmware.com/go/getfusion&#34;&gt;VMware Fusion&lt;/a&gt;) and configure the &lt;code&gt;vmrun&lt;/code&gt; command. The installation process can refer to &lt;a href=&#34;https://raw.githubusercontent.com/xlang-ai/OSWorld/main/INSTALL_VMWARE.md&#34;&gt;How to install VMware Worksation Pro&lt;/a&gt;. Verify the successful installation by running the following:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;vmrun -T ws list&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the installation along with the environment variable set is successful, you will see the message showing the current running virtual machines.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We will also support using &lt;a href=&#34;https://www.virtualbox.org/&#34;&gt;VirtualBox&lt;/a&gt; in the near future if you have issues with VMware Pro. However, features such as parallelism and macOS on Apple chips are not supported.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;All set! Our setup script will automatically download the necessary virtual machines and configure the environment for you.&lt;/p&gt; &#xA;&lt;h3&gt;On AWS or Azure (Virtualized platform)&lt;/h3&gt; &#xA;&lt;p&gt;We are working on supporting it üë∑. Please hold tight!&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;Run the following minimal example to interact with the environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from desktop_env.envs.desktop_env import DesktopEnv&#xA;&#xA;example = {&#xA;    &#34;id&#34;: &#34;94d95f96-9699-4208-98ba-3c3119edf9c2&#34;,&#xA;    &#34;instruction&#34;: &#34;I want to install Spotify on my current system. Could you please help me?&#34;,&#xA;    &#34;config&#34;: [&#xA;        {&#xA;            &#34;type&#34;: &#34;execute&#34;,&#xA;            &#34;parameters&#34;: {&#xA;                &#34;command&#34;: [&#xA;                    &#34;python&#34;,&#xA;                    &#34;-c&#34;,&#xA;                    &#34;import pyautogui; import time; pyautogui.click(960, 540); time.sleep(0.5);&#34;&#xA;                ]&#xA;            }&#xA;        }&#xA;    ],&#xA;    &#34;evaluator&#34;: {&#xA;        &#34;func&#34;: &#34;check_include_exclude&#34;,&#xA;        &#34;result&#34;: {&#xA;            &#34;type&#34;: &#34;vm_command_line&#34;,&#xA;            &#34;command&#34;: &#34;which spotify&#34;&#xA;        },&#xA;        &#34;expected&#34;: {&#xA;            &#34;type&#34;: &#34;rule&#34;,&#xA;            &#34;rules&#34;: {&#xA;                &#34;include&#34;: [&#34;spotify&#34;],&#xA;                &#34;exclude&#34;: [&#34;not found&#34;]&#xA;            }&#xA;        }&#xA;    }&#xA;}&#xA;&#xA;env = DesktopEnv(action_space=&#34;pyautogui&#34;)&#xA;&#xA;obs = env.reset(task_config=example)&#xA;obs, reward, done, info = env.step(&#34;pyautogui.rightClick()&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You will see all the logs of the system running normally, including the successful creation of the environment, completion of setup, and successful execution of actions. In the end, you will observe a successful right-click on the screen, which means you are ready to go.&lt;/p&gt; &#xA;&lt;h2&gt;üß™ Experiments&lt;/h2&gt; &#xA;&lt;h3&gt;Agent Baselines&lt;/h3&gt; &#xA;&lt;p&gt;If you wish to run the baseline agent used in our paper, you can execute the following command as an example under the GPT-4V pure-screenshot setting:&lt;/p&gt; &#xA;&lt;p&gt;Set &lt;strong&gt;OPENAI_API_KEY&lt;/strong&gt; environment variable with your API key&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;export OPENAI_API_KEY=&#39;changme&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python run.py --path_to_vm Ubuntu/Ubuntu.vmx --headless --observation_type screenshot --model gpt-4-vision-preview --result_dir ./results&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The results, which include screenshots, actions, and video recordings of the agent&#39;s task completion, will be saved in the &lt;code&gt;./results&lt;/code&gt; directory in this case. You can then run the following command to obtain the result:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python show_result.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Please start by reading through the &lt;a href=&#34;https://github.com/xlang-ai/OSWorld/raw/main/mm_agents/README.md&#34;&gt;agent interface&lt;/a&gt; and the &lt;a href=&#34;https://github.com/xlang-ai/OSWorld/raw/main/desktop_env/README.md&#34;&gt;environment interface&lt;/a&gt;. Correctly implement the agent interface and import your customized version in the &lt;code&gt;run.py&lt;/code&gt; file. Afterward, you can execute a command similar to the one in the previous section to run the benchmark on your agent.&lt;/p&gt; &#xA;&lt;h2&gt;‚ùì FAQ&lt;/h2&gt; &#xA;&lt;h3&gt;What is the username and password for the virtual machines?&lt;/h3&gt; &#xA;&lt;p&gt;The username and password for the virtual machines are as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ubuntu:&lt;/strong&gt; &lt;code&gt;user&lt;/code&gt; / &lt;code&gt;password&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;How can I configure a proxy for the VM if I&#39;m behind a GFW?&lt;/h3&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/xlang-ai/OSWorld/main/PROXY_GUIDELINE.md&#34;&gt;Proxy Guideline&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;What are the running times and costs under different settings?&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Setting&lt;/th&gt; &#xA;   &lt;th&gt;Expected Time*&lt;/th&gt; &#xA;   &lt;th&gt;Budget Cost (Full Test Set/Small Test Set)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4V (screenshot)&lt;/td&gt; &#xA;   &lt;td&gt;10h&lt;/td&gt; &#xA;   &lt;td&gt;$100 ($10)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Gemini-ProV (screenshot)&lt;/td&gt; &#xA;   &lt;td&gt;15h&lt;/td&gt; &#xA;   &lt;td&gt;$0 ($0)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Claude-3 Opus (screenshot)&lt;/td&gt; &#xA;   &lt;td&gt;15h&lt;/td&gt; &#xA;   &lt;td&gt;$150 ($15)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-4V (a11y tree, SoM, etc.)&lt;/td&gt; &#xA;   &lt;td&gt;30h&lt;/td&gt; &#xA;   &lt;td&gt;$500 ($50)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;*No environment parallelism. Calculated in April 2024.&lt;/p&gt; &#xA;&lt;h2&gt;üìÑ Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this environment useful, please consider citing our work:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{OSWorld,&#xA;      title={OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments}, &#xA;      author={Tianbao Xie and Danyang Zhang and Jixuan Chen and Xiaochuan Li and Siheng Zhao and Ruisheng Cao and Toh Jing Hua and Zhoujun Cheng and Dongchan Shin and Fangyu Lei and Yitao Liu and Yiheng Xu and Shuyan Zhou and Silvio Savarese and Caiming Xiong and Victor Zhong and Tao Yu},&#xA;      year={2024},&#xA;      eprint={2404.07972},&#xA;      archivePrefix={arXiv},&#xA;      primaryClass={cs.AI}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>