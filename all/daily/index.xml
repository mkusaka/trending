<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-13T01:23:41Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>linkedin/school-of-sre</title>
    <updated>2023-09-13T01:23:41Z</updated>
    <id>tag:github.com,2023-09-13:/linkedin/school-of-sre</id>
    <link href="https://github.com/linkedin/school-of-sre" rel="alternate"></link>
    <summary type="html">&lt;p&gt;At LinkedIn, we are using this curriculum for onboarding our entry-level talents into the SRE role.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;School of SRE&lt;/h1&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/linkedin/school-of-sre/main/img/sos.png&#34; width=&#34;200&#34;&gt; &#xA;&lt;p&gt;Site Reliability Engineers (SREs) sits at the intersection of software engineering and systems engineering. While there are potentially infinite permutations and combinations of how infrastructure and software components can be put together to achieve an objective, focusing on foundational skills allows SREs to work with complex systems and software, regardless of whether these systems are proprietary, 3rd party, open systems, run on cloud/on-prem infrastructure, etc. Particularly important is to gain a deep understanding of how these areas of systems and infrastructure relate to each other and interact with each other. The combination of software and systems engineering skills is rare and is generally built over time with exposure to a wide variety of infrastructure, systems, and software.&lt;/p&gt; &#xA;&lt;p&gt;SREs bring in engineering practices to keep the site up. Each distributed system is an agglomeration of many components. SREs validate business requirements, convert them to SLAs for each of the components that constitute the distributed system, monitor and measure adherence to SLAs, re-architect or scale out to mitigate or avoid SLA breaches, add these learnings as feedback to new systems or projects and thereby reduce operational toil. Hence SREs play a vital role right from the day 0 design of the system.&lt;/p&gt; &#xA;&lt;p&gt;In early 2019, we started visiting campuses across India to recruit the best and brightest minds to make sure LinkedIn, and all the services that make up its complex technology stack are always available for everyone. This critical function at LinkedIn falls under the purview of the Site Engineering team and Site Reliability Engineers (SREs) who are Software Engineers, specialized in reliability.&lt;/p&gt; &#xA;&lt;p&gt;As we continued on this journey we started getting a lot of questions from these campuses on what exactly the site reliability engineering role entails? And, how could someone learn the skills and the disciplines involved to become a successful site reliability engineer? Fast forward a few months, and a few of these campus students had joined LinkedIn either as interns or as full-time engineers to become a part of the Site Engineering team; we also had a few lateral hires who joined our organization who were not from a traditional SRE background. That&#39;s when a few of us got together and started to think about how we can onboard new graduate engineers to the Site Engineering team.&lt;/p&gt; &#xA;&lt;p&gt;There are very few resources out there guiding someone on the basic skill sets one has to acquire as a beginner SRE. Because of the lack of these resources, we felt that individuals have a tough time getting into open positions in the industry. We created the School Of SRE as a starting point for anyone wanting to build their career as an SRE. In this course, we are focusing on building strong foundational skills. The course is structured in a way to provide more real life examples and how learning each of these topics can play an important role in day to day job responsibilities of an SRE. Currently we are covering the following topics under the School Of SRE:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Level 101&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Fundamentals Series &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/linux_basics/intro/&#34;&gt;Linux Basics&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/git/git-basics/&#34;&gt;Git&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/linux_networking/intro/&#34;&gt;Linux Networking&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/python_web/intro/&#34;&gt;Python and Web&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Data &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/databases_sql/intro/&#34;&gt;Relational databases(MySQL)&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/databases_nosql/intro/&#34;&gt;NoSQL concepts&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/big_data/intro/&#34;&gt;Big Data&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/systems_design/intro/&#34;&gt;Systems Design&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/metrics_and_monitoring/introduction/&#34;&gt;Metrics and Monitoring&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level101/security/intro/&#34;&gt;Security&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Level 102&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level102/linux_intermediate/introduction/&#34;&gt;Linux Intermediate&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Linux Advanced &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level102/containerization_and_orchestration/intro/&#34;&gt;Containers and orchestration&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level102/system_calls_and_signals/intro/&#34;&gt;System Calls and Signals&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level102/networking/introduction/&#34;&gt;Networking&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level102/system_design/intro/&#34;&gt;System Design&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level102/system_troubleshooting_and_performance/introduction/&#34;&gt;System troubleshooting and performance improvements&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://linkedin.github.io/school-of-sre/level102/continuous_integration_and_continuous_delivery/introduction/&#34;&gt;Continuous Integration and Continuous Delivery&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We believe continuous learning will help in acquiring deeper knowledge and competencies in order to expand your skill sets, every module has added references that could be a guide for further learning. Our hope is that by going through these modules we should be able to build the essential skills required for a Site Reliability Engineer.&lt;/p&gt; &#xA;&lt;p&gt;At LinkedIn, we are using this curriculum for onboarding our non-traditional hires and new college grads into the SRE role. We had multiple rounds of successful onboarding experiences with new employees and the course helped them be productive in a very short period of time. This motivated us to open source the content for helping other organizations in onboarding new engineers into the role and provide guidance for aspiring individuals to get into the role. We realize that the initial content we created is just a starting point and we hope that the community can help in the journey of refining and expanding the content. Check out &lt;a href=&#34;https://raw.githubusercontent.com/linkedin/school-of-sre/main/CONTRIBUTING.md&#34;&gt;the contributing guide&lt;/a&gt; to get started.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>tencent-ailab/IP-Adapter</title>
    <updated>2023-09-13T01:23:41Z</updated>
    <id>tag:github.com,2023-09-13:/tencent-ailab/IP-Adapter</id>
    <link href="https://github.com/tencent-ailab/IP-Adapter" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The image prompt adapter is designed to enable a pretrained text-to-image diffusion model to generate images with image prompt.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://ip-adapter.github.io&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/2308.06721&#34;&gt;&lt;strong&gt;Paper (ArXiv)&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;we present IP-Adapter, an effective and lightweight adapter to achieve image prompt capability for the pre-trained text-to-image diffusion models. An IP-Adapter with only 22M parameters can achieve comparable or even better performance to a fine-tuned image prompt model. IP-Adapter can be generalized not only to other custom models fine-tuned from the same base model, but also to controllable generation using existing controllable tools. Moreover, the image prompt can also work well with the text prompt to accomplish multimodal image generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/figs/fig1.png&#34; alt=&#34;arch&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Release&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/9/08] üî• Update a new version of IP-Adapter with SDXL_1.0. More information can be found &lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/#sdxl_10&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/9/05] üî•üî•üî• IP-Adapter is supported in &lt;a href=&#34;https://github.com/Mikubill/sd-webui-controlnet/discussions/2039&#34;&gt;WebUI&lt;/a&gt; and &lt;a href=&#34;https://github.com/laksjdjf/IPAdapter-ComfyUI&#34;&gt;ComfyUI&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/30] üî• Add an IP-Adapter with face image as prompt. The demo is &lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter-plus-face_demo.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/29] üî• Release the training code.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/23] üî• Add code and models of IP-Adapter with fine-grained features. The demo is &lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter-plus_demo.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/18] üî• Add code and models for &lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;SDXL 1.0&lt;/a&gt;. The demo is &lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_sdxl_demo.ipynb&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;[2023/8/16] üî• We release the code and models.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Dependencies&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;diffusers &amp;gt;= 0.19.3&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Download Models&lt;/h2&gt; &#xA;&lt;p&gt;you can download models from &lt;a href=&#34;https://huggingface.co/h94/IP-Adapter&#34;&gt;here&lt;/a&gt;. To run the demo, you should also download the following models:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;runwayml/stable-diffusion-v1-5&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse&#34;&gt;stabilityai/sd-vae-ft-mse&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/SG161222/Realistic_Vision_V4.0_noVAE&#34;&gt;SG161222/Realistic_Vision_V4.0_noVAE&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/lllyasviel&#34;&gt;ControlNet models&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Use&lt;/h2&gt; &#xA;&lt;h3&gt;SD_1.5&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_demo&lt;/strong&gt;&lt;/a&gt;: image variations, image-to-image, and inpainting with image prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/tencent-ailab/IP-Adapter/blob/main/ip_adapter_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;ip_adapter_demo&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/image_variations.jpg&#34; alt=&#34;image variations&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/image-to-image.jpg&#34; alt=&#34;image-to-image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/inpainting.jpg&#34; alt=&#34;inpainting&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_controlnet_demo_new.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_controlnet_demo&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_t2i-adapter_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_t2i-adapter&lt;/strong&gt;&lt;/a&gt;: structural generation with image prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/tencent-ailab/IP-Adapter/blob/main/ip_adapter_controlnet_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;ip_adapter_controlnet_demo&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/structural_cond.jpg&#34; alt=&#34;structural_cond&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/t2i-adapter_demo.jpg&#34; alt=&#34;structural_cond2&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_multimodal_prompts_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_multimodal_prompts_demo&lt;/strong&gt;&lt;/a&gt;: generation with multimodal prompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://colab.research.google.com/github/tencent-ailab/IP-Adapter/blob/main/ip_adapter_multimodal_prompts_demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;ip_adapter_multimodal_prompts_demo&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/multi_prompts.jpg&#34; alt=&#34;multi_prompts&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter-plus_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter-plus_demo&lt;/strong&gt;&lt;/a&gt;: the demo of IP-Adapter with fine-grained features.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/ip_adpter_plus_image_variations.jpg&#34; alt=&#34;ip_adpter_plus_image_variations&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/ip_adpter_plus_multi.jpg&#34; alt=&#34;ip_adpter_plus_multi&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter-plus-face_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter-plus-face_demo&lt;/strong&gt;&lt;/a&gt;: generation with face image as prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/sd15_face.jpg&#34; alt=&#34;ip_adpter_plus_face&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Best Practice&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you only use the image prompt, you can set the &lt;code&gt;scale=1.0&lt;/code&gt; and &lt;code&gt;text_prompt=&#34;&#34;&lt;/code&gt;(or some generic text prompts, e.g. &#34;best quality&#34;, you can also use any negative text prompt). If you lower the &lt;code&gt;scale&lt;/code&gt;, more diverse images can be generated, but they may not be as consistent with the image prompt.&lt;/li&gt; &#xA; &lt;li&gt;For multimodal prompts, you can adjust the &lt;code&gt;scale&lt;/code&gt; to get the best results. In most cases, setting &lt;code&gt;scale=0.5&lt;/code&gt; can get good results. For the version of SD 1.5, we recommend using community models to generate good images.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SDXL_1.0&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_sdxl_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_sdxl_demo&lt;/strong&gt;&lt;/a&gt;: image variations with image prompt.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/ip_adapter_sdxl_controlnet_demo.ipynb&#34;&gt;&lt;strong&gt;ip_adapter_sdxl_controlnet_demo&lt;/strong&gt;&lt;/a&gt;: structural generation with image prompt.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The comparison of &lt;strong&gt;IP-Adapter_XL&lt;/strong&gt; with &lt;a href=&#34;https://clipdrop.co/stable-diffusion-reimagine&#34;&gt;Reimagine XL&lt;/a&gt; is shown as follows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/tencent-ailab/IP-Adapter/main/assets/demo/sdxl_cmp.jpg&#34; alt=&#34;sdxl_demo&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Improvements in new version (2023.9.8)&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Switch to CLIP-ViT-H&lt;/strong&gt;: we trained the new IP-Adapter with &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K&#34;&gt;OpenCLIP-ViT-H-14&lt;/a&gt; instead of &lt;a href=&#34;https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k&#34;&gt;OpenCLIP-ViT-bigG-14&lt;/a&gt;. Although ViT-bigG is much larger than ViT-H, our experimental results did not find a significant difference, and the smaller model can reduce the memory usage in the inference phase.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;A Faster and better training recipe&lt;/strong&gt;: In our previous version, training directly at a resolution of 1024x1024 proved to be highly inefficient. However, in the new version, we have implemented a more effective two-stage training strategy. Firstly, we perform pre-training at a resolution of 512x512. Then, we employ a multi-scale strategy for fine-tuning. (Maybe this training strategy can also be used to speed up the training of controlnet).&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How to Train&lt;/h2&gt; &#xA;&lt;p&gt;For training, you should install &lt;a href=&#34;https://github.com/huggingface/accelerate&#34;&gt;accelerate&lt;/a&gt; and make your own dataset into a json file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;accelerate launch --num_processes 8 --multi_gpu --mixed_precision &#34;fp16&#34; \&#xA;  tutorial_train.py \&#xA;  --pretrained_model_name_or_path=&#34;runwayml/stable-diffusion-v1-5/&#34; \&#xA;  --image_encoder_path=&#34;{image_encoder_path}&#34; \&#xA;  --data_json_file=&#34;{data.json}&#34; \&#xA;  --data_root_path=&#34;{image_path}&#34; \&#xA;  --mixed_precision=&#34;fp16&#34; \&#xA;  --resolution=512 \&#xA;  --train_batch_size=8 \&#xA;  --dataloader_num_workers=4 \&#xA;  --learning_rate=1e-04 \&#xA;  --weight_decay=0.01 \&#xA;  --output_dir=&#34;{output_dir}&#34; \&#xA;  --save_steps=10000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;This project strives to positively impact the domain of AI-driven image generation. Users are granted the freedom to create images using this tool, but they are expected to comply with local laws and utilize it in a responsible manner. &lt;strong&gt;The developers do not assume any responsibility for potential misuse by users.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find IP-Adapter useful for your research and applications, please cite using this BibTeX:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{ye2023ip-adapter,&#xA;  title={IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models},&#xA;  author={Ye, Hu and Zhang, Jun and Liu, Sibo and Han, Xiao and Yang, Wei},&#xA;  booktitle={arXiv preprint arxiv:2308.06721},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>honojs/hono</title>
    <updated>2023-09-13T01:23:41Z</updated>
    <id>tag:github.com,2023-09-13:/honojs/hono</id>
    <link href="https://github.com/honojs/hono" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Ultrafast web framework for the Edges&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://hono.dev&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/honojs/hono/main/docs/images/hono-title.png&#34; width=&#34;500&#34; height=&#34;auto&#34; alt=&#34;Hono&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://hono.dev&#34;&gt;&lt;b&gt;Documentation &lt;span&gt;üëâ&lt;/span&gt; hono.dev&lt;/b&gt;&lt;/a&gt;&lt;br&gt; &lt;i&gt;v3 has been released!&lt;/i&gt; &lt;a href=&#34;https://raw.githubusercontent.com/honojs/hono/main/docs/MIGRATION.md&#34;&gt;Migration guide &lt;/a&gt;&lt;/p&gt;&#xA;&lt;a href=&#34;https://raw.githubusercontent.com/honojs/hono/main/docs/MIGRATION.md&#34;&gt; &#xA; &lt;hr&gt; &lt;/a&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/honojs/hono/main/docs/MIGRATION.md&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/honojs/hono/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/honojs/hono/ci.yml?branch=main&#34; alt=&#34;GitHub Workflow Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/honojs/hono/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/honojs/hono&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/hono&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/hono&#34; alt=&#34;npm&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/hono&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/dm/hono&#34; alt=&#34;npm&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bundlephobia.com/result?p=hono&#34;&gt;&lt;img src=&#34;https://img.shields.io/bundlephobia/min/hono&#34; alt=&#34;Bundle Size&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://bundlephobia.com/result?p=hono&#34;&gt;&lt;img src=&#34;https://img.shields.io/bundlephobia/minzip/hono&#34; alt=&#34;Bundle Size&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.npmjs.com/package/hono&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/types/hono&#34; alt=&#34;npm type definitions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/honojs/hono/pulse&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/commit-activity/m/honojs/hono&#34; alt=&#34;GitHub commit activity&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/honojs/hono/commits/main&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/last-commit/honojs/hono&#34; alt=&#34;GitHub last commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://doc.deno.land/https/deno.land/x/hono/mod.ts&#34;&gt;&lt;img src=&#34;https://img.shields.io/endpoint?url=https%3A%2F%2Fdeno-visualizer.danopia.net%2Fshields%2Flatest-version%2Fx%2Fhono%2Fmod.ts&#34; alt=&#34;Deno badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/KMh2eNSdxV&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1011308539819597844?label=Discord&amp;amp;logo=Discord&#34; alt=&#34;Discord badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Hono - &lt;em&gt;&lt;strong&gt;[ÁÇé] means flameüî• in Japanese&lt;/strong&gt;&lt;/em&gt; - is a small, simple, and ultrafast web framework for the Edges. It works on any JavaScript runtime: Cloudflare Workers, Fastly Compute@Edge, Deno, Bun, Vercel, Lagon, AWS Lambda, Lambda@Edge, and Node.js.&lt;/p&gt; &#xA;&lt;p&gt;Fast, but not only fast.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-ts&#34;&gt;import { Hono } from &#39;hono&#39;&#xA;const app = new Hono()&#xA;&#xA;app.get(&#39;/&#39;, (c) =&amp;gt; c.text(&#39;Hono!&#39;))&#xA;&#xA;export default app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Quick Start&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm create hono@latest my-app&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ultrafast&lt;/strong&gt; üöÄ - The router &lt;code&gt;RegExpRouter&lt;/code&gt; is really fast. Not using linear loops. Fast.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Lightweight&lt;/strong&gt; ü™∂ - The &lt;code&gt;hono/tiny&lt;/code&gt; preset is under 12kB. Hono has zero dependencies and uses only the Web Standard API.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Multi-runtime&lt;/strong&gt; üåç - Works on Cloudflare Workers, Fastly Compute@Edge, Deno, Bun, Lagon, AWS Lambda, Lambda@Edge, or Node.js. The same code runs on all platforms.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Batteries Included&lt;/strong&gt; üîã - Hono has built-in middleware, custom middleware, and third-party middleware. Batteries included.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Delightful DX&lt;/strong&gt; üõ†Ô∏è - Super clean APIs. First-class TypeScript support. Now, we&#39;ve got &#34;Types&#34;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hono is the fastest&lt;/strong&gt;, compared to other routers for Cloudflare Workers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;Hono x 510,171 ops/sec ¬±4.61% (82 runs sampled)&#xA;itty-router x 285,810 ops/sec ¬±4.13% (85 runs sampled)&#xA;sunder x 345,272 ops/sec ¬±4.46% (87 runs sampled)&#xA;worktop x 203,468 ops/sec ¬±3.03% (91 runs sampled)&#xA;Fastest is Hono&#xA;‚ú®  Done in 28.68s.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The documentation is available on &lt;a href=&#34;https://hono.dev&#34;&gt;hono.dev&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Migration&lt;/h2&gt; &#xA;&lt;p&gt;The migration guide is available on &lt;a href=&#34;https://raw.githubusercontent.com/honojs/hono/main/docs/MIGRATION.md&#34;&gt;docs/MIGRATION.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Communication&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://twitter.com/honojs&#34;&gt;Twitter&lt;/a&gt; and &lt;a href=&#34;https://discord.gg/KMh2eNSdxV&#34;&gt;Discord channel&lt;/a&gt; are available.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions Welcome! You can contribute in the following ways.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Create an Issue - Propose a new feature. Report a bug.&lt;/li&gt; &#xA; &lt;li&gt;Pull Request - Fix a bug and typo. Refactor the code.&lt;/li&gt; &#xA; &lt;li&gt;Create third-party middleware - Instruct below.&lt;/li&gt; &#xA; &lt;li&gt;Share - Share your thoughts on the Blog, Twitter, and others.&lt;/li&gt; &#xA; &lt;li&gt;Make your application - Please try to use Hono.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For more details, see &lt;a href=&#34;https://raw.githubusercontent.com/honojs/hono/main/docs/CONTRIBUTING.md&#34;&gt;docs/CONTRIBUTING.md&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://github.com/honojs/hono/graphs/contributors&#34;&gt;all contributors&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Authors&lt;/h2&gt; &#xA;&lt;p&gt;Yusuke Wada &lt;a href=&#34;https://github.com/yusukebe&#34;&gt;https://github.com/yusukebe&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;RegExpRouter&lt;/em&gt;, &lt;em&gt;SmartRouter&lt;/em&gt;, &lt;em&gt;LinearRouter&lt;/em&gt;, and &lt;em&gt;PatternRouter&lt;/em&gt; are created by Taku Amano &lt;a href=&#34;https://github.com/usualoma&#34;&gt;https://github.com/usualoma&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Distributed under the MIT License. See &lt;a href=&#34;https://raw.githubusercontent.com/honojs/hono/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
</feed>