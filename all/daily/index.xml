<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-02-02T01:28:37Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>oumi-ai/oumi</title>
    <updated>2025-02-02T01:28:37Z</updated>
    <id>tag:github.com,2025-02-02:/oumi-ai/oumi</id>
    <link href="https://github.com/oumi-ai/oumi" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Everything you need to build state-of-the-art foundation models, end-to-end.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/docs/_static/logo/header_logo.png&#34; alt=&#34;# Oumi: Open Universal Machine Intelligence&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://oumi.ai/docs/en/latest/index.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Documentation-oumi-blue.svg?sanitize=true&#34; alt=&#34;Documentation&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://oumi.ai/blog&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Blog-oumi-blue.svg?sanitize=true&#34; alt=&#34;Blog&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/oumi&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1286348126797430814?label=Discord&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/oumi&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/oumi.svg?sanitize=true&#34; alt=&#34;PyPI version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://opensource.org/licenses/Apache-2.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/oumi-ai/oumi/actions/workflows/pretest.yaml&#34;&gt;&lt;img src=&#34;https://github.com/oumi-ai/oumi/actions/workflows/pretest.yaml/badge.svg?branch=main&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/oumi-ai/oumi/actions/workflows/gpu_tests.yaml&#34;&gt;&lt;img src=&#34;https://github.com/oumi-ai/oumi/actions/workflows/gpu_tests.yaml/badge.svg?branch=main&#34; alt=&#34;GPU Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/oumi-ai/oumi&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/oumi-ai/oumi&#34; alt=&#34;GitHub Repo stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/psf/black&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/code%20style-black-000000.svg?sanitize=true&#34; alt=&#34;Code style: black&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/pre-commit/pre-commit&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&#34; alt=&#34;pre-commit&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://oumi.ai&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/About-oumi-blue.svg?sanitize=true&#34; alt=&#34;About&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Everything you need to build state-of-the-art foundation models, end-to-end.&lt;/h3&gt; &#xA;&lt;p&gt;Oumi is a fully open-source platform that streamlines the entire lifecycle of foundation models - from data preparation and training to evaluation and deployment. Whether you&#39;re developing on a laptop, launching large scale experiments on a cluster, or deploying models in production, Oumi provides the tools and workflows you need.&lt;/p&gt; &#xA;&lt;p&gt;With Oumi, you can:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üöÄ Train and fine-tune models from 10M to 405B parameters using state-of-the-art techniques (SFT, LoRA, QLoRA, DPO, and more)&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ Work with both text and multimodal models (Llama, Qwen, Phi, and others)&lt;/li&gt; &#xA; &lt;li&gt;üîÑ Synthesize and curate training data with LLM judges&lt;/li&gt; &#xA; &lt;li&gt;‚ö°Ô∏è Deploy models efficiently with popular inference engines (vLLM, SGLang)&lt;/li&gt; &#xA; &lt;li&gt;üìä Evaluate models comprehensively across standard benchmarks&lt;/li&gt; &#xA; &lt;li&gt;üåé Run anywhere - from laptops to clusters to clouds (AWS, Azure, GCP, Lambda, and more)&lt;/li&gt; &#xA; &lt;li&gt;üîå Integrate with both open models and commercial APIs (OpenAI, Anthropic, Vertex AI, Together, Parasail, ...)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;All with one consistent API, production-grade reliability, and all the flexibility you need for research.&lt;/p&gt; &#xA;&lt;p&gt;Learn more at &lt;a href=&#34;https://oumi.ai/docs&#34;&gt;oumi.ai&lt;/a&gt;, or jump right in with the &lt;a href=&#34;https://oumi.ai/docs/en/latest/get_started/quickstart.html&#34;&gt;quickstart guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ Getting Started&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Notebook&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Try in Colab&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Goal&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üéØ Getting Started: A Tour&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20A%20Tour.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Quick tour of core features: training, evaluation, inference, and job management&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üîß Model Finetuning Guide&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Finetuning%20Tutorial.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;End-to-end guide to LoRA tuning with data prep, training, and evaluation&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üìö Model Distillation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Distill%20a%20Large%20Model.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Guide to distilling large models into smaller, efficient ones&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üìã Model Evaluation&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Evaluation%20with%20Oumi.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Comprehensive model evaluation using Oumi&#39;s evaluation framework&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;‚òÅÔ∏è Remote Training&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Running%20Jobs%20Remotely.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch and monitor training jobs on cloud (AWS, Azure, GCP, Lambda, etc.) platforms&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üìà LLM-as-a-Judge&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Oumi%20Judge.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Filter and curate training data with built-in judges&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;üîÑ vLLM Inference Engine&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a target=&#34;_blank&#34; href=&#34;https://colab.research.google.com/github/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20Using%20vLLM%20Engine%20for%20Inference.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Fast inference at scale with the vLLM engine&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;üîß Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;Installing oumi in your environment is straightforward:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Install the package (CPU &amp;amp; NPU only)&#xA;pip install oumi  # For local development &amp;amp; testing&#xA;&#xA;# OR, with GPU support (Requires Nvidia or AMD GPU)&#xA;pip install oumi[gpu]  # For GPU training&#xA;&#xA;# To get the latest version, install from the source&#xA;pip install git+https://github.com/oumi-ai/oumi.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more advanced installation options, see the &lt;a href=&#34;https://oumi.ai/docs/en/latest/get_started/installation.html&#34;&gt;installation guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Oumi CLI&lt;/h3&gt; &#xA;&lt;p&gt;You can quickly use the &lt;code&gt;oumi&lt;/code&gt; command to train, evaluate, and infer models using one of the existing &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes&#34;&gt;recipes&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# Training&#xA;oumi train -c configs/recipes/smollm/sft/135m/quickstart_train.yaml&#xA;&#xA;# Evaluation&#xA;oumi evaluate -c configs/recipes/smollm/evaluation/135m/quickstart_eval.yaml&#xA;&#xA;# Inference&#xA;oumi infer -c configs/recipes/smollm/inference/135m_infer.yaml --interactive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For more advanced options, see the &lt;a href=&#34;https://oumi.ai/docs/en/latest/user_guides/train/train.html&#34;&gt;training&lt;/a&gt;, &lt;a href=&#34;https://oumi.ai/docs/en/latest/user_guides/evaluate/evaluate.html&#34;&gt;evaluation&lt;/a&gt;, &lt;a href=&#34;https://oumi.ai/docs/en/latest/user_guides/infer/infer.html&#34;&gt;inference&lt;/a&gt;, and &lt;a href=&#34;https://oumi.ai/docs/en/latest/user_guides/judge/judge.html&#34;&gt;llm-as-a-judge&lt;/a&gt; guides.&lt;/p&gt; &#xA;&lt;h3&gt;Running Jobs Remotely&lt;/h3&gt; &#xA;&lt;p&gt;You can run jobs remotely on cloud platforms (AWS, Azure, GCP, Lambda, etc.) using the &lt;code&gt;oumi launch&lt;/code&gt; command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# GCP&#xA;oumi launch up -c configs/recipes/smollm/sft/135m/quickstart_gcp_job.yaml&#xA;&#xA;# AWS&#xA;oumi launch up -c configs/recipes/smollm/sft/135m/quickstart_aws_job.yaml&#xA;&#xA;# Azure&#xA;oumi launch up -c configs/recipes/smollm/sft/135m/quickstart_azure_job.yaml&#xA;&#xA;# Lambda&#xA;oumi launch up -c configs/recipes/smollm/sft/135m/quickstart_lambda_job.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Oumi is in &lt;ins&gt;beta&lt;/ins&gt; and under active development. The core features are stable, but some advanced features might change as the platform improves.&lt;/p&gt; &#xA;&lt;h2&gt;üíª Why use Oumi?&lt;/h2&gt; &#xA;&lt;p&gt;If you need a comprehensive platform for training, evaluating, or deploying models, Oumi is a great choice.&lt;/p&gt; &#xA;&lt;p&gt;Here are some of the key features that make Oumi stand out:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üîß &lt;strong&gt;Zero Boilerplate&lt;/strong&gt;: Get started in minutes with ready-to-use recipes for popular models and workflows. No need to write training loops or data pipelines.&lt;/li&gt; &#xA; &lt;li&gt;üè¢ &lt;strong&gt;Enterprise-Grade&lt;/strong&gt;: Built and validated by teams training models at scale&lt;/li&gt; &#xA; &lt;li&gt;üéØ &lt;strong&gt;Research Ready&lt;/strong&gt;: Perfect for ML research with easily reproducible experiments, and flexible interfaces for customizing each component.&lt;/li&gt; &#xA; &lt;li&gt;üåê &lt;strong&gt;Broad Model Support&lt;/strong&gt;: Works with most popular model architectures - from tiny models to the largest ones, text-only to multimodal.&lt;/li&gt; &#xA; &lt;li&gt;üöÄ &lt;strong&gt;SOTA Performance&lt;/strong&gt;: Native support for distributed training techniques (FSDP, DDP) and optimized inference engines (vLLM, SGLang).&lt;/li&gt; &#xA; &lt;li&gt;ü§ù &lt;strong&gt;Community First&lt;/strong&gt;: 100% open source with an active community. No vendor lock-in, no strings attached.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üìö Examples &amp;amp; Recipes&lt;/h2&gt; &#xA;&lt;p&gt;Explore the growing collection of ready-to-use configurations for state-of-the-art models and training workflows:&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; These configurations are not an exhaustive list of what&#39;s supported, simply examples to get you started. You can find a more exhaustive list of supported &lt;a href=&#34;https://oumi.ai/docs/en/latest/resources/models/supported_models.html&#34;&gt;models&lt;/a&gt;, and datasets (&lt;a href=&#34;https://oumi.ai/docs/en/latest/resources/datasets/sft_datasets.html&#34;&gt;supervised fine-tuning&lt;/a&gt;, &lt;a href=&#34;https://oumi.ai/docs/en/latest/resources/datasets/pretraining_datasets.html&#34;&gt;pre-training&lt;/a&gt;, &lt;a href=&#34;https://oumi.ai/docs/en/latest/resources/datasets/preference_datasets.html&#34;&gt;preference tuning&lt;/a&gt;, and &lt;a href=&#34;https://oumi.ai/docs/en/latest/resources/datasets/vl_sft_datasets.html&#34;&gt;vision-language finetuning&lt;/a&gt;) in the oumi documentation.&lt;/p&gt; &#xA;&lt;h3&gt;üêã DeepSeek R1 Family&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Example Configurations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DeepSeek R1 671B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/inference/671b_together_infer.yaml&#34;&gt;Inference (Together AI)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Distilled Llama 8B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/sft/distill_llama_8b/full_train.yaml&#34;&gt;FFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/sft/distill_llama_8b/lora_train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/sft/distill_llama_8b/qlora_train.yaml&#34;&gt;QLoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/inference/distill_llama_8b_infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/evaluation/distill_llama_8b/eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Distilled Llama 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/sft/distill_llama_70b/full_train.yaml&#34;&gt;FFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/sft/distill_llama_70b/lora_train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/sft/distill_llama_70b/qlora_train.yaml&#34;&gt;QLoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/inference/distill_llama_70b_infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/evaluation/distill_llama_70b/eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Distilled Qwen 1.5B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/sft/distill_qwen_1_5b/full_train.yaml&#34;&gt;FFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/sft/distill_qwen_1_5b/lora_train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/inference/distill_qwen_1_5b_infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/evaluation/distill_qwen_1_5b/eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Distilled Qwen 32B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/sft/distill_qwen_32b/lora_train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/inference/distill_qwen_32b_infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/deepseek_r1/evaluation/distill_qwen_32b/eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;ü¶ô Llama Family&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Example Configurations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.1 8B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/sft/8b_full/train.yaml&#34;&gt;FFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/sft/8b_lora/train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/sft/8b_qlora/train.yaml&#34;&gt;QLoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/pretraining/8b/train.yaml&#34;&gt;Pre-training&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/inference/8b_rvllm_infer.yaml&#34;&gt;Inference (vLLM)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/inference/8b_infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/evaluation/8b_eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.1 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/sft/70b_full/train.yaml&#34;&gt;FFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/sft/70b_lora/train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/sft/70b_qlora/train.yaml&#34;&gt;QLoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/inference/70b_infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/evaluation/70b_eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.1 405B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/sft/405b_full/train.yaml&#34;&gt;FFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/sft/405b_lora/train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_1/sft/405b_qlora/train.yaml&#34;&gt;QLoRA&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.2 1B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/sft/1b_full/train.yaml&#34;&gt;FFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/sft/1b_lora/train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/sft/1b_qlora/train.yaml&#34;&gt;QLoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/inference/1b_vllm_infer.yaml&#34;&gt;Inference (vLLM)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/inference/1b_sglang_infer.yaml&#34;&gt;Inference (SGLang)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/inference/1b_infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/evaluation/1b_eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.2 3B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/sft/3b_full/train.yaml&#34;&gt;FFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/sft/3b_lora/train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/sft/3b_qlora/train.yaml&#34;&gt;QLoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/inference/3b_vllm_infer.yaml&#34;&gt;Inference (vLLM)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/inference/3b_sglang_infer.yaml&#34;&gt;Inference (SGLang)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/inference/3b_infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_2/evaluation/3b_eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.3 70B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_3/sft/70b_full/train.yaml&#34;&gt;FFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_3/sft/70b_lora/train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_3/sft/70b_qlora/train.yaml&#34;&gt;QLoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_3/inference/70b_vllm_infer.yaml&#34;&gt;Inference (vLLM)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_3/inference/70b_infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/llama3_3/evaluation/70b_eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.2 Vision 11B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llama3_2_vision/sft/11b_full/train.yaml&#34;&gt;SFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llama3_2_vision/inference/11b_rvllm_infer.yaml&#34;&gt;Inference (vLLM)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llama3_2_vision/inference/11b_sglang_infer.yaml&#34;&gt;Inference (SGLang)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llama3_2_vision/evaluation/11b_eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;üé® Vision Models&lt;/h3&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Example Configurations&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Llama 3.2 Vision 11B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llama3_2_vision/sft/11b_full/train.yaml&#34;&gt;SFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llama3_2_vision/sft/11b_lora/train.yaml&#34;&gt;LoRA&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llama3_2_vision/inference/11b_rvllm_infer.yaml&#34;&gt;Inference (vLLM)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llama3_2_vision/inference/11b_sglang_infer.yaml&#34;&gt;Inference (SGLang)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llama3_2_vision/evaluation/11b_eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;LLaVA 7B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llava_7b/sft/train.yaml&#34;&gt;SFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llava_7b/inference/vllm_infer.yaml&#34;&gt;Inference (vLLM)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/llava_7b/inference/infer.yaml&#34;&gt;Inference&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Phi3 Vision 4.2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/phi3/sft/train.yaml&#34;&gt;SFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/phi3/inference/vllm_infer.yaml&#34;&gt;Inference (vLLM)&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Qwen2-VL 2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/qwen2_vl_2b/sft/train.yaml&#34;&gt;SFT&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/qwen2_vl_2b/inference/vllm_infer.yaml&#34;&gt;Inference (vLLM)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/qwen2_vl_2b/inference/sglang_infer.yaml&#34;&gt;Inference (SGLang)&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/qwen2_vl_2b/inference/infer.yaml&#34;&gt;Inference&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/qwen2_vl_2b/evaluation/eval.yaml&#34;&gt;Evaluation&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SmolVLM-Instruct 2B&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes/vision/smolvlm/sft/gcp_job.yaml&#34;&gt;SFT&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;üîç Even more options&lt;/h3&gt; &#xA;&lt;p&gt;This section lists all the language models that can be used with Oumi. Thanks to the integration with the &lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;ü§ó Transformers&lt;/a&gt; library, you can easily use any of these models for training, evaluation, or inference.&lt;/p&gt; &#xA;&lt;p&gt;Models prefixed with a checkmark (‚úÖ) have been thoroughly tested and validated by the Oumi community, with ready-to-use recipes available in the &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/configs/recipes&#34;&gt;configs/recipes&lt;/a&gt; directory.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üìã Click to see more supported models&lt;/summary&gt; &#xA; &lt;h4&gt;Instruct Models&lt;/h4&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;Paper&lt;/th&gt; &#xA;    &lt;th&gt;HF Hub&lt;/th&gt; &#xA;    &lt;th&gt;License&lt;/th&gt; &#xA;    &lt;th&gt;Open [^1]&lt;/th&gt; &#xA;    &lt;th&gt;Recommended Parameters&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ SmolLM-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;135M/360M/1.7B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/blog/smollm&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ DeepSeek R1 Family&lt;/td&gt; &#xA;    &lt;td&gt;1.5B/8B/32B/70B/671B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://api-docs.deepseek.com/news/news250120&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/DeepSeek-R1&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;MIT&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Llama 3.1 Instruct&lt;/td&gt; &#xA;    &lt;td&gt;8B/70B/405B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.21783&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-3.1-70b-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Llama 3.2 Instruct&lt;/td&gt; &#xA;    &lt;td&gt;1B/3B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.21783&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-3.2-3b-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Llama 3.3 Instruct&lt;/td&gt; &#xA;    &lt;td&gt;70B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.21783&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-3.3-70b-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Phi-3.5-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;4B/14B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.14219&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft/Phi-3.5-mini-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Qwen2.5-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;0.5B-70B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.16609&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-7B-Instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OLMo 2 Instruct&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.00838&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-2-1124-7B&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MPT-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.mosaicml.com/blog/mpt-7b&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Command R&lt;/td&gt; &#xA;    &lt;td&gt;35B/104B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://cohere.com/blog/command-r7b&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/CohereForAI/c4ai-command-r-plus&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://cohere.com/c4ai-cc-by-nc-license&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Granite-3.1-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;2B/8B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/ibm-granite/granite-3.0-language-models/raw/main/paper.pdf&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/ibm-granite/granite-3.1-8b-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Gemma 2 Instruct&lt;/td&gt; &#xA;    &lt;td&gt;2B/9B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://ai.google.dev/gemma&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google/gemma-2-2b-it&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://ai.google.dev/gemma/terms&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;DBRX-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;130B MoE&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/databricks/dbrx-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Falcon-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;7B/40B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.01116&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/tiiuae/falcon-7b-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h4&gt;Vision-Language Models&lt;/h4&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;Paper&lt;/th&gt; &#xA;    &lt;th&gt;HF Hub&lt;/th&gt; &#xA;    &lt;th&gt;License&lt;/th&gt; &#xA;    &lt;th&gt;Open&lt;/th&gt; &#xA;    &lt;th&gt;Recommended Parameters&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Llama 3.2 Vision&lt;/td&gt; &#xA;    &lt;td&gt;11B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.21783&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-3.2-11b-vision&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ LLaVA-1.5&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.03744&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/llava-hf/llava-1.5-7b-hf&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://ai.meta.com/llama/license&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Phi-3 Vision&lt;/td&gt; &#xA;    &lt;td&gt;4.2B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.14219&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft/Phi-3-vision-128k-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ BLIP-2&lt;/td&gt; &#xA;    &lt;td&gt;3.6B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.12597&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Salesforce/blip2-opt-2.7b&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;MIT&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Qwen2-VL&lt;/td&gt; &#xA;    &lt;td&gt;2B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2-vl/&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ SmolVLM-Instruct&lt;/td&gt; &#xA;    &lt;td&gt;2B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/blog/smolvlm&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h4&gt;Base Models&lt;/h4&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;Paper&lt;/th&gt; &#xA;    &lt;th&gt;HF Hub&lt;/th&gt; &#xA;    &lt;th&gt;License&lt;/th&gt; &#xA;    &lt;th&gt;Open&lt;/th&gt; &#xA;    &lt;th&gt;Recommended Parameters&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ SmolLM2&lt;/td&gt; &#xA;    &lt;td&gt;135M/360M/1.7B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/blog/smollm&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/HuggingFaceTB/SmolLM2-135M&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Llama 3.2&lt;/td&gt; &#xA;    &lt;td&gt;1B/3B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.21783&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-3.2-3b&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Llama 3.1&lt;/td&gt; &#xA;    &lt;td&gt;8B/70B/405B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.21783&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-3.1-70b&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://llama.meta.com/llama3/license/&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ GPT-2&lt;/td&gt; &#xA;    &lt;td&gt;124M-1.5B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/gpt2&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;MIT&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;DeepSeek V2&lt;/td&gt; &#xA;    &lt;td&gt;7B/13B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.deepseek.com/blogs/deepseek-v2&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-llm-7b-v2&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Gemma2&lt;/td&gt; &#xA;    &lt;td&gt;2B/9B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://ai.google.dev/gemma&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google/gemma2-7b&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://ai.google.dev/gemma/terms&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;GPT-J&lt;/td&gt; &#xA;    &lt;td&gt;6B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.eleuther.ai/artifacts/gpt-j&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6b&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;GPT-NeoX&lt;/td&gt; &#xA;    &lt;td&gt;20B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.06745&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-neox-20b&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mistral&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.06825&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-v0.1&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Mixtral&lt;/td&gt; &#xA;    &lt;td&gt;8x7B/8x22B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://mistral.ai/news/mixtral-of-experts/&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mistralai/Mixtral-8x7B-v0.1&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;MPT&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://www.mosaicml.com/blog/mpt-7b&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;OLMo&lt;/td&gt; &#xA;    &lt;td&gt;1B/7B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.00838&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/allenai/OLMo-7B-hf&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h4&gt;Reasoning Models&lt;/h4&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;Paper&lt;/th&gt; &#xA;    &lt;th&gt;HF Hub&lt;/th&gt; &#xA;    &lt;th&gt;License&lt;/th&gt; &#xA;    &lt;th&gt;Open&lt;/th&gt; &#xA;    &lt;th&gt;Recommended Parameters&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;Qwen QwQ&lt;/td&gt; &#xA;    &lt;td&gt;32B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwq-32b-preview/&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen/QwQ-32B-Preview&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h4&gt;Code Models&lt;/h4&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;Paper&lt;/th&gt; &#xA;    &lt;th&gt;HF Hub&lt;/th&gt; &#xA;    &lt;th&gt;License&lt;/th&gt; &#xA;    &lt;th&gt;Open&lt;/th&gt; &#xA;    &lt;th&gt;Recommended Parameters&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;‚úÖ Qwen2.5 Coder&lt;/td&gt; &#xA;    &lt;td&gt;0.5B-32B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://qwenlm.github.io/blog/qwen2.5/&#34;&gt;Blog&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen/raw/main/LICENSE&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;DeepSeek Coder&lt;/td&gt; &#xA;    &lt;td&gt;1.3B-33B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.02954&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;StarCoder 2&lt;/td&gt; &#xA;    &lt;td&gt;3B/7B/15B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2402.19173&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoder2-15b&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA; &lt;h4&gt;Math Models&lt;/h4&gt; &#xA; &lt;table&gt; &#xA;  &lt;thead&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;th&gt;Model&lt;/th&gt; &#xA;    &lt;th&gt;Size&lt;/th&gt; &#xA;    &lt;th&gt;Paper&lt;/th&gt; &#xA;    &lt;th&gt;HF Hub&lt;/th&gt; &#xA;    &lt;th&gt;License&lt;/th&gt; &#xA;    &lt;th&gt;Open&lt;/th&gt; &#xA;    &lt;th&gt;Recommended Parameters&lt;/th&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/thead&gt; &#xA;  &lt;tbody&gt; &#xA;   &lt;tr&gt; &#xA;    &lt;td&gt;DeepSeek Math&lt;/td&gt; &#xA;    &lt;td&gt;7B&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2401.02954&#34;&gt;Paper&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://huggingface.co/deepseek-ai/deepseek-math-7b-instruct&#34;&gt;Hub&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;&lt;a href=&#34;https://github.com/deepseek-ai/DeepSeek-LLM/raw/main/LICENSE-MODEL&#34;&gt;License&lt;/a&gt;&lt;/td&gt; &#xA;    &lt;td&gt;‚ùå&lt;/td&gt; &#xA;    &lt;td&gt;&lt;/td&gt; &#xA;   &lt;/tr&gt; &#xA;  &lt;/tbody&gt; &#xA; &lt;/table&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üìñ Documentation&lt;/h2&gt; &#xA;&lt;p&gt;To learn more about all the platform&#39;s capabilities, see the &lt;a href=&#34;https://oumi.ai/docs&#34;&gt;Oumi documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;ü§ù Join the Community!&lt;/h2&gt; &#xA;&lt;p&gt;Oumi is a community-first effort. Whether you are a developer, a researcher, or a non-technical user, all contributions are very welcome!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To contribute to the &lt;code&gt;oumi&lt;/code&gt; repository, please check the &lt;a href=&#34;https://github.com/oumi-ai/oumi/raw/main/CONTRIBUTING.md&#34;&gt;&lt;code&gt;CONTRIBUTING.md&lt;/code&gt;&lt;/a&gt; for guidance on how to contribute to send your first Pull Request.&lt;/li&gt; &#xA; &lt;li&gt;Make sure to join our &lt;a href=&#34;https://discord.gg/oumi&#34;&gt;Discord community&lt;/a&gt; to get help, share your experiences, and contribute to the project!&lt;/li&gt; &#xA; &lt;li&gt;If you are interested in joining one of the community&#39;s open-science efforts, check out our &lt;a href=&#34;https://oumi.ai/community&#34;&gt;open collaboration&lt;/a&gt; page.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üôè Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;Oumi makes use of &lt;a href=&#34;https://oumi.ai/docs/en/latest/about/acknowledgements.html&#34;&gt;several libraries&lt;/a&gt; and tools from the open-source community. We would like to acknowledge and deeply thank the contributors of these projects! ‚ú® üåü üí´&lt;/p&gt; &#xA;&lt;h2&gt;üìù Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find Oumi useful in your research, please consider citing it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{oumi2025,&#xA;  author = {Oumi Community},&#xA;  title = {Oumi: an Open, End-to-end Platform for Building Large Foundation Models},&#xA;  month = {January},&#xA;  year = {2025},&#xA;  url = {https://github.com/oumi-ai/oumi}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üìú License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the Apache License 2.0. See the &lt;a href=&#34;https://raw.githubusercontent.com/oumi-ai/oumi/main/LICENSE&#34;&gt;LICENSE&lt;/a&gt; file for details.&lt;/p&gt; &#xA;&lt;p&gt;[^1]: Open models are defined as models with fully open weights, training code, and data, and a permissive license. See &lt;a href=&#34;https://opensource.org/ai&#34;&gt;Open Source Definitions&lt;/a&gt; for more information.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>google/meridian</title>
    <updated>2025-02-02T01:28:37Z</updated>
    <id>tag:github.com,2025-02-02:/google/meridian</id>
    <link href="https://github.com/google/meridian" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Meridian is an MMM framework that enables advertisers to set up and run their own in-house models.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;About Meridian&lt;/h1&gt; &#xA;&lt;p&gt;Marketing mix modeling (MMM) is a statistical analysis technique that measures the impact of marketing campaigns and activities to guide budget planning decisions and improve overall media effectiveness. MMM uses aggregated data to measure impact across marketing channels and account for non-marketing factors that impact sales and other key performance indicators (KPIs). MMM is privacy-safe and does not use any cookie or user-level information.&lt;/p&gt; &#xA;&lt;p&gt;Meridian is an MMM framework that enables advertisers to set up and run their own in-house models. Meridian helps you answer key questions such as:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;How did the marketing channels drive my revenue or other KPI?&lt;/li&gt; &#xA; &lt;li&gt;What was my marketing return on investment (ROI)?&lt;/li&gt; &#xA; &lt;li&gt;How do I optimize my marketing budget allocation for the future?&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Meridian is a highly customizable modeling framework that is based on &lt;a href=&#34;https://developers.google.com/meridian/docs/basics/bayesian-inference&#34;&gt;Bayesian causal inference&lt;/a&gt;. It is capable of handling large scale geo-level data, which is encouraged if available, but it can also be used for national-level modeling. Meridian provides clear insights and visualizations to inform business decisions around marketing budget and planning. Additionally, Meridian provides methodologies to support calibration of MMM with experiments and other prior information, and to optimize target ad frequency by utilizing reach and frequency data.&lt;/p&gt; &#xA;&lt;p&gt;If you are using LightweightMMM, see the &lt;a href=&#34;https://developers.google.com/meridian/docs/migrate&#34;&gt;migration guide&lt;/a&gt; to help you understand the differences between these MMM projects.&lt;/p&gt; &#xA;&lt;h1&gt;Install Meridian&lt;/h1&gt; &#xA;&lt;p&gt;Python 3.11 or 3.12 is required to use Meridian. We also recommend using a minimum of 1 GPU.&lt;/p&gt; &#xA;&lt;p&gt;Note: This project has been tested on V100 and T4 GPU using 16 GB of RAM.&lt;/p&gt; &#xA;&lt;p&gt;To install Meridian, run the following command to automatically install the latest release from PyPI.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install --upgrade google-meridian&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Alternatively, run the following command to install the most recent, unreleased version from GitHub.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;$ pip install --upgrade git+https://github.com/google/meridian.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We recommend to install Meridian in a fresh &lt;a href=&#34;https://virtualenv.pypa.io/en/latest/user_guide.html#quick-start&#34;&gt;virtual environment&lt;/a&gt; to make sure that correct versions of all the dependencies are installed, as defined in &lt;a href=&#34;https://github.com/google/meridian/raw/main/pyproject.toml&#34;&gt;pyproject.toml&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;How to use the Meridian library&lt;/h2&gt; &#xA;&lt;p&gt;To get started with Meridian, you can run the code programmatically using sample data with the &lt;a href=&#34;https://developers.google.com/meridian/notebook/meridian-getting-started&#34;&gt;Getting Started Colab&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;The Meridian model uses a holistic MCMC sampling approach called &lt;a href=&#34;https://www.tensorflow.org/probability/api_docs/python/tfp/experimental/mcmc/NoUTurnSampler&#34;&gt;No U Turn Sampler (NUTS)&lt;/a&gt; which can be compute intensive. To help with this, GPU support has been developed across the library (out-of-the-box) using tensors. We recommend running your Meridian model on GPUs to get real time optimization results and significantly reduce training time.&lt;/p&gt; &#xA;&lt;h1&gt;Meridian Documentation &amp;amp; Tutorials&lt;/h1&gt; &#xA;&lt;p&gt;The following documentation, colab, and video resources will help you get started quickly with using Meridian:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Resource&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian&#34;&gt;Meridian documentation&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Main landing page for Meridian documentation.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/basics/about-the-project&#34;&gt;Meridian basics&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn about Meridian features, methodologies, and the model math.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/notebook/meridian-getting-started&#34;&gt;Getting started colab&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Install and quickly learn how to use Meridian with this colab tutorial using sample data.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/user-guide/installing&#34;&gt;User guide&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;A detailed walk-through of how to use Meridian and generating visualizations using your own data.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/user-guide/collect-data&#34;&gt;Pre-modeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Prepare and analyze your data before modeling.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/advanced-modeling/control-variables&#34;&gt;Modeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Modeling guidance for model refinement and edge cases.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/advanced-modeling/model-fit&#34;&gt;Post-modeling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Post-modeling guidance for model fit, visualizations, optimizations, refreshing the model, and debugging.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/migrate&#34;&gt;Migrate from LMMM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Learn about the differences between Meridian and LightweightMMM as you consider migrating.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/reference/api/meridian&#34;&gt;API Reference&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;API reference documentation for the Meridian package.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://developers.google.com/meridian/docs/reference-list&#34;&gt;Reference list&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;White papers and other referenced material.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Support&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Questions about methodology&lt;/strong&gt;: Please see the &lt;a href=&#34;https://developers.google.com/meridian/docs/basics/about-the-project&#34;&gt;Modeling&lt;/a&gt; tab in the technical documentation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Issues installing or using Meridian&lt;/strong&gt;: Feel free to post questions in the &lt;a href=&#34;https://github.com/google/meridian/discussions&#34;&gt;Discussions&lt;/a&gt; or &lt;a href=&#34;https://github.com/google/meridian/issues&#34;&gt;Issues&lt;/a&gt; tabs of the Meridian GitHub repository. The Meridian team responds to these questions weekly in batches, so please be patient and don&#39;t reach out directly to your Google Account teams.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Bug reports&lt;/strong&gt;: Please post bug reports to the &lt;a href=&#34;https://github.com/google/meridian/issues&#34;&gt;Issues&lt;/a&gt; tab of the Meridian GitHub repository. We also encourage the community to share tips and advice with each other on the &lt;a href=&#34;https://github.com/google/meridian/issues&#34;&gt;Issues&lt;/a&gt; tab. When our team addresses or resolves a new bug, we will notify you through the comments on the issue.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Feature requests&lt;/strong&gt;: Please post these to the &lt;a href=&#34;https://github.com/google/meridian/discussions&#34;&gt;Discussions&lt;/a&gt; tab of the Meridian GitHub repository. We have an internal roadmap for Meridian development, but would love your inputs for new feature requests so that we can prioritize them based on the roadmap.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Pull requests&lt;/strong&gt;: These are appreciated but are very difficult for us to merge because the code in this repository is linked to Google internal systems and has to pass internal review. If you submit a pull request and we believe that we can incorporate a change in the base code, we will reach out to you directly about this.&lt;/p&gt; &#xA;&lt;h2&gt;Citing Meridian&lt;/h2&gt; &#xA;&lt;p&gt;To cite this repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@software{meridian_github,&#xA;  author = {Google Meridian Marketing Mix Modeling Team},&#xA;  title = {Meridian: Marketing Mix Modeling},&#xA;  url = {https://github.com/google/meridian},&#xA;  version = {1.0.0},&#xA;  year = {2025},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>containers/ramalama</title>
    <updated>2025-02-02T01:28:37Z</updated>
    <id>tag:github.com,2025-02-02:/containers/ramalama</id>
    <link href="https://github.com/containers/ramalama" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The goal of RamaLama is to make working with AI boring.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/containers/ramalama/main/logos/PNG/ramalama-logo-full-vertical-added-bg.png&#34; alt=&#34;RAMALAMA logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;RamaLama&lt;/h1&gt; &#xA;&lt;p&gt;The RamaLama project&#39;s goal is to make working with AI boring through the use of OCI containers.&lt;/p&gt; &#xA;&lt;p&gt;RamaLama tool facilitates local management and serving of AI Models.&lt;/p&gt; &#xA;&lt;p&gt;On first run RamaLama inspects your system for GPU support, falling back to CPU support if no GPUs are present.&lt;/p&gt; &#xA;&lt;p&gt;RamaLama uses container engines like Podman or Docker to pull the appropriate OCI image with all of the software necessary to run an AI Model for your systems setup.&lt;/p&gt; &#xA;&lt;p&gt;Running in containers eliminates the need for users to configure the host system for AI. After the initialization, RamaLama runs the AI Models within a container based on the OCI image.&lt;/p&gt; &#xA;&lt;p&gt;RamaLama then pulls AI Models from model registries. Starting a chatbot or a rest API service from a simple single command. Models are treated similarly to how Podman and Docker treat container images.&lt;/p&gt; &#xA;&lt;p&gt;When both Podman and Docker are installed, RamaLama defaults to Podman, The &lt;code&gt;RAMALAMA_CONTAINER_ENGINE=docker&lt;/code&gt; environment variable can override this behaviour. When neither are installed RamaLama will attempt to run the model with software on the local system.&lt;/p&gt; &#xA;&lt;p&gt;RamaLama supports multiple AI model registries types called transports. Supported transports:&lt;/p&gt; &#xA;&lt;h2&gt;TRANSPORTS&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Transports&lt;/th&gt; &#xA;   &lt;th&gt;Web Site&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;HuggingFace&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.huggingface.co&#34;&gt;&lt;code&gt;huggingface.co&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Ollama&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.ollama.com&#34;&gt;&lt;code&gt;ollama.com&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OCI Container Registries&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://opencontainers.org&#34;&gt;&lt;code&gt;opencontainers.org&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Examples: &lt;a href=&#34;https://quay.io&#34;&gt;&lt;code&gt;quay.io&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://docker.io&#34;&gt;&lt;code&gt;Docker Hub&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://artifactory.com&#34;&gt;&lt;code&gt;Artifactory&lt;/code&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;RamaLama uses the Ollama registry transport by default. Use the RAMALAMA_TRANSPORTS environment variable to modify the default. &lt;code&gt;export RAMALAMA_TRANSPORT=huggingface&lt;/code&gt; Changes RamaLama to use huggingface transport.&lt;/p&gt; &#xA;&lt;p&gt;Individual model transports can be modifies when specifying a model via the &lt;code&gt;huggingface://&lt;/code&gt;, &lt;code&gt;oci://&lt;/code&gt;, or &lt;code&gt;ollama://&lt;/code&gt; prefix.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;ramalama pull huggingface://afrideva/Tiny-Vicuna-1B-GGUF/tiny-vicuna-1b.q2_k.gguf&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;To make it easier for users, RamaLama uses shortname files, which container alias names for fully specified AI Models allowing users to specify the shorter names when referring to models. RamaLama reads shortnames.conf files if they exist . These files contain a list of name value pairs for specification of the model. The following table specifies the order which RamaLama reads the files . Any duplicate names that exist override previously defined shortnames.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Shortnames type&lt;/th&gt; &#xA;   &lt;th&gt;Path&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Distribution&lt;/td&gt; &#xA;   &lt;td&gt;/usr/share/ramalama/shortnames.conf&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Administrators&lt;/td&gt; &#xA;   &lt;td&gt;/etc/ramamala/shortnames.conf&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Users&lt;/td&gt; &#xA;   &lt;td&gt;$HOME/.config/ramalama/shortnames.conf&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-code&#34;&gt;$ cat /usr/share/ramalama/shortnames.conf&#xA;[shortnames]&#xA;  &#34;tiny&#34; = &#34;ollama://tinyllama&#34;&#xA;  &#34;granite&#34; = &#34;huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf&#34;&#xA;  &#34;granite:7b&#34; = &#34;huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf&#34;&#xA;  &#34;ibm/granite&#34; = &#34;huggingface://instructlab/granite-7b-lab-GGUF/granite-7b-lab-Q4_K_M.gguf&#34;&#xA;  &#34;merlinite&#34; = &#34;huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf&#34;&#xA;  &#34;merlinite:7b&#34; = &#34;huggingface://instructlab/merlinite-7b-lab-GGUF/merlinite-7b-lab-Q4_K_M.gguf&#34;&#xA;...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h2&gt;Install via PyPi&lt;/h2&gt; &#xA;&lt;p&gt;RamaLama is available via PyPi &lt;a href=&#34;https://pypi.org/project/ramalama&#34;&gt;https://pypi.org/project/ramalama&lt;/a&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install ramalama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Install by script&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!TIP] If you are a macOS user, this is the preferred method.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;Install RamaLama by running this one-liner:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -fsSL https://raw.githubusercontent.com/containers/ramalama/s/install.sh | bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Hardware Support&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Hardware&lt;/th&gt; &#xA;   &lt;th&gt;Enabled&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CPU&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Apple Silicon GPU (Linux / Asahi)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Apple Silicon GPU (macOS)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Apple Silicon GPU (podman-machine)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Nvidia GPU (cuda)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AMD GPU (rocm)&lt;/td&gt; &#xA;   &lt;td&gt;&lt;span&gt;‚úÖ&lt;/span&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;COMMANDS&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Command&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama.1.md&#34;&gt;ramalama(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;primary RamaLama man page&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-containers.1.md&#34;&gt;ramalama-containers(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;list all RamaLama containers&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-info.1.md&#34;&gt;ramalama-info(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;display RamaLama configuration information&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-list.1.md&#34;&gt;ramalama-list(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;list all downloaded AI Models&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-login.1.md&#34;&gt;ramalama-login(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;login to remote registry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-logout.1.md&#34;&gt;ramalama-logout(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;logout from remote registry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-pull.1.md&#34;&gt;ramalama-pull(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;pull AI Model from Model registry to local storage&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-push.1.md&#34;&gt;ramalama-push(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;push AI Model from local storage to remote registry&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-rm.1.md&#34;&gt;ramalama-rm(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;remove AI Model from local storage&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-run.1.md&#34;&gt;ramalama-run(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;run specified AI Model as a chatbot&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-serve.1.md&#34;&gt;ramalama-serve(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;serve REST API on specified AI Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-stop.1.md&#34;&gt;ramalama-stop(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;stop named container that is running AI Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/containers/ramalama/raw/main/docs/ramalama-version.1.md&#34;&gt;ramalama-version(1)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;display version of AI Model&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Running Models&lt;/h3&gt; &#xA;&lt;p&gt;You can &lt;code&gt;run&lt;/code&gt; a chatbot on a model using the &lt;code&gt;run&lt;/code&gt; command. By default, it pulls from the Ollama registry.&lt;/p&gt; &#xA;&lt;p&gt;Note: RamaLama will inspect your machine for native GPU support and then will use a container engine like Podman to pull an OCI container image with the appropriate code and libraries to run the AI Model. This can take a long time to setup, but only on the first run.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama run instructlab/merlinite-7b-lab&#xA;Copying blob 5448ec8c0696 [--------------------------------------] 0.0b / 63.6MiB (skipped: 0.0b = 0.00%)&#xA;Copying blob cbd7e392a514 [--------------------------------------] 0.0b / 65.3MiB (skipped: 0.0b = 0.00%)&#xA;Copying blob 5d6c72bcd967 done  208.5MiB / 208.5MiB (skipped: 0.0b = 0.00%)&#xA;Copying blob 9ccfa45da380 [--------------------------------------] 0.0b / 7.6MiB (skipped: 0.0b = 0.00%)&#xA;Copying blob 4472627772b1 [--------------------------------------] 0.0b / 120.0b (skipped: 0.0b = 0.00%)&#xA;&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After the initial container image has been downloaded, you can interact with different models, using the container image.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama run granite3-moe&#xA;&amp;gt; Write a hello world application in python&#xA;&#xA;print(&#34;Hello World&#34;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In a different terminal window see the running podman container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ podman ps&#xA;CONTAINER ID  IMAGE                             COMMAND               CREATED        STATUS        PORTS       NAMES&#xA;91df4a39a360  quay.io/ramalama/ramalama:latest  /home/dwalsh/rama...  4 minutes ago  Up 4 minutes              gifted_volhard&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Listing Models&lt;/h3&gt; &#xA;&lt;p&gt;You can &lt;code&gt;list&lt;/code&gt; all models pulled into local storage.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama list&#xA;NAME                                                                MODIFIED     SIZE&#xA;ollama://smollm:135m                                                16 hours ago 5.5M&#xA;huggingface://afrideva/Tiny-Vicuna-1B-GGUF/tiny-vicuna-1b.q2_k.gguf 14 hours ago 460M&#xA;ollama://moondream:latest                                           6 days ago   791M&#xA;ollama://phi4:latest                                                6 days ago   8.43 GB&#xA;ollama://tinyllama:latest                                           1 week ago   608.16 MB&#xA;ollama://granite3-moe:3b                                            1 week ago   1.92 GB&#xA;ollama://granite3-moe:latest                                        3 months ago 1.92 GB&#xA;ollama://llama3.1:8b                                                2 months ago 4.34 GB&#xA;ollama://llama3.1:latest                                            2 months ago 4.34 GB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Pulling Models&lt;/h3&gt; &#xA;&lt;p&gt;You can &lt;code&gt;pull&lt;/code&gt; a model using the &lt;code&gt;pull&lt;/code&gt; command. By default, it pulls from the Ollama registry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama pull granite3-moe&#xA; 31% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    |  250.11 MB/ 783.77 MB  36.95 MB/s       14s&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Serving Models&lt;/h3&gt; &#xA;&lt;p&gt;You can &lt;code&gt;serve&lt;/code&gt; multiple models using the &lt;code&gt;serve&lt;/code&gt; command. By default, it pulls from the Ollama registry.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama serve --name mylama llama3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Stopping servers&lt;/h3&gt; &#xA;&lt;p&gt;You can stop a running model if it is running in a container.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;$ ramalama stop mylama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;UI support&lt;/h3&gt; &#xA;&lt;p&gt;To use a UI, run a &lt;code&gt;ramalama serve&lt;/code&gt; command, then connect via your browser at:&lt;/p&gt; &#xA;&lt;p&gt;127.0.0.1:8080&lt;/p&gt; &#xA;&lt;h2&gt;Diagram&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;+---------------------------+&#xA;|                           |&#xA;| ramalama run granite3-moe |&#xA;|                           |&#xA;+-------+-------------------+&#xA;&#x9;|&#xA;&#x9;|&#xA;        |           +------------------+           +------------------+&#xA;        |           | Pull inferencing |           | Pull model layer |&#xA;        +-----------| runtime (cuda)   |----------&amp;gt;| granite3-moe     |&#xA;                    +------------------+           +------------------+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | Repo options:    |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   +-+-------+------+-+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;     |       |      |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;     v       v      v&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;     +---------+ +------+ +----------+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;     | Hugging | | OCI  | | Ollama   |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;     | Face    | |      | | Registry |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;     +-------+-+ +---+--+ +-+--------+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;     |       |      |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;     v       v      v&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   +------------------+&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | Start with       |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | cuda runtime     |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | and              |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   | granite3-moe     |&#xA;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;   +------------------+&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;In development&lt;/h2&gt; &#xA;&lt;p&gt;Regard this alpha, everything is under development, so expect breaking changes, luckily it&#39;s easy to reset everything and re-install:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;rm -rf /var/lib/ramalama # only required if running as root user&#xA;rm -rf $HOME/.local/share/ramalama&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;and install again.&lt;/p&gt; &#xA;&lt;h2&gt;Credit where credit is due&lt;/h2&gt; &#xA;&lt;p&gt;This project wouldn&#39;t be possible without the help of other projects like:&lt;/p&gt; &#xA;&lt;p&gt;llama.cpp&lt;br&gt; whisper.cpp&lt;br&gt; vllm&lt;br&gt; podman&lt;br&gt; huggingface&lt;/p&gt; &#xA;&lt;p&gt;so if you like this tool, give some of these repos a &lt;span&gt;‚≠ê&lt;/span&gt;, and hey, give us a &lt;span&gt;‚≠ê&lt;/span&gt; too while you are at it.&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://matrix.to/#/%23ramalama:fedoraproject.org&#34;&gt;&lt;code&gt;Matrix&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;p&gt;Open to contributors&lt;/p&gt; &#xA;&lt;a href=&#34;https://github.com/containers/ramalama/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=containers/ramalama&#34;&gt; &lt;/a&gt;</summary>
  </entry>
</feed>