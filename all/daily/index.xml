<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-08-06T01:26:37Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>guochengqian/Magic123</title>
    <updated>2023-08-06T01:26:37Z</updated>
    <id>tag:github.com,2023-08-06:/guochengqian/Magic123</id>
    <link href="https://github.com/guochengqian/Magic123" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official PyTorch Implementation of Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2306.17843&#34;&gt;arXiv&lt;/a&gt; | &lt;a href=&#34;https://guochengqian.github.io/project/magic123/&#34;&gt;webpage&lt;/a&gt;&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/guochengqian/Magic123/main/docs/static/magic123.gif&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://guochengqian.github.io/&#34;&gt;Guocheng Qian&lt;/a&gt; &lt;sup&gt;1,2&lt;/sup&gt;, &lt;a href=&#34;https://cemse.kaust.edu.sa/people/person/jinjie-mai&#34;&gt;Jinjie Mai&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;https://abdullahamdi.com/&#34;&gt;Abdullah Hamdi&lt;/a&gt; &lt;sup&gt;3&lt;/sup&gt;, &lt;a href=&#34;https://alanspike.github.io/&#34;&gt;Jian Ren&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://aliaksandrsiarohin.github.io/aliaksandr-siarohin-website/&#34;&gt;Aliaksandr Siarohin&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://cemse.kaust.edu.sa/people/person/bing-li&#34;&gt;Bing Li&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://hsinyinglee.com/&#34;&gt;Hsin-Ying Lee&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://universome.github.io/&#34;&gt;Ivan Skorokhodov&lt;/a&gt; &lt;sup&gt;1,2&lt;/sup&gt;, &lt;a href=&#34;https://peterwonka.net/&#34;&gt;Peter Wonka&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;, &lt;a href=&#34;http://www.stulyakov.com/&#34;&gt;Sergey Tulyakov&lt;/a&gt; &lt;sup&gt;2&lt;/sup&gt;, &lt;a href=&#34;https://www.bernardghanem.com/&#34;&gt;Bernard Ghanem&lt;/a&gt; &lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;a href=&#34;https://www.kaust.edu.sa/&#34;&gt;King Abdullah University of Science and Technology (KAUST)&lt;/a&gt;, &lt;sup&gt;2&lt;/sup&gt; &lt;a href=&#34;https://www.snap.com/&#34;&gt;Snap Inc.&lt;/a&gt;, &lt;sup&gt;3&lt;/sup&gt; &lt;a href=&#34;http://www.robots.ox.ac.uk/~vgg/&#34;&gt;Visual Geometry Group, University of Oxford&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Training convergence of a demo example: &lt;img src=&#34;https://raw.githubusercontent.com/guochengqian/Magic123/main/docs/static/ironman-val-magic123.gif&#34; width=&#34;800&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Compare Magic123 without textual inversion with abaltions using only 2D prior (SDS) or using only 3D prior (Zero123):&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/guochengqian/Magic123/assets/48788073/e5a3c3cb-bcb1-4b10-8bfb-2c2eb79a9289&#34;&gt;https://github.com/guochengqian/Magic123/assets/48788073/e5a3c3cb-bcb1-4b10-8bfb-2c2eb79a9289&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Effects of Joint Prior. Increasing the strength of 2D prior leads to more imagination, more details, and less 3D consistencies.&lt;/p&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/guochengqian/Magic123/main/docs/static/2d_3d.png&#34; width=&#34;800&#34;&gt; &#xA;&lt;p&gt;Official PyTorch Implementation of Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors. Code is built upon &lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;Stable-DreamFusion&lt;/a&gt; repo.&lt;/p&gt; &#xA;&lt;h1&gt;NEWS:&lt;/h1&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[2023/07/25] Code is available at &lt;a href=&#34;https://github.com/guochengqian/Magic123&#34;&gt;GitHub&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023/07/03] Paper is available at &lt;a href=&#34;https://arxiv.org/abs/2306.17843&#34;&gt;arXiv&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023/06/25] Much better performance than the submitted version is achieved by 1）reimplementing Magic123 using &lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;Stable DreamFusion code&lt;/a&gt;, 2）fixing some gradient issues, 3）leveraging the &lt;a href=&#34;https://raw.githubusercontent.com/guochengqian/Magic123/main/#tips-and-tricks&#34;&gt;tricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;[2023] Initial version of Magic123 submitted to conference&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Install&lt;/h1&gt; &#xA;&lt;h3&gt;Install Environment&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;source install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Download pre-trained models&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;Zero-1-to-3&lt;/a&gt; for 3D diffusion prior. We use &lt;code&gt;105000.ckpt&lt;/code&gt; by default, reimplementation borrowed from Stable Diffusion repo, and is available in &lt;code&gt;guidance/zero123_utils.py&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd pretrained/zero123&#xA;wget https://huggingface.co/cvlab/zero123-weights/resolve/main/105000.ckpt&#xA;cd .../../&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/isl-org/MiDaS&#34;&gt;MiDaS&lt;/a&gt; for depth estimation. We use &lt;code&gt;dpt_beit_large_512.pt&lt;/code&gt;. Put it in folder &lt;code&gt;pretrained/midas/&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;mkdir -p pretrained/midas&#xA;cd pretrained/midas&#xA;wget https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt&#xA;cd ../../&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Usage&lt;/h1&gt; &#xA;&lt;h2&gt;Preprocess [Optional]&lt;/h2&gt; &#xA;&lt;p&gt;We have included all preprocessed files in &lt;code&gt;./data&lt;/code&gt; directory. Preprocessing is only necessary if you want to test on your own examples.&lt;/p&gt; &#xA;&lt;h3&gt;Step1: Extract depth&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python preprocess_image.py --path /path/to/image &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Step 2: Textural inversion [Optional]&lt;/h3&gt; &#xA;&lt;p&gt;Magic123 uses the defualt &lt;a href=&#34;https://huggingface.co/docs/diffusers/training/text_inversion&#34;&gt;textural inversion&lt;/a&gt; from diffuers, which consumes around 2.5 hours on a 32G V100. If you do not want to spend time in this textural inversion, you can: (1) study whether there is other faster textural inversion; or (2) do not use textural inversion in the loss of texture and shape consistencies. To run textural inversion:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bash scripts/texural_inversion/textural_inversion.sh $GPU_IDX runwayml/stable-diffusion-v1-5 /path/to/example/rgba.png /path/to/save $token_name $init_token --max_train_steps 5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;$token_name is a the special token, usually name that by &lt;em&gt;examplename&lt;/em&gt; $init_token is a single token to describe the image using natural language&lt;/p&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/texural_inversion/textural_inversion.sh runwayml/stable-diffusion-v1-5 data/demo/ironman/rgba.png out/textual_inversion/ironman _ironman_ ironman --max_train_steps 3000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Don&#39;t forget to move the final &lt;code&gt;learned_embeds.bin&lt;/code&gt; under data/demo/ironman/&lt;/p&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;h3&gt;Run Magic123 for a single example&lt;/h3&gt; &#xA;&lt;p&gt;Takes ~40 mins for the coarse stage and ~20 mins for the second stage on a 32G V100.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/magic123/run_both_priors.sh $GPU_NO $JOBNAME_First_Stage $JOBNAME_Second_Stage $PATH_to_Example_Directory $IMAGE_BASE_NAME $Enable_First_Stage $Enable_Second_Stage {More_Arugments}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;As an example, run Magic123 in the dragon example using both stages in GPU 0 and set the jobname for the first stage as &lt;code&gt;default&lt;/code&gt; and the jobname for the second stage as &lt;code&gt;dmtet&lt;/code&gt;, by the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/magic123/run_both_priors.sh 0 default dmtet data/realfusion15/metal_dragon_statue rgba.png 1 1 &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;More arguments (e.g. &lt;code&gt;--lambda_guidance 1 40&lt;/code&gt;) can be appended to the command line such as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bash scripts/magic123/run_both_priors.sh 0 default dmtet data/realfusion15/metal_dragon_statue rgba.png 1 1 --lambda_guidance 1 40&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Magic123 for a group of examples&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Run all examples in a folder, check the scripts &lt;code&gt;scripts/magic123/run_folder_both_priors.sh&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run all examples in a given list, check the scripts &lt;code&gt;scripts/magic123/run_list_both_priors.sh&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run Magic123 on a single example without textural inversion&lt;/h3&gt; &#xA;&lt;p&gt;Textural inversion is tedious (requires ~2.5 hours optimization), if you want to test Magic123 quickly on your own example without texural inversion (might degrade the performance), try the following:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;first, foreground and depth estimation&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python preprocess_image.py --path data/demo/ironman/ironman.png&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 coarse stage without textural inversion, takes ~40 mins&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export RUN_ID=&#39;default-a-full-body-ironman&#39;&#xA;export DATA_DIR=&#39;data/demo/ironman&#39;&#xA;export IMAGE_NAME=&#39;rgba.png&#39;&#xA;export FILENAME=$(basename $DATA_DIR)&#xA;export dataset=$(basename $(dirname $DATA_DIR))&#xA;CUDA_VISIBLE_DEVICES=0 python main.py -O \&#xA;--text &#34;A high-resolution DSLR image of a full body ironman&#34; \&#xA;--sd_version 1.5 \&#xA;--image ${DATA_DIR}/${IMAGE_NAME} \&#xA;--workspace out/magic123-${RUN_ID}-coarse/$dataset/magic123_${FILENAME}_${RUN_ID}_coarse \&#xA;--optim adam \&#xA;--iters 5000 \&#xA;--guidance SD zero123 \&#xA;--lambda_guidance 1.0 40 \&#xA;--guidance_scale 100 5 \&#xA;--latent_iter_ratio 0 \&#xA;--normal_iter_ratio 0.2 \&#xA;--t_range 0.2 0.6 \&#xA;--bg_radius -1 \&#xA;--save_mesh&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 fine stage without textural inversion, takes around ~20 mins&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export RUN_ID=&#39;default-a-full-body-ironman&#39;&#xA;export RUN_ID2=&#39;dmtet&#39;&#xA;export DATA_DIR=&#39;data/demo/ironman&#39;&#xA;export IMAGE_NAME=&#39;rgba.png&#39;&#xA;export FILENAME=$(basename $DATA_DIR)&#xA;export dataset=$(basename $(dirname $DATA_DIR))&#xA;CUDA_VISIBLE_DEVICES=0 python main.py -O \&#xA;--text &#34;A high-resolution DSLR image of a full body ironman&#34; \&#xA;--sd_version 1.5 \&#xA;--image ${DATA_DIR}/${IMAGE_NAME} \&#xA;--workspace out/magic123-${RUN_ID}-${RUN_ID2}/$dataset/magic123_${FILENAME}_${RUN_ID}_${RUN_ID2} \&#xA;--dmtet --init_ckpt out/magic123-${RUN_ID}-coarse/$dataset/magic123_${FILENAME}_${RUN_ID}_coarse/checkpoints/magic123_${FILENAME}_${RUN_ID}_coarse.pth \&#xA;--iters 5000 \&#xA;--optim adam \&#xA;--known_view_interval 4 \&#xA;--latent_iter_ratio 0 \&#xA;--guidance SD zero123 \&#xA;--lambda_guidance 1e-3 0.01 \&#xA;--guidance_scale 100 5 \&#xA;--rm_edge \&#xA;--bg_radius -1 \&#xA;--save_mesh &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Run ablation studies&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 with only 2D prior &lt;em&gt;with&lt;/em&gt; textural inversion (Like RealFusion but we achieve much better performance through training stragies and the coarse-to-fine pipeline)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bash scripts/magic123/run_2dprior.sh 0 default dmtet data/realfusion15/metal_dragon_statue rgba.png 1 1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 with only 2D prior &lt;em&gt;without&lt;/em&gt; textural inversion (Like RealFusion but we achieve much better performance through training stragies and the coarse-to-fine pipeline)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bash scripts/magic123/run_2dprior_notextinv_ironman.sh 0 default 1 1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;note: change the path and the text prompt inside the script if you wana test another example.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Run Magic123 with only 3D prior (Like Zero-1-to-3 but we achieve much better performance through training stragies and the coarse-to-fine pipeline)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;bash scripts/magic123/run_3dprior.sh 0 default dmtet data/demo/ironman rgba.png 1 1&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Tips and Tricks&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Fix camera distance (&lt;em&gt;radius_range&lt;/em&gt;) and FOV (&lt;em&gt;fovy_range&lt;/em&gt;) and tune the camera polar range (&lt;em&gt;theta_range&lt;/em&gt;). Note it is better to keep camera jittering to reduce grid artifacts.&lt;/li&gt; &#xA; &lt;li&gt;Smaller range of time steps for the defusion noise (t_range). We find &lt;em&gt;[0.2, 0.6]&lt;/em&gt; gives better performance for image-to-3D tasks.&lt;/li&gt; &#xA; &lt;li&gt;Using normals as latent in the first 2000 improves generated geometry a bit gernerally (but not always). We turn on this for Magic123 corase stage in the script &lt;code&gt;--normal_iter_ratio 0.2&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;We erode segmentation edges (makes the segmentation map 2 pixels shrinked towards internal side) to remove artifacts due to segmentation erros. This is turned on in the fine stage in magic123 in the script through &lt;code&gt;--rm_edge&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Other general tricks such as improved texural inversion, advanced diffusion prior (DeepFloyd, SD-XL), stronger 3D prior (Zero123-XL), and larger batch size can be adopted as well but not studied in this work.&lt;/li&gt; &#xA; &lt;li&gt;textural inversion is not very necessary for well-known things (e.g. ironman) and easily described textures and geoemtries, since pure texts contains these texture information and will be understood by diffusion models. We use textural inversion by default in all experiments.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;This work is build upon Stable DreamFusion, many thanks to the author &lt;a href=&#34;https://github.com/ashawkey&#34;&gt;Kiui Jiaxiang Tang&lt;/a&gt; and many other contributors.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ashawkey/stable-dreamfusion&#34;&gt;Stable DreamFusion&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{stable-dreamfusion,&#xA;    Author = {Jiaxiang Tang},&#xA;    Year = {2022},&#xA;    Note = {https://github.com/ashawkey/stable-dreamfusion},&#xA;    Title = {Stable-dreamfusion: Text-to-3D with Stable-diffusion}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We also get inspirations from a list of amazing research works and open-source projects, thanks a lot to all the authors for sharing!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://dreamfusion3d.github.io/&#34;&gt;DreamFusion: Text-to-3D using 2D Diffusion&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@article{poole2022dreamfusion,&#xA;    author = {Poole, Ben and Jain, Ajay and Barron, Jonathan T. and Mildenhall, Ben},&#xA;    title = {DreamFusion: Text-to-3D using 2D Diffusion},&#xA;    journal = {arXiv},&#xA;    year = {2022},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://research.nvidia.com/labs/dir/magic3d/&#34;&gt;Magic3D: High-Resolution Text-to-3D Content Creation&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@inproceedings{lin2023magic3d,&#xA;   title={Magic3D: High-Resolution Text-to-3D Content Creation},&#xA;   author={Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},&#xA;   booktitle={IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})},&#xA;   year={2023}&#xA; }&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/cvlab-columbia/zero123&#34;&gt;Zero-1-to-3: Zero-shot One Image to 3D Object&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@misc{liu2023zero1to3,&#xA;    title={Zero-1-to-3: Zero-shot One Image to 3D Object},&#xA;    author={Ruoshi Liu and Rundi Wu and Basile Van Hoorick and Pavel Tokmakov and Sergey Zakharov and Carl Vondrick},&#xA;    year={2023},&#xA;    eprint={2303.11328},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/lukemelas/realfusion&#34;&gt;RealFusion: 360° Reconstruction of Any Object from a Single Image&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@inproceedings{melaskyriazi2023realfusion,&#xA;    author = {Melas-Kyriazi, Luke and Rupprecht, Christian and Laina, Iro and Vedaldi, Andrea},&#xA;    title = {RealFusion: 360 Reconstruction of Any Object from a Single Image},&#xA;    booktitle={CVPR}&#xA;    year = {2023},&#xA;    url = {https://arxiv.org/abs/2302.10663},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.14184&#34;&gt;Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@article{tang2023make-it-3d,&#xA;    title={Make-it-3d: High-fidelity 3d creation from a single image with diffusion prior},&#xA;    author={Tang, Junshu and Wang, Tengfei and Zhang, Bo and Zhang, Ting and Yi, Ran and Ma, Lizhuang and Chen, Dong},&#xA;    journal={arXiv preprint arXiv:2303.14184},&#xA;    year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/CompVis/stable-diffusion&#34;&gt;Stable Diffusion&lt;/a&gt; and the &lt;a href=&#34;https://github.com/huggingface/diffusers&#34;&gt;diffusers&lt;/a&gt; library.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;@misc{rombach2021highresolution,&#xA;    title={High-Resolution Image Synthesis with Latent Diffusion Models},&#xA;    author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},&#xA;    year={2021},&#xA;    eprint={2112.10752},&#xA;    archivePrefix={arXiv},&#xA;    primaryClass={cs.CV}&#xA;}&#xA;&#xA;@misc{von-platen-etal-2022-diffusers,&#xA;    author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Thomas Wolf},&#xA;    title = {Diffusers: State-of-the-art diffusion models},&#xA;    year = {2022},&#xA;    publisher = {GitHub},&#xA;    journal = {GitHub repository},&#xA;    howpublished = {\url{https://github.com/huggingface/diffusers}}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Cite&lt;/h1&gt; &#xA;&lt;p&gt;If you find this work useful, a citation will be appreciated via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{qian2023magic123,&#xA;  title={Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors},&#xA;  author={Qian, Guocheng and Mai, Jinjie and Hamdi, Abdullah and Ren, Jian and Siarohin, Aliaksandr and Li, Bing and Lee, Hsin-Ying and Skorokhodov, Ivan and Wonka, Peter and Tulyakov, Sergey and others},&#xA;  journal={arXiv preprint arXiv:2306.17843},&#xA;  year={2023}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/azurechatgpt</title>
    <updated>2023-08-06T01:26:37Z</updated>
    <id>tag:github.com,2023-08-06:/microsoft/azurechatgpt</id>
    <link href="https://github.com/microsoft/azurechatgpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🤖 Azure ChatGPT: Private &amp; secure ChatGPT for internal enterprise use 💼&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Unleash the Power of Azure Open AI&lt;/h1&gt; &#xA;&lt;p&gt;ChatGPT has grown explosively in popularity as we all know now. Business users across the globe often tap into the public service to work more productively or act as a creative assistant.&lt;/p&gt; &#xA;&lt;p&gt;However, ChatGPT risks exposing confidential intellectual property. One option is to block corporate access to ChatGPT, but people always find workarounds. This also limits the powerful capabilities of ChatGPT and reduces employee productivity and their work experience.&lt;/p&gt; &#xA;&lt;p&gt;Azure ChatGPT is our enterprise option. This is the exact same service but offered as your private ChatGPT.&lt;/p&gt; &#xA;&lt;h3&gt;Benefits are:&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;1. Private:&lt;/strong&gt; Built-in guarantees around the privacy of your data and fully isolated from those operated by OpenAI.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;2. Controlled:&lt;/strong&gt; Network traffic can be fully isolated to your network and other enterprise grade security controls are built in.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;3. Value:&lt;/strong&gt; Deliver added business value with your own internal data sources (plug and play) or use plug-ins to integrate with your internal services (e.g., ServiceNow, etc).&lt;/p&gt; &#xA;&lt;p&gt;We&#39;ve built a Solution Accelerator to empower your workforce with Azure ChatGPT.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/azurechatgpt/main/images/intro.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;📘 Prerequisites&lt;/h1&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://azure.microsoft.com/en-us/products/cognitive-services/openai-service/&#34;&gt;Azure OpenAI&lt;/a&gt;: To deploy and run ChatGPT on Azure, you&#39;ll need an Azure subscription with access to the Azure OpenAI service. Request access &lt;a href=&#34;https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOFA5Qk1UWDRBMjg0WFhPMkIzTzhKQ1dWNyQlQCN0PWcu&#34;&gt;here&lt;/a&gt;. Once you have access, follow the instructions in this &lt;a href=&#34;https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal&#34;&gt;link&lt;/a&gt; to deploy the gpt-35-turbo or gpt-4 models.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Setup GitHub or Azure AD for Authentication: The &lt;a href=&#34;https://github.com/oliverlabs/azurechatgpt#-add-an-identity-provider&#34;&gt;add an identity provider&lt;/a&gt; section below shows how to configure authentication providers.&lt;/p&gt; &lt;p&gt;💡Note: You can configure the authentication provider to your identity solution using &lt;a href=&#34;https://next-auth.js.org/providers/&#34;&gt;NextAuth providers&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h1&gt;👋🏻 Introduction&lt;/h1&gt; &#xA;&lt;p&gt;Azure ChatGPT is built with the following technologies.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nodejs.org/en&#34;&gt;Node.js 18&lt;/a&gt;: an open-source, cross-platform JavaScript runtime environment.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://nextjs.org/docs&#34;&gt;Next.js 13&lt;/a&gt;: enables you to create full-stack web applications by extending the latest React features&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://next-auth.js.org/&#34;&gt;NextAuth.js&lt;/a&gt;: configurable authentication framework for Next.js 13&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.langchain.com/&#34;&gt;LangChain JS&lt;/a&gt;: AI orchestration layer to build intelligent apps&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://tailwindcss.com/&#34;&gt;Tailwind CSS&lt;/a&gt;: is a utility-first CSS framework that provides a series of predefined classes that can be used to style each element by mixing and matching&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://ui.shadcn.com/&#34;&gt;shadcn/ui&lt;/a&gt;: re-usable components built using Radix UI and Tailwind CSS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/en-GB/azure/cosmos-db/nosql/&#34;&gt;Azure Cosmos DB&lt;/a&gt;: fully managed platform-as-a-service (PaaS) NoSQL database to store chat history&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/azure/app-service/&#34;&gt;Azure App Service&lt;/a&gt;: fully managed platform-as-a-service (PaaS) for hosting web applications, REST APIs, and mobile back ends.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/azurechatgpt/main/images/architecture.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;💙 One click Azure deployment&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://aka.ms/anzappazurechatgpt&#34;&gt;&lt;img src=&#34;https://aka.ms/deploytoazurebutton&#34; alt=&#34;Deploy to Azure&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Click on the Deploy to Azure button and configure your settings in the Azure Portal as described in the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/azurechatgpt/main/#%F0%9F%94%91-environment-variables&#34;&gt;Environment variables&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://raw.githubusercontent.com/microsoft/azurechatgpt/main/#%F0%9F%AA%AA-add-an-identity-provider&#34;&gt;section below&lt;/a&gt; for important information about adding authentication to your app.&lt;/p&gt; &#xA;&lt;h1&gt;👨🏻‍💻 Run from your local machine&lt;/h1&gt; &#xA;&lt;p&gt;Clone this repository locally or fork to your Github account. Run all of the the steps below from the &#34;src&#34; directory.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Make sure you deploy an instance of Cosmos DB in your Azure Subscription&lt;/li&gt; &#xA; &lt;li&gt;Create a new file named &lt;code&gt;.env.local&lt;/code&gt; to store the environment variables add the following variables.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Please note:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Do not use double-quotes and do not delete any of the variables.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Make sure that &lt;code&gt;NEXTAUTH_URL=http://localhost:3000&lt;/code&gt; has no comments in the same line.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Azure OpenAI configuration&#xA;AZURE_OPENAI_API_KEY=&#xA;AZURE_OPENAI_API_INSTANCE_NAME=&#xA;AZURE_OPENAI_API_DEPLOYMENT_NAME=&#xA;AZURE_OPENAI_API_VERSION=&#xA;&#xA;# GitHub OAuth app configuration&#xA;AUTH_GITHUB_ID=&#xA;AUTH_GITHUB_SECRET=&#xA;&#xA;# Azure AD OAuth app configuration&#xA;AZURE_AD_CLIENT_ID=&#xA;AZURE_AD_CLIENT_SECRET=&#xA;AZURE_AD_TENANT_ID=&#xA;&#xA;# When deploying to production,&#xA;# set the NEXTAUTH_URL environment variable to the canonical URL of your site.&#xA;# More information: https://next-auth.js.org/configuration/options&#xA;&#xA;NEXTAUTH_SECRET=&#xA;NEXTAUTH_URL=http://localhost:3000&#xA;&#xA;AZURE_COSMOSDB_URI=&#xA;AZURE_COSMOSDB_KEY=&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;change the active directory to be &lt;code&gt;src&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install npm packages by running &lt;code&gt;npm install&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Start the project by running &lt;code&gt;npm run dev&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You should now be prompted to login with your chosen OAuth provider. Once successfully logged in, you can start creating new conversations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/azurechatgpt/main/images/chat-home.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/microsoft/azurechatgpt/main/images/chat-history.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;☁️ Deploy to Azure - GitHub Actions&lt;/h1&gt; &#xA;&lt;h3&gt;🧬 Fork the repository&lt;/h3&gt; &#xA;&lt;p&gt;Fork this repository to your own organisation so that you can execute GitHub Actions against your own Azure Subscription.&lt;/p&gt; &#xA;&lt;h3&gt;🗝️ Configure secrets in your GitHub repository&lt;/h3&gt; &#xA;&lt;h3&gt;1. AZURE_CREDENTIALS&lt;/h3&gt; &#xA;&lt;p&gt;The GitHub workflow requires a secret named &lt;code&gt;AZURE_CREDENTIALS&lt;/code&gt; to authenticate with Azure. The secret contains the credentials for a service principal with the Contributor role on the resource group containing the container app and container registry.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Create a service principal with the Contributor role on the resource group that contains the Azure App Service.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;az ad sp create-for-rbac&#xA;   --name &amp;lt;NAME OF THE CREDENTIAL&amp;gt; --role contributor --scopes /subscriptions/&amp;lt;SUBSCRIPTION ID&amp;gt;/resourceGroups/&amp;lt;RESOURCE GROUP&amp;gt; --sdk-auth --output json&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Copy the JSON output from the command.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In the GitHub repository, navigate to Settings &amp;gt; Secrets &amp;gt; Actions and select New repository secret.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Enter &lt;code&gt;AZURE_CREDENTIALS&lt;/code&gt; as the name and paste the contents of the JSON output as the value.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Select &lt;strong&gt;Add secret&lt;/strong&gt;.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2. AZURE_APP_SERVICE_NAME&lt;/h3&gt; &#xA;&lt;p&gt;Under the same repository secrets add a new variable &lt;code&gt;AZURE_APP_SERVICE_NAME&lt;/code&gt; to deploy to your Azure Web app. The value of this secret is the name of your Azure Web app e.g. &lt;code&gt;my-web-app-name&lt;/code&gt; from the domain &lt;a href=&#34;https://my-web-app-name.azurewebsites.net/&#34;&gt;https://my-web-app-name.azurewebsites.net/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;🔄 Run GitHub Actions&lt;/h3&gt; &#xA;&lt;p&gt;Once the secrets are configured, the GitHub Actions will be triggered for every code push to the repository. Alternatively, you can manually run the workflow by clicking on the &#34;Run Workflow&#34; button in the Actions tab in GitHub.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/microsoft/azurechatgpt/main/images/runworkflow.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;🪪 Add an identity provider&lt;/h1&gt; &#xA;&lt;p&gt;Once the deployment is complete, you will need to add an identity provider to authenticate your app.&lt;/p&gt; &#xA;&lt;p&gt;⚠️ Note: Only one of the identity provider is required below.&lt;/p&gt; &#xA;&lt;h2&gt;GitHub Authentication provider&lt;/h2&gt; &#xA;&lt;p&gt;We&#39;ll create two GitHub apps: one for testing locally and another for production.&lt;/p&gt; &#xA;&lt;h3&gt;🟡 Development app setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to GitHub OAuth Apps setup &lt;a href=&#34;https://github.com/settings/developers&#34;&gt;https://github.com/settings/developers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;New OAuth App&lt;/code&gt; &lt;a href=&#34;https://github.com/settings/applications/new&#34;&gt;https://github.com/settings/applications/new&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fill in the following details&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;Application name: Azure ChatGPT DEV Environment&#xA;Homepage URL: http://localhost:3000&#xA;Authorization callback URL: http://localhost:3000/api/auth/callback/github&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;🟢 Production app setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to GitHub OAuth Apps setup &lt;a href=&#34;https://github.com/settings/developers&#34;&gt;https://github.com/settings/developers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;New OAuth App&lt;/code&gt; &lt;a href=&#34;https://github.com/settings/applications/new&#34;&gt;https://github.com/settings/applications/new&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fill in the following details&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;Application name: Azure ChatGPT Production&#xA;Homepage URL: https://YOUR-WEBSITE-NAME.azurewebsites.net&#xA;Authorization callback URL: https://YOUR-WEBSITE-NAME.azurewebsites.net/api/auth/callback/github&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;⚠️ After completing app setup, ensure your environment variables locally and on Azure App Service are up to date.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;   # GitHub OAuth app configuration&#xA;   AUTH_GITHUB_ID=&#xA;   AUTH_GITHUB_SECRET=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Azure AD Authentication provider&lt;/h2&gt; &#xA;&lt;h3&gt;🟡 Development app setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to Azure AD Apps setup &lt;a href=&#34;https://portal.azure.com/#view/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/~/RegisteredApps&#34;&gt;https://portal.azure.com/#view/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/~/RegisteredApps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;New Registration&lt;/code&gt; &lt;a href=&#34;https://portal.azure.com/#view/Microsoft_AAD_RegisteredApps/CreateApplicationBlade/quickStartType~/null/isMSAApp~/false&#34;&gt;https://portal.azure.com/#view/Microsoft_AAD_RegisteredApps/CreateApplicationBlade/quickStartType~/null/isMSAApp~/false&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fill in the following details&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;Application name: Azure ChatGPT DEV Environment&#xA;Supported account types: Accounts in this organizational directory only&#xA;Redirect URI Platform: Web&#xA;Redirect URI: http://localhost:3000/api/auth/callback/azure-ad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;🟢 Production app setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Navigate to Azure AD Apps setup &lt;a href=&#34;https://portal.azure.com/#view/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/~/RegisteredApps&#34;&gt;https://portal.azure.com/#view/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/~/RegisteredApps&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;New Registration&lt;/code&gt; &lt;a href=&#34;https://portal.azure.com/#view/Microsoft_AAD_RegisteredApps/CreateApplicationBlade/quickStartType~/null/isMSAApp~/false&#34;&gt;https://portal.azure.com/#view/Microsoft_AAD_RegisteredApps/CreateApplicationBlade/quickStartType~/null/isMSAApp~/false&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Fill in the following details&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;Application name: Azure ChatGPT Production&#xA;Supported account types: Accounts in this organizational directory only&#xA;Redirect URI Platform: Web&#xA;Redirect URI: https://YOUR-WEBSITE-NAME.azurewebsites.net/api/auth/callback/azure-ad&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;⚠️ After completing app setup, ensure your environment variables locally and on Azure App Service are up to date.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;# Azure AD OAuth app configuration&#xA;&#xA;AZURE_AD_CLIENT_ID=&#xA;AZURE_AD_CLIENT_SECRET=&#xA;AZURE_AD_TENANT_ID=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;🔑 Environment variables&lt;/h2&gt; &#xA;&lt;p&gt;Below are the required environment variables, to be added to the Azure Portal or in the &lt;code&gt;.env.local&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;App Setting&lt;/th&gt; &#xA;   &lt;th&gt;Value&lt;/th&gt; &#xA;   &lt;th&gt;Note&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AZURE_OPENAI_API_KEY&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;API keys of your Azure OpenAI resource&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AZURE_OPENAI_API_INSTANCE_NAME&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;the name of your Azure OpenAI resource&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AZURE_OPENAI_API_DEPLOYMENT_NAME&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The name of your model deployment&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AZURE_OPENAI_API_VERSION&lt;/td&gt; &#xA;   &lt;td&gt;2023-03-15-preview&lt;/td&gt; &#xA;   &lt;td&gt;API version when using gpt chat&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AUTH_GITHUB_ID&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Client ID of your GitHub OAuth application&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AUTH_GITHUB_SECRET&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Client Secret of your GitHub OAuth application&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NEXTAUTH_SECRET&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Used to encrypt the NextAuth.js JWT, and to hash email verification tokens. &lt;strong&gt;This set by default as part of the deployment template&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;NEXTAUTH_URL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Current webs hosting domain name with HTTP or HTTPS. &lt;strong&gt;This set by default as part of the deployment template&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AZURE_COSMOSDB_URI&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;URL of the Azure CosmosDB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AZURE_COSMOSDB_KEY&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;API Key for Azure Cosmos DB&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AZURE_AD_CLIENT_ID&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The client id specific to the application&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AZURE_AD_CLIENT_SECRET&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The client secret specific to the application&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;AZURE_AD_TENANT_ID&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The organisation Tenant ID&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>cyphar/incus</title>
    <updated>2023-08-06T01:26:37Z</updated>
    <id>tag:github.com,2023-08-06:/cyphar/incus</id>
    <link href="https://github.com/cyphar/incus" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Powerful system container and virtual machine manager&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Incus&lt;/h1&gt; &#xA;&lt;p&gt;Incus is a modern, secure and powerful system container and virtual machine manager.&lt;/p&gt; &#xA;&lt;!-- Include start Incus intro --&gt; &#xA;&lt;p&gt;It provides a unified experience for running and managing full Linux systems inside containers or virtual machines. Incus supports images for a large number of Linux distributions (official Ubuntu images and images provided by the community) and is built around a very powerful, yet pretty simple, REST API. Incus scales from one instance on a single machine to a cluster in a full data center rack, making it suitable for running workloads both for development and in production.&lt;/p&gt; &#xA;&lt;p&gt;Incus allows you to easily set up a system that feels like a small private cloud. You can run any type of workload in an efficient way while keeping your resources optimized.&lt;/p&gt; &#xA;&lt;p&gt;You should consider using Incus if you want to containerize different environments or run virtual machines, or in general run and manage your infrastructure in a cost-effective way.&lt;/p&gt; &#xA;&lt;!-- Include end Incus intro --&gt; &#xA;&lt;h2&gt;Fork of Canonical LXD&lt;/h2&gt; &#xA;&lt;p&gt;Incus, which is named after the &lt;a href=&#34;https://en.wikipedia.org/wiki/Cumulonimbus_incus&#34;&gt;Cumulonimbus incus&lt;/a&gt; or anvil cloud is a community fork of Canonical&#39;s LXD.&lt;/p&gt; &#xA;&lt;p&gt;This fork was made in response to &lt;a href=&#34;https://linuxcontainers.org/lxd/&#34;&gt;Canonical&#39;s takeover&lt;/a&gt; of the LXD project from the Linux Containers community.&lt;/p&gt; &#xA;&lt;p&gt;The main aim of this fork is to provide once again a real community project where everyone&#39;s contributions are welcome and no one single commercial entity is in charge of the project.&lt;/p&gt; &#xA;&lt;p&gt;The fork was done at the LXD 5.16 release, making it possible to upgrade from LXD releases up to and including LXD 5.16. Upgrading from a later LXD release may not work as the two projects are likely to start diverging from this point onwards.&lt;/p&gt; &#xA;&lt;p&gt;Incus will keep monitoring and importing relevant changes from LXD over time though changes and features that are specific to Ubuntu or Canonical&#39;s products are unlikely to be carried over.&lt;/p&gt; &#xA;&lt;h2&gt;Get started&lt;/h2&gt; &#xA;&lt;p&gt;This is still the very early days of this fork. No packages or even releases currently exist. For production use, you are likely better off sticking with Canonical&#39;s LXD for the time being until stable release of Incus exist.&lt;/p&gt; &#xA;&lt;h2&gt;Status&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Type&lt;/th&gt; &#xA;   &lt;th&gt;Service&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CI (client)&lt;/td&gt; &#xA;   &lt;td&gt;GitHub&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cyphar/incus/actions&#34;&gt;&lt;img src=&#34;https://github.com/cyphar/incus/workflows/Client%20build%20and%20unit%20tests/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CI (server)&lt;/td&gt; &#xA;   &lt;td&gt;GitHub&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/cyphar/incus/actions&#34;&gt;&lt;img src=&#34;https://github.com/cyphar/incus/workflows/Tests/badge.svg?sanitize=true&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Go documentation&lt;/td&gt; &#xA;   &lt;td&gt;Godoc&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://godoc.org/github.com/cyphar/incus/client&#34;&gt;&lt;img src=&#34;https://godoc.org/github.com/cyphar/incus/client?status.svg?sanitize=true&#34; alt=&#34;GoDoc&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Static analysis&lt;/td&gt; &#xA;   &lt;td&gt;GoReport&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://goreportcard.com/report/github.com/cyphar/incus&#34;&gt;&lt;img src=&#34;https://goreportcard.com/badge/github.com/cyphar/incus&#34; alt=&#34;Go Report Card&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Security&lt;/h2&gt; &#xA;&lt;!-- Include start security --&gt; &#xA;&lt;p&gt;Consider the following aspects to ensure that your Incus installation is secure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Keep your operating system up-to-date and install all available security patches.&lt;/li&gt; &#xA; &lt;li&gt;Use only supported Incus versions.&lt;/li&gt; &#xA; &lt;li&gt;Restrict access to the Incus daemon and the remote API.&lt;/li&gt; &#xA; &lt;li&gt;Do not use privileged containers unless required. If you use privileged containers, put appropriate security measures in place. See the &lt;a href=&#34;https://linuxcontainers.org/lxc/security/&#34;&gt;LXC security page&lt;/a&gt; for more information.&lt;/li&gt; &#xA; &lt;li&gt;Configure your network interfaces to be secure.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- Include end security --&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/cyphar/incus/raw/main/doc/explanation/security.md&#34;&gt;Security&lt;/a&gt; for detailed information.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;!-- Include start security note --&gt; &#xA;&lt;p&gt;Local access to Incus through the Unix socket always grants full access to Incus. This includes the ability to attach file system paths or devices to any instance as well as tweak the security features on any instance.&lt;/p&gt; &#xA;&lt;p&gt;Therefore, you should only give such access to users who you&#39;d trust with root access to your system.&lt;/p&gt; &#xA;&lt;!-- Include end security note --&gt; &#xA;&lt;!-- Include start support --&gt; &#xA;&lt;h2&gt;Support and community&lt;/h2&gt; &#xA;&lt;p&gt;The following channels are available for you to interact with the Incus community.&lt;/p&gt; &#xA;&lt;h3&gt;Bug reports&lt;/h3&gt; &#xA;&lt;p&gt;You can file bug reports and feature requests at: &lt;a href=&#34;https://github.com/cyphar/incus/issues/new&#34;&gt;&lt;code&gt;https://github.com/cyphar/incus/issues/new&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Documentation&lt;/h2&gt; &#xA;&lt;p&gt;The official documentation is available at: &lt;a href=&#34;https://github.com/cyphar/incus/tree/main/doc&#34;&gt;&lt;code&gt;https://github.com/cyphar/incus/tree/main/doc&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;!-- Include end support --&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Fixes and new features are greatly appreciated. Make sure to read our &lt;a href=&#34;https://raw.githubusercontent.com/cyphar/incus/main/CONTRIBUTING.md&#34;&gt;contributing guidelines&lt;/a&gt; first!&lt;/p&gt;</summary>
  </entry>
</feed>