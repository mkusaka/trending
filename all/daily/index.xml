<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-25T01:28:41Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>ExpressLRS/ExpressLRS</title>
    <updated>2024-06-25T01:28:41Z</updated>
    <id>tag:github.com,2024-06-25:/ExpressLRS/ExpressLRS</id>
    <link href="https://github.com/ExpressLRS/ExpressLRS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;STM32/ESP32/ESP8285-based High-Performance Radio Link for RC applications&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://github.com/ExpressLRS/ExpressLRS-Hardware/raw/master/img/banner.png?raw=true&#34; alt=&#34;Banner&#34;&gt;&lt;/p&gt; &#xA;&lt;center&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/ExpressLRS/ExpressLRS/releases&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/v/release/ExpressLRS/ExpressLRS?style=flat-square&#34; alt=&#34;Release&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ExpressLRS/ExpressLRS/actions&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/ExpressLRS/ExpressLRS/build.yml?logo=github&amp;amp;style=flat-square&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ExpressLRS/ExpressLRS/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/ExpressLRS/ExpressLRS?style=flat-square&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ExpressLRS/ExpressLRS/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/ExpressLRS/ExpressLRS?style=flat-square&#34; alt=&#34;Stars&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/expresslrs&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/596350022191415318?color=%235865F2&amp;amp;logo=discord&amp;amp;logoColor=%23FFFFFF&amp;amp;style=flat-square&#34; alt=&#34;Chat&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/center&gt; &#xA;&lt;h2&gt;Support ExpressLRS&lt;/h2&gt; &#xA;&lt;p&gt;You can support ExpressLRS by contributing code, testing new features, sharing your ideas, or helping others get started. We are exceptionally grateful for those who donate their time to our passion.&lt;/p&gt; &#xA;&lt;p&gt;If you don&#39;t have time to lend a hand in that way but still want to have an impact, consider donating. Donations are used for infrastructure costs and to buy test equipment needed to further the project and make it securely accessible. ExpressLRS accepts donations through Open Collective, which provides recognition of donors and transparency on how that support is utilized.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opencollective.com/expresslrs&#34;&gt;&lt;img src=&#34;https://img.shields.io/opencollective/backers/expresslrs?label=Open%20Collective%20backers&amp;amp;style=flat-square&#34; alt=&#34;Open Collective backers&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;We appreciate all forms of contribution and hope you will join us on Discord!&lt;/p&gt; &#xA;&lt;h2&gt;Website&lt;/h2&gt; &#xA;&lt;p&gt;For general information on the project please refer to our guides on the &lt;a href=&#34;https://www.expresslrs.org/&#34;&gt;website&lt;/a&gt;, and our &lt;a href=&#34;https://www.expresslrs.org/2.0/faq/&#34;&gt;FAQ&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;About&lt;/h2&gt; &#xA;&lt;p&gt;ExpressLRS is an open source Radio Link for Radio Control applications. Designed to be the best FPV Racing link, it is based on the fantastic Semtech &lt;strong&gt;SX127x&lt;/strong&gt;/&lt;strong&gt;SX1280&lt;/strong&gt; LoRa hardware combined with an Espressif or STM32 Processor. Using LoRa modulation as well as reduced packet size it achieves best in class range and latency. It achieves this using a highly optimized over-the-air packet structure, giving simultaneous range and latency advantages. It supports both 900 MHz and 2.4 GHz links, each with their own benefits. 900 MHz supports a maximum of 200 Hz packet rate, with higher penetration. 2.4 GHz supports a blistering fast 1000 Hz on &lt;a href=&#34;http://edgetx.org/&#34;&gt;EdgeTX&lt;/a&gt;. With hundreds of different hardware targets from a wide range of hardware manufacturers, the choice of hardware is constantly growing, with different hardware suited to different requirements.&lt;/p&gt; &#xA;&lt;h2&gt;Configurator&lt;/h2&gt; &#xA;&lt;p&gt;To configure your ExpressLRS hardware, the ExpressLRS Configurator can be used, which is found here:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/ExpressLRS/ExpressLRS-Configurator/releases/&#34;&gt;https://github.com/ExpressLRS/ExpressLRS-Configurator/releases/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;We have both a &lt;a href=&#34;https://discord.gg/expresslrs&#34;&gt;Discord Server&lt;/a&gt; and &lt;a href=&#34;https://www.facebook.com/groups/636441730280366&#34;&gt;Facebook Group&lt;/a&gt;, which have great support for new users and constant ongoing development discussion&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;p&gt;ExpressLRS has the following features:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Up to 1000 Hz Packet Rate&lt;/li&gt; &#xA; &lt;li&gt;Telemetry (Betaflight Lua Compatibility)&lt;/li&gt; &#xA; &lt;li&gt;Wifi Updates&lt;/li&gt; &#xA; &lt;li&gt;Bluetooth or WiFi Sim Joystick&lt;/li&gt; &#xA; &lt;li&gt;Oled &amp;amp; TFT Displays&lt;/li&gt; &#xA; &lt;li&gt;2.4 GHz, 900 MHz, and Dual-Band RC Link&lt;/li&gt; &#xA; &lt;li&gt;SMD Antenna - allows for easier installation into micros&lt;/li&gt; &#xA; &lt;li&gt;Supported receiver protocols: CRSF, SBUS, SUMD, HoTT Telemetry, and PWM&lt;/li&gt; &#xA; &lt;li&gt;VTX and VRX Frequency adjustments from the Lua&lt;/li&gt; &#xA; &lt;li&gt;Bind Phrases - no need for button binding&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;with many more features on the way!&lt;/p&gt; &#xA;&lt;h2&gt;Supported Hardware&lt;/h2&gt; &#xA;&lt;p&gt;ExpressLRS currently supports hardware from a wide range of manufacturers. In principle, the targets listed in the &lt;a href=&#34;https://github.com/ExpressLRS/ExpressLRS-Configurator/releases/&#34;&gt;ExpressLRS Configurator&lt;/a&gt; are tested and supported hardware.&lt;/p&gt; &#xA;&lt;p&gt;Please refer to the [Hardware Selection] page (&lt;a href=&#34;https://www.expresslrs.org/hardware/hardware-selection/&#34;&gt;https://www.expresslrs.org/hardware/hardware-selection/&lt;/a&gt;) on the website for guidance. We do not manufacture any of our hardware, so we can only provide limited support for faulty hardware.&lt;/p&gt; &#xA;&lt;h2&gt;Developers&lt;/h2&gt; &#xA;&lt;p&gt;If you are a developer and would like to contribute to the project, feel free to join the &lt;a href=&#34;https://discord.gg/expresslrs&#34;&gt;discord&lt;/a&gt; and chat about bugs and issues. You can also look for issues at the &lt;a href=&#34;https://github.com/ExpressLRS/ExpressLRS/issues&#34;&gt;GitHub Issue Tracker&lt;/a&gt;. The best thing to do is to submit a Pull Request to the GitHub Repository.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ExpressLRS/ExpressLRS-Hardware/raw/master/img/community.png?raw=true&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>modelscope/DiffSynth-Studio</title>
    <updated>2024-06-25T01:28:41Z</updated>
    <id>tag:github.com,2024-06-25:/modelscope/DiffSynth-Studio</id>
    <link href="https://github.com/modelscope/DiffSynth-Studio" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Enjoy the magic of Diffusion models!&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;DiffSynth Studio&lt;/h1&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;DiffSynth Studio is a Diffusion engine. We have restructured architectures including Text Encoder, UNet, VAE, among others, maintaining compatibility with models from the open-source community while enhancing computational performance. We provide many interesting features. Enjoy the magic of Diffusion models!&lt;/p&gt; &#xA;&lt;h2&gt;Roadmap&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Aug 29, 2023. We propose DiffSynth, a video synthesis framework. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffSynth.github.io/&#34;&gt;Project Page&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/DiffSynth&#34;&gt;EasyNLP&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (ECML PKDD 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2308.03463&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Oct 1, 2023. We release an early version of this project, namely FastSDXL. A try for building a diffusion engine. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The source codes are released on &lt;a href=&#34;https://github.com/Artiprocher/FastSDXL&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;FastSDXL includes a trainable OLSS scheduler for efficiency improvement. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;The original repo of OLSS is &lt;a href=&#34;https://github.com/alibaba/EasyNLP/tree/master/diffusion/olss_scheduler&#34;&gt;here&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;The technical report (CIKM 2023) is released on &lt;a href=&#34;https://arxiv.org/abs/2305.14677&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;A demo video is shown on &lt;a href=&#34;https://www.bilibili.com/video/BV1w8411y7uj&#34;&gt;Bilibili&lt;/a&gt;.&lt;/li&gt; &#xA;     &lt;li&gt;Since OLSS requires additional training, we don&#39;t implement it in this project.&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Nov 15, 2023. We propose FastBlend, a powerful video deflickering algorithm. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;The sd-webui extension is released on &lt;a href=&#34;https://github.com/Artiprocher/sd-webui-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Demo videos are shown on Bilibili, including three tasks. &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1d94y1W7PE&#34;&gt;Video deflickering&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1Lw411m71p&#34;&gt;Video interpolation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://www.bilibili.com/video/BV1RB4y1Z7LF&#34;&gt;Image-driven video rendering&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;The technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2311.09265&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;An unofficial ComfyUI extension developed by other users is released on &lt;a href=&#34;https://github.com/AInseven/ComfyUI-fastblend&#34;&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Dec 8, 2023. We decide to develop a new Project, aiming to release the potential of diffusion models, especially in video synthesis. The development of this project is started.&lt;/li&gt; &#xA; &lt;li&gt;Jan 29, 2024. We propose Diffutoon, a fantastic solution for toon shading. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/DiffutoonProjectPage/&#34;&gt;Project Page&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;The source codes are released in this project.&lt;/li&gt; &#xA;   &lt;li&gt;The technical report (IJCAI 2024) is released on &lt;a href=&#34;https://arxiv.org/abs/2401.16224&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;June 13, 2024. DiffSynth Studio is transfered to ModelScope. The developers have transitioned from &#34;I&#34; to &#34;we&#34;. Of course, I will still participate in development and maintenance.&lt;/li&gt; &#xA; &lt;li&gt;June 21, 2024. We propose ExVideo, a post-tuning technique aimed at enhancing the capability of video generation models. We have extended Stable Video Diffusion to achieve the generation of long videos up to 128 frames. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ecnu-cilab.github.io/ExVideoProjectPage/&#34;&gt;Project Page&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Source code is released in this repo. See &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Models are released on &lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;HuggingFace&lt;/a&gt; and &lt;a href=&#34;https://modelscope.cn/models/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ModelScope&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Technical report is released on &lt;a href=&#34;https://arxiv.org/abs/2406.14130&#34;&gt;arXiv&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Until now, DiffSynth Studio has supported the following models: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/runwayml/stable-diffusion-v1-5&#34;&gt;Stable Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#34;&gt;Stable Diffusion XL&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/guoyww/animatediff/&#34;&gt;AnimateDiff&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/tencent-ailab/IP-Adapter&#34;&gt;Ip-Adapter&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/xinntao/ESRGAN&#34;&gt;ESRGAN&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/hzwer/ECCV2022-RIFE&#34;&gt;RIFE&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://github.com/Tencent/HunyuanDiT&#34;&gt;Hunyuan-DiT&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt&#34;&gt;Stable Video Diffusion&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/ECNU-CILab/ExVideo-SVD-128f-v1&#34;&gt;ExVideo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/modelscope/DiffSynth-Studio.git&#xA;cd DiffSynth-Studio&#xA;pip install -e .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage (in Python code)&lt;/h2&gt; &#xA;&lt;p&gt;The Python examples are in &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/&#34;&gt;&lt;code&gt;examples&lt;/code&gt;&lt;/a&gt;. We provide an overview here.&lt;/p&gt; &#xA;&lt;h3&gt;Long Video Synthesis&lt;/h3&gt; &#xA;&lt;p&gt;We trained an extended video synthesis model, which can generate 128 frames. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/ExVideo/&#34;&gt;&lt;code&gt;examples/ExVideo&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&#34;&gt;https://github.com/modelscope/DiffSynth-Studio/assets/35051019/d97f6aa9-8064-4b5b-9d49-ed6001bb9acc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Image Synthesis&lt;/h3&gt; &#xA;&lt;p&gt;Generate high-resolution images, by breaking the limitation of diffusion models! &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/image_synthesis/&#34;&gt;&lt;code&gt;examples/image_synthesis&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;512*512&lt;/th&gt; &#xA;   &lt;th&gt;1024*1024&lt;/th&gt; &#xA;   &lt;th&gt;2048*2048&lt;/th&gt; &#xA;   &lt;th&gt;4096*4096&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/55f679e9-7445-4605-9315-302e93d11370&#34; alt=&#34;512&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/6fc84611-8da6-4a1f-8fee-9a34eba3b4a5&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/9087a73c-9164-4c58-b2a0-effc694143fb&#34; alt=&#34;2048&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/edee9e71-fc39-4d1c-9ca9-fa52002c67ac&#34; alt=&#34;4096&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1024*1024&lt;/th&gt; &#xA;   &lt;th&gt;2048*2048&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/67687748-e738-438c-aee5-96096f09ac90&#34; alt=&#34;1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/584186bc-9855-4140-878e-99541f9a757f&#34; alt=&#34;2048&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Toon Shading&lt;/h3&gt; &#xA;&lt;p&gt;Render realistic videos in a flatten style and enable video editing features. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/Diffutoon/&#34;&gt;&lt;code&gt;examples/Diffutoon&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/b54c05c5-d747-4709-be5e-b39af82404dd&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/20528af5-5100-474a-8cdc-440b9efdd86c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Video Stylization&lt;/h3&gt; &#xA;&lt;p&gt;Video stylization without video models. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/diffsynth/&#34;&gt;&lt;code&gt;examples/diffsynth&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/59fb2f7b-8de0-4481-b79f-0c3a7361a1ea&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Chinese Models&lt;/h3&gt; &#xA;&lt;p&gt;Use Hunyuan-DiT to generate images with Chinese prompts. We also support LoRA fine-tuning of this model. &lt;a href=&#34;https://raw.githubusercontent.com/modelscope/DiffSynth-Studio/main/examples/hunyuan_dit/&#34;&gt;&lt;code&gt;examples/hunyuan_dit&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Prompt: 少女手捧鲜花，坐在公园的长椅上，夕阳的余晖洒在少女的脸庞，整个画面充满诗意的美感&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;1024x1024&lt;/th&gt; &#xA;   &lt;th&gt;2048x2048 (highres-fix)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/2b6528cf-a229-46e9-b7dd-4a9475b07308&#34; alt=&#34;image_1024&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/11d264ec-966b-45c9-9804-74b60428b866&#34; alt=&#34;image_2048&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Prompt: 一只小狗蹦蹦跳跳，周围是姹紫嫣红的鲜花，远处是山脉&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Without LoRA&lt;/th&gt; &#xA;   &lt;th&gt;With LoRA&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/1aa21de5-a992-4b66-b14f-caa44e08876e&#34; alt=&#34;image_without_lora&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/83a0a41a-691f-4610-8e7b-d8e17c50a282&#34; alt=&#34;image_with_lora&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Usage (in WebUI)&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;python -m streamlit run DiffSynth_Studio.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&#34;&gt;https://github.com/Artiprocher/DiffSynth-Studio/assets/35051019/93085557-73f3-4eee-a205-9829591ef954&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>