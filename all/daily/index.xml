<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-24T02:10:39Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>run-llama/rags</title>
    <updated>2023-11-24T02:10:39Z</updated>
    <id>tag:github.com,2023-11-24:/run-llama/rags</id>
    <link href="https://github.com/run-llama/rags" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;RAGs&lt;/h1&gt; &#xA;&lt;p&gt;RAGs is a Streamlit app that lets you create a RAG pipeline from a data source using natural language.&lt;/p&gt; &#xA;&lt;p&gt;You get to do the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Describe your task (e.g. &#34;load this web page&#34;) and the parameters you want from your RAG systems (e.g. &#34;i want to retrieve X number of docs&#34;)&lt;/li&gt; &#xA; &lt;li&gt;Go into the config view and view/alter generated parameters (top-k, summarization, etc.) as needed.&lt;/li&gt; &#xA; &lt;li&gt;Query the RAG agent over data with your questions.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;This project is inspired by &lt;a href=&#34;https://openai.com/blog/introducing-gpts&#34;&gt;GPTs&lt;/a&gt;, launched by OpenAI.&lt;/p&gt; &#xA;&lt;h2&gt;Installation and Setup&lt;/h2&gt; &#xA;&lt;p&gt;Clone this project, go into the &lt;code&gt;rags&lt;/code&gt; project folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;By default, we use OpenAI for both the builder agent as well as the generated RAG agent. Please &lt;code&gt;.streamlit/secrets.toml&lt;/code&gt; in the home folder.&lt;/p&gt; &#xA;&lt;p&gt;Then put the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;openai_key = &#34;&amp;lt;openai_key&amp;gt;&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then run the app from the &#34;home page&#34; file.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;streamlit run 1_üè†_Home.py&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Detailed Overview&lt;/h2&gt; &#xA;&lt;p&gt;The app contains the following sections, corresponding to the steps listed above.&lt;/p&gt; &#xA;&lt;h3&gt;1. üè† Home Page&lt;/h3&gt; &#xA;&lt;p&gt;This is the section where you build a RAG pipeline by instructing the &#34;builder agent&#34;. Typically to setup a RAG pipeline you need the following components:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Describe the dataset. Currently we support either &lt;strong&gt;a single local file&lt;/strong&gt; or a &lt;strong&gt;web page&lt;/strong&gt;. We&#39;re open to suggestions here!&lt;/li&gt; &#xA; &lt;li&gt;Describe the task. Concretely this description will be used to initialize the &#34;system prompt&#34; of the LLM powering the RAG pipeline.&lt;/li&gt; &#xA; &lt;li&gt;Define the typical parameters for a RAG setup. See the below section for the list of parameters.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;2. ‚öôÔ∏è RAG Config&lt;/h3&gt; &#xA;&lt;p&gt;This section contains the RAG parameters, generated by the &#34;builder agent&#34; in the previous section. In this section, you have a UI showcasing the generated parameters and have full freedom to manually edit/change them as necessary.&lt;/p&gt; &#xA;&lt;p&gt;Currently the set of parameters is as follows:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;System Prompt&lt;/li&gt; &#xA; &lt;li&gt;Include Summarization: whether to also add a summarization tool (instead of only doing top-k retrieval.)&lt;/li&gt; &#xA; &lt;li&gt;Top-K&lt;/li&gt; &#xA; &lt;li&gt;Chunk Size&lt;/li&gt; &#xA; &lt;li&gt;Embed Model&lt;/li&gt; &#xA; &lt;li&gt;LLM&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you manually change parameters, you can press the &#34;Update Agent&#34; button in order to update the agent.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-{tip}&#34;&gt;If you don&#39;t see the `Update Agent` button, that&#39;s because you haven&#39;t created the agent yet. Please go to the previous &#34;Home&#34; page and complete the setup process.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We can always add more parameters to make this more &#34;advanced&#34; üõ†Ô∏è, but thought this would be a good place to start.&lt;/p&gt; &#xA;&lt;h3&gt;3. Generated RAG Agent&lt;/h3&gt; &#xA;&lt;p&gt;Once your RAG agent is created, you have access to this page.&lt;/p&gt; &#xA;&lt;p&gt;This is a standard chatbot interface where you can query the RAG agent and it will answer questions over your data.&lt;/p&gt; &#xA;&lt;p&gt;It will be able to pick the right RAG tools (either top-k vector search or optionally summarization) in order to fulfill the query.&lt;/p&gt; &#xA;&lt;h2&gt;Supported LLMs and Embeddings&lt;/h2&gt; &#xA;&lt;h3&gt;Builder Agent&lt;/h3&gt; &#xA;&lt;p&gt;By default the builder agent uses OpenAI. This is defined in the &lt;code&gt;builder_config.py&lt;/code&gt; file.&lt;/p&gt; &#xA;&lt;p&gt;You can customize this to whatever LLM you want (an example is provided for Anthropic).&lt;/p&gt; &#xA;&lt;p&gt;Note that GPT-4 variants will give the most reliable results in terms of actually constructing an agent (we couldn&#39;t get Claude to work).&lt;/p&gt; &#xA;&lt;h3&gt;Generated RAG Agent&lt;/h3&gt; &#xA;&lt;p&gt;You can set the configuration either through natural language or manually for both the embedding model and LLM.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;LLM&lt;/strong&gt;: We support the following LLMs, but you need to explicitly specify the ID to the builder agent. &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;OpenAI: ID is &#34;openai:&amp;lt;model_name&amp;gt;&#34; e.g. &#34;openai:gpt-4-1106-preview&#34;&lt;/li&gt; &#xA;   &lt;li&gt;Anthropic: ID is &#34;anthropic:&amp;lt;model_name&amp;gt;&#34; e.g. &#34;anthropic:claude-2&#34;&lt;/li&gt; &#xA;   &lt;li&gt;Replicate: ID is &#34;replicate:&amp;lt;model_name&amp;gt;&#34;&lt;/li&gt; &#xA;   &lt;li&gt;HuggingFace: ID is &#34;local:&amp;lt;model_name&amp;gt;&#34; e.g. &#34;local:BAAI/bge-small-en&#34;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Embeddings&lt;/strong&gt;: Supports text-embedding-ada-002 by default, but also supports Hugging Face models. To use a hugging face model simply prepend with local, e.g. local:BAAI/bge-small-en.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Issues / Contributions&lt;/h2&gt; &#xA;&lt;p&gt;Running into issues? Please file a Github issue or join our &lt;a href=&#34;https://discord.gg/dGcwcsnxhU&#34;&gt;Discord&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>emilwallner/Screenshot-to-code</title>
    <updated>2023-11-24T02:10:39Z</updated>
    <id>tag:github.com,2023-11-24:/emilwallner/Screenshot-to-code</id>
    <link href="https://github.com/emilwallner/Screenshot-to-code" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A neural network that transforms a design mock-up into a static website.&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/screenshot-to-code.svg?raw=true&#34; width=&#34;800px&#34;&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;&lt;strong&gt;A detailed tutorial covering the code in this repository:&lt;/strong&gt; &lt;a href=&#34;https://emilwallner.medium.com/how-you-can-train-an-ai-to-convert-your-design-mockups-into-html-and-css-cc7afd82fed4&#34;&gt;Turning design mockups into code with deep learning&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Plug:&lt;/strong&gt; üëâ Check out my 60-page guide, &lt;a href=&#34;https://twitter.com/EmilWallner/status/1528961488206979072&#34;&gt;No ML Degree&lt;/a&gt;, on how to land a machine learning job without a degree.&lt;/p&gt; &#xA;&lt;p&gt;The neural network is built in three iterations. Starting with a Hello World version, followed by the main neural network layers, and ending by training it to generalize.&lt;/p&gt; &#xA;&lt;p&gt;The models are based on Tony Beltramelli&#39;s &lt;a href=&#34;https://github.com/tonybeltramelli/pix2code&#34;&gt;pix2code&lt;/a&gt;, and inspired by Airbnb&#39;s &lt;a href=&#34;https://airbnb.design/sketching-interfaces/&#34;&gt;sketching interfaces&lt;/a&gt;, and Harvard&#39;s &lt;a href=&#34;https://github.com/harvardnlp/im2markup&#34;&gt;im2markup&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; only the Bootstrap version can generalize on new design mock-ups. It uses 16 domain-specific tokens which are translated into HTML/CSS. It has a 97% accuracy. The best model uses a GRU instead of an LSTM. This version can be trained on a few GPUs. The raw HTML version has potential to generalize, but is still unproven and requires a significant amount of GPUs to train. The current model is also trained on a homogeneous and small dataset, thus it&#39;s hard to tell how well it behaves on more complex layouts.&lt;/p&gt; &#xA;&lt;p&gt;A quick overview of the process:&lt;/p&gt; &#xA;&lt;h3&gt;1) Give a design image to the trained neural network&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/LDmoLLV.png&#34; alt=&#34;Insert image&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2) The neural network converts the image into HTML markup&lt;/h3&gt; &#xA;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/html_display.gif?raw=true&#34; width=&#34;800px&#34;&gt; &#xA;&lt;h3&gt;3) Rendered output&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://i.imgur.com/tEAfyZ8.png&#34; alt=&#34;Screenshot&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;FloydHub&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://floydhub.com/run?template=https://github.com/floydhub/pix2code-template&#34;&gt;&lt;img src=&#34;https://static.floydhub.com/button/button.svg?sanitize=true&#34; alt=&#34;Run on FloydHub&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Click this button to open a &lt;a href=&#34;https://blog.floydhub.com/workspaces/&#34;&gt;Workspace&lt;/a&gt; on &lt;a href=&#34;https://www.floydhub.com/?utm_medium=readme&amp;amp;utm_source=pix2code&amp;amp;utm_campaign=aug_2018&#34;&gt;FloydHub&lt;/a&gt; where you will find the same environment and dataset used for the &lt;em&gt;Bootstrap version&lt;/em&gt;. You can also find the trained models for testing.&lt;/p&gt; &#xA;&lt;h3&gt;Local&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install keras tensorflow pillow h5py jupyter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/emilwallner/Screenshot-to-code.git&#xA;cd Screenshot-to-code/&#xA;jupyter notebook&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Go do the desired notebook, files that end with &#39;.ipynb&#39;. To run the model, go to the menu then click on Cell &amp;gt; Run all&lt;/p&gt; &#xA;&lt;p&gt;The final version, the Bootstrap version, is prepared with a small set to test run the model. If you want to try it with all the data, you need to download the data here: &lt;a href=&#34;https://www.floydhub.com/emilwallner/datasets/imagetocode&#34;&gt;https://www.floydhub.com/emilwallner/datasets/imagetocode&lt;/a&gt;, and specify the correct &lt;code&gt;dir_name&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Folder structure&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  |  |-Bootstrap                           #The Bootstrap version&#xA;  |  |  |-compiler                         #A compiler to turn the tokens to HTML/CSS (by pix2code)&#xA;  |  |  |-resources&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#x9;&#xA;  |  |  |  |-eval_light                    #10 test images and markup&#xA;  |  |-Hello_world                         #The Hello World version&#xA;  |  |-HTML                                #The HTML version&#xA;  |  |  |-Resources_for_index_file         #CSS,images and scripts to test index.html file&#xA;  |  |  |-html                             #HTML files to train it on&#xA;  |  |  |-images                           #Screenshots for training&#xA;  |-readme_images                          #Images for the readme page&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Hello World&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/Hello_world_model.png?raw=true&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;HTML&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/HTML_model.png?raw=true&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Bootstrap&lt;/h2&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/emilwallner/Screenshot-to-code/master/README_images/Bootstrap_model.png?raw=true&#34; width=&#34;400px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Model weights&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.floydhub.com/emilwallner/datasets/imagetocode&#34;&gt;Bootstrap&lt;/a&gt; (The pre-trained model uses GRUs instead of LSTMs)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.floydhub.com/emilwallner/datasets/html_models&#34;&gt;HTML&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to IBM for donating computing power through their PowerAI platform&lt;/li&gt; &#xA; &lt;li&gt;The code is largely influenced by Tony Beltramelli&#39;s pix2code paper. &lt;a href=&#34;https://github.com/tonybeltramelli/pix2code&#34;&gt;Code&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1705.07962&#34;&gt;Paper&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;The structure and some of the functions are from Jason Brownlee&#39;s &lt;a href=&#34;https://machinelearningmastery.com/develop-a-caption-generation-model-in-keras/&#34;&gt;excellent tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>flowtyone/flowty-realtime-lcm-canvas</title>
    <updated>2023-11-24T02:10:39Z</updated>
    <id>tag:github.com,2023-11-24:/flowtyone/flowty-realtime-lcm-canvas</id>
    <link href="https://github.com/flowtyone/flowty-realtime-lcm-canvas" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A realtime sketch to image demo using LCM and the gradio library.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;flowty-realtime-lcm-canvas&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/flowtyone/flowty-realtime-lcm-canvas/main/example.gif&#34; alt=&#34;example gif&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is a realtime sketch to image demo using LCM and the gradio library. If you&#39;re not familiar with LCM, read about it here - &lt;a href=&#34;https://huggingface.co/blog/lcm_lora&#34;&gt;article on huggingface&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Thanks to LCM LoRA, you can also use different models by altering the model id in the ui. The desired effect was for you to be able to draw on one side and see the changes at close to real-time on the other side.&lt;/p&gt; &#xA;&lt;p&gt;Needless to say, this will perform worse on some GPUs, and better on some GPUs. 4090s usually perform best in the realtime scenario. Share your results!&lt;/p&gt; &#xA;&lt;p&gt;This was tested on a macbook pro with M2 Max, 30 GPU - 32GB combo, python 3.10. Inference times were tolerable, about 1.2s per render. If you&#39;re getting good performance on your machine, feel free to tweak the parameters in order to get better results. You can also change the canvas size to 768 / 1024 in ui.py, depending on your model.&lt;/p&gt; &#xA;&lt;h3&gt;Setup:&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Setup a venv if you feel like: &lt;code&gt;python -m venv env&lt;/code&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;activate on MacOS: &lt;code&gt;source ./env/bin/activate&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;activate on Windows: &lt;code&gt;env\Scripts\activate&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Nvidia users should install pytorch using this command: &lt;code&gt;pip install torch --extra-index-url https://download.pytorch.org/whl/cu121&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Install the requirements: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run ui.py: &lt;code&gt;python ui.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;After you run ui.py, models should be downloaded automatically to the models directory. It might take a few minutes depending on your network. After that gradio will print to the console the url where you can access the ui.&lt;/p&gt; &#xA;&lt;h3&gt;Use Google Colab&lt;/h3&gt; &#xA;&lt;p&gt;Google Colab users can also enjoy it by executing the following command and accessing the generated Gradio Public URL.&lt;br&gt; (I think this is currently only available in the Colab Pro.)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;!git clone https://github.com/flowtyone/flowty-realtime-lcm-canvas.git&#xA;%cd flowty-realtime-lcm-canvas&#xA;!pip install -r requirements.txt&#xA;!python ui.py --share&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This is a community project from &lt;a href=&#34;https://flowt.ai&#34;&gt;flowt.ai&lt;/a&gt;. If you like it, check us out!&lt;/p&gt; &#xA;&lt;picture&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;logo-dark.svg&#34; height=&#34;50&#34;&gt; &#xA; &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;logo.svg&#34; height=&#34;50&#34;&gt; &#xA; &lt;img alt=&#34;flowt.ai logo&#34; src=&#34;https://raw.githubusercontent.com/flowtyone/flowty-realtime-lcm-canvas/main/flowt.png&#34; height=&#34;50&#34;&gt; &#xA;&lt;/picture&gt;</summary>
  </entry>
</feed>