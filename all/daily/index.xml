<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-10-05T01:24:01Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>mit-han-lab/streaming-llm</title>
    <updated>2023-10-05T01:24:01Z</updated>
    <id>tag:github.com,2023-10-05:/mit-han-lab/streaming-llm</id>
    <link href="https://github.com/mit-han-lab/streaming-llm" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Efficient Streaming Language Models with Attention Sinks&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Efficient Streaming Language Models with Attention Sinks [&lt;a href=&#34;http://arxiv.org/abs/2309.17453&#34;&gt;paper&lt;/a&gt;]&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/mit-han-lab/streaming-llm/main/figures/schemes.png&#34; alt=&#34;schemes&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/mit-han-lab/streaming-llm/assets/40906949/2bd1cda4-a0bd-47d1-a023-fbf7779b8358&#34;&gt;https://github.com/mit-han-lab/streaming-llm/assets/40906949/2bd1cda4-a0bd-47d1-a023-fbf7779b8358&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;TL;DR&lt;/h2&gt; &#xA;&lt;p&gt;We deploy LLMs for infinite-length inputs without sacrificing efficiency and performance.&lt;/p&gt; &#xA;&lt;h2&gt;Abstract&lt;/h2&gt; &#xA;&lt;p&gt;Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens&#39; Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach --- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a ``sink&#39;&#39; even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Environment Setup&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -yn streaming python=3.8&#xA;conda activate streaming&#xA;&#xA;pip install torch torchvision torchaudio&#xA;pip install transformers==4.33.0 accelerate datasets evaluate wandb scikit-learn scipy sentencepiece&#xA;&#xA;python setup.py develop&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Run Streaming Llama Chatbot&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;CUDA_VISIBLE_DEVICES=0 python examples/run_streaming_llama.py  --enable_streaming&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;What does &#34;working on infinite-length inputs&#34; imply for LLMs?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Handling infinite-length text with LLMs presents challenges. Notably, storing all previous Key and Value (KV) states demands significant memory, and models might struggle to generate text beyond their training sequence length. StreamingLLM addresses this by retaining only the most recent tokens and attention sinks, discarding intermediate tokens. This enables the model to generate coherent text from recent tokens without a cache reset â€” a capability not seen in earlier methods.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Is the context window of LLMs expanded?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;No. The context window remains unchanged. Only the most recent tokens and attention sinks are retained, discarding middle tokens. This means the model can only process the latest tokens. The context window remains constrained by its initial pre-training. For instance, if Llama-2 is pre-trained with a context window of 4096 tokens, then the maximum cache size for StreamingLLM on Llama-2 remains 4096.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Can I input an extensive text, like a book, into StreamingLLM for summarization?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;While you can input a lengthy text, the model will only recognize the latest tokens. Thus, if a book is an input, StreamingLLM might only summarize the concluding paragraphs, which might not be very insightful. As emphasized earlier, we neither expand the LLMs&#39; context window nor enhance their long-term memory. StreamingLLM&#39;s strength lies in generating fluent text from recent tokens without needing a cache refresh.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;What is the ideal use case for StreamingLLM?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;StreamingLLM is optimized for streaming applications, such as multi-round dialogues. It&#39;s ideal for scenarios where a model needs to operate continually without requiring extensive memory or dependency on past data. An example is a daily assistant based on LLMs. StreamingLLM would let the model function continuously, basing its responses on recent conversations without needing to refresh its cache. Earlier methods would either need a cache reset when the conversation length exceeded the training length (losing recent context) or recompute KV states from recent text history, which can be time-consuming.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;How does StreamingLLM relate to recent works on context extension?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;StreamingLLM is orthogonal to recent context extension methods and can be integrated with them. In StreamingLLM&#39;s context, &#34;context extension&#34; refers to the possibility of using a larger cache size to store more recent tokens. For a practical demonstration, refer to Figure 9 in our paper, where we implement StreamingLLM with models like LongChat-7B-v1.5-32K and Llama-2-7B-32K-Instruct.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;TODOs&lt;/h2&gt; &#xA;&lt;p&gt;We will release the code and data in the following order, please stay tuned!&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release core code of StreamingLLM, including Llama-2, MPT, Falcon, and Pythia.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release perplexity evaluation code&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Release Streaming Llama Chatbot demo.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Release StreamEval dataset and evaluation code.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find StreamingLLM useful or relevant to your project and research, please kindly cite our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{xiao2023streamingllm,&#xA;        title={Efficient Streaming Language Models with Attention Sinks},&#xA;        author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},&#xA;        journal={arXiv},&#xA;        year={2023}&#xA;        }&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>danielgross/localpilot</title>
    <updated>2023-10-05T01:24:01Z</updated>
    <id>tag:github.com,2023-10-05:/danielgross/localpilot</id>
    <link href="https://github.com/danielgross/localpilot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;localpilot&lt;/h1&gt; &#xA;&lt;p&gt;&lt;em&gt;Use GitHub Copilot locally on your Macbook with one-click!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/danielgross/localpilot/assets/279531/521d0613-7423-4839-a5e8-42098cd65a5e&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Demo Video&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/danielgross/localpilot/assets/279531/3259981b-39f7-4bfa-8a45-84bde6d4ba4c&#34;&gt;https://github.com/danielgross/localpilot/assets/279531/3259981b-39f7-4bfa-8a45-84bde6d4ba4c&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This video is not sped up or slowed down.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;First, open VS Code Settings and add the following to your settings.json file:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&#34;github.copilot.advanced&#34;: {&#xA;    &#34;debug.testOverrideProxyUrl&#34;: &#34;http://localhost:5001&#34;,&#xA;    &#34;debug.overrideProxyUrl&#34;: &#34;http://localhost:5001&#34;&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Create a virtualenv to run this Python process, install the requirements, and download the models.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;virtualenv venv&#xA;source venv/bin/activate&#xA;pip install -r requirements.txt&#xA;# First setup run. This will download several models to your ~/models folder.&#xA;python app.py --setup &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run it!&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;python app.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Enjoy your on-device Copilot!&lt;/p&gt; &#xA;&lt;h2&gt;Caveat FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is the code as good as GitHub Copilot?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For simple line completions yes. For simple function completions, mostly. For complex functions... maybe.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Is it as fast as GitHub Copilot?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;On my Macbook Pro with an Apple M2 Max, the 7b models are roughly as fast. The 34b models are not. Please consider this repo a demonstration of a very inefficient implementation. I&#39;m sure we can make it faster; please do submit a pull request if you&#39;d like to help. For example, I think we need debouncer because sometimes llama.cpp/GGML isn&#39;t fast at interrupting itself when a newer request comes in.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Can this be packaged as a simple Mac app?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes!, I&#39;m sure it can be, I just haven&#39;t had the time. Please do submit a pull request if you&#39;re into that sort of thing!&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Should there be a meta-model that routes to a 1b for autocomplete, 7b for more complex autocomplete, and a 34b for program completion?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Hmm, that seems like an interesting idea.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OK, but in summary, is it good?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Only if your network is bad. I don&#39;t think it&#39;s competitive if you have fast Internet. But it sure is awesome on airplanes and while tethering!&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ossamamehmood/Hacktoberfest2023</title>
    <updated>2023-10-05T01:24:01Z</updated>
    <id>tag:github.com,2023-10-05:/ossamamehmood/Hacktoberfest2023</id>
    <link href="https://github.com/ossamamehmood/Hacktoberfest2023" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hacktoberfest 2023 OPEN FIRST Pull Request - FREE T-SHIRTðŸŽ‰&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;HacktoberFest 2023 &lt;code&gt;OPEN FIRST&lt;/code&gt; Pull Request ðŸŽ‰&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/ossamamehmood/Hacktoberfest2023/raw/main/.github/logo.png&#34; alt=&#34;HacktoberFest 2021&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contributors of &lt;code&gt;Hacktoberfest 2023&lt;/code&gt;&lt;/h2&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ossamamehmood/Hacktoberfest2023/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=ossamamehmood/Hacktoberfest2023&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;This Project Is Perfect For Your First Pull Request&lt;/h2&gt; &#xA;&lt;p&gt;ðŸ—£ &lt;strong&gt;Hacktoberfest encourages participation in the open source community, which grows bigger every year.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;ðŸ“¢ &lt;strong&gt;Register &lt;a href=&#34;https://hacktoberfest.digitalocean.com&#34;&gt;here&lt;/a&gt; for Hacktoberfest and make four pull requests (PRs) between October 1st-31st to grab free DIGITAL-SWAGS ðŸ”¥.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/badge/hacktoberfest-2023-blueviolet&#34; alt=&#34;Hacktober Badge&#34;&gt; &#xA; &lt;img src=&#34;https://img.shields.io/static/v1?label=%F0%9F%8C%9F&amp;amp;message=If%20Useful&amp;amp;style=style=flat&amp;amp;color=BC4E99&#34; alt=&#34;Star Badge&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/ossamamehmood&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributions-welcome-violet.svg?style=flat&amp;amp;logo=git&#34; alt=&#34;Contributions&#34;&gt;&lt;/a&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2023/pulls&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/issues-pr/ossamamehmood/hacktoberfest2023&#34; alt=&#34;Pull Requests Badge&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2023/graphs/contributors&#34;&gt;&lt;img alt=&#34;GitHub contributors&#34; src=&#34;https://img.shields.io/github/contributors/ossamamehmood/hacktoberfest2023?color=2b9348&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ossamamehmood/hacktoberfest2023/raw/master/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/ossamamehmood/hacktoberfest2023?color=2b9348&#34; alt=&#34;License Badge&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;INSTRUCTIONS-&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Fork this Repository using the button at the top on right corner.&lt;/li&gt; &#xA; &lt;li&gt;Clone your forked repository to your pc ( git clone &#34;url from clone option.)&lt;/li&gt; &#xA; &lt;li&gt;Create a new branch for your modifications (ie. &lt;code&gt;git branch new-user&lt;/code&gt; and check it out &lt;code&gt;git checkout new-user&lt;/code&gt; or simply do &lt;code&gt;git checkout -b new-user&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Add your profile image in &lt;code&gt;static/images/&lt;/code&gt; ( use drag and drop option or upload by commands.)&lt;/li&gt; &#xA; &lt;li&gt;Add your profile data in &lt;code&gt;content/participant/&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add your files (&lt;code&gt;git add -A&lt;/code&gt;), commit (&lt;code&gt;git commit -m &#34;added myself&#34;&lt;/code&gt;) and push (&lt;code&gt;git push origin new-user&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Create a pull request&lt;/li&gt; &#xA; &lt;li&gt;Star this repository&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;How To Make Your First Pull Request&lt;/h1&gt; &#xA;&lt;p&gt;Replace &lt;code&gt;&amp;lt;YOUR-USERNAME&amp;gt;&lt;/code&gt; with your GitHub username in this guide.&lt;/p&gt; &#xA;&lt;h2&gt;1. Add your profile picture to the folder&lt;/h2&gt; &#xA;&lt;p&gt;Add a picture picture of your choice in &lt;code&gt;static/images/&lt;/code&gt;. Accepted files are &lt;strong&gt;png&lt;/strong&gt; and &lt;strong&gt;jpg&lt;/strong&gt;, should be squared and minimum size 544x544 pixels. Ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;static/images/&amp;lt;YOUR-USERNAME&amp;gt;.png&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;2. Add your profile information&lt;/h2&gt; &#xA;&lt;p&gt;Create a markdown file in your folder following the convention &lt;code&gt;content/participant/&amp;lt;YOUR-USERNAME&amp;gt;.md&lt;/code&gt;. Ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;content/participant/&amp;lt;YOUR-USERNAME&amp;gt;.md&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Copy the next template into your file, delete the boilerplate data and fill the information with yours.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;---&#xA;name: YOURNAME&#xA;institution/company: INSTITUTION_NAME&#xA;github:USERNAME&#xA;---&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h2&gt; OR &lt;/h2&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;3. Create / Upload Your Code / Algorithms&lt;/h2&gt; &#xA;&lt;p&gt;Create/Upload your code in folder following the convention &lt;code&gt;Add Code Here&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Choose an exact lanaguage folder and &lt;code&gt;drop your code&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Below is &lt;code&gt;an example&lt;/code&gt; to add file properly&lt;/li&gt; &#xA; &lt;li&gt;You can follow up &lt;code&gt;any languages&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;Add Code Here/PYTHON/&amp;lt;YOUR-FILERNAME&amp;gt;.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;pre&gt;&lt;code&gt;Add Code Here/C++/&amp;lt;YOUR-FILERNAME&amp;gt;.cpp&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can follow any pathway a &lt;code&gt;code&lt;/code&gt; or &lt;code&gt;profile information&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;4. Wait for Pull Request to merge&lt;/h2&gt; &#xA;&lt;h2&gt;5. Celebrate - you&#39;ve done your first pull request!!&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&#39;&#39;&#39;&#xA;Always make more then 4 pull requests.&#xA;Lets say you have made only 4 pull request to different projects,&#xA;but one project is excluded from hackoctoberfest event then your pull request will not be counted and &#xA;then you have remaining 3 valid pull requests if these projects is not excluded.&#xA;If you fail to make 4 pull requests then you can&#39;t get digital-swags.&#xA;I will recommend you to make pull request to your own repo which is very very safest side for you..&#xA;keep in mind that repo has hacktoberfest topic..&#xA;&#39;&#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;+ Follow Me : } Quick Approval of Pull Request&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-py&#34;&gt;&#39;&#39;&#39;&#xA;To get approval of the pull request much quicker and faster (`Follow Me`)ðŸš€&#xA;&#39;&#39;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;a href=&#34;https://github.com/ossamamehmood&#34;&gt;&lt;kbd&gt;&lt;img src=&#34;https://avatars3.githubusercontent.com/ossamamehmood?size=100&#34; width=&#34;100px;&#34; alt=&#34;&#34;&gt;&lt;/kbd&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Ossama Mehmood&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&#xA;&lt;br&gt;</summary>
  </entry>
</feed>