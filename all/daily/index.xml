<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-11-12T01:27:36Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>SawyerHood/draw-a-ui</title>
    <updated>2023-11-12T01:27:36Z</updated>
    <id>tag:github.com,2023-11-12:/SawyerHood/draw-a-ui</id>
    <link href="https://github.com/SawyerHood/draw-a-ui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Draw a mockup and generate html for it&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;draw-a-ui&lt;/h1&gt; &#xA;&lt;p&gt;This is an app that uses tldraw and the gpt-4-vision api to generate html based on a wireframe you draw.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SawyerHood/draw-a-ui/main/demo.gif&#34; alt=&#34;A demo of the app&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This works by just taking the current canvas SVG, converting it to a PNG, and sending that png to gpt-4-vision with instructions to return a single html file with tailwind.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;Disclaimer: This is a demo and is not intended for production use. It doesn&#39;t have any auth so you will go broke if you deploy it.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;This is a Next.js app. To get started run the following commands in the root directory of the project. You will need an OpenAI API key with access to the GPT-4 Vision API.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;echo &#34;OPENAI_API_KEY=sk-your-key&#34; &amp;gt; .env.local&#xA;npm install&#xA;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Open &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt; with your browser to see the result.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>microsoft/inshellisense</title>
    <updated>2023-11-12T01:27:36Z</updated>
    <id>tag:github.com,2023-11-12:/microsoft/inshellisense</id>
    <link href="https://github.com/microsoft/inshellisense" rel="alternate"></link>
    <summary type="html">&lt;p&gt;IDE style command line auto complete&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;inshellisense&lt;/h1&gt; &#xA;&lt;p&gt;&lt;code&gt;inshellisense&lt;/code&gt; provides IDE style autocomplete for shells. It&#39;s a terminal native runtime for &lt;a href=&#34;https://github.com/withfig/autocomplete&#34;&gt;autocomplete&lt;/a&gt; which has support for 600+ command line tools. &lt;code&gt;inshellisense&lt;/code&gt; supports Windows, Linux, &amp;amp; MacOS.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt;&lt;img alt=&#34;demo of inshellisense working&#34; src=&#34;https://raw.githubusercontent.com/microsoft/inshellisense/main/docs/demo.gif&#34; height=&#34;450px&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;node &amp;gt;= 16.x&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install -g @microsoft/inshellisense&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Quickstart&lt;/h3&gt; &#xA;&lt;p&gt;After completing the installation, you can already run &lt;code&gt;inshellisense --shell &amp;lt;shell&amp;gt;&lt;/code&gt; to start the autocomplete session for your desired shell. Additionally, you can bind &lt;code&gt;inshellisense&lt;/code&gt; to a keybinding of &lt;code&gt;CTRL+a&lt;/code&gt; by running the below command. This brings the added advantages of automatically starting the autocomplete session with your current shell and injecting any accepted command into your shell&#39;s history.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;inshellisense bind&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Additionally, inshellisense is also aliased under &lt;code&gt;is&lt;/code&gt; after install for convenience.&lt;/p&gt; &#xA;&lt;h2&gt;Integrations&lt;/h2&gt; &#xA;&lt;p&gt;inshellisense supports the following shells:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.gnu.org/software/bash/&#34;&gt;bash&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.zsh.org/&#34;&gt;zsh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/fish-shell/fish-shell&#34;&gt;fish&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PowerShell/PowerShell&#34;&gt;pwsh&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://learn.microsoft.com/en-us/powershell/scripting/windows-powershell/starting-windows-powershell&#34;&gt;powershell&lt;/a&gt; (Windows Powershell)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit &lt;a href=&#34;https://cla.opensource.microsoft.com&#34;&gt;https://cla.opensource.microsoft.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.&lt;/p&gt; &#xA;&lt;p&gt;This project has adopted the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/&#34;&gt;Microsoft Open Source Code of Conduct&lt;/a&gt;. For more information see the &lt;a href=&#34;https://opensource.microsoft.com/codeofconduct/faq/&#34;&gt;Code of Conduct FAQ&lt;/a&gt; or contact &lt;a href=&#34;mailto:opencode@microsoft.com&#34;&gt;opencode@microsoft.com&lt;/a&gt; with any additional questions or comments.&lt;/p&gt; &#xA;&lt;h2&gt;Trademarks&lt;/h2&gt; &#xA;&lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow &lt;a href=&#34;https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general&#34;&gt;Microsoft&#39;s Trademark &amp;amp; Brand Guidelines&lt;/a&gt;. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party&#39;s policies.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>roboflow/awesome-openai-vision-api-experiments</title>
    <updated>2023-11-12T01:27:36Z</updated>
    <id>tag:github.com,2023-11-12:/roboflow/awesome-openai-vision-api-experiments</id>
    <link href="https://github.com/roboflow/awesome-openai-vision-api-experiments" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Must-have resource for anyone who wants to experiment with and build on the OpenAI Vision API üî•&lt;/p&gt;&lt;hr&gt;&lt;h1 align=&#34;center&#34;&gt;openai vision api experiments üß™&lt;/h1&gt; &#xA;&lt;h2&gt;üëã Hello&lt;/h2&gt; &#xA;&lt;p&gt;The must-have resource for anyone who wants to experiment with and build on the &lt;a href=&#34;https://platform.openai.com/docs/guides/vision&#34;&gt;OpenAI Vision API&lt;/a&gt;. This repository serves as a hub for innovative experiments, showcasing a variety of applications ranging from simple image classifications to advanced zero-shot learning models. It&#39;s a space for both beginners and experts to explore the capabilities of the Vision API, share their findings, and collaborate on pushing the boundaries of visual AI.&lt;/p&gt; &#xA;&lt;p&gt;Experimenting with the OpenAI API requires an API üîë. You can get one &lt;a href=&#34;https://platform.openai.com/api-keys&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;‚ö†Ô∏è Limitations&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;100 API requests per single API key per day&lt;/li&gt; &#xA; &lt;li&gt;Can&#39;t be used for object detection or image segmentation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üß™ Experiments&lt;/h2&gt; &#xA;&lt;!-- AUTOGENERATED_EXPERIMENTS_LIST --&gt; &#xA;&lt;!--&#xA;   WARNING: DO NOT EDIT THIS LIST MANUALLY. IT IS AUTOMATICALLY GENERATED.&#xA;   HEAD OVER TO CONTRIBUTING.MD FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.&#xA;--&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;experiment&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;complementary materials&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;strong&gt;authors&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;WebcamGPT - chat with video stream&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow/awesome-openai-vision-api-experiments/raw/main/experiments/webcam-gpt&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/webcamGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Gradio&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;@SkalskiP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;HotDogGPT - simple image classification application&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow/awesome-openai-vision-api-experiments/raw/main/experiments/hot-dog-not-hot-dog&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/HotDogGPT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Gradio&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;@SkalskiP&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zero-shot image classifier with GPT-4V&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow/awesome-openai-vision-api-experiments/tree/main/experiments/gpt4v-classification&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;@capjamesg&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;zero-shot object detection with GroundingDINO + GPT-4V&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow/awesome-openai-vision-api-experiments/tree/main/experiments/gpt4v-grounding-dino-detection&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/DINO-GPT4V&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Gradio&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;@capjamesg&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPT-4V vs. CLIP&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/roboflow/awesome-openai-vision-api-experiments/tree/main/experiments/gpt4v-vs-clip&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;@capjamesg&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;GPT-4V with Set-of-Mark (SoM)&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;a href=&#34;https://github.com/microsoft/SoM&#34;&gt;&lt;img src=&#34;https://badges.aleen42.com/src/github.svg?sanitize=true&#34; alt=&#34;GitHub&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- AUTOGENERATED_EXPERIMENTS_LIST --&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/awesome-openai-vision-api-experiments/assets/26109316/c63fa3c0-4564-49ee-8982-a9e6a23dae9b&#34;&gt;https://github.com/roboflow/awesome-openai-vision-api-experiments/assets/26109316/c63fa3c0-4564-49ee-8982-a9e6a23dae9b&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üóûÔ∏è Must Read Papers&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.11441&#34;&gt;Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V&lt;/a&gt; by Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, Jianfeng Gao&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2309.17421&#34;&gt;The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)&lt;/a&gt; by Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, Lijuan Wang&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/papers/gpt-4-system-card.pdf&#34;&gt;GPT-4 System Card&lt;/a&gt; by OpenAI&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;ü¶∏ Contribution&lt;/h2&gt; &#xA;&lt;p&gt;I would love your help in making this repository even better! Whether you want to correct a typo, add some new experiment, or if you have any suggestions for improvement, feel free to open an &lt;a href=&#34;https://github.com/roboflow/awesome-openai-vision-api-experiments/issues&#34;&gt;issue&lt;/a&gt; or &lt;a href=&#34;https://github.com/roboflow/awesome-openai-vision-api-experiments/pulls&#34;&gt;pull request&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
</feed>