<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-05-12T01:28:44Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>eugeneyan/open-llms</title>
    <updated>2023-05-12T01:28:44Z</updated>
    <id>tag:github.com,2023-05-12:/eugeneyan/open-llms</id>
    <link href="https://github.com/eugeneyan/open-llms" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🤖 A list of open LLMs available for commercial use.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Open LLMs&lt;/h1&gt; &#xA;&lt;p&gt;These LLMs are all licensed for commercial use (e.g., Apache 2.0, MIT, OpenRAIL-M). Contributions welcome!&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language Model&lt;/th&gt; &#xA;   &lt;th&gt;Release Date&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoints&lt;/th&gt; &#xA;   &lt;th&gt;Paper/Blog&lt;/th&gt; &#xA;   &lt;th&gt;Params (B)&lt;/th&gt; &#xA;   &lt;th&gt;Context Length&lt;/th&gt; &#xA;   &lt;th&gt;Licence&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;T5&lt;/td&gt; &#xA;   &lt;td&gt;2019/10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/t5x/raw/main/docs/models.md#flan-t5-checkpoints&#34;&gt;T5 &amp;amp; Flan-T5&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/google/flan-t5-xxl&#34;&gt;Flan-T5-xxl (HF)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints&#34;&gt;Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.06 - 11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://discuss.huggingface.co/t/does-t5-truncate-input-longer-than-512-internally/3602&#34;&gt;512&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;UL2&lt;/td&gt; &#xA;   &lt;td&gt;2022/10&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/google-research/google-research/tree/master/ul2#checkpoints&#34;&gt;UL2 &amp;amp; Flan-UL2&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/google/flan-ul2&#34;&gt;Flan-UL2 (HF)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html&#34;&gt;UL2 20B: An Open Source Unified Language Learner&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/google/flan-ul2#tldr&#34;&gt;512, 2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Cerebras-GPT&lt;/td&gt; &#xA;   &lt;td&gt;2023/03&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/cerebras&#34;&gt;Cerebras-GPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/&#34;&gt;Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models&lt;/a&gt; (&lt;a href=&#34;https://arxiv.org/abs/2304.03208&#34;&gt;Paper&lt;/a&gt;)&lt;/td&gt; &#xA;   &lt;td&gt;0.111 - 13&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/cerebras/Cerebras-GPT-13B#model-details&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Open Assistant (Pythia family)&lt;/td&gt; &#xA;   &lt;td&gt;2023/03&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps&#34;&gt;OA-Pythia-12B-SFT-8&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5&#34;&gt;OA-Pythia-12B-SFT-4&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b&#34;&gt;OA-Pythia-12B-SFT-1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.07327&#34;&gt;Democratizing Large Language Model Alignment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;12&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Pythia&lt;/td&gt; &#xA;   &lt;td&gt;2023/04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/EleutherAI/pythia&#34;&gt;pythia 70M - 12B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.01373&#34;&gt;Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.07 - 12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.01373.pdf&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Dolly&lt;/td&gt; &#xA;   &lt;td&gt;2023/04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/databricks/dolly-v2-12b&#34;&gt;dolly-v2-12b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm&#34;&gt;Free Dolly: Introducing the World&#39;s First Truly Open Instruction-Tuned LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3, 7, 12&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/databrickslabs/dolly#dolly&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;MIT&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;DLite&lt;/td&gt; &#xA;   &lt;td&gt;2023/05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/aisquared/dlite-v2-1_5b&#34;&gt;dlite-v2-1_5b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e&#34;&gt;Announcing DLite V2: Lightweight, Open LLMs That Can Run&amp;nbsp;Anywhere&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.124 - 1.5&lt;/td&gt; &#xA;   &lt;td&gt;1024&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RWKV&lt;/td&gt; &#xA;   &lt;td&gt;2021/08&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v&#34;&gt;RWKV, ChatRWKV&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM&#34;&gt;The RWKV Language Model (and my LM tricks)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.1 - 14&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v&#34;&gt;infinity (RNN)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-J-6B&lt;/td&gt; &#xA;   &lt;td&gt;2023/06&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b&#34;&gt;GPT-J-6B&lt;/a&gt;, &lt;a href=&#34;https://github.com/nomic-ai/gpt4all#raw-model&#34;&gt;GPT4All-J&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/&#34;&gt;GPT-J-6B: 6B JAX-Based Transformer&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;6&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;GPT-NeoX-20B&lt;/td&gt; &#xA;   &lt;td&gt;2022/04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-neox-20b&#34;&gt;GPT-NEOX-20B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.04165&#34;&gt;GPT-NeoX-20B: An Open-Source Autoregressive Language Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;20&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-neox-20b&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Bloom&lt;/td&gt; &#xA;   &lt;td&gt;2022/11&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;Bloom&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.05100&#34;&gt;BLOOM: A 176B-Parameter Open-Access Multilingual Language Model&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;176&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&#34;&gt;OpenRAIL-M v1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StableLM-Alpha&lt;/td&gt; &#xA;   &lt;td&gt;2023/04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Stability-AI/StableLM#stablelm-alpha&#34;&gt;StableLM-Alpha&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models&#34;&gt;Stability AI Launches the First of its StableLM Suite of Language Models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3 - 65&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Stability-AI/StableLM#stablelm-alpha&#34;&gt;4096&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CC BY-SA-4.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;FastChat-T5&lt;/td&gt; &#xA;   &lt;td&gt;2023/04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/lmsys/fastchat-t5-3b-v1.0&#34;&gt;fastchat-t5-3b-v1.0&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://twitter.com/lmsysorg/status/1652037026705985537?s=20&#34;&gt;We are excited to release FastChat-T5: our compact and commercial-friendly chatbot!&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3&lt;/td&gt; &#xA;   &lt;td&gt;512&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;h2oGPT&lt;/td&gt; &#xA;   &lt;td&gt;2023/05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/h2oai/h2ogpt&#34;&gt;h2oGPT&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://h2o.ai/blog/building-the-worlds-best-open-source-large-language-model-h2o-ais-journey/&#34;&gt;Building the World’s Best Open-Source Large Language Model: H2O.ai’s Journey&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;12 - 20&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/h2oai&#34;&gt;256 - 2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MPT-7B&lt;/td&gt; &#xA;   &lt;td&gt;2023/05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b&#34;&gt;MPT-7B&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b-instruct&#34;&gt;MPT-7B-Instruct&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.mosaicml.com/blog/mpt-7b&#34;&gt;Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/mosaicml/mpt-7b#how-is-this-model-different&#34;&gt;84k (ALiBi)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0, CC BY-SA-3.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RedPajama-INCITE&lt;/td&gt; &#xA;   &lt;td&gt;2023/05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/togethercomputer&#34;&gt;RedPajama-INCITE&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.together.xyz/blog/redpajama-models-v1&#34;&gt;Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned &amp;amp; chat models&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;3 - 7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1/blob/157bf3174feebb67f37e131ea68f84dee007c687/config.json#L13&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenLLaMA&lt;/td&gt; &#xA;   &lt;td&gt;2023/05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/openlm-research/open_llama_7b_preview_300bt&#34;&gt;OpenLLaMA-7b-preview-300bt&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/openlm-research/open_llama&#34;&gt;OpenLLaMA: An Open Reproduction of LLaMA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/h2oai&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Open LLMs for code&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Language Model&lt;/th&gt; &#xA;   &lt;th&gt;Release Date&lt;/th&gt; &#xA;   &lt;th&gt;Checkpoints&lt;/th&gt; &#xA;   &lt;th&gt;Paper/Blog&lt;/th&gt; &#xA;   &lt;th&gt;Params (B)&lt;/th&gt; &#xA;   &lt;th&gt;Context Length&lt;/th&gt; &#xA;   &lt;th&gt;Licence&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;SantaCoder&lt;/td&gt; &#xA;   &lt;td&gt;TODO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode/santacoder&#34;&gt;santacoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.03988&#34;&gt;SantaCoder: don&#39;t reach for the stars!&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.1&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode/santacoder/blob/main/README.md#model-summary&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&#34;&gt;OpenRAIL-M v1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarCoder&lt;/td&gt; &#xA;   &lt;td&gt;TODO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoder&#34;&gt;starcoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/blog/starcoder&#34;&gt;StarCoder: A State-of-the-Art LLM for Code&lt;/a&gt;, &lt;a href=&#34;https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view&#34;&gt;StarCoder: May the source be with you!&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoder#model-summary&#34;&gt;8192&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&#34;&gt;OpenRAIL-M v1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;StarChat Alpha&lt;/td&gt; &#xA;   &lt;td&gt;TODO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/HuggingFaceH4/starchat-alpha&#34;&gt;starchat-alpha&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/blog/starchat-alpha&#34;&gt;Creating a Coding Assistant with StarCoder&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/bigcode/starcoder#model-summary&#34;&gt;8192&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&#34;&gt;OpenRAIL-M v1&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Replit Code&lt;/td&gt; &#xA;   &lt;td&gt;TODO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/replit/replit-code-v1-3b&#34;&gt;replit-code-v1-3b&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.latent.space/p/reza-shabani#details&#34;&gt;Training a SOTA Code LLM in 1 week and Quantifying the Vibes — with Reza Shabani of Replit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;2.7&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/replit/replit-code-v1-3b#model-description&#34;&gt;infinity? (ALiBi)&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;CC BY-SA-4.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;CodeGen2&lt;/td&gt; &#xA;   &lt;td&gt;TODO&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/salesforce/CodeGen2&#34;&gt;codegen2 1B-16B&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.02309&#34;&gt;CodeGen2: Lessons for Training LLMs on Programming and Natural Languages&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1 - 16&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://arxiv.org/abs/2305.02309&#34;&gt;2048&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Open LLM datasets for pre-training&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Release Date&lt;/th&gt; &#xA;   &lt;th&gt;Paper/Blog&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Tokens (T)&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;starcoderdata&lt;/td&gt; &#xA;   &lt;td&gt;2023/05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/blog/starcoder&#34;&gt;StarCoder: A State-of-the-Art LLM for Code&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/bigcode/starcoderdata&#34;&gt;starcoderdata&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;?&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;RedPajama&lt;/td&gt; &#xA;   &lt;td&gt;2023/04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.together.xyz/blog/redpajama&#34;&gt;RedPajama, a project to create leading open-source models, starts by reproducing LLaMA training dataset of over 1.2 trillion tokens&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/togethercomputer/RedPajama-Data&#34;&gt;RedPajama-Data&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;1.2&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Open LLM datasets for instruction-tuning&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Release Date&lt;/th&gt; &#xA;   &lt;th&gt;Paper/Blog&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Samples (K)&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;MPT-7B-Instruct&lt;/td&gt; &#xA;   &lt;td&gt;2023/05&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.mosaicml.com/blog/mpt-7b&#34;&gt;Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/mosaicml/dolly_hhrlhf&#34;&gt;dolly_hhrlhf&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;59&lt;/td&gt; &#xA;   &lt;td&gt;CC BY-SA-3.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;databricks-dolly-15k&lt;/td&gt; &#xA;   &lt;td&gt;2023/04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm&#34;&gt;Free Dolly: Introducing the World&#39;s First Truly Open Instruction-Tuned LLM&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/databricks/databricks-dolly-15k&#34;&gt;databricks-dolly-15k&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;15&lt;/td&gt; &#xA;   &lt;td&gt;CC BY-SA-3.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OIG (Open Instruction Generalist)&lt;/td&gt; &#xA;   &lt;td&gt;2023/03&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://laion.ai/blog/oig-dataset/&#34;&gt;THE OIG DATASET&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/laion/OIG&#34;&gt;OIG&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;44,000&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Open LLM datasets for alignment-tuning&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Name&lt;/th&gt; &#xA;   &lt;th&gt;Release Date&lt;/th&gt; &#xA;   &lt;th&gt;Paper/Blog&lt;/th&gt; &#xA;   &lt;th&gt;Dataset&lt;/th&gt; &#xA;   &lt;th&gt;Samples (K)&lt;/th&gt; &#xA;   &lt;th&gt;License&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenAssistant Conversations Dataset&lt;/td&gt; &#xA;   &lt;td&gt;2023/04&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view&#34;&gt;OpenAssistant Conversations - Democratizing Large Language Model Alignment&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/datasets/OpenAssistant/oasst1&#34;&gt;oasst1&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;161&lt;/td&gt; &#xA;   &lt;td&gt;Apache 2.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Evals on open LLMs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://chat.lmsys.org/?leaderboard&#34;&gt;Leaderboard by lmsys.org&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://twitter.com/jefrankle/status/1654631746506301441&#34;&gt;Evals by MosaicML&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://crfm.stanford.edu/helm/latest/?groups=1&#34;&gt;Holistic Evaluation of Language Models (HELM)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/LudwigStumpp/llm-leaderboard&#34;&gt;LLM-Leaderboard&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bellard.org/ts_server/&#34;&gt;TextSynth Server Benchmarks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;What do the licences mean?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Apache_License&#34;&gt;Apache 2.0&lt;/a&gt;: Allows users to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software under the terms of the license, without concern for royalties.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/MIT_License&#34;&gt;MIT&lt;/a&gt;: Similar to Apache 2.0 but shorter and simpler. Also, in contrast to Apache 2.0, does not require stating any significant changes to the original code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;CC BY-SA-4.0&lt;/a&gt;: Allows (i) copying and redistributing the material and (ii) remixing, transforming, and building upon the material for any purpose, even commercially. But if you do the latter, you &lt;strong&gt;must distribute your contributions under the same license as the original.&lt;/strong&gt; (Thus, may not be viable for internal teams.)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://www.bigcode-project.org/docs/pages/model-license/&#34;&gt;OpenRAIL-M v1&lt;/a&gt;: Allows royalty-free access and flexible downstream use and sharing of the model and modifications of it, and comes with a set of use restrictions (see &lt;a href=&#34;https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement&#34;&gt;Attachment A&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; The information provided in this repo does not, and is not intended to, constitute legal advice. Maintainers of this repo are not responsible for the actions of third parties who use the models. Please consult an attorney before using models for commercial purposes.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h3&gt;Improvements&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Complete entries for context length, and check entries with &lt;code&gt;?&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; &lt;del&gt;Add number of tokens trained?&lt;/del&gt; (see &lt;a href=&#34;https://github.com/eugeneyan/open-llms/issues/7&#34;&gt;considerations&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add (links to) training code?&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add (links to) eval benchmarks?&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>SCIR-HI/Huatuo-Llama-Med-Chinese</title>
    <updated>2023-05-12T01:28:44Z</updated>
    <id>tag:github.com,2023-05-12:/SCIR-HI/Huatuo-Llama-Med-Chinese</id>
    <link href="https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Repo for HuaTuo (华驼), Llama-7B tuned with Chinese medical knowledge. 华驼模型仓库，基于中文医学知识的LLaMA模型指令微调&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/SCIR-HI/Huatuo-Llama-Med-Chinese/main/README.md&#34;&gt;&lt;strong&gt;中文&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/SCIR-HI/Huatuo-Llama-Med-Chinese/main/README_EN.md&#34;&gt;&lt;strong&gt;English&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;a href=&#34;https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SCIR-HI/Huatuo-Llama-Med-Chinese/main/assets/logo/logo.png&#34; alt=&#34;SCIR-HI-HuaTuo&#34; style=&#34;width: 60%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;h1&gt;华驼(HuaTuo): 基于中文医学知识的LLaMA微调模型&lt;/h1&gt; &#xA;&lt;h3&gt;HuaTuo: Tuning LLaMA Model With Chinese Medical Instructions&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg?sanitize=true&#34; alt=&#34;Code License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/release/python-390/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/python-3.9+-blue.svg?sanitize=true&#34; alt=&#34;Python 3.9+&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;本项目开源了经过中文医学指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。我们通过医学知识图谱和GPT3.5 API构建了中文医学指令数据集，并在此基础上对LLaMA进行了指令微调，提高了LLaMA在医疗领域的问答效果。&lt;/p&gt; &#xA;&lt;p&gt;基于相同的数据，我们还训练了医疗版本的ChatGLM模型: &lt;a href=&#34;https://github.com/SCIR-HI/Med-ChatGLM&#34;&gt;ChatGLM-6B-Med&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;此外，我们还尝试利用GPT3.5 API将医学文献中的【结论】作为外部信息融入多轮对话中，在此基础上对LLaMA进行了指令微调。目前。我们只开放针对&#34;肝癌&#34;单个疾病训练的模型参数。在未来，我们计划发布融入文献结论的医学对话数据集，并且会针对“肝胆胰”相关16种疾病训练模型。&lt;/p&gt; &#xA;&lt;p&gt;我们即将发布我们研发的新模型-&lt;a href=&#34;https://github.com/SCIR-HI/Bian-Que_Pien-Chueh&#34;&gt;扁鹊（PienChueh）&lt;/a&gt;，欢迎大家届时使用体验。&lt;/p&gt; &#xA;&lt;h2&gt;更新日志&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;[2023/04/28] 增加了基于&lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;中文Alpaca大模型&lt;/a&gt;进行指令微调的模型发布。&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;[2023/04/24] 增加了基于LLaMA和医学文献进行指令微调的模型发布。&lt;/p&gt; &#xA;&lt;p&gt;[2023/03/31] 发布了基于LLaMA和医学知识库进行指令微调的模型发布。&lt;/p&gt; &#xA;&lt;h2&gt;A Quick Start&lt;/h2&gt; &#xA;&lt;p&gt;首先安装依赖包，python环境建议3.9+&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;pip install -r requirements.txt&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;模型下载&lt;/h3&gt; &#xA;&lt;p&gt;LoRA权重可以通过百度网盘或Huggingface下载：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;对LLaMA进行指令微调的LoRA权重文件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;基于医学知识库 &lt;a href=&#34;https://pan.baidu.com/s/1jih-pEr6jzEa6n2u6sUMOg?pwd=jjpf&#34;&gt;百度网盘&lt;/a&gt;和&lt;a href=&#34;https://huggingface.co/thinksoso/lora-llama-med&#34;&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;基于医学文献 &lt;a href=&#34;https://pan.baidu.com/s/1jADypClR2bLyXItuFfSjPA?pwd=odsk&#34;&gt;百度网盘&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;对Alpaca进行指令微调的LoRA权重文件&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;基于医学知识库 &lt;a href=&#34;https://pan.baidu.com/s/16oxcjzXnXjDpL8SKihgNxw?pwd=scir&#34;&gt;百度网盘&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;基于医学知识库和医学文献 &lt;a href=&#34;https://pan.baidu.com/s/1HDdK84ASHmzOFlkmypBIJw?pwd=scir&#34;&gt;百度网盘&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;下载LoRA权重并解压，解压后的格式如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#1.对LLaMA进行指令微调的LoRA权重文件&#xA;#基于医学知识库&#xA;lora-llama-med/&#xA;&amp;nbsp; - adapter_config.json &amp;nbsp; # LoRA权重配置文件&#xA;&amp;nbsp; - adapter_model.bin &amp;nbsp; # LoRA权重文件&#xA;&#xA;#基于医学文献&#xA;lora-llama-med-literature/&#xA;&amp;nbsp; - adapter_config.json &amp;nbsp; # LoRA权重配置文件&#xA;&amp;nbsp; - adapter_model.bin &amp;nbsp; # LoRA权重文件&#xA;&#xA;&#xA;#2. 对Alpaca进行指令微调的LoRA权重文件&#xA;#基于医学知识库&#xA;lora-alpaca-med-alpaca/&#xA;&amp;nbsp; - adapter_config.json &amp;nbsp; # LoRA权重配置文件&#xA;&amp;nbsp; - adapter_model.bin &amp;nbsp; # LoRA权重文件&#xA;&#xA;#基于医学知识库和医学文献&#xA;lora-alpaca-med-alpaca-alldata/&#xA;&amp;nbsp; - adapter_config.json &amp;nbsp; # LoRA权重配置文件&#xA;&amp;nbsp; - adapter_model.bin &amp;nbsp; # LoRA权重文件&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Infer&lt;/h3&gt; &#xA;&lt;p&gt;我们在&lt;code&gt;./data/infer.json&lt;/code&gt;中提供了一些测试用例，可以替换成其它的数据集，请注意保持格式一致&lt;/p&gt; &#xA;&lt;p&gt;运行infer脚本&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;#基于医学知识库&#xA;bash ./scripts/infer.sh&#xA;&#xA;#基于医学文献&#xA;#单轮&#xA;bash ./scripts/infer-literature-single.sh&#xA;&#xA;#多轮&#xA;bash ./scripts/infer-literature-multi.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;也可参考&lt;code&gt;./scripts/test.sh&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;数据集构建&lt;/h3&gt; &#xA;&lt;p&gt;我们采用了公开和自建的中文医学知识库，主要参考了&lt;a href=&#34;https://github.com/king-yyf/CMeKG_tools&#34;&gt;cMeKG&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;医学知识库围绕疾病、药物、检查指标等构建，字段包括并发症，高危因素，组织学检查，临床症状，药物治疗，辅助治疗等。知识库示例如下:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;{&#34;中心词&#34;: &#34;偏头痛&#34;, &#34;相关疾病&#34;: [&#34;妊娠合并偏头痛&#34;, &#34;恶寒发热&#34;], &#34;相关症状&#34;: [&#34;皮肤变硬&#34;, &#34;头部及眼后部疼痛并能听到连续不断的隆隆声&#34;, &#34;晨起头痛加重&#34;], &#34;所属科室&#34;: [&#34;中西医结合科&#34;, &#34;内科&#34;], &#34;发病部位&#34;: [&#34;头部&#34;]}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;我们利用GPT3.5接口围绕医学知识库构建问答数据，设置了多种Prompt形式来充分利用知识。&lt;/p&gt; &#xA;&lt;p&gt;指令微调的训练集数据示例如下：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;&#34;问题：一位年轻男性长期使用可卡因，突然出现胸痛、呕吐、出汗等症状，经检查发现心电图反映心肌急性损伤，请问可能患的是什么疾病？治疗方式是什么？&#34;&#xA;&#xA;回答: 可能患的是心肌梗塞，需要进行维拉帕米、依普利酮、硝酸甘油、ß阻滞剂、吗啡等药物治疗，并进行溶栓治疗、低分子量肝素、钙通道阻滞剂等辅助治疗。此外需要及时停用可卡因等药物，以防止病情加重。&#34;&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;我们提供了模型的训练数据集，共计八千余条，需要注意的是，虽然训练集的构建融入了知识，但是仍存在错误和不完善的地方，后续我们会利用更好的策略迭代更新数据集。&lt;/p&gt; &#xA;&lt;p&gt;指令微调数据集质量仍有限，后续将进行不断迭代，同时医学知识库和数据集构建代码还在整理中，整理完成将会发布。&lt;/p&gt; &#xA;&lt;p&gt;此外，我们收集了2023年关于肝癌疾病的中文医学文献，利用GPT3.5接口围绕医学文献多轮问答数据。在·&lt;code&gt;./data_literature/liver_cancer.json&lt;/code&gt;中我们提供了其中的1k条训练样例。目前，训练样本的质量仍然有限，在后续我们会进一步迭代数据，会以&lt;code&gt;公开数据集&lt;/code&gt;的形式对外进行发布。训练样本的示例如下：&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34; width=&#34;100%&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese/&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/SCIR-HI/Huatuo-Llama-Med-Chinese/main/assets/case.png&#34; alt=&#34;SCIR-HI-HuaTuo-literature&#34; style=&#34;width: 100%; min-width: 300px; display: block; margin: auto;&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Finetune&lt;/h3&gt; &#xA;&lt;p&gt;如果想用自己的数据集微调LLaMA，请按照&lt;code&gt;./data/llama_data.json&lt;/code&gt;的格式构建自己的数据集&lt;/p&gt; &#xA;&lt;p&gt;运行finetune脚本&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;bash ./scripts/finetune.sh&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;训练细节&lt;/h2&gt; &#xA;&lt;h3&gt;计算资源需求&lt;/h3&gt; &#xA;&lt;p&gt;我们在一张A100-SXM-80GB显卡上进行了训练，训练总轮次10轮，耗时约2h17m。batch_size=128的情况下显存占用在40G左右。预计3090/4090显卡(24GB显存)以上显卡可以较好支持，根据显存大小来调整batch_size。&lt;/p&gt; &#xA;&lt;h3&gt;实验过程&lt;/h3&gt; &#xA;&lt;p&gt;超参设置和实验过程可以参考wandb的链接:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://wandb.ai/thinksoso/llama_med/runs/a5wgcnzt/overview?workspace=user-thinksoso&#34;&gt;https://wandb.ai/thinksoso/llama_med/runs/a5wgcnzt/overview?workspace=user-thinksoso&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;模型效果对比&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;测试输入&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Llama输出 &amp;nbsp;&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Alpaca输出&lt;/th&gt; &#xA;   &lt;th align=&#34;left&#34;&gt;Huatuo华驼输出&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;问题：小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;最近，小张一直感觉身体不舒服，有心烦意乱、呼吸急促等症状。体格检查显示心脏增大，肌肉无力。&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;小张可能患有心肌炎，建议尽快到医院进行检查和治疗。治疗方案包括药物治疗和手术治疗。&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;麻风病和儿童哮喘的病因是否一致？&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;儿童哮喘和麻风病的病因是一致，但是儿童哮喘和麻风病的病因不一致&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;是的，哮喘和儿童喘息有相似的原因。&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;不一致，麻风病的病因是麻风杆菌感染，而儿童哮喘的病因是喂养不良、吸烟等因素。&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;一位患者出现了持续性的胀痛和黄疸，经影像学检查后确诊为肝胆管结石病，应该采取哪些治疗方案？&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;肝胆管结石病患者应该采取哪些治疗方案？&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;患者应接受手术和化疗相结合的治疗。&lt;/td&gt; &#xA;   &lt;td align=&#34;left&#34;&gt;&lt;strong&gt;针对肝胆管结石病，治疗方案包括手术治疗、药物治疗和辅助治疗等。具体治疗方案需要根据患者的具体情况而定。&lt;/strong&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;常见问题&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Q: 为什么是&#34;华驼&#34;而不是&#34;华佗&#34;？&lt;/p&gt; &lt;p&gt;A: 叫”驼“是因为我们的基模型LLaMA是美洲驼，Alpaca是羊驼，受他们名字的启发以及华佗的谐音梗，我们将我们的模型起名为华驼。&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Q: 有使用中医理论或者中医数据吗？&lt;/p&gt; &lt;p&gt;A: 目前还没有&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Q: 模型运行的结果不同、效果有限&lt;/p&gt; &lt;p&gt;A: 由于生成模型生成多样性的考量，多次运行的结果可能会有差异。当前开源的模型由于LLaMA及Alpaca中文语料有限，且知识结合的方式较为粗糙，目前我们在进行相关改进研究，完成后欢迎大家的关注。&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;项目参与者&lt;/h2&gt; &#xA;&lt;p&gt;本项目由哈尔滨工业大学社会计算与信息检索研究中心健康智能组&lt;a href=&#34;https://github.com/s65b40&#34;&gt;王昊淳&lt;/a&gt; 、&lt;a href=&#34;https://github.com/DYR1&#34;&gt;杜晏睿&lt;/a&gt;、&lt;a href=&#34;https://github.com/thinksoso&#34;&gt;刘驰&lt;/a&gt;、&lt;a href=&#34;https://github.com/RuiBai1999&#34;&gt;白睿&lt;/a&gt;、&lt;a href=&#34;https://github.com/rootnx&#34;&gt;席奴瓦&lt;/a&gt;、&lt;a href=&#34;https://github.com/Imsovegetable&#34;&gt;陈雨晗&lt;/a&gt;、&lt;a href=&#34;https://github.com/1278882181&#34;&gt;强泽文&lt;/a&gt;、&lt;a href=&#34;https://github.com/JianyuChen01&#34;&gt;陈健宇&lt;/a&gt;、&lt;a href=&#34;https://github.com/FlowolfzzZ&#34;&gt;李子健&lt;/a&gt;完成，指导教师为赵森栋副教授，秦兵教授以及刘挺教授。&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#SCIR-HI/Huatuo-Llama-Med-Chinese&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=SCIR-HI/Huatuo-Llama-Med-Chinese&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;致谢&lt;/h2&gt; &#xA;&lt;p&gt;本项目参考了以下开源项目，在此对相关项目和研究开发人员表示感谢。&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Facebook LLaMA: &lt;a href=&#34;https://github.com/facebookresearch/llama&#34;&gt;https://github.com/facebookresearch/llama&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Stanford Alpaca: &lt;a href=&#34;https://github.com/tatsu-lab/stanford_alpaca&#34;&gt;https://github.com/tatsu-lab/stanford_alpaca&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;alpaca-lora by @tloen: &lt;a href=&#34;https://github.com/tloen/alpaca-lora&#34;&gt;https://github.com/tloen/alpaca-lora&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;CMeKG &lt;a href=&#34;https://github.com/king-yyf/CMeKG_tools&#34;&gt;https://github.com/king-yyf/CMeKG_tools&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;文心一言 &lt;a href=&#34;https://yiyan.baidu.com/welcome&#34;&gt;https://yiyan.baidu.com/welcome&lt;/a&gt; 本项目的logo由文心一言自动生成&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;免责声明&lt;/h2&gt; &#xA;&lt;p&gt;本项目相关资源仅供学术研究之用，严禁用于商业用途。使用涉及第三方代码的部分时，请严格遵循相应的开源协议。模型生成的内容受模型计算、随机性和量化精度损失等因素影响，本项目无法对其准确性作出保证。本项目数据集绝大部分由模型生成，即使符合某些医学事实，也不能被用作实际医学诊断的依据。对于模型输出的任何内容，本项目不承担任何法律责任，亦不对因使用相关资源和输出结果而可能产生的任何损失承担责任。&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;如果你使用了本项目的数据或者代码，请声明引用&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA;@misc{wang2023huatuo,&#xA;&amp;nbsp; &amp;nbsp; &amp;nbsp; title={HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge},&#xA;&amp;nbsp; &amp;nbsp; &amp;nbsp; author={Haochun Wang and Chi Liu and Nuwa Xi and Zewen Qiang and Sendong Zhao and Bing Qin and Ting Liu},&#xA;&amp;nbsp; &amp;nbsp; &amp;nbsp; year={2023},&#xA;&amp;nbsp; &amp;nbsp; &amp;nbsp; eprint={2304.06975},&#xA;&amp;nbsp; &amp;nbsp; &amp;nbsp; archivePrefix={arXiv},&#xA;&amp;nbsp; &amp;nbsp; &amp;nbsp; primaryClass={cs.CL}&#xA;}&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>MakiNaruto/Automatic_ticket_purchase</title>
    <updated>2023-05-12T01:28:44Z</updated>
    <id>tag:github.com,2023-05-12:/MakiNaruto/Automatic_ticket_purchase</id>
    <link href="https://github.com/MakiNaruto/Automatic_ticket_purchase" rel="alternate"></link>
    <summary type="html">&lt;p&gt;大麦网抢票脚本&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;大麦抢票脚本 V2.1&lt;/h1&gt; &#xA;&lt;p&gt;更新&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;增加选座购买，暂时只支持抢购指定价格下的座位，且暂不支持连坐购买。&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;大麦抢票脚本 V2.0&lt;/h1&gt; &#xA;&lt;p&gt;在学习到接口相关知识后，决定改造之前的脚本。&lt;/p&gt; &#xA;&lt;h2&gt;功能介绍&lt;/h2&gt; &#xA;&lt;p&gt;之前的版本通过按钮操作，还要等待页面元素加载，效率低下。 此版本仅需登录时用到页面，通过selenium打开页面进行登录。其余操作均通过requests进行请求。&lt;/p&gt; &#xA;&lt;p&gt;ps: 暂不支持选座购买。&lt;/p&gt; &#xA;&lt;p&gt;其流程图如下:&lt;/p&gt; &#xA;&lt;img src=&#34;https://github.com/MakiNaruto/Automatic_ticket_purchase/raw/master/images/flow_chart.jpeg&#34; width=&#34;50%&#34; height=&#34;50%&#34;&gt; &#xA;&lt;h2&gt;准备工作&lt;/h2&gt; &#xA;&lt;h3&gt;1. 配置环境&lt;/h3&gt; &#xA;&lt;p&gt;1.1 安装所需要的环境&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;1.2 需要下载与系统安装对应的ChromeDriver驱动并配置(也可以改用其他浏览器驱动)，&lt;/p&gt; &#xA;&lt;p&gt;下载地址: &lt;a href=&#34;http://chromedriver.storage.googleapis.com/index.html&#34;&gt;http://chromedriver.storage.googleapis.com/index.html&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;1.3 配置驱动路径，默认在项目根目录下。&lt;/p&gt; &#xA;&lt;p&gt;例如：windows系统下，则重命名下载的chromedriver，将其重命名为chromedriver_windows&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def account_login():&#xA;    if platform.system().lower() == &#39;linux&#39;:&#xA;        chromedriver = os.path.join(os.getcwd(), &#39;chromedriver_linux&#39;)&#xA;    elif platform.system().lower() == &#39;windows&#39;:&#xA;        chromedriver = os.path.join(os.getcwd(), &#39;chromedriver_windows&#39;)&#xA;    else:&#xA;        chromedriver = os.path.join(os.getcwd(), &#39;chromedriver_mac&#39;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2. 运行&lt;/h3&gt; &#xA;&lt;p&gt;2.1 若采取账号方式，修改代码中下面的信息，进行抢票。&lt;/p&gt; &#xA;&lt;p&gt;item_id根据地区来确定,每一个城市对应不同的item_id。选择相应地区后将箭头指向的item_id填写到函数内。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-text&#34;&gt;def __init__(self):&#xA;    ...&#xA;    # 若选择账号登录方式，则需要填写&#xA;    self.login_id: str = &#39;account&#39;          # 大麦网登录账户名&#xA;    self.login_password: str = &#39;password&#39;   # 大麦网登录密码&#xA;    # 以下为抢票必须的参数&#xA;    self.item_id: int = 610820299671        # 商品id&#xA;    self.viewer: list = [&#39;viewer1&#39;]         # 在大麦网已填写的观影人&#xA;    self.buy_nums: int = 1                  # 购买影票数量, 需与观影人数量一致&#xA;    self.ticket_price: int = 180            # 购买指定票价&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/MakiNaruto/Automatic_ticket_purchase/master/images/item_id.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;2.2 运行&lt;/p&gt; &#xA;&lt;p&gt;初次登陆没有cookies，默认登录方式为账号密码登录方式，可改成其他方式进行登录，如扫码或短信登录。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 默认登录方式&#xA;python Automatic_ticket_purchase.py&#xA;# 指定其他方式登录&#xA;python Automatic_ticket_purchase.py --mode qr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;免责声明：详见MIT License，此仓库仅用于个人参考学习，但如他人用本仓库代码用于商业用途(鄙视黄牛)，侵犯到大麦网利益等，本人不承担任何责任。&lt;/p&gt;</summary>
  </entry>
</feed>