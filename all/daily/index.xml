<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-12T01:29:28Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Mail-0/Zero</title>
    <updated>2025-05-12T01:29:28Z</updated>
    <id>tag:github.com,2025-05-12:/Mail-0/Zero</id>
    <link href="https://github.com/Mail-0/Zero" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Experience email the way you want with 0 ‚Äì the first open source email app that puts your privacy and safety first. Join the discord: https://discord.gg/0email&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source srcset=&#34;apps/mail/public/white-icon.svg&#34; media=&#34;(prefers-color-scheme: dark)&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/Mail-0/Zero/main/apps/mail/public/black-icon.svg?sanitize=true&#34; alt=&#34;Zero Logo&#34; width=&#34;64&#34; style=&#34;background-color: #000; padding: 10px;&#34;&gt; &#xA; &lt;/picture&gt; &lt;/p&gt; &#xA;&lt;h1&gt;Zero&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fnizzyabi%2FMail0&amp;amp;env=DATABASE_URL,BETTER_AUTH_SECRET,BETTER_AUTH_URL,BETTER_AUTH_TRUSTED_ORIGINS,GOOGLE_CLIENT_ID,GOOGLE_CLIENT_SECRET,GOOGLE_REDIRECT_URI,GITHUB_CLIENT_ID,GITHUB_CLIENT_SECRET,GITHUB_REDIRECT_URI&amp;amp;envDescription=For%20more%20info%20on%20setting%20up%20your%20API%20keys%2C%20checkout%20the%20Readme%20below&amp;amp;envLink=https%3A%2F%2Fgithub.com%2Fnizzyabi%2FMail0%2Fblob%2Fmain%2FREADME.md&amp;amp;project-name=0&amp;amp;repository-name=0&amp;amp;redirect-url=0.email&amp;amp;demo-title=0&amp;amp;demo-description=An%20open%20source%20email%20app&amp;amp;demo-url=0.email&#34;&gt;&lt;img src=&#34;https://vercel.com/button&#34; alt=&#34;Deploy with Vercel&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;An Open-Source Gmail Alternative for the Future of Email&lt;/p&gt; &#xA;&lt;h2&gt;What is Zero?&lt;/h2&gt; &#xA;&lt;p&gt;Zero is an open-source AI email solution that gives users the power to &lt;strong&gt;self-host&lt;/strong&gt; their own email app while also integrating external services like Gmail and other email providers. Our goal is to modernize and improve emails through AI agents to truly modernize emails.&lt;/p&gt; &#xA;&lt;h2&gt;Why Zero?&lt;/h2&gt; &#xA;&lt;p&gt;Most email services today are either &lt;strong&gt;closed-source&lt;/strong&gt;, &lt;strong&gt;data-hungry&lt;/strong&gt;, or &lt;strong&gt;too complex to self-host&lt;/strong&gt;. 0.email is different:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚úÖ &lt;strong&gt;Open-Source&lt;/strong&gt; ‚Äì No hidden agendas, fully transparent.&lt;/li&gt; &#xA; &lt;li&gt;ü¶æ &lt;strong&gt;AI Driven&lt;/strong&gt; - Enhance your emails with Agents &amp;amp; LLMs.&lt;/li&gt; &#xA; &lt;li&gt;üîí &lt;strong&gt;Data Privacy First&lt;/strong&gt; ‚Äì Your emails, your data. Zero does not track, collect, or sell your data in any way. Please note: while we integrate with external services, the data passed through them is not under our control and falls under their respective privacy policies and terms of service.&lt;/li&gt; &#xA; &lt;li&gt;‚öôÔ∏è &lt;strong&gt;Self-Hosting Freedom&lt;/strong&gt; ‚Äì Run your own email app with ease.&lt;/li&gt; &#xA; &lt;li&gt;üì¨ &lt;strong&gt;Unified Inbox&lt;/strong&gt; ‚Äì Connect multiple email providers like Gmail, Outlook, and more.&lt;/li&gt; &#xA; &lt;li&gt;üé® &lt;strong&gt;Customizable UI &amp;amp; Features&lt;/strong&gt; ‚Äì Tailor your email experience the way you want it.&lt;/li&gt; &#xA; &lt;li&gt;üöÄ &lt;strong&gt;Developer-Friendly&lt;/strong&gt; ‚Äì Built with extensibility and integrations in mind.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Tech Stack&lt;/h2&gt; &#xA;&lt;p&gt;Zero is built with modern and reliable technologies:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Next.js, React, TypeScript, TailwindCSS, Shadcn UI&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: Node.js, Drizzle ORM&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: PostgreSQL&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Authentication&lt;/strong&gt;: Better Auth, Google OAuth&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;!-- - **Testing**: Jest, React Testing Library --&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;Required Versions:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://nodejs.org/en/download&#34;&gt;Node.js&lt;/a&gt; (v18 or higher)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://bun.sh&#34;&gt;Bun&lt;/a&gt; (v1.2 or higher)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/engine/install/&#34;&gt;Docker&lt;/a&gt; (v20 or higher)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Before running the application, you&#39;ll need to set up services and configure environment variables. For more details on environment variables, see the &lt;a href=&#34;https://raw.githubusercontent.com/Mail-0/Zero/main/#environment-variables&#34;&gt;Environment Variables&lt;/a&gt; section.&lt;/p&gt; &#xA;&lt;h3&gt;Setup Options&lt;/h3&gt; &#xA;&lt;p&gt;You can set up Zero in two ways:&lt;/p&gt; &#xA;&lt;details open&gt; &#xA; &lt;summary&gt;&lt;b&gt;Option 1: Standard Setup (Recommended)&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;h4&gt;Quick Start Guide&lt;/h4&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Clone and Install&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Clone the repository&#xA;git clone https://github.com/Mail-0/Zero.git&#xA;cd Zero&#xA;&#xA;# Install dependencies&#xA;bun install&#xA;&#xA;# Start database locally&#xA;bun docker:up&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set Up Environment&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Copy &lt;code&gt;.env.example&lt;/code&gt; to &lt;code&gt;.env&lt;/code&gt; in project root &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cp .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;    &lt;li&gt;Configure your environment variables (see below)&lt;/li&gt; &#xA;    &lt;li&gt;Start the database with the provided docker compose setup: &lt;code&gt;bun docker:up&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Initialize the database: &lt;code&gt;bun db:push&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the App&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun dev&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open in Browser&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Visit &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;&lt;/p&gt; &lt;/li&gt;&#xA; &lt;/ol&gt;&#xA;&lt;/details&gt;   &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;Option 2: Dev Container Setup (For VS Code Users)&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;p&gt;This option uses VS Code&#39;s Dev Containers feature to provide a fully configured development environment with all dependencies pre-installed. It&#39;s great for ensuring everyone on the team has the same setup.&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;Docker&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://code.visualstudio.com/&#34;&gt;VS Code&lt;/a&gt; or compatible editor&lt;/li&gt; &#xA;    &lt;li&gt;&lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers&#34;&gt;Dev Containers extension&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Open in Dev Container&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Clone the repository: &lt;code&gt;git clone https://github.com/Mail-0/Zero.git&lt;/code&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Open the folder in VS Code&lt;/li&gt; &#xA;    &lt;li&gt;When prompted, click &#34;Reopen in Container&#34; or run the &#34;Dev Containers: Open Folder in Container&#34; command&lt;/li&gt; &#xA;    &lt;li&gt;VS Code will build and start the dev container (this may take a few minutes the first time)&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Access the App&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;The app will be available at &lt;a href=&#34;http://localhost:3000&#34;&gt;http://localhost:3000&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;&lt;strong&gt;Troubleshooting&lt;/strong&gt;&lt;/p&gt; &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;If you encounter issues with the container, try rebuilding it using the &#34;Dev Containers: Rebuild Container&#34; command&lt;/li&gt; &#xA;    &lt;li&gt;For dependency issues inside the container: &lt;code&gt;bash rm -rf node_modules rm bun.lockb bun install &lt;/code&gt; &lt;/li&gt;&#xA;   &lt;/ul&gt;&lt;/li&gt;&#xA; &lt;/ol&gt;&#xA;&lt;/details&gt;     &#xA;&lt;h3&gt;Environment Setup&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Better Auth Setup&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Open the &lt;code&gt;.env&lt;/code&gt; file and change the BETTER_AUTH_SECRET to a random string. (Use &lt;code&gt;openssl rand -hex 32&lt;/code&gt; to generate a 32 character string)&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;BETTER_AUTH_SECRET=your_secret_key&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Google OAuth Setup&lt;/strong&gt; (Required for Gmail integration)&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;Go to &lt;a href=&#34;https://console.cloud.google.com&#34;&gt;Google Cloud Console&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Create a new project&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Add the following APIs in your Google Cloud Project: &lt;a href=&#34;https://console.cloud.google.com/apis/library/people.googleapis.com&#34;&gt;People API&lt;/a&gt;, &lt;a href=&#34;https://console.cloud.google.com/apis/library/gmail.googleapis.com&#34;&gt;Gmail API&lt;/a&gt;&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Use the links above and click &#39;Enable&#39; or&lt;/li&gt; &#xA;     &lt;li&gt;Go to &#39;APIs and Services&#39; &amp;gt; &#39;Enable APIs and Services&#39; &amp;gt; Search for &#39;Google People API&#39; and click &#39;Enable&#39;&lt;/li&gt; &#xA;     &lt;li&gt;Go to &#39;APIs and Services&#39; &amp;gt; &#39;Enable APIs and Services&#39; &amp;gt; Search for &#39;Gmail API&#39; and click &#39;Enable&#39;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Enable the Google OAuth2 API&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Create OAuth 2.0 credentials (Web application type)&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Add authorized redirect URIs:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Development: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;code&gt;http://localhost:3000/api/auth/callback/google&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;     &lt;li&gt;Production: &#xA;      &lt;ul&gt; &#xA;       &lt;li&gt;&lt;code&gt;https://your-production-url/api/auth/callback/google&lt;/code&gt;&lt;/li&gt; &#xA;      &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Add to &lt;code&gt;.env&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;GOOGLE_CLIENT_ID=your_client_id&#xA;GOOGLE_CLIENT_SECRET=your_client_secret&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;Add yourself as a test user:&lt;/p&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;Go to &lt;a href=&#34;https://console.cloud.google.com/auth/audience&#34;&gt;&lt;code&gt;Audience&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;Under &#39;Test users&#39; click &#39;Add Users&#39;&lt;/li&gt; &#xA;     &lt;li&gt;Add your email and click &#39;Save&#39;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!WARNING] The authorized redirect URIs in Google Cloud Console must match &lt;strong&gt;exactly&lt;/strong&gt; what you configure in the &lt;code&gt;.env&lt;/code&gt;, including the protocol (http/https), domain, and path - these are provided above.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Environment Variables&lt;/h3&gt; &#xA;&lt;p&gt;Copy &lt;code&gt;.env.example&lt;/code&gt; located in the project folder to &lt;code&gt;.env&lt;/code&gt; in the same folder and configure the following variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-env&#34;&gt;# Auth&#xA;BETTER_AUTH_SECRET=     # Required: Secret key for authentication&#xA;&#xA;# Google OAuth (Required for Gmail integration)&#xA;GOOGLE_CLIENT_ID=       # Required for Gmail integration&#xA;GOOGLE_CLIENT_SECRET=   # Required for Gmail integration&#xA;&#xA;# Database&#xA;DATABASE_URL=           # Required: PostgreSQL connection string for backend connection&#xA;&#xA;# Redis&#xA;REDIS_URL=              # Redis URL for caching (http://localhost:8079 for local dev)&#xA;REDIS_TOKEN=            # Redis token (upstash-local-token for local dev)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For local development a connection string example is provided in the &lt;code&gt;.env.example&lt;/code&gt; file located in the same folder as the database.&lt;/p&gt; &#xA;&lt;h3&gt;Database Setup&lt;/h3&gt; &#xA;&lt;p&gt;Zero uses PostgreSQL for storing data. Here&#39;s how to set it up:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Start the Database&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Run this command to start a local PostgreSQL instance:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun docker:up&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This creates a database with:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Name: &lt;code&gt;zerodotemail&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Username: &lt;code&gt;postgres&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Password: &lt;code&gt;postgres&lt;/code&gt;&lt;/li&gt; &#xA;   &lt;li&gt;Port: &lt;code&gt;5432&lt;/code&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set Up Database Connection&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Make sure your database connection string is in &lt;code&gt;.env&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;For local development use:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;DATABASE_URL=&#34;postgresql://postgres:postgres@localhost:5432/zerodotemail&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Database Commands&lt;/strong&gt;&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Set up database tables&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun db:push&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Create migration files&lt;/strong&gt; (after schema changes):&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun db:generate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;Apply migrations&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun db:migrate&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;strong&gt;View database content&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;bun db:studio&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;    &lt;blockquote&gt; &#xA;     &lt;p&gt;If you run &lt;code&gt;bun dev&lt;/code&gt; in your terminal, the studio command should be automatically running with the app.&lt;/p&gt; &#xA;    &lt;/blockquote&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;Please refer to the &lt;a href=&#34;https://raw.githubusercontent.com/Mail-0/Zero/main/.github/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;d like to help with translating Zero to other languages, check out our &lt;a href=&#34;https://raw.githubusercontent.com/Mail-0/Zero/main/.github/TRANSLATION.md&#34;&gt;translation guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#Mail-0/Zero&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Mail-0/Zero&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;This project wouldn&#39;t be possible without these awesome companies&lt;/h2&gt; &#xA;&lt;div style=&#34;display: flex; justify-content: center;&#34;&gt; &#xA; &lt;a href=&#34;https://vercel.com&#34; style=&#34;text-decoration: none;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Mail-0/Zero/main/public/vercel.png&#34; alt=&#34;Vercel&#34; width=&#34;96&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://better-auth.com&#34; style=&#34;text-decoration: none;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Mail-0/Zero/main/public/better-auth.png&#34; alt=&#34;Better Auth&#34; width=&#34;96&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://orm.drizzle.team&#34; style=&#34;text-decoration: none;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Mail-0/Zero/main/public/drizzle-orm.png&#34; alt=&#34;Drizzle ORM&#34; width=&#34;96&#34;&gt; &lt;/a&gt; &#xA; &lt;a href=&#34;https://coderabbit.com&#34; style=&#34;text-decoration: none;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/Mail-0/Zero/main/public/coderabbit.png&#34; alt=&#34;Coderabbit AI&#34; width=&#34;96&#34;&gt; &lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;ü§ç The team&lt;/h2&gt; &#xA;&lt;p&gt;Curious who makes Zero? Here are our &lt;a href=&#34;https://0.email/contributors&#34;&gt;contributors and maintainers&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ggml-org/llama.cpp</title>
    <updated>2025-05-12T01:29:28Z</updated>
    <id>tag:github.com,2025-05-12:/ggml-org/llama.cpp</id>
    <link href="https://github.com/ggml-org/llama.cpp" rel="alternate"></link>
    <summary type="html">&lt;p&gt;LLM inference in C/C++&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;llama.cpp&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png&#34; alt=&#34;llama&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://opensource.org/licenses/MIT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/license-MIT-blue.svg?sanitize=true&#34; alt=&#34;License: MIT&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml&#34;&gt;&lt;img src=&#34;https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg?sanitize=true&#34; alt=&#34;Server&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/users/ggerganov/projects/7&#34;&gt;Roadmap&lt;/a&gt; / &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/3471&#34;&gt;Project status&lt;/a&gt; / &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/205&#34;&gt;Manifesto&lt;/a&gt; / &lt;a href=&#34;https://github.com/ggml-org/ggml&#34;&gt;ggml&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Inference of Meta&#39;s &lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA&lt;/a&gt; model (and others) in pure C/C++&lt;/p&gt; &#xA;&lt;h2&gt;Recent API changes&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/issues/9289&#34;&gt;Changelog for &lt;code&gt;libllama&lt;/code&gt; API&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/issues/9291&#34;&gt;Changelog for &lt;code&gt;llama-server&lt;/code&gt; REST API&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hot topics&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üî• Multimodal support arrived in &lt;code&gt;llama-server&lt;/code&gt;: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/12898&#34;&gt;#12898&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/multimodal.md&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;GGML developer experience survey (organized and reviewed by NVIDIA):&lt;/strong&gt; &lt;a href=&#34;https://forms.gle/Gasw3cRgyhNEnrwK9&#34;&gt;link&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A new binary &lt;code&gt;llama-mtmd-cli&lt;/code&gt; is introduced to replace &lt;code&gt;llava-cli&lt;/code&gt;, &lt;code&gt;minicpmv-cli&lt;/code&gt;, &lt;code&gt;gemma3-cli&lt;/code&gt; (&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/13012&#34;&gt;#13012&lt;/a&gt;) and &lt;code&gt;qwen2vl-cli&lt;/code&gt; (&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/13141&#34;&gt;#13141&lt;/a&gt;), &lt;code&gt;libllava&lt;/code&gt; will be deprecated&lt;/li&gt; &#xA; &lt;li&gt;VS Code extension for FIM completions: &lt;a href=&#34;https://github.com/ggml-org/llama.vscode&#34;&gt;https://github.com/ggml-org/llama.vscode&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Universal &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/function-calling.md&#34;&gt;tool call support&lt;/a&gt; in &lt;code&gt;llama-server&lt;/code&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/9639&#34;&gt;https://github.com/ggml-org/llama.cpp/pull/9639&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Vim/Neovim plugin for FIM completions: &lt;a href=&#34;https://github.com/ggml-org/llama.vim&#34;&gt;https://github.com/ggml-org/llama.vim&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Introducing GGUF-my-LoRA &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/10123&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face Inference Endpoints now support GGUF out of the box! &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/9669&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Hugging Face GGUF editor: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/9268&#34;&gt;discussion&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/spaces/CISCai/gguf-editor&#34;&gt;tool&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Description&lt;/h2&gt; &#xA;&lt;p&gt;The main goal of &lt;code&gt;llama.cpp&lt;/code&gt; is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Plain C/C++ implementation without any dependencies&lt;/li&gt; &#xA; &lt;li&gt;Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks&lt;/li&gt; &#xA; &lt;li&gt;AVX, AVX2, AVX512 and AMX support for x86 architectures&lt;/li&gt; &#xA; &lt;li&gt;1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use&lt;/li&gt; &#xA; &lt;li&gt;Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads MTT GPUs via MUSA)&lt;/li&gt; &#xA; &lt;li&gt;Vulkan and SYCL backend support&lt;/li&gt; &#xA; &lt;li&gt;CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The &lt;code&gt;llama.cpp&lt;/code&gt; project is the main playground for developing new features for the &lt;a href=&#34;https://github.com/ggml-org/ggml&#34;&gt;ggml&lt;/a&gt; library.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Models&lt;/summary&gt; &#xA; &lt;p&gt;Typically finetunes of the base models below are supported as well.&lt;/p&gt; &#xA; &lt;p&gt;Instructions for adding support for new models: &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/HOWTO-add-model.md&#34;&gt;HOWTO-add-model.md&lt;/a&gt;&lt;/p&gt; &#xA; &lt;h4&gt;Text-only&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLaMA ü¶ô&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLaMA 2 ü¶ôü¶ô&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; LLaMA 3 ü¶ôü¶ôü¶ô&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/mistralai/Mistral-7B-v0.1&#34;&gt;Mistral 7B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=mistral-ai/Mixtral&#34;&gt;Mixtral MoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/databricks/dbrx-instruct&#34;&gt;DBRX&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=tiiuae/falcon&#34;&gt;Falcon&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca&#34;&gt;Chinese LLaMA / Alpaca&lt;/a&gt; and &lt;a href=&#34;https://github.com/ymcui/Chinese-LLaMA-Alpaca-2&#34;&gt;Chinese LLaMA-2 / Alpaca-2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/bofenghuang/vigogne&#34;&gt;Vigogne (French)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/5423&#34;&gt;BERT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://bair.berkeley.edu/blog/2023/04/03/koala/&#34;&gt;Koala&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=baichuan-inc/Baichuan&#34;&gt;Baichuan 1 &amp;amp; 2&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/hiyouga/baichuan-7b-sft&#34;&gt;derivations&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=BAAI/Aquila&#34;&gt;Aquila 1 &amp;amp; 2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/3187&#34;&gt;Starcoder models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/smallcloudai/Refact-1_6B-fim&#34;&gt;Refact&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/3417&#34;&gt;MPT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/3553&#34;&gt;Bloom&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=01-ai/Yi&#34;&gt;Yi models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/stabilityai&#34;&gt;StableLM models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=deepseek-ai/deepseek&#34;&gt;Deepseek models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=Qwen/Qwen&#34;&gt;Qwen models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/3557&#34;&gt;PLaMo-13B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=microsoft/phi&#34;&gt;Phi models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/11003&#34;&gt;PhiMoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/gpt2&#34;&gt;GPT-2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/5118&#34;&gt;Orion 14B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=internlm2&#34;&gt;InternLM2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/WisdomShell/codeshell&#34;&gt;CodeShell&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://ai.google.dev/gemma&#34;&gt;Gemma&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/state-spaces/mamba&#34;&gt;Mamba&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/keyfan/grok-1-hf&#34;&gt;Grok-1&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=xverse&#34;&gt;Xverse&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=CohereForAI/c4ai-command-r&#34;&gt;Command-R models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=sea-lion&#34;&gt;SEA-LION&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/GritLM/GritLM-7B&#34;&gt;GritLM-7B&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/GritLM/GritLM-8x7B&#34;&gt;GritLM-8x7B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://allenai.org/olmo&#34;&gt;OLMo&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://allenai.org/olmo&#34;&gt;OLMo 2&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/allenai/OLMoE-1B-7B-0924&#34;&gt;OLMoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330&#34;&gt;Granite models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/EleutherAI/gpt-neox&#34;&gt;GPT-NeoX&lt;/a&gt; + &lt;a href=&#34;https://github.com/EleutherAI/pythia&#34;&gt;Pythia&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520&#34;&gt;Snowflake-Arctic MoE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=Smaug&#34;&gt;Smaug&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/LumiOpen/Poro-34B&#34;&gt;Poro 34B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/1bitLLM&#34;&gt;Bitnet b1.58 models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=flan-t5&#34;&gt;Flan T5&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca&#34;&gt;Open Elm models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/THUDM/chatglm3-6b&#34;&gt;ChatGLM3-6b&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/THUDM/glm-4-9b&#34;&gt;ChatGLM4-9b&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/THUDM/glm-edge-1.5b-chat&#34;&gt;GLMEdge-1.5b&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/THUDM/glm-edge-4b-chat&#34;&gt;GLMEdge-4b&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e&#34;&gt;GLM-4-0414&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966&#34;&gt;SmolLM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct&#34;&gt;EXAONE-3.0-7.8B-Instruct&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a&#34;&gt;FalconMamba Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/inceptionai/jais-13b-chat&#34;&gt;Jais&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a&#34;&gt;Bielik-11B-v2.3&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/BlinkDL/RWKV-LM&#34;&gt;RWKV-6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1&#34;&gt;QRWKV-6&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct&#34;&gt;GigaChat-20B-A3B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/trillionlabs/Trillion-7B-preview&#34;&gt;Trillion-7B-preview&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32&#34;&gt;Ling models&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA; &lt;h4&gt;Multimodal&lt;/h4&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e&#34;&gt;LLaVA 1.5 models&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2&#34;&gt;LLaVA 1.6 models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=SkunkworksAI/Bakllava&#34;&gt;BakLLaVA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/NousResearch/Obsidian-3B-V0.5&#34;&gt;Obsidian&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=Lin-Chen/ShareGPT4V&#34;&gt;ShareGPT4V&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=mobileVLM&#34;&gt;MobileVLM 1.7B/3B models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=Yi-VL&#34;&gt;Yi-VL&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=MiniCPM&#34;&gt;Mini CPM&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/vikhyatk/moondream2&#34;&gt;Moondream&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://github.com/BAAI-DCAI/Bunny&#34;&gt;Bunny&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/models?search=glm-edge&#34;&gt;GLM-EDGE&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;a href=&#34;https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d&#34;&gt;Qwen2-VL&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Bindings&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Python: &lt;a href=&#34;https://github.com/abetlen/llama-cpp-python&#34;&gt;abetlen/llama-cpp-python&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Go: &lt;a href=&#34;https://github.com/go-skynet/go-llama.cpp&#34;&gt;go-skynet/go-llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Node.js: &lt;a href=&#34;https://github.com/withcatai/node-llama-cpp&#34;&gt;withcatai/node-llama-cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;JS/TS (llama.cpp server client): &lt;a href=&#34;https://modelfusion.dev/integration/model-provider/llamacpp&#34;&gt;lgrammel/modelfusion&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;JS/TS (Programmable Prompt Engine CLI): &lt;a href=&#34;https://github.com/offline-ai/cli&#34;&gt;offline-ai/cli&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;JavaScript/Wasm (works in browser): &lt;a href=&#34;https://github.com/tangledgroup/llama-cpp-wasm&#34;&gt;tangledgroup/llama-cpp-wasm&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Typescript/Wasm (nicer API, available on npm): &lt;a href=&#34;https://github.com/ngxson/wllama&#34;&gt;ngxson/wllama&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Ruby: &lt;a href=&#34;https://github.com/yoshoku/llama_cpp.rb&#34;&gt;yoshoku/llama_cpp.rb&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Rust (more features): &lt;a href=&#34;https://github.com/edgenai/llama_cpp-rs&#34;&gt;edgenai/llama_cpp-rs&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Rust (nicer API): &lt;a href=&#34;https://github.com/mdrokz/rust-llama.cpp&#34;&gt;mdrokz/rust-llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Rust (more direct bindings): &lt;a href=&#34;https://github.com/utilityai/llama-cpp-rs&#34;&gt;utilityai/llama-cpp-rs&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Rust (automated build from crates.io): &lt;a href=&#34;https://github.com/ShelbyJenkins/llm_client&#34;&gt;ShelbyJenkins/llm_client&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;C#/.NET: &lt;a href=&#34;https://github.com/SciSharp/LLamaSharp&#34;&gt;SciSharp/LLamaSharp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;C#/VB.NET (more features - community license): &lt;a href=&#34;https://docs.lm-kit.com/lm-kit-net/index.html&#34;&gt;LM-Kit.NET&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Scala 3: &lt;a href=&#34;https://github.com/donderom/llm4s&#34;&gt;donderom/llm4s&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Clojure: &lt;a href=&#34;https://github.com/phronmophobic/llama.clj&#34;&gt;phronmophobic/llama.clj&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;React Native: &lt;a href=&#34;https://github.com/mybigday/llama.rn&#34;&gt;mybigday/llama.rn&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Java: &lt;a href=&#34;https://github.com/kherud/java-llama.cpp&#34;&gt;kherud/java-llama.cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Zig: &lt;a href=&#34;https://github.com/Deins/llama.cpp.zig&#34;&gt;deins/llama.cpp.zig&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Flutter/Dart: &lt;a href=&#34;https://github.com/netdur/llama_cpp_dart&#34;&gt;netdur/llama_cpp_dart&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Flutter: &lt;a href=&#34;https://github.com/xuegao-tzx/Fllama&#34;&gt;xuegao-tzx/Fllama&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;PHP (API bindings and features built on top of llama.cpp): &lt;a href=&#34;https://github.com/distantmagic/resonance&#34;&gt;distantmagic/resonance&lt;/a&gt; &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/pull/6326&#34;&gt;(more info)&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Guile Scheme: &lt;a href=&#34;https://savannah.nongnu.org/projects/guile-llama-cpp&#34;&gt;guile_llama_cpp&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Swift &lt;a href=&#34;https://github.com/srgtuszy/llama-cpp-swift&#34;&gt;srgtuszy/llama-cpp-swift&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Swift &lt;a href=&#34;https://github.com/ShenghaiWang/SwiftLlama&#34;&gt;ShenghaiWang/SwiftLlama&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Delphi &lt;a href=&#34;https://github.com/Embarcadero/llama-cpp-delphi&#34;&gt;Embarcadero/llama-cpp-delphi&lt;/a&gt;&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;UIs&lt;/summary&gt; &#xA; &lt;p&gt;&lt;em&gt;(to have a project listed here, it should clearly state that it depends on &lt;code&gt;llama.cpp&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/yaroslavyaroslav/OpenAI-sublime-text&#34;&gt;AI Sublime Text plugin&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/cztomsik/ava&#34;&gt;cztomsik/ava&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/alexpinel/Dot&#34;&gt;Dot&lt;/a&gt; (GPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ylsdamxssjxxdd/eva&#34;&gt;eva&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/iohub/coLLaMA&#34;&gt;iohub/collama&lt;/a&gt; (Apache-2.0)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/janhq/jan&#34;&gt;janhq/jan&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/johnbean393/Sidekick&#34;&gt;johnbean393/Sidekick&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/zhouwg/kantv?tab=readme-ov-file&#34;&gt;KanTV&lt;/a&gt; (Apache-2.0)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/firatkiral/kodibot&#34;&gt;KodiBot&lt;/a&gt; (GPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.vim&#34;&gt;llama.vim&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/abgulati/LARS&#34;&gt;LARS&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/vietanhdev/llama-assistant&#34;&gt;Llama Assistant&lt;/a&gt; (GPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/guinmoon/LLMFarm?tab=readme-ov-file&#34;&gt;LLMFarm&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/undreamai/LLMUnity&#34;&gt;LLMUnity&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://lmstudio.ai/&#34;&gt;LMStudio&lt;/a&gt; (proprietary)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/mudler/LocalAI&#34;&gt;LocalAI&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/LostRuins/koboldcpp&#34;&gt;LostRuins/koboldcpp&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://mindmac.app&#34;&gt;MindMac&lt;/a&gt; (proprietary)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/MindWorkAI/AI-Studio&#34;&gt;MindWorkAI/AI-Studio&lt;/a&gt; (FSL-1.1-MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Mobile-Artificial-Intelligence/maid&#34;&gt;Mobile-Artificial-Intelligence/maid&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/Mozilla-Ocho/llamafile&#34;&gt;Mozilla-Ocho/llamafile&lt;/a&gt; (Apache-2.0)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/nat/openplayground&#34;&gt;nat/openplayground&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/nomic-ai/gpt4all&#34;&gt;nomic-ai/gpt4all&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ollama/ollama&#34;&gt;ollama/ollama&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui&#34;&gt;oobabooga/text-generation-webui&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/a-ghorbani/pocketpal-ai&#34;&gt;PocketPal AI&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/psugihara/FreeChat&#34;&gt;psugihara/FreeChat&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/ptsochantaris/emeltal&#34;&gt;ptsochantaris/emeltal&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/pythops/tenere&#34;&gt;pythops/tenere&lt;/a&gt; (AGPL)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/containers/ramalama&#34;&gt;ramalama&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/semperai/amica&#34;&gt;semperai/amica&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/withcatai/catai&#34;&gt;withcatai/catai&lt;/a&gt; (MIT)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/blackhole89/autopen&#34;&gt;Autopen&lt;/a&gt; (GPL)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Tools&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/akx/ggify&#34;&gt;akx/ggify&lt;/a&gt; ‚Äì download PyTorch models from HuggingFace Hub and convert them to GGML&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/akx/ollama-dl&#34;&gt;akx/ollama-dl&lt;/a&gt; ‚Äì download models from the Ollama library to be used directly with llama.cpp&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/crashr/gppm&#34;&gt;crashr/gppm&lt;/a&gt; ‚Äì launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser&#34;&gt;gpustack/gguf-parser&lt;/a&gt; - review/check the GGUF file and estimate the memory usage&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902&#34;&gt;Styled Lines&lt;/a&gt; (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Infrastructure&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/distantmagic/paddler&#34;&gt;Paddler&lt;/a&gt; - Stateful load balancer custom-tailored for llama.cpp&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/gpustack/gpustack&#34;&gt;GPUStack&lt;/a&gt; - Manage GPU clusters for running LLMs&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/onicai/llama_cpp_canister&#34;&gt;llama_cpp_canister&lt;/a&gt; - llama.cpp as a smart contract on the Internet Computer, using WebAssembly&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/mostlygeek/llama-swap&#34;&gt;llama-swap&lt;/a&gt; - transparent proxy that adds automatic model switching with llama-server&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/kalavai-net/kalavai-client&#34;&gt;Kalavai&lt;/a&gt; - Crowdsource end to end LLM deployment at any scale&lt;/li&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/InftyAI/llmaz&#34;&gt;llmaz&lt;/a&gt; - ‚ò∏Ô∏è Easy, advanced inference platform for large language models on Kubernetes.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Games&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;a href=&#34;https://github.com/MorganRO8/Lucys_Labyrinth&#34;&gt;Lucy&#39;s Labyrinth&lt;/a&gt; - A simple maze game where agents controlled by an AI model will try to trick you.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Supported backends&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Backend&lt;/th&gt; &#xA;   &lt;th&gt;Target devices&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#metal-build&#34;&gt;Metal&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Apple Silicon&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#blas-build&#34;&gt;BLAS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;All&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/BLIS.md&#34;&gt;BLIS&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;All&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/SYCL.md&#34;&gt;SYCL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Intel and Nvidia GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#musa&#34;&gt;MUSA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Moore Threads MTT GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cuda&#34;&gt;CUDA&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Nvidia GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#hip&#34;&gt;HIP&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AMD GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#vulkan&#34;&gt;Vulkan&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md#cann&#34;&gt;CANN&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Ascend NPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/backend/OPENCL.md&#34;&gt;OpenCL&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Adreno GPU&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc&#34;&gt;RPC&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;All&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Building the project&lt;/h2&gt; &#xA;&lt;p&gt;The main product of this project is the &lt;code&gt;llama&lt;/code&gt; library. Its C-style interface can be found in &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/include/llama.h&#34;&gt;include/llama.h&lt;/a&gt;. The project also includes many example programs and tools using the &lt;code&gt;llama&lt;/code&gt; library. The examples range from simple, minimal code snippets to sophisticated sub-projects such as an OpenAI-compatible HTTP server. Possible methods for obtaining the binaries:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Clone this repository and build locally, see &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&#34;&gt;how to build&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;On MacOS or Linux, install &lt;code&gt;llama.cpp&lt;/code&gt; via &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/install.md&#34;&gt;brew, flox or nix&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use a Docker image, see &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&#34;&gt;documentation for Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Download pre-built binaries from &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/releases&#34;&gt;releases&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Obtaining and quantizing models&lt;/h2&gt; &#xA;&lt;p&gt;The &lt;a href=&#34;https://huggingface.co&#34;&gt;Hugging Face&lt;/a&gt; platform hosts a &lt;a href=&#34;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&#34;&gt;number of LLMs&lt;/a&gt; compatible with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?library=gguf&amp;amp;sort=trending&#34;&gt;Trending&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?sort=trending&amp;amp;search=llama+gguf&#34;&gt;LLaMA&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can either manually download the GGUF file or directly use any &lt;code&gt;llama.cpp&lt;/code&gt;-compatible models from &lt;a href=&#34;https://huggingface.co/&#34;&gt;Hugging Face&lt;/a&gt; or other model hosting sites, such as &lt;a href=&#34;https://modelscope.cn/&#34;&gt;ModelScope&lt;/a&gt;, by using this CLI argument: &lt;code&gt;-hf &amp;lt;user&amp;gt;/&amp;lt;model&amp;gt;[:quant]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;By default, the CLI would download from Hugging Face, you can switch to other options with the environment variable &lt;code&gt;MODEL_ENDPOINT&lt;/code&gt;. For example, you may opt to downloading model checkpoints from ModelScope or other model sharing communities by setting the environment variable, e.g. &lt;code&gt;MODEL_ENDPOINT=https://www.modelscope.cn/&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;After downloading a model, use the CLI tools to run it locally - see below.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt; requires the model to be stored in the &lt;a href=&#34;https://github.com/ggml-org/ggml/raw/master/docs/gguf.md&#34;&gt;GGUF&lt;/a&gt; file format. Models in other data formats can be converted to GGUF using the &lt;code&gt;convert_*.py&lt;/code&gt; Python scripts in this repo.&lt;/p&gt; &#xA;&lt;p&gt;The Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with &lt;code&gt;llama.cpp&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://huggingface.co/spaces/ggml-org/gguf-my-repo&#34;&gt;GGUF-my-repo space&lt;/a&gt; to convert to GGUF format and quantize model weights to smaller sizes&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://huggingface.co/spaces/ggml-org/gguf-my-lora&#34;&gt;GGUF-my-LoRA space&lt;/a&gt; to convert LoRA adapters to GGUF format (more info: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/10123&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/10123&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://huggingface.co/spaces/CISCai/gguf-editor&#34;&gt;GGUF-editor space&lt;/a&gt; to edit GGUF meta data in the browser (more info: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/9268&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/9268&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Use the &lt;a href=&#34;https://ui.endpoints.huggingface.co/&#34;&gt;Inference Endpoints&lt;/a&gt; to directly host &lt;code&gt;llama.cpp&lt;/code&gt; in the cloud (more info: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/9669&#34;&gt;https://github.com/ggml-org/llama.cpp/discussions/9669&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To learn more about model quantization, &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/quantize/README.md&#34;&gt;read this documentation&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/main&#34;&gt;&lt;code&gt;llama-cli&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A CLI tool for accessing and experimenting with most of &lt;code&gt;llama.cpp&lt;/code&gt;&#39;s functionality.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary&gt;Run in conversation mode&lt;/summary&gt; &#xA;   &lt;p&gt;Models with a built-in chat template will automatically activate conversation mode. If this doesn&#39;t occur, you can manually enable it by adding &lt;code&gt;-cnv&lt;/code&gt; and specifying a suitable chat template with &lt;code&gt;--chat-template NAME&lt;/code&gt;&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-cli -m model.gguf&#xA;&#xA;# &amp;gt; hi, who are you?&#xA;# Hi there! I&#39;m your helpful assistant! I&#39;m an AI-powered chatbot designed to assist and provide information to users like you. I&#39;m here to help answer your questions, provide guidance, and offer support on a wide range of topics. I&#39;m a friendly and knowledgeable AI, and I&#39;m always happy to help with anything you need. What&#39;s on your mind, and how can I assist you today?&#xA;#&#xA;# &amp;gt; what is 1+1?&#xA;# Easy peasy! The answer to 1+1 is... 2!&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Run in conversation mode with custom chat template&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use the &#34;chatml&#34; template (use -h to see the list of supported templates)&#xA;llama-cli -m model.gguf -cnv --chat-template chatml&#xA;&#xA;# use a custom template&#xA;llama-cli -m model.gguf -cnv --in-prefix &#39;User: &#39; --reverse-prompt &#39;User:&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Run simple text completion&lt;/summary&gt; &#xA;   &lt;p&gt;To disable conversation mode explicitly, use &lt;code&gt;-no-cnv&lt;/code&gt;&lt;/p&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-cli -m model.gguf -p &#34;I believe the meaning of life is&#34; -n 128 -no-cnv&#xA;&#xA;# I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don&#39;t align with societal expectations. I think that&#39;s what I love about yoga ‚Äì it&#39;s not just a physical practice, but a spiritual one too. It&#39;s about connecting with yourself, listening to your inner voice, and honoring your own unique journey.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Constrain the output with a custom grammar&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p &#39;Request: schedule a call at 8pm; Command:&#39;&#xA;&#xA;# {&#34;appointmentTime&#34;: &#34;8pm&#34;, &#34;appointmentDetails&#34;: &#34;schedule a a call&#34;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;   &lt;p&gt;The &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/&#34;&gt;grammars/&lt;/a&gt; folder contains a handful of sample grammars. To write your own, check out the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&#34;&gt;GBNF Guide&lt;/a&gt;.&lt;/p&gt; &#xA;   &lt;p&gt;For authoring more complex JSON grammars, check out &lt;a href=&#34;https://grammar.intrinsiclabs.ai/&#34;&gt;https://grammar.intrinsiclabs.ai/&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server&#34;&gt;&lt;code&gt;llama-server&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A lightweight, &lt;a href=&#34;https://github.com/openai/openai-openapi&#34;&gt;OpenAI API&lt;/a&gt; compatible, HTTP server for serving LLMs.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary&gt;Start a local HTTP server with default configuration on port 8080&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-server -m model.gguf --port 8080&#xA;&#xA;# Basic web UI can be accessed via browser: http://localhost:8080&#xA;# Chat completion endpoint: http://localhost:8080/v1/chat/completions&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Support multiple-users and parallel decoding&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# up to 4 concurrent requests, each with 4096 max context&#xA;llama-server -m model.gguf -c 16384 -np 4&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Enable speculative decoding&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# the draft.gguf model should be a small variant of the target model.gguf&#xA;llama-server -m model.gguf -md draft.gguf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Serve an embedding model&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use the /embedding endpoint&#xA;llama-server -m model.gguf --embedding --pooling cls -ub 8192&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Serve a reranking model&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# use the /reranking endpoint&#xA;llama-server -m model.gguf --reranking&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Constrain all outputs with a grammar&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# custom grammar&#xA;llama-server -m model.gguf --grammar-file grammar.gbnf&#xA;&#xA;# JSON&#xA;llama-server -m model.gguf --grammar-file grammars/json.gbnf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/perplexity&#34;&gt;&lt;code&gt;llama-perplexity&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A tool for measuring the perplexity &lt;a href=&#34;%5Bhttps://huggingface.co/docs/transformers/perplexity%5D(https://huggingface.co/docs/transformers/perplexity)&#34;&gt;^1&lt;/a&gt; (and other quality metrics) of a model over a given text.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary&gt;Measure the perplexity over a text file&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-perplexity -m model.gguf -f file.txt&#xA;&#xA;# [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...&#xA;# Final estimate: PPL = 5.4007 +/- 0.67339&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Measure KL divergence&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# TODO&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/llama-bench&#34;&gt;&lt;code&gt;llama-bench&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;Benchmark the performance of the inference for various parameters.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details open&gt; &#xA;   &lt;summary&gt;Run default benchmark&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-bench -m model.gguf&#xA;&#xA;# Output:&#xA;# | model               |       size |     params | backend    | threads |          test |                  t/s |&#xA;# | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |&#xA;# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 ¬± 20.55 |&#xA;# | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 ¬± 0.81 |&#xA;#&#xA;# build: 3e0ba0e60 (4229)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/run&#34;&gt;&lt;code&gt;llama-run&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A comprehensive example for running &lt;code&gt;llama.cpp&lt;/code&gt; models. Useful for inferencing. Used with RamaLama &lt;a href=&#34;%5BRamaLama%5D(https://github.com/containers/ramalama)&#34;&gt;^3&lt;/a&gt;.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Run a model with a specific prompt (by default it&#39;s pulled from Ollama registry)&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-run granite-code&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/examples/simple&#34;&gt;&lt;code&gt;llama-simple&lt;/code&gt;&lt;/a&gt;&lt;/h2&gt; &#xA;&lt;h4&gt;A minimal example for implementing apps with &lt;code&gt;llama.cpp&lt;/code&gt;. Useful for developers.&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &#xA;  &lt;details&gt; &#xA;   &lt;summary&gt;Basic text completion&lt;/summary&gt; &#xA;   &lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;llama-simple -m model.gguf&#xA;&#xA;# Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called &#34;The Art of&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;  &lt;/details&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Contributors can open PRs&lt;/li&gt; &#xA; &lt;li&gt;Collaborators can push to branches in the &lt;code&gt;llama.cpp&lt;/code&gt; repo and merge PRs into the &lt;code&gt;master&lt;/code&gt; branch&lt;/li&gt; &#xA; &lt;li&gt;Collaborators will be invited based on contributions&lt;/li&gt; &#xA; &lt;li&gt;Any help with managing issues, PRs and projects is very appreciated!&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22&#34;&gt;good first issues&lt;/a&gt; for tasks suitable for first contributions&lt;/li&gt; &#xA; &lt;li&gt;Read the &lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt; for more information&lt;/li&gt; &#xA; &lt;li&gt;Make sure to read this: &lt;a href=&#34;https://github.com/ggml-org/llama.cpp/discussions/205&#34;&gt;Inference at the edge&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;A bit of backstory for those who are interested: &lt;a href=&#34;https://changelog.com/podcast/532&#34;&gt;Changelog podcast&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Other documentation&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/main/README.md&#34;&gt;main (cli)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/tools/server/README.md&#34;&gt;server&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/grammars/README.md&#34;&gt;GBNF grammars&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Development documentation&lt;/h4&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/build.md&#34;&gt;How to build&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/docker.md&#34;&gt;Running on Docker&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/android.md&#34;&gt;Build on Android&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ggml-org/llama.cpp/master/docs/development/token_generation_performance_tips.md&#34;&gt;Performance troubleshooting&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&amp;amp;-Tricks&#34;&gt;GGML tips &amp;amp; tricks&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h4&gt;Seminal papers and background on the models&lt;/h4&gt; &#xA;&lt;p&gt;If your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;LLaMA: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34;&gt;Introducing LLaMA: A foundational, 65-billion-parameter large language model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GPT-3 &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34;&gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;GPT-3.5 / InstructGPT / ChatGPT: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://openai.com/research/instruction-following&#34;&gt;Aligning language models to follow instructions&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;XCFramework&lt;/h2&gt; &#xA;&lt;p&gt;The XCFramework is a precompiled version of the library for iOS, visionOS, tvOS, and macOS. It can be used in Swift projects without the need to compile the library from source. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-swift&#34;&gt;// swift-tools-version: 5.10&#xA;// The swift-tools-version declares the minimum version of Swift required to build this package.&#xA;&#xA;import PackageDescription&#xA;&#xA;let package = Package(&#xA;    name: &#34;MyLlamaPackage&#34;,&#xA;    targets: [&#xA;        .executableTarget(&#xA;            name: &#34;MyLlamaPackage&#34;,&#xA;            dependencies: [&#xA;                &#34;LlamaFramework&#34;&#xA;            ]),&#xA;        .binaryTarget(&#xA;            name: &#34;LlamaFramework&#34;,&#xA;            url: &#34;https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip&#34;,&#xA;            checksum: &#34;c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab&#34;&#xA;        )&#xA;    ]&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The above example is using an intermediate build &lt;code&gt;b5046&lt;/code&gt; of the library. This can be modified to use a different version by changing the URL and checksum.&lt;/p&gt; &#xA;&lt;h2&gt;Completions&lt;/h2&gt; &#xA;&lt;p&gt;Command-line completion is available for some environments.&lt;/p&gt; &#xA;&lt;h4&gt;Bash Completion&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ build/bin/llama-cli --completion-bash &amp;gt; ~/.llama-completion.bash&#xA;$ source ~/.llama-completion.bash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Optionally this can be added to your &lt;code&gt;.bashrc&lt;/code&gt; or &lt;code&gt;.bash_profile&lt;/code&gt; to load it automatically. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-console&#34;&gt;$ echo &#34;source ~/.llama-completion.bash&#34; &amp;gt;&amp;gt; ~/.bashrc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;References&lt;/h2&gt;</summary>
  </entry>
  <entry>
    <title>Lightricks/LTX-Video</title>
    <updated>2025-05-12T01:29:28Z</updated>
    <id>tag:github.com,2025-05-12:/Lightricks/LTX-Video</id>
    <link href="https://github.com/Lightricks/LTX-Video" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Official repository for LTX-Video&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;h1&gt;LTX-Video&lt;/h1&gt; &#xA; &lt;p&gt;This is the official repository for LTX-Video.&lt;/p&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://www.lightricks.com/ltxv&#34;&gt;Website&lt;/a&gt; | &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video&#34;&gt;Model&lt;/a&gt; | &lt;a href=&#34;https://app.ltx.studio/ltx-video&#34;&gt;Demo&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2501.00103&#34;&gt;Paper&lt;/a&gt; | &lt;a href=&#34;https://discord.gg/Mn8BRgUKKy&#34;&gt;Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#news&#34;&gt;What&#39;s new&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#quick-start-guide&#34;&gt;Quick Start Guide&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#online-demo&#34;&gt;Online demo&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#run-locally&#34;&gt;Run locally&lt;/a&gt; &#xA;    &lt;ul&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA;     &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI Integration&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#diffusers-integration&#34;&gt;Diffusers Integration&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#model-user-guide&#34;&gt;Model User Guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#community-contribution&#34;&gt;Community Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#trining&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#join-us&#34;&gt;Join Us!&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Introduction&lt;/h1&gt; &#xA;&lt;p&gt;LTX-Video is the first DiT-based video generation model that can generate high-quality videos in &lt;em&gt;real-time&lt;/em&gt;. It can generate 30 FPS videos at 1216√ó704 resolution, faster than it takes to watch them. The model is trained on a large-scale dataset of diverse videos and can generate high-resolution videos with realistic and diverse content.&lt;/p&gt; &#xA;&lt;p&gt;The model supports text-to-image, image-to-video, keyframe-based animation, video extension (both forward and backward), video-to-video transformations, and any combination of these features.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00001.gif&#34; alt=&#34;example1&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with long brown hair and light skin smiles at another woman...&lt;/summary&gt;A woman with long brown hair and light skin smiles at another woman with long blonde hair. The woman with brown hair wears a black jacket and has a small, barely noticeable mole on her right cheek. The camera angle is a close-up, focused on the woman with brown hair&#39;s face. The lighting is warm and natural, likely from the setting sun, casting a soft glow on the scene. The scene appears to be real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00002.gif&#34; alt=&#34;example2&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman walks away from a white Jeep parked on a city street at night...&lt;/summary&gt;A woman walks away from a white Jeep parked on a city street at night, then ascends a staircase and knocks on a door. The woman, wearing a dark jacket and jeans, walks away from the Jeep parked on the left side of the street, her back to the camera; she walks at a steady pace, her arms swinging slightly by her sides; the street is dimly lit, with streetlights casting pools of light on the wet pavement; a man in a dark jacket and jeans walks past the Jeep in the opposite direction; the camera follows the woman from behind as she walks up a set of stairs towards a building with a green door; she reaches the top of the stairs and turns left, continuing to walk towards the building; she reaches the door and knocks on it with her right hand; the camera remains stationary, focused on the doorway; the scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00003.gif&#34; alt=&#34;example3&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with blonde hair styled up, wearing a black dress...&lt;/summary&gt;A woman with blonde hair styled up, wearing a black dress with sequins and pearl earrings, looks down with a sad expression on her face. The camera remains stationary, focused on the woman&#39;s face. The lighting is dim, casting soft shadows on her face. The scene appears to be from a movie or TV show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00004.gif&#34; alt=&#34;example4&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;The camera pans over a snow-covered mountain range...&lt;/summary&gt;The camera pans over a snow-covered mountain range, revealing a vast expanse of snow-capped peaks and valleys.The mountains are covered in a thick layer of snow, with some areas appearing almost white while others have a slightly darker, almost grayish hue. The peaks are jagged and irregular, with some rising sharply into the sky while others are more rounded. The valleys are deep and narrow, with steep slopes that are also covered in snow. The trees in the foreground are mostly bare, with only a few leaves remaining on their branches. The sky is overcast, with thick clouds obscuring the sun. The overall impression is one of peace and tranquility, with the snow-covered mountains standing as a testament to the power and beauty of nature.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00005.gif&#34; alt=&#34;example5&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with light skin, wearing a blue jacket and a black hat...&lt;/summary&gt;A woman with light skin, wearing a blue jacket and a black hat with a veil, looks down and to her right, then back up as she speaks; she has brown hair styled in an updo, light brown eyebrows, and is wearing a white collared shirt under her jacket; the camera remains stationary on her face as she speaks; the background is out of focus, but shows trees and people in period clothing; the scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00006.gif&#34; alt=&#34;example6&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man in a dimly lit room talks on a vintage telephone...&lt;/summary&gt;A man in a dimly lit room talks on a vintage telephone, hangs up, and looks down with a sad expression. He holds the black rotary phone to his right ear with his right hand, his left hand holding a rocks glass with amber liquid. He wears a brown suit jacket over a white shirt, and a gold ring on his left ring finger. His short hair is neatly combed, and he has light skin with visible wrinkles around his eyes. The camera remains stationary, focused on his face and upper body. The room is dark, lit only by a warm light source off-screen to the left, casting shadows on the wall behind him. The scene appears to be from a movie.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00007.gif&#34; alt=&#34;example7&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A prison guard unlocks and opens a cell door...&lt;/summary&gt;A prison guard unlocks and opens a cell door to reveal a young man sitting at a table with a woman. The guard, wearing a dark blue uniform with a badge on his left chest, unlocks the cell door with a key held in his right hand and pulls it open; he has short brown hair, light skin, and a neutral expression. The young man, wearing a black and white striped shirt, sits at a table covered with a white tablecloth, facing the woman; he has short brown hair, light skin, and a neutral expression. The woman, wearing a dark blue shirt, sits opposite the young man, her face turned towards him; she has short blonde hair and light skin. The camera remains stationary, capturing the scene from a medium distance, positioned slightly to the right of the guard. The room is dimly lit, with a single light fixture illuminating the table and the two figures. The walls are made of large, grey concrete blocks, and a metal door is visible in the background. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00008.gif&#34; alt=&#34;example8&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with blood on her face and a white tank top...&lt;/summary&gt;A woman with blood on her face and a white tank top looks down and to her right, then back up as she speaks. She has dark hair pulled back, light skin, and her face and chest are covered in blood. The camera angle is a close-up, focused on the woman&#39;s face and upper torso. The lighting is dim and blue-toned, creating a somber and intense atmosphere. The scene appears to be from a movie or TV show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00009.gif&#34; alt=&#34;example9&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man with graying hair, a beard, and a gray shirt...&lt;/summary&gt;A man with graying hair, a beard, and a gray shirt looks down and to his right, then turns his head to the left. The camera angle is a close-up, focused on the man&#39;s face. The lighting is dim, with a greenish tint. The scene appears to be real-life footage. Step&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00010.gif&#34; alt=&#34;example10&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A clear, turquoise river flows through a rocky canyon...&lt;/summary&gt;A clear, turquoise river flows through a rocky canyon, cascading over a small waterfall and forming a pool of water at the bottom.The river is the main focus of the scene, with its clear water reflecting the surrounding trees and rocks. The canyon walls are steep and rocky, with some vegetation growing on them. The trees are mostly pine trees, with their green needles contrasting with the brown and gray rocks. The overall tone of the scene is one of peace and tranquility.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00011.gif&#34; alt=&#34;example11&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man in a suit enters a room and speaks to two women...&lt;/summary&gt;A man in a suit enters a room and speaks to two women sitting on a couch. The man, wearing a dark suit with a gold tie, enters the room from the left and walks towards the center of the frame. He has short gray hair, light skin, and a serious expression. He places his right hand on the back of a chair as he approaches the couch. Two women are seated on a light-colored couch in the background. The woman on the left wears a light blue sweater and has short blonde hair. The woman on the right wears a white sweater and has short blonde hair. The camera remains stationary, focusing on the man as he enters the room. The room is brightly lit, with warm tones reflecting off the walls and furniture. The scene appears to be from a film or television show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00012.gif&#34; alt=&#34;example12&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;The waves crash against the jagged rocks of the shoreline...&lt;/summary&gt;The waves crash against the jagged rocks of the shoreline, sending spray high into the air.The rocks are a dark gray color, with sharp edges and deep crevices. The water is a clear blue-green, with white foam where the waves break against the rocks. The sky is a light gray, with a few white clouds dotting the horizon.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00013.gif&#34; alt=&#34;example13&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;The camera pans across a cityscape of tall buildings...&lt;/summary&gt;The camera pans across a cityscape of tall buildings with a circular building in the center. The camera moves from left to right, showing the tops of the buildings and the circular building in the center. The buildings are various shades of gray and white, and the circular building has a green roof. The camera angle is high, looking down at the city. The lighting is bright, with the sun shining from the upper left, casting shadows from the buildings. The scene is computer-generated imagery.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00014.gif&#34; alt=&#34;example14&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A man walks towards a window, looks out, and then turns around...&lt;/summary&gt;A man walks towards a window, looks out, and then turns around. He has short, dark hair, dark skin, and is wearing a brown coat over a red and gray scarf. He walks from left to right towards a window, his gaze fixed on something outside. The camera follows him from behind at a medium distance. The room is brightly lit, with white walls and a large window covered by a white curtain. As he approaches the window, he turns his head slightly to the left, then back to the right. He then turns his entire body to the right, facing the window. The camera remains stationary as he stands in front of the window. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00015.gif&#34; alt=&#34;example15&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;Two police officers in dark blue uniforms and matching hats...&lt;/summary&gt;Two police officers in dark blue uniforms and matching hats enter a dimly lit room through a doorway on the left side of the frame. The first officer, with short brown hair and a mustache, steps inside first, followed by his partner, who has a shaved head and a goatee. Both officers have serious expressions and maintain a steady pace as they move deeper into the room. The camera remains stationary, capturing them from a slightly low angle as they enter. The room has exposed brick walls and a corrugated metal ceiling, with a barred window visible in the background. The lighting is low-key, casting shadows on the officers&#39; faces and emphasizing the grim atmosphere. The scene appears to be from a film or television show.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/docs/_static/ltx-video_example_00016.gif&#34; alt=&#34;example16&#34;&gt;&lt;br&gt;&#xA;    &lt;details style=&#34;max-width: 300px; margin: auto;&#34;&gt;&#xA;     &lt;summary&gt;A woman with short brown hair, wearing a maroon sleeveless top...&lt;/summary&gt;A woman with short brown hair, wearing a maroon sleeveless top and a silver necklace, walks through a room while talking, then a woman with pink hair and a white shirt appears in the doorway and yells. The first woman walks from left to right, her expression serious; she has light skin and her eyebrows are slightly furrowed. The second woman stands in the doorway, her mouth open in a yell; she has light skin and her eyes are wide. The room is dimly lit, with a bookshelf visible in the background. The camera follows the first woman as she walks, then cuts to a close-up of the second woman&#39;s face. The scene is captured in real-life footage.&#xA;    &lt;/details&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;News&lt;/h1&gt; &#xA;&lt;h2&gt;May, 5th, 2025: New model 13B v0.9.7:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new 13B model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev.safetensors&#34;&gt;ltxv-13b-0.9.7-dev&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Release a new quantized model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-13b-0.9.7-dev-fp8.safetensors&#34;&gt;ltxv-13b-0.9.7-dev-fp8&lt;/a&gt; for faster inference with less VRam (Supported in the &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;official CompfyUI workflow&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Release a new upscalers &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-temporal-upscaler-0.9.7.safetensors&#34;&gt;ltxv-temporal-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-spatial-upscaler-0.9.7.safetensors&#34;&gt;ltxv-spatial-upscaler-0.9.7&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Breakthrough prompt adherence and physical understanding.&lt;/li&gt; &#xA; &lt;li&gt;New Pipeline for multi-scale video rendering for fast and high quality results&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;April, 15th, 2025: New checkpoints v0.9.6:&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new checkpoint &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-dev-04-25.safetensors&#34;&gt;ltxv-2b-0.9.6-dev-04-25&lt;/a&gt; with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Release a new distilled model &lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/blob/main/ltxv-2b-0.9.6-distilled-04-25.safetensors&#34;&gt;ltxv-2b-0.9.6-distilled-04-25&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;15x faster inference than non-distilled model.&lt;/li&gt; &#xA;   &lt;li&gt;Does not require classifier-free guidance and spatio-temporal guidance.&lt;/li&gt; &#xA;   &lt;li&gt;Supports sampling with 8 (recommended), 4, 2 or 1 diffusion steps.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Improved prompt adherence, motion quality and fine details.&lt;/li&gt; &#xA; &lt;li&gt;New default resolution and FPS: 1216 √ó 704 pixels at 30 FPS &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Still real time on H100 with the distilled model.&lt;/li&gt; &#xA;   &lt;li&gt;Other resolutions and FPS are still supported.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Support stochastic inference (can improve visual quality when using the distilled model)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;March, 5th, 2025: New checkpoint v0.9.5&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;New license for commercial use (&lt;a href=&#34;https://huggingface.co/Lightricks/LTX-Video/ltx-video-2b-v0.9.5.license.txt&#34;&gt;OpenRail-M&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Release a new checkpoint v0.9.5 with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Support keyframes and video extension&lt;/li&gt; &#xA; &lt;li&gt;Support higher resolutions&lt;/li&gt; &#xA; &lt;li&gt;Improved prompt understanding&lt;/li&gt; &#xA; &lt;li&gt;Improved VAE&lt;/li&gt; &#xA; &lt;li&gt;New online web app in &lt;a href=&#34;https://app.ltx.studio/ltx-video&#34;&gt;LTX-Studio&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Automatic prompt enhancement&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;February, 20th, 2025: More inference options&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Improve STG (Spatiotemporal Guidance) for LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Support MPS on macOS with PyTorch 2.3.0&lt;/li&gt; &#xA; &lt;li&gt;Add support for 8-bit model, LTX-VideoQ8&lt;/li&gt; &#xA; &lt;li&gt;Add TeaCache for LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Add &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Add Diffusion-Pipe&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;December 31st, 2024: Research paper&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release the &lt;a href=&#34;https://arxiv.org/abs/2501.00103&#34;&gt;research paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;December 20th, 2024: New checkpoint v0.9.1&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Release a new checkpoint v0.9.1 with improved quality&lt;/li&gt; &#xA; &lt;li&gt;Support for STG / PAG&lt;/li&gt; &#xA; &lt;li&gt;Support loading checkpoints of LTX-Video in Diffusers format (conversion is done on-the-fly)&lt;/li&gt; &#xA; &lt;li&gt;Support offloading unused parts to CPU&lt;/li&gt; &#xA; &lt;li&gt;Support the new timestep-conditioned VAE decoder&lt;/li&gt; &#xA; &lt;li&gt;Reference contributions from the community in the readme file&lt;/li&gt; &#xA; &lt;li&gt;Relax transformers dependency&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;November 21th, 2024: Initial release v0.9.0&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Initial release of LTX-Video&lt;/li&gt; &#xA; &lt;li&gt;Support text-to-video and image-to-video generation&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Models&lt;/h1&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model&lt;/th&gt; &#xA;   &lt;th&gt;Version&lt;/th&gt; &#xA;   &lt;th&gt;Notes&lt;/th&gt; &#xA;   &lt;th&gt;inference.py config&lt;/th&gt; &#xA;   &lt;th&gt;ComfyUI workflow (Recommended)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;0.9.7&lt;/td&gt; &#xA;   &lt;td&gt;Highest quality, requires more VRAM&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-13b-0.9.7-dev.yaml&#34;&gt;ltxv-13b-0.9.7-dev.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base.json&#34;&gt;ltxv-13b-i2v-base.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-13b-fp8&lt;/td&gt; &#xA;   &lt;td&gt;0.9.7&lt;/td&gt; &#xA;   &lt;td&gt;Quantized version of ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;Coming soon&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/ltxv-13b-i2v-base-fp8.json&#34;&gt;ltxv-13b-i2v-base-fp8.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-2b&lt;/td&gt; &#xA;   &lt;td&gt;0.9.6&lt;/td&gt; &#xA;   &lt;td&gt;Good quality, lower VRAM requirement than ltxv-13b&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-dev.yaml&#34;&gt;ltxv-2b-0.9.6-dev.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v.json&#34;&gt;ltxvideo-i2v.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ltxv-2b-distilled&lt;/td&gt; &#xA;   &lt;td&gt;0.9.6&lt;/td&gt; &#xA;   &lt;td&gt;15√ó faster, real-time capable, fewer steps needed, no STG/CFG required&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/LTX-Video/raw/main/configs/ltxv-2b-0.9.6-distilled.yaml&#34;&gt;ltxv-2b-0.9.6-distilled.yaml&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/raw/master/example_workflows/low_level/ltxvideo-i2v-distilled.json&#34;&gt;ltxvideo-i2v-distilled.json&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h1&gt;Quick Start Guide&lt;/h1&gt; &#xA;&lt;h2&gt;Online inference&lt;/h2&gt; &#xA;&lt;p&gt;The model is accessible right away via the following links:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://app.ltx.studio/ltx-video&#34;&gt;LTX-Studio image-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/ltx-video&#34;&gt;Fal.ai text-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://fal.ai/models/fal-ai/ltx-video/image-to-video&#34;&gt;Fal.ai image-to-video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://replicate.com/lightricks/ltx-video&#34;&gt;Replicate text-to-video and image-to-video&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Run locally&lt;/h2&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch &amp;gt;= 2.1.2. On macos, MPS was tested with PyTorch 2.3.0, and should support PyTorch == 2.3 or &amp;gt;= 2.6.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Lightricks/LTX-Video.git&#xA;cd LTX-Video&#xA;&#xA;# create env&#xA;python -m venv env&#xA;source env/bin/activate&#xA;python -m pip install -e .\[inference-script\]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; For best results, we recommend using our &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#comfyui-integration&#34;&gt;ComfyUI&lt;/a&gt; workflow. We‚Äôre working on updating the inference.py script to match the high quality and output fidelity of ComfyUI.&lt;/p&gt; &#xA;&lt;p&gt;To use our model, please follow the inference code in &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/inference.py&#34;&gt;inference.py&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;h4&gt;For text-to-video generation:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-dev.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For image-to-video generation:&lt;/h4&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths IMAGE_PATH --conditioning_start_frames 0 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-dev.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Extending a video:&lt;/h4&gt; &#xA;&lt;p&gt;üìù &lt;strong&gt;Note:&lt;/strong&gt; Input video segments must contain a multiple of 8 frames plus 1 (e.g., 9, 17, 25, etc.), and the target frame number should be a multiple of 8.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths VIDEO_PATH --conditioning_start_frames START_FRAME --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-dev.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;For video generation with multiple conditions:&lt;/h4&gt; &#xA;&lt;p&gt;You can now generate a video conditioned on a set of images and/or short video segments. Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python inference.py --prompt &#34;PROMPT&#34; --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.7-dev.yaml&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;ComfyUI Integration&lt;/h2&gt; &#xA;&lt;p&gt;To use our model with ComfyUI, please follow the instructions at &lt;a href=&#34;https://github.com/Lightricks/ComfyUI-LTXVideo/&#34;&gt;https://github.com/Lightricks/ComfyUI-LTXVideo/&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Diffusers Integration&lt;/h2&gt; &#xA;&lt;p&gt;To use our model with the Diffusers Python library, check out the &lt;a href=&#34;https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Diffusers also support an 8-bit version of LTX-Video, &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#ltx-videoq8&#34;&gt;see details below&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Model User Guide&lt;/h1&gt; &#xA;&lt;h2&gt;üìù Prompt Engineering&lt;/h2&gt; &#xA;&lt;p&gt;When writing prompts, focus on detailed, chronological descriptions of actions and scenes. Include specific movements, appearances, camera angles, and environmental details - all in a single flowing paragraph. Start directly with the action, and keep descriptions literal and precise. Think like a cinematographer describing a shot list. Keep within 200 words. For best results, build your prompts using this structure:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Start with main action in a single sentence&lt;/li&gt; &#xA; &lt;li&gt;Add specific details about movements and gestures&lt;/li&gt; &#xA; &lt;li&gt;Describe character/object appearances precisely&lt;/li&gt; &#xA; &lt;li&gt;Include background and environment details&lt;/li&gt; &#xA; &lt;li&gt;Specify camera angles and movements&lt;/li&gt; &#xA; &lt;li&gt;Describe lighting and colors&lt;/li&gt; &#xA; &lt;li&gt;Note any changes or sudden events&lt;/li&gt; &#xA; &lt;li&gt;See &lt;a href=&#34;https://raw.githubusercontent.com/Lightricks/LTX-Video/main/#introduction&#34;&gt;examples&lt;/a&gt; for more inspiration.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Automatic Prompt Enhancement&lt;/h3&gt; &#xA;&lt;p&gt;When using &lt;code&gt;inference.py&lt;/code&gt;, shorts prompts (below &lt;code&gt;prompt_enhancement_words_threshold&lt;/code&gt; words) are automatically enhanced by a language model. This is supported with text-to-video and image-to-video (first-frame conditioning).&lt;/p&gt; &#xA;&lt;p&gt;When using &lt;code&gt;LTXVideoPipeline&lt;/code&gt; directly, you can enable prompt enhancement by setting &lt;code&gt;enhance_prompt=True&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üéÆ Parameter Guide&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Resolution Preset: Higher resolutions for detailed scenes, lower for faster generation and simpler scenes. The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames. The model works best on resolutions under 720 x 1280 and number of frames below 257&lt;/li&gt; &#xA; &lt;li&gt;Seed: Save seed values to recreate specific styles or compositions you like&lt;/li&gt; &#xA; &lt;li&gt;Guidance Scale: 3-3.5 are the recommended values&lt;/li&gt; &#xA; &lt;li&gt;Inference Steps: More steps (40+) for quality, fewer steps (20-30) for speed&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;üìù For advanced parameters usage, please see &lt;code&gt;python inference.py --help&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Community Contribution&lt;/h2&gt; &#xA;&lt;h3&gt;ComfyUI-LTXTricks üõ†Ô∏è&lt;/h3&gt; &#xA;&lt;p&gt;A community project providing additional nodes for enhanced control over the LTX Video model. It includes implementations of advanced techniques like RF-Inversion, RF-Edit, FlowEdit, and more. These nodes enable workflows such as Image and Video to Video (I+V2V), enhanced sampling via Spatiotemporal Skip Guidance (STG), and interpolation with precise frame settings.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks&#34;&gt;ComfyUI-LTXTricks&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üîÑ &lt;strong&gt;RF-Inversion:&lt;/strong&gt; Implements &lt;a href=&#34;https://rf-inversion.github.io/&#34;&gt;RF-Inversion&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_inversion.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;‚úÇÔ∏è &lt;strong&gt;RF-Edit:&lt;/strong&gt; Implements &lt;a href=&#34;https://github.com/wangjiangshan0725/RF-Solver-Edit&#34;&gt;RF-Solver-Edit&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_rf_edit.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;üåä &lt;strong&gt;FlowEdit:&lt;/strong&gt; Implements &lt;a href=&#34;https://github.com/fallenshock/FlowEdit&#34;&gt;FlowEdit&lt;/a&gt; with an &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_flow_edit.json&#34;&gt;example workflow here&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;üé• &lt;strong&gt;I+V2V:&lt;/strong&gt; Enables Video to Video with a reference image. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_iv2v.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;‚ú® &lt;strong&gt;Enhance:&lt;/strong&gt; Partial implementation of &lt;a href=&#34;https://junhahyung.github.io/STGuidance/&#34;&gt;STGuidance&lt;/a&gt;. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltxv_stg.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;üñºÔ∏è &lt;strong&gt;Interpolation and Frame Setting:&lt;/strong&gt; Nodes for precise control of latents per frame. &lt;a href=&#34;https://github.com/logtd/ComfyUI-LTXTricks/raw/main/example_workflows/example_ltx_interpolation.json&#34;&gt;Example workflow&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;LTX-VideoQ8 üé± &lt;a id=&#34;ltx-videoq8&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;LTX-VideoQ8&lt;/strong&gt; is an 8-bit optimized version of &lt;a href=&#34;https://github.com/Lightricks/LTX-Video&#34;&gt;LTX-Video&lt;/a&gt;, designed for faster performance on NVIDIA ADA GPUs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/KONAKONA666/LTX-Video&#34;&gt;LTX-VideoQ8&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üöÄ Up to 3X speed-up with no accuracy loss&lt;/li&gt; &#xA;   &lt;li&gt;üé• Generate 720x480x121 videos in under a minute on RTX 4060 (8GB VRAM)&lt;/li&gt; &#xA;   &lt;li&gt;üõ†Ô∏è Fine-tune 2B transformer models with precalculated latents&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Community Discussion:&lt;/strong&gt; &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/1h79ks2/fast_ltx_video_on_rtx_4060_and_other_ada_gpus/&#34;&gt;Reddit Thread&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Diffusers integration:&lt;/strong&gt; A diffusers integration for the 8-bit model is already out! &lt;a href=&#34;https://github.com/sayakpaul/q8-ltx-video&#34;&gt;Details here&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TeaCache for LTX-Video üçµ &lt;a id=&#34;TeaCache&#34;&gt;&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;&lt;strong&gt;TeaCache&lt;/strong&gt; is a training-free caching approach that leverages timestep differences across model outputs to accelerate LTX-Video inference by up to 2x without significant visual quality degradation.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/ali-vilab/TeaCache/tree/main/TeaCache4LTX-Video&#34;&gt;TeaCache4LTX-Video&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üöÄ Speeds up LTX-Video inference.&lt;/li&gt; &#xA;   &lt;li&gt;üìä Adjustable trade-offs between speed (up to 2x) and visual quality using configurable parameters.&lt;/li&gt; &#xA;   &lt;li&gt;üõ†Ô∏è No retraining required: Works directly with existing models.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Your Contribution&lt;/h3&gt; &#xA;&lt;p&gt;...is welcome! If you have a project or tool that integrates with LTX-Video, please let us know by opening an issue or pull request.&lt;/p&gt; &#xA;&lt;h1&gt;Training&lt;/h1&gt; &#xA;&lt;h2&gt;Diffusers&lt;/h2&gt; &#xA;&lt;p&gt;Diffusers implemented &lt;a href=&#34;https://github.com/huggingface/diffusers/pull/10228&#34;&gt;LoRA support&lt;/a&gt;, with a training script for fine-tuning. More information and training script in &lt;a href=&#34;https://github.com/a-r-r-o-w/finetrainers?tab=readme-ov-file#training&#34;&gt;finetrainers&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Diffusion-Pipe&lt;/h2&gt; &#xA;&lt;p&gt;An experimental training framework with pipeline parallelism, enabling fine-tuning of large models like &lt;strong&gt;LTX-Video&lt;/strong&gt; across multiple GPUs.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Repository:&lt;/strong&gt; &lt;a href=&#34;https://github.com/tdrussell/diffusion-pipe&#34;&gt;Diffusion-Pipe&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Features:&lt;/strong&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;üõ†Ô∏è Full fine-tune support for LTX-Video using LoRA&lt;/li&gt; &#xA;   &lt;li&gt;üìä Useful metrics logged to Tensorboard&lt;/li&gt; &#xA;   &lt;li&gt;üîÑ Training state checkpointing and resumption&lt;/li&gt; &#xA;   &lt;li&gt;‚ö° Efficient pre-caching of latents and text embeddings for multi-GPU setups&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Join Us üöÄ&lt;/h1&gt; &#xA;&lt;p&gt;Want to work on cutting-edge AI research and make a real impact on millions of users worldwide?&lt;/p&gt; &#xA;&lt;p&gt;At &lt;strong&gt;Lightricks&lt;/strong&gt;, an AI-first company, we&#39;re revolutionizing how visual content is created.&lt;/p&gt; &#xA;&lt;p&gt;If you are passionate about AI, computer vision, and video generation, we would love to hear from you!&lt;/p&gt; &#xA;&lt;p&gt;Please visit our &lt;a href=&#34;https://careers.lightricks.com/careers?query=&amp;amp;office=all&amp;amp;department=R%26D&#34;&gt;careers page&lt;/a&gt; for more information.&lt;/p&gt; &#xA;&lt;h1&gt;Acknowledgement&lt;/h1&gt; &#xA;&lt;p&gt;We are grateful for the following awesome projects when implementing LTX-Video:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt; and &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-alpha&lt;/a&gt;: vision transformers for image generation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;üìÑ Our tech report is out! If you find our work helpful, please ‚≠êÔ∏è star the repository and cite our paper.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@article{HaCohen2024LTXVideo,&#xA;  title={LTX-Video: Realtime Video Latent Diffusion},&#xA;  author={HaCohen, Yoav and Chiprut, Nisan and Brazowski, Benny and Shalem, Daniel and Moshe, Dudu and Richardson, Eitan and Levin, Eran and Shiran, Guy and Zabari, Nir and Gordon, Ori and Panet, Poriya and Weissbuch, Sapir and Kulikov, Victor and Bitterman, Yaki and Melumian, Zeev and Bibi, Ofir},&#xA;  journal={arXiv preprint arXiv:2501.00103},&#xA;  year={2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
</feed>