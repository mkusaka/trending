<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-03-07T01:30:18Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>oobabooga/text-generation-webui</title>
    <updated>2023-03-07T01:30:18Z</updated>
    <id>tag:github.com,2023-03-07:/oobabooga/text-generation-webui</id>
    <link href="https://github.com/oobabooga/text-generation-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, GPT-Neo, and Pygmalion.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Text generation web UI&lt;/h1&gt; &#xA;&lt;p&gt;A gradio web UI for running Large Language Models like GPT-J 6B, OPT, GALACTICA, GPT-Neo, and Pygmalion.&lt;/p&gt; &#xA;&lt;p&gt;Its goal is to become the &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt; of text generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/oobabooga/AI-Notebooks/blob/main/Colab-TextGen-GPU.ipynb&#34;&gt;[Try it on Google Colab]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/qa.png&#34; alt=&#34;Image1&#34;&gt;&lt;/th&gt; &#xA;   &lt;th align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/cai3.png&#34; alt=&#34;Image2&#34;&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/gpt4chan.png&#34; alt=&#34;Image3&#34;&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://github.com/oobabooga/screenshots/raw/main/galactica.png&#34; alt=&#34;Image4&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Switch between different models using a dropdown menu.&lt;/li&gt; &#xA; &lt;li&gt;Notebook mode that resembles OpenAI&#39;s playground.&lt;/li&gt; &#xA; &lt;li&gt;Chat mode for conversation and role playing.&lt;/li&gt; &#xA; &lt;li&gt;Generate nice HTML output for GPT-4chan.&lt;/li&gt; &#xA; &lt;li&gt;Generate Markdown output for &lt;a href=&#34;https://github.com/paperswithcode/galai&#34;&gt;GALACTICA&lt;/a&gt;, including LaTeX support.&lt;/li&gt; &#xA; &lt;li&gt;Support for &lt;a href=&#34;https://huggingface.co/models?search=pygmalionai/pygmalion&#34;&gt;Pygmalion&lt;/a&gt; and custom characters in JSON or TavernAI Character Card formats (&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Pygmalion-chat-model-FAQ&#34;&gt;FAQ&lt;/a&gt;).&lt;/li&gt; &#xA; &lt;li&gt;Advanced chat features (send images, get audio responses with TTS).&lt;/li&gt; &#xA; &lt;li&gt;Stream the text output in real time.&lt;/li&gt; &#xA; &lt;li&gt;Load parameter presets from text files.&lt;/li&gt; &#xA; &lt;li&gt;Load large models in 8-bit mode (see &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/147#issuecomment-1456040134&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/issues/20#issuecomment-1411650652&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://www.reddit.com/r/PygmalionAI/comments/1115gom/running_pygmalion_6b_with_8gb_of_vram/&#34;&gt;here&lt;/a&gt; if you are on Windows).&lt;/li&gt; &#xA; &lt;li&gt;Split large models across your GPU(s), CPU, and disk.&lt;/li&gt; &#xA; &lt;li&gt;CPU mode.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/FlexGen&#34;&gt;FlexGen offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/DeepSpeed&#34;&gt;DeepSpeed ZeRO-3 offload&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Get responses via API, &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example-streaming.py&#34;&gt;with&lt;/a&gt; or &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/raw/main/api-example.py&#34;&gt;without&lt;/a&gt; streaming.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/RWKV-model&#34;&gt;Supports the RWKV model&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Supports softprompts.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Extensions&#34;&gt;Supports extensions&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Running-on-Colab&#34;&gt;Works on Google Colab&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation option 1: conda&lt;/h2&gt; &#xA;&lt;p&gt;Open a terminal and copy and paste these commands one at a time (&lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;install conda&lt;/a&gt; first if you don&#39;t have it already):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda create -n textgen&#xA;conda activate textgen&#xA;conda install torchvision torchaudio pytorch-cuda=11.7 git -c pytorch -c nvidia&#xA;git clone https://github.com/oobabooga/text-generation-webui&#xA;cd text-generation-webui&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The third line assumes that you have an NVIDIA GPU.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you have an AMD GPU, replace the third command with this one:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/rocm5.2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;If you are running in CPU mode, replace the third command with this one:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda install pytorch torchvision torchaudio git -c pytorch&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installation option 2: one-click installers&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga-windows.zip&#34;&gt;oobabooga-windows.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/releases/download/installers/oobabooga-linux.zip&#34;&gt;oobabooga-linux.zip&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Just download the zip above, extract it, and double click on &#34;install&#34;. The web UI and all its dependencies will be installed in the same folder.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;To download a model, double click on &#34;download-model&#34;&lt;/li&gt; &#xA; &lt;li&gt;To start the web UI, double click on &#34;start-webui&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Downloading models&lt;/h2&gt; &#xA;&lt;p&gt;Models should be placed under &lt;code&gt;models/model-name&lt;/code&gt;. For instance, &lt;code&gt;models/gpt-j-6B&lt;/code&gt; for &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h4&gt;Hugging Face&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&#34;&gt;Hugging Face&lt;/a&gt; is the main place to download models. These are some noteworthy examples:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/tree/main&#34;&gt;GPT-J 6B&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;sort=downloads&amp;amp;search=eleutherai+%2F+gpt-neo&#34;&gt;GPT-Neo&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=eleutherai/pythia&#34;&gt;Pythia&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/opt&#34;&gt;OPT&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=facebook/galactica&#34;&gt;GALACTICA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=erebus&#34;&gt;*-Erebus&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/models?search=pygmalion&#34;&gt;Pygmalion&lt;/a&gt; (NSFW)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;You can automatically download a model from HF using the script &lt;code&gt;download-model.py&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py organization/model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For instance:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py facebook/opt-1.3b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to download a model manually, note that all you need are the json, txt, and pytorch*.bin (or model*.safetensors) files. The remaining files are not necessary.&lt;/p&gt; &#xA;&lt;h4&gt;GPT-4chan&lt;/h4&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/ykilcher/gpt-4chan&#34;&gt;GPT-4chan&lt;/a&gt; has been shut down from Hugging Face, so you need to download it elsewhere. You have two options:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Torrent: &lt;a href=&#34;https://archive.org/details/gpt4chan_model_float16&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://archive.org/details/gpt4chan_model&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Direct download: &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model_float16/&#34;&gt;16-bit&lt;/a&gt; / &lt;a href=&#34;https://theswissbay.ch/pdf/_notpdf_/gpt4chan_model/&#34;&gt;32-bit&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;The 32-bit version is only relevant if you intend to run the model in CPU mode. Otherwise, you should use the 16-bit version.&lt;/p&gt; &#xA;&lt;p&gt;After downloading the model, follow these steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Place the files under &lt;code&gt;models/gpt4chan_model_float16&lt;/code&gt; or &lt;code&gt;models/gpt4chan_model&lt;/code&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Place GPT-J 6B&#39;s config.json file in that same folder: &lt;a href=&#34;https://huggingface.co/EleutherAI/gpt-j-6B/raw/main/config.json&#34;&gt;config.json&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Download GPT-J 6B&#39;s tokenizer files (they will be automatically detected when you attempt to load GPT-4chan):&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;python download-model.py EleutherAI/gpt-j-6B --text-only&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Starting the web UI&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;conda activate textgen&#xA;python server.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then browse to&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;http://localhost:7860/?__theme=dark&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Optionally, you can use the following command-line flags:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Flag&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;-h&lt;/code&gt;, &lt;code&gt;--help&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;show this help message and exit&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--model MODEL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Name of the model to load by default.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--notebook&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in notebook mode, where the output is written to the same text box as the input.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cai-chat&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Launch the web UI in chat mode with a style similar to Character.AI&#39;s. If the file &lt;code&gt;img_bot.png&lt;/code&gt; or &lt;code&gt;img_bot.jpg&lt;/code&gt; exists in the same folder as server.py, this image will be used as the bot&#39;s profile picture. Similarly, &lt;code&gt;img_me.png&lt;/code&gt; or &lt;code&gt;img_me.jpg&lt;/code&gt; will be used as your profile picture.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Use the CPU to generate text.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--load-in-8bit&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with 8-bit precision.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--bf16&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the model with bfloat16 precision. Requires NVIDIA Ampere GPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--auto-devices&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Automatically split the model across the available GPU(s) and CPU.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;If the model is too large for your GPU(s) and CPU combined, send the remaining layers to the disk.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--disk-cache-dir DISK_CACHE_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Directory to save the disk cache to. Defaults to &lt;code&gt;cache/&lt;/code&gt;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--gpu-memory GPU_MEMORY [GPU_MEMORY ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maxmimum GPU memory in GiB to be allocated per GPU. Example: &lt;code&gt;--gpu-memory 10&lt;/code&gt; for a single GPU, &lt;code&gt;--gpu-memory 10 5&lt;/code&gt; for two GPUs.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--cpu-memory CPU_MEMORY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Maximum CPU memory in GiB to allocate for offloaded weights. Must be an integer number. Defaults to 99.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--flexgen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of FlexGen offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--percent PERCENT [PERCENT ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: allocation percentages. Must be 6 numbers separated by spaces (default: 0, 100, 100, 0, 100, 0).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--compress-weight&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: Whether to compress weight (default: False).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--pin-weight [PIN_WEIGHT]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;FlexGen: whether to pin weights (setting this to False reduces CPU memory by 20%).&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--deepspeed&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Enable the use of DeepSpeed ZeRO-3 for inference via the Transformers integration.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--nvme-offload-dir NVME_OFFLOAD_DIR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Directory to use for ZeRO-3 NVME offloading.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--local_rank LOCAL_RANK&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;DeepSpeed: Optional argument for distributed setups.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-strategy RWKV_STRATEGY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: The strategy to use while loading the model. Examples: &#34;cpu fp32&#34;, &#34;cuda fp16&#34;, &#34;cuda fp16i8&#34;.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--rwkv-cuda-on&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;RWKV: Compile the CUDA kernel for better performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--no-stream&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Don&#39;t stream the text output in real time. This improves the text generation performance.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--settings SETTINGS_FILE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Load the default interface settings from this json file. See &lt;code&gt;settings-template.json&lt;/code&gt; for an example. If you create a file called &lt;code&gt;settings.json&lt;/code&gt;, this file will be loaded by default without the need to use the &lt;code&gt;--settings&lt;/code&gt; flag.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--extensions EXTENSIONS [EXTENSIONS ...]&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The list of extensions to load. If you want to load more than one extension, write the names separated by spaces.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Make the web UI reachable from your local network.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--listen-port LISTEN_PORT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;The listening port that the server will use.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--share&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Create a public URL. This is useful for running the web UI on Google Colab or similar.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;--verbose&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;Print the prompts to the terminal.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Out of memory errors? &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/Low-VRAM-guide&#34;&gt;Check this guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Presets&lt;/h2&gt; &#xA;&lt;p&gt;Inference settings presets can be created under &lt;code&gt;presets/&lt;/code&gt; as text files. These files are detected automatically at startup.&lt;/p&gt; &#xA;&lt;p&gt;By default, 10 presets by NovelAI and KoboldAI are included. These were selected out of a sample of 43 presets after applying a K-Means clustering algorithm and selecting the elements closest to the average of each cluster.&lt;/p&gt; &#xA;&lt;h2&gt;System requirements&lt;/h2&gt; &#xA;&lt;p&gt;Check the &lt;a href=&#34;https://github.com/oobabooga/text-generation-webui/wiki/System-requirements&#34;&gt;wiki&lt;/a&gt; for some examples of VRAM and RAM usage in both GPU and CPU mode.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Pull requests, suggestions, and issue reports are welcome.&lt;/p&gt; &#xA;&lt;p&gt;Before reporting a bug, make sure that you have created a conda environment and installed the dependencies exactly as in the &lt;em&gt;Installation&lt;/em&gt; section above.&lt;/p&gt; &#xA;&lt;p&gt;These issues are known:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8-bit doesn&#39;t work properly on Windows or older GPUs.&lt;/li&gt; &#xA; &lt;li&gt;DeepSpeed doesn&#39;t work properly on Windows.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For these two, please try commenting on an existing issue instead of creating a new one.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;NovelAI and KoboldAI presets: &lt;a href=&#34;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&#34;&gt;https://github.com/KoboldAI/KoboldAI-Client/wiki/Settings-Presets&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Pygmalion preset, code for early stopping in chat mode, code for some of the sliders, --chat mode colors: &lt;a href=&#34;https://github.com/PygmalionAI/gradio-ui/&#34;&gt;https://github.com/PygmalionAI/gradio-ui/&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Verbose preset: Anonymous 4chan user.&lt;/li&gt; &#xA; &lt;li&gt;Instruct-Joi preset: &lt;a href=&#34;https://huggingface.co/Rallio67/joi_12B_instruct_alpha&#34;&gt;https://huggingface.co/Rallio67/joi_12B_instruct_alpha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Gradio dropdown menu refresh button: &lt;a href=&#34;https://github.com/AUTOMATIC1111/stable-diffusion-webui&#34;&gt;https://github.com/AUTOMATIC1111/stable-diffusion-webui&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>GaiZhenbiao/ChuanhuChatGPT</title>
    <updated>2023-03-07T01:30:18Z</updated>
    <id>tag:github.com,2023-03-07:/GaiZhenbiao/ChuanhuChatGPT</id>
    <link href="https://github.com/GaiZhenbiao/ChuanhuChatGPT" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GUI for ChatGPT API&lt;/p&gt;&lt;hr&gt;&lt;img height=&#34;128&#34; align=&#34;left&#34; src=&#34;https://user-images.githubusercontent.com/51039745/222689546-7612df0e-e28b-4693-9f5f-4ef2be3daf48.png&#34; alt=&#34;Logo&#34;&gt; &#xA;&lt;h1&gt;川虎 ChatGPT / Chuanhu ChatGPT&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/license/GaiZhenbiao/ChuanhuChatGPT&#34; alt=&#34;LICENSE&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://gradio.app/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Base-Gradio-fb7d1a?style=flat&#34; alt=&#34;Base&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.bilibili.com/video/BV1mo4y1r7eE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Bilibili-%E8%A7%86%E9%A2%91%E6%95%99%E7%A8%8B-ff69b4?style=flat&amp;amp;logo=bilibili&#34; alt=&#34;Bilibili&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;为ChatGPT API提供了一个Web图形界面。在Bilibili上&lt;a href=&#34;https://www.bilibili.com/video/BV1mo4y1r7eE/&#34;&gt;观看视频教程&lt;/a&gt;。也可以在Hugging Face上&lt;a href=&#34;https://huggingface.co/spaces/JohnSmith9982/ChuanhuChatGPT&#34;&gt;在线体验&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/51039745/223148794-f4fd2fcb-3e48-4cdf-a759-7aa463d3f14c.gif&#34; alt=&#34;Animation Demo&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;🎉🎉🎉 重大更新&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;精简了UI&lt;/li&gt; &#xA; &lt;li&gt;像官方ChatGPT那样实时回复&lt;/li&gt; &#xA; &lt;li&gt;改进的保存/加载功能&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;功能&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 像官方客户端那样支持实时显示回答！&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 重试对话，让ChatGPT再回答一次。&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 优化Tokens，减少Tokens占用，以支持更长的对话。&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 设置System Prompt，有效地设定前置条件&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 保存/加载对话历史记录&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; 在图形界面中添加API key&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; System Prompt模板功能，从预置的Prompt库中选择&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; 实时显示Tokens用量&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;使用技巧&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;使用System Prompt可以很有效地设定前提条件&lt;/li&gt; &#xA; &lt;li&gt;对于长对话，可以使用“优化Tokens”按钮减少Tokens占用。&lt;/li&gt; &#xA; &lt;li&gt;如果部署到服务器，将程序最后一句改成&lt;code&gt;demo.launch(server_name=&#34;0.0.0.0&#34;, server_port=99999)&lt;/code&gt;。其中&lt;code&gt;99999&lt;/code&gt;是端口号，应该是1000-65535任意可用端口，请自行更改为实际端口号。&lt;/li&gt; &#xA; &lt;li&gt;如果需要获取公共链接，将程序最后一句改成&lt;code&gt;demo.launch(share=True)&lt;/code&gt;。注意程序必须在运行，才能通过公共链接访问&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;安装方式&lt;/h2&gt; &#xA;&lt;h3&gt;填写API密钥&lt;/h3&gt; &#xA;&lt;h4&gt;在图形界面中填写你的API密钥&lt;/h4&gt; &#xA;&lt;p&gt;这样设置的密钥会在页面刷新后被清除&lt;/p&gt; &#xA;&lt;img width=&#34;760&#34; alt=&#34;image&#34; src=&#34;https://user-images.githubusercontent.com/51039745/222873756-3858bb82-30b9-49bc-9019-36e378ee624d.png&#34;&gt; &#xA;&lt;h4&gt;……或者在代码中填入你的 OpenAI API 密钥&lt;/h4&gt; &#xA;&lt;p&gt;这样设置的密钥会成为默认密钥&lt;/p&gt; &#xA;&lt;img width=&#34;552&#34; alt=&#34;SCR-20230302-sula&#34; src=&#34;https://user-images.githubusercontent.com/51039745/222445258-248f2789-81d2-4f0a-8697-c720f588d8de.png&#34;&gt; &#xA;&lt;h3&gt;安装依赖&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如果报错，试试&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如果还是不行，请先&lt;a href=&#34;https://www.runoob.com/python/python-install.html&#34;&gt;安装Python&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;如果下载慢，建议&lt;a href=&#34;https://mirrors.tuna.tsinghua.edu.cn/help/pypi/&#34;&gt;配置清华源&lt;/a&gt;，或者科学上网。&lt;/p&gt; &#xA;&lt;h3&gt;启动&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python ChuanhuChatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如果报错，试试&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python3 ChuanhuChatbot.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;如果还是不行，请先&lt;a href=&#34;https://www.runoob.com/python/python-install.html&#34;&gt;安装Python&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;或者，使用Docker 安装与运行&lt;/h2&gt; &#xA;&lt;h3&gt;从本项目的Packages页面拉取&lt;/h3&gt; &#xA;&lt;p&gt;从本项目的&lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/pkgs/container/chuanhuchatgpt&#34;&gt;Packages&lt;/a&gt;页面拉取Docker镜像，使用Github Actions自动创建。也可以去本项目的&lt;a href=&#34;https://hub.docker.com/r/tuchuanhuhuhu/chuanhuchatgpt&#34;&gt;Dockerhub页面&lt;/a&gt;拉取。&lt;/p&gt; &#xA;&lt;h3&gt;手动构建镜像&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker build -t chuanhuchatgpt:latest .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;运行&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -d --name chatgpt -e my_api_key=&#34;替换成API&#34;  --network host chuanhuchatgpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;查看本地访问地址&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker logs chatgpt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;部署相关&lt;/h2&gt; &#xA;&lt;h3&gt;部署到公网服务器&lt;/h3&gt; &#xA;&lt;p&gt;将最后一句修改为&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;demo.queue().launch(server_name=&#34;0.0.0.0&#34;, server_port=7860, share=False) # 可自定义端口&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;用账号密码保护页面&lt;/h3&gt; &#xA;&lt;p&gt;将最后一句修改为&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;demo.queue().launch(server_name=&#34;0.0.0.0&#34;, server_port=7860,auth=(&#34;在这里填写用户名&#34;, &#34;在这里填写密码&#34;)) # 可设置用户名与密码&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;疑难杂症解决&lt;/h2&gt; &#xA;&lt;h3&gt;No module named &#39;_bz2&#39;&lt;/h3&gt; &#xA;&lt;p&gt;太空急先锋：部署在CentOS7.6,Python3.11.0上,最后报错ModuleNotFoundError: No module named &#39;_bz2&#39;&lt;/p&gt; &#xA;&lt;p&gt;解决方案：安装python前得下个bzip编译环境&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;sudo yum install bzip2-devel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;openai.error.APIConnectionError&lt;/h3&gt; &#xA;&lt;p&gt;我是一只孤猫 &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/issues/5&#34;&gt;#5&lt;/a&gt;：&lt;/p&gt; &#xA;&lt;p&gt;如果有人也出现了&lt;code&gt;openai.error.APIConnectionError&lt;/code&gt;提示的报错，那可能是&lt;code&gt;urllib3&lt;/code&gt;的版本导致的。&lt;code&gt;urllib3&lt;/code&gt;版本大于&lt;code&gt;1.25.11&lt;/code&gt;，就会出现这个问题。&lt;/p&gt; &#xA;&lt;p&gt;解决方案是卸载&lt;code&gt;urllib3&lt;/code&gt;然后重装至&lt;code&gt;1.25.11&lt;/code&gt;版本再重新运行一遍就可以&lt;/p&gt; &#xA;&lt;p&gt;在终端或命令提示符中卸载&lt;code&gt;urllib3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip uninstall urllib3&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后，您可以通过使用指定版本号的&lt;code&gt;pip install&lt;/code&gt;命令来安装所需的版本：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install urllib3==1.25.11&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;参考自： &lt;a href=&#34;https://zhuanlan.zhihu.com/p/611080662&#34;&gt;解决OpenAI API 挂了代理还是连接不上的问题&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;API 被墙了怎么办&lt;/h3&gt; &#xA;&lt;p&gt;建议把&lt;code&gt;openai.com&lt;/code&gt;加入Clash等软件的分流规则中。&lt;/p&gt; &#xA;&lt;p&gt;跑起来之后，输入问题好像就没反应了，也没报错 &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/issues/25&#34;&gt;#25&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;在 Python 文件里 设定 API Key 之后验证失败&lt;/h3&gt; &#xA;&lt;p&gt;在ChuanhuChatbot.py中设置APIkey后验证出错，提示“发生了未知错误Orz” &lt;a href=&#34;https://github.com/GaiZhenbiao/ChuanhuChatGPT/issues/26&#34;&gt;#26&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;重装 gradio&lt;/h3&gt; &#xA;&lt;p&gt;很多时候，这样就可以解决问题。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install gradio --upgrade --force_reinstall&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;网页提示错误&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;Something went wrong&#xA;Expecting value: 1ine 1 column 1 (char o)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;出现这个错误的原因是&lt;code&gt;127.0.0.1&lt;/code&gt;被代理了，导致网页无法和后端通信。请设置代理软件，将&lt;code&gt;127.0.0.1&lt;/code&gt;加入直连。&lt;/p&gt; &#xA;&lt;h3&gt;No matching distribution found for openai&amp;gt;=0.27.0&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;openai&lt;/code&gt;这个依赖已经被移除了。请尝试下载最新版脚本。&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>mckaywrigley/paul-graham-gpt</title>
    <updated>2023-03-07T01:30:18Z</updated>
    <id>tag:github.com,2023-03-07:/mckaywrigley/paul-graham-gpt</id>
    <link href="https://github.com/mckaywrigley/paul-graham-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;AI search &amp; chat for all of Paul Graham’s essays.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Paul Graham GPT&lt;/h1&gt; &#xA;&lt;p&gt;AI-powered search and chat for &lt;a href=&#34;https://twitter.com/paulg&#34;&gt;Paul Graham&#39;s&lt;/a&gt; &lt;a href=&#34;http://www.paulgraham.com/articles.html&#34;&gt;essays&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;All code &amp;amp; data used is 100% open-source.&lt;/p&gt; &#xA;&lt;h2&gt;Dataset&lt;/h2&gt; &#xA;&lt;p&gt;The dataset is a CSV file containing all text &amp;amp; embeddings used.&lt;/p&gt; &#xA;&lt;p&gt;Download it &lt;a href=&#34;https://drive.google.com/file/d/1BxcPw2mn0VYFucc62wlt9H0nQiOu38ki/view?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;I recommend getting familiar with fetching, cleaning, and storing data as outlined in the scraping and embedding scripts below, but feel free to skip those steps and just use the dataset.&lt;/p&gt; &#xA;&lt;h2&gt;How It Works&lt;/h2&gt; &#xA;&lt;p&gt;Paul Graham GPT provides 2 things:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;A search interface.&lt;/li&gt; &#xA; &lt;li&gt;A chat interface.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Search&lt;/h3&gt; &#xA;&lt;p&gt;Search was created with &lt;a href=&#34;https://platform.openai.com/docs/guides/embeddings&#34;&gt;OpenAI Embeddings&lt;/a&gt; (&lt;code&gt;text-embedding-ada-002&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;First, we loop over the essays and generate embeddings for each chunk of text.&lt;/p&gt; &#xA;&lt;p&gt;Then in the app we take the user&#39;s search query, generate an embedding, and use the result to find the most similar passages from the book.&lt;/p&gt; &#xA;&lt;p&gt;The comparison is done using cosine similarity across our database of vectors.&lt;/p&gt; &#xA;&lt;p&gt;Our database is a Postgres database with the &lt;a href=&#34;https://github.com/pgvector/pgvector&#34;&gt;pgvector&lt;/a&gt; extension hosted on &lt;a href=&#34;https://supabase.com/&#34;&gt;Supabase&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Results are ranked by similarity score and returned to the user.&lt;/p&gt; &#xA;&lt;h3&gt;Chat&lt;/h3&gt; &#xA;&lt;p&gt;Chat builds on top of search. It uses search results to create a prompt that is fed into GPT-3.5-turbo.&lt;/p&gt; &#xA;&lt;p&gt;This allows for a chat-like experience where the user can ask questions about the book and get answers.&lt;/p&gt; &#xA;&lt;h2&gt;Running Locally&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s a quick overview of how to run it locally.&lt;/p&gt; &#xA;&lt;h3&gt;Requirements&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Set up OpenAI&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You&#39;ll need an OpenAI API key to generate embeddings.&lt;/p&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Set up Supabase and create a database&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Note: You don&#39;t have to use Supabase. Use whatever method you prefer to store your data. But I like Supabase and think it&#39;s easy to use.&lt;/p&gt; &#xA;&lt;p&gt;There is a schema.sql file in the root of the repo that you can use to set up the database.&lt;/p&gt; &#xA;&lt;p&gt;Run that in the SQL editor in Supabase as directed.&lt;/p&gt; &#xA;&lt;p&gt;I recommend turning on Row Level Security and setting up a service role to use with the app.&lt;/p&gt; &#xA;&lt;h3&gt;Repo Setup&lt;/h3&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Clone repo&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/mckaywrigley/paul-graham-gpt.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Install dependencies&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm i&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Set up environment variables&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;Create a .env.local file in the root of the repo with the following variables:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;OPENAI_API_KEY=&#xA;&#xA;NEXT_PUBLIC_SUPABASE_URL=&#xA;SUPABASE_SERVICE_ROLE_KEY=&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Dataset&lt;/h3&gt; &#xA;&lt;ol start=&#34;6&#34;&gt; &#xA; &lt;li&gt;Run scraping script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run scrape&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This scrapes all of the essays from Paul Graham&#39;s website and saves them to a json file.&lt;/p&gt; &#xA;&lt;ol start=&#34;7&#34;&gt; &#xA; &lt;li&gt;Run embedding script&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run embed&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This reads the json file, generates embeddings for each chunk of text, and saves the results to your database.&lt;/p&gt; &#xA;&lt;p&gt;There is a 200ms delay between each request to avoid rate limiting.&lt;/p&gt; &#xA;&lt;p&gt;This process will take 20-30 minutes.&lt;/p&gt; &#xA;&lt;h3&gt;App&lt;/h3&gt; &#xA;&lt;ol start=&#34;8&#34;&gt; &#xA; &lt;li&gt;Run app&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Thanks to &lt;a href=&#34;https://twitter.com/paulg&#34;&gt;Paul Graham&lt;/a&gt; for his writing.&lt;/p&gt; &#xA;&lt;p&gt;I highly recommend you read his essays.&lt;/p&gt; &#xA;&lt;p&gt;3 years ago they convinced me to learn to code, and it changed my life.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;If you have any questions, feel free to reach out to me on &lt;a href=&#34;https://twitter.com/mckaywrigley&#34;&gt;Twitter&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;Notes&lt;/h2&gt; &#xA;&lt;p&gt;I sacrificed composability for simplicity in the app.&lt;/p&gt; &#xA;&lt;p&gt;Yes, you can make things more modular and reusable.&lt;/p&gt; &#xA;&lt;p&gt;But I kept pretty much everything in the homepage component for the sake of simplicity.&lt;/p&gt;</summary>
  </entry>
</feed>