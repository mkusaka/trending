<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-03-20T01:22:27Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hpcaitech/Open-Sora</title>
    <updated>2024-03-20T01:22:27Z</updated>
    <id>tag:github.com,2024-03-20:/hpcaitech/Open-Sora</id>
    <link href="https://github.com/hpcaitech/Open-Sora" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open-Sora: Democratizing Efficient Video Production for All&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/icon.png&#34; width=&#34;250&#34;&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/hpcaitech/Open-Sora?style=social&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://hpcaitech.github.io/Open-Sora/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Gallery-View-orange?logo=&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://discord.gg/kZakZzrSUT&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-join-blueviolet?logo=discord&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-247ipg9fk-KRRYmUl~u2ll2637WRURVA&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Slack-ColossalAI-blueviolet?logo=slack&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://twitter.com/yangyou1991/status/1769411544083996787?s=61&amp;amp;t=jT0Dsx2d-MS5vS9rNM5e5g&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Twitter-Discuss-blue?logo=twitter&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/ÂæÆ‰ø°-Â∞èÂä©ÊâãÂä†Áæ§-green?logo=wechat&amp;amp;&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://hpc-ai.com/blog/open-sora-v1.0&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Open_Sora-Blog-blue&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Open-Sora: Democratizing Efficient Video Production for All&lt;/h2&gt; &#xA;&lt;p&gt;We present &lt;strong&gt;Open-Sora&lt;/strong&gt;, an initiative dedicated to &lt;strong&gt;efficiently&lt;/strong&gt; produce high-quality video and make the model, tools and contents accessible to all. By embracing &lt;strong&gt;open-source&lt;/strong&gt; principles, Open-Sora not only democratizes access to advanced video generation techniques, but also offers a streamlined and user-friendly platform that simplifies the complexities of video production. With Open-Sora, we aim to inspire innovation, creativity, and inclusivity in the realm of content creation. &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/README_zh.md&#34;&gt;[‰∏≠Êñá]&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Open-Sora is still at an early stage and under active development.&lt;/h4&gt; &#xA;&lt;h2&gt;üì∞ News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.03.18]&lt;/strong&gt; üî• We release &lt;strong&gt;Open-Sora 1.0&lt;/strong&gt;, a fully open-source project for video generation. Open-Sora 1.0 supports a full pipeline of video data preprocessing, training with &lt;a href=&#34;https://github.com/hpcaitech/ColossalAI&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/colossal_ai.png&#34; width=&#34;8%&#34;&gt;&lt;/a&gt; acceleration, inference, and more. Our provided &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#model-weights&#34;&gt;checkpoints&lt;/a&gt; can produce 2s 512x512 videos with only 3 days training.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;[2024.03.04]&lt;/strong&gt; Open-Sora provides training with 46% cost reduction.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üé• Latest Demo&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;2s 512√ó512&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;2s 512√ó512&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;2s 512√ó512&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/de1963d3-b43b-4e68-a670-bb821ebb6f80&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_0.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/13f8338f-3d42-4b71-8142-d234fbd746cc&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_1.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/fa6a65a6-e32a-4d64-9a9e-eabb0ebb8c16&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_2.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A serene night scene in a forested area. [...] The video is a time-lapse, capturing the transition from day to night, with the lake and forest serving as a constant backdrop.&lt;/td&gt; &#xA;   &lt;td&gt;A soaring drone footage captures the majestic beauty of a coastal cliff, [...] The water gently laps at the rock base and the greenery that clings to the top of the cliff.&lt;/td&gt; &#xA;   &lt;td&gt;The majestic beauty of a waterfall cascading down a cliff into a serene lake. [...] The camera angle provides a bird&#39;s eye view of the waterfall.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/64232f84-1b36-4750-a6c0-3e610fa9aa94&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_3.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/983a1965-a374-41a7-a76b-c07941a6c1e9&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_4.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/assets/99191637/ec10c879-9767-4c31-865f-2e8d6cf11e65&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/readme/sample_5.gif&#34; width=&#34;&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;A bustling city street at night, filled with the glow of car headlights and the ambient light of streetlights. [...]&lt;/td&gt; &#xA;   &lt;td&gt;The vibrant beauty of a sunflower field. The sunflowers are arranged in neat rows, creating a sense of order and symmetry. [...]&lt;/td&gt; &#xA;   &lt;td&gt;A serene underwater scene featuring a sea turtle swimming through a coral reef. The turtle, with its greenish-brown shell [...]&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Videos are downsampled to &lt;code&gt;.gif&lt;/code&gt; for display. Click for original videos. Prompts are trimmed for display, see &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/assets/texts/t2v_samples.txt&#34;&gt;here&lt;/a&gt; for full prompts. See more samples at our &lt;a href=&#34;https://hpcaitech.github.io/Open-Sora/&#34;&gt;gallery&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üîÜ New Features/Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üìç Open-Sora-v1 released. Model weights are available &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#model-weights&#34;&gt;here&lt;/a&gt;. With only 400K video clips and 200 H800 days (compared with 152M samples in Stable Video Diffusion), we are able to generate 2s 512√ó512 videos.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Three stages training from an image diffusion model to a video diffusion model. We provide the weights for each stage.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Support training acceleration including accelerated transformer, faster T5 and VAE, and sequence parallelism. Open-Sora improve &lt;strong&gt;55%&lt;/strong&gt; training speed when training on 64x512x512 videos. Details locates at &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/acceleration.md&#34;&gt;acceleration.md&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ We provide data preprocessing pipeline, including &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/datasets/README.md&#34;&gt;downloading&lt;/a&gt;, &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/scenedetect/README.md&#34;&gt;video cutting&lt;/a&gt;, and &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/caption/README.md&#34;&gt;captioning&lt;/a&gt; tools. Our data collection plan can be found at &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/datasets.md&#34;&gt;datasets.md&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ We find VQ-VAE from &lt;a href=&#34;https://wilson1yan.github.io/videogpt/index.html&#34;&gt;VideoGPT&lt;/a&gt; has a low quality and thus adopt a better VAE from &lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse-original&#34;&gt;Stability-AI&lt;/a&gt;. We also find patching in the time dimension deteriorates the quality. See our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_v1.md&#34;&gt;report&lt;/a&gt;&lt;/strong&gt; for more discussions.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ We investigate different architectures including DiT, Latte, and our proposed STDiT. Our &lt;strong&gt;STDiT&lt;/strong&gt; achieves a better trade-off between quality and speed. See our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_v1.md&#34;&gt;report&lt;/a&gt;&lt;/strong&gt; for more discussions.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Support clip and T5 text conditioning.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ By viewing images as one-frame videos, our project supports training DiT on both images and videos (e.g., ImageNet &amp;amp; UCF101). See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/command.md&#34;&gt;command.md&lt;/a&gt; for more instructions.&lt;/li&gt; &#xA; &lt;li&gt;‚úÖ Support inference with official weights from &lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt;, &lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt;, and &lt;a href=&#34;https://pixart-alpha.github.io/&#34;&gt;PixArt&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;‚úÖ Refactor the codebase. See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/structure.md&#34;&gt;structure.md&lt;/a&gt; to learn the project structure and how to use the config files.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;TODO list sorted by priority&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Complete the data processing pipeline (including dense optical flow, aesthetics scores, text-image similarity, deduplication, etc.). See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/datasets.md&#34;&gt;datasets.md&lt;/a&gt; for more information. &lt;strong&gt;[WIP]&lt;/strong&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Training Video-VAE. &lt;strong&gt;[WIP]&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;View more&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support image and video conditioning.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Evaluation pipeline.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Incoporate a better scheduler, e.g., rectified flow in SD3.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support variable aspect ratios, resolutions, durations.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Support SD3 when released.&lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#model-weights&#34;&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#data-processing&#34;&gt;Data Processing&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#contribution&#34;&gt;Contribution&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#acknowledgement&#34;&gt;Acknowledgement&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# create a virtual env&#xA;conda create -n opensora python=3.10&#xA;&#xA;# install torch&#xA;# the command below is for CUDA 12.1, choose install commands from &#xA;# https://pytorch.org/get-started/locally/ based on your own CUDA version&#xA;pip3 install torch torchvision&#xA;&#xA;# install flash attention (optional)&#xA;pip install packaging ninja&#xA;pip install flash-attn --no-build-isolation&#xA;&#xA;# install apex (optional)&#xA;pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings &#34;--build-option=--cpp_ext&#34; --config-settings &#34;--build-option=--cuda_ext&#34; git+https://github.com/NVIDIA/apex.git&#xA;&#xA;# install xformers&#xA;pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu121&#xA;&#xA;# install this project&#xA;git clone https://github.com/hpcaitech/Open-Sora&#xA;cd Open-Sora&#xA;pip install -v .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;After installation, we suggest reading &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/structure.md&#34;&gt;structure.md&lt;/a&gt; to learn the project structure and how to use the config files.&lt;/p&gt; &#xA;&lt;h2&gt;Model Weights&lt;/h2&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Resolution&lt;/th&gt; &#xA;   &lt;th&gt;Data&lt;/th&gt; &#xA;   &lt;th&gt;#iterations&lt;/th&gt; &#xA;   &lt;th&gt;Batch Size&lt;/th&gt; &#xA;   &lt;th&gt;GPU days (H800)&lt;/th&gt; &#xA;   &lt;th&gt;URL&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16√ó256√ó256&lt;/td&gt; &#xA;   &lt;td&gt;366K&lt;/td&gt; &#xA;   &lt;td&gt;80k&lt;/td&gt; &#xA;   &lt;td&gt;8√ó64&lt;/td&gt; &#xA;   &lt;td&gt;117&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/Open-Sora/blob/main/OpenSora-v1-16x256x256.pth&#34;&gt;&lt;span&gt;üîó&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16√ó256√ó256&lt;/td&gt; &#xA;   &lt;td&gt;20K HQ&lt;/td&gt; &#xA;   &lt;td&gt;24k&lt;/td&gt; &#xA;   &lt;td&gt;8√ó64&lt;/td&gt; &#xA;   &lt;td&gt;45&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/Open-Sora/blob/main/OpenSora-v1-HQ-16x256x256.pth&#34;&gt;&lt;span&gt;üîó&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;16√ó512√ó512&lt;/td&gt; &#xA;   &lt;td&gt;20K HQ&lt;/td&gt; &#xA;   &lt;td&gt;20k&lt;/td&gt; &#xA;   &lt;td&gt;2√ó64&lt;/td&gt; &#xA;   &lt;td&gt;35&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/hpcai-tech/Open-Sora/blob/main/OpenSora-v1-HQ-16x512x512.pth&#34;&gt;&lt;span&gt;üîó&lt;/span&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Our model&#39;s weight is partially initialized from &lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt-Œ±&lt;/a&gt;. The number of parameters is 724M. More information about training can be found in our &lt;strong&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/report_v1.md&#34;&gt;report&lt;/a&gt;&lt;/strong&gt;. More about dataset can be found in &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/dataset.md&#34;&gt;dataset.md&lt;/a&gt;. HQ means high quality.&lt;/p&gt; &#xA;&lt;p&gt;&lt;span&gt;‚ö†&lt;/span&gt; &lt;strong&gt;LIMITATION&lt;/strong&gt;: Our model is trained on a limited budget. The quality and text alignment is relatively poor. The model performs badly especially on generating human beings and cannot follow detailed instructions. We are working on improving the quality and text alignment.&lt;/p&gt; &#xA;&lt;h2&gt;Inference&lt;/h2&gt; &#xA;&lt;p&gt;To run inference with our provided weights, first download &lt;a href=&#34;https://huggingface.co/DeepFloyd/t5-v1_1-xxl/tree/main&#34;&gt;T5&lt;/a&gt; weights into &lt;code&gt;pretrained_models/t5_ckpts/t5-v1_1-xxl&lt;/code&gt;. Then download the model weights from &lt;a href=&#34;https://huggingface.co/hpcai-tech/Open-Sora/tree/main&#34;&gt;huggingface&lt;/a&gt;. Run the following commands to generate samples. To change sampling prompts, modify the txt file passed to &lt;code&gt;--prompt-path&lt;/code&gt;. See &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/structure.md#inference-config-demos&#34;&gt;here&lt;/a&gt; to customize the configuration.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Sample 16x256x256 (5s/sample, 100 time steps, 22 GB memory)&#xA;torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/16x256x256.py --ckpt-path ./path/to/your/ckpt.pth --prompt-path ./asserts/texts/t2v_samples.txt&#xA;# Auto Download&#xA;torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/16x256x256.py --ckpt-path OpenSora-v1-HQ-16x256x256.pth --prompt-path ./assets/texts/t2v_samples.txt&#xA;&#xA;# Sample 16x512x512 (20s/sample, 100 time steps, 24 GB memory)&#xA;torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/16x512x512.py --ckpt-path ./path/to/your/ckpt.pth --prompt-path ./asserts/texts/t2v_samples.txt&#xA;# Auto Download&#xA;torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/16x512x512.py --ckpt-path OpenSora-v1-HQ-16x512x512.pth --prompt-path ./assets/texts/t2v_samples.txt&#xA;&#xA;# Sample 64x512x512 (40s/sample, 100 time steps)&#xA;torchrun --standalone --nproc_per_node 1 scripts/inference.py configs/opensora/inference/64x512x512.py --ckpt-path ./path/to/your/ckpt.pth --prompt-path ./asserts/texts/t2v_samples.txt&#xA;&#xA;# Sample 64x512x512 with sequence parallelism (30s/sample, 100 time steps)&#xA;# sequence parallelism is enabled automatically when nproc_per_node is larger than 1&#xA;torchrun --standalone --nproc_per_node 2 scripts/inference.py configs/opensora/inference/64x512x512.py --ckpt-path ./path/to/your/ckpt.pth --prompt-path ./asserts/texts/t2v_samples.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The speed is tested on H800 GPUs. For inference with other models, see &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/commands.md&#34;&gt;here&lt;/a&gt; for more instructions. To lower the memory usage, set a smaller &lt;code&gt;vae.micro_batch_size&lt;/code&gt; in the config (slightly lower sampling speed).&lt;/p&gt; &#xA;&lt;h2&gt;Data Processing&lt;/h2&gt; &#xA;&lt;p&gt;High-quality Data is the key to high-quality models. Our used datasets and data collection plan is &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/datasets.md&#34;&gt;here&lt;/a&gt;. We provide tools to process video data. Currently, our data processing pipeline includes the following steps:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Downloading datasets. [&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/datasets/README.md&#34;&gt;docs&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Split videos into clips. [&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/scenedetect/README.md&#34;&gt;docs&lt;/a&gt;]&lt;/li&gt; &#xA; &lt;li&gt;Generate video captions. [&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/tools/caption/README.md&#34;&gt;docs&lt;/a&gt;]&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Training&lt;/h2&gt; &#xA;&lt;p&gt;To launch training, first download &lt;a href=&#34;https://huggingface.co/DeepFloyd/t5-v1_1-xxl/tree/main&#34;&gt;T5&lt;/a&gt; weights into &lt;code&gt;pretrained_models/t5_ckpts/t5-v1_1-xxl&lt;/code&gt;. Then run the following commands to launch training on a single node.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# 1 GPU, 16x256x256&#xA;torchrun --nnodes=1 --nproc_per_node=1 scripts/train.py configs/opensora/train/16x256x256.py --data-path YOUR_CSV_PATH&#xA;# 8 GPUs, 64x512x512&#xA;torchrun --nnodes=1 --nproc_per_node=8 scripts/train.py configs/opensora/train/64x512x512.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To launch training on multiple nodes, prepare a hostfile according to &lt;a href=&#34;https://colossalai.org/docs/basics/launch_colossalai/#launch-with-colossal-ai-cli&#34;&gt;ColossalAI&lt;/a&gt;, and run the following commands.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;colossalai run --nproc_per_node 8 --hostfile hostfile scripts/train.py configs/opensora/train/64x512x512.py --data-path YOUR_CSV_PATH --ckpt-path YOUR_PRETRAINED_CKPT&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For training other models and advanced usage, see &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/docs/commands.md&#34;&gt;here&lt;/a&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h2&gt;Contribution&lt;/h2&gt; &#xA;&lt;p&gt;Thanks goes to these wonderful contributors (&lt;a href=&#34;https://allcontributors.org/docs/en/emoji-key&#34;&gt;emoji key&lt;/a&gt; following &lt;a href=&#34;https://github.com/all-contributors/all-contributors&#34;&gt;all-contributors&lt;/a&gt; specification):&lt;/p&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section --&gt; &#xA;&lt;!-- prettier-ignore-start --&gt; &#xA;&lt;!-- markdownlint-disable --&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/zhengzangw&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/zhengzangw?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;zhengzangw&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;zhengzangw&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/commits?author=zhengzangw&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/commits?author=zhengzangw&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#ideas-zhengzangw&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#video-zhengzangw&#34; title=&#34;Videos&#34;&gt;üìπ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#maintenance-zhengzangw&#34; title=&#34;Maintenance&#34;&gt;üöß&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/ver217&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/ver217?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;ver217&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;ver217&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/commits?author=ver217&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#ideas-ver217&#34; title=&#34;Ideas, Planning, &amp;amp; Feedback&#34;&gt;ü§î&lt;/a&gt; &lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/commits?author=ver217&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#bug-ver217&#34; title=&#34;Bug reports&#34;&gt;üêõ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/nkLeeeee&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/nkLeeeee?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;nkLeeeee&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;nkLeeeee&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/commits?author=nkLeeeee&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#infra-nkLeeeee&#34; title=&#34;Infrastructure (Hosting, Build-Tools, etc)&#34;&gt;üöá&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#tool-nkLeeeee&#34; title=&#34;Tools&#34;&gt;üîß&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/xyupeng&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/xyupeng?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;xyupeng&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;xyupeng&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://github.com/hpcaitech/Open-Sora/commits?author=xyupeng&#34; title=&#34;Code&#34;&gt;üíª&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#doc-xyupeng&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#design-xyupeng&#34; title=&#34;Design&#34;&gt;üé®&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/Yanjia0&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/Yanjia0?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;Yanjia0&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;Yanjia0&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#doc-Yanjia0&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/binmakeswell&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/binmakeswell?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;binmakeswell&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;binmakeswell&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#doc-binmakeswell&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/eltociear&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/eltociear?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;eltociear&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;eltociear&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#doc-eltociear&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/ganeshkrishnan1&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/ganeshkrishnan1?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;ganeshkrishnan1&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;ganeshkrishnan1&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#doc-ganeshkrishnan1&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/fastalgo&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/fastalgo?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;fastalgo&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;fastalgo&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#doc-fastalgo&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34; valign=&#34;top&#34; width=&#34;14.28%&#34;&gt;&lt;a href=&#34;https://github.com/powerzbt&#34;&gt;&lt;img src=&#34;https://avatars.githubusercontent.com/powerzbt?v=4?s=100&#34; width=&#34;100px;&#34; alt=&#34;powerzbt&#34;&gt;&lt;br&gt;&lt;sub&gt;&lt;b&gt;powerzbt&lt;/b&gt;&lt;/sub&gt;&lt;/a&gt;&lt;br&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/#doc-powerzbt&#34; title=&#34;Documentation&#34;&gt;üìñ&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;!-- markdownlint-restore --&gt; &#xA;&lt;!-- prettier-ignore-end --&gt; &#xA;&lt;!-- ALL-CONTRIBUTORS-LIST:END --&gt; &#xA;&lt;p&gt;If you wish to contribute to this project, you can refer to the &lt;a href=&#34;https://raw.githubusercontent.com/hpcaitech/Open-Sora/main/CONTRIBUTING.md&#34;&gt;Contribution Guideline&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/DiT&#34;&gt;DiT&lt;/a&gt;: Scalable Diffusion Models with Transformers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/NUS-HPC-AI-Lab/OpenDiT&#34;&gt;OpenDiT&lt;/a&gt;: An acceleration for DiT training. We adopt valuable acceleration strategies for training progress from OpenDiT.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/PixArt-alpha/PixArt-alpha&#34;&gt;PixArt&lt;/a&gt;: An open-source DiT-based text-to-image model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Vchitect/Latte&#34;&gt;Latte&lt;/a&gt;: An attempt to efficiently train DiT for video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/sd-vae-ft-mse-original&#34;&gt;StabilityAI VAE&lt;/a&gt;: A powerful image VAE model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/openai/CLIP&#34;&gt;CLIP&lt;/a&gt;: A powerful text-image embedding model.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/google-research/text-to-text-transfer-transformer&#34;&gt;T5&lt;/a&gt;: A powerful text encoder.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/haotian-liu/LLaVA&#34;&gt;LLaVA&lt;/a&gt;: A powerful image captioning model based on &lt;a href=&#34;https://huggingface.co/01-ai/Yi-34B&#34;&gt;Yi-34B&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;We are grateful for their exceptional work and generous contribution to open source.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@software{opensora,&#xA;  author = {Zangwei Zheng and Xiangyu Peng and Yang You},&#xA;  title = {Open-Sora: Democratizing Efficient Video Production for All},&#xA;  month = {March},&#xA;  year = {2024},&#xA;  url = {https://github.com/hpcaitech/Open-Sora}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/zhengzangw&#34;&gt;Zangwei Zheng&lt;/a&gt; and &lt;a href=&#34;https://github.com/xyupeng&#34;&gt;Xiangyu Peng&lt;/a&gt; equally contributed to this work during their internship at &lt;a href=&#34;https://hpc-ai.com/&#34;&gt;HPC-AI Tech&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#hpcaitech/Open-Sora&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=hpcaitech/Open-Sora&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>roboflow/supervision</title>
    <updated>2024-03-20T01:22:27Z</updated>
    <id>tag:github.com,2024-03-20:/roboflow/supervision</id>
    <link href="https://github.com/roboflow/supervision" rel="alternate"></link>
    <summary type="html">&lt;p&gt;We write your reusable computer vision tools. üíú&lt;/p&gt;&lt;hr&gt;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt; &lt;a align=&#34;center&#34; href=&#34;&#34; target=&#34;https://supervision.roboflow.com&#34;&gt; &lt;img width=&#34;100%&#34; src=&#34;https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/notebooks&#34;&gt;notebooks&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/inference&#34;&gt;inference&lt;/a&gt; | &lt;a href=&#34;https://github.com/autodistill/autodistill&#34;&gt;autodistill&lt;/a&gt; | &lt;a href=&#34;https://github.com/roboflow/multimodal-maestro&#34;&gt;maestro&lt;/a&gt;&lt;/p&gt; &#xA; &lt;br&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://badge.fury.io/py/supervision.svg?sanitize=true&#34; alt=&#34;version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypistats.org/packages/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/dm/supervision&#34; alt=&#34;downloads&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/LICENSE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/l/supervision&#34; alt=&#34;license&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://badge.fury.io/py/supervision&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/supervision&#34; alt=&#34;python-version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Colab&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/spaces/Roboflow/Annotators&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue&#34; alt=&#34;Gradio&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/GbfgXGJ8Bk&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1159501506232451173&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://squidfunk.github.io/mkdocs-material/&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&amp;amp;logoColor=white&#34; alt=&#34;Built with Material for MkDocs&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;üëã hello&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;We write your reusable computer vision tools.&lt;/strong&gt; Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! ü§ù&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/orgs/roboflow/projects/10&#34;&gt;&lt;img src=&#34;https://github.com/roboflow/supervision/assets/26109316/c05cc954-b9a6-4ed5-9a52-d0b4b619ff65&#34; alt=&#34;supervision-hackfest&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üíª install&lt;/h2&gt; &#xA;&lt;p&gt;Pip install the supervision package in a &lt;a href=&#34;https://www.python.org/&#34;&gt;&lt;strong&gt;Python&amp;gt;=3.8&lt;/strong&gt;&lt;/a&gt; environment.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install supervision&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Read more about desktop, headless, and local installation in our &lt;a href=&#34;https://roboflow.github.io/supervision/&#34;&gt;guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üî• quickstart&lt;/h2&gt; &#xA;&lt;h3&gt;models&lt;/h3&gt; &#xA;&lt;p&gt;Supervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created &lt;a href=&#34;https://supervision.roboflow.com/latest/detection/core/#detections&#34;&gt;connectors&lt;/a&gt; for the most popular libraries like Ultralytics, Transformers, or MMDetection.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;from ultralytics import YOLO&#xA;&#xA;image = cv2.imread(...)&#xA;model = YOLO(&#39;yolov8s.pt&#39;)&#xA;result = model(image)[0]&#xA;detections = sv.Detections.from_ultralytics(result)&#xA;&#xA;len(detections)&#xA;# 5&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;üëâ more model connectors&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;inference&lt;/p&gt; &lt;p&gt;Running with &lt;a href=&#34;https://github.com/roboflow/inference&#34;&gt;Inference&lt;/a&gt; requires a &lt;a href=&#34;https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key&#34;&gt;Roboflow API KEY&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;from inference.models.utils import get_roboflow_model&#xA;&#xA;image = cv2.imread(...)&#xA;model = get_roboflow_model(model_id=&#34;yolov8s-640&#34;, api_key=&amp;lt;ROBOFLOW API KEY&amp;gt;)&#xA;result = model.infer(image)[0]&#xA;detections = sv.Detections.from_inference(result)&#xA;&#xA;len(detections)&#xA;#&amp;nbsp;5&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h3&gt;annotators&lt;/h3&gt; &#xA;&lt;p&gt;Supervision offers a wide range of highly customizable &lt;a href=&#34;https://supervision.roboflow.com/latest/annotators/&#34;&gt;annotators&lt;/a&gt;, allowing you to compose the perfect visualization for your use case.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import cv2&#xA;import supervision as sv&#xA;&#xA;image = cv2.imread(...)&#xA;detections = sv.Detections(...)&#xA;&#xA;bounding_box_annotator = sv.BoundingBoxAnnotator()&#xA;annotated_frame = bounding_box_annotator.annotate(&#xA;    scene=image.copy(),&#xA;    detections=detections&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;datasets&lt;/h3&gt; &#xA;&lt;p&gt;Supervision provides a set of &lt;a href=&#34;https://supervision.roboflow.com/latest/datasets/&#34;&gt;utils&lt;/a&gt; that allow you to load, split, merge, and save datasets in one of the supported formats.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import supervision as sv&#xA;&#xA;dataset = sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset.classes&#xA;[&#39;dog&#39;, &#39;person&#39;]&#xA;&#xA;len(dataset)&#xA;#&amp;nbsp;1000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details close&gt; &#xA; &lt;summary&gt;üëâ more dataset utils&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt; &lt;p&gt;load&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset = sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset = sv.DetectionDataset.from_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&#xA;dataset = sv.DetectionDataset.from_coco(&#xA;    images_directory_path=...,&#xA;    annotations_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;split&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_dataset, test_dataset = dataset.split(split_ratio=0.7)&#xA;test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)&#xA;&#xA;len(train_dataset), len(test_dataset), len(valid_dataset)&#xA;#&amp;nbsp;(700, 150, 150)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;merge&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;ds_1 = sv.DetectionDataset(...)&#xA;len(ds_1)&#xA;#&amp;nbsp;100&#xA;ds_1.classes&#xA;#&amp;nbsp;[&#39;dog&#39;, &#39;person&#39;]&#xA;&#xA;ds_2 = sv.DetectionDataset(...)&#xA;len(ds_2)&#xA;# 200&#xA;ds_2.classes&#xA;#&amp;nbsp;[&#39;cat&#39;]&#xA;&#xA;ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])&#xA;len(ds_merged)&#xA;#&amp;nbsp;300&#xA;ds_merged.classes&#xA;#&amp;nbsp;[&#39;cat&#39;, &#39;dog&#39;, &#39;person&#39;]&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;save&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dataset.as_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;)&#xA;&#xA;dataset.as_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&#xA;dataset.as_coco(&#xA;    images_directory_path=...,&#xA;    annotations_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;  &lt;li&gt; &lt;p&gt;convert&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sv.DetectionDataset.from_yolo(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...,&#xA;    data_yaml_path=...&#xA;).as_pascal_voc(&#xA;    images_directory_path=...,&#xA;    annotations_directory_path=...&#xA;)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;üé¨ tutorials&lt;/h2&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/uWP6UjDeZvY&#34; title=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34;&gt;&lt;img src=&#34;https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91&#34; alt=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/uWP6UjDeZvY&#34; title=&#34;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&#34;&gt;&lt;strong&gt;Speed Estimation &amp;amp; Vehicle Tracking | Computer Vision | Open Source&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 11 Jan 2024&lt;/strong&gt; | &#xA; &lt;strong&gt;Updated: 11 Jan 2024&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.&#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p align=&#34;left&#34;&gt; &lt;a href=&#34;https://youtu.be/4Q3ut7vqD5o&#34; title=&#34;Traffic Analysis with YOLOv8 and ByteTrack - Vehicle Detection and Tracking&#34;&gt;&lt;img src=&#34;https://github.com/roboflow/supervision/assets/26109316/54afdf1c-218c-4451-8f12-627fb85f1682&#34; alt=&#34;Traffic Analysis with YOLOv8 and ByteTrack - Vehicle Detection and Tracking&#34; width=&#34;300px&#34; align=&#34;left&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://youtu.be/4Q3ut7vqD5o&#34; title=&#34;Traffic Analysis with YOLOv8 and ByteTrack - Vehicle Detection and Tracking&#34;&gt;&lt;strong&gt;Traffic Analysis with YOLOv8 and ByteTrack - Vehicle Detection and Tracking&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt;&#xA;&lt;div&gt;&#xA; &lt;strong&gt;Created: 6 Sep 2023&lt;/strong&gt; | &#xA; &lt;strong&gt;Updated: 6 Sep 2023&lt;/strong&gt;&#xA;&lt;/div&gt; &#xA;&lt;br&gt; In this video, we explore real-time traffic analysis using YOLOv8 and ByteTrack to detect and track vehicles on aerial images. Harnessing the power of Python and Supervision, we delve deep into assigning cars to specific entry zones and understanding their direction of movement. By visualizing their paths, we gain insights into traffic flow across bustling roundabouts... &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üíú built with supervision&lt;/h2&gt; &#xA;&lt;p&gt;Did you build something cool using supervision? &lt;a href=&#34;https://github.com/roboflow/supervision/discussions/categories/built-with-supervision&#34;&gt;Let us know!&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&#34;&gt;https://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f&#34;&gt;https://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üìö documentation&lt;/h2&gt; &#xA;&lt;p&gt;Visit our &lt;a href=&#34;https://roboflow.github.io/supervision&#34;&gt;documentation&lt;/a&gt; page to learn how supervision can help you build computer vision applications faster and more reliably.&lt;/p&gt; &#xA;&lt;h2&gt;üèÜ contribution&lt;/h2&gt; &#xA;&lt;p&gt;We love your input! Please see our &lt;a href=&#34;https://github.com/roboflow/supervision/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt; to get started. Thank you üôè to all our contributors!&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://github.com/roboflow/supervision/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=roboflow/supervision&#34;&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;div align=&#34;center&#34;&gt; &#xA;  &lt;a href=&#34;https://youtube.com/roboflow&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634652&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949746649&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://www.linkedin.com/company/roboflow-ai/&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633691&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://docs.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949634511&#34; width=&#34;3%&#34;&gt; &lt;/a&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &#xA;  &lt;a href=&#34;https://disuss.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633584&#34; width=&#34;3%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;&#xA;  &lt;a href=&#34;https://blog.roboflow.com&#34;&gt; &lt;img src=&#34;https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&amp;amp;updatedAt=1672949633605&#34; width=&#34;3%&#34;&gt; &lt;/a&gt;  &#xA; &lt;/div&gt; &#xA;&lt;/div&gt;</summary>
  </entry>
  <entry>
    <title>continuedev/continue</title>
    <updated>2024-03-20T01:22:27Z</updated>
    <id>tag:github.com,2024-03-20:/continuedev/continue</id>
    <link href="https://github.com/continuedev/continue" rel="alternate"></link>
    <summary type="html">&lt;p&gt;‚è© The easiest way to code with any LLM‚ÄîContinue is an open-source autopilot for VS Code and JetBrains&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/continuedev/continue/main/media/c_d.png&#34; alt=&#34;Continue logo&#34;&gt;&lt;/p&gt; &#xA;&lt;h1 align=&#34;center&#34;&gt;Continue&lt;/h1&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://continue.dev/docs&#34;&gt;Continue&lt;/a&gt; is an open-source autopilot for &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=Continue.continue&#34;&gt;VS Code&lt;/a&gt; and &lt;a href=&#34;https://plugins.jetbrains.com/plugin/22707-continue-extension&#34;&gt;JetBrains&lt;/a&gt;‚Äîthe easiest way to code with any LLM&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://opensource.org/licenses/Apache-2.0&#34; style=&#34;background:none&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; style=&#34;height: 22px;&#34;&gt; &lt;/a&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://continue.dev/docs&#34; style=&#34;background:none&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/continue_docs-%23BE1B55&#34; style=&#34;height: 22px;&#34;&gt; &lt;/a&gt; &#xA; &lt;a target=&#34;_blank&#34; href=&#34;https://discord.gg/vapESyrFmJ&#34; style=&#34;background:none&#34;&gt; &lt;img src=&#34;https://img.shields.io/badge/discord-join-continue.svg?labelColor=191937&amp;amp;color=6F6FF7&amp;amp;logo=discord&#34; style=&#34;height: 22px;&#34;&gt; &lt;/a&gt; &#xA; &lt;p&gt;&lt;/p&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/continuedev/continue/main/media/readme.gif&#34; alt=&#34;Editing With Continue&#34;&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h2&gt;Task and tab autocomplete&lt;/h2&gt; &#xA;&lt;h3&gt;Answer coding questions&lt;/h3&gt; &#xA;&lt;p&gt;Highlight + select sections of code and ask Continue for another perspective&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚Äúwhat does this forRoot() static function do in nestjs?‚Äù&lt;/li&gt; &#xA; &lt;li&gt;‚Äúwhy is the first left join in this query necessary here?‚Äù&lt;/li&gt; &#xA; &lt;li&gt;‚Äúhow do I run a performance benchmark on this rust binary?‚Äù&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Edit in natural language&lt;/h3&gt; &#xA;&lt;p&gt;Highlight + select a section of code and instruct Continue to refactor it&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚Äú/edit rewrite this to return a flattened list from a 3x3 matrix‚Äù&lt;/li&gt; &#xA; &lt;li&gt;‚Äú/edit refactor these into an angular flex layout on one line&#34;&lt;/li&gt; &#xA; &lt;li&gt;‚Äú/edit define a type here for a list of lists of dictionaries‚Äù&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Generate files from scratch&lt;/h3&gt; &#xA;&lt;p&gt;Open a blank file and let Continue start new Python scripts, React components, etc.&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;‚Äú/edit get me started with a basic supabase edge function‚Äù&lt;/li&gt; &#xA; &lt;li&gt;‚Äú/edit implement a c++ shortest path algo in a concise way‚Äù&lt;/li&gt; &#xA; &lt;li&gt;‚Äú/edit create a docker compose file with php and mysql server&#34;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;And much more!&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Try out &lt;a href=&#34;https://continue.dev/docs/walkthroughs/tab-autocomplete&#34;&gt;experimental support for local tab autocomplete&lt;/a&gt; in VS Code&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://continue.dev/docs/customization/context-providers#built-in-context-providers&#34;&gt;built-in context providers&lt;/a&gt; or create your own &lt;a href=&#34;https://continue.dev/docs/customization/context-providers#building-your-own-context-provider&#34;&gt;custom context providers&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Use &lt;a href=&#34;https://arc.net/l/quote/zbhwfjmp&#34;&gt;built-in slash commands&lt;/a&gt; or create your own &lt;a href=&#34;https://continue.dev/docs/customization/slash-commands#custom-slash-commands&#34;&gt;custom slash commands&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;h4&gt;Download for &lt;a href=&#34;https://marketplace.visualstudio.com/items?itemName=Continue.continue&#34;&gt;VS Code&lt;/a&gt; and &lt;a href=&#34;https://plugins.jetbrains.com/plugin/22707-continue-extension&#34;&gt;JetBrains&lt;/a&gt;&lt;/h4&gt; &#xA;&lt;p&gt;You can try out Continue for free using a proxy server that securely makes calls with our API key to models like GPT-4, Gemini Pro, and Phind CodeLlama via OpenAI, Google, and Together respectively.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;re ready to use your own API key or a different model / provider, press the &lt;code&gt;+&lt;/code&gt; button in the bottom left to add a new model to your &lt;code&gt;config.json&lt;/code&gt;. Learn more about the models and providers &lt;a href=&#34;https://continue.dev/docs/model-setup/overview&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Check out the &lt;a href=&#34;https://github.com/orgs/continuedev/projects/2&#34;&gt;contribution ideas board&lt;/a&gt;, read the &lt;a href=&#34;https://github.com/continuedev/continue/raw/main/CONTRIBUTING.md&#34;&gt;contributing guide&lt;/a&gt;, and join &lt;a href=&#34;https://discord.gg/vapESyrFmJ&#34;&gt;#contribute on Discord&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/continuedev/continue/main/LICENSE&#34;&gt;Apache 2.0 ¬© 2023 Continue Dev, Inc.&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>