<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-09-21T01:22:47Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>grafana/beyla</title>
    <updated>2023-09-21T01:22:47Z</updated>
    <id>tag:github.com,2023-09-21:/grafana/beyla</id>
    <link href="https://github.com/grafana/beyla" rel="alternate"></link>
    <summary type="html">&lt;p&gt;eBPF-based autoinstrumentation of HTTP and HTTPS services&lt;/p&gt;&lt;hr&gt;&lt;img src=&#34;https://raw.githubusercontent.com/grafana/beyla/main/docs/sources/assets/logo.png&#34; height=&#34;226&#34; alt=&#34;Grafana Beyla logo&#34;&gt; &#xA;&lt;h1&gt;Grafana Beyla&lt;/h1&gt; &#xA;&lt;p&gt;eBPF-based auto-instrumentation of HTTP/HTTPS/GRPC Go services, as well as HTTP/HTTPS services written in other languages (intercepting Kernel-level socket operations as well as OpenSSL invocations).&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://drone.grafana.net/grafana/beyla&#34;&gt;&lt;img src=&#34;https://drone.grafana.net/api/badges/grafana/beyla/status.svg?ref=refs/heads/main&#34; alt=&#34;Build Status&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Getting Started&lt;/h2&gt; &#xA;&lt;p&gt;To try out Beyla, you need to run a network service for Beyla to instrument. Beyla supports a wide range of programming languages (Go, Java, .NET, NodeJS, Python, Ruby, Rust, etc.), so if you already have an example service you can use it. If you don&#39;t have an example, you can download and run &lt;code&gt;example-http-service.go&lt;/code&gt; from the &lt;code&gt;examples/&lt;/code&gt; directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;curl -OL https://raw.githubusercontent.com/grafana/beyla/main/examples/example-http-service/example-http-service.go&#xA;go run ./example-http-service.go&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next, generate some traffic. The following command will trigger a GET request to &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt; every two seconds.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;watch curl -s http://localhost:8080&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now that we have an example running, we are ready to download and run Beyla.&lt;/p&gt; &#xA;&lt;p&gt;First, download and unpack the latest release from the &lt;a href=&#34;https://github.com/grafana/beyla/releases&#34;&gt;Github releases page&lt;/a&gt;. The release should contain the &lt;code&gt;./beyla&lt;/code&gt; executable.&lt;/p&gt; &#xA;&lt;p&gt;Beyla supports multiple ways to find the service to be instrumented (by network port, executable name, process ID), and multiple exposition formats (Prometheus, OpenTelemetry metrics, Single Span traces).&lt;/p&gt; &#xA;&lt;p&gt;For getting started, we&#39;ll tell Beyla to instrument the service running on port 8080 (our example service) and expose metrics in Prometheus format on port 9400.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;export BEYLA_PROMETHEUS_PORT=9400&#xA;export OPEN_PORT=8080&#xA;sudo -E ./beyla&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Now, you should see metrics on &lt;a href=&#34;http://localhost:9400/metrics&#34;&gt;http://localhost:9400/metrics&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://grafana.com/docs/grafana-cloud/monitor-applications/beyla/&#34;&gt;Documentation&lt;/a&gt; and the &lt;a href=&#34;https://raw.githubusercontent.com/grafana/beyla/main/docs/sources/tutorial/index.md&#34;&gt;quickstart tutorial&lt;/a&gt; for more info.&lt;/p&gt; &#xA;&lt;h2&gt;Requirements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Linux with Kernel 4.18 or higher&lt;/li&gt; &#xA; &lt;li&gt;eBPF enabled in the host&lt;/li&gt; &#xA; &lt;li&gt;For instrumenting Go programs, they must have been compiled with Go 1.17 or higher&lt;/li&gt; &#xA; &lt;li&gt;Administrative access to execute the instrumenter &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;Or execute it from a user enabling the &lt;code&gt;SYS_ADMIN&lt;/code&gt; capability.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;If you want to instrument HTTP calls at kernel-level (for other languages than Go), your Kernel needs to enable BTF (&lt;a href=&#34;https://www.baeldung.com/linux/kernel-config&#34;&gt;compiled with &lt;code&gt;CONFIG_DEBUG_INFO_BTF&lt;/code&gt;&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Library&lt;/th&gt; &#xA;   &lt;th&gt;Working&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Kernel-level HTTP calls&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;OpenSSL library&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Standard &lt;code&gt;net/http&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/gorilla/mux&#34;&gt;Gorilla Mux&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://gin-gonic.com/&#34;&gt;Gin&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://github.com/grpc/grpc-go&#34;&gt;gRPC-Go&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Kubernetes&lt;/h2&gt; &#xA;&lt;p&gt;You can just trigger the Kubernetes descriptors in the &lt;code&gt;deployments/&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Provide your Grafana credentials. Use the following &lt;a href=&#34;https://raw.githubusercontent.com/grafana/beyla/main/deployments/01-grafana-credentials.template.yml&#34;&gt;K8s Secret template&lt;/a&gt; to introduce the endpoints, usernames and API keys for Mimir and Tempo:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ cp deployments/01-grafana-credentials.template.yml 01-grafana-credentials.yml&#xA;$ # EDIT the fields&#xA;$ vim 01-grafana-credentials.yml&#xA;$ kubectl apply -f 01-grafana-credentials.yml &#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy the Grafana Agent:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl apply -f deployments/02-grafana-agent.yml&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Deploy a demo app with the auto-instrumenter as a sidecar. You can use the blog example in the &lt;a href=&#34;https://raw.githubusercontent.com/grafana/beyla/main/deployments/03-instrumented-app.yml&#34;&gt;deployments/03-instrumented-app.yml&lt;/a&gt; file.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ kubectl apply -f ./deployments/03-instrumented-app.yml&#xA;$ kubectl port-forward service/goblog 8443:8443&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;You should be able to query traces and metrics in your Grafana board.&lt;/p&gt; &#xA;&lt;h2&gt;Development recipes&lt;/h2&gt; &#xA;&lt;h3&gt;How to regenerate the eBPF Kernel binaries&lt;/h3&gt; &#xA;&lt;p&gt;The eBPF program is embedded into the &lt;code&gt;pkg/internal/ebpf/bpf_*&lt;/code&gt; generated files. This step is generally not needed unless you change the C code in the &lt;code&gt;bpf&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;p&gt;If you have Docker installed, you just need to run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;make docker-generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you can&#39;t install docker, you should locally install the following required packages:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;dnf install -y kernel-devel make llvm clang glibc-devel.i686&#xA;make generate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Tested in Fedora 35, 38 and Red Hat Enterprise Linux 8.&lt;/p&gt; &#xA;&lt;h2&gt;Credits&lt;/h2&gt; &#xA;&lt;p&gt;Part of the code is taken from: &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-go-instrumentation&#34;&gt;https://github.com/open-telemetry/opentelemetry-go-instrumentation&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>williamyang1991/Rerender_A_Video</title>
    <updated>2023-09-21T01:22:47Z</updated>
    <id>tag:github.com,2023-09-21:/williamyang1991/Rerender_A_Video</id>
    <link href="https://github.com/williamyang1991/Rerender_A_Video" rel="alternate"></link>
    <summary type="html">&lt;p&gt;[SIGGRAPH Asia 2023] Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Rerender A Video - Official PyTorch Implementation&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/aa7dc164-dab7-43f4-a46b-758b34911f16&#34; alt=&#34;teaser&#34;&gt;&lt;/p&gt; &#xA;&lt;!--https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/82c35efb-e86b-4376-bfbe-6b69159b8879--&gt; &#xA;&lt;p&gt;&lt;strong&gt;Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation&lt;/strong&gt;&lt;br&gt; &lt;a href=&#34;https://williamyang1991.github.io/&#34;&gt;Shuai Yang&lt;/a&gt;, &lt;a href=&#34;https://zhouyifan.net/&#34;&gt;Yifan Zhou&lt;/a&gt;, &lt;a href=&#34;https://liuziwei7.github.io/&#34;&gt;Ziwei Liu&lt;/a&gt; and &lt;a href=&#34;https://www.mmlab-ntu.com/person/ccloy/&#34;&gt;Chen Change Loy&lt;/a&gt;&lt;br&gt; in SIGGRAPH Asia 2023 Conference Proceedings &lt;br&gt; &lt;a href=&#34;https://www.mmlab-ntu.com/project/rerender/&#34;&gt;&lt;strong&gt;Project Page&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://arxiv.org/abs/2306.07954&#34;&gt;&lt;strong&gt;Paper&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://youtu.be/cxfxdepKVaM&#34;&gt;&lt;strong&gt;Supplementary Video&lt;/strong&gt;&lt;/a&gt; | &lt;a href=&#34;https://drive.google.com/file/d/1HkxG5eiLM_TQbbMZYOwjDbd5gWisOy4m/view?usp=sharing&#34;&gt;&lt;strong&gt;Input Data and Video Results&lt;/strong&gt;&lt;/a&gt; &lt;br&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://huggingface.co/spaces/Anonymous-sub/Rerender&#34;&gt;&lt;img src=&#34;https://huggingface.co/datasets/huggingface/badges/raw/main/open-in-hf-spaces-sm-dark.svg?sanitize=true&#34; alt=&#34;Web Demo&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://visitor-badge.laobi.icu/badge?page_id=williamyang1991/Rerender_A_Video&#34; alt=&#34;visitors&#34;&gt;&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; &lt;em&gt;Large text-to-image diffusion models have exhibited impressive proficiency in generating high-quality images. However, when applying these models to video domain, ensuring temporal consistency across video frames remains a formidable challenge. This paper proposes a novel zero-shot text-guided video-to-video translation framework to adapt image models to videos. The framework includes two parts: key frame translation and full video translation. The first part uses an adapted diffusion model to generate key frames, with hierarchical cross-frame constraints applied to enforce coherence in shapes, textures and colors. The second part propagates the key frames to other frames with temporal-aware patch matching and frame blending. Our framework achieves global style and local texture temporal consistency at a low cost (without re-training or optimization). The adaptation is compatible with existing image diffusion techniques, allowing our framework to take advantage of them, such as customizing a specific subject with LoRA, and introducing extra spatial guidance with ControlNet. Extensive experimental results demonstrate the effectiveness of our proposed framework over existing methods in rendering high-quality and temporally-coherent videos.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;:&lt;br&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Temporal consistency&lt;/strong&gt;: cross-frame constraints for low-level temporal consistency.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Zero-shot&lt;/strong&gt;: no training or fine-tuning required.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;: compatible with off-the-shelf models (e.g., &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;, &lt;a href=&#34;https://civitai.com/&#34;&gt;LoRA&lt;/a&gt;) for customized translation.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/811fdea3-f0da-49c9-92b8-2d2ad360f0d6&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/811fdea3-f0da-49c9-92b8-2d2ad360f0d6&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Updates&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;[09/2023] Code is released.&lt;/li&gt; &#xA; &lt;li&gt;[09/2023] Accepted to SIGGRAPH Asia 2023 Conference Proceedings!&lt;/li&gt; &#xA; &lt;li&gt;[06/2023] Integrated to ü§ó &lt;a href=&#34;https://huggingface.co/spaces/Anonymous-sub/Rerender&#34;&gt;Hugging Face&lt;/a&gt;. Enjoy the web demo!&lt;/li&gt; &#xA; &lt;li&gt;[05/2023] This website is created.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TODO&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate into Diffusers.&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;del&gt;Add Inference instructions in README.md.&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;del&gt;Add Examples to webUI.&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;del&gt;Add optional poisson fusion to the pipeline.&lt;/del&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; &lt;del&gt;Add Installation instructions for Windows&lt;/del&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;&lt;em&gt;Please make sure your installation path only contain English letters or _&lt;/em&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository. (Don&#39;t forget --recursive. Otherwise, please run &lt;code&gt;git submodule update --init --recursive&lt;/code&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;git clone git@github.com:williamyang1991/Rerender_A_Video.git --recursive&#xA;cd Rerender_A_Video&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;If you have installed PyTorch CUDA, you can simply set up the environment with pip.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You can also create a new conda environment from scratch.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;conda env create -f environment.yml&#xA;conda activate rerender&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Run the installation script. The required models will be downloaded in &lt;code&gt;./models&lt;/code&gt;.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python install.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;You can run the demo with &lt;code&gt;rerender.py&lt;/code&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python rerender.py --cfg config/real2sculpture.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Installation on Windows&lt;/summary&gt; &#xA; &lt;p&gt;Before running the above 1-4 steps, you need prepare:&lt;/p&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://developer.nvidia.com/cuda-toolkit-archive&#34;&gt;CUDA&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://git-scm.com/download/win&#34;&gt;git&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;Install &lt;a href=&#34;https://visualstudio.microsoft.com/&#34;&gt;VS&lt;/a&gt; with Windows 10/11 SDK (for building deps/ebsynth/bin/ebsynth.exe)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Installation Fails?&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;In case building ebsynth fails, we provides our complied &lt;a href=&#34;https://drive.google.com/drive/folders/1oSB3imKwZGz69q2unBUfcgmQpzwccoyD?usp=sharing&#34;&gt;ebsynth&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;FileNotFoundError: [Errno 2] No such file or directory: &#39;xxxx.bin&#39; or &#39;xxxx.jpg&#39;&lt;/code&gt;: make sure your path only contains English letters or _ (&lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/issues/18#issuecomment-1723361433&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/issues/18#issuecomment-1723361433&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;KeyError: &#39;dataset&#39;&lt;/code&gt;: upgrade Gradio to the latest version (&lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/issues/14#issuecomment-1722778672&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/issues/14#issuecomment-1722778672&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;Error when processing videos: manually install ffmpeg (&lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/issues/19#issuecomment-1723685825&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/issues/19#issuecomment-1723685825&lt;/a&gt;, &lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/issues/29#issuecomment-1726091112&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/issues/29#issuecomment-1726091112&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;ERR_ADDRESS_INVALID&lt;/code&gt; Cannot open the webUI in browser: replace 0.0.0.0 with 127.0.0.1 in webUI.py (&lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/issues/19#issuecomment-1723685825&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/issues/19#issuecomment-1723685825&lt;/a&gt;)&lt;/li&gt; &#xA;  &lt;li&gt;&lt;code&gt;CUDA out of memory&lt;/code&gt;: (&lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/pull/23#issue-1900789461&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/pull/23#issue-1900789461&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;(1) Inference&lt;/h2&gt; &#xA;&lt;h3&gt;WebUI (recommended)&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code&gt;python webUI.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The Gradio app also allows you to flexibly change the inference options. Just try it for more details. (For WebUI, you need to download &lt;a href=&#34;https://civitai.com/models/7371/rev-animated?modelVersionId=19575&#34;&gt;revAnimated_v11&lt;/a&gt; and &lt;a href=&#34;https://civitai.com/models/4201?modelVersionId=29460&#34;&gt;realisticVisionV20_v20&lt;/a&gt; to &lt;code&gt;./models/&lt;/code&gt; after Installation)&lt;/p&gt; &#xA;&lt;p&gt;Upload your video, input the prompt, select the seed, and hit:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run 1st Key Frame&lt;/strong&gt;: only translate the first frame, so you can adjust the prompts/models/parameters to find your ideal output appearance before running the whole video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run Key Frames&lt;/strong&gt;: translate all the key frames based on the settings of the first frame, so you can adjust the temporal-related parameters for better temporal consistency before running the whole video.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run Propagation&lt;/strong&gt;: propagate the key frames to other frames for full video translation&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Run All&lt;/strong&gt;: &lt;strong&gt;Run 1st Key Frame&lt;/strong&gt;, &lt;strong&gt;Run Key Frames&lt;/strong&gt; and &lt;strong&gt;Run Propagation&lt;/strong&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/eb4e1ddc-11a3-42dd-baa4-622eecef04c7&#34; alt=&#34;UI&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;We provide abundant advanced options to play with&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;Using customized models&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ul&gt; &#xA;  &lt;li&gt;Using LoRA/Dreambooth/Finetuned/Mixed SD models &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Modify &lt;code&gt;sd_model_cfg.py&lt;/code&gt; to add paths to the saved SD models&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Using other controls from ControlNet (e.g., Depth, Pose) &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Add more options like &lt;code&gt;control_type = gr.Dropdown([&#39;HED&#39;, &#39;canny&#39;, &#39;depth&#39;]&lt;/code&gt; here &lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/raw/b6cafb5d80a79a3ef831c689ffad92ec095f2794/webUI.py#L690&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/blob/b6cafb5d80a79a3ef831c689ffad92ec095f2794/webUI.py#L690&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Add model loading options like &lt;code&gt;elif control_type == &#39;depth&#39;:&lt;/code&gt; following &lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/raw/b6cafb5d80a79a3ef831c689ffad92ec095f2794/webUI.py#L88&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/blob/b6cafb5d80a79a3ef831c689ffad92ec095f2794/webUI.py#L88&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;Add model detectors like &lt;code&gt;elif control_type == &#39;depth&#39;:&lt;/code&gt; following &lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/raw/b6cafb5d80a79a3ef831c689ffad92ec095f2794/webUI.py#L122&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/blob/b6cafb5d80a79a3ef831c689ffad92ec095f2794/webUI.py#L122&lt;/a&gt;&lt;/li&gt; &#xA;    &lt;li&gt;One example is given &lt;a href=&#34;https://huggingface.co/spaces/Anonymous-sub/Rerender/discussions/10/files&#34;&gt;here&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ul&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;Advanced options for the 1st frame translation&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Resolution related (&lt;strong&gt;Frame resolution&lt;/strong&gt;, &lt;strong&gt;left/top/right/bottom crop length&lt;/strong&gt;): crop the frame and resize its short side to 512.&lt;/li&gt; &#xA;  &lt;li&gt;ControlNet related: &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;ControlNet strength&lt;/strong&gt;: how well the output matches the input control edges&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Control type&lt;/strong&gt;: HED edge or Canny edge&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Canny low/high threshold&lt;/strong&gt;: low values for more edge details&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;SDEdit related: &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Denoising strength&lt;/strong&gt;: repaint degree (low value to make the output look more like the original video)&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Preserve color&lt;/strong&gt;: preserve the color of the original video&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;SD related: &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Steps&lt;/strong&gt;: denoising step&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;CFG scale&lt;/strong&gt;: how well the output matches the prompt&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Base model&lt;/strong&gt;: base Stable Diffusion model (SD 1.5) &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;Stable Diffusion 1.5: official model&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://civitai.com/models/7371/rev-animated?modelVersionId=19575&#34;&gt;revAnimated_v11&lt;/a&gt;: a semi-realistic (2.5D) model&lt;/li&gt; &#xA;      &lt;li&gt;&lt;a href=&#34;https://civitai.com/models/4201?modelVersionId=29460&#34;&gt;realisticVisionV20_v20&lt;/a&gt;: a photo-realistic model&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Added prompt/Negative prompt&lt;/strong&gt;: supplementary prompts&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;Advanced options for the key frame translation&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;Key frame related &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Key frame frequency (K)&lt;/strong&gt;: Uniformly sample the key frame every K frames. Small value for large or fast motions.&lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Number of key frames (M)&lt;/strong&gt;: The final output video will have K*M+1 frames with M+1 key frames.&lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA;  &lt;li&gt;Temporal consistency related &#xA;   &lt;ul&gt; &#xA;    &lt;li&gt;Cross-frame attention: &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;strong&gt;Cross-frame attention start/end&lt;/strong&gt;: When applying cross-frame attention for global style consistency&lt;/li&gt; &#xA;      &lt;li&gt;&lt;strong&gt;Cross-frame attention update frequency (N)&lt;/strong&gt;: Update the reference style frame every N key frames. Should be large for long videos to avoid error accumulation.&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Shape-aware fusion&lt;/strong&gt; Check to use this feature &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;strong&gt;Shape-aware fusion start/end&lt;/strong&gt;: When applying shape-aware fusion for local shape consistency&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Pixel-aware fusion&lt;/strong&gt; Check to use this feature &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;strong&gt;Pixel-aware fusion start/end&lt;/strong&gt;: When applying pixel-aware fusion for pixel-level temporal consistency&lt;/li&gt; &#xA;      &lt;li&gt;&lt;strong&gt;Pixel-aware fusion strength&lt;/strong&gt;: The strength to preserve the non-inpainting region. Small to avoid error accumulation. Large to avoid burry textures.&lt;/li&gt; &#xA;      &lt;li&gt;&lt;strong&gt;Pixel-aware fusion detail level&lt;/strong&gt;: The strength to sharpen the inpainting region. Small to avoid error accumulation. Large to avoid burry textures.&lt;/li&gt; &#xA;      &lt;li&gt;&lt;strong&gt;Smooth fusion boundary&lt;/strong&gt;: Check to smooth the inpainting boundary (avoid error accumulation).&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;    &lt;li&gt;&lt;strong&gt;Color-aware AdaIN&lt;/strong&gt; Check to use this feature &#xA;     &lt;ul&gt; &#xA;      &lt;li&gt;&lt;strong&gt;Color-aware AdaIN start/end&lt;/strong&gt;: When applying AdaIN to make the video color consistent with the first frame&lt;/li&gt; &#xA;     &lt;/ul&gt; &lt;/li&gt; &#xA;   &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt; &lt;b&gt;Advanced options for the full video translation&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;ol&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Gradient blending&lt;/strong&gt;: apply Poisson Blending to reduce ghosting artifacts. May slow the process and increase flickers.&lt;/li&gt; &#xA;  &lt;li&gt;&lt;strong&gt;Number of parallel processes&lt;/strong&gt;: multiprocessing to speed up the process. Large value (8) is recommended.&lt;/li&gt; &#xA; &lt;/ol&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/ffebac15-e7e0-4cd4-a8fe-60f243450172&#34; alt=&#34;options&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Command Line&lt;/h3&gt; &#xA;&lt;p&gt;We also provide a flexible script &lt;code&gt;rerender.py&lt;/code&gt; to run our method.&lt;/p&gt; &#xA;&lt;h4&gt;Simple mode&lt;/h4&gt; &#xA;&lt;p&gt;Set the options via command line. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python rerender.py --input videos/pexels-antoni-shkraba-8048492-540x960-25fps.mp4 --output result/man/man.mp4 --prompt &#34;a handsome man in van gogh painting&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script will run the full pipeline. A work directory will be created at &lt;code&gt;result/man&lt;/code&gt; and the result video will be saved as &lt;code&gt;result/man/man.mp4&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Advanced mode&lt;/h4&gt; &#xA;&lt;p&gt;Set the options via a config file. For example,&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python rerender.py --cfg config/van_gogh_man.json&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The script will run the full pipeline. We provide some examples of the config in &lt;code&gt;config&lt;/code&gt; directory. Most options in the config is the same as those in WebUI. Please check the explanations in the WebUI section.&lt;/p&gt; &#xA;&lt;p&gt;Specifying customized models by setting &lt;code&gt;sd_model&lt;/code&gt; in config. For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;{&#xA;  &#34;sd_model&#34;: &#34;models/realisticVisionV20_v20.safetensors&#34;,&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Customize the pipeline&lt;/h4&gt; &#xA;&lt;p&gt;Similar to WebUI, we provide three-step workflow: Rerender the first key frame, then rerender the full key frames, finally rerender the full video with propagation. To run only a single step, specify options &lt;code&gt;-one&lt;/code&gt;, &lt;code&gt;-nb&lt;/code&gt; and &lt;code&gt;-nr&lt;/code&gt;:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Rerender the first key frame&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python rerender.py --cfg config/van_gogh_man.json -one -nb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Rerender the full key frames&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python rerender.py --cfg config/van_gogh_man.json -nb&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Rerender the full video with propagation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python rerender.py --cfg config/van_gogh_man.json -nr&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h4&gt;Our Ebsynth implementation&lt;/h4&gt; &#xA;&lt;p&gt;We provide a separate Ebsynth python script &lt;code&gt;video_blend.py&lt;/code&gt; with the temporal blending algorithm introduced in &lt;a href=&#34;https://dcgi.fel.cvut.cz/home/sykorad/ebsynth.html&#34;&gt;Stylizing Video by Example&lt;/a&gt; for interpolating style between key frames. It can work on your own stylized key frames independently of our Rerender algorithm.&lt;/p&gt; &#xA;&lt;p&gt;Usage:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;video_blend.py [-h] [--output OUTPUT] [--fps FPS] [--beg BEG] [--end END] [--itv ITV] [--key KEY]&#xA;                      [--n_proc N_PROC] [-ps] [-ne] [-tmp]&#xA;                      name&#xA;&#xA;positional arguments:&#xA;  name             Path to input video&#xA;&#xA;optional arguments:&#xA;  -h, --help       show this help message and exit&#xA;  --output OUTPUT  Path to output video&#xA;  --fps FPS        The FPS of output video&#xA;  --beg BEG        The index of the first frame to be stylized&#xA;  --end END        The index of the last frame to be stylized&#xA;  --itv ITV        The interval of key frame&#xA;  --key KEY        The subfolder name of stylized key frames&#xA;  --n_proc N_PROC  The max process count&#xA;  -ps              Use poisson gradient blending&#xA;  -ne              Do not run ebsynth (use previous ebsynth output)&#xA;  -tmp             Keep temporary output&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example, to run Ebsynth on video &lt;code&gt;man.mp4&lt;/code&gt;,&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Put the stylized key frames to &lt;code&gt;videos/man/keys&lt;/code&gt; for every 10 frames (named as &lt;code&gt;0001.png&lt;/code&gt;, &lt;code&gt;0011.png&lt;/code&gt;, ...)&lt;/li&gt; &#xA; &lt;li&gt;Put the original video frames in &lt;code&gt;videos/man/video&lt;/code&gt; (named as &lt;code&gt;0001.png&lt;/code&gt;, &lt;code&gt;0002.png&lt;/code&gt;, ...).&lt;/li&gt; &#xA; &lt;li&gt;Run Ebsynth on the first 101 frames of the video with poisson gradient blending and save the result to &lt;code&gt;videos/man/blend.mp4&lt;/code&gt; under FPS 25 with the following command:&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python video_blend.py videos/man \&#xA;  --beg 1 \&#xA;  --end 101 \&#xA;  --itv 10 \&#xA;  --key keys \&#xA;  --output videos/man/blend.mp4 \&#xA;  --fps 25.0 \&#xA;  -ps&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;(2) Results&lt;/h2&gt; &#xA;&lt;h3&gt;Key frame translation&lt;/h3&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/18666871-f273-44b2-ae67-7be85d43e2f6&#34; raw=&#34;true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/61f59540-f06e-4e5a-86b6-1d7cb8ed6300&#34; raw=&#34;true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/8e8ad51a-6a71-4b34-8633-382192d0f17c&#34; raw=&#34;true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/b03cd35f-5d90-471a-9aa9-5c7773d7ac39&#34; raw=&#34;true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;27.5%&#34; align=&#34;center&#34;&gt;white ancient Greek sculpture, Venus de Milo, light pink and blue background&lt;/td&gt; &#xA;   &lt;td width=&#34;27.5%&#34; align=&#34;center&#34;&gt;a handsome Greek man&lt;/td&gt; &#xA;   &lt;td width=&#34;21.5%&#34; align=&#34;center&#34;&gt;a traditional mountain in chinese ink wash painting&lt;/td&gt; &#xA;   &lt;td width=&#34;23.5%&#34; align=&#34;center&#34;&gt;a cartoon tiger&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;table class=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/649a789e-0c41-41cf-94a4-0d524dcfb282&#34; raw=&#34;true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/73590c16-916f-4ee6-881a-44a201dd85dd&#34; raw=&#34;true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/fbdc0b8e-6046-414f-a37e-3cd9dd0adf5d&#34; raw=&#34;true&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/eb11d807-2afa-4609-a074-34300b67e6aa&#34; raw=&#34;true&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td width=&#34;26.0%&#34; align=&#34;center&#34;&gt;a swan in chinese ink wash painting, monochrome&lt;/td&gt; &#xA;   &lt;td width=&#34;29.0%&#34; align=&#34;center&#34;&gt;a beautiful woman in CG style&lt;/td&gt; &#xA;   &lt;td width=&#34;21.5%&#34; align=&#34;center&#34;&gt;a clean simple white jade sculpture&lt;/td&gt; &#xA;   &lt;td width=&#34;24.0%&#34; align=&#34;center&#34;&gt;a fluorescent jellyfish in the deep dark blue sea&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Full video translation&lt;/h3&gt; &#xA;&lt;p&gt;Text-guided virtual character generation.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/1405b257-e59a-427f-890d-7652e6bed0a4&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/1405b257-e59a-427f-890d-7652e6bed0a4&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/efee8cc6-9708-4124-bf6a-49baf91349fc&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/efee8cc6-9708-4124-bf6a-49baf91349fc&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Video stylization and video editing.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/1b72585c-99c0-401d-b240-5b8016df7a3f&#34;&gt;https://github.com/williamyang1991/Rerender_A_Video/assets/18130694/1b72585c-99c0-401d-b240-5b8016df7a3f&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this work useful for your research, please consider citing our paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{yang2023rerender,&#xA;‚ÄÉtitle = {Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation},&#xA;‚ÄÉauthor = {Yang, Shuai and Zhou, Yifan and Liu, Ziwei and and Loy, Chen Change},&#xA; booktitle = {ACM SIGGRAPH Asia Conference Proceedings},&#xA;‚ÄÉyear = {2023},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgments&lt;/h2&gt; &#xA;&lt;p&gt;The code is mainly developed based on &lt;a href=&#34;https://github.com/lllyasviel/ControlNet&#34;&gt;ControlNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/Stability-AI/stablediffusion&#34;&gt;Stable Diffusion&lt;/a&gt;, &lt;a href=&#34;https://github.com/haofeixu/gmflow&#34;&gt;GMFlow&lt;/a&gt; and &lt;a href=&#34;https://github.com/jamriska/ebsynth&#34;&gt;Ebsynth&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>hyperdxio/hyperdx</title>
    <updated>2023-09-21T01:22:47Z</updated>
    <id>tag:github.com,2023-09-21:/hyperdxio/hyperdx</id>
    <link href="https://github.com/hyperdxio/hyperdx" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Resolve production issues, fast. An open source observability platform unifying session replays, logs, metrics, traces and errors.&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://hyperdx.io&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;./.github/images/logo_dark.png#gh-dark-mode-only&#34;&gt; &#xA;   &lt;img alt=&#34;hyperdx logo&#34; src=&#34;https://raw.githubusercontent.com/hyperdxio/hyperdx/main/.github/images/logo_light.png#gh-light-mode-only&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h1&gt;HyperDX&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hyperdx.io&#34;&gt;HyperDX&lt;/a&gt; helps engineers figure out why production is broken faster by centralizing and correlating logs, metrics, traces, exceptions and session replays in one place. An open source and developer-friendly alternative to Datadog and New Relic.&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.hyperdx.io/docs&#34;&gt;Documentation&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://discord.gg/FErRRKU78j&#34;&gt;Chat on Discord&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://api.hyperdx.io/login/demo&#34;&gt;Live Demo&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://github.com/hyperdxio/hyperdx/issues/new&#34;&gt;Bug Reports&lt;/a&gt; ‚Ä¢ &lt;a href=&#34;https://raw.githubusercontent.com/hyperdxio/hyperdx/main/CONTRIBUTING.md&#34;&gt;Contributing&lt;/a&gt; &lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;üïµÔ∏è Correlate end to end, go from browser session replay to logs and traces in just a few clicks&lt;/li&gt; &#xA; &lt;li&gt;üî• Blazing fast performance powered by Clickhouse&lt;/li&gt; &#xA; &lt;li&gt;üîç Intuitive full-text search and property search syntax (ex. &lt;code&gt;level:err&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;ü§ñ Automatically cluster event patterns from billions of events&lt;/li&gt; &#xA; &lt;li&gt;üìà Dashboard high cardinality events without a complex query language&lt;/li&gt; &#xA; &lt;li&gt;üîî Set up alerts in just a few clicks&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;{&lt;/code&gt; Automatic JSON/structured log parsing&lt;/li&gt; &#xA; &lt;li&gt;üî≠ OpenTelemetry native&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;img alt=&#34;Search logs and traces all in one place&#34; src=&#34;https://raw.githubusercontent.com/hyperdxio/hyperdx/main/.github/images/search_splash.png&#34; title=&#34;Search logs and traces all in one place&#34;&gt; &#xA;&lt;h3&gt;Additional Screenshots&lt;/h3&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;üìà Dashboards&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;img alt=&#34;Dashboard&#34; src=&#34;https://raw.githubusercontent.com/hyperdxio/hyperdx/main/.github/images/dashboard.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;ü§ñ Automatic Event Pattern Clustering&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;img alt=&#34;Event Pattern Clustering&#34; src=&#34;https://raw.githubusercontent.com/hyperdxio/hyperdx/main/.github/images/pattern3.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;&lt;b&gt;üñ•Ô∏è Session Replay &amp;amp; RUM&lt;/b&gt;&lt;/summary&gt; &#xA; &lt;img alt=&#34;Event Pattern Clustering&#34; src=&#34;https://raw.githubusercontent.com/hyperdxio/hyperdx/main/.github/images/session.png&#34;&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Spinning Up HyperDX&lt;/h2&gt; &#xA;&lt;p&gt;The HyperDX stack ingests, stores, and searches/graphs your telemetry data. After standing up the Docker Compose stack, you&#39;ll want to instrument your app to send data over to HyperDX.&lt;/p&gt; &#xA;&lt;p&gt;You can get started by deploying a complete stack via Docker Compose. After cloning this repository, simply start the stack with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Afterwards, you can visit &lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt; to access the HyperDX UI.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If your server is behind a firewall, you&#39;ll need to open/forward port 8080, 8000 and 4318 on your firewall for the UI, API and OTel collector respectively.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;We recommend at least 4GB of RAM and 2 cores for testing.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;strong&gt;Enabling Self-instrumentation/Demo Logs&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To get a quick preview of HyperDX, you can enable self-instrumentation and demo logs by setting the &lt;code&gt;HYPERDX_API_KEY&lt;/code&gt; to your ingestion key (go to &lt;a href=&#34;http://localhost:8080/team&#34;&gt;http://localhost:8080/team&lt;/a&gt; after creating your account) and then restart the stack.&lt;/p&gt; &#xA;&lt;p&gt;This will redirect internal telemetry from the frontend app, API, host metrics and demo logs to your new HyperDX instance.&lt;/p&gt; &#xA;&lt;p&gt;ex.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;HYPERDX_API_KEY=&amp;lt;YOUR_INGESTION_KEY&amp;gt; docker compose up -d&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;If you need to use &lt;code&gt;sudo&lt;/code&gt; for docker, make sure to forward the environment variable with the &lt;code&gt;-E&lt;/code&gt; flag: &lt;code&gt;HYPERDX_API_KEY=&amp;lt;YOUR_KEY&amp;gt; sudo -E docker compose up -d&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h3&gt;Hosted Cloud&lt;/h3&gt; &#xA;&lt;p&gt;HyperDX is also available as a hosted cloud service at &lt;a href=&#34;https://hyperdx.io&#34;&gt;hyperdx.io&lt;/a&gt;. You can sign up for a free account and start sending data in minutes.&lt;/p&gt; &#xA;&lt;h2&gt;Instrumenting Your App&lt;/h2&gt; &#xA;&lt;p&gt;To get logs, metrics, traces, session replay, etc into HyperDX, you&#39;ll need to instrument your app to collect and send telemetry data over to your HyperDX instance.&lt;/p&gt; &#xA;&lt;p&gt;We provide a set of SDKs and integration options to make it easier to get started with HyperDX, such as &lt;a href=&#34;https://www.hyperdx.io/docs/install/browser&#34;&gt;Browser&lt;/a&gt;, &lt;a href=&#34;https://www.hyperdx.io/docs/install/javascript&#34;&gt;Node.js&lt;/a&gt;, and &lt;a href=&#34;https://www.hyperdx.io/docs/install/python&#34;&gt;Python&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;You can find the full list in &lt;a href=&#34;https://www.hyperdx.io/docs&#34;&gt;our docs&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;OpenTelemetry&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Additionally, HyperDX is compatible with &lt;a href=&#34;https://opentelemetry.io/&#34;&gt;OpenTelemetry&lt;/a&gt;, a vendor-neutral standard for instrumenting your application backed by CNCF. Supported languages/platforms include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Kubernetes&lt;/li&gt; &#xA; &lt;li&gt;Javascript&lt;/li&gt; &#xA; &lt;li&gt;Python&lt;/li&gt; &#xA; &lt;li&gt;Java&lt;/li&gt; &#xA; &lt;li&gt;Go&lt;/li&gt; &#xA; &lt;li&gt;Ruby&lt;/li&gt; &#xA; &lt;li&gt;PHP&lt;/li&gt; &#xA; &lt;li&gt;.NET&lt;/li&gt; &#xA; &lt;li&gt;Elixir&lt;/li&gt; &#xA; &lt;li&gt;Rust&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;(Full list &lt;a href=&#34;https://opentelemetry.io/docs/instrumentation/&#34;&gt;here&lt;/a&gt;)&lt;/p&gt; &#xA;&lt;p&gt;Once HyperDX is running, you can point your OpenTelemetry SDK to the OpenTelemetry collector spun up at &lt;code&gt;http://localhost:4318&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;We welcome all contributions! There&#39;s many ways to contribute to the project, including but not limited to:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Opening a PR (&lt;a href=&#34;https://raw.githubusercontent.com/hyperdxio/hyperdx/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hyperdxio/hyperdx/issues/new&#34;&gt;Submitting feature requests or bugs&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Improving our product or contribution documentation&lt;/li&gt; &#xA; &lt;li&gt;Voting on &lt;a href=&#34;https://github.com/hyperdxio/hyperdx/issues&#34;&gt;open issues&lt;/a&gt; or contributing use cases to a feature request&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;Our mission is to help engineers ship reliable software. To enable that, we believe every engineer needs to be able to easily leverage production telemetry to quickly solve burning production issues.&lt;/p&gt; &#xA;&lt;p&gt;However, in our experience, the existing tools we&#39;ve used tend to fall short in a few ways:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;They&#39;re expensive, and the pricing has failed to scale with TBs of telemetry becoming the norm, leading to teams aggressively cutting the amount of data they can collect.&lt;/li&gt; &#xA; &lt;li&gt;They&#39;re hard to use, requiring full-time SREs to set up, and domain experts to use confidently.&lt;/li&gt; &#xA; &lt;li&gt;They requiring hopping from tool to tool (logs, session replay, APM, exceptions, etc.) to stitch together the clues yourself.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;We&#39;re still early on in our journey, but are building in the open to solve these key issues in observability. We hope you give HyperDX a try and let us know how we&#39;re doing!&lt;/p&gt; &#xA;&lt;h2&gt;Open Source vs Hosted Cloud&lt;/h2&gt; &#xA;&lt;p&gt;HyperDX is open core, with most of our features available here under an MIT license. We have a cloud-hosted version available at &lt;a href=&#34;https://hyperdx.io&#34;&gt;hyperdx.io&lt;/a&gt; with a few &lt;a href=&#34;https://www.hyperdx.io/docs/oss-vs-cloud&#34;&gt;additional features&lt;/a&gt; beyond what&#39;s offered in the open source version.&lt;/p&gt; &#xA;&lt;p&gt;Our cloud hosted version exists so that we can build a sustainable business and continue building HyperDX as an open source platform. We hope to have more comprehensive documentation on how we balance between cloud-only and open source features in the future. In the meantime, we&#39;re highly aligned with Gitlab&#39;s &lt;a href=&#34;https://handbook.gitlab.com/handbook/company/stewardship/&#34;&gt;stewardship model&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/hyperdxio/hyperdx/issues/new&#34;&gt;Open an Issue&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://discord.gg/FErRRKU78j&#34;&gt;Discord&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;mailto:support@hyperdx.io&#34;&gt;Email&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/hyperdxio/hyperdx/main/LICENSE&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
</feed>