<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2025-05-25T01:29:26Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>Fosowl/agenticSeek</title>
    <updated>2025-05-25T01:29:26Z</updated>
    <id>tag:github.com,2025-05-25:/Fosowl/agenticSeek</id>
    <link href="https://github.com/Fosowl/agenticSeek" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;AgenticSeek: Private, Local Manus Alternative.&lt;/h1&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/agentic_seek_logo.png&#34; width=&#34;300&#34; height=&#34;300&#34; alt=&#34;Agentic Seek Logo&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p&gt;English | &lt;a href=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHS.md&#34;&gt;‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_CHT.md&#34;&gt;ÁπÅÈ´î‰∏≠Êñá&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_FR.md&#34;&gt;Fran√ßais&lt;/a&gt; | &lt;a href=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/README_JP.md&#34;&gt;Êó•Êú¨Ë™û&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;A &lt;strong&gt;100% local alternative to Manus AI&lt;/strong&gt;, this voice-enabled AI assistant autonomously browses the web, writes code, and plans tasks while keeping all data on your device. Tailored for local reasoning models, it runs entirely on your hardware, ensuring complete privacy and zero cloud dependency.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://fosowl.github.io/agenticSeek.html&#34;&gt;&lt;img src=&#34;https://img.shields.io/static/v1?label=Website&amp;amp;message=AgenticSeek&amp;amp;color=blue&amp;amp;style=flat-square&#34; alt=&#34;Visit AgenticSeek&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/badge/license-GPL--3.0-green&#34; alt=&#34;License&#34;&gt; &lt;a href=&#34;https://discord.gg/8hGDaME3TC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Discord-Join%20Us-7289DA?logo=discord&amp;amp;logoColor=white&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://x.com/Martin993886460&#34;&gt;&lt;img src=&#34;https://img.shields.io/twitter/url/https/twitter.com/fosowl.svg?style=social&amp;amp;label=Update%20%40Fosowl&#34; alt=&#34;Twitter&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/Fosowl/agenticSeek/stargazers&#34;&gt;&lt;img src=&#34;https://img.shields.io/github/stars/Fosowl/agenticSeek?style=social&#34; alt=&#34;GitHub stars&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Why AgenticSeek ?&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;üîí Fully Local &amp;amp; Private - Everything runs on your machine ‚Äî no cloud, no data sharing. Your files, conversations, and searches stay private.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üåê Smart Web Browsing - AgenticSeek can browse the internet by itself ‚Äî search, read, extract info, fill web form ‚Äî all hands-free.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üíª Autonomous Coding Assistant - Need code? It can write, debug, and run programs in Python, C, Go, Java, and more ‚Äî all without supervision.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üß† Smart Agent Selection - You ask, it figures out the best agent for the job automatically. Like having a team of experts ready to help.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üìã Plans &amp;amp; Executes Complex Tasks - From trip planning to complex projects ‚Äî it can split big tasks into steps and get things done using multiple AI agents.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;üéôÔ∏è Voice-Enabled - Clean, fast, futuristic voice and speech to text allowing you to talk to it like it&#39;s your personal AI from a sci-fi movie&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;&lt;strong&gt;Demo&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Can you search for the agenticSeek project, learn what skills are required, then open the CV_candidates.zip and then tell me which match best the project&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316&#34;&gt;https://github.com/user-attachments/assets/b8ca60e9-7b3b-4533-840e-08f9ac426316&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Disclaimer: This demo, including all the files that appear (e.g: CV_candidates.zip), are entirely fictional. We are not a corporation, we seek open-source contributors not candidates.&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;üõ†Ô∏è &lt;strong&gt;Work in Progress&lt;/strong&gt; ‚Äì Looking for contributors!&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Make sure you have chrome driver, docker and python3.10 installed.&lt;/p&gt; &#xA;&lt;p&gt;We highly advice you use exactly python3.10 for the setup. Dependencies error might happen otherwise.&lt;/p&gt; &#xA;&lt;p&gt;For issues related to chrome driver, see the &lt;strong&gt;Chromedriver&lt;/strong&gt; section.&lt;/p&gt; &#xA;&lt;h3&gt;1Ô∏è‚É£ &lt;strong&gt;Clone the repository and setup&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/Fosowl/agenticSeek.git&#xA;cd agenticSeek&#xA;mv .env.example .env&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;2Ô∏è &lt;strong&gt;Create a virtual env&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 -m venv agentic_seek_env&#xA;source agentic_seek_env/bin/activate&#xA;# On Windows: agentic_seek_env\Scripts\activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;3Ô∏è‚É£ &lt;strong&gt;Install package&lt;/strong&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Ensure Python, Docker and docker compose, and Google chrome are installed.&lt;/p&gt; &#xA;&lt;p&gt;We recommand Python 3.10.0.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Automatic Installation (Recommanded):&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;For Linux/Macos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For windows:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./install.bat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Manually:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Note: For any OS, ensure the ChromeDriver you install matches your installed Chrome version. Run &lt;code&gt;google-chrome --version&lt;/code&gt;. See known issues if you have chrome &amp;gt;135&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Linux&lt;/em&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Update Package List: &lt;code&gt;sudo apt update&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install Dependencies: &lt;code&gt;sudo apt install -y alsa-utils portaudio19-dev python3-pyaudio libgtk-3-dev libnotify-dev libgconf-2-4 libnss3 libxss1&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install ChromeDriver matching your Chrome browser version: &lt;code&gt;sudo apt install -y chromium-chromedriver&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install requirements: &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Macos&lt;/em&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Update brew : &lt;code&gt;brew update&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install chromedriver : &lt;code&gt;brew install --cask chromedriver&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install portaudio: &lt;code&gt;brew install portaudio&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Upgrade pip : &lt;code&gt;python3 -m pip install --upgrade pip&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Upgrade wheel : : &lt;code&gt;pip3 install --upgrade setuptools wheel&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install requirements: &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;em&gt;Windows&lt;/em&gt;:&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Install pyreadline3 &lt;code&gt;pip install pyreadline3&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Install portaudio manually (e.g., via vcpkg or prebuilt binaries) and then run: &lt;code&gt;pip install pyaudio&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Download and install chromedriver manually from: &lt;a href=&#34;https://sites.google.com/chromium.org/driver/getting-started&#34;&gt;https://sites.google.com/chromium.org/driver/getting-started&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Place chromedriver in a directory included in your PATH.&lt;/p&gt; &#xA;&lt;p&gt;Install requirements: &lt;code&gt;pip3 install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Setup for running LLM locally on your machine&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Hardware Requirements:&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;To run LLMs locally, you&#39;ll need sufficient hardware. At a minimum, a GPU capable of running Qwen/Deepseek 14B is required. See the FAQ for detailed model/performance recommendations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Setup your local provider&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Start your local provider, for example with ollama:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ollama serve&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See below for a list of local supported provider.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Update the config.ini&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Change the config.ini file to set the provider_name to a supported provider and provider_model to a LLM supported by your provider. We recommand reasoning model such as &lt;em&gt;Qwen&lt;/em&gt; or &lt;em&gt;Deepseek&lt;/em&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the &lt;strong&gt;FAQ&lt;/strong&gt; at the end of the README for required hardware.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[MAIN]&#xA;is_local = True # Whenever you are running locally or with remote provider.&#xA;provider_name = ollama # or lm-studio, openai, etc..&#xA;provider_model = deepseek-r1:14b # choose a model that fit your hardware&#xA;provider_server_address = 127.0.0.1:11434&#xA;agent_name = Jarvis # name of your AI&#xA;recover_last_session = True # whenever to recover the previous session&#xA;save_session = True # whenever to remember the current session&#xA;speak = True # text to speech&#xA;listen = False # Speech to text, only for CLI&#xA;work_dir =  /Users/mlg/Documents/workspace # The workspace for AgenticSeek.&#xA;jarvis_personality = False # Whenever to use a more &#34;Jarvis&#34; like personality (experimental)&#xA;languages = en zh # The list of languages, Text to speech will default to the first language on the list&#xA;[BROWSER]&#xA;headless_browser = True # Whenever to use headless browser, recommanded only if you use web interface.&#xA;stealth_mode = True # Use undetected selenium to reduce browser detection&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Warning: Do &lt;em&gt;NOT&lt;/em&gt; set provider_name to &lt;code&gt;openai&lt;/code&gt; if using LM-studio for running LLMs. Set it to &lt;code&gt;lm-studio&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Note: Some provider (eg: lm-studio) require you to have &lt;code&gt;http://&lt;/code&gt; in front of the IP. For example &lt;code&gt;http://127.0.0.1:1234&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;List of local providers&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Provider&lt;/th&gt; &#xA;   &lt;th&gt;Local?&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ollama&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Run LLMs locally with ease using ollama as a LLM provider&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lm-studio&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Run LLM locally with LM studio (set &lt;code&gt;provider_name&lt;/code&gt; to &lt;code&gt;lm-studio&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Use openai compatible API (eg: llama.cpp server)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;Next step: &lt;a href=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#Start-services-and-Run&#34;&gt;Start services and run AgenticSeek&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;See the &lt;strong&gt;Known issues&lt;/strong&gt; section if you are having issues&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;See the &lt;strong&gt;Run with an API&lt;/strong&gt; section if your hardware can&#39;t run deepseek locally&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;See the &lt;strong&gt;Config&lt;/strong&gt; section for detailled config file explanation.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Setup to run with an API&lt;/h2&gt; &#xA;&lt;p&gt;Set the desired provider in the &lt;code&gt;config.ini&lt;/code&gt;. See below for a list of API providers.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[MAIN]&#xA;is_local = False&#xA;provider_name = google&#xA;provider_model = gemini-2.0-flash&#xA;provider_server_address = 127.0.0.1:5000 # doesn&#39;t matter&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Warning: Make sure there is not trailing space in the config.&lt;/p&gt; &#xA;&lt;p&gt;Export your API key: &lt;code&gt;export &amp;lt;&amp;lt;PROVIDER&amp;gt;&amp;gt;_API_KEY=&#34;xxx&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Example: export &lt;code&gt;TOGETHER_API_KEY=&#34;xxxxx&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;List of API providers&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Provider&lt;/th&gt; &#xA;   &lt;th&gt;Local?&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai&lt;/td&gt; &#xA;   &lt;td&gt;Depends&lt;/td&gt; &#xA;   &lt;td&gt;Use ChatGPT API&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;deepseek&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Deepseek API (non-private)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;huggingface&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Hugging-Face API (non-private)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;togetherAI&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Use together AI API (non-private)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;google&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Use google gemini API (non-private)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;em&gt;We advice against using gpt-4o or other closedAI models&lt;/em&gt;, performance are poor for web browsing and task planning.&lt;/p&gt; &#xA;&lt;p&gt;Please also note that coding/bash might fail with gemini, it seem to ignore our prompt for format to respect, which are optimized for deepseek r1.&lt;/p&gt; &#xA;&lt;p&gt;Next step: &lt;a href=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#Start-services-and-Run&#34;&gt;Start services and run AgenticSeek&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;See the &lt;strong&gt;Known issues&lt;/strong&gt; section if you are having issues&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;See the &lt;strong&gt;Config&lt;/strong&gt; section for detailled config file explanation.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Start services and Run&lt;/h2&gt; &#xA;&lt;p&gt;Activate your python env if needed.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;source agentic_seek_env/bin/activate&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Start required services. This will start all services from the docker-compose.yml, including: - searxng - redis (required by searxng) - frontend&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;sudo ./start_services.sh # MacOS&#xA;start ./start_services.cmd # Window&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Options 1:&lt;/strong&gt; Run with the CLI interface.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 cli.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;We advice you set &lt;code&gt;headless_browser&lt;/code&gt; to False in the config.ini for CLI mode.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Options 2:&lt;/strong&gt; Run with the Web interface.&lt;/p&gt; &#xA;&lt;p&gt;Start the backend.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 api.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Go to &lt;code&gt;http://localhost:3000/&lt;/code&gt; and you should see the web interface.&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Make sure the services are up and running with &lt;code&gt;./start_services.sh&lt;/code&gt; and run the AgenticSeek with &lt;code&gt;python3 cli.py&lt;/code&gt; for CLI mode or &lt;code&gt;python3 api.py&lt;/code&gt; then go to &lt;code&gt;localhost:3000&lt;/code&gt; for web interface.&lt;/p&gt; &#xA;&lt;p&gt;You can also use speech to text by setting &lt;code&gt;listen = True&lt;/code&gt; in the config. Only for CLI mode.&lt;/p&gt; &#xA;&lt;p&gt;To exit, simply say/type &lt;code&gt;goodbye&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Here are some example usage:&lt;/p&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Make a snake game in python!&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Search the web for top cafes in Rennes, France, and save a list of three with their addresses in rennes_cafes.txt.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Write a Go program to calculate the factorial of a number, save it as factorial.go in your workspace&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Search my summer_pictures folder for all JPG files, rename them with today‚Äôs date, and save a list of renamed files in photos_list.txt&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Search online for popular sci-fi movies from 2024 and pick three to watch tonight. Save the list in movie_night.txt.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Search the web for the latest AI news articles from 2025, select three, and write a Python script to scrape their titles and summaries. Save the script as news_scraper.py and the summaries in ai_news.txt in /home/projects&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;em&gt;Friday, search the web for a free stock price API, register with &lt;a href=&#34;mailto:supersuper7434567@gmail.com&#34;&gt;supersuper7434567@gmail.com&lt;/a&gt; then write a Python script to fetch using the API daily prices for Tesla, and save the results in stock_prices.csv&lt;/em&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;&lt;em&gt;Note that form filling capabilities are still experimental and might fail.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;After you type your query, AgenticSeek will allocate the best agent for the task.&lt;/p&gt; &#xA;&lt;p&gt;Because this is an early prototype, the agent routing system might not always allocate the right agent based on your query.&lt;/p&gt; &#xA;&lt;p&gt;Therefore, you should be very explicit in what you want and how the AI might proceed for example if you want it to conduct a web search, do not say:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Do you know some good countries for solo-travel?&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;Instead, ask:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Do a web search and find out which are the best country for solo-travel&lt;/code&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;&lt;strong&gt;Setup to run the LLM on your own server&lt;/strong&gt;&lt;/h2&gt; &#xA;&lt;p&gt;If you have a powerful computer or a server that you can use, but you want to use it from your laptop you have the options to run the LLM on a remote server using our custom llm server.&lt;/p&gt; &#xA;&lt;p&gt;On your &#34;server&#34; that will run the AI model, get the ip address&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ip a | grep &#34;inet &#34; | grep -v 127.0.0.1 | awk &#39;{print $2}&#39; | cut -d/ -f1 # local ip&#xA;curl https://ipinfo.io/ip # public ip&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: For Windows or macOS, use ipconfig or ifconfig respectively to find the IP address.&lt;/p&gt; &#xA;&lt;p&gt;Clone the repository and enter the &lt;code&gt;server/&lt;/code&gt;folder.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone --depth 1 https://github.com/Fosowl/agenticSeek.git&#xA;cd agenticSeek/llm_server/&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install server specific requirements:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;pip3 install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the server script.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;python3 app.py --provider ollama --port 3333&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;You have the choice between using &lt;code&gt;ollama&lt;/code&gt; and &lt;code&gt;llamacpp&lt;/code&gt; as a LLM service.&lt;/p&gt; &#xA;&lt;p&gt;Now on your personal computer:&lt;/p&gt; &#xA;&lt;p&gt;Change the &lt;code&gt;config.ini&lt;/code&gt; file to set the &lt;code&gt;provider_name&lt;/code&gt; to &lt;code&gt;server&lt;/code&gt; and &lt;code&gt;provider_model&lt;/code&gt; to &lt;code&gt;deepseek-r1:xxb&lt;/code&gt;. Set the &lt;code&gt;provider_server_address&lt;/code&gt; to the ip address of the machine that will run the model.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;[MAIN]&#xA;is_local = False&#xA;provider_name = server&#xA;provider_model = deepseek-r1:70b&#xA;provider_server_address = x.x.x.x:3333&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Next step: &lt;a href=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/#Start-services-and-Run&#34;&gt;Start services and run AgenticSeek&lt;/a&gt;&lt;/p&gt; &#xA;&lt;hr&gt; &#xA;&lt;h2&gt;Speech to Text&lt;/h2&gt; &#xA;&lt;p&gt;Please note that currently speech to text only work in english.&lt;/p&gt; &#xA;&lt;p&gt;The speech-to-text functionality is disabled by default. To enable it, set the listen option to True in the config.ini file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;listen = True&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When enabled, the speech-to-text feature listens for a trigger keyword, which is the agent&#39;s name, before it begins processing your input. You can customize the agent&#39;s name by updating the &lt;code&gt;agent_name&lt;/code&gt; value in the &lt;em&gt;config.ini&lt;/em&gt; file:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;agent_name = Friday&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For optimal recognition, we recommend using a common English name like &#34;John&#34; or &#34;Emma&#34; as the agent name&lt;/p&gt; &#xA;&lt;p&gt;Once you see the transcript start to appear, say the agent&#39;s name aloud to wake it up (e.g., &#34;Friday&#34;).&lt;/p&gt; &#xA;&lt;p&gt;Speak your query clearly.&lt;/p&gt; &#xA;&lt;p&gt;End your request with a confirmation phrase to signal the system to proceed. Examples of confirmation phrases include:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#34;do it&#34;, &#34;go ahead&#34;, &#34;execute&#34;, &#34;run&#34;, &#34;start&#34;, &#34;thanks&#34;, &#34;would ya&#34;, &#34;please&#34;, &#34;okay?&#34;, &#34;proceed&#34;, &#34;continue&#34;, &#34;go on&#34;, &#34;do that&#34;, &#34;go it&#34;, &#34;do you understand?&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Config&lt;/h2&gt; &#xA;&lt;p&gt;Example config:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;[MAIN]&#xA;is_local = True&#xA;provider_name = ollama&#xA;provider_model = deepseek-r1:32b&#xA;provider_server_address = 127.0.0.1:11434&#xA;agent_name = Friday&#xA;recover_last_session = False&#xA;save_session = False&#xA;speak = False&#xA;listen = False&#xA;work_dir =  /Users/mlg/Documents/ai_folder&#xA;jarvis_personality = False&#xA;languages = en zh&#xA;[BROWSER]&#xA;headless_browser = False&#xA;stealth_mode = False&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Explanation&lt;/strong&gt;:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;is_local -&amp;gt; Runs the agent locally (True) or on a remote server (False).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;provider_name -&amp;gt; The provider to use (one of: &lt;code&gt;ollama&lt;/code&gt;, &lt;code&gt;server&lt;/code&gt;, &lt;code&gt;lm-studio&lt;/code&gt;, &lt;code&gt;deepseek-api&lt;/code&gt;)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;provider_model -&amp;gt; The model used, e.g., deepseek-r1:32b.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;provider_server_address -&amp;gt; Server address, e.g., 127.0.0.1:11434 for local. Set to anything for non-local API.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;agent_name -&amp;gt; Name of the agent, e.g., Friday. Used as a trigger word for TTS.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;recover_last_session -&amp;gt; Restarts from last session (True) or not (False).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;save_session -&amp;gt; Saves session data (True) or not (False).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;speak -&amp;gt; Enables voice output (True) or not (False).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;listen -&amp;gt; listen to voice input (True) or not (False).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;work_dir -&amp;gt; Folder the AI will have access to. eg: /Users/user/Documents/.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;jarvis_personality -&amp;gt; Uses a JARVIS-like personality (True) or not (False). This simply change the prompt file.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;languages -&amp;gt; The list of supported language, needed for the llm router to work properly, avoid putting too many or too similar languages.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;headless_browser -&amp;gt; Runs browser without a visible window (True) or not (False).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;stealth_mode -&amp;gt; Make bot detector time harder. Only downside is you have to manually install the anticaptcha extension.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;languages -&amp;gt; List of supported languages. Required for agent routing system. The longer the languages list the more model will be downloaded.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Providers&lt;/h2&gt; &#xA;&lt;p&gt;The table below show the available providers:&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Provider&lt;/th&gt; &#xA;   &lt;th&gt;Local?&lt;/th&gt; &#xA;   &lt;th&gt;Description&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;ollama&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Run LLMs locally with ease using ollama as a LLM provider&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;server&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Host the model on another machine, run your local machine&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;lm-studio&lt;/td&gt; &#xA;   &lt;td&gt;Yes&lt;/td&gt; &#xA;   &lt;td&gt;Run LLM locally with LM studio (&lt;code&gt;lm-studio&lt;/code&gt;)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;openai&lt;/td&gt; &#xA;   &lt;td&gt;Depends&lt;/td&gt; &#xA;   &lt;td&gt;Use ChatGPT API (non-private) or openai compatible API&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;deepseek-api&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Deepseek API (non-private)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;huggingface&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Hugging-Face API (non-private)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;togetherAI&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Use together AI API (non-private)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;google&lt;/td&gt; &#xA;   &lt;td&gt;No&lt;/td&gt; &#xA;   &lt;td&gt;Use google gemini API (non-private)&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;To select a provider change the config.ini:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;is_local = True&#xA;provider_name = ollama&#xA;provider_model = deepseek-r1:32b&#xA;provider_server_address = 127.0.0.1:5000&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;code&gt;is_local&lt;/code&gt;: should be True for any locally running LLM, otherwise False.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;provider_name&lt;/code&gt;: Select the provider to use by it&#39;s name, see the provider list above.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;provider_model&lt;/code&gt;: Set the model to use by the agent.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;provider_server_address&lt;/code&gt;: can be set to anything if you are not using the server provider.&lt;/p&gt; &#xA;&lt;h1&gt;Known issues&lt;/h1&gt; &#xA;&lt;h2&gt;Chromedriver Issues&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Known error #1:&lt;/strong&gt; &lt;em&gt;chromedriver mismatch&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;Exception: Failed to initialize browser: Message: session not created: This version of ChromeDriver only supports Chrome version 113 Current browser version is 134.0.6998.89 with binary path&lt;/code&gt;&lt;/p&gt; &#xA;&lt;p&gt;This happen if there is a mismatch between your browser and chromedriver version.&lt;/p&gt; &#xA;&lt;p&gt;You need to navigate to download the latest version:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://developer.chrome.com/docs/chromedriver/downloads&#34;&gt;https://developer.chrome.com/docs/chromedriver/downloads&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;If you&#39;re using Chrome version 115 or newer go to:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://googlechromelabs.github.io/chrome-for-testing/&#34;&gt;https://googlechromelabs.github.io/chrome-for-testing/&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;And download the chromedriver version matching your OS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/media/chromedriver_readme.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;If this section is incomplete please raise an issue.&lt;/p&gt; &#xA;&lt;h2&gt;connection adapters Issues&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;Exception: Provider lm-studio failed: HTTP request failed: No connection adapters were found for &#39;127.0.0.1:11434/v1/chat/completions&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure you have &lt;code&gt;http://&lt;/code&gt; in front of the provider IP address :&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;provider_server_address = http://127.0.0.1:11434&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;SearxNG base URL must be provided&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;raise ValueError(&#34;SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&#34;)&#xA;ValueError: SearxNG base URL must be provided either as an argument or via the SEARXNG_BASE_URL environment variable.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Maybe you didn&#39;t move &lt;code&gt;.env.example&lt;/code&gt; as &lt;code&gt;.env&lt;/code&gt; ? You can also export SEARXNG_BASE_URL:&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;export SEARXNG_BASE_URL=&#34;http://127.0.0.1:8080&#34;&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h2&gt;FAQ&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: What hardware do I need?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Model Size&lt;/th&gt; &#xA;   &lt;th&gt;GPU&lt;/th&gt; &#xA;   &lt;th&gt;Comment&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;7B&lt;/td&gt; &#xA;   &lt;td&gt;8GB Vram&lt;/td&gt; &#xA;   &lt;td&gt;‚ö†Ô∏è Not recommended. Performance is poor, frequent hallucinations, and planner agents will likely fail.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;14B&lt;/td&gt; &#xA;   &lt;td&gt;12 GB VRAM (e.g. RTX 3060)&lt;/td&gt; &#xA;   &lt;td&gt;‚úÖ Usable for simple tasks. May struggle with web browsing and planning tasks.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;32B&lt;/td&gt; &#xA;   &lt;td&gt;24+ GB VRAM (e.g. RTX 4090)&lt;/td&gt; &#xA;   &lt;td&gt;üöÄ Success with most tasks, might still struggle with task planning&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;70B+&lt;/td&gt; &#xA;   &lt;td&gt;48+ GB Vram (eg. mac studio)&lt;/td&gt; &#xA;   &lt;td&gt;üí™ Excellent. Recommended for advanced use cases.&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why Deepseek R1 over other models?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Deepseek R1 excels at reasoning and tool use for its size. We think it‚Äôs a solid fit for our needs other models work fine, but Deepseek is our primary pick.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: I get an error running &lt;code&gt;cli.py&lt;/code&gt;. What do I do?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Ensure local is running (&lt;code&gt;ollama serve&lt;/code&gt;), your &lt;code&gt;config.ini&lt;/code&gt; matches your provider, and dependencies are installed. If none work feel free to raise an issue.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Can it really run 100% locally?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;Yes with Ollama, lm-studio or server providers, all speech to text, LLM and text to speech model run locally. Non-local options (OpenAI or others API) are optional.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: Why should I use AgenticSeek when I have Manus?&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;This started as Side-Project we did out of interest about AI agents. What‚Äôs special about it is that we want to use local model and avoid APIs. We draw inspiration from Jarvis and Friday (Iron man movies) to make it &#34;cool&#34; but for functionnality we take more inspiration from Manus, because that&#39;s what people want in the first place: a local manus alternative. Unlike Manus, AgenticSeek prioritizes independence from external systems, giving you more control, privacy and avoid api cost.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;We‚Äôre looking for developers to improve AgenticSeek! Check out open issues or discussion.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/docs/CONTRIBUTING.md&#34;&gt;Contribution guide&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.star-history.com/#Fosowl/agenticSeek&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=Fosowl/agenticSeek&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Maintainers:&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/Fosowl&#34;&gt;Fosowl&lt;/a&gt; | Paris Time | (Sometime busy)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/Fosowl/agenticSeek/main/antoineVIVIES&#34;&gt;https://github.com/antoineVIVIES&lt;/a&gt; | Taipei Time | (Often busy)&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/steveh8758&#34;&gt;steveh8758&lt;/a&gt; | Taipei Time | (Always busy)&lt;/p&gt; &#xA;&lt;/blockquote&gt;</summary>
  </entry>
</feed>