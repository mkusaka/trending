<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-04-10T01:28:53Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>l15y/wenda</title>
    <updated>2023-04-10T01:28:53Z</updated>
    <id>tag:github.com,2023-04-10:/l15y/wenda</id>
    <link href="https://github.com/l15y/wenda" rel="alternate"></link>
    <summary type="html">&lt;p&gt;闻达：一个LLM调用平台。旨在通过使用为小模型外挂知识库查找的方式，在不能涌现的前提下实现近似于大模型的生成能力&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;闻达：一个大规模语言模型调用平台&lt;/h1&gt; &#xA;&lt;h2&gt;简介&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;目前支持模型：&lt;code&gt;chatGLM-6B&lt;/code&gt;、&lt;code&gt;chatRWKV&lt;/code&gt;、&lt;code&gt;chatYuan&lt;/code&gt;。&lt;/li&gt; &#xA; &lt;li&gt;知识库自动查找&lt;/li&gt; &#xA; &lt;li&gt;支持参数在线调整&lt;/li&gt; &#xA; &lt;li&gt;支持&lt;code&gt;chatGLM-6B&lt;/code&gt;、&lt;code&gt;chatRWKV&lt;/code&gt;流式输出和输出过程中中断&lt;/li&gt; &#xA; &lt;li&gt;自动保存对话历史至浏览器（多用户同时使用不会冲突）&lt;/li&gt; &#xA; &lt;li&gt;对话历史管理（删除单条、清空）&lt;/li&gt; &#xA; &lt;li&gt;支持局域网、内网部署和多用户同时使用。（内网部署需手动将前端静态资源切换成本地）&lt;/li&gt; &#xA; &lt;li&gt;多用户同时使用中会自动排队，并显示当前用户。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;欢迎同学们制作教学视频、懒人包等，做好请和我联系，我会把相关链接加到readme里&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;交流QQ群：162451840&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h2&gt;截图&lt;/h2&gt; &#xA;&lt;h4&gt;设置和预设功能&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/setting.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;预设功能使用&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/func.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;懒人包&lt;/h2&gt; &#xA;&lt;p&gt;链接：&lt;a href=&#34;https://pan.baidu.com/s/105nOsldGt5mEPoT2np1ZoA?pwd=lyqz&#34;&gt;https://pan.baidu.com/s/105nOsldGt5mEPoT2np1ZoA?pwd=lyqz&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;提取码：lyqz&lt;/p&gt; &#xA;&lt;p&gt;默认参数在GTX1660Ti（6G显存）上运行良好。&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;旧版包含程序主体和chatGLM-6B、chatYuan，分别是独立的压缩文件。&lt;/li&gt; &#xA; &lt;li&gt;chatRWKV模型更新频繁，请去官方链接下最新的。暂不支持chatPDF功能，很快就加上。&lt;/li&gt; &#xA; &lt;li&gt;新版暂时只有chatGLM-6B，但重新制作，体积更新，包含各种优化，集成知识库功能，推荐使用。&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;自行安装&lt;/h2&gt; &#xA;&lt;h3&gt;1.安装库&lt;/h3&gt; &#xA;&lt;p&gt;知识库索引模式：&lt;code&gt;pip install -r requirements-sy.txt&lt;/code&gt; 知识库语义模式：&lt;code&gt;pip install -r requirements-yy.txt&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2.下载模型&lt;/h3&gt; &#xA;&lt;p&gt;根据需要，下载对应模型。&lt;/p&gt; &#xA;&lt;p&gt;建议使用chatRWKV的RWKV-4-Raven-7B-v7-ChnEng-20230404-ctx2048（截止4月6日效果较好），或chatGLM-6B。&lt;/p&gt; &#xA;&lt;h3&gt;3.参数设置&lt;/h3&gt; &#xA;&lt;p&gt;根据&lt;code&gt;settings.bat&lt;/code&gt;中说明，填写你的模型下载位置等信息&lt;/p&gt; &#xA;&lt;h3&gt;4.生成知识库&lt;/h3&gt; &#xA;&lt;p&gt;将txt格式的语料放到txt文件夹中，运行&lt;code&gt;run_data_processing.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;h2&gt;知识库&lt;/h2&gt; &#xA;&lt;p&gt;知识库最终效果是生成一些提示信息，会插入到对话里面。 首先要把txt目录下的文件喂给一个类似搜索引擎的东西，然后在对话过程中去查询这个搜索引擎获得提示信息，然后在回答之前插入提示信息，知识库的数据就被模型知道了。 主要是有以下两种方案：&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;s模式，基于 whoosh 搜索引擎，生成提示语。&lt;/li&gt; &#xA; &lt;li&gt;x模式，基于 model/simcse-chinese-roberta-wwm-ext 模型，去生成提示语 为防止爆显存，插入的数据不能太长，所以有字数限制&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h4&gt;chatGLM-6B模型&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/zsk-glm.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;chatRWKV模型&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/zsk-rwkv.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;1.索引语料&lt;/h3&gt; &#xA;&lt;p&gt;把自己的txt格式的文档放在名为txt的文件夹里，运行: &lt;code&gt;run_data_processing.bat&lt;/code&gt;&lt;/p&gt; &#xA;&lt;h3&gt;2.使用&lt;/h3&gt; &#xA;&lt;p&gt;正常使用中，勾选右上角知识库&lt;/p&gt; &#xA;&lt;h2&gt;chatGLM-6B&lt;/h2&gt; &#xA;&lt;p&gt;运行：&lt;code&gt;run_GLM6B.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;模型位置等参数：修改&lt;code&gt;settings.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;默认参数在GTX1660Ti（6G显存）上运行良好。&lt;/p&gt; &#xA;&lt;h2&gt;chatRWKV&lt;/h2&gt; &#xA;&lt;p&gt;运行：&lt;code&gt;run_rwkv.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;模型位置等参数：修改&lt;code&gt;settings.bat&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;默认参数在GTX1660Ti（6G显存）上正常运行，但速度较慢。&lt;/p&gt; &#xA;&lt;h3&gt;生成小说&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/novel.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h4&gt;文字冒险游戏&lt;/h4&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/l15y/wenda/main/imgs/wzmx.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;chatYuan&lt;/h2&gt; &#xA;&lt;p&gt;YuanAPI.py&lt;/p&gt; &#xA;&lt;p&gt;模型默认位置：ChatYuan-large-v2&lt;/p&gt; &#xA;&lt;p&gt;这个最轻量，是电脑都能跑，但是智力差点&lt;/p&gt; &#xA;&lt;h2&gt;TODO&lt;/h2&gt; &#xA;&lt;p&gt;实现以下知识库插件：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;文本检索-完成&#xA;语义向量-完成-待优化&#xA;知识图谱&#xA;行业数据库&#xA;搜索引擎&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>BuilderIO/ai-shell</title>
    <updated>2023-04-10T01:28:53Z</updated>
    <id>tag:github.com,2023-04-10:/BuilderIO/ai-shell</id>
    <link href="https://github.com/BuilderIO/ai-shell" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A CLI that converts natural language to shell commands.&lt;/p&gt;&lt;hr&gt;&lt;h2 align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Fb5b9997cec2c4fffb3e5c5e9bb4fed7d&#34;&gt; &#xA;  &lt;img width=&#34;300&#34; alt=&#34;AI Shell logo&#34; src=&#34;https://raw.githubusercontent.com/BuilderIO/ai-shell/main/%5Bhttps://user-images.githubusercontent.com/844291/230786555-a58479e4-75f3-4222-a6eb-74c5af953eac.png%5D(https://cdn.builder.io/api/v1/image/assets%2FYJIGb4i01jvw0SRdL5Bt%2Fb7f9d2d9911a4199a9d26f8ba210b3f8)&#34;&gt; &#xA; &lt;/picture&gt; &lt;/h2&gt; &#xA;&lt;h4 align=&#34;center&#34;&gt; A CLI that converts natural language to shell commands. &lt;/h4&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.npmjs.com/package/@builder.io/ai-shell&#34;&gt;&lt;img src=&#34;https://img.shields.io/npm/v/@builder.io/ai-shell&#34; alt=&#34;Current version&#34;&gt;&lt;/a&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img alt=&#34;Gif Demo&#34; src=&#34;https://user-images.githubusercontent.com/844291/230413167-773845e7-4c9f-44a5-909c-02802b5e49f6.gif&#34;&gt; &lt;/p&gt;&#xA;&lt;p&gt; &lt;/p&gt;&#xA;&lt;p align=&#34;center&#34;&gt; Inspired by the &lt;a href=&#34;https://githubnext.com/projects/copilot-cli&#34;&gt;Github Copilot X CLI&lt;/a&gt;, but open source for everyone. &lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;AI Shell&lt;/h1&gt; &#xA;&lt;h2&gt;Setup&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;The minimum supported version of Node.js is v14&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install &lt;em&gt;ai shell&lt;/em&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;npm install -g @builder.io/ai-shell&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Retrieve your API key from &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;OpenAI&lt;/a&gt;&lt;/p&gt; &#xA;  &lt;blockquote&gt; &#xA;   &lt;p&gt;Note: If you haven&#39;t already, you&#39;ll have to create an account and set up billing.&lt;/p&gt; &#xA;  &lt;/blockquote&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Set the key so ai-shell can use it:&lt;/p&gt; &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;ai-shell config set OPENAI_KEY=&amp;lt;your token&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create a &lt;code&gt;.ai-shell&lt;/code&gt; file in your home directory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ai &amp;lt;prompt&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For example:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ai list all log files&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then you will get an output like this, where you can choose to run the suggested command, revise the command via a prompt, or cancel:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;◇  Your script:&#xA;│&#xA;│  find . -name &#34;*.log&#34;&#xA;│&#xA;◇  Explanation:&#xA;│&#xA;│  1. Searches for all files with the extension &#34;.log&#34; in the current directory and any subdirectories.&#xA;│&#xA;◆  Run this script?&#xA;│  ● ✅ Yes (Lets go!)&#xA;│  ○ 📝 Revise&#xA;│  ○ ❌ Cancel&#xA;└&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Special characters&lt;/h3&gt; &#xA;&lt;p&gt;Note that some shells handle certain characters like the &lt;code&gt;?&lt;/code&gt; or &lt;code&gt;*&lt;/code&gt; or things that look like file paths specially. If you are getting strange behaviors, you can wrap the prompt in quotes to avoid issues, like below:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ai &#39;what is my ip address&#39;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Upgrading&lt;/h3&gt; &#xA;&lt;p&gt;Check the installed version with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;ai-shell --version&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If it&#39;s not the &lt;a href=&#34;https://github.com/BuilderIO/ai-shell/tags&#34;&gt;latest version&lt;/a&gt;, run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;npm update -g @builder.io/ai-shell&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Common Issues&lt;/h2&gt; &#xA;&lt;h3&gt;429 error&lt;/h3&gt; &#xA;&lt;p&gt;Some users are reporting a 429 from OpenAI. This is due to incorrect billing setup or excessive quota usage. Please follow &lt;a href=&#34;https://help.openai.com/en/articles/6891831-error-code-429-you-exceeded-your-current-quota-please-check-your-plan-and-billing-details&#34;&gt;this guide&lt;/a&gt; to fix it.&lt;/p&gt; &#xA;&lt;p&gt;You can activate billing at &lt;a href=&#34;https://platform.openai.com/account/billing/overview&#34;&gt;this link&lt;/a&gt;. Make sure to add a payment method if not under an active grant from OpenAI.&lt;/p&gt; &#xA;&lt;h2&gt;Motivation&lt;/h2&gt; &#xA;&lt;p&gt;I am not a bash wizard, and am dying for access to the copilot CLI, and got impatient.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;If you want to help fix a bug or implement a feature in &lt;a href=&#34;https://github.com/BuilderIO/ai-shell/issues&#34;&gt;Issues&lt;/a&gt; (tip: look out for the &lt;code&gt;help wanted&lt;/code&gt; label), checkout the &lt;a href=&#34;https://raw.githubusercontent.com/BuilderIO/ai-shell/main/CONTRIBUTING.md&#34;&gt;Contribution Guide&lt;/a&gt; to learn how to setup the project.&lt;/p&gt; &#xA;&lt;h2&gt;Credit&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to Github Copilot for their amazing tools and the idea for this&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Hassan and his work on &lt;a href=&#34;https://github.com/Nutlope/aicommits&#34;&gt;aicommits&lt;/a&gt; which inspired the workflow and some parts of the code and flows&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Community&lt;/h2&gt; &#xA;&lt;p&gt;Come join the &lt;a href=&#34;https://discord.gg/EMx6e58xnw&#34;&gt;Builder.io discord&lt;/a&gt; and chat with us in the #ai-shell-general room&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;a href=&#34;https://www.builder.io/m/developers&#34;&gt; &#xA;  &lt;picture&gt; &#xA;   &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://user-images.githubusercontent.com/844291/230786554-eb225eeb-2f6b-4286-b8c2-535b1131744a.png&#34;&gt; &#xA;   &lt;img width=&#34;250&#34; alt=&#34;Made with love by Builder.io&#34; src=&#34;https://user-images.githubusercontent.com/844291/230786555-a58479e4-75f3-4222-a6eb-74c5af953eac.png&#34;&gt; &#xA;  &lt;/picture&gt; &lt;/a&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Akegarasu/ChatGLM-webui</title>
    <updated>2023-04-10T01:28:53Z</updated>
    <id>tag:github.com,2023-04-10:/Akegarasu/ChatGLM-webui</id>
    <link href="https://github.com/Akegarasu/ChatGLM-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A WebUI for ChatGLM-6B&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;ChatGLM-webui&lt;/h1&gt; &#xA;&lt;p&gt;A webui for ChatGLM made by THUDM. &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;chatglm-6b&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/36563862/226985330-48e3b7f8-8c03-4778-af39-fd9b3a993d19.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Original Chat like &lt;a href=&#34;https://huggingface.co/THUDM/chatglm-6b&#34;&gt;chatglm-6b&lt;/a&gt;&#39;s demo, but use Gradio Chatbox for better user experience.&lt;/li&gt; &#xA; &lt;li&gt;One click install script (but you still must install python)&lt;/li&gt; &#xA; &lt;li&gt;More parameters that can be freely adjusted&lt;/li&gt; &#xA; &lt;li&gt;Convenient save/load dialog history, presets&lt;/li&gt; &#xA; &lt;li&gt;Custom maximum context length&lt;/li&gt; &#xA; &lt;li&gt;Save to Markdown&lt;/li&gt; &#xA; &lt;li&gt;Use program arguments to specify model and caculation accuracy&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Install&lt;/h2&gt; &#xA;&lt;h3&gt;requirements&lt;/h3&gt; &#xA;&lt;p&gt;python3.10&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117&#xA;pip install --upgrade -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bash install.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Run&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;python webui.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Arguments&lt;/h3&gt; &#xA;&lt;p&gt;&lt;code&gt;--model-path&lt;/code&gt;: specify model path. If this parameter is not specified manually, the default value is &lt;code&gt;THUDM/chatglm-6b&lt;/code&gt;. Transformers will automatically download model from huggingface.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--listen&lt;/code&gt;: launch gradio with 0.0.0.0 as server name, allowing to respond to network requests&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--port&lt;/code&gt;: webui port&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--share&lt;/code&gt;: use gradio to share&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--precision&lt;/code&gt;: fp32(CPU only), fp16, int4(CUDA GPU only), int8(CUDA GPU only)&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;--cpu&lt;/code&gt;: use cpu&lt;/p&gt;</summary>
  </entry>
</feed>