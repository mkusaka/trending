<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-06-09T01:28:57Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>huggingface/lerobot</title>
    <updated>2024-06-09T01:28:57Z</updated>
    <id>tag:github.com,2024-06-09:/huggingface/lerobot</id>
    <link href="https://github.com/huggingface/lerobot" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🤗 LeRobot: End-to-end Learning for Real-World Robotics in Pytorch&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;media/lerobot-logo-thumbnail.png&#34;&gt; &#xA;  &lt;img alt=&#34;LeRobot, Hugging Face Robotics Library&#34; src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/lerobot-logo-thumbnail.png&#34; style=&#34;max-width: 100%;&#34;&gt; &#xA; &lt;/picture&gt; &lt;br&gt; &lt;br&gt; &lt;/p&gt; &#xA;&lt;div align=&#34;center&#34;&gt; &#xA; &lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml?query=branch%3Amain&#34;&gt;&lt;img src=&#34;https://github.com/huggingface/lerobot/actions/workflows/nightly-tests.yml/badge.svg?branch=main&#34; alt=&#34;Tests&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://codecov.io/gh/huggingface/lerobot&#34;&gt;&lt;img src=&#34;https://codecov.io/gh/huggingface/lerobot/branch/main/graph/badge.svg?token=TODO&#34; alt=&#34;Coverage&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/pyversions/lerobot&#34; alt=&#34;Python versions&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache%202.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/status/lerobot&#34; alt=&#34;Status&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://pypi.org/project/lerobot/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/lerobot&#34; alt=&#34;Version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/tree/main/examples&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Examples-green.svg?sanitize=true&#34; alt=&#34;Examples&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CODE_OF_CONDUCT.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/Contributor%20Covenant-v2.1%20adopted-ff69b4.svg?sanitize=true&#34; alt=&#34;Contributor Covenant&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://discord.gg/s3KuuzsPFb&#34;&gt;&lt;img src=&#34;https://dcbadge.vercel.app/api/server/C5P34WJ68S?style=flat&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;h3 align=&#34;center&#34;&gt; &lt;p&gt;State-of-the-art Machine Learning for real-world robotics&lt;/p&gt; &lt;/h3&gt; &#xA;&lt;hr&gt; &#xA;&lt;p&gt;🤗 LeRobot aims to provide models, datasets, and tools for real-world robotics in PyTorch. The goal is to lower the barrier to entry to robotics so that everyone can contribute and benefit from sharing datasets and pretrained models.&lt;/p&gt; &#xA;&lt;p&gt;🤗 LeRobot contains state-of-the-art approaches that have been shown to transfer to the real-world with a focus on imitation learning and reinforcement learning.&lt;/p&gt; &#xA;&lt;p&gt;🤗 LeRobot already provides a set of pretrained models, datasets with human collected demonstrations, and simulation environments to get started without assembling a robot. In the coming weeks, the plan is to add more and more support for real-world robotics on the most affordable and capable robots out there.&lt;/p&gt; &#xA;&lt;p&gt;🤗 LeRobot hosts pretrained models and datasets on this Hugging Face community page: &lt;a href=&#34;https://huggingface.co/lerobot&#34;&gt;huggingface.co/lerobot&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h4&gt;Examples of pretrained models on simulation environments&lt;/h4&gt; &#xA;&lt;table&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/aloha_act.gif&#34; width=&#34;100%&#34; alt=&#34;ACT policy on ALOHA env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/simxarm_tdmpc.gif&#34; width=&#34;100%&#34; alt=&#34;TDMPC policy on SimXArm env&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;http://remicadene.com/assets/gif/pusht_diffusion.gif&#34; width=&#34;100%&#34; alt=&#34;Diffusion policy on PushT env&#34;&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;ACT policy on ALOHA env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;TDMPC policy on SimXArm env&lt;/td&gt; &#xA;   &lt;td align=&#34;center&#34;&gt;Diffusion policy on PushT env&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Acknowledgment&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Thanks to Tony Zaho, Zipeng Fu and colleagues for open sourcing ACT policy, ALOHA environments and datasets. Ours are adapted from &lt;a href=&#34;https://tonyzhaozh.github.io/aloha&#34;&gt;ALOHA&lt;/a&gt; and &lt;a href=&#34;https://mobile-aloha.github.io&#34;&gt;Mobile ALOHA&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Cheng Chi, Zhenjia Xu and colleagues for open sourcing Diffusion policy, Pusht environment and datasets, as well as UMI datasets. Ours are adapted from &lt;a href=&#34;https://diffusion-policy.cs.columbia.edu&#34;&gt;Diffusion Policy&lt;/a&gt; and &lt;a href=&#34;https://umi-gripper.github.io&#34;&gt;UMI Gripper&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Nicklas Hansen, Yunhai Feng and colleagues for open sourcing TDMPC policy, Simxarm environments and datasets. Ours are adapted from &lt;a href=&#34;https://github.com/nicklashansen/tdmpc&#34;&gt;TDMPC&lt;/a&gt; and &lt;a href=&#34;https://www.yunhaifeng.com/FOWM&#34;&gt;FOWM&lt;/a&gt;.&lt;/li&gt; &#xA; &lt;li&gt;Thanks to Antonio Loquercio and Ashish Kumar for their early support.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Download our source code:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/huggingface/lerobot.git &amp;amp;&amp;amp; cd lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Create a virtual environment with Python 3.10 and activate it, e.g. with &lt;a href=&#34;https://docs.anaconda.com/free/miniconda/index.html&#34;&gt;&lt;code&gt;miniconda&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda create -y -n lerobot python=3.10 &amp;amp;&amp;amp; conda activate lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install 🤗 LeRobot:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install .&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Depending on your platform, If you encounter any build errors during this step you may need to install &lt;code&gt;cmake&lt;/code&gt; and &lt;code&gt;build-essential&lt;/code&gt; for building some of our dependencies. On linux: &lt;code&gt;sudo apt-get install cmake build-essential&lt;/code&gt;&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;For simulations, 🤗 LeRobot comes with gymnasium environments that can be installed as extras:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-aloha&#34;&gt;aloha&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-xarm&#34;&gt;xarm&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/huggingface/gym-pusht&#34;&gt;pusht&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;For instance, to install 🤗 LeRobot with aloha and pusht, use:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install &#34;.[aloha, pusht]&#34;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use &lt;a href=&#34;https://docs.wandb.ai/quickstart&#34;&gt;Weights and Biases&lt;/a&gt; for experiment tracking, log in with&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wandb login&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;(note: you will also need to enable WandB in the configuration. See below.)&lt;/p&gt; &#xA;&lt;h2&gt;Walkthrough&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;.&#xA;├── examples             # contains demonstration examples, start here to learn about LeRobot&#xA;|   └── advanced         # contains even more examples for those who have mastered the basics&#xA;├── lerobot&#xA;|   ├── configs          # contains hydra yaml files with all options that you can override in the command line&#xA;|   |   ├── default.yaml   # selected by default, it loads pusht environment and diffusion policy&#xA;|   |   ├── env            # various sim environments and their datasets: aloha.yaml, pusht.yaml, xarm.yaml&#xA;|   |   └── policy         # various policies: act.yaml, diffusion.yaml, tdmpc.yaml&#xA;|   ├── common           # contains classes and utilities&#xA;|   |   ├── datasets       # various datasets of human demonstrations: aloha, pusht, xarm&#xA;|   |   ├── envs           # various sim environments: aloha, pusht, xarm&#xA;|   |   ├── policies       # various policies: act, diffusion, tdmpc&#xA;|   |   └── utils          # various utilities&#xA;|   └── scripts          # contains functions to execute via command line&#xA;|       ├── eval.py                 # load policy and evaluate it on an environment&#xA;|       ├── train.py                # train a policy via imitation learning and/or reinforcement learning&#xA;|       ├── push_dataset_to_hub.py  # convert your dataset into LeRobot dataset format and upload it to the Hugging Face hub&#xA;|       └── visualize_dataset.py    # load a dataset and render its demonstrations&#xA;├── outputs               # contains results of scripts execution: logs, videos, model checkpoints&#xA;└── tests                 # contains pytest utilities for continuous integration&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Visualize datasets&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/1_load_lerobot_dataset.py&#34;&gt;example 1&lt;/a&gt; that illustrates how to use our dataset class which automatically download data from the Hugging Face hub.&lt;/p&gt; &#xA;&lt;p&gt;You can also locally visualize episodes from a dataset by executing our script from the command line:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/visualize_dataset.py \&#xA;    --repo-id lerobot/pusht \&#xA;    --episode-index 0&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;It will open &lt;code&gt;rerun.io&lt;/code&gt; and display the camera streams, robot states and actions, like this:&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&#34;&gt;https://github-production-user-asset-6210df.s3.amazonaws.com/4681518/328035972-fd46b787-b532-47e2-bb6f-fd536a55a7ed.mov?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;amp;X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240505%2Fus-east-1%2Fs3%2Faws4_request&amp;amp;X-Amz-Date=20240505T172924Z&amp;amp;X-Amz-Expires=300&amp;amp;X-Amz-Signature=d680b26c532eeaf80740f08af3320d22ad0b8a4e4da1bcc4f33142c15b509eda&amp;amp;X-Amz-SignedHeaders=host&amp;amp;actor_id=24889239&amp;amp;key_id=0&amp;amp;repo_id=748713144&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;Our script can also visualize datasets stored on a distant server. See &lt;code&gt;python lerobot/scripts/visualize_dataset.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluate a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/2_evaluate_pretrained_policy.py&#34;&gt;example 2&lt;/a&gt; that illustrates how to download a pretrained policy from Hugging Face hub, and run an evaluation on its corresponding environment.&lt;/p&gt; &#xA;&lt;p&gt;We also provide a more capable script to parallelize the evaluation over multiple environments during the same rollout. Here is an example with a pretrained model hosted on &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py \&#xA;    -p lerobot/diffusion_pusht \&#xA;    eval.n_episodes=10 \&#xA;    eval.batch_size=10&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Note: After training your own policy, you can re-evaluate the checkpoints with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/eval.py -p {OUTPUT_DIR}/checkpoints/last/pretrained_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h3&gt;Train your own policy&lt;/h3&gt; &#xA;&lt;p&gt;Check out &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/3_train_policy.py&#34;&gt;example 3&lt;/a&gt; that illustrates how to train a model using our core library in python, and &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/examples/4_train_policy_with_script.md&#34;&gt;example 4&lt;/a&gt; that shows how to use our training script from command line.&lt;/p&gt; &#xA;&lt;p&gt;In general, you can use our training script to easily train any policy. Here is an example of training the ACT policy on trajectories collected by humans on the Aloha simulation environment for the insertion task:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/train.py \&#xA;    policy=act \&#xA;    env=aloha \&#xA;    env.task=AlohaInsertion-v0 \&#xA;    dataset_repo_id=lerobot/aloha_sim_insertion_human \&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;The experiment directory is automatically generated and will show up in yellow in your terminal. It looks like &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default&lt;/code&gt;. You can manually specify an experiment directory by adding this argument to the &lt;code&gt;train.py&lt;/code&gt; python command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    hydra.run.dir=your/new/experiment/dir&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;In the experiment directory there will be a folder called &lt;code&gt;checkpoints&lt;/code&gt; which will have the following structure:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;checkpoints&#xA;├── 000250  # checkpoint_dir for training step 250&#xA;│   ├── pretrained_model  # Hugging Face pretrained model dir&#xA;│   │   ├── config.json  # Hugging Face pretrained model config&#xA;│   │   ├── config.yaml  # consolidated Hydra config&#xA;│   │   ├── model.safetensors  # model weights&#xA;│   │   └── README.md  # Hugging Face model card&#xA;│   └── training_state.pth  # optimizer/scheduler/rng state and training step&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;To use wandb for logging training and evaluation curves, make sure you&#39;ve run &lt;code&gt;wandb login&lt;/code&gt; as a one-time setup step. Then, when running the training command above, enable WandB in the configuration by adding:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;    wandb.enable=true&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;A link to the wandb logs for the run will also show up in yellow in your terminal. Here is an example of what they look like in your browser:&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/media/wandb.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Note: For efficiency, during training every checkpoint is evaluated on a low number of episodes. You may use &lt;code&gt;eval.n_episodes=500&lt;/code&gt; to evaluate on more episodes than the default. Or, after training, you may want to re-evaluate your best checkpoints on more episodes or change the evaluation settings. See &lt;code&gt;python lerobot/scripts/eval.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;h4&gt;Reproduce state-of-the-art (SOTA)&lt;/h4&gt; &#xA;&lt;p&gt;We have organized our configuration files (found under &lt;a href=&#34;https://raw.githubusercontent.com/huggingface/lerobot/main/lerobot/configs&#34;&gt;&lt;code&gt;lerobot/configs&lt;/code&gt;&lt;/a&gt;) such that they reproduce SOTA results from a given model variant in their respective original works. Simply running:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/train.py policy=diffusion env=pusht&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;reproduces SOTA results for Diffusion Policy on the PushT task.&lt;/p&gt; &#xA;&lt;p&gt;Pretrained policies, along with reproduction details, can be found under the &#34;Models&#34; section of &lt;a href=&#34;https://huggingface.co/lerobot&#34;&gt;https://huggingface.co/lerobot&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Contribute&lt;/h2&gt; &#xA;&lt;p&gt;If you would like to contribute to 🤗 LeRobot, please check out our &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/CONTRIBUTING.md&#34;&gt;contribution guide&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Add a new dataset&lt;/h3&gt; &#xA;&lt;p&gt;To add a dataset to the hub, you need to login using a write-access token, which can be generated from the &lt;a href=&#34;https://huggingface.co/settings/tokens&#34;&gt;Hugging Face settings&lt;/a&gt;:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli login --token ${HUGGINGFACE_TOKEN} --add-to-git-credential&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then move your dataset folder in &lt;code&gt;data&lt;/code&gt; directory (e.g. &lt;code&gt;data/aloha_static_pingpong_test&lt;/code&gt;), and push your dataset to the hub with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python lerobot/scripts/push_dataset_to_hub.py \&#xA;--data-dir data \&#xA;--dataset-id aloha_static_pingpong_test \&#xA;--raw-format aloha_hdf5 \&#xA;--community-id lerobot&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;code&gt;python lerobot/scripts/push_dataset_to_hub.py --help&lt;/code&gt; for more instructions.&lt;/p&gt; &#xA;&lt;p&gt;If your dataset format is not supported, implement your own in &lt;code&gt;lerobot/common/datasets/push_dataset_to_hub/${raw_format}_format.py&lt;/code&gt; by copying examples like &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/pusht_zarr_format.py&#34;&gt;pusht_zarr&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/umi_zarr_format.py&#34;&gt;umi_zarr&lt;/a&gt;, &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/aloha_hdf5_format.py&#34;&gt;aloha_hdf5&lt;/a&gt;, or &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/common/datasets/push_dataset_to_hub/xarm_pkl_format.py&#34;&gt;xarm_pkl&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Add a pretrained policy&lt;/h3&gt; &#xA;&lt;p&gt;Once you have trained a policy you may upload it to the Hugging Face hub using a hub id that looks like &lt;code&gt;${hf_user}/${repo_name}&lt;/code&gt; (e.g. &lt;a href=&#34;https://huggingface.co/lerobot/diffusion_pusht&#34;&gt;lerobot/diffusion_pusht&lt;/a&gt;).&lt;/p&gt; &#xA;&lt;p&gt;You first need to find the checkpoint folder located inside your experiment directory (e.g. &lt;code&gt;outputs/train/2024-05-05/20-21-12_aloha_act_default/checkpoints/002500&lt;/code&gt;). Within that there is a &lt;code&gt;pretrained_model&lt;/code&gt; directory which should contain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;config.json&lt;/code&gt;: A serialized version of the policy configuration (following the policy&#39;s dataclass config).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;model.safetensors&lt;/code&gt;: A set of &lt;code&gt;torch.nn.Module&lt;/code&gt; parameters, saved in &lt;a href=&#34;https://huggingface.co/docs/safetensors/index&#34;&gt;Hugging Face Safetensors&lt;/a&gt; format.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;config.yaml&lt;/code&gt;: A consolidated Hydra training configuration containing the policy, environment, and dataset configs. The policy configuration should match &lt;code&gt;config.json&lt;/code&gt; exactly. The environment config is useful for anyone who wants to evaluate your policy. The dataset config just serves as a paper trail for reproducibility.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;To upload these to the hub, run the following:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;huggingface-cli upload ${hf_user}/${repo_name} path/to/pretrained_model&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;See &lt;a href=&#34;https://github.com/huggingface/lerobot/raw/main/lerobot/scripts/eval.py&#34;&gt;eval.py&lt;/a&gt; for an example of how other people may use your policy.&lt;/p&gt; &#xA;&lt;h3&gt;Improve your code with profiling&lt;/h3&gt; &#xA;&lt;p&gt;An example of a code snippet to profile the evaluation of a policy:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from torch.profiler import profile, record_function, ProfilerActivity&#xA;&#xA;def trace_handler(prof):&#xA;    prof.export_chrome_trace(f&#34;tmp/trace_schedule_{prof.step_num}.json&#34;)&#xA;&#xA;with profile(&#xA;    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],&#xA;    schedule=torch.profiler.schedule(&#xA;        wait=2,&#xA;        warmup=2,&#xA;        active=3,&#xA;    ),&#xA;    on_trace_ready=trace_handler&#xA;) as prof:&#xA;    with record_function(&#34;eval_policy&#34;):&#xA;        for i in range(num_episodes):&#xA;            prof.step()&#xA;            # insert code to profile, potentially whole body of eval_policy function&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you want, you can cite this work with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;@misc{cadene2024lerobot,&#xA;    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},&#xA;    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},&#xA;    howpublished = &#34;\url{https://github.com/huggingface/lerobot}&#34;,&#xA;    year = {2024}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>idootop/mi-gpt</title>
    <updated>2024-06-09T01:28:57Z</updated>
    <id>tag:github.com,2024-06-09:/idootop/mi-gpt</id>
    <link href="https://github.com/idootop/mi-gpt" rel="alternate"></link>
    <summary type="html">&lt;p&gt;🏠 将小爱音箱接入 ChatGPT 和豆包，改造成你的专属语音助手。&lt;/p&gt;&lt;hr&gt;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/idootop/mi-gpt/main/assets/demo.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;h1&gt;MiGPT：智能家居，从未如此贴心 ❤️&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.npmjs.com/package/mi-gpt&#34;&gt;&lt;img src=&#34;https://badge.fury.io/js/mi-gpt.svg?sanitize=true&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://hub.docker.com/r/idootop/mi-gpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/idootop/mi-gpt?color=%23086DCD&amp;amp;label=docker%20image&#34; alt=&#34;Docker Image Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;在这个数字化的世界里，家已不仅仅是一个居住的地方，而是我们数字生活的延伸。&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;MiGPT&lt;/code&gt; 通过将小爱音箱、米家智能设备，与 ChatGPT 的理解能力完美融合，让你的智能家居更懂你。&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;MiGPT&lt;/code&gt; 不仅仅是关于设备自动化，而是关于：&lt;strong&gt;打造一个懂你、有温度、与你共同进化的家&lt;/strong&gt;。&lt;/p&gt; &#xA;&lt;p&gt;未来，你的每个智能家居设备，从灯泡、插座，到扫地机器人、电视等，&lt;/p&gt; &#xA;&lt;p&gt;都可以作为一个个独立的智能体 (Agent)，更智能、更贴心的响应你的指令。&lt;/p&gt; &#xA;&lt;p&gt;这些独立的智能体，也可以彼此感知，彼此配合，构成一个更强大的协作网络。&lt;/p&gt; &#xA;&lt;p&gt;而小爱音箱就像是你的智能家居专属管家，全心全意为你服务，释放智能家居的真正潜力。&lt;/p&gt; &#xA;&lt;h2&gt;⚡️ 项目预览&lt;/h2&gt; &#xA;&lt;p&gt;👉 查看完整演示视频：【&lt;a href=&#34;https://www.bilibili.com/video/BV1N1421y7qn/?share_source=copy_web&amp;amp;vd_source=5d4e78ff2a0dc6a661baa65f479199c1&#34;&gt;整活！将小爱音箱接入 ChatGPT 和豆包，改造成你的专属语音助手～&lt;/a&gt;】&lt;/p&gt; &#xA;&lt;p&gt;&#xA; &lt;video src=&#34;https://github.com/idootop/mi-gpt/assets/35302658/dc336916-9087-418b-bc1b-04d5534dce8f&#34;&gt;&lt;/video&gt;&lt;/p&gt; &#xA;&lt;h2&gt;✨ 项目亮点&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;🎓 LLM 回答&lt;/strong&gt;。想象一下，你的小爱音箱变身聊天高手，可以使用 &lt;a href=&#34;https://chat.openai.com&#34;&gt;ChatGPT&lt;/a&gt; 等大模型来回答你的问题。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;🎭 角色扮演&lt;/strong&gt;。一秒调教小爱，无论是成为你的完美伴侣，还是那个能听你倾诉心事的贴心闺蜜，都不在话下。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;💬 流式响应&lt;/strong&gt;。爱情来得太快就像龙卷风，而你的小爱音箱也是，对你的爱意秒回，爱你不会让你等太久。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;🧠 长短期记忆&lt;/strong&gt;。小爱音箱现在能记住你们之间的每一次对话，越聊越默契，就像是你身边的老朋友。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;🔊 自定义 TTS&lt;/strong&gt;。厌倦了小爱同学的语音？帮你解锁&lt;a href=&#34;https://doubao.com&#34;&gt;「豆包」&lt;/a&gt;同款音色，就像真人在回你的消息。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;🤖️ 智能家居 Agent&lt;/strong&gt;。心情不好？小爱立刻懂你，自动帮你播放喜欢的音乐，调节灯光，逗你开心。&lt;em&gt;TODO&lt;/em&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;🚀 启动项目&lt;/h2&gt; &#xA;&lt;p&gt;&lt;code&gt;MiGPT&lt;/code&gt; 有两种启动方式: &lt;a href=&#34;https://raw.githubusercontent.com/idootop/mi-gpt/main/#docker&#34;&gt;Docker&lt;/a&gt; 和 &lt;a href=&#34;https://raw.githubusercontent.com/idootop/mi-gpt/main/#nodejs&#34;&gt;Node.js&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;h3&gt;Docker&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/idootop/mi-gpt&#34;&gt;&lt;img src=&#34;https://img.shields.io/docker/v/idootop/mi-gpt?color=%23086DCD&amp;amp;label=docker%20image&#34; alt=&#34;Docker Image Version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;对于电脑小白或者不想自己配置代码运行环境（Node）的同学，可以使用 Docker 启动方式。&lt;/p&gt; &#xA;&lt;p&gt;请先按照&lt;a href=&#34;https://raw.githubusercontent.com/idootop/mi-gpt/main/#%EF%B8%8F-%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0&#34;&gt;「配置参数」&lt;/a&gt;章节，配置好你的 &lt;code&gt;.env&lt;/code&gt; 和 &lt;code&gt;.migpt.js&lt;/code&gt; 文件，然后使用以下命令启动 docker：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d  --env-file $(pwd)/.env \&#xA;    -v $(pwd)/.migpt.js:/app/.migpt.js \&#xA;    idootop/mi-gpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注意：在 Windows 终端下不支持使用 &lt;code&gt;$(pwd)&lt;/code&gt; 获取当前工作路径，需要将配置文件路径替换为绝对路径。&lt;/p&gt; &#xA;&lt;h3&gt;Node.js&lt;/h3&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.npmjs.com/package/mi-gpt&#34;&gt;&lt;img src=&#34;https://badge.fury.io/js/mi-gpt.svg?sanitize=true&#34; alt=&#34;npm version&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果你是一名前端 (Node) 开发者，也可以通过 NPM 安装 &lt;code&gt;mi-gpt&lt;/code&gt; 启动 &lt;code&gt;MiGPT&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;npm install mi-gpt # 安装依赖&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;然后，创建并启动 &lt;code&gt;MiGPT&lt;/code&gt; 实例。初始化参数的具体含义请看下面的&lt;a href=&#34;https://raw.githubusercontent.com/idootop/mi-gpt/main/#%EF%B8%8F-%E9%85%8D%E7%BD%AE%E5%8F%82%E6%95%B0&#34;&gt;「配置参数」&lt;/a&gt;章节。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-typescript&#34;&gt;import { MiGPT } from &#34;mi-gpt&#34;;&#xA;&#xA;async function main() {&#xA;  const client = MiGPT.create({&#xA;    speaker: {&#xA;      userId: &#34;987654321&#34;, // 注意：不是手机号或邮箱，请在「个人信息」-「小米 ID」查看&#xA;      password: &#34;123456&#34;, // 账号密码&#xA;      did: &#34;小爱音箱Pro&#34;, // 小爱音箱 ID 或在米家中设置的名称&#xA;    },&#xA;  });&#xA;  await client.start();&#xA;}&#xA;&#xA;main();&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;注意：此模式下并不会主动读取 &lt;code&gt;.env&lt;/code&gt; 和 &lt;code&gt;.migpt.json&lt;/code&gt; 中的配置信息，你需要自己初始化 Node 环境变量，&lt;/p&gt; &#xA;&lt;p&gt;并将 &lt;code&gt;.migpt.json&lt;/code&gt; 中的参数作为 &lt;code&gt;MiGPT.create&lt;/code&gt; 的初始化参数传入。👉 &lt;a href=&#34;https://github.com/idootop/mi-gpt/raw/example/index.ts&#34;&gt;示例代码&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;⚙️ 配置参数&lt;/h2&gt; &#xA;&lt;h3&gt;.migpt.js&lt;/h3&gt; &#xA;&lt;p&gt;重命名本项目根目录下的 &lt;a href=&#34;https://github.com/idootop/mi-gpt/raw/main/.migpt.example.js&#34;&gt;.migpt.example.js&lt;/a&gt; 文件为 &lt;code&gt;.migpt.js&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;然后，将里面的配置参数修改成你自己的，参数含义如下：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;参数名称&lt;/th&gt; &#xA;   &lt;th&gt;描述&lt;/th&gt; &#xA;   &lt;th&gt;示例&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;bot&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;对方名称（小爱音箱）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;傻妞&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;profile&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;对方的个人简介/人设&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;性别女，性格乖巧可爱，喜欢搞怪，爱吃醋。&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;master&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;主人名称（我自己）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;陆小千&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;profile&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;主人的个人简介/人设&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;性别男，善良正直，总是舍己为人，是傻妞的主人。&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;room&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;name&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;会话群名称&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;魔幻手机&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;description&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;会话群简介&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;傻妞和陆小千的私聊&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;speaker&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;userId&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://account.xiaomi.com/fe/service/account/profile&#34;&gt;小米 ID&lt;/a&gt;（注意：不是手机号或邮箱）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;987654321&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;password&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;账户密码&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;123456&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;did&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;小爱音箱 ID 或名称&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;小爱音箱 Pro&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;ttsCommand&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;小爱音箱 TTS 指令（&lt;a href=&#34;https://home.miot-spec.com&#34;&gt;可在此查询&lt;/a&gt;）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[5, 1]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;wakeUpCommand&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;小爱音箱唤醒指令（&lt;a href=&#34;https://home.miot-spec.com&#34;&gt;可在此查询&lt;/a&gt;）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[5, 3]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;speaker 其他参数（可选）&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;callAIKeywords&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;当消息以关键词开头时，会调用 AI 来响应用户消息&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;请&#34;, &#34;傻妞&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;wakeUpKeywords&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;当消息以关键词开头时，会进入 AI 唤醒状态&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;召唤傻妞&#34;, &#34;打开傻妞&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;exitKeywords&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;当消息以关键词开头时，会退出 AI 唤醒状态&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;退出傻妞&#34;, &#34;关闭傻妞&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;onEnterAI&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;进入 AI 模式的欢迎语&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;你好，我是傻妞，很高兴认识你&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;onExitAI&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;退出 AI 模式的提示语&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;傻妞已退出&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;onAIAsking&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AI 开始回答时的提示语&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;让我先想想&#34;, &#34;请稍等&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;onAIReplied&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AI 结束回答时的提示语&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;我说完了&#34;, &#34;还有其他问题吗&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;onAIError&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;AI 回答异常时的提示语&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[&#34;出错了，请稍后再试吧！&#34;]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;playingCommand&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;查询小爱音箱是否在播放中指令（&lt;a href=&#34;https://home.miot-spec.com&#34;&gt;可在此查询&lt;/a&gt;）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;[3, 1, 1]&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;streamResponse&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;是否启用流式响应（部分小爱音箱型号不支持查询播放状态，此时需要关闭流式响应）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;true&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;exitKeepAliveAfter&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;无响应一段时间后，多久自动退出唤醒模式（单位秒，默认 30 秒）&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;30&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;环境变量&lt;/h3&gt; &#xA;&lt;p&gt;重命名本项目根目录下的 &lt;a href=&#34;https://github.com/idootop/mi-gpt/raw/main/.env.example&#34;&gt;.env.example&lt;/a&gt; 文件为 &lt;code&gt;.env&lt;/code&gt;。&lt;/p&gt; &#xA;&lt;p&gt;然后，将里面的环境变量修改成你自己的，参数含义如下：&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;环境变量名称&lt;/th&gt; &#xA;   &lt;th&gt;描述&lt;/th&gt; &#xA;   &lt;th&gt;示例&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;OpenAI API 密钥&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;abc123&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OPENAI_MODEL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;使用的 OpenAI 模型&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;gpt-4o&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;OPENAI_BASE_URL&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;可选，OpenAI API BaseURL&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;https://api.openai.com/v1&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AZURE_OPENAI_API_KEY&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;可选，&lt;a href=&#34;https://www.npmjs.com/package/openai#microsoft-azure-openai&#34;&gt;Microsoft Azure OpenAI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;abc123&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;提示音效（可选）&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AUDIO_SILENT&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;静音音频链接&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;https://example.com/slient.wav&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AUDIO_BEEP&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;默认提示音链接&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;https://example.com/beep.wav&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AUDIO_ACTIVE&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;唤醒提示音链接&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;https://example.com/active.wav&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;AUDIO_ERROR&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;出错提示音链接&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;https://example.com/error.wav&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;strong&gt;豆包 TTS（可选）&lt;/strong&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;TTS_DOUBAO&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;豆包 TTS 接口&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;https://example.com/tts.wav&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;code&gt;SPEAKERS_DOUBAO&lt;/code&gt;&lt;/td&gt; &#xA;   &lt;td&gt;豆包 TTS 音色列表接口&lt;/td&gt; &#xA;   &lt;td&gt;&lt;code&gt;&#34;https://example.com/tts-speakers&#34;&lt;/code&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;💬 常见问题&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：支持哪些型号的小爱音箱？&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;大部分型号的小爱音箱都支持，推荐小爱音箱 Pro（完美运行）。部分机型的 MioT 接口开放能力并不完整，比如小米音箱 Play 增强版（L05C），将会导致 &lt;code&gt;MiGPT&lt;/code&gt; 部分功能异常，相关 &lt;a href=&#34;https://github.com/idootop/mi-gpt/issues/14&#34;&gt;issue&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：除了 OpenAI 还支持哪些模型，如何设置？&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;理论上兼容 &lt;a href=&#34;https://www.npmjs.com/package/openai&#34;&gt;OpenAI SDK&lt;/a&gt; 的模型都支持，只需修改环境变量即可接入到 MiGPT。&lt;/p&gt; &#xA;&lt;p&gt;比如：&lt;a href=&#34;https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.i1&#34;&gt;通义千问&lt;/a&gt;、&lt;a href=&#34;https://platform.01.ai/docs#making-an-api-request&#34;&gt;零一万物&lt;/a&gt;、&lt;a href=&#34;https://platform.moonshot.cn/docs/api/chat&#34;&gt;Moonshot&lt;/a&gt;、&lt;a href=&#34;https://platform.deepseek.com/api-docs/&#34;&gt;DeepSeek&lt;/a&gt; 等，以 Moonshot 为例：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;OPENAI_BASE_URL=https://api.moonshot.cn/v1&#xA;OPENAI_MODEL=moonshot-v1-8k&#xA;OPENAI_API_KEY=$MOONSHOT_API_KEY&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：什么是唤醒模式？&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;唤醒模式&lt;/code&gt; 类似于小爱技能，可能让你在跟小爱互动的时候，无需每句话都要以“小爱同学”开头唤醒。&lt;/p&gt; &#xA;&lt;p&gt;关于唤醒模式的更多细节，请查看这里：&lt;a href=&#34;https://github.com/idootop/mi-gpt/issues/28&#34;&gt;https://github.com/idootop/mi-gpt/issues/28&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：提示登录小米账号失败，无法正常启动&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;账号密码不正确&lt;/strong&gt;：小米 ID 并非手机号或邮箱，请在&lt;a href=&#34;https://account.xiaomi.com/fe/service/account/profile&#34;&gt;「个人信息」-「小米 ID」&lt;/a&gt;查看。&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;网络环境异常&lt;/strong&gt;：如果你是在海外服务器等，非中国大陆网络环境下登录小米账号，需要先同意小米的「个人数据跨境传输」协议，然后按照提示验证手机号或邮箱，等待大约 30 分钟之后即可正常登录。&lt;a href=&#34;https://github.com/idootop/mi-gpt/issues/22#issuecomment-2150535622&#34;&gt;👉 相关教程&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：小爱音箱收到消息后，没有调用 AI 进行回复&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;MiGPT&lt;/code&gt; 收到消息默认不会调用 AI 进行回复，只会回复以唤醒词开头的消息，比如：“请问 xxx”、“你 xxx” 等，你也可以自定义唤醒词（&lt;code&gt;callAIKeywords&lt;/code&gt;）列表。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：小爱音箱没有播放 AI 的回答，但控制台有打印 AI 的回复&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;不同型号的小爱音箱 TTS 指令不同: &lt;a href=&#34;https://github.com/idootop/mi-gpt/issues/5#issuecomment-2122881495&#34;&gt;issues#5&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;请到 &lt;a href=&#34;https://home.miot-spec.com&#34;&gt;https://home.miot-spec.com&lt;/a&gt; 查询具体指令，并修改配置文件中的 &lt;code&gt;ttsCommand&lt;/code&gt; 参数。&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;👉 查看教程&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/idootop/mi-gpt/main/assets/search.jpg&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/idootop/mi-gpt/main/assets/command.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：小爱音箱没有读完整个句子，总是戛然而止&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;部分型号的小爱音箱不支持通过 Mina 获取设备播放状态，只能通过 MiOT 指令查询。&lt;/p&gt; &#xA;&lt;p&gt;请到 &lt;a href=&#34;https://home.miot-spec.com&#34;&gt;https://home.miot-spec.com&lt;/a&gt; 查询具体指令，并修改配置文件中的 &lt;code&gt;playingCommand&lt;/code&gt; 参数。&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;👉 查看教程&lt;/summary&gt; &#xA; &lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/idootop/mi-gpt/main/assets/playing.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;如果修改参数后问题仍然存在，说明你的设备不支持通过开放接口查询播放状态（比如：小米音箱 Play 增强版），&lt;strong&gt;此问题无解&lt;/strong&gt;。建议更换其他型号的小爱音箱（推荐小爱音箱 Pro），相关 &lt;a href=&#34;https://github.com/idootop/mi-gpt/issues/14&#34;&gt;issue&lt;/a&gt;。&lt;/p&gt; &#xA;&lt;p&gt;或者你也可以关闭配置文件中的流式响应（streamResponse）选项，确保小爱能够回复完整的句子。不过需要注意的是，关闭流式响应后，唤醒模式等功能将会失效。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q: 为什么小爱音箱会在 AI 回答之前抢话？&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;与本项目的实现原理有关。本项目通过轮询小米接口获取最新的对话信息，当检测到小爱在回复的时候会通过播放静音音频等方式快速 mute 掉小爱原来的回复。&lt;/p&gt; &#xA;&lt;p&gt;但是从小爱开始回复，到上报状态给小米服务云端，再到本项目通过小米云端接口轮训到这个状态变更，中间会有大约 1 -2 秒的延迟时间，无解。&lt;/p&gt; &#xA;&lt;p&gt;这个问题，理论上需要通过刷机才能完美解决，可以参考下面的相关讨论：&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yihong0618/xiaogpt/issues/515#issuecomment-2121602572&#34;&gt;https://github.com/yihong0618/xiaogpt/issues/515#issuecomment-2121602572&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/idootop/mi-gpt/issues/21#issuecomment-2147125219&#34;&gt;https://github.com/idootop/mi-gpt/issues/21#issuecomment-2147125219&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：启动 docker 提示 ERR_MODULE_NOT_FOUND，无法正常启动&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;在 Windows 终端（比如：PowerShell、cmd）下，无法使用 &lt;code&gt;$(pwd)&lt;/code&gt; 获取当前工作目录绝对路径，需要填写 &lt;code&gt;.env&lt;/code&gt; 和 &lt;code&gt;.migpt.js&lt;/code&gt; 文件的绝对路径。相关 &lt;a href=&#34;https://github.com/idootop/mi-gpt/issues/26#issuecomment-2151381521&#34;&gt;issue&lt;/a&gt;&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;👉 查看示例&lt;/summary&gt; &#xA; &lt;p&gt;请将下面的 &lt;code&gt;/绝对路径/&lt;/code&gt; 替换为你当前目录的绝对路径：&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --env-file /绝对路径/.env \&#xA;    -v /绝对路径/.migpt.js:/app/.migpt.js \&#xA;    idootop/mi-gpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Windows PowerShell 终端&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --env-file $pwd\.env `&#xA;    -v $pwd\.migpt.js:/app/.migpt.js `&#xA;    idootop/mi-gpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA; &lt;p&gt;Windows cmd 终端&lt;/p&gt; &#xA; &lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;docker run -d --env-file %cd%\.env ^&#xA;    -v %cd%\.migpt.js:/app/.migpt.js ^&#xA;    idootop/mi-gpt:latest&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;/details&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：我 Clone 了这个仓库，但是本地启动失败&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;如果你是通过 clone 本项目仓库的方式来运行，记得在 &lt;code&gt;start&lt;/code&gt; 之前先 &lt;code&gt;build&lt;/code&gt; 一下。&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;pnpm install &amp;amp;&amp;amp; pnpm build &amp;amp;&amp;amp; pnpm start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;另外， &lt;code&gt;start&lt;/code&gt; 命令默认没有注入 &lt;code&gt;.env&lt;/code&gt; 文件里的环境变量。你可以在 VS Code 里按 F5 直接运行，会自动读取 &lt;code&gt;.env&lt;/code&gt; ，或者将启动脚本改为：&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;node --env-file=.env app.js&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：怎样使用豆包的音色&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;此功能需要豆包 TTS 接口支持，本项目暂不对外提供此服务。&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Q：我还有其他问题&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;p&gt;请在此处提交 &lt;a href=&#34;https://github.com/idootop/mi-gpt/issues&#34;&gt;issue&lt;/a&gt; 反馈，并提供详细的问题描述和相关错误截图。&lt;/p&gt; &#xA;&lt;h2&gt;🚨 免责声明&lt;/h2&gt; &#xA;&lt;p&gt;本项目仅供学习和研究目的，不得用于任何商业活动。用户在使用本项目时应遵守所在地区的法律法规，对于违法使用所导致的后果，本项目及作者不承担任何责任。 本项目可能存在未知的缺陷和风险（包括但不限于设备损坏和账号封禁等），使用者应自行承担使用本项目所产生的所有风险及责任。 作者不保证本项目的准确性、完整性、及时性、可靠性，也不承担任何因使用本项目而产生的任何损失或损害责任。 使用本项目即表示您已阅读并同意本免责声明的全部内容。&lt;/p&gt; &#xA;&lt;h2&gt;❤️ 鸣谢&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/yihong0618/xiaogpt&#34;&gt;https://github.com/yihong0618/xiaogpt&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/inu1255/mi-service&#34;&gt;https://github.com/inu1255/mi-service&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/Yonsm/MiService&#34;&gt;https://github.com/Yonsm/MiService&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
  <entry>
    <title>hackerb9/lsix</title>
    <updated>2024-06-09T01:28:57Z</updated>
    <id>tag:github.com,2024-06-09:/hackerb9/lsix</id>
    <link href="https://github.com/hackerb9/lsix" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Like &#34;ls&#34;, but for images. Shows thumbnails in terminal using sixel graphics.&lt;/p&gt;&lt;hr&gt;&lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/hackerb9/lsix/master/README.md.d/thumb.png&#34;&gt; &#xA;&lt;h1&gt;lsix&lt;/h1&gt; &#xA;&lt;p&gt;Like &#34;ls&#34;, but for images. Shows thumbnails in terminal using &lt;a href=&#34;https://en.wikipedia.org/wiki/Sixel&#34;&gt;sixel&lt;/a&gt; graphics.&lt;/p&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;lsix [ FILES ... ]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Examples&lt;/h2&gt; &#xA;&lt;h3&gt;Basic Usage&lt;/h3&gt; &#xA;&lt;p&gt;Just typing &lt;code&gt;lsix&lt;/code&gt; will show images in the current working directory. You can also specify filenames and, of course, use shell wild cards (e.g., &lt;code&gt;lsix *jpg *png&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;Because lsix uses ImageMagick pretty much any image format will be supported. However, some may be slow to render (like PDF), so lsix doesn&#39;t show them unless you ask specifically. If you want to force a listing of a certain type of image simply specify the filenames or use a wildcard (&lt;code&gt;*.pdf&lt;/code&gt; in the example below),.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hackerb9/lsix/master/README.md.d/example1.png&#34; alt=&#34;Example 1 of lsix usage&#34; title=&#34;Most basic usage&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Expanding GIFs&lt;/h3&gt; &#xA;&lt;p&gt;If you specify a GIF (or actually any file that has multiple images in it) on the command line, all the frames will get expanded and shown in a montage. For example, &lt;code&gt;lsix nyancat.gif&lt;/code&gt; shows all the frames. Note that GIF stores some frames as only the pixels that differ from the previous frame. &lt;img src=&#34;https://raw.githubusercontent.com/hackerb9/lsix/master/README.md.d/example2.png&#34; alt=&#34;Example 2 of lsix usage&#34; title=&#34;GIFs get expanded&#34;&gt;&lt;/p&gt; &#xA;&lt;h3&gt;Terminal background color is detected&lt;/h3&gt; &#xA;&lt;p&gt;You may have noticed that PNGs and SVG files have correct alpha channel for the terminal background. That is because lsix uses terminal escape sequences to try to figure out your foreground and background colors. (Foreground is used for the text fill color.)&lt;/p&gt; &#xA;&lt;p&gt;In the first example below, after running &lt;code&gt;lsix&lt;/code&gt; in a white on black xterm, I sent an escape sequence to swap foreground and background colors. When I ran it again, &lt;code&gt;lsix&lt;/code&gt; detected it and changed the background color to white. Of course, you can pick whatever default colors you want (e.g., &lt;code&gt;xterm -bg blue&lt;/code&gt;, in the second example below).&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hackerb9/lsix/master/README.md.d/example3.png&#34; alt=&#34;Example 3 of lsix usage&#34; title=&#34;Reverse video works&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/hackerb9/lsix/master/README.md.d/example4.png&#34; alt=&#34;Example 4 of lsix usage&#34; title=&#34;Even &#39;xterm -bg blue&#39; works&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Detects if your terminal can display SIXEL graphics inline using &lt;a href=&#34;https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h2-Sixel-Graphics&#34;&gt;control sequences&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Works great over ssh. Perfect for manipulating those images on the web server when you can&#39;t quite remember what each one was.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Non-bitmap graphics often work fine (.svg, .eps, .pdf, .xcf).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Automatically detects if your terminal, like xterm, can increase the number of color registers to improve the image quality and does so.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Automatically detects terminal&#39;s foreground and background colors.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;In terminals that support dtterm WindowOps, the number of tiles per row will adjust appropriately to the window width.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If there are many images in a directory (&amp;gt;21), lsix will display them one row at a time so you don&#39;t need to wait for the entire montage to be created.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If your filenames are too long, lsix will wrap the text before passing it into ImageMagick&#39;s &lt;code&gt;montage&lt;/code&gt;. (Without lsix, &lt;code&gt;montage&lt;/code&gt; just jumbles long filenames on top of one another.)&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;You can easily change things like the width of each tile in the montage, the font family, and point size by editing simple variables at the top of the file. &lt;em&gt;(Tip: try &lt;code&gt;convert -list font&lt;/code&gt; to see what fonts you have on your machine.)&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Unicode filenames work fine, as long as your font has the glyphs.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Just put the &lt;a href=&#34;https://raw.githubusercontent.com/hackerb9/lsix/master/lsix&#34;&gt;&lt;code&gt;lsix&lt;/code&gt;&lt;/a&gt; file in your path (e.g., /usr/local/bin) and run it. It&#39;s just a BASH shell script.&lt;/p&gt; &#xA;&lt;p&gt;The only prerequisite software is ImageMagick. If you don&#39;t have it yet, your OS&#39;s package manager will make it easy to get. (E.g., &lt;code&gt;apt-get install imagemagick&lt;/code&gt;).&lt;/p&gt; &#xA;&lt;p&gt;MacOS users may prefer to install lsix using &lt;code&gt;brew install lsix&lt;/code&gt; which installs ImageMagick, if necessary.&lt;/p&gt; &#xA;&lt;h2&gt;Your Terminal must support Sixel graphics&lt;/h2&gt; &#xA;&lt;p&gt;I developed this using &lt;a href=&#34;https://invisible-island.net/xterm/&#34;&gt;xterm&lt;/a&gt; in vt340 emulation mode, but I believe this should work on any Sixel compatible terminal. You may test your terminal by viewing a single image, like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;convert  foo.jpg  -geometry 800x480  sixel:- &#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;XTerm&lt;/h3&gt; &#xA;&lt;p&gt;Note that xterm does not have Sixel mode enabled by default, so you need to either run it like so:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;xterm -ti vt340&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or, make vt340 the default terminal type for xterm. Add the following to your &lt;code&gt;.Xresources&lt;/code&gt; file and run &lt;code&gt;xrdb -merge .Xresources&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;! Allow sixel graphics. (Try: &#34;convert -colors 16 foo.jpg sixel:-&#34;).&#xA;xterm*decTerminalID&#x9;:&#x9;vt340&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Further, some distributions, such as Fedora, appear to not compile &lt;code&gt;xterm&lt;/code&gt; with sixel support. In that case, try an alternate terminal, such as &lt;code&gt;foot&lt;/code&gt; or &lt;code&gt;mlterm&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;SIXEL compatible terminals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;XTerm (tested)&lt;/li&gt; &#xA; &lt;li&gt;MLterm (tested)&lt;/li&gt; &#xA; &lt;li&gt;foot (tested)&lt;/li&gt; &#xA; &lt;li&gt;Wezterm (tested)&lt;/li&gt; &#xA; &lt;li&gt;Contour (tested)&lt;/li&gt; &#xA; &lt;li&gt;iTerm2 for Apple MacOS (tested)&lt;/li&gt; &#xA; &lt;li&gt;Konsole (reported)&lt;/li&gt; &#xA; &lt;li&gt;yakuake (reported)&lt;/li&gt; &#xA; &lt;li&gt;WSLtty for Microsoft Windows (reported)&lt;/li&gt; &#xA; &lt;li&gt;MinTTY for Cygwin (Microsoft Windows) (reported)&lt;/li&gt; &#xA; &lt;li&gt;Yaft for Linux framebuffer (tested)&lt;/li&gt; &#xA; &lt;li&gt;VTE (special compilation, reported)&lt;/li&gt; &#xA; &lt;li&gt;sixel-tmux (fork of tmux, reported)&lt;/li&gt; &#xA; &lt;li&gt;ttyd (reported)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SIXEL incompatible terminals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;MacOS Terminal, kitty&lt;/li&gt; &#xA; &lt;li&gt;All standard libvte based terminals &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;gnome-terminal&lt;/li&gt; &#xA;   &lt;li&gt;terminator&lt;/li&gt; &#xA;   &lt;li&gt;lxterm&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Alacritty (might work with &lt;a href=&#34;https://github.com/alacritty/alacritty/pull/4763&#34;&gt;a patch&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Configuration&lt;/h2&gt; &#xA;&lt;p&gt;Because &lt;code&gt;lsix&lt;/code&gt; is currently designed to be very simple, there are no command line flags, no configuration files, no knobs to twiddle, or frobs to frobnosticate. However, since the script is so simple, if you want to make a change, it&#39;s pretty easy to do just by editing the file. Everything is nicely commented with the most common default variables at the top.&lt;/p&gt; &#xA;&lt;h2&gt;Contact the author&lt;/h2&gt; &#xA;&lt;p&gt;I welcome feedback. If you use lsix and like it or have suggestions for how it can be improved, please go ahead and send your thoughts to me &lt;a href=&#34;https://github.com/hackerb9/lsix/issues/new&#34;&gt;@hackerb9&lt;/a&gt; via GitHub.&lt;/p&gt; &#xA;&lt;h2&gt;Bugs&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;XTerm&#39;s reverse video mode (&lt;code&gt;xterm -rv&lt;/code&gt;) is different from specifying the foreground and background explicitly. There is a way to detect the latter, but not the former. That means the background color will be incorrect for folks who use XTerm&#39;s reverseVideo resource. (See issue #20).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;XTerm&#39;s screen width is currently limited to 1000px due to a misfeature which causes it to silently show nothing. This limitation will be removed once xterm can handle images greater than 1000x1000. [Last tested with XTerm(344)].&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Filenames that begin with &#34;@&#34; are special to ImageMagick and it&#39;ll freak out if you don&#39;t prepend a directory. (&lt;code&gt;lsix ./@foo.png&lt;/code&gt;) (This is a bug in ImageMagick, not lsix).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Specifying the empty string &lt;code&gt;&#34;&#34;&lt;/code&gt; as a filename makes ImageMagick hang. (This appears to be an ImageMagick bug / misfeature).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Long filenames are wrapped, but not intelligently. Would it complicate this script too much to make it prefer to wrap on whites space, dashes, underscores, and periods? Maybe.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Directories specified on the command line are processed as if the user had cd&#39;d to that directory. It wouldn&#39;t be hard to implement recursion, but is there actually a need? I&#39;m reluctant to complicate such a simple script with command line flags.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;If you run &lt;code&gt;lsix foo.avi&lt;/code&gt;, you&#39;re asking for trouble.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Future Issues&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;The Sixel standard doesn&#39;t appear to have a way to query the size of the graphics screen. Reading the VT340 documentation, it appears your program has to already know the resolution of the device you&#39;re rendering on.&lt;/p&gt; &lt;p&gt;XTerm, as of version 344, has added &lt;a href=&#34;https://invisible-island.net/xterm/ctlseqs/ctlseqs.html#h2-Functions-using-CSI-_-ordered-by-the-final-character_s_&#34;&gt;a control sequence&lt;/a&gt; that solves the problem — &lt;code&gt;CSI ? Pi ; Pa ; Pv S&lt;/code&gt; — but some terminals, for example &lt;code&gt;mlterm&lt;/code&gt;, haven&#39;t yet implemented it.&lt;/p&gt; &lt;p&gt;There is an alternate way to read the window size using the dtterm WindowOps extension but it is not quite the right solution as the geometry of the Sixel graphics screen is not necessarily the same as the window size. (For example, xterm limits the graphics geometry to 1000x1000, even though the window can actually be larger.) To help with terminals such as mlterm, &lt;code&gt;lsix&lt;/code&gt; will use the dtterm WindowOps as a fallback.&lt;/p&gt; &lt;p&gt;If neither solution works, &lt;code&gt;lsix&lt;/code&gt; will assume you are on a VT340 (800x480) and can fit only 6 tiles per row.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The Sixel standard also lacks a way to query the number of color registers available. I used the extensions from &lt;code&gt;xterm&lt;/code&gt; to do so, but I do not know how widely implemented they are. If a terminal does not respond, &lt;code&gt;lsix&lt;/code&gt; presumes you&#39;re on an original vt340 and uses only 16 color registers. (Sorry, 4-gray vt330 users! Time to upgrade. ;-) )&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;The &lt;a href=&#34;https://kermitproject.org/&#34;&gt;Kermit project&lt;/a&gt; created a MS-DOS terminal emulator that was popular in the late 1980s/early 1990s. Its sixel implementation is not compatible with lsix because it shows the graphics on a screen separate from the text. However, I noticed one feature in its documentation: an escape sequence to request the current graphics window size and number of colors:&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;pre&gt;&lt;code&gt; ESC [ ? 256 n                  Request screen size report&#xA;&#xA;        Report is ESC [ ? 256; Ph; Pw; Pc n     for graphics systems&#xA;&#xA;        where   Ph is screen height in dots&#xA;                Pw is screen width in dots&#xA;                Pc is number of colors (0, 1 or 16, for none, b/w, ega/vga)&#xA;&#xA;        Report is ESC [ ? 24; 80; 0 n  for pure text mono systems.&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Did any other terminal emulators ever use the sequence? Would it be worthwhile to add to &lt;code&gt;lsix&lt;/code&gt;?&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/saitoha/libsixel&#34;&gt;libsixel&lt;/a&gt; is an excellent project for writing programs that can output optimized Sixel graphics commands. Because I have a lot of respect for the project, I feel I should explain why &lt;code&gt;lsix&lt;/code&gt; does not use libsixel.&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;(a) I wanted lsix to work everywhere easily. Bash and imagemagick are ubiquitous, so a shell script is a natural solution.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;(b) I wanted &lt;code&gt;lsix&lt;/code&gt; to be simple enough that it could be easily customized and extended by other people. (Including myself.)&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;(c) ImageMagick has better support for reading different formats than stb_image (the library used by libsixel&#39;s &lt;code&gt;img2sixel&lt;/code&gt;). (For example: xpm, svg, 16-bit png, and even sixel files are not recognized by img2sixel). Since ImageMagick can read all of those and write sixel output directly, it made sense to use it for both.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;(d) While libsixel is optimized and would surely be faster than ImageMagick, it&#39;s overkill. For a simple directory listing, this is plenty fast enough.&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Resources&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://invisible-island.net/xterm/ctlseqs/ctlseqs.html&#34;&gt;XTerm Control Sequences&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://imagemagick.org/&#34;&gt;ImageMagick&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://vt100.net/docs/vt3xx-gp/&#34;&gt;VT340 Programmer&#39;s Reference&lt;/a&gt;:&lt;/p&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://vt100.net/docs/vt3xx-gp/chapter14.html&#34;&gt;Chapter 14&lt;/a&gt;. Sixel Graphics.&lt;/p&gt; &lt;/li&gt; &#xA;   &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://vt100.net/docs/vt3xx-gp/chapter16.html#S16.3&#34;&gt;Chapter 16&lt;/a&gt; Difference between Level 1 and Level 2 Sixel implementations.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Nota bene: this reference has the sense for DECSDM (sixel display mode) reversed! The actual behaviour of the VT340 is that when DECSDM is reset (the default), sixel scrolling is enabled. This can be done by sending &lt;em&gt;&lt;code&gt;Esc[?80l&lt;/code&gt;&lt;/em&gt;, but lsix does not do so as it would break many current terminal emulators. See issue #41 for details.&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://archive.org/details/bitsavers_decstandar0VideoSystemsReferenceManualDec91_74264381&#34;&gt;DEC STD 070 Video Systems Reference Manual&lt;/a&gt;. A weighty tome which covers nearly everything in exacting detail. I referred mostly to sections 4 (escape sequences) and 9 (sixel programming).&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;https://github.com/hackerb9/vt340test&#34;&gt;VT340 Test&lt;/a&gt;, a project to document the actual behaviour of the DEC VT340 hardware.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;a href=&#34;http://www.vaxhaven.com/images/f/f7/EK-PPLV2-PM-B01.pdf&#34;&gt;Digital ANSI-Compliant Printing Protocol: Level 2 Programming Reference Manual&lt;/a&gt;, Chapter 5: Sixel Graphics. An excellent and reasonably clear discussion for anyone who wants to generate or parse sixel graphics.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt;</summary>
  </entry>
</feed>