<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-05-05T01:28:07Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>dnhkng/GlaDOS</title>
    <updated>2024-05-05T01:28:07Z</updated>
    <id>tag:github.com,2024-05-05:/dnhkng/GlaDOS</id>
    <link href="https://github.com/dnhkng/GlaDOS" rel="alternate"></link>
    <summary type="html">&lt;p&gt;This is the Personality Core for GLaDOS, the first steps towards a real-life implementation of the AI from the Portal series by Valve.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;GLaDOS Personality Core&lt;/h1&gt; &#xA;&lt;p&gt;This is a project dedicated to building a real-life version of GLaDOS.&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=KbUfWpykBGg&#34;&gt;&lt;img src=&#34;https://img.youtube.com/vi/KbUfWpykBGg/0.jpg&#34; alt=&#34;localGLaDOS&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;em&gt;This is a hardware and software project that will create an aware, interactive, and embodied GLaDOS.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;This will entail:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Train GLaDOS voice generator&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Generate a prompt that leads to a realistic &#34;Personality Core&#34;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Generate a &lt;a href=&#34;https://memgpt.readthedocs.io/en/latest/&#34;&gt;MemGPT&lt;/a&gt; medium- and long-term memory for GLaDOS&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Give GLaDOS vision via &lt;a href=&#34;https://llava-vl.github.io/&#34;&gt;LLaVA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Create 3D-printable parts&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Design the animatronics system&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Software Architecture&lt;/h2&gt; &#xA;&lt;p&gt;The initial goals are to develop a low-latency platform, where GLaDOS can respond to voice interactions within 600ms.&lt;/p&gt; &#xA;&lt;p&gt;To do this, the system constantly records data to a circular buffer, waiting for &lt;a href=&#34;https://github.com/snakers4/silero-vad&#34;&gt;voice to be detected&lt;/a&gt;. When it&#39;s determined that the voice has stopped (including detection of normal pauses), it will be &lt;a href=&#34;https://github.com/huggingface/distil-whisper&#34;&gt;transcribed quickly&lt;/a&gt;. This is then passed to streaming &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;local Large Language Model&lt;/a&gt;, where the streamed text is broken by sentence, and passed to a &lt;a href=&#34;https://github.com/rhasspy/piper&#34;&gt;text-to-speech system&lt;/a&gt;. This means further sentences can be generated while the current is playing, reducing latency substantially.&lt;/p&gt; &#xA;&lt;h3&gt;Subgoals&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;The other aim of the project is to minimize dependencies, so this can run on constrained hardware. That means no PyTorch or other large packages.&lt;/li&gt; &#xA; &lt;li&gt;As I want to fully understand the system, I have removed a large amount of redirection: which means extracting and rewriting code. i.e. as GLaDOS only speaks English, I have rewritten the wrapper around &lt;a href=&#34;https://espeak.sourceforge.net/&#34;&gt;espeak&lt;/a&gt; and the entire Text-to-Speech subsystem is about 500 LOC and has only 3 dependencies: numpy, onnxruntime, and sounddevice.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Hardware System&lt;/h2&gt; &#xA;&lt;p&gt;This will be based on servo- and stepper-motors. 3D printable STL will be provided to create GlaDOS&#39;s body, and she will be given a set of animations to express herself. The vision system will allow her to track and turn toward people and things of interest.&lt;/p&gt; &#xA;&lt;h2&gt;Installation Instruction&lt;/h2&gt; &#xA;&lt;p&gt;If you want to install the TTS Engine on your machine, please follow the steps below. This has only been tested on Linux, but I think it will work on Windows with small tweaks.&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the &lt;a href=&#34;https://github.com/espeak-ng/espeak-ng&#34;&gt;&lt;code&gt;espeak&lt;/code&gt;&lt;/a&gt; synthesizer according to the &lt;a href=&#34;https://github.com/espeak-ng/espeak-ng/raw/master/docs/guide.md&#34;&gt;installation instructions&lt;/a&gt; for your operating system.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Install the required Python packages, e.g., by running &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For the LLM, install &lt;a href=&#34;https://github.com/ggerganov/llama.cpp&#34;&gt;Llama.cpp&lt;/a&gt;, and compile it for your CPU or GPU. Edit the LLAMA_SERVER_PATH parameter in glados.py to match your installation path.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;For voice recognition, install &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp&#34;&gt;Whisper.cpp&lt;/a&gt;, and after compiling, run &lt;code&gt;make libwhisper.so&lt;/code&gt; and then move the &#34;libwhisper.so&#34; file to the &#34;glados&#34; folder or add it to your path. For Windows, check out the discussion in my &lt;a href=&#34;https://github.com/ggerganov/whisper.cpp/pull/1524&#34;&gt;whisper pull request&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Download the models:&lt;/p&gt; &#xA;  &lt;ol&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/distil-whisper/distil-medium.en/resolve/main/ggml-medium-32-2.en.bin?download=true&#34;&gt;voice recognition model&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-IQ3_XS.gguf?download=true&#34;&gt;Llama-3 8B&lt;/a&gt; or&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://huggingface.co/MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF/resolve/main/Meta-Llama-3-70B-Instruct.IQ4_XS.gguf?download=true&#34;&gt;Llama-3 70B&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ol&gt; &lt;p&gt;and put them in the &#34;models&#34; directory.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Testing&lt;/h2&gt; &#xA;&lt;p&gt;You can test the systems by exploring the &#39;demo.ipynb&#39;.&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#dnhkng/GlaDOS&amp;amp;Date&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=dnhkng/GlaDOS&amp;amp;type=Date&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://trendshift.io/repositories/9828&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://trendshift.io/api/badge/repositories/9828&#34; alt=&#34;dnhkng%2FGlaDOS | Trendshift&#34; style=&#34;width: 250px; height: 55px;&#34; width=&#34;250&#34; height=&#34;55&#34;&gt;&lt;/a&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>IvanGlinkin/CCTV</title>
    <updated>2024-05-05T01:28:07Z</updated>
    <id>tag:github.com,2024-05-05:/IvanGlinkin/CCTV</id>
    <link href="https://github.com/IvanGlinkin/CCTV" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Close-Circuit Telegram Vision revolutionizes location tracking with its open-source design and Telegram API integration. Offering precise tracking within 50-100 meters, users can monitor others in real-time for logistics or safety, redefining how we navigate our surroundings&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;CCTV&lt;/h1&gt; &#xA;&lt;p&gt;Close-Circuit Telegram Vision revolutionizes location tracking with its open-source design and Telegram API integration. Offering precise tracking within 50-100 meters, users can monitor others in real-time for logistics or safety, redefining how we navigate our surroundings&lt;/p&gt; &#xA;&lt;h2&gt;Usage example:&lt;/h2&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Installation&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/IvanGlinkin/CCTV.git&#xA;cd CCTV&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;2&#34;&gt; &#xA; &lt;li&gt;Registering Telegram creds&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;visit https://my.telegram.org/auth web-site&#xA;input your phone number&#xA;input the confirmation/login code&#xA;follow &#34;API development tools&#34; link&#xA;register the application&#xA;get App&#39;s api_id, api_hash, title and name&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;3&#34;&gt; &#xA; &lt;li&gt;Setting up Telegram creds&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;nano ./backend/telegram_creds.py&#xA;  phone_number = &#39;your_phone_number&#39; // put your phone number &#xA;  telegram_name = &#39;api_name&#39; // put your API name from the step 2&#xA;  telegram_api_id = &#39;api_id&#39; // put your API ID from the step 2&#xA;  telegram_api_hash = &#39;api_hash&#39; // put your API Hash from the step 2&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;4&#34;&gt; &#xA; &lt;li&gt;Launch&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;pre&gt;&lt;code&gt;nano ./backend/general_settings.py // set the location and radius&#xA;python3 start.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;ol start=&#34;5&#34;&gt; &#xA; &lt;li&gt;Read the data by opening ./reports-html/_combined_data.html&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h2&gt;Help message:&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code&gt;&#xA; ██████╗██╗      ██████╗ ███████╗███████╗     ██████╗██╗██████╗  ██████╗██╗   ██╗██╗████████╗                      &#xA;██╔════╝██║     ██╔═══██╗██╔════╝██╔════╝    ██╔════╝██║██╔══██╗██╔════╝██║   ██║██║╚══██╔══╝                      &#xA;██║     ██║     ██║   ██║███████╗█████╗█████╗██║     ██║██████╔╝██║     ██║   ██║██║   ██║                         &#xA;██║     ██║     ██║   ██║╚════██║██╔══╝╚════╝██║     ██║██╔══██╗██║     ██║   ██║██║   ██║                         &#xA;╚██████╗███████╗╚██████╔╝███████║███████╗    ╚██████╗██║██║  ██║╚██████╗╚██████╔╝██║   ██║                         &#xA; ╚═════╝╚══════╝ ╚═════╝ ╚══════╝╚══════╝     ╚═════╝╚═╝╚═╝  ╚═╝ ╚═════╝ ╚═════╝ ╚═╝   ╚═╝                         &#xA;                                                                                                                   &#xA;████████╗███████╗██╗     ███████╗ ██████╗ ██████╗  █████╗ ███╗   ███╗    ██╗   ██╗██╗███████╗██╗ ██████╗ ███╗   ██╗&#xA;╚══██╔══╝██╔════╝██║     ██╔════╝██╔════╝ ██╔══██╗██╔══██╗████╗ ████║    ██║   ██║██║██╔════╝██║██╔═══██╗████╗  ██║&#xA;   ██║   █████╗  ██║     █████╗  ██║  ███╗██████╔╝███████║██╔████╔██║    ██║   ██║██║███████╗██║██║   ██║██╔██╗ ██║&#xA;   ██║   ██╔══╝  ██║     ██╔══╝  ██║   ██║██╔══██╗██╔══██║██║╚██╔╝██║    ╚██╗ ██╔╝██║╚════██║██║██║   ██║██║╚██╗██║&#xA;   ██║   ███████╗███████╗███████╗╚██████╔╝██║  ██║██║  ██║██║ ╚═╝ ██║     ╚████╔╝ ██║███████║██║╚██████╔╝██║ ╚████║&#xA;   ╚═╝   ╚══════╝╚══════╝╚══════╝ ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝     ╚═╝      ╚═══╝  ╚═╝╚══════╝╚═╝ ╚═════╝ ╚═╝  ╚═══╝&#xA;&#xA;usage: start.py [-h] [-lat LATITUDE] [-long LONGITUDE] [-m METERS] [-t TIMESLEEP] [-tn TELEGRAM_NAME]&#xA;                [-ti TELEGRAM_API_ID] [-th TELEGRAM_API_HASH]&#xA;&#xA;Custom settings for script launch&#xA;&#xA;optional arguments:&#xA;  -h, --help            show this help message and exit&#xA;  -lat LATITUDE, --latitude LATITUDE&#xA;                        Latitude setting&#xA;  -long LONGITUDE, --longitude LONGITUDE&#xA;                        Longitude setting&#xA;  -m METERS, --meters METERS&#xA;                        Meters setting&#xA;  -t TIMESLEEP, --timesleep TIMESLEEP&#xA;                        Timesleep setting&#xA;  -tn TELEGRAM_NAME, --telegram_name TELEGRAM_NAME&#xA;                        Telegram username&#xA;  -ti TELEGRAM_API_ID, --telegram_api_id TELEGRAM_API_ID&#xA;                        Telegram API ID&#xA;  -th TELEGRAM_API_HASH, --telegram_api_hash TELEGRAM_API_HASH&#xA;                        Telegram API hash&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Video example:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=y9jEiZS5pAc&#34; title=&#34;Close-Curcuit Telegram Vision PoC&#34;&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IvanGlinkin/media_support/main/CCTV_youtube.png&#34; alt=&#34;Close-Curcuit Telegram Vision PoC&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Screenshots:&lt;/h2&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/IvanGlinkin/media_support/main/CCTV1.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/IvanGlinkin/media_support/main/CCTV2.png&#34; alt=&#34;&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/IvanGlinkin/media_support/main/CCTV4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>verlab/accelerated_features</title>
    <updated>2024-05-05T01:28:07Z</updated>
    <id>tag:github.com,2024-05-05:/verlab/accelerated_features</id>
    <link href="https://github.com/verlab/accelerated_features" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Do you need robust and fast local feature extraction? You are in the right place!&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;XFeat: Accelerated Features for Lightweight Image Matching&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://guipotje.github.io/&#34;&gt;Guilherme Potje&lt;/a&gt; · &lt;a href=&#34;https://eucadar.com/&#34;&gt;Felipe Cadar&lt;/a&gt; · &lt;a href=&#34;https://andrefaraujo.github.io/&#34;&gt;Andre Araujo&lt;/a&gt; · &lt;a href=&#34;https://renatojmsdh.github.io/&#34;&gt;Renato Martins&lt;/a&gt; · &lt;a href=&#34;https://homepages.dcc.ufmg.br/~erickson/&#34;&gt;Erickson R. Nascimento&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://colab.research.google.com/github/verlab/accelerated_features/blob/main/notebooks/xfeat_matching.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h3&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.19174&#34;&gt;[ArXiv]&lt;/a&gt; | &lt;a href=&#34;https://www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24/&#34;&gt;[Project Page]&lt;/a&gt; | &lt;a href=&#34;https://cvpr.thecvf.com/&#34;&gt;[CVPR&#39;24 Paper]&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;div align=&#34;center&#34; style=&#34;display: flex; justify-content: center; align-items: center; flex-direction: column;&#34;&gt; &#xA; &lt;div style=&#34;display: flex; justify-content: space-around; width: 100%;&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/figs/xfeat.gif&#34; width=&#34;400&#34;&gt; &#xA;  &lt;img src=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/figs/sift.gif&#34; width=&#34;400&#34;&gt; &#xA; &lt;/div&gt; &#xA; &lt;p&gt;Real-time XFeat demonstration (left) compared to SIFT (right) on a textureless scene. SIFT cannot handle fast camera movements, while XFeat provides robust matches under adverse conditions, while being faster than SIFT on CPU.&lt;/p&gt; &#xA;&lt;/div&gt; &#xA;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Really fast learned keypoint detector and descriptor. Supports sparse and semi-dense matching.&lt;/p&gt; &#xA;&lt;p&gt;Just wanna quickly try on your images? Check this out: &lt;a href=&#34;https://colab.research.google.com/github/verlab/accelerated_features/blob/main/notebooks/xfeat_torch_hub.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#introduction&#34;&gt;Introduction&lt;/a&gt; &lt;img align=&#34;right&#34; src=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/figs/xfeat_quali.jpg&#34; width=&#34;360&#34;&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#installation&#34;&gt;Installation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#usage&#34;&gt;Usage&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#inference&#34;&gt;Inference&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#training&#34;&gt;Training&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#evaluation&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#real-time-demo&#34;&gt;Real-time demo app&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#contributing&#34;&gt;Contribute&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#citation&#34;&gt;Citation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#license&#34;&gt;License&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Introduction&lt;/h2&gt; &#xA;&lt;p&gt;This repository contains the official implementation of the paper: &lt;em&gt;&lt;a href=&#34;https://arxiv.org/abs/2404.19174&#34;&gt;XFeat: Accelerated Features for Lightweight Image Matching&lt;/a&gt;&lt;/em&gt;, to be presented at CVPR 2024.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Motivation.&lt;/strong&gt; Why another keypoint detector and descriptor among dozens of existing ones? We noticed that the current trend in the literature focuses on accuracy but often neglects compute efficiency, especially when deploying these solutions in the real-world. For applications in mobile robotics and augmented reality, it is critical that models can run on hardware-constrained computers. To this end, XFeat was designed as an agnostic solution focusing on both accuracy and efficiency in an image matching pipeline.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Capabilities.&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Real-time sparse inference on CPU for VGA images (tested on laptop with an i5 CPU and vanilla pytorch);&lt;/li&gt; &#xA; &lt;li&gt;Simple architecture components which facilitates deployment on embedded devices (jetson, raspberry pi, custom AI chips, etc..);&lt;/li&gt; &#xA; &lt;li&gt;Supports both sparse and semi-dense matching of local features;&lt;/li&gt; &#xA; &lt;li&gt;Compact descriptors (64D);&lt;/li&gt; &#xA; &lt;li&gt;Performance comparable to known deep local features such as SuperPoint while being significantly faster and more lightweight. Also, XFeat exhibits much better robustness to viewpoint and illumination changes than classic local features as ORB and SIFT;&lt;/li&gt; &#xA; &lt;li&gt;Supports batched inference if you want ridiculously fast feature extraction. On VGA sparse setting, we achieved about 1,400 FPS using an RTX 4090.&lt;/li&gt; &#xA; &lt;li&gt;For single batch inference on GPU (VGA), one can easily achieve over 150 FPS while leaving lots of room on the GPU for other concurrent tasks.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;Paper Abstract.&lt;/strong&gt; We introduce a lightweight and accurate architecture for resource-efficient visual correspondence. Our method, dubbed XFeat (Accelerated Features), revisits fundamental design choices in convolutional neural networks for detecting, extracting, and matching local features. Our new model satisfies a critical need for fast and robust algorithms suitable to resource-limited devices. In particular, accurate image matching requires sufficiently large image resolutions -- for this reason, we keep the resolution as large as possible while limiting the number of channels in the network. Besides, our model is designed to offer the choice of matching at the sparse or semi-dense levels, each of which may be more suitable for different downstream applications, such as visual navigation and augmented reality. Our model is the first to offer semi-dense matching efficiently, leveraging a novel match refinement module that relies on coarse local descriptors. XFeat is versatile and hardware-independent, surpassing current deep learning-based local features in speed (up to 5x faster) with comparable or better accuracy, proven in pose estimation and visual localization. We showcase it running in real-time on an inexpensive laptop CPU without specialized hardware optimizations.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;Overview of XFeat&#39;s achitecture.&lt;/strong&gt; XFeat extracts a keypoint heatmap $\mathbf{K}$, a compact 64-D dense descriptor map $\mathbf{F}$, and a reliability heatmap $\mathbf{R}$. It achieves unparalleled speed via early downsampling and shallow convolutions, followed by deeper convolutions in later encoders for robustness. Contrary to typical methods, it separates keypoint detection into a distinct branch, using $1 \times 1$ convolutions on an $8 \times 8$ tensor-block-transformed image for fast processing, being one of the few current learned methods that decouples detection &amp;amp; description and can be processed independently.&lt;/p&gt; &#xA;&lt;img align=&#34;center&#34; src=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/figs/xfeat_arq.png&#34; width=&#34;1000&#34;&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;XFeat has minimal dependencies, only relying on torch. Also, XFeat does not need a GPU for real-time sparse inference (vanilla pytorch w/o any special optimization), unless you run it on high-res images. If you want to run the real-time matching demo, you will also need OpenCV. We recommend using conda, but you can use any virtualenv of your choice. If you use conda, just create a new env with:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/verlab/accelerated_features.git&#xA;cd accelerated_features&#xA;&#xA;#Create conda env&#xA;conda create -n xfeat python=3.8&#xA;conda activate xfeat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Then, install &lt;a href=&#34;https://pytorch.org/get-started/previous-versions/&#34;&gt;pytorch (&amp;gt;=1.10)&lt;/a&gt; and then the rest of depencencies in case you want to run the demos:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&#xA;#CPU only, for GPU check in pytorch website the most suitable version to your gpu.&#xA;pip install torch==1.10.1+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html&#xA;# CPU only for MacOS&#xA;# pip install torch==1.10.1 -f https://download.pytorch.org/whl/cpu/torch_stable.html&#xA;&#xA;#Install dependencies for the demo&#xA;pip install opencv-contrib-python tqdm&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;For your convenience, we provide ready to use notebooks for some examples.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt; &#xA;   &lt;th&gt;&lt;strong&gt;Notebook&lt;/strong&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Minimal example&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/verlab/accelerated_features/blob/main/notebooks/minimal_example.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Matching &amp;amp; registration example&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/verlab/accelerated_features/blob/main/notebooks/xfeat_matching.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;Torch hub example&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://colab.research.google.com/github/verlab/accelerated_features/blob/main/notebooks/xfeat_torch_hub.ipynb&#34;&gt;&lt;img src=&#34;https://colab.research.google.com/assets/colab-badge.svg?sanitize=true&#34; alt=&#34;Open In Colab&#34;&gt;&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h3&gt;Inference&lt;/h3&gt; &#xA;&lt;p&gt;To run XFeat on an image, three lines of code is enough:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from modules.xfeat import XFeat&#xA;&#xA;xfeat = XFeat()&#xA;&#xA;#Simple inference with batch sz = 1&#xA;output = xfeat.detectAndCompute(torch.randn(1,3,480,640), top_k = 4096)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or you can use this &lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/minimal_example.py&#34;&gt;script&lt;/a&gt; in the root folder:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 minimal_example.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you already have pytorch, simply use torch hub if you like it:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch&#xA;&#xA;xfeat = torch.hub.load(&#39;verlab/accelerated_features&#39;, &#39;XFeat&#39;, pretrained = True, top_k = 4096)&#xA;&#xA;#Simple inference with batch sz = 1&#xA;output = xfeat.detectAndCompute(torch.randn(1,3,480,640), top_k = 4096)[0]&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Training&lt;/h3&gt; &#xA;&lt;p&gt;XFeat training code will be released soon. Please stay tuned.&lt;/p&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;XFeat evaluation code will be released soon, alongside the training scripts. Please stay tuned.&lt;/p&gt; &#xA;&lt;h2&gt;Real-time Demo&lt;/h2&gt; &#xA;&lt;p&gt;To demonstrate the capabilities of XFeat, we provide a real-time matching demo with Homography registration. Currently, you can experiment with XFeat, ORB and SIFT. You will need a working webcam. To run the demo and show the possible input flags, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 realtime_demo.py -h&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Don&#39;t forget to press &#39;s&#39; to set a desired reference image. Notice that the demo only works correctly for planar scenes and rotation-only motion, because we&#39;re using a homography model.&lt;/p&gt; &#xA;&lt;p&gt;If you want to run the demo with XFeat, please run:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 realtime_demo.py --method XFeat&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or test with SIFT or ORB:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 realtime_demo.py --method SIFT&#xA;python3 realtime_demo.py --method ORB&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Contributions to XFeat are welcome! Currently, it would be nice to have an export script to efficient deployment engines such as TensorRT and ONNX. Also, it would be cool to train a lightweight learned matcher on top of XFeat local features.&lt;/p&gt; &#xA;&lt;h2&gt;Citation&lt;/h2&gt; &#xA;&lt;p&gt;If you find this code useful for your research, please cite the paper:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@INPROCEEDINGS{potje2024cvpr,&#xA;  author={Guilherme {Potje} and and Felipe {Cadar} and Andre {Araujo} and Renato {Martins} and Erickson R. {Nascimento}},&#xA;  booktitle={2024 IEEE / CVF Computer Vision and Pattern Recognition (CVPR)}, &#xA;  title={XFeat: Accelerated Features for Lightweight Image Matching}, &#xA;  year={2024}}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/License-Apache_2.0-blue.svg?sanitize=true&#34; alt=&#34;License&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;We thank the agencies CAPES, CNPq, and Google for funding different parts of this work.&lt;/li&gt; &#xA; &lt;li&gt;We thank the developers of Kornia for the &lt;a href=&#34;https://github.com/kornia/kornia&#34;&gt;kornia library&lt;/a&gt;!&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;VeRLab:&lt;/strong&gt; Laboratory of Computer Vison and Robotics &lt;a href=&#34;https://www.verlab.dcc.ufmg.br&#34;&gt;https://www.verlab.dcc.ufmg.br&lt;/a&gt; &lt;br&gt; &lt;img align=&#34;left&#34; width=&#34;auto&#34; height=&#34;50&#34; src=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/figs/ufmg.png&#34;&gt; &lt;img align=&#34;right&#34; width=&#34;auto&#34; height=&#34;50&#34; src=&#34;https://raw.githubusercontent.com/verlab/accelerated_features/main/figs/verlab.png&#34;&gt; &lt;br&gt;&lt;/p&gt;</summary>
  </entry>
</feed>