<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-12-21T01:25:41Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>open-mmlab/Amphion</title>
    <updated>2023-12-21T01:25:41Z</updated>
    <id>tag:github.com,2023-12-21:/open-mmlab/Amphion</id>
    <link href="https://github.com/open-mmlab/Amphion" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Amphion (/√¶mÀàfa…™…ôn/) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Amphion: An Open-Source Audio, Music, and Speech Generation Toolkit&lt;/h1&gt; &#xA;&lt;div&gt; &#xA; &lt;a href=&#34;https://arxiv.org/abs/2312.09911&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-&lt;COLOR&gt;.svg&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://huggingface.co/amphion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://openxlab.org.cn/usercenter/Amphion&#34;&gt;&lt;img src=&#34;https://cdn-static.openxlab.org.cn/app-center/openxlab_app.svg?sanitize=true&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tts/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-TTS-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/svc/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-SVC-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tta/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-TTA-blue&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/vocoder/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Vocoder-purple&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/metrics/README.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Evaluation-yellow&#34;&gt;&lt;/a&gt; &#xA; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/LICENSE&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/LICENSE-MIT-red&#34;&gt;&lt;/a&gt; &#xA;&lt;/div&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;&lt;strong&gt;Amphion (/√¶mÀàfa…™…ôn/) is a toolkit for Audio, Music, and Speech Generation.&lt;/strong&gt; Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development. Amphion offers a unique feature: &lt;strong&gt;visualizations&lt;/strong&gt; of classic models or architectures. We believe that these visualizations are beneficial for junior researchers and engineers who wish to gain a better understanding of the model.&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;The North-Star objective of Amphion is to offer a platform for studying the conversion of any inputs into audio.&lt;/strong&gt; Amphion is designed to support individual generation tasks, including but not limited to,&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTS&lt;/strong&gt;: Text to Speech (‚õ≥&amp;nbsp;supported)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SVS&lt;/strong&gt;: Singing Voice Synthesis (üë®‚Äçüíª&amp;nbsp;developing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;VC&lt;/strong&gt;: Voice Conversion (üë®‚Äçüíª&amp;nbsp;developing)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;SVC&lt;/strong&gt;: Singing Voice Conversion (‚õ≥&amp;nbsp;supported)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTA&lt;/strong&gt;: Text to Audio (‚õ≥&amp;nbsp;supported)&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;TTM&lt;/strong&gt;: Text to Music (üë®‚Äçüíª&amp;nbsp;developing)&lt;/li&gt; &#xA; &lt;li&gt;more‚Ä¶&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;In addition to the specific generation tasks, Amphion also includes several &lt;strong&gt;vocoders&lt;/strong&gt; and &lt;strong&gt;evaluation metrics&lt;/strong&gt;. A vocoder is an important module for producing high-quality audio signals, while evaluation metrics are critical for ensuring consistent metrics in generation tasks.&lt;/p&gt; &#xA;&lt;p&gt;Here is the Amphion v0.1 demo, whose voice, audio effects, and singing voice are generated by our models. Just enjoy it!&lt;/p&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/Amphion/assets/24860155/7fcdcea5-3d95-4b31-bd93-4b4da734ef9b&#34;&gt;amphion-v0.1-en&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;üöÄ&amp;nbsp;News&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/12/18&lt;/strong&gt;: Amphion v0.1 release. &lt;a href=&#34;https://arxiv.org/abs/2312.09911&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://huggingface.co/amphion&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-Amphion-pink&#34; alt=&#34;hf&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://www.youtube.com/watch?v=1aw0HhcggvQ&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/YouTube-Demo-red&#34; alt=&#34;youtube&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://github.com/open-mmlab/Amphion/pull/39&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;2023/11/28&lt;/strong&gt;: Amphion alpha release. &lt;a href=&#34;https://github.com/open-mmlab/Amphion/pull/2&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Key%20Features-blue&#34; alt=&#34;readme&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;‚≠ê&amp;nbsp;Key Features&lt;/h2&gt; &#xA;&lt;h3&gt;TTS: Text to Speech&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion achieves state-of-the-art performance when compared with existing open-source repositories on text-to-speech (TTS) systems. It supports the following models or architectures: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2006.04558&#34;&gt;FastSpeech2&lt;/a&gt;: A non-autoregressive TTS architecture that utilizes feed-forward Transformer blocks.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.06103&#34;&gt;VITS&lt;/a&gt;: An end-to-end TTS architecture that utilizes conditional variational autoencoder with adversarial learning&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.02111&#34;&gt;Vall-E&lt;/a&gt;: A zero-shot TTS architecture that uses a neural codec language model with discrete codes.&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.09116&#34;&gt;NaturalSpeech2&lt;/a&gt;: An architecture for TTS that utilizes a latent diffusion model to generate natural-sounding voices.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;SVC: Singing Voice Conversion&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Ampion supports multiple content-based features from various pretrained models, including &lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;WeNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt;, and &lt;a href=&#34;https://github.com/auspicious3000/contentvec&#34;&gt;ContentVec&lt;/a&gt;. Their specific roles in SVC has been investigated in our NeurIPS 2023 workshop paper. &lt;a href=&#34;https://arxiv.org/abs/2310.11160&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/svc/MultipleContentsSVC&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Code-red&#34; alt=&#34;code&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Amphion implements several state-of-the-art model architectures, including diffusion-, transformer-, VAE- and flow-based models. The diffusion-based architecture uses &lt;a href=&#34;https://openreview.net/pdf?id=a-xFK8Ymz5J&#34;&gt;Bidirectional dilated CNN&lt;/a&gt; as a backend and supports several sampling algorithms such as &lt;a href=&#34;https://arxiv.org/pdf/2006.11239.pdf&#34;&gt;DDPM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/2010.02502.pdf&#34;&gt;DDIM&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/pdf/2202.09778.pdf&#34;&gt;PNDM&lt;/a&gt;. Additionally, it supports single-step inference based on the &lt;a href=&#34;https://openreview.net/pdf?id=FmqFfMTNnv&#34;&gt;Consistency Model&lt;/a&gt;.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;TTA: Text to Audio&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion supports the TTA with a latent diffusion model. It is designed like &lt;a href=&#34;https://arxiv.org/abs/2301.12503&#34;&gt;AudioLDM&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2301.12661&#34;&gt;Make-an-Audio&lt;/a&gt;, and &lt;a href=&#34;https://arxiv.org/abs/2304.00830&#34;&gt;AUDIT&lt;/a&gt;. It is also the official implementation of the text-to-audio generation part of our NeurIPS 2023 paper. &lt;a href=&#34;https://arxiv.org/abs/2304.00830&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tta/RECIPE.md&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Code-red&#34; alt=&#34;code&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Vocoder&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Amphion supports various widely-used neural vocoders, including: &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;GAN-based vocoders: &lt;a href=&#34;https://arxiv.org/abs/1910.06711&#34;&gt;MelGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2010.05646&#34;&gt;HiFi-GAN&lt;/a&gt;, &lt;a href=&#34;https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts&#34;&gt;NSF-HiFiGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2206.04658&#34;&gt;BigVGAN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/2305.07952&#34;&gt;APNet&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Flow-based vocoders: &lt;a href=&#34;https://arxiv.org/abs/1811.00002&#34;&gt;WaveGlow&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Diffusion-based vocoders: &lt;a href=&#34;https://arxiv.org/abs/2009.09761&#34;&gt;Diffwave&lt;/a&gt;.&lt;/li&gt; &#xA;   &lt;li&gt;Auto-regressive based vocoders: &lt;a href=&#34;https://arxiv.org/abs/1609.03499&#34;&gt;WaveNet&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1802.08435v1&#34;&gt;WaveRNN&lt;/a&gt;.&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;Amphion provides the official implementation of &lt;a href=&#34;https://arxiv.org/abs/2311.14957&#34;&gt;Multi-Scale Constant-Q Transform Discriminator&lt;/a&gt; (our ICASSP 2024 paper). It can be used to enhance any architecture GAN-based vocoders during training, and keep the inference stage (such as memory or speed) unchanged. &lt;a href=&#34;https://arxiv.org/abs/2311.14957&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/arXiv-Paper-%3CCOLOR%3E.svg?sanitize=true&#34; alt=&#34;arXiv&#34;&gt;&lt;/a&gt; &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/vocoder/gan/tfr_enhanced_hifigan&#34;&gt;&lt;img src=&#34;https://img.shields.io/badge/README-Code-red&#34; alt=&#34;code&#34;&gt;&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Evaluation&lt;/h3&gt; &#xA;&lt;p&gt;Amphion provides a comprehensive objective evaluation of the generated audio. The evaluation metrics contain:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;strong&gt;F0 Modeling&lt;/strong&gt;: F0 Pearson Coefficients, F0 Periodicity Root Mean Square Error, F0 Root Mean Square Error, Voiced/Unvoiced F1 Score, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Energy Modeling&lt;/strong&gt;: Energy Root Mean Square Error, Energy Pearson Coefficients, etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Intelligibility&lt;/strong&gt;: Character/Word Error Rate, which can be calculated based on &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt; and more.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Spectrogram Distortion&lt;/strong&gt;: Frechet Audio Distance (FAD), Mel Cepstral Distortion (MCD), Multi-Resolution STFT Distance (MSTFT), Perceptual Evaluation of Speech Quality (PESQ), Short Time Objective Intelligibility (STOI), etc.&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Speaker Similarity&lt;/strong&gt;: Cosine similarity, which can be calculated based on &lt;a href=&#34;https://github.com/Jungjee/RawNet&#34;&gt;RawNet3&lt;/a&gt;, &lt;a href=&#34;https://github.com/wenet-e2e/wespeaker&#34;&gt;WeSpeaker&lt;/a&gt;, and more.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Datasets&lt;/h3&gt; &#xA;&lt;p&gt;Amphion unifies the data preprocess of the open-source datasets including &lt;a href=&#34;https://audiocaps.github.io/&#34;&gt;AudioCaps&lt;/a&gt;, &lt;a href=&#34;https://www.openslr.org/60/&#34;&gt;LibriTTS&lt;/a&gt;, &lt;a href=&#34;https://keithito.com/LJ-Speech-Dataset/&#34;&gt;LJSpeech&lt;/a&gt;, &lt;a href=&#34;https://github.com/M4Singer/M4Singer&#34;&gt;M4Singer&lt;/a&gt;, &lt;a href=&#34;https://wenet.org.cn/opencpop/&#34;&gt;Opencpop&lt;/a&gt;, &lt;a href=&#34;https://github.com/Multi-Singer/Multi-Singer.github.io&#34;&gt;OpenSinger&lt;/a&gt;, &lt;a href=&#34;http://vc-challenge.org/&#34;&gt;SVCC&lt;/a&gt;, &lt;a href=&#34;https://datashare.ed.ac.uk/handle/10283/3443&#34;&gt;VCTK&lt;/a&gt;, and more. The supported dataset list can be seen &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/datasets/README.md&#34;&gt;here&lt;/a&gt; (updating).&lt;/p&gt; &#xA;&lt;h2&gt;üìÄ Installation&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/open-mmlab/Amphion.git&#xA;cd Amphion&#xA;&#xA;# Install Python Environment&#xA;conda create --name amphion python=3.9.15&#xA;conda activate amphion&#xA;&#xA;# Install Python Packages Dependencies&#xA;sh env.sh&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;üêç Usage in Python&lt;/h2&gt; &#xA;&lt;p&gt;We detail the instructions of different tasks in the following recipes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tts/README.md&#34;&gt;Text to Speech (TTS)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/svc/README.md&#34;&gt;Singing Voice Conversion (SVC)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/tta/README.md&#34;&gt;Text to Audio (TTA)&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/vocoder/README.md&#34;&gt;Vocoder&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/egs/metrics/README.md&#34;&gt;Evaluation&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;üôè&amp;nbsp;Acknowledgement&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/ming024/FastSpeech2&#34;&gt;ming024&#39;s FastSpeech2&lt;/a&gt; and &lt;a href=&#34;https://github.com/jaywalnut310/vits&#34;&gt;jaywalnut310&#39;s VITS&lt;/a&gt; for model architecture code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/lifeiteng/vall-e&#34;&gt;lifeiteng&#39;s VALL-E&lt;/a&gt; for training pipeline and model architecture design.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/wenet-e2e/wenet&#34;&gt;WeNet&lt;/a&gt;, &lt;a href=&#34;https://github.com/openai/whisper&#34;&gt;Whisper&lt;/a&gt;, &lt;a href=&#34;https://github.com/auspicious3000/contentvec&#34;&gt;ContentVec&lt;/a&gt;, and &lt;a href=&#34;https://github.com/Jungjee/RawNet&#34;&gt;RawNet3&lt;/a&gt; for pretrained models and inference code.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/jik876/hifi-gan&#34;&gt;HiFi-GAN&lt;/a&gt; for GAN-based Vocoder&#39;s architecture design and training strategy.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/encodec&#34;&gt;Encodec&lt;/a&gt; for well-organized GAN Discriminator&#39;s architecture and basic blocks.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/CompVis/latent-diffusion&#34;&gt;Latent Diffusion&lt;/a&gt; for model architecture design.&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://github.com/TensorSpeech/TensorFlowTTS&#34;&gt;TensorFlowTTS&lt;/a&gt; for preparing the MFA tools.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;¬©Ô∏è&amp;nbsp;License&lt;/h2&gt; &#xA;&lt;p&gt;Amphion is under the &lt;a href=&#34;https://raw.githubusercontent.com/open-mmlab/Amphion/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;. It is free for both research and commercial use cases.&lt;/p&gt; &#xA;&lt;h2&gt;üìö Citations&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{zhang2023amphion,&#xA;      title={Amphion: An Open-Source Audio, Music and Speech Generation Toolkit}, &#xA;      author={Xueyao Zhang and Liumeng Xue and Yuancheng Wang and Yicheng Gu and Xi Chen and Zihao Fang and Haopeng Chen and Lexiao Zou and Chaoren Wang and Jun Han and Kai Chen and Haizhou Li and Zhizheng Wu},&#xA;      journal={arXiv},&#xA;      year={2023},&#xA;      volume={abs/2312.09911}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt;</summary>
  </entry>
  <entry>
    <title>Nukem9/dlssg-to-fsr3</title>
    <updated>2023-12-21T01:25:41Z</updated>
    <id>tag:github.com,2023-12-21:/Nukem9/dlssg-to-fsr3</id>
    <link href="https://github.com/Nukem9/dlssg-to-fsr3" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Adds AMD FSR3 Frame Generation to games by replacing Nvidia DLSS-G Frame Generation (nvngx_dlssg).&lt;/p&gt;&lt;hr&gt;&lt;h2&gt;Builds have been moved to Nexus Mods.&lt;/h2&gt; &#xA;&lt;h3&gt;Download link: &lt;a href=&#34;https://www.nexusmods.com/site/mods/738?tab=files&#34;&gt;https://www.nexusmods.com/site/mods/738?tab=files&lt;/a&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Installation instructions are the exact same. You can keep using video guides. Moving to Nexus Mods is to make my life easier.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>Librum-Reader/Librum</title>
    <updated>2023-12-21T01:25:41Z</updated>
    <id>tag:github.com,2023-12-21:/Librum-Reader/Librum</id>
    <link href="https://github.com/Librum-Reader/Librum" rel="alternate"></link>
    <summary type="html">&lt;p&gt;The Librum client application&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Librum&lt;/h1&gt; &#xA;&lt;p&gt;Librum is an application designed to make reading &lt;b&gt;enjoyable&lt;/b&gt; and &lt;b&gt;straightforward&lt;/b&gt; for everyone.&lt;/p&gt; &#xA;&lt;p&gt;It&#39;s not &lt;strong&gt;just&lt;/strong&gt; an e-book reader. With Librum, you can manage your own online library and access it from any device anytime, anywhere. It has features like note-taking, bookmarking, and highlighting, while offering customization to make it as personal as you want!&lt;/p&gt; &#xA;&lt;p&gt;Librum also provides free access to over 70,000 books and personal reading statistics while being free and completely open source.&lt;/p&gt; &#xA;&lt;h1&gt;Preview&lt;/h1&gt; &#xA;&lt;p&gt;A simple and modern interface&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Librum-Reader/Librum/assets/69865187/bf1d0401-62bd-4f4e-b008-523fb2efd275&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Setup and manage your own library&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Librum-Reader/Librum/assets/69865187/ea94fc68-1bf0-4933-8d80-43a57c6590c5&#34; alt=&#34;HomeScreenDark&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Customize Librum to make it personal to you&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/Librum-Reader/Librum/assets/69865187/b8995cf1-a0e6-4993-8c8b-92f7f8e79ebd&#34; alt=&#34;image&#34;&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;How can I get Librum?&lt;/h1&gt; &#xA;&lt;p&gt;Simply go to &lt;a href=&#34;https://librumreader.com&#34;&gt;https://librumreader.com&lt;/a&gt; to download Librum.&lt;/p&gt; &#xA;&lt;p&gt;If you want to build Librum from source, follow the instructions &lt;a href=&#34;https://raw.githubusercontent.com/Librum-Reader/Librum/main/#build-guide&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Translations&lt;/h1&gt; &#xA;&lt;p&gt;Librum is currently available in:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;English&lt;/li&gt; &#xA; &lt;li&gt;German&lt;/li&gt; &#xA; &lt;li&gt;Russian&lt;/li&gt; &#xA; &lt;li&gt;Mandarin&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;If you want to translate Librum to another language, follow the steps below:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Download the file at: &lt;a href=&#34;https://github.com/Librum-Reader/Librum/raw/main/src/presentation/translations/librum_en.ts&#34;&gt;https://github.com/Librum-Reader/Librum/blob/main/src/presentation/translations/librum_en.ts&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Rename the file to contain your language&#39;s suffix, e.g. &#34;librum_ru.ts&#34; for Russian or &#34;librum_de.ts&#34; for German&lt;/li&gt; &#xA; &lt;li&gt;Download the translation software (Qt Linguist) either for Windows from &lt;a href=&#34;https://github.com/thurask/Qt-Linguist&#34;&gt;https://github.com/thurask/Qt-Linguist&lt;/a&gt; or using the Qt Installer at &lt;a href=&#34;https://www.qt.io/download-open-source&#34;&gt;https://www.qt.io/download-open-source&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Now start Qt Linguist, open the downloaded file, set the target language to the language you want to translate to and start translating. (For a quick guide on Qt Linguist, check out: &lt;a href=&#34;https://youtu.be/xNIz78IPBu0?t=347&#34;&gt;https://youtu.be/xNIz78IPBu0?t=347&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Once you are done, create a pull request or an issue with your new translation file!&lt;br&gt; If you run into any problems, need guidance or have questions, feel free to reach out to us at: &lt;a href=&#34;mailto:contact@librumreader.com&#34;&gt;contact@librumreader.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;p&gt;Notes:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure that your translations are approximately the same length as the original text&lt;/li&gt; &#xA; &lt;li&gt;Make sure that you keep to the punctuation and capitalisation&lt;/li&gt; &#xA; &lt;li&gt;Make sure that your translations make sense in the context they are in&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Documentation&lt;/h1&gt; &#xA;&lt;p&gt;For documentation go to &lt;a href=&#34;https://github.com/Librum-Reader/Librum/wiki&#34;&gt;Librum&#39;s GitHub-wiki&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Donations&lt;/h1&gt; &#xA;&lt;p&gt;Donations make it possible for us to cover our server costs and allow us to make investments into new areas of development. &lt;br&gt; If you would like to support us, check out: &lt;a href=&#34;https://librumreader.com/contribute/donate&#34;&gt;https://librumreader.com/contribute/donate&lt;/a&gt; or become a github sponsor! &lt;br&gt; &lt;br&gt; As a team of opensource developers we rely on donations to continue working on projects like Librum. Your help is greatly appreciated.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Contributing&lt;/h1&gt; &#xA;&lt;p&gt;If you&#39;d like to contribute, check out: &lt;a href=&#34;https://librumreader.com/contribute&#34;&gt;https://librumreader.com/contribute&lt;/a&gt; &lt;br&gt; &lt;br&gt; If you are interested in contributing, feel free to contact us on either:&lt;br&gt;&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Discord (m_david#0631)&lt;/li&gt; &#xA; &lt;li&gt;Email (&lt;a href=&#34;mailto:contact@librumreader.com&#34;&gt;contact@librumreader.com&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; We are following a pull request workflow where every contribution is sent as a pull request and merged into the dev/develop branch for testing. &#xA;&lt;br&gt; Please make sure to run clang format, keep to the conventions used throughout the application and ensure that all tests pass, before submitting any pull request. &#xA;&lt;br&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Contact&lt;/h1&gt; &#xA;&lt;p&gt;For questions, you can reach us under: &lt;a href=&#34;mailto:help@librumreader.com&#34;&gt;help@librumreader.com&lt;/a&gt; &lt;br&gt; For business related contact, reach out to us here: &lt;a href=&#34;mailto:contact@librumreader.com&#34;&gt;contact@librumreader.com&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h1&gt;Details&lt;/h1&gt; &#xA;&lt;h3&gt;Supported platforms&lt;/h3&gt; &#xA;&lt;p&gt;Part of Librum&#39;s aim is to work on &lt;strong&gt;any&lt;/strong&gt; platform. No matter where you are or which device you use, you can always continue your book with Librum, as it is &lt;b&gt;cross platform&lt;/b&gt;.&lt;br&gt; We support:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Windows&lt;/li&gt; &#xA; &lt;li&gt;GNU/Linux&lt;/li&gt; &#xA; &lt;li&gt;MacOS&lt;/li&gt; &#xA; &lt;li&gt;IOS (Coming Soon)&lt;/li&gt; &#xA; &lt;li&gt;Android (Coming Soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Supported formats&lt;/h3&gt; &#xA;&lt;p&gt;Librum is the best choice for all kinds of books, since Librum supports &lt;b&gt;all&lt;/b&gt; major book formats&lt;br&gt; including:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;PDF&lt;/li&gt; &#xA; &lt;li&gt;EPUB&lt;/li&gt; &#xA; &lt;li&gt;CBZ (Comic books)&lt;/li&gt; &#xA; &lt;li&gt;XPS&lt;/li&gt; &#xA; &lt;li&gt;PS&lt;/li&gt; &#xA; &lt;li&gt;All plain text formats&lt;/li&gt; &#xA; &lt;li&gt;Images&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Features&lt;/h3&gt; &#xA;&lt;p&gt;Librum&#39;s objective is to make your reading more &lt;b&gt;productive&lt;/b&gt;; to that end, we provide you with a variety of features that you can access via a &lt;b&gt;simple&lt;/b&gt; and &lt;b&gt;straightforward&lt;/b&gt; interface.&lt;br&gt; These features include:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;A modern e-reader&lt;/li&gt; &#xA; &lt;li&gt;A personalized and customizable library&lt;/li&gt; &#xA; &lt;li&gt;Book meta-data editing&lt;/li&gt; &#xA; &lt;li&gt;A free in-app bookstore with more than 70.000 books&lt;/li&gt; &#xA; &lt;li&gt;Book syncing across all of your devices&lt;/li&gt; &#xA; &lt;li&gt;Highlighting&lt;/li&gt; &#xA; &lt;li&gt;Bookmarking&lt;/li&gt; &#xA; &lt;li&gt;Text search&lt;/li&gt; &#xA; &lt;li&gt;Unlimited customization&lt;/li&gt; &#xA; &lt;li&gt;Note-taking (Coming Soon)&lt;/li&gt; &#xA; &lt;li&gt;TTS (Coming Soon)&lt;/li&gt; &#xA; &lt;li&gt;Personalized reading statistics (Coming Soon)&lt;/li&gt; &#xA; &lt;li&gt;No-login book reading (Coming Soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Want a new feature? Feel free to leave a feature request ticket!&lt;/p&gt; &#xA;&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Build Guide&lt;/h1&gt; &#xA;&lt;p&gt;Follow this guide to build Librum from source. &lt;br&gt;&lt;/p&gt; &#xA;&lt;h2&gt;For GNU/Linux&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmake (&lt;a href=&#34;https://cmake.org/download&#34;&gt;https://cmake.org/download&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;make (&lt;a href=&#34;http://ftp.gnu.org/gnu/make&#34;&gt;http://ftp.gnu.org/gnu/make&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;g++ (&lt;a href=&#34;https://gcc.gnu.org&#34;&gt;https://gcc.gnu.org&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;python3-venv (on ubuntu use &lt;code&gt;sudo apt install python3-venv&lt;/code&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Qt 6.5 (&lt;a href=&#34;https://www.qt.io/download-open-source&#34;&gt;https://www.qt.io/download-open-source&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;The installation is straight forward, just follow the steps below:&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/Librum-Reader/Librum.git --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Step into the cloned project folder. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd Librum&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create the build folder and step into it. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mkdir build-Release&#xA;cd build-Release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run cmake. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=&amp;lt;path/to/Qt&amp;gt; ..&#xA;&lt;/code&gt;&lt;/pre&gt; Set &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; to your Qt installation path. Installing Qt via the online installer usually installs it to &lt;code&gt;/home/&amp;lt;name&amp;gt;/Qt/&amp;lt;version&amp;gt;/gcc_64&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build the project &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake --build . -j $(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install Librum &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake --install .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt; &#xA;&lt;h3&gt;Troubleshooting&lt;/h3&gt; &#xA;&lt;p&gt;Here are solutions to some common errors. If your error is not listed here, please open an issue. &lt;br&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt; &lt;p&gt;Error: &lt;code&gt;Failed to find required Qt component &#34;Quick&#34;.&lt;/code&gt;&lt;br&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Solution: Install the libGL mesa dev package, on ubuntu its &lt;code&gt;sudo apt install libgl1-mesa-dev&lt;/code&gt; and on fedora its &lt;code&gt;sudo dnf install mesa-libGL-devel&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Error: &lt;code&gt;Could not load the qt platform plugin &#34;xcb&#34; even though it was found&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;Solution: Install the libxcb-cursor-dev, on ubuntu its &lt;code&gt;sudo apt install libxcb-cursor-dev&lt;/code&gt;&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;For Windows&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmake (&lt;a href=&#34;https://cmake.org/download&#34;&gt;https://cmake.org/download&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Visual Studio &lt;b&gt;19&lt;/b&gt; (&lt;a href=&#34;https://visualstudio.microsoft.com/de/vs/older-downloads&#34;&gt;https://visualstudio.microsoft.com/de/vs/older-downloads&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Python (&lt;a href=&#34;https://www.python.org/downloads&#34;&gt;https://www.python.org/downloads&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Qt 6.5 (&lt;a href=&#34;https://www.qt.io/download-open-source&#34;&gt;https://www.qt.io/download-open-source&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;To build Librum on windows, run the following commands in the Powershell:&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/Librum-Reader/Librum.git --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Step into the cloned project folder. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd Librum&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create the build folder and step into it. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mkdir build&#xA;cd build&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run cmake. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=&amp;lt;path/to/qt&amp;gt; ..&#xA;&lt;/code&gt;&lt;/pre&gt; Set &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; to your Qt installation path. Installing Qt via the online installer usually installs it to &lt;code&gt;&amp;lt;Drive&amp;gt;\\Qt\\&amp;lt;version&amp;gt;\\msvc2019_64&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build the project &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake --build . --config Release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run the app &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;./librum&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Additional Info&lt;/h3&gt; &#xA;&lt;p&gt;Here are some things to keep in mind during the build process. &lt;br&gt;&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Make sure to add cmake and the Qt binaries to the &lt;code&gt;PATH&lt;/code&gt; environment variable&lt;/li&gt; &#xA; &lt;li&gt;You need Visual Studio 2019, newer versions will &lt;strong&gt;not&lt;/strong&gt; work&lt;/li&gt; &#xA; &lt;li&gt;For the Qt installation, you &lt;strong&gt;only&lt;/strong&gt; need to choose &#34;MSVC 2019 64-bit&#34;, you can untick everything else to reduce the download size&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;For MacOS&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;cmake (&lt;a href=&#34;https://cmake.org/download&#34;&gt;https://cmake.org/download&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;make (&lt;a href=&#34;http://ftp.gnu.org/gnu/make&#34;&gt;http://ftp.gnu.org/gnu/make&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;g++ (&lt;a href=&#34;https://gcc.gnu.org&#34;&gt;https://gcc.gnu.org&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;python3 (&lt;a href=&#34;https://www.python.org/downloads&#34;&gt;https://www.python.org/downloads&lt;/a&gt;)&lt;/li&gt; &#xA; &lt;li&gt;Qt 6.5 (&lt;a href=&#34;https://www.qt.io/download-open-source&#34;&gt;https://www.qt.io/download-open-source&lt;/a&gt;)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Installation&lt;/h3&gt; &#xA;&lt;p&gt;The installation is straight forward, just follow the steps below:&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Clone the repository. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;git clone https://github.com/Librum-Reader/Librum.git --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Step into the cloned project folder. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cd Librum&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Create the build folder and step into it. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;mkdir build-Release&#xA;cd build-Release&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Run cmake. &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake -DCMAKE_INSTALL_PREFIX=/usr/local -DCMAKE_BUILD_TYPE=Release -DBUILD_TESTS=Off -DCMAKE_PREFIX_PATH=&amp;lt;path/to/Qt&amp;gt; ..&#xA;&lt;/code&gt;&lt;/pre&gt; Set &lt;code&gt;CMAKE_PREFIX_PATH&lt;/code&gt; to your Qt installation path. Installing Qt via the online installer usually installs it to &lt;code&gt;/Users/&amp;lt;name&amp;gt;/Qt/&amp;lt;version&amp;gt;/macos&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Build the project &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake --build . -j $(nproc)&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA; &lt;li&gt;Install Librum &lt;pre&gt;&lt;code class=&#34;language-sh&#34;&gt;cmake --install .&#xA;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;br&gt;</summary>
  </entry>
</feed>