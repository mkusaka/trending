<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2024-04-29T01:25:48Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>hydralauncher/hydra</title>
    <updated>2024-04-29T01:25:48Z</updated>
    <id>tag:github.com,2024-04-29:/hydralauncher/hydra</id>
    <link href="https://github.com/hydralauncher/hydra" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Hydra is a game launcher with its own embedded bittorrent client and a self-managed repack scraper.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Hydra&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://discord.gg/hydralauncher&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;https://img.shields.io/discord/1220692017311645737?style=flat&amp;amp;logo=discord&amp;amp;label=Hydra&amp;amp;labelColor=%231c1c1c&#34; alt=&#34;Discord&#34;&gt;&lt;/a&gt; &lt;img src=&#34;https://img.shields.io/github/actions/workflow/status/hydralauncher/hydra/build.yml&#34; alt=&#34;GitHub Actions Workflow Status&#34;&gt; &lt;img src=&#34;https://img.shields.io/github/package-json/v/hydralauncher/hydra&#34; alt=&#34;GitHub package.json version&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Hydra is a game launcher with its own embedded bittorrent client and a self-managed repack scraper. The launcher is written in TypeScript (Electron) and Python, which handles the torrenting system by using &lt;a href=&#34;https://www.libtorrent.org/&#34;&gt;libtorrent&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/hydralauncher/hydra/main/docs/screenshot.png&#34; alt=&#34;Hydra Catalogue&#34;&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;h3&gt;Install Node.js&lt;/h3&gt; &#xA;&lt;p&gt;Ensure you have Node.js installed on your machine. If not, download and install it from &lt;a href=&#34;https://nodejs.org/&#34;&gt;nodejs.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Install Yarn&lt;/h3&gt; &#xA;&lt;p&gt;Yarn is a package manager for Node.js. If you haven&#39;t installed Yarn yet, you can do so by following the instructions on &lt;a href=&#34;https://classic.yarnpkg.com/lang/en/docs/install/&#34;&gt;yarnpkg.com&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Clone the Repository&lt;/h3&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;git clone https://github.com/hydralauncher/hydra.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install Node Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Navigate to the project directory and install the Node dependencies using Yarn:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cd hydra&#xA;yarn&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install Python 3.9&lt;/h3&gt; &#xA;&lt;p&gt;Ensure you have Python installed on your machine. You can download and install it from &lt;a href=&#34;https://www.python.org/downloads/release/python-3919/&#34;&gt;python.org&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h3&gt;Install Python Dependencies&lt;/h3&gt; &#xA;&lt;p&gt;Install the required Python dependencies using pip:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Environment variables&lt;/h2&gt; &#xA;&lt;p&gt;You&#39;ll need a SteamGridDB API Key in order to fetch the game icons on installation. If you want to have onlinefix as a repacker you&#39;ll need to add your credentials to the .env&lt;/p&gt; &#xA;&lt;p&gt;Once you have it, you can paste the &lt;code&gt;.env.example&lt;/code&gt; file and put it on &lt;code&gt;STEAMGRIDDB_API_KEY&lt;/code&gt;, &lt;code&gt;ONLINEFIX_USERNAME&lt;/code&gt;, &lt;code&gt;ONLINEFIX_PASSWORD&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Running&lt;/h2&gt; &#xA;&lt;p&gt;Once you&#39;ve got all things set up, you can run the following command to start both the Electron process and the bittorrent client:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yarn start&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Build&lt;/h2&gt; &#xA;&lt;h3&gt;Build the bittorrent client&lt;/h3&gt; &#xA;&lt;p&gt;Build the bittorrent client by using this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python torrent-client/setup.py build&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Build the Electron application&lt;/h3&gt; &#xA;&lt;p&gt;Build the Electron application by using this command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;yarn make&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Contributors&lt;/h2&gt; &#xA;&lt;a href=&#34;https://github.com/hydralauncher/hydra/graphs/contributors&#34;&gt; &lt;img src=&#34;https://contrib.rocks/image?repo=hydralauncher/hydra&#34;&gt; &lt;/a&gt; &#xA;&lt;p&gt;Made with &lt;a href=&#34;https://contrib.rocks&#34;&gt;contrib.rocks&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;Hydra is licensed under the &lt;a href=&#34;https://raw.githubusercontent.com/hydralauncher/hydra/main/LICENSE&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>adam-maj/tiny-gpu</title>
    <updated>2024-04-29T01:25:48Z</updated>
    <id>tag:github.com,2024-04-29:/adam-maj/tiny-gpu</id>
    <link href="https://github.com/adam-maj/tiny-gpu" rel="alternate"></link>
    <summary type="html">&lt;p&gt;A minimal GPU design in Verilog to learn how GPUs work from the ground up&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;tiny-gpu&lt;/h1&gt; &#xA;&lt;p&gt;A minimal GPU implementation in Verilog optimized for learning about how GPUs work from the ground up.&lt;/p&gt; &#xA;&lt;p&gt;Built with &amp;lt;15 files of fully documented Verilog, complete documentation on architecture &amp;amp; ISA, working matrix addition/multiplication kernels, and full support for kernel simulation &amp;amp; execution traces.&lt;/p&gt; &#xA;&lt;h3&gt;Table of Contents&lt;/h3&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#overview&#34;&gt;Overview&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#architecture&#34;&gt;Architecture&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#gpu&#34;&gt;GPU&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#memory&#34;&gt;Memory&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#core&#34;&gt;Core&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#isa&#34;&gt;ISA&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#execution&#34;&gt;Execution&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#core-1&#34;&gt;Core&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#thread&#34;&gt;Thread&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#kernels&#34;&gt;Kernels&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#matrix-addition&#34;&gt;Matrix Addition&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/tree/master?tab=readme-ov-file#matrix-multiplication&#34;&gt;Matrix Multiplication&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#simulation&#34;&gt;Simulation&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#advanced-functionality&#34;&gt;Advanced Functionality&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#next-steps&#34;&gt;Next Steps&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h1&gt;Overview&lt;/h1&gt; &#xA;&lt;p&gt;If you want to learn how a CPU works all the way from architecture to control signals, there are many resources online to help you.&lt;/p&gt; &#xA;&lt;p&gt;GPUs are not the same.&lt;/p&gt; &#xA;&lt;p&gt;Because the GPU market is so competitive, low-level technical details for all modern architectures remain proprietary.&lt;/p&gt; &#xA;&lt;p&gt;While there are lots of resources to learn about GPU programming, there&#39;s almost nothing available to learn about how GPU&#39;s work at a hardware level.&lt;/p&gt; &#xA;&lt;p&gt;The best option is to go through open-source GPU implementations like &lt;a href=&#34;https://github.com/VerticalResearchGroup/miaow&#34;&gt;Miaow&lt;/a&gt; and &lt;a href=&#34;https://github.com/hughperkins/VeriGPU/tree/main&#34;&gt;VeriGPU&lt;/a&gt; and try to figure out what&#39;s going on. This is challenging since these projects aim at being feature complete and functional, so they&#39;re quite complex.&lt;/p&gt; &#xA;&lt;p&gt;This is why I built &lt;code&gt;tiny-gpu&lt;/code&gt;!&lt;/p&gt; &#xA;&lt;h2&gt;What is tiny-gpu?&lt;/h2&gt; &#xA;&lt;blockquote&gt; &#xA; &lt;p&gt;[!IMPORTANT]&lt;/p&gt; &#xA; &lt;p&gt;&lt;strong&gt;tiny-gpu&lt;/strong&gt; is a minimal GPU implementation optimized for learning about how GPUs work from the ground up.&lt;/p&gt; &#xA; &lt;p&gt;Specifically, with the trend toward general-purpose GPUs (GPGPUs) and ML-accelerators like Google&#39;s TPU, tiny-gpu focuses on highlighting the general principles of all of these architectures, rather than on the details of graphics-specific hardware.&lt;/p&gt; &#xA;&lt;/blockquote&gt; &#xA;&lt;p&gt;With this motivation in mind, we can simplify GPUs by cutting out the majority of complexity involved with building a production-grade graphics card, and focus on the core elements that are critical to all of these modern hardwareaccelerators.&lt;/p&gt; &#xA;&lt;p&gt;This project is primarily focused on exploring:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt; - What does the architecture of a GPU look like? What are the most important elements?&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Parallelization&lt;/strong&gt; - How is the SIMD progamming model implemented in hardware?&lt;/li&gt; &#xA; &lt;li&gt;&lt;strong&gt;Memory&lt;/strong&gt; - How does a GPU work around the constraints of limited memory bandwidth?&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;After understanding the fundamentals laid out in this project, you can checkout the &lt;a href=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/#advanced-functionality&#34;&gt;advanced functionality section&lt;/a&gt; to understand some of the most important optimizations made in production grade GPUs (that are more challenging to implement) which improve performance.&lt;/p&gt; &#xA;&lt;h1&gt;Architecture&lt;/h1&gt; &#xA;&lt;p float=&#34;left&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/docs/images/gpu.png&#34; alt=&#34;GPU&#34; width=&#34;48%&#34;&gt; &lt;img src=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/docs/images/core.png&#34; alt=&#34;Core&#34; width=&#34;48%&#34;&gt; &lt;/p&gt; &#xA;&lt;h2&gt;GPU&lt;/h2&gt; &#xA;&lt;p&gt;tiny-gpu is built to execute a single kernel at a time.&lt;/p&gt; &#xA;&lt;p&gt;In order to launch a kernel, we need to do the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Load global program memory with the kernel code&lt;/li&gt; &#xA; &lt;li&gt;Load data memory with the necessary data&lt;/li&gt; &#xA; &lt;li&gt;Specify the number of threads to launch in the device control register&lt;/li&gt; &#xA; &lt;li&gt;Launch the kernel by setting the start signal to high.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The GPU itself consists of the following units:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;Device control register&lt;/li&gt; &#xA; &lt;li&gt;Dispatcher&lt;/li&gt; &#xA; &lt;li&gt;Variable number of compute cores&lt;/li&gt; &#xA; &lt;li&gt;Memory controllers for data memory &amp;amp; program memory&lt;/li&gt; &#xA; &lt;li&gt;Cache&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;h3&gt;Device Control Register&lt;/h3&gt; &#xA;&lt;p&gt;The device control register usually stores metadata specifying how kernels should be executed on the GPU.&lt;/p&gt; &#xA;&lt;p&gt;In this case, the device control register just stores the &lt;code&gt;thread_count&lt;/code&gt; - the total number of threads to launch for the active kernel.&lt;/p&gt; &#xA;&lt;h3&gt;Dispatcher&lt;/h3&gt; &#xA;&lt;p&gt;Once a kernel is launched, the dispatcher is the unit that actually manages the distribution of threads to different compute cores.&lt;/p&gt; &#xA;&lt;p&gt;The dispatcher organizes threads into groups that can be executed in parallel on a single core called &lt;strong&gt;blocks&lt;/strong&gt; and sends these blocks off to be processed by available cores.&lt;/p&gt; &#xA;&lt;p&gt;Once all blocks have been processed, the dispatcher reports back that the kernel execution is done.&lt;/p&gt; &#xA;&lt;h2&gt;Memory&lt;/h2&gt; &#xA;&lt;p&gt;The GPU is built to interface with an external global memory. Here, data memory and program memory are separated out for simplicity.&lt;/p&gt; &#xA;&lt;h3&gt;Global Memory&lt;/h3&gt; &#xA;&lt;p&gt;tiny-gpu data memory has the following specifications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8 bit addressability (256 total rows of data memory)&lt;/li&gt; &#xA; &lt;li&gt;8 bit data (stores values of &amp;lt;256 for each row)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;tiny-gpu program memory has the following specifications:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;8 bit addressability (256 rows of program memory)&lt;/li&gt; &#xA; &lt;li&gt;16 bit data (each instruction is 16 bits as specified by the ISA)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h3&gt;Memory Controllers&lt;/h3&gt; &#xA;&lt;p&gt;Global memory has fixed read/write bandwidth, but there may be far more incoming requests across all cores to access data from memory than the external memory is actually able to handle.&lt;/p&gt; &#xA;&lt;p&gt;The memory controllers keep track of all the outgoing requests to memory from the compute cores, throttle requests based on actual external memory bandwidth, and relay responses from external memory back to the proper resources.&lt;/p&gt; &#xA;&lt;p&gt;Each memory controller has a fixed number of channels based on the bandwidth of global memory.&lt;/p&gt; &#xA;&lt;h3&gt;Cache (WIP)&lt;/h3&gt; &#xA;&lt;p&gt;The same data is often requested from global memory by multiple cores. Constantly access global memory repeatedly is expensive, and since the data has already been fetched once, it would be more efficient to store it on device in SRAM to be retrieved much quicker on later requests.&lt;/p&gt; &#xA;&lt;p&gt;This is exactly what the cache is used for. Data retrieved from external memory is stored in cache and can be retrieved from there on later requests, freeing up memory bandwidth to be used for new data.&lt;/p&gt; &#xA;&lt;h2&gt;Core&lt;/h2&gt; &#xA;&lt;p&gt;Each core has a number of compute resources, often built around a certain number of threads it can support. In order to maximize parallelization, these resources need to be managed optimally to maximize resource utilization.&lt;/p&gt; &#xA;&lt;p&gt;In this simplified GPU, each core processed one &lt;strong&gt;block&lt;/strong&gt; at a time, and for each thread in a block, the core has a dedicated ALU, LSU, PC, and register file. Managing the execution of thread instructions on these resources is one of the most challening problems in GPUs.&lt;/p&gt; &#xA;&lt;h3&gt;Scheduler&lt;/h3&gt; &#xA;&lt;p&gt;Each core has a single scheduler that manages the execution of threads.&lt;/p&gt; &#xA;&lt;p&gt;The tiny-gpu scheduler executes instructions for a single block to completion before picking up a new block, and it executes instructions for all threads in-sync and sequentially.&lt;/p&gt; &#xA;&lt;p&gt;In more advanced schedulers, techniques like &lt;strong&gt;pipelining&lt;/strong&gt; are used to stream the execution of multiple instructions subsequent instructions to maximize resource utilization before previous instructions are fully complete. Additionally, &lt;strong&gt;warp scheduling&lt;/strong&gt; can be use to execute multiple batches of threads within a block in parallel.&lt;/p&gt; &#xA;&lt;p&gt;The main constraint the scheduler has to work around is the latency associated with loading &amp;amp; storing data from global memory. While most instructions can be executed synchronously, these load-store operations are asynchronous, meaning the rest of the instruction execution has to be built around these long wait times.&lt;/p&gt; &#xA;&lt;h3&gt;Fetcher&lt;/h3&gt; &#xA;&lt;p&gt;Asynchronously fetches the instruction at the current program counter from program memory (most should actually be fetching from cache after a single block is executed).&lt;/p&gt; &#xA;&lt;h3&gt;Decoder&lt;/h3&gt; &#xA;&lt;p&gt;Decodes the fetched instruction into control signals for thread execution.&lt;/p&gt; &#xA;&lt;h3&gt;Register Files&lt;/h3&gt; &#xA;&lt;p&gt;Each thread has it&#39;s own dedicated set of register files. The register files hold the data that each thread is performing computations on, which enables the same-instruction multiple-data (SIMD) pattern.&lt;/p&gt; &#xA;&lt;p&gt;Importantly, each register file contains a few read-only registers holding data about the current block &amp;amp; thread being executed locally, enabling kernels to be executed with different data based on the local thread id.&lt;/p&gt; &#xA;&lt;h3&gt;ALUs&lt;/h3&gt; &#xA;&lt;p&gt;Dedicated arithmetic-logic unit for each thread to perform computations. Handles the &lt;code&gt;ADD&lt;/code&gt;, &lt;code&gt;SUB&lt;/code&gt;, &lt;code&gt;MUL&lt;/code&gt;, &lt;code&gt;DIV&lt;/code&gt; arithmetic instructions.&lt;/p&gt; &#xA;&lt;p&gt;Also handles the &lt;code&gt;CMP&lt;/code&gt; comparison instruction which actually outputs whether the result of the difference between two registers is negative, zero or positive - and stores the result in the &lt;code&gt;NZP&lt;/code&gt; register in the PC unit.&lt;/p&gt; &#xA;&lt;h3&gt;LSUs&lt;/h3&gt; &#xA;&lt;p&gt;Dedicated load-store unit for each thread to access global data memory.&lt;/p&gt; &#xA;&lt;p&gt;Handles the &lt;code&gt;LDR&lt;/code&gt; &amp;amp; &lt;code&gt;STR&lt;/code&gt; instructions - and handles async wait times for memory requests to be processed and relayed by the memory controller.&lt;/p&gt; &#xA;&lt;h3&gt;PCs&lt;/h3&gt; &#xA;&lt;p&gt;Dedicated program-counter for each unit to determine the next instructions to execute on each thread.&lt;/p&gt; &#xA;&lt;p&gt;By default, the PC increments by 1 after every instruction.&lt;/p&gt; &#xA;&lt;p&gt;With the &lt;code&gt;BRnzp&lt;/code&gt; instruction, the NZP register checks to see if the NZP register (set by a previous &lt;code&gt;CMP&lt;/code&gt; instruction) matches some case - and if it does, it will branch to a specific line of program memory. &lt;em&gt;This is how loops and conditionals are implemented.&lt;/em&gt;&lt;/p&gt; &#xA;&lt;p&gt;Since threads are processed in parallel, tiny-gpu assumes that all threads &#34;converge&#34; to the same program counter after each instruction - which is a naive assumption for the sake of simplicity.&lt;/p&gt; &#xA;&lt;p&gt;In real GPUs, individual threads can branch to different PCs, causing &lt;strong&gt;branch divergence&lt;/strong&gt; where a group of threads threads initially being processed together has to split out into separate execution.&lt;/p&gt; &#xA;&lt;h1&gt;ISA&lt;/h1&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/docs/images/isa.png&#34; alt=&#34;ISA&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;tiny-gpu implements a simple 11 instruction ISA built to enable simple kernels for proof-of-concept like matrix addition &amp;amp; matrix multiplication (implementation further down on this page).&lt;/p&gt; &#xA;&lt;p&gt;For these purposes, it supports the following instructions:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;code&gt;BRnzp&lt;/code&gt; - Branch instruction to jump to another line of program memory if the NZP register matches the &lt;code&gt;nzp&lt;/code&gt; condition in the instruction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CMP&lt;/code&gt; - Compare the value of two registers and store the result in the NZP register to use for a later &lt;code&gt;BRnzp&lt;/code&gt; instruction.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;ADD&lt;/code&gt;, &lt;code&gt;SUB&lt;/code&gt;, &lt;code&gt;MUL&lt;/code&gt;, &lt;code&gt;DIV&lt;/code&gt; - Basic arithmetic operations to enable tensor math.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;LDR&lt;/code&gt; - Load data from global memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;STR&lt;/code&gt; - Store data into global memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;CONST&lt;/code&gt; - Load a constant value into a register.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;RET&lt;/code&gt; - Signal that the current thread has reached the end of execution.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Each register is specified by 4 bits, meaning that there are 16 total registers. The first 13 register &lt;code&gt;R0&lt;/code&gt; - &lt;code&gt;R12&lt;/code&gt; are free registers that support read/write. The last 3 registers are special read-only registers used to supply the &lt;code&gt;%blockIdx&lt;/code&gt;, &lt;code&gt;%blockDim&lt;/code&gt;, and &lt;code&gt;%threadIdx&lt;/code&gt; critical to SIMD.&lt;/p&gt; &#xA;&lt;h1&gt;Execution&lt;/h1&gt; &#xA;&lt;h3&gt;Core&lt;/h3&gt; &#xA;&lt;p&gt;Each core follows the following control flow going through different stages to execute each instruction:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt;&lt;code&gt;FETCH&lt;/code&gt; - Fetch the next instruction at current program counter from program memory.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;DECODE&lt;/code&gt; - Decode the instruction into control signals.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;REQUEST&lt;/code&gt; - Request data from global memory if necessary (if &lt;code&gt;LDR&lt;/code&gt; or &lt;code&gt;STR&lt;/code&gt; instruction).&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;WAIT&lt;/code&gt; - Wait for data from global memory if applicable.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;EXECUTE&lt;/code&gt; - Execute any computations on data.&lt;/li&gt; &#xA; &lt;li&gt;&lt;code&gt;UPDATE&lt;/code&gt; - Update register files and NZP register.&lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;The control flow is laid out like this for the sake of simplicity and understandability.&lt;/p&gt; &#xA;&lt;p&gt;In practice, several of these steps could be compressed to be optimize processing times, and the GPU could also use &lt;strong&gt;pipelining&lt;/strong&gt; to stream and coordinate the execution of many instructions on a cores resources without waiting for previous instructions to finish.&lt;/p&gt; &#xA;&lt;h3&gt;Thread&lt;/h3&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/docs/images/thread.png&#34; alt=&#34;Thread&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;Each thread within each core follows the above execution path to perform computations on the data in it&#39;s dedicated register file.&lt;/p&gt; &#xA;&lt;p&gt;This resembles a standard CPU diagram, and is quite similar in functionality as well. The main difference is that the &lt;code&gt;%blockIdx&lt;/code&gt;, &lt;code&gt;%blockDim&lt;/code&gt;, and &lt;code&gt;%threadIdx&lt;/code&gt; values lie in the read-only registers for each thread, enabling SIMD functionality.&lt;/p&gt; &#xA;&lt;h1&gt;Kernels&lt;/h1&gt; &#xA;&lt;p&gt;I wrote a matrix addition and matrix multiplication kernel using my ISA as a proof of concept to demonstrate SIMD programming and execution with my GPU. The test files in this repository are capable of fully simulating the execution of these kernels on the GPU, producing data memory states and a complete execution trace.&lt;/p&gt; &#xA;&lt;h3&gt;Matrix Addition&lt;/h3&gt; &#xA;&lt;p&gt;This matrix addition kernel adds two 1 x 8 matrices by performing 8 element wise additions in separate threads.&lt;/p&gt; &#xA;&lt;p&gt;This demonstration makes use of the &lt;code&gt;%blockIdx&lt;/code&gt;, &lt;code&gt;%blockDim&lt;/code&gt;, and &lt;code&gt;%threadIdx&lt;/code&gt; registers to show SIMD programming on this GPU. It also uses the &lt;code&gt;LDR&lt;/code&gt; and &lt;code&gt;STR&lt;/code&gt; instructions which require async memory management.&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;matadd.asm&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-asm&#34;&gt;.threads 8&#xA;.data 0 1 2 3 4 5 6 7          ; matrix A (1 x 8)&#xA;.data 0 1 2 3 4 5 6 7          ; matrix B (1 x 8)&#xA;&#xA;MUL R0, %blockIdx, %blockDim&#xA;ADD R0, R0, %threadIdx         ; i = blockIdx * blockDim + threadIdx&#xA;&#xA;CONST R1, #0                   ; baseA (matrix A base address)&#xA;CONST R2, #8                   ; baseB (matrix B base address)&#xA;CONST R3, #16                  ; baseC (matrix C base address)&#xA;&#xA;ADD R4, R1, R0                 ; addr(A[i]) = baseA + i&#xA;LDR R4, R4                     ; load A[i] from global memory&#xA;&#xA;ADD R5, R2, R0                 ; addr(B[i]) = baseB + i&#xA;LDR R5, R5                     ; load B[i] from global memory&#xA;&#xA;ADD R6, R4, R5                 ; C[i] = A[i] + B[i]&#xA;&#xA;ADD R7, R3, R0                 ; addr(C[i]) = baseC + i&#xA;STR R7, R6                     ; store C[i] in global memory&#xA;&#xA;RET                            ; end of kernel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Matrix Multiplication&lt;/h3&gt; &#xA;&lt;p&gt;The matrix multiplication kernel multiplies two 2x2 matrices. It performs element wise calculation of the dot product of the relevant row and column and uses the &lt;code&gt;CMP&lt;/code&gt; and &lt;code&gt;BRnzp&lt;/code&gt; instructions to demonstrate branching within the threads (notably, all branches converge so this kernel works on the current tiny-gpu implementation).&lt;/p&gt; &#xA;&lt;p&gt;&lt;code&gt;matmul.asm&lt;/code&gt;&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-asm&#34;&gt;.threads 4&#xA;.data 1 2 3 4                  ; matrix A (2 x 2)&#xA;.data 1 2 3 4                  ; matrix B (2 x 2)&#xA;&#xA;MUL R0, %blockIdx, %blockDim&#xA;ADD R0, R0, %threadIdx         ; i = blockIdx * blockDim + threadIdx&#xA;&#xA;CONST R1, #1                   ; increment&#xA;CONST R2, #2                   ; N (matrix inner dimension)&#xA;CONST R3, #0                   ; baseA (matrix A base address)&#xA;CONST R4, #4                   ; baseB (matrix B base address)&#xA;CONST R5, #8                   ; baseC (matrix C base address)&#xA;&#xA;DIV R6, R0, R2                 ; row = i // N&#xA;MUL R7, R6, R2&#xA;SUB R7, R0, R7                 ; col = i % N&#xA;&#xA;CONST R8, #0                   ; acc = 0&#xA;CONST R9, #0                   ; k = 0&#xA;&#xA;LOOP:&#xA;  MUL R10, R6, R2&#xA;  ADD R10, R10, R9&#xA;  ADD R10, R10, R3             ; addr(A[i]) = row * N + k + baseA&#xA;  LDR R10, R10                 ; load A[i] from global memory&#xA;&#xA;  MUL R11, R9, R2&#xA;  ADD R11, R11, R7&#xA;  ADD R11, R11, R4             ; addr(B[i]) = k * N + col + baseB&#xA;  LDR R11, R11                 ; load B[i] from global memory&#xA;&#xA;  MUL R12, R10, R11&#xA;  ADD R8, R8, R12              ; acc = acc + A[i] * B[i]&#xA;&#xA;  ADD R9, R9, R1               ; increment k&#xA;&#xA;  CMP R9, R2&#xA;  BRn LOOP                    ; loop while k &amp;lt; N&#xA;&#xA;ADD R9, R5, R0                 ; addr(C[i]) = baseC + i&#xA;STR R9, R8                     ; store C[i] in global memory&#xA;&#xA;RET                            ; end of kernel&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h1&gt;Simulation&lt;/h1&gt; &#xA;&lt;p&gt;tiny-gpu is setup to simulate the execution of both of the above kernels. Before simulating, you&#39;ll need to install &lt;a href=&#34;https://steveicarus.github.io/iverilog/usage/installation.html&#34;&gt;iverilog&lt;/a&gt; and &lt;a href=&#34;https://docs.cocotb.org/en/stable/install.html&#34;&gt;cocotb&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Once you&#39;ve installed the pre-requisites, you can run the kernel simulations with &lt;code&gt;make test_matadd&lt;/code&gt; and &lt;code&gt;make test_matmul&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;p&gt;Executing the simulations will output a log file in &lt;code&gt;test/logs&lt;/code&gt; with the initial data memory state, complete execution trace of the kernel, and final data memory state.&lt;/p&gt; &#xA;&lt;p&gt;If you look at the initial data memory state logged at the start of the logfile for each, you should see the two start matrices for the calculation, and in the final data memory at the end of the file you should also see the resultant matrix.&lt;/p&gt; &#xA;&lt;p&gt;Below is a sample of the execution traces, showing on each cycle the execution of every thread within every core, including the current instruction, PC, register values, states, etc.&lt;/p&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/adam-maj/tiny-gpu/master/docs/images/trace.png&#34; alt=&#34;execution trace&#34;&gt;&lt;/p&gt; &#xA;&lt;p&gt;&lt;strong&gt;For anyone trying to run the simulation or play with this repo, please feel free to DM me on &lt;a href=&#34;https://twitter.com/majmudaradam&#34;&gt;twitter&lt;/a&gt; if you run into any issues - I want you to get this running!&lt;/strong&gt;&lt;/p&gt; &#xA;&lt;h1&gt;Advanced Functionality&lt;/h1&gt; &#xA;&lt;p&gt;For the sake of simplicity, there were many additional features implemented in modern GPUs that heavily improve performance &amp;amp; functionality that tiny-gpu omits. We&#39;ll discuss some of those most critical features in this section.&lt;/p&gt; &#xA;&lt;h3&gt;Multi-layered Cache &amp;amp; Shared Memory&lt;/h3&gt; &#xA;&lt;p&gt;In modern GPUs, multiple different levels of caches are used to minimize the amount of data that needs to get accessed from global memory. tiny-gpu implements only one cache layer between individual compute units requesting memory and the memory controllers which stores recent cached data.&lt;/p&gt; &#xA;&lt;p&gt;Implementing multi-layered caches allows frequently accessed data to be cached more locally to where it&#39;s being used (with some caches within individual compute cores), minimizing load times for this data.&lt;/p&gt; &#xA;&lt;p&gt;Different caching algorithms are used to maximize cache-hits - this is a critical dimension that can be improved on to optimize memory access.&lt;/p&gt; &#xA;&lt;p&gt;Additionally, GPUs often use &lt;strong&gt;shared memory&lt;/strong&gt; for threads within the same block to access a single memory space that can be used to share results with other threads.&lt;/p&gt; &#xA;&lt;h3&gt;Memory Coalescing&lt;/h3&gt; &#xA;&lt;p&gt;Another critical memory optimization used by GPUs is &lt;strong&gt;memory coalescing.&lt;/strong&gt; Multiple threads running in parallel often need to access sequential addresses in memory (for example, a group of threads accessing neighboring elements in a matrix) - but each of these memory requests is put in separately.&lt;/p&gt; &#xA;&lt;p&gt;Memory coalescing is used to analyzing queued memory requests and combine neighboring requests into a single transaction, minimizing time spent on addressing, and making all the requests together.&lt;/p&gt; &#xA;&lt;h3&gt;Pipelining&lt;/h3&gt; &#xA;&lt;p&gt;In the control flow for tiny-gpu, cores wait for one instruction to be executed on a group of threads before starting execution of the next instruction.&lt;/p&gt; &#xA;&lt;p&gt;Modern GPUs use &lt;strong&gt;pipelining&lt;/strong&gt; to stream execution of multiple sequential instructions at once while ensuring that instructions with dependencies on each other still get executed sequentially.&lt;/p&gt; &#xA;&lt;p&gt;This helps to maximize resource utilization within cores as resources are not sitting idle while waiting (ex: during async memory requests).&lt;/p&gt; &#xA;&lt;h3&gt;Warp Scheduling&lt;/h3&gt; &#xA;&lt;p&gt;Another strategy used to maximize resource utilization on course is &lt;strong&gt;warp scheduling.&lt;/strong&gt; This approach involves breaking up blocks into individual batches of theads that can be executed together.&lt;/p&gt; &#xA;&lt;p&gt;Multiple warps can be executed on a single core simultaneously by executing instructions from one warp while another warp is waiting. This is similar to pipelining, but dealing with instructions from different threads.&lt;/p&gt; &#xA;&lt;h3&gt;Branch Divergence&lt;/h3&gt; &#xA;&lt;p&gt;tiny-gpu assumes that all threads in a single batch end up on the same PC after each instruction, meaning that threads can be executed in parallel for their entire lifetime.&lt;/p&gt; &#xA;&lt;p&gt;In reality, individual threads could diverge from each other and branch to different lines based on their data. With different PCs, these threads would need to split into separate lines of execution, which requires managing diverging threads &amp;amp; paying attention to when threads converge again.&lt;/p&gt; &#xA;&lt;h3&gt;Synchronization &amp;amp; Barriers&lt;/h3&gt; &#xA;&lt;p&gt;Another core functionality of modern GPUs is the ability to set &lt;strong&gt;barriers&lt;/strong&gt; so that groups of threads in a block can synchronize and wait until all other threads in the same block have gotten to a certain point before continuing execution.&lt;/p&gt; &#xA;&lt;p&gt;This is useful for cases where threads need to exchange shared data with each other so they can ensure that the data has been fully processed.&lt;/p&gt; &#xA;&lt;h1&gt;Next Steps&lt;/h1&gt; &#xA;&lt;p&gt;Updates I want to make in the future to improve the design, anyone else is welcome to contribute as well:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add a simple cache for instructions&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Build an adapter to use GPU with Tiny Tapeout 7&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add basic branch divergence&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add basic memory coalescing&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Add basic pipelining&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Optimize control flow and use of registers to improve cycle time&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Write a basic graphics kernel or add simple graphics hardware to demonstrate graphics functionality&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;&lt;strong&gt;For anyone curious to play around or make a contribution, feel free to put up a PR with any improvements you&#39;d like to add 😄&lt;/strong&gt;&lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>dcharatan/flowmap</title>
    <updated>2024-04-29T01:25:48Z</updated>
    <id>tag:github.com,2024-04-29:/dcharatan/flowmap</id>
    <link href="https://github.com/dcharatan/flowmap" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Code for &#34;FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent&#34; by Cameron Smith*, David Charatan*, Ayush Tewari, and Vincent Sitzmann&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FlowMap&lt;/h1&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/dcharatan/flowmap/assets/13124225/9dc9cc9a-083e-4fd1-b833-09365385cf59&#34;&gt;https://github.com/dcharatan/flowmap/assets/13124225/9dc9cc9a-083e-4fd1-b833-09365385cf59&lt;/a&gt;&lt;/p&gt; &#xA;&lt;p&gt;This is the official implementation for &lt;strong&gt;FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent&lt;/strong&gt; by Cameron Smith*, David Charatan*, Ayush Tewari, and Vincent Sitzmann.&lt;/p&gt; &#xA;&lt;p&gt;Check out the project website &lt;a href=&#34;https://cameronosmith.github.io/flowmap/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;To get started on Linux, create a Python virtual environment:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3.11 -m venv venv&#xA;source venv/bin/activate&#xA;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;For pretraining, make sure GMFlow is installed as a submodule:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git submodule update --init --recursive&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If the above requirements don&#39;t work, you can try &lt;code&gt;requirements_exact.txt&lt;/code&gt; instead.&lt;/p&gt; &#xA;&lt;h2&gt;Running the Code&lt;/h2&gt; &#xA;&lt;p&gt;The main entry point is &lt;code&gt;flowmap/overfit.py&lt;/code&gt;. Call it via:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m flowmap.overfit dataset=images dataset.images.root=path/to/folder/with/images&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Make sure the virtual environment has been activated via &lt;code&gt;source venv/bin/activate&lt;/code&gt; first.&lt;/p&gt; &#xA;&lt;h2&gt;Pre-trained Initialization&lt;/h2&gt; &#xA;&lt;p&gt;The checkpoint we used to initialize FlowMap can be found &lt;a href=&#34;https://drive.google.com/drive/folders/1PqByQSfzyLjfdZZDwn6RXIECso7WB9IY?usp=drive_link&#34;&gt;here&lt;/a&gt;. To train your own, download the &lt;a href=&#34;https://google.github.io/realestate10k/&#34;&gt;Real Estate 10k&lt;/a&gt; and &lt;a href=&#34;https://github.com/facebookresearch/co3d&#34;&gt;CO3Dv2&lt;/a&gt; datasets and run the following script:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python3 -m flowmap.pretrain&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Some of the videos in the Real Estate 10k dataset are no longer publicly available. Reach out to us via email if you want our downloaded version of the dataset.&lt;/p&gt; &#xA;&lt;h2&gt;Evaluation Datasets&lt;/h2&gt; &#xA;&lt;p&gt;We evaluated FlowMap using video subsets of the &lt;a href=&#34;https://drive.google.com/drive/folders/1M-_Fdn4ajDa0CS8-iqejv0fQQeuonpKF?usp=drive_link&#34;&gt;Local Light Field Fusion (LLFF)&lt;/a&gt;, &lt;a href=&#34;https://jonbarron.info/mipnerf360/&#34;&gt;Mip-NeRF 360&lt;/a&gt;, and &lt;a href=&#34;https://www.tanksandtemples.org/download/&#34;&gt;Tanks &amp;amp; Temples&lt;/a&gt; datasets. We&#39;ve uploaded a compilation of these datasets &lt;a href=&#34;https://drive.google.com/drive/folders/1PqByQSfzyLjfdZZDwn6RXIECso7WB9IY?usp=drive_link&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;details&gt; &#xA; &lt;summary&gt;Dataset Details&lt;/summary&gt; &#xA; &lt;h3&gt;NeRF Local Light Field Fusion (LLFF) Scenes&lt;/h3&gt; &#xA; &lt;p&gt;These are the LLFF scenes from the &lt;a href=&#34;https://www.matthewtancik.com/nerf&#34;&gt;NeRF&lt;/a&gt; paper, which were originally uploaded &lt;a href=&#34;https://drive.google.com/drive/folders/14boI-o5hGO9srnWaaogTU5_ji7wkX2S7?usp=drive_link&#34;&gt;here&lt;/a&gt;. We used all 8 scenes (&lt;code&gt;fern&lt;/code&gt;, &lt;code&gt;flower&lt;/code&gt;, &lt;code&gt;fortress&lt;/code&gt;, &lt;code&gt;horns&lt;/code&gt;, &lt;code&gt;leaves&lt;/code&gt;, &lt;code&gt;orchids&lt;/code&gt;, &lt;code&gt;room&lt;/code&gt;, and &lt;code&gt;trex&lt;/code&gt;).&lt;/p&gt; &#xA; &lt;h3&gt;Mip-NeRF 360 Scenes&lt;/h3&gt; &#xA; &lt;p&gt;These are scenes from the &lt;a href=&#34;https://jonbarron.info/mipnerf360/&#34;&gt;Mip-NeRF 360&lt;/a&gt; paper, which were originally uploaded &lt;a href=&#34;http://storage.googleapis.com/gresearch/refraw360/360_v2.zip&#34;&gt;here&lt;/a&gt;. We used the &lt;code&gt;bonsai&lt;/code&gt;, &lt;code&gt;counter&lt;/code&gt;, and &lt;code&gt;kitchen&lt;/code&gt; scenes. The original &lt;code&gt;kitchen&lt;/code&gt; scene consists of several concatenated video sequences; for FlowMap, we use the first one (65 frames). We also included the &lt;code&gt;garden&lt;/code&gt; scene, which is somewhat video-like, but contain large jumps that make optical flow estimation struggle.&lt;/p&gt; &#xA; &lt;h3&gt;Tanks &amp;amp; Temples Scenes&lt;/h3&gt; &#xA; &lt;p&gt;We used all scenes from the &lt;a href=&#34;https://tanksandtemples.org/download/&#34;&gt;Tanks &amp;amp; Temples&lt;/a&gt; dataset: &lt;code&gt;auditorium&lt;/code&gt;, &lt;code&gt;ballroom&lt;/code&gt;, &lt;code&gt;barn&lt;/code&gt;, &lt;code&gt;caterpillar&lt;/code&gt;, &lt;code&gt;church&lt;/code&gt;, &lt;code&gt;courthouse&lt;/code&gt;, &lt;code&gt;family&lt;/code&gt;, &lt;code&gt;francis&lt;/code&gt;, &lt;code&gt;horse&lt;/code&gt;, &lt;code&gt;ignatius&lt;/code&gt;, &lt;code&gt;lighthouse&lt;/code&gt;, &lt;code&gt;m60&lt;/code&gt;, &lt;code&gt;meetingroom&lt;/code&gt;, &lt;code&gt;museum&lt;/code&gt;, &lt;code&gt;palace&lt;/code&gt;, &lt;code&gt;panther&lt;/code&gt;, &lt;code&gt;playground&lt;/code&gt;, &lt;code&gt;temple&lt;/code&gt;, &lt;code&gt;train&lt;/code&gt;, and &lt;code&gt;truck&lt;/code&gt;. We preprocessed the raw videos from the dataset using the script at &lt;code&gt;flowmap/subsample.py&lt;/code&gt;. This script samples 150 frames from the first minute of video evenly based on mean optical flow.&lt;/p&gt; &#xA;&lt;/details&gt; &#xA;&lt;h2&gt;Running Ablations&lt;/h2&gt; &#xA;&lt;p&gt;Each ablation shown in the paper has a &lt;a href=&#34;https://hydra.cc/docs/intro/&#34;&gt;Hydra&lt;/a&gt; configuration at &lt;code&gt;config/experiment&lt;/code&gt;. For example, to run the ablation where point tracking is disabled, add &lt;code&gt;+experiment=ablation_no_tracks&lt;/code&gt; to the overfitting command. Note that you can stack most of the ablations, e.g., &lt;code&gt;+experiment=[ablation_no_tracks,ablation_random_initialization]&lt;/code&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Figure and Table Generation&lt;/h2&gt; &#xA;&lt;p&gt;Some of the code used to generate the tables and figures in the paper can be found in the &lt;code&gt;assets&lt;/code&gt; folder.&lt;/p&gt; &#xA;&lt;h2&gt;BibTeX&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{smith24flowmap,&#xA;      title={FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent},&#xA;      author={Cameron Smith and David Charatan and Ayush Tewari and Vincent Sitzmann},&#xA;      year={2024},&#xA;      booktitle={arXiv},&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Acknowledgements&lt;/h2&gt; &#xA;&lt;p&gt;This work was supported by the National Science Foundation under Grant No. 2211259, by the Singapore DSTA under DST00OECI20300823 (New Representations for Vision), by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) under 140D0423C0075, by the Amazon Science Hub, and by IBM. The Toyota Research Institute also partially supported this work. The views and conclusions contained herein reflect the opinions and conclusions of its authors and no other entity.&lt;/p&gt;</summary>
  </entry>
</feed>