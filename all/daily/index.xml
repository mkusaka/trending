<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom">
  <title>GitHub All Languages Daily Trending</title>
  <id>http://mshibanami.github.io/GitHubTrendingRSS</id>
  <updated>2023-07-20T01:29:15Z</updated>
  <subtitle>Daily Trending of All Languages in GitHub</subtitle>
  <link href="http://mshibanami.github.io/GitHubTrendingRSS"></link>
  <entry>
    <title>shroominic/codeinterpreter-api</title>
    <updated>2023-07-20T01:29:15Z</updated>
    <id>tag:github.com,2023-07-20:/shroominic/codeinterpreter-api</id>
    <link href="https://github.com/shroominic/codeinterpreter-api" rel="alternate"></link>
    <summary type="html">&lt;p&gt;Open source implementation of the ChatGPT Code Interpreter üëæ&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;Code Interpreter API&lt;/h1&gt; &#xA;&lt;p&gt;A LangChain implementation of the ChatGPT Code Interpreter. Using CodeBoxes as backend for sandboxed python code execution. &lt;a href=&#34;https://github.com/shroominic/codebox-api/tree/main&#34;&gt;CodeBox&lt;/a&gt; is the simplest cloud infrastructure for your LLM Apps. You can run everything local except the LLM using your own OpenAI API Key.&lt;/p&gt; &#xA;&lt;h2&gt;Features&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Dataset Analysis, Stock Charting, Image Manipulation, ....&lt;/li&gt; &#xA; &lt;li&gt;Internet access and auto Python package installation&lt;/li&gt; &#xA; &lt;li&gt;Input &lt;code&gt;text + files&lt;/code&gt; -&amp;gt; Receive &lt;code&gt;text + files&lt;/code&gt;&lt;/li&gt; &#xA; &lt;li&gt;Conversation Memory: respond based on previous inputs&lt;/li&gt; &#xA; &lt;li&gt;Run everything local except the OpenAI API (OpenOrca or others maybe soon)&lt;/li&gt; &#xA; &lt;li&gt;Use CodeBox API for easy scaling in production (coming soon)&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Get your OpenAI API Key &lt;a href=&#34;https://platform.openai.com/account/api-keys&#34;&gt;here&lt;/a&gt; and install the package.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install codeinterpreterapi&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Usage&lt;/h2&gt; &#xA;&lt;p&gt;Make sure to set the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable (or use a &lt;code&gt;.env&lt;/code&gt; file)&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from codeinterpreterapi import CodeInterpreterSession&#xA;&#xA;&#xA;async def main():&#xA;    # create a session&#xA;    session = CodeInterpreterSession()&#xA;    await session.astart()&#xA;&#xA;    # generate a response based on user input&#xA;    output = await session.generate_response(&#xA;        &#34;Plot the bitcoin chart of 2023 YTD&#34;&#xA;    )&#xA;&#xA;    # output the response (text + image)&#xA;    print(&#34;AI: &#34;, response.content)&#xA;    for file in response.files:&#xA;        file.show_image()&#xA;&#xA;    # terminate the session&#xA;    await session.astop()&#xA;    &#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    import asyncio&#xA;    # run the async function&#xA;    asyncio.run(main())&#xA;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/shroominic/codeinterpreter-api/raw/main/examples/assets/bitcoin_chart.png?raw=true&#34; alt=&#34;Bitcoin YTD&#34;&gt;&lt;br&gt; Bitcoin YTD Chart Output&lt;/p&gt; &#xA;&lt;h2&gt;Dataset Analysis&lt;/h2&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from codeinterpreterapi import CodeInterpreterSession, File&#xA;&#xA;&#xA;async def main():&#xA;    # context manager for auto start/stop of the session&#xA;    async with CodeInterpreterSession() as session:&#xA;        # define the user request&#xA;        user_request = &#34;Analyze this dataset and plot something interesting about it.&#34;&#xA;        files = [&#xA;            File.from_path(&#34;examples/assets/iris.csv&#34;),&#xA;        ]&#xA;        &#xA;        # generate the response&#xA;        response = await session.generate_response(&#xA;            user_request, files=files&#xA;        )&#xA;&#xA;        # output to the user&#xA;        print(&#34;AI: &#34;, response.content)&#xA;        for file in response.files:&#xA;            file.show_image()&#xA;&#xA;&#xA;if __name__ == &#34;__main__&#34;:&#xA;    import asyncio&#xA;&#xA;    asyncio.run(main())&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;&lt;img src=&#34;https://github.com/shroominic/codeinterpreter-api/raw/main/examples/assets/iris_analysis.png?raw=true&#34; alt=&#34;Iris Dataset Analysis&#34;&gt;&lt;br&gt; Iris Dataset Analysis Output&lt;/p&gt; &#xA;&lt;h2&gt;Production&lt;/h2&gt; &#xA;&lt;p&gt;In case you want to deploy to production you can use the CodeBox API for easy scaling.&lt;/p&gt; &#xA;&lt;p&gt;Please contact me if you interested in this, because it&#39;s still in early development.&lt;/p&gt; &#xA;&lt;h2&gt;Contributing&lt;/h2&gt; &#xA;&lt;p&gt;There are some TODOs left in the code so if you want to contribute feel free to do so. You can also suggest new features. Code refactoring is also welcome. Just open an issue or pull request and I will review it.&lt;/p&gt; &#xA;&lt;p&gt;Also please submit any bugs you find as an issue with a minimal code example or screenshot. This helps me a lot to improve the code.&lt;/p&gt; &#xA;&lt;p&gt;Thanks!&lt;/p&gt; &#xA;&lt;h2&gt;License&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://choosealicense.com/licenses/mit/&#34;&gt;MIT&lt;/a&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Contact&lt;/h2&gt; &#xA;&lt;p&gt;You can contact me at &lt;a href=&#34;mailto:contact@shroominic.com&#34;&gt;contact@shroominic.com&lt;/a&gt;. But I prefer to use &lt;a href=&#34;https://twitter.com/shroominic&#34;&gt;Twitter&lt;/a&gt; or &lt;a href=&#34;https://gptassistant.app/community&#34;&gt;Discord&lt;/a&gt; DMs.&lt;/p&gt; &#xA;&lt;h2&gt;Support this project&lt;/h2&gt; &#xA;&lt;p&gt;If you want to help this project with a donation you can &lt;a href=&#34;https://ko-fi.com/shroominic&#34;&gt;click here&lt;/a&gt;. Thanks this helps alot! ‚ù§Ô∏è&lt;/p&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;a href=&#34;https://star-history.com/#shroominic/codeinterpreter-api&amp;amp;Date&#34;&gt; &#xA; &lt;picture&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: dark)&#34; srcset=&#34;https://api.star-history.com/svg?repos=shroominic/codeinterpreter-api&amp;amp;type=Date&amp;amp;theme=dark&#34;&gt; &#xA;  &lt;source media=&#34;(prefers-color-scheme: light)&#34; srcset=&#34;https://api.star-history.com/svg?repos=shroominic/codeinterpreter-api&amp;amp;type=Date&#34;&gt; &#xA;  &lt;img alt=&#34;Star History Chart&#34; src=&#34;https://api.star-history.com/svg?repos=shroominic/codeinterpreter-api&amp;amp;type=Date&#34;&gt; &#xA; &lt;/picture&gt; &lt;/a&gt;</summary>
  </entry>
  <entry>
    <title>bigscience-workshop/petals</title>
    <updated>2023-07-20T01:29:15Z</updated>
    <id>tag:github.com,2023-07-20:/bigscience-workshop/petals</id>
    <link href="https://github.com/bigscience-workshop/petals" rel="alternate"></link>
    <summary type="html">&lt;p&gt;üå∏ Run large language models at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading&lt;/p&gt;&lt;hr&gt;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://i.imgur.com/7eR7Pan.png&#34; width=&#34;400&#34;&gt;&lt;br&gt; Run large language models at home, BitTorrent-style.&lt;br&gt; Fine-tuning and inference &lt;a href=&#34;https://github.com/bigscience-workshop/petals#benchmarks&#34;&gt;up to 10x faster&lt;/a&gt; than offloading&lt;br&gt;&lt;br&gt; &lt;a href=&#34;https://pypi.org/project/petals/&#34;&gt;&lt;img src=&#34;https://img.shields.io/pypi/v/petals.svg?color=green&#34;&gt;&lt;/a&gt;&lt;br&gt; &lt;/p&gt; &#xA;&lt;p&gt;Generate text with distributed &lt;a href=&#34;https://ai.meta.com/llama/&#34;&gt;LLaMA 2&lt;/a&gt; (&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-70b-hf&#34;&gt;70B&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/meta-llama/Llama-2-70b-chat-hf&#34;&gt;70B-Chat&lt;/a&gt;), &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/llama_v1/MODEL_CARD.md&#34;&gt;LLaMA-65B&lt;/a&gt;, &lt;a href=&#34;https://huggingface.co/timdettmers/guanaco-65b&#34;&gt;Guanaco-65B&lt;/a&gt; or &lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;BLOOM-176B&lt;/a&gt; and fine‚Äëtune them for your own tasks ‚Äî right from your desktop computer or Google Colab:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from transformers import AutoTokenizer&#xA;from petals import AutoDistributedModelForCausalLM&#xA;&#xA;model_name = &#34;enoch/llama-65b-hf&#34;&#xA;# You can also use &#34;meta-llama/Llama-2-70b-hf&#34;, &#34;meta-llama/Llama-2-70b-chat-hf&#34;,&#xA;# &#34;bigscience/bloom&#34;, or &#34;bigscience/bloomz&#34;&#xA;&#xA;tokenizer = AutoTokenizer.from_pretrained(model_name)&#xA;model = AutoDistributedModelForCausalLM.from_pretrained(model_name)&#xA;# Embeddings &amp;amp; prompts are on your device, transformer blocks are distributed across the Internet&#xA;&#xA;inputs = tokenizer(&#34;A cat sat&#34;, return_tensors=&#34;pt&#34;)[&#34;input_ids&#34;]&#xA;outputs = model.generate(inputs, max_new_tokens=5)&#xA;print(tokenizer.decode(outputs[0]))  # A cat sat on a mat...&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üöÄ &amp;nbsp;&lt;b&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing&#34;&gt;Try now in Colab&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; &#xA;&lt;p&gt;üìã Make sure you follow the model&#39;s terms of use (see &lt;a href=&#34;https://bit.ly/llama2-license&#34;&gt;LLaMA 2&lt;/a&gt;, &lt;a href=&#34;https://bit.ly/llama-license&#34;&gt;LLaMA&lt;/a&gt; and &lt;a href=&#34;https://bit.ly/bloom-license&#34;&gt;BLOOM&lt;/a&gt; licenses).&lt;/p&gt; &#xA;&lt;p&gt;üîè Your data will be processed by other people in the public swarm. Learn more about privacy &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety&#34;&gt;here&lt;/a&gt;. For sensitive data, you can set up a &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm&#34;&gt;private swarm&lt;/a&gt; among people you trust.&lt;/p&gt; &#xA;&lt;h3&gt;Connect your GPU and increase Petals capacity&lt;/h3&gt; &#xA;&lt;p&gt;Run these commands in an &lt;a href=&#34;https://www.anaconda.com&#34;&gt;Anaconda&lt;/a&gt; env (requires Linux and Python 3.8+):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;pip install --upgrade petals&#xA;python -m petals.cli.run_server enoch/llama-65b-hf --adapters timdettmers/guanaco-65b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Or run our &lt;a href=&#34;https://www.docker.com&#34;&gt;Docker&lt;/a&gt; image (works on Linux, macOS, and Windows with &lt;a href=&#34;https://learn.microsoft.com/en-us/windows/ai/directml/gpu-cuda-in-wsl&#34;&gt;WSL2&lt;/a&gt;):&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo docker run -p 31330:31330 --ipc host --gpus all --volume petals-cache:/cache --rm learningathome/petals:main \&#xA;    python -m petals.cli.run_server --port 31330 enoch/llama-65b-hf --adapters timdettmers/guanaco-65b&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;This will host a part of LLaMA-65B with optional &lt;a href=&#34;https://huggingface.co/timdettmers/guanaco-65b&#34;&gt;Guanaco&lt;/a&gt; adapters on your machine. You can also host &lt;code&gt;meta-llama/Llama-2-70b-hf&lt;/code&gt;, &lt;code&gt;meta-llama/Llama-2-70b-chat-hf&lt;/code&gt;, &lt;code&gt;bigscience/bloom&lt;/code&gt;, &lt;code&gt;bigscience/bloomz&lt;/code&gt;, and other compatible models from ü§ó &lt;a href=&#34;https://huggingface.co/models&#34;&gt;Model Hub&lt;/a&gt;, or &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Run-a-custom-model-with-Petals&#34;&gt;add support&lt;/a&gt; for new model architectures.&lt;/p&gt; &#xA;&lt;p&gt;üîí Hosting a server does not allow others to run custom code on your computer. Learn more about security &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Security,-privacy,-and-AI-safety&#34;&gt;here&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;üí¨ See &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#running-a-server&#34;&gt;FAQ&lt;/a&gt; to learn how to use multple GPUs, restart the server on reboot, etc. If you have any issues or feedback, ping us in &lt;a href=&#34;https://discord.gg/D9MwApKgWa&#34;&gt;our Discord&lt;/a&gt;!&lt;/p&gt; &#xA;&lt;h3&gt;Check out tutorials, examples, and more&lt;/h3&gt; &#xA;&lt;p&gt;Basic tutorials:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Getting started: &lt;a href=&#34;https://colab.research.google.com/drive/1uCphNY7gfAUkdDrTx21dZZwCOUDCMPw8?usp=sharing&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prompt-tune LLaMA-65B for text semantic classification: &lt;a href=&#34;https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Prompt-tune BLOOM to create a personified chatbot: &lt;a href=&#34;https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-personachat.ipynb&#34;&gt;tutorial&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Useful tools and advanced guides:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://chat.petals.ml&#34;&gt;Chatbot web app&lt;/a&gt; (connects to Petals via an HTTP/WebSocket endpoint): &lt;a href=&#34;https://github.com/borzunov/chat.petals.ml&#34;&gt;source code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;http://health.petals.ml&#34;&gt;Monitor&lt;/a&gt; for the public swarm: &lt;a href=&#34;https://github.com/borzunov/health.petals.ml&#34;&gt;source code&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Launch your own swarm: &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Launch-your-own-swarm&#34;&gt;guide&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;Run a custom foundation model: &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/Run-a-custom-model-with-Petals&#34;&gt;guide&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p&gt;Learning more:&lt;/p&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Frequently asked questions: &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions&#34;&gt;FAQ&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;In-depth system description: &lt;a href=&#34;https://arxiv.org/abs/2209.01188&#34;&gt;paper&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;How does it work?&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Petals runs large language models like &lt;a href=&#34;https://github.com/facebookresearch/llama/raw/main/MODEL_CARD.md&#34;&gt;LLaMA&lt;/a&gt; and &lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34;&gt;BLOOM&lt;/a&gt; &lt;strong&gt;collaboratively&lt;/strong&gt; ‚Äî you load a small part of the model, then team up with people serving the other parts to run inference or fine-tuning.&lt;/li&gt; &#xA; &lt;li&gt;Single-batch inference runs at up to 6 steps/sec for LLaMA 2 (70B) and ‚âà 1 step/sec for BLOOM-176B. This is &lt;a href=&#34;https://github.com/bigscience-workshop/petals#benchmarks&#34;&gt;up to 10x faster&lt;/a&gt; than offloading, enough for &lt;a href=&#34;http://chat.petals.ml&#34;&gt;chatbots&lt;/a&gt; and other interactive apps. Parallel inference reaches hundreds of tokens/sec.&lt;/li&gt; &#xA; &lt;li&gt;Beyond classic language model APIs ‚Äî you can employ any fine-tuning and sampling methods, execute custom paths through the model, or see its hidden states. You get the comforts of an API with the flexibility of PyTorch.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://i.imgur.com/RTYF3yW.png&#34; width=&#34;800&#34;&gt; &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; üìö &amp;nbsp;&lt;b&gt;&lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions&#34;&gt;See FAQ&lt;/a&gt;&lt;/b&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; üìú &amp;nbsp;&lt;b&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.01188.pdf&#34;&gt;Read paper&lt;/a&gt;&lt;/b&gt; &lt;/p&gt; &#xA;&lt;h2&gt;Installation&lt;/h2&gt; &#xA;&lt;p&gt;Here&#39;s how to install Petals with &lt;a href=&#34;https://www.anaconda.com/products/distribution&#34;&gt;Anaconda&lt;/a&gt; on Linux:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;conda install pytorch pytorch-cuda=11.7 -c pytorch -c nvidia&#xA;pip install --upgrade petals&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;If you don&#39;t use Anaconda, you can install PyTorch in &lt;a href=&#34;https://pytorch.org/get-started/locally/&#34;&gt;any other way&lt;/a&gt;. If you want to run models with 8-bit weights, please install PyTorch with CUDA 11.x or newer for compatility with &lt;a href=&#34;https://github.com/timDettmers/bitsandbytes&#34;&gt;bitsandbytes&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;p&gt;See the instructions for macOS and Windows, the full requirements, and troubleshooting advice in our &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#running-a-client&#34;&gt;FAQ&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Benchmarks&lt;/h2&gt; &#xA;&lt;p&gt;The benchmarks below are for BLOOM-176B:&lt;/p&gt; &#xA;&lt;table align=&#34;center&#34;&gt; &#xA; &lt;tbody&gt;&#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Network&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Single-batch inference&lt;br&gt;(steps/s)&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Parallel forward&lt;br&gt;(tokens/s)&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Bandwidth&lt;/th&gt; &#xA;   &lt;th rowspan=&#34;2&#34;&gt;Round-trip&lt;br&gt;latency&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Sequence length&lt;/th&gt; &#xA;   &lt;th colspan=&#34;2&#34;&gt;Batch size&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;128&lt;/td&gt; &#xA;   &lt;td&gt;2048&lt;/td&gt; &#xA;   &lt;td&gt;1&lt;/td&gt; &#xA;   &lt;td&gt;64&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;6&#34;&gt;Offloading, max. possible speed on 1x A100 &lt;sup&gt;1&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;256 Gbit/s&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.18&lt;/td&gt; &#xA;   &lt;td&gt;0.18&lt;/td&gt; &#xA;   &lt;td&gt;2.7&lt;/td&gt; &#xA;   &lt;td&gt;170.3&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;128 Gbit/s&lt;/td&gt; &#xA;   &lt;td&gt;&lt;/td&gt; &#xA;   &lt;td&gt;0.09&lt;/td&gt; &#xA;   &lt;td&gt;0.09&lt;/td&gt; &#xA;   &lt;td&gt;2.4&lt;/td&gt; &#xA;   &lt;td&gt;152.8&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;6&#34;&gt;Petals on 14 heterogeneous servers across Europe and North America &lt;sup&gt;2&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td colspan=&#34;2&#34;&gt;Real world&lt;/td&gt; &#xA;   &lt;td&gt;0.83&lt;/td&gt; &#xA;   &lt;td&gt;0.79&lt;/td&gt; &#xA;   &lt;td&gt;32.6&lt;/td&gt; &#xA;   &lt;td&gt;179.4&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th colspan=&#34;6&#34;&gt;Petals on 3 servers, with one A100 each &lt;sup&gt;3&lt;/sup&gt;&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;1 Gbit/s&lt;/td&gt; &#xA;   &lt;td&gt;&amp;lt; 5 ms&lt;/td&gt; &#xA;   &lt;td&gt;1.71&lt;/td&gt; &#xA;   &lt;td&gt;1.54&lt;/td&gt; &#xA;   &lt;td&gt;70.0&lt;/td&gt; &#xA;   &lt;td&gt;253.6&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;100 Mbit/s&lt;/td&gt; &#xA;   &lt;td&gt;&amp;lt; 5 ms&lt;/td&gt; &#xA;   &lt;td&gt;1.66&lt;/td&gt; &#xA;   &lt;td&gt;1.49&lt;/td&gt; &#xA;   &lt;td&gt;56.4&lt;/td&gt; &#xA;   &lt;td&gt;182.0&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr align=&#34;center&#34;&gt; &#xA;   &lt;td&gt;100 Mbit/s&lt;/td&gt; &#xA;   &lt;td&gt;100 ms&lt;/td&gt; &#xA;   &lt;td&gt;1.23&lt;/td&gt; &#xA;   &lt;td&gt;1.11&lt;/td&gt; &#xA;   &lt;td&gt;19.7&lt;/td&gt; &#xA;   &lt;td&gt;112.2&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt;&#xA;&lt;/table&gt; &#xA;&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; &lt;strong&gt;An upper bound for offloading performance.&lt;/strong&gt; We base our offloading numbers on the best possible hardware setup for offloading: CPU RAM offloading via PCIe 4.0 with 16 PCIe lanes per GPU and PCIe switches for pairs of GPUs. We assume zero latency for the upper bound estimation. In 8-bit, the model uses 1 GB of memory per billion parameters. PCIe 4.0 with 16 lanes has a throughput of 256 Gbit/s, so offloading 176B parameters takes 5.5 seconds. The throughput is twice as slow (128 Gbit/s) if we have two GPUs behind the same PCIe switch.&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;2&lt;/sup&gt; &lt;strong&gt;A real-world distributed setting&lt;/strong&gt; with 14 servers holding 2√ó RTX 3060, 4√ó 2080Ti, 2√ó 3090, 2√ó A4000, and 4√ó A5000 GPUs. These are personal servers and servers from university labs, spread across Europe and North America and connected to the Internet at speeds of 100‚Äì1000 Mbit/s. 4 servers operate from under firewalls.&lt;/p&gt; &#xA;&lt;p&gt;&lt;sup&gt;3&lt;/sup&gt; &lt;strong&gt;An optimistic setup&lt;/strong&gt; that requires least communication. The client nodes have 8 CPU cores and no GPU.&lt;/p&gt; &#xA;&lt;p&gt;We provide more evaluations and discuss these results in more detail in &lt;strong&gt;Section 3.3&lt;/strong&gt; of our &lt;a href=&#34;https://arxiv.org/pdf/2209.01188.pdf&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;üõ†Ô∏è Contributing&lt;/h2&gt; &#xA;&lt;p&gt;Please see our &lt;a href=&#34;https://github.com/bigscience-workshop/petals/wiki/FAQ:-Frequently-asked-questions#contributing&#34;&gt;FAQ&lt;/a&gt; on contributing.&lt;/p&gt; &#xA;&lt;h2&gt;üìú Citation&lt;/h2&gt; &#xA;&lt;p&gt;Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. &lt;a href=&#34;https://arxiv.org/abs/2209.01188&#34;&gt;Petals: Collaborative Inference and Fine-tuning of Large Models.&lt;/a&gt; &lt;em&gt;arXiv preprint arXiv:2209.01188,&lt;/em&gt; 2022.&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@article{borzunov2022petals,&#xA;  title = {Petals: Collaborative Inference and Fine-tuning of Large Models},&#xA;  author = {Borzunov, Alexander and Baranchuk, Dmitry and Dettmers, Tim and Ryabinin, Max and Belkada, Younes and Chumachenko, Artem and Samygin, Pavel and Raffel, Colin},&#xA;  journal = {arXiv preprint arXiv:2209.01188},&#xA;  year = {2022},&#xA;  url = {https://arxiv.org/abs/2209.01188}&#xA;}&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;hr&gt; &#xA;&lt;p align=&#34;center&#34;&gt; This project is a part of the &lt;a href=&#34;https://bigscience.huggingface.co/&#34;&gt;BigScience&lt;/a&gt; research workshop. &lt;/p&gt; &#xA;&lt;p align=&#34;center&#34;&gt; &lt;img src=&#34;https://petals.ml/bigscience.png&#34; width=&#34;150&#34;&gt; &lt;/p&gt;</summary>
  </entry>
  <entry>
    <title>ramonvc/freegpt-webui</title>
    <updated>2023-07-20T01:29:15Z</updated>
    <id>tag:github.com,2023-07-20:/ramonvc/freegpt-webui</id>
    <link href="https://github.com/ramonvc/freegpt-webui" rel="alternate"></link>
    <summary type="html">&lt;p&gt;GPT 3.5/4 with a Chat Web UI. No API key required.&lt;/p&gt;&lt;hr&gt;&lt;h1&gt;FreeGPT WebUI&lt;/h1&gt; &#xA;&lt;h2&gt;GPT 3.5/4&lt;/h2&gt; &#xA;&lt;p&gt;&lt;strong&gt;NOT REQUIRE ANY API KEY&lt;/strong&gt; ‚ùåüîë&lt;/p&gt; &#xA;&lt;p&gt;This project features a WebUI utilizing the &lt;a href=&#34;https://github.com/xtekky/gpt4free&#34;&gt;G4F API&lt;/a&gt;. &lt;br&gt; Experience the power of ChatGPT with a user-friendly interface, enhanced jailbreaks, and completely free.&lt;/p&gt; &#xA;&lt;h2&gt;Known bugs üöß&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;Stream mode not working properly.&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;News üì¢&lt;/h2&gt; &#xA;&lt;p&gt;I have created a new version of FreeGPT WebUI using the &lt;a href=&#34;https://chimeragpt.adventblocks.cc/&#34;&gt;ChimeraGPT API&lt;/a&gt;. &lt;br&gt; &lt;br&gt; This free API allows you to use various AI chat models, including &lt;strong&gt;GPT-4, GPT-4-32k, Claude-2, Claude-2-100k, and more.&lt;/strong&gt; &lt;br&gt; Check out the project here: &lt;a href=&#34;https://github.com/ramonvc/freegpt-webui/tree/chimeragpt-version&#34;&gt;FreeGPT WebUI - Chimera Version&lt;/a&gt;.&lt;/p&gt; &#xA;&lt;h2&gt;Project Hosting and Demonstration üåêüöÄ&lt;/h2&gt; &#xA;&lt;p&gt;The project is hosted on multiple platforms to be tested and modified.&lt;/p&gt; &#xA;&lt;table&gt; &#xA; &lt;thead&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;th&gt;Plataform&lt;/th&gt; &#xA;   &lt;th&gt;Status&lt;/th&gt; &#xA;   &lt;th&gt;API Key&lt;/th&gt; &#xA;   &lt;th&gt;Free&lt;/th&gt; &#xA;   &lt;th&gt;Repo&lt;/th&gt; &#xA;   &lt;th&gt;Demo&lt;/th&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/thead&gt; &#xA; &lt;tbody&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://replit.com/&#34;&gt;replit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://img.shields.io/badge/Active-brightgreen&#34; alt=&#34;Active&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚óºÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;‚òëÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://replit.com/@ramonvc/freegpt-webui&#34;&gt;FreeGPT WebUI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://freegpt-webui.ramonvc.repl.co/chat/&#34;&gt;Chat&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co&#34;&gt;hugging face&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://img.shields.io/badge/Active-brightgreen&#34; alt=&#34;Active&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚óºÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;‚òëÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/monra/freegpt-webui/tree/main&#34;&gt;FreeGPT WebUI&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://huggingface.co/spaces/monra/freegpt-webui&#34;&gt;Chat&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA;  &lt;tr&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://replit.com/&#34;&gt;replit&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;img src=&#34;https://img.shields.io/badge/Active-brightgreen&#34; alt=&#34;Active&#34;&gt;&lt;/td&gt; &#xA;   &lt;td&gt;‚òëÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;‚òëÔ∏è&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://replit.com/@ramonvc/freegpt-webui-chimera&#34;&gt;FreeGPT WebUI - Chimera Version&lt;/a&gt;&lt;/td&gt; &#xA;   &lt;td&gt;&lt;a href=&#34;https://freegpt-webui-chimera.ramonvc.repl.co/chat/&#34;&gt;Chat&lt;/a&gt;&lt;/td&gt; &#xA;  &lt;/tr&gt; &#xA; &lt;/tbody&gt; &#xA;&lt;/table&gt; &#xA;&lt;h2&gt;Note ‚ÑπÔ∏è&lt;/h2&gt; &#xA;&lt;p&gt; FreeGPT is a project that utilizes various free AI conversation API Providers. Each Provider is an API that provides responses generated by different AI models. The source code related to these services is available in &lt;a href=&#34;https://github.com/ramonvc/freegpt-webui/tree/main/g4f&#34;&gt;G4F folder&lt;/a&gt;. &lt;/p&gt;&#xA;&lt;p&gt;It is important to note that, due to the extensive reach of this project, the free services registered here may receive a significant number of requests, which can result in temporary unavailability or access limitations. Therefore, it is common to encounter these services being offline or unstable.&lt;/p&gt; &#xA;&lt;p&gt;We recommend that you search for your own Providers and add them to your personal projects to avoid service instability and unavailability. Within the project, in the &lt;a href=&#34;https://github.com/ramonvc/freegpt-webui/tree/main/g4f/Provider/Providers&#34;&gt;Providers folder&lt;/a&gt;, you will find several examples of Providers that have worked in the past or are still functioning. It is easy to follow the logic of these examples to find free GPT services and incorporate the requests into your specific FreeGPT project.&lt;/p&gt; &#xA;&lt;p&gt;Please note that the choice and integration of additional Providers are the user&#39;s responsibility and are not directly related to the FreeGPT project, as the project serves as an example of how to combine the &lt;a href=&#34;https://github.com/xtekky/gpt4free&#34;&gt;G4F API&lt;/a&gt; with a web interface.&lt;/p&gt; &#xA;&lt;p&gt;&lt;/p&gt; &#xA;&lt;h2&gt;Table of Contents&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#to-do-list-%EF%B8%8F&#34;&gt;To-Do List&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#getting-started-white_check_mark&#34;&gt;Getting Started&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#cloning-the-repository-inbox_tray&#34;&gt;Cloning the Repository&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#install-dependencies-wrench&#34;&gt;Install Dependencies&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#running-the-application-rocket&#34;&gt;Running the Application&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#docker-&#34;&gt;Docker&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#prerequisites&#34;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#running-the-docker&#34;&gt;Running the Docker&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#incorporated-projects-busts_in_silhouette&#34;&gt;Incorporated Projects&lt;/a&gt; &#xA;  &lt;ul&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#webui&#34;&gt;WebUI&lt;/a&gt;&lt;/li&gt; &#xA;   &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#api-g4f&#34;&gt;API FreeGPT&lt;/a&gt;&lt;/li&gt; &#xA;  &lt;/ul&gt; &lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#star-history&#34;&gt;Star History&lt;/a&gt;&lt;/li&gt; &#xA; &lt;li&gt;&lt;a href=&#34;https://raw.githubusercontent.com/ramonvc/freegpt-webui/main/#legal-notice&#34;&gt;Legal Notice&lt;/a&gt;&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;&lt;/h2&gt; &#xA;&lt;h2&gt;To-Do List ‚úîÔ∏è&lt;/h2&gt; &#xA;&lt;ul&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Integrate the free GPT API into the WebUI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Create Docker support&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Improve the Jailbreak functionality&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Add the GPT-4 model&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled&gt; Enhance the user interface&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Check status of API Providers (online/offline)&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Enable editing and creating Jailbreaks/Roles in the WebUI&lt;/li&gt; &#xA; &lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled&gt; Refactor web client&lt;/li&gt; &#xA;&lt;/ul&gt; &#xA;&lt;h2&gt;Getting Started &lt;span&gt;‚úÖ&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To get started with this project, you&#39;ll need to clone the repository and have &lt;a href=&#34;https://www.python.org/downloads/&#34;&gt;Python&lt;/a&gt; installed on your system.&lt;/p&gt; &#xA;&lt;h3&gt;Cloning the Repository &lt;span&gt;üì•&lt;/span&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Run the following command to clone the repository:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;git clone https://github.com/ramonvc/freegpt-webui.git&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h3&gt;Install Dependencies &lt;span&gt;üîß&lt;/span&gt;&lt;/h3&gt; &#xA;&lt;p&gt;Navigate to the project directory:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;cd freegpt-webui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Install the dependencies:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Running the Application &lt;span&gt;üöÄ&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;To run the application, run the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;python run.py&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Access the application in your browser using the URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://127.0.0.1:1338&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://localhost:1338&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Docker üê≥&lt;/h2&gt; &#xA;&lt;h3&gt;Prerequisites&lt;/h3&gt; &#xA;&lt;p&gt;Before you start, make sure you have installed &lt;a href=&#34;https://www.docker.com/get-started&#34;&gt;Docker&lt;/a&gt; on your machine.&lt;/p&gt; &#xA;&lt;h3&gt;Running the Docker&lt;/h3&gt; &#xA;&lt;p&gt;Pull the Docker image from Docker Hub:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker pull ramonvc/freegpt-webui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Run the application using Docker:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker run -p 1338:1338 ramonvc/freegpt-webui&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;Access the application in your browser using the URL:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://127.0.0.1:1338&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;or&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;http://localhost:1338&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;p&gt;When you&#39;re done using the application, stop the Docker containers using the following command:&lt;/p&gt; &#xA;&lt;pre&gt;&lt;code&gt;docker stop &amp;lt;container-id&amp;gt;&#xA;&lt;/code&gt;&lt;/pre&gt; &#xA;&lt;h2&gt;Incorporated Projects &lt;span&gt;üë•&lt;/span&gt;&lt;/h2&gt; &#xA;&lt;p&gt;I highly recommend visiting and supporting both projects.&lt;/p&gt; &#xA;&lt;h3&gt;WebUI&lt;/h3&gt; &#xA;&lt;p&gt;The application interface was incorporated from the &lt;a href=&#34;https://github.com/xtekky/chatgpt-clone&#34;&gt;chatgpt-clone&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;h3&gt;API G4F&lt;/h3&gt; &#xA;&lt;p&gt;The free GPT-4 API was incorporated from the &lt;a href=&#34;https://github.com/xtekky/gpt4free&#34;&gt;GPT4Free&lt;/a&gt; repository.&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Star History&lt;/h2&gt; &#xA;&lt;p&gt;&lt;a href=&#34;https://star-history.com/#ramonvc/freegpt-webui&amp;amp;Timeline&#34;&gt;&lt;img src=&#34;https://api.star-history.com/svg?repos=ramonvc/freegpt-webui&amp;amp;type=Timeline&#34; alt=&#34;Star History Chart&#34;&gt;&lt;/a&gt;&lt;/p&gt; &#xA;&lt;br&gt; &#xA;&lt;h2&gt;Legal Notice&lt;/h2&gt; &#xA;&lt;p&gt;This repository is &lt;em&gt;not&lt;/em&gt; associated with or endorsed by providers of the APIs contained in this GitHub repository. This project is intended &lt;strong&gt;for educational purposes only&lt;/strong&gt;. This is just a little personal project. Sites may contact me to improve their security or request the removal of their site from this repository.&lt;/p&gt; &#xA;&lt;p&gt;Please note the following:&lt;/p&gt; &#xA;&lt;ol&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;: The APIs, services, and trademarks mentioned in this repository belong to their respective owners. This project is &lt;em&gt;not&lt;/em&gt; claiming any right over them nor is it affiliated with or endorsed by any of the providers mentioned.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Responsibility&lt;/strong&gt;: The author of this repository is &lt;em&gt;not&lt;/em&gt; responsible for any consequences, damages, or losses arising from the use or misuse of this repository or the content provided by the third-party APIs. Users are solely responsible for their actions and any repercussions that may follow. We strongly recommend the users to follow the TOS of the each Website.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Educational Purposes Only&lt;/strong&gt;: This repository and its content are provided strictly for educational purposes. By using the information and code provided, users acknowledge that they are using the APIs and models at their own risk and agree to comply with any applicable laws and regulations.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Copyright&lt;/strong&gt;: All content in this repository, including but not limited to code, images, and documentation, is the intellectual property of the repository author, unless otherwise stated. Unauthorized copying, distribution, or use of any content in this repository is strictly prohibited without the express written consent of the repository author.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Indemnification&lt;/strong&gt;: Users agree to indemnify, defend, and hold harmless the author of this repository from and against any and all claims, liabilities, damages, losses, or expenses, including legal fees and costs, arising out of or in any way connected with their use or misuse of this repository, its content, or related third-party APIs.&lt;/p&gt; &lt;/li&gt; &#xA; &lt;li&gt; &lt;p&gt;&lt;strong&gt;Updates and Changes&lt;/strong&gt;: The author reserves the right to modify, update, or remove any content, information, or features in this repository at any time without prior notice. Users are responsible for regularly reviewing the content and any changes made to this repository.&lt;/p&gt; &lt;/li&gt; &#xA;&lt;/ol&gt; &#xA;&lt;p&gt;By using this repository or any code related to it, you agree to these terms. The author is not responsible for any copies, forks, or reuploads made by other users. This is the author&#39;s only account and repository. To prevent impersonation or irresponsible actions, you may comply with the GNU GPL license this Repository uses.&lt;/p&gt;</summary>
  </entry>
</feed>